{"name": "optimize your algorithms ", "full_name": " h1 Optimize Your Algorithms h2 Index h2 1 Importing Libraries and Data Cleaning h1 2 Data Analysis h1 3 Testing different models with default parameters h1 4 Support Vector Machines SVM h3 How does it work h3 What parameters can we tune h3 Default parameters h2 C Penalty Parameter h2 Gamma How exact should I fit the data h2 Kernel How do I fit the data h2 Degree How sharp are the Poly Kernel s lines h2 C Again h1 5 Ridge Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 Alpha Forcing coefficients toward zero h1 6 Decision Tree Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 min samples leaf Minimum number of samples h2 max depth h2 Depth Samples h1 7 K Neighbors Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 n neighbors How many points are around me h2 Algorithm How do I find my neighbors h2 Weight Are all my neighbors equal h1 8 Extra Trees Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 n estimators How many h1 9 Random Forest Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h1 10 Gradient Boosting Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 learning rate How much does a single Decision Tree contribute h1 11 Ada Boost Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 base estimator What kind of model do I use h1 12 Final Score h1 13 Conclusion h1 14 Sources ", "stargazers_count": 0, "forks_count": 0, "description": "Testing different models with default parametersBefore we start optimising our models we ll first run them with the default parameters. We have already discussed max_depth and min_samples_leaf in Decision Trees so we will skip their explanation in this part. Now let s take a look at some of the differences in the data between benign and malignant cell nuclei starting with radius and texture. This function fits and tests the model multiple times and then returns a list of scores. The AdaBoostClassifier is the best model however with almost 98 accuracy. K Neighbors Classifier8. Default parameters n_estimators 10 max_depth None min_samples_leaf 1 n_estimators How many The value for this parameter determines how many Decision Trees are made by the model. 7 but to be sure we ll graph a large range of alphas. Importing Libraries and Data Cleaning2. Just as we concluded earlier a high value of the radius texture perimeter and smoothness are all indicators of malignant cell nucleusAgain these correlations match up with our earlier conclusions. Final ScoreSo now we can see the results of our hard work. Let s start by looking at the distribution of benign and malignant cell nuclei. Algorithm How do I find my neighbors There are 3 types of algorithms to use for finding neighbors ball_tree kd_tree brute force If you give the parameter auto the model will try to find the best model himself. So if you have the time I definitely recommend optimizing your models. The worst of a data point is the highest largest number of the data for this cell nucleus. html SVM Tuning https medium. We can drop it without affecting our data. These kernels can all be tuned using different parameters. Using the sort_values function on the DataFrame gives us a nice overview of the performance of all the tested models. These values don t seem to change our predictions so we ll wait untill we have some more parameters figured out and then we ll come back to the C parameter. Every leaf has a condition that is either True or False. Max_depth doesn t matter much after 4 layers. Using the mean of this list we get an average score of the model. Starting with a simple list of values from 0. 001 to 1000This range doesn t give us any different scores so we ll try a more extreme range. Kernels Parameters Linear C RBF C gamma Poly C gamma degree Default parametersBy default SVC uses these default values kernel rbf C 1. Using the graphviz library we ll visualize the end result of the Decision Tree. Data AnalysisNow that our data has been cleaned we can take a look at what it contains. It chooses coefficients in this formula based on the data given. As expected we don t see any spikes in accuracy above 0. Sources Introduction To Machine Learning by A. The mean of the concavity and concave points in a malignant cell nucleus are more than triple the mean of a benign cell nucleus When we take a closer look at the fractal dimension of a cell nucleus we can see that while the mean doesn t differ much between benign and malignant cell nuclei the worst values of malignant cell nuclei are higher than those of benign cell nuclei. The standard deviation of the fractal dimensions of a malignant cell nucleus are also higher though not as significantly as some other values. Gamma How exact should I fit the data The higher the gamma value the harder it tries to exactly fit the training data setLet s start by using a range of 0. The mean radius and texture of malignant cell nuclei are significantly higher than those of the benign cell nuclei. These values can later be used to compare the optimised models against them. max_depthThe integer you set here determines the maximum number of layers your Decision Tree makes. Support Vector Machines SVM How does it work An SVM uses hyperplanes to classify the data. This will ensure more optimal results later on. Using the default parameters of a model can be useful if you need a quick indication of its effectiveness but it won t always give you the accuracy you need. We have already discussed all of these in previous models max_depth min_samples_leaf in Decision Tree and n_estimators in Extra Trees. And finally the fractional dimension worst and standard deviation are also both useful in predicting malignant cell nuclei. Guido Scikit learn https scikit learn. The fractal dimension mean is the only mean that isn t effective when trying to identify a malignant cell nucleus. 1 n_estimators 10 max_depth None learning_rate How much does a single Decision Tree contribute The learning_rate parameter shrinks the contribution of each tree by learning_rate. There seems to be a column named Unnamed 32 that is full of NaN values. If alpha is higher than it will force the coefficients more towards zero. We are going to test the models using cross validation score. Looking at the remaining data we can see this trend continuing for every data point except fractal dimension. Default parameters n_estimators 10 max_depth None min_samples_leaf 1Let s start by trying N 1 100 Depth 1 30 and Samples 1 5Looking at the plots we can clearly see that n_estimators should be somewhere between 40 and 80 max_depth should be above 5 and samples can vary wildly. However this does require a lot more work and time than just using the default parameters. Ridge Classifier How does it work The Ridge Classifier creates a formula that outputs the prediction. This sounds really similar to the Extra Trees Classifier model because it is. The closer these coefficients are to 0 the better the predictions. So these are clear indications of cell nuclei being malignant. The big difference is that the Random Forest Classifier chooses the features it will use by the most discriminative thresholds instead of randomly. C AgainNow that we have some more parameters tuned let s try our C parameter again. Let s start by trying N 1 100 Depth 1 30 and Samples 0. Gradient Boosting Classifier How does it work The Gradient Boosting Classifier builds multiple Decision Trees just like the Random Forest Classifier but does this in different stages while trying to optimize the loss function. In this kernel I ll show you some predictive models and I ll also show you how to tune them. It controls the trade off between smooth decision boundary and classifying the training points correctly. We have 2 types of weight to give to data points uniform All data points are equal distance The closer a data point is the more influential it is to the predictionThe uniform weight apparently suits our data better though it doesn t differ much from the distance weight. The mean of the perimeter also shows a significant increase when a cell nucleus is malignant. Let s start by creating a List with some basic values to check the average score they give our model. We have already discussed max_depth in Decision Tree and n_estimators in Extra Trees. Extra Trees Classifier9. Importing Libraries and Data CleaningBefore we can start trying different algorithms we will have to clean the data we re going to use. For each point of data except diagnosis we get 3 numbers the mean the standard deviation and the worst. Knowing this we can try the other parameters a bit more in depth. The best depths are above 10 according to the plot. What parameters can we tune The Gradient Boosting Classifier that we ll tune are learning_rate n_estimators max_depth. We are going to start by importing the libraries we are going to use and turning our csv into a pandas DataFrame. Random Forest Classifier How does it work The Random Forest Classifier fits a number of randomized Decision Trees on sub samples of the dataset and then uses averaging to get a more accurate prediction. What parameters can we tune SVM s are able to use different kernels to classify the given data. Another clear indication. Support Vector Machines SVM 5. The data contains 357 benign and 212 malignant cell nuclei. All of the models that we ve tuned have higher outcomes than the default models. What parameters can we tune This algorithm uses 3 parameters that we can tune n_neighbors weight and algorithm. When possible we use random_state 0 to ensure fair results. What parameters can we tune The Random Forest parameters that we ll tune are n_estimators max_depth and min_samples_leaf. The leaves show multiple pieces of information Condition of the leaf Gini or chance of incorrect measurement of a random training sample at that point The number of samples that passed during fitting Class or prediction of the sample at that pointAs we can see in the image not all bottom leaves have a gini equal to 0. This time we ll use all numbers between 0. We get a lot of data about the cell nucleus radius perimeter area smoothness compactness concavity concave points symmetry fractal dimension and diagnosis. Depth SamplesLet s try both max_depth and min_samples_leaf at once to make sure we have the optimal values. This means that approximately 1 out of 3 cell nuclei in the dataset is malignant. 1 1Looking at these results and plots we can see that n_estimators should be somewhere above 60 a max_depth of 1 is optimal and the learning_rate should be somewhere between 0. It seems that the algorithm doesn t matter much this time but it never hurts to have checked. What parameters can we tune We ll use the following parameters base_estimator n_estimators learning_rateWe have already discussed n_estimators in Extra Trees and learning_rate in Gradient Boosting. It will only be allowed if there are at least X training samples in both the left and right branch of the node where X is the integer you gave the parameter. Weight Are all my neighbors equal We have all these different neighbors but they aren t all the same. 001Now we get some good results out of the SVM. Extra Trees Classifier How does it work The Extra Trees Classifier fits a number of randomized Decision Trees on sub samples of the dataset and then uses averaging to get a more accurate prediction. Hyperplanes are subspaces that consist of one less dimension than the original space. com all things ai in depth parameter tuning for gradient boosting 3363992e9bae linear algebra data processing CSV file I O e. Before we take a deeper look at these correlations we have to edit our data so the diagnosis becomes a boolean instead of a character. So a 3D space becomes 2D a 2D space becomes 1D etcetera. K Neighbors Classifier How does it work The K Neighbors Classifier searches the correct answer based on the data it already has. Default parameters n_neighbors 5 algorithm auto weight uniform n_neighbors How many points are around me The value of this parameter determines how much points around the target data point are measured to determine the outcome. 0 Alpha Forcing coefficients toward zeroRidge Classifiers use alpha to force coefficients more less toward zero. By tuning your parameters you can increase or decrease the accuracy of your model. 001 to 1000 we ll try to find the optimal alpha. 0 gamma auto 1 n_features degree 3 C Penalty ParameterC is the penalty parameter of the error term. The accuracy became around 0. read_csv Gaussian Process Naive Bayes. Random Forest Classifier10. Not every neighbor is equally far away from our data point. 1 1Looking at the results and at the heatmap we see that samples isn t really affecting our average score. These could be improved by gathering more data that involves these leaves though this is not always possible. What parameters can we tune The parameters we ll tune are min_samples_leaf and max_depth Default parametersBy default Decision Tree Classifiers use these default values min_samples_leaf 2 max_depth None min_samples_leaf Minimum number of samplesThe min_samples_leaf parameter decides if a Decision Tree is allowed to split a node. The most surprising one is the SVM who went up by about 0. As shown in the table and plot the optimal number of neighbors is 14. The models are going to predict whether the cell nucleus is malignant or not. Decision Tree Classifier How does it work Decision Trees create a network of leaves or nodes and branches between these leaves. The other plots show that the optimal number of trees is somewhere between 10 and 40. Gradient Boosting Classifier11. Default parameters learning_rate 0. This impacts our predictions. The plots show that the optimal n_estimators are mostly around 35 and the optimal maximum depth is mostly above 17. We ll try values between 1 and 3. It tries to predict the classification by looking at which data points are near it and what they are classified as. 3 points higher Kernel How do I fit the data The kernel decides how the data gets seperated on the hyperplane. Here is where the parameters come in. Let s take a look at the data we ve just imported. Default parameters base_estimator DecisionTreeClassifier max_depth 1 n_estimators 50 learning_rate 1 base_estimator What kind of model do I use The base estimator is the model on which the boost is built. Let s start by trying N 1 100 Depth 1 30 and learning_rate 0. Ada Boost Classifier How does it work The Ada Boost Classifier builds the same model multiple times but the data gets assigned different weights every time. ConclusionBy tuning your models you can squeeze a bit more performance out of them. Optimize Your AlgorithmsDo you ever wonder if you can squeeze some more performance out of your models I certainly do. This then sends the user over to another leaf etcetera. These leaves are the ones that could give use incorrect predictions. Ada Boost Classifier 12. Any values above this will only lead to really slow models. This should be more than enough to train our models. com all things ai in depth parameter tuning for svc 758215394769 GBC Tuning https medium. Testing different models with default parameters4. It turns out that the Poly Kernel is the best type of kernel for our data Degree How sharp are the Poly Kernel s lines Degree determines the degrees of the polynomial used to split the data on the hyperplane. What parameters can we tune The only parameter in this model that we can tune is alpha Default parametersBy default Ridge Classifiers uses these default values alpha 1. It s clear that lower alphas are better in this case so we ll create a longer list with some lower valuesIt s clear in this graph that we won t get a much better prediction than around 0. Decision Tree Classifier7. To view the model s performance we ll make a new DataFrame in which we ll store the Algorithm it s scores and the standard deviation. What parameters can we tune The Extra Tree Classifier parameters that we ll tune are n_estimators max_depth and min_samples_leaf. ", "id": "veleon/optimize-your-algorithms", "size": "15335", "language": "python", "html_url": "https://www.kaggle.com/code/veleon/optimize-your-algorithms", "git_url": "https://www.kaggle.com/code/veleon/optimize-your-algorithms", "script": "ensemble seaborn naive_bayes sklearn linear_model neighbors matplotlib.pyplot cross_val_score gaussian_process tree svm sklearn.model_selection pandas numpy ", "entities": "(('we', 'alphas'), 'be') (('you', 'always accuracy'), 'be') (('it', 'most discriminative thresholds'), 'be') (('boost', 'which'), 'parameter') (('SVM', 'data'), 'SVM') (('at once we', 'optimal values'), 'SamplesLet') (('So these', 'cell clear nuclei'), 'be') (('neighbor', 'data equally far away point'), 'be') (('We', 'Gradient Boosting'), 'tune') (('Max_depth doesn', 't much after 4 layers'), 'matter') (('Degree', 'hyperplane'), 'turn') (('We', 'data'), 'drop') (('finally fractional dimension worst deviation', 'cell also both malignant nuclei'), 'be') (('Decision Tree', 'layers'), 'determine') (('samples', '5'), 'parameter') (('Random Forest Classifier', 'then more accurate prediction'), 'classifier') (('we', 'default models'), 'have') (('we', 'around 0'), 's') (('we', 'Decision Tree'), 'visualize') (('Kernels Parameters Linear C RBF C gamma Poly C gamma degree', 'default default Default parametersBy values'), 'use') (('cell approximately 1 out of 3 nuclei', 'dataset'), 'mean') (('we', 'pandas'), 'go') (('001 to 1000 we', 'optimal alpha'), 'try') (('we', 'data'), 'let') (('mostly around optimal maximum depth', 'mostly 17'), 'show') (('how data', 'hyperplane'), 'Kernel') (('001Now we', 'SVM'), 'get') (('data', 'cell 357 benign 212 malignant nuclei'), 'contain') (('s', 'cell benign nuclei'), 'let') (('cell when nucleus', 'also significant increase'), 'show') (('diagnosis', 'data'), 'have') (('AdaBoostClassifier', 'best however almost 98 accuracy'), 'be') (('mean doesn', 'cell benign nuclei'), 'see') (('we', 'standard deviation'), 'get') (('Decision Tree', 'node'), 'tune') (('We', 'perimeter area smoothness compactness concavity concave points symmetry fractal dimension'), 'get') (('we', 'model'), 'get') (('mean radius', 'cell benign nuclei'), 'be') (('values', 'only really slow models'), 'lead') (('It', 'training points'), 'control') (('t', 'distance much weight'), 'have') (('we', 'default parameters'), 'start') (('learning_rate', 'somewhere 0'), 'see') (('that', 'prediction'), 'Classifier') (('Decision how many Trees', 'model'), 'parameter') (('Default default Ridge alpha parametersBy Classifiers', 'alpha'), 'tune') (('This', 'leaf'), 'send') (('we', 'given data'), 'tune') (('We', 'Extra Trees'), 'discuss') (('isn t', 'really average score'), '1looking') (('standard deviation', 'cell malignant nucleus'), 'be') (('it', 'more zero'), 'be') (('It', 'data'), 'choose') (('we', 'bit more depth'), 'try') (('gamma C Penalty 0 auto 1 degree 3 ParameterC', 'penalty error term'), 'be') (('isn only t', 'cell when malignant nucleus'), 'be') (('they', 'model'), 'let') (('you', 'parameter'), 'allow') (('Using', 'tested models'), 'give') (('I', 'how them'), 'show') (('then we', 'C back parameter'), 'seem') (('function', 'scores'), 'fit') (('we', 'Extra Tree Classifier parameters'), 'tune') (('The closer coefficients', '0'), 'be') (('that', 'original space'), 'be') (('Decision single Tree', 'learning_rate'), '10') (('s', 'C parameter'), 'AgainNow') (('we', 'that'), 'tune') (('optimal number', 'trees'), 'show') (('that', 'incorrect predictions'), 'be') (('best depths', 'plot'), 'be') (('Final now we', 'hard work'), 'ScoreSo') (('this', 'leaves'), 'improve') (('Extra Trees Classifier', 'then more accurate prediction'), 'classifier') (('Now s', 'radius'), 'let') (('who', 'about 0'), 'be') (('Alpha Forcing 0 coefficients', 'more less zero'), 'use') (('it', 'Algorithm'), 'make') (('This', 'more optimal results'), 'ensure') (('we', 'data'), 'have') (('values', 'them'), 'use') (('we', 'Random Forest parameters'), 'tune') (('I', 'definitely models'), 'so') (('we', 'more extreme range'), '001') (('I', 'models'), 'optimize') (('time we', '0'), 'use') (('bottom leaves', 'equal 0'), 'show') (('However this', 'default just parameters'), 'require') (('model', 'best model'), 'Algorithm') (('how much points', 'outcome'), 'parameter') (('Decision Trees', 'leaves'), 'Classifier') (('This', 'enough models'), 'be') (('you', 'them'), 'tune') (('so we', 'part'), 'discuss') (('that', 'condition'), 'have') (('We', 'cross validation score'), 'go') (('it', 'Extra Trees Classifier really model'), 'sound') (('they', 'what'), 'try') (('we', '0'), 'see') (('it', 'data'), 'Classifier') (('When we', '0 fair results'), 'use') (('trend', 'fractal dimension'), 'see') (('worst', 'cell nucleus'), 'be') (('you', 'model'), 'increase') (('we', 'n_neighbors weight'), 'tune') (('training data exactly setLet', '0'), 'gamma') (('kernels', 'different parameters'), 'tune') (('32 that', 'NaN values'), 'seem') (('correlations', 'earlier conclusions'), 'be') (('it', 'what'), 'AnalysisNow') (('Gradient Boosting Classifier', 'loss function'), 'Classifier') (('multiple times data', 'different weights'), 'Classifier') (('they', 'different neighbors'), 'be') "}