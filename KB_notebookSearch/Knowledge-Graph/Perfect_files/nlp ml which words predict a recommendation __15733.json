{"name": "nlp ml which words predict a recommendation ", "full_name": " h1 Machine Learning with text data h4 Can it be predicted whether or not a customer would recommend a purchased item of clothing based on solely their written review h2 1 Text Preprocessing h3 Noise Removal h3 Lexicon Normalisation h1 2 Getting a text matrix h1 3 Data Exploration h3 Merging datasets h3 Checking out the most salient words h3 Quick peak at the target variable h2 4 Machine Learning h3 Set up h3 Algorithms round 1 h3 Algorithms round 2 h2 5 Best Model Random Forest Classifier h3 Assessing feature importance h3 Precision Recall Curve h3 ROC AUC Curve h3 ROC AUC score h2 6 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "Machine LearningWhile a lot of the statistics that I created earlier on such as Polarity and Keyword are interesting I want to focus model predictions purely on the weighted text matrix. Taken the text reviews cleaned them and built them into a matrix Briefly explored the data before applying initial Machine Learning Algorithms Balanced the unvenly weighted target variables before re running the same Algorithms with improved findings Identified the best performing model Random Forest Classifier explored it s most important features and computed further Precision Recall metrics including ROC AUC. For example play player played plays and playing are the different variations of the word play. This step deals with removal of all types of noisy entities present in the text. The more sparse the data is the more challenging it will be to model but that s a discussion for another day Now that we have term counts for each document the TfidfTransformer can be applied to calculate the weights for each term in each document Great we have our weighted words Just a few more steps required below I m going to extract all of the feature names which are the n grams and put these into a DataFrame along with the corresponding weights per review. Quick peak at the target variableA binary variable Recommended will be the focus of upcoming Machine Learning prediction. Machine Learning with text data Can it be predicted whether or not a customer would recommend a purchased item of clothing based on solely their written review In this my first Machine Learning project working with text data and applying Natural Language Processing NLP techniques I will aim to predict with words alone whether or not a customer would recommend a purchased item of clothing. For Example let say there is a dataset of N text documents In any document D TF and IDF will be defined as Term Frequency TF TF for a term t is defined as the count of a term t in a document D Inverse Document Frequency IDF IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T. This information can be displayed visually as a Presicion Recall curve which has usefulness in allowing us to tailor n algorithm to more exact precision and recall requirements. Random Forest Classifier 3. Of particular interest to a retailer might be words such as Soft Comfortable Revealing the importance of texture and how the clothes actually feel Fit Size Both similar in nature and high ranking the product needs to fit well. Gaussian Naive Bayes 2. The relation between them can be plotted The red line represents a purely random classifier e. Upon first inspection it looks like there are some missing reviews fill in. Accuracy deals with ones and zeros meaning you either got the class label right or you didn t. This means that the data is not readily analysable without any pre processing. Various thresholds result in different true positive false positive rates. Lexicon NormalisationAnother type of textual noise is about the multiple representations exhibited by single word. Backing up the graph a very strong ROC AUC Score of over 99 has been achieved by this model. The specific model in question is known as Term Frequency Inverse Document Frequency TF IDF TF IDF is a weighted model commonly used for information retrieval problems. 5 won t get you far but 0. The most natural threshold is of course 0. This shouldn t come as too much of a surprise given that love is a highly emotive word that conveys a larger feeling of positivity as opposed to like or nice for example. It is able to get all the answers right but it outputs 0. It s great to see the Naive Bayes Classifier now up at 85 as this algorithm is typically well suited to crunching text data. The entire process of cleaning and standardization of text making it noise free and ready for analysis is known as text preprocessing. So anything that doesn t feature as part of this will now be removed from the DataFrame. Text PreprocessingText is the most unstructured form of all the available data therefore various types of noise are present in it. 7 thereafter Recall plummets eventually down to near zero. Clearly a threshold of 0. Other interesting questions within this kernel include Which emotive words are most popular Which aspect of the product is most important the fit colour texture price And morever what would such information mean for a retailer looking to best align their strategy to market demand I m not sure about you but that s enough to spike my interest There are 23 000 reviews and a binary target variable for overall product recommendation let s crack on. There are two methods of lexicon normalisation Stemming or Lemmatization. The below code will generate this for us The optimum Precision Recall threshold here looks within the region of 0. As depicted earlier there are around five times fewer 0 classifications compared to 1 classifications which can sometimes be problematic within Machine Learning whereby modelling generally tends to work better when there is an almost equal numbers of samples from each class in the target variable. I m only interested in three columns within this project which have been seelcted below. Now let s read in the data all 23 000 clothing reviews. Function defined now let s get training I m only going to focus on four algorithms in this kernel starting off with Algorithms round 1Four models down and we re up to 88 Accuracy with the Linear SVC. I don t intend to use this for any machine learning purposes more so out of interest to understand whether reviews lean towards positivity or negativity. Depending upon the usage text features can be constructed using a variety of techniques in this kernel I will be converting the data into statistical features. You will recall that I pulled a Polarity statistic during Step 1 let s see how this shapes up per target outcome Comparing the above two graphs it appears as though Polarity can recognise that reviews left by customers who recommended their product gave more positive reviews compared to those who did not recommend. I will opt for Lemmatization as this will return the root form of each word rather than just stripping suffixes which is stemming. If I wanted to be more or less restrictive on n gram selection I could adjust the min_df and max_df parameters within my CountVectorizer which controls for the minimum and maximum amount of documents each word should feature in. Linear SVC Import the hopeful solution to our problems Setting up new variables for ML Defining a new function with revised inputs for the new SMOTE variables 1. But many classifiers are able to quantify their uncertainty about the answer by outputting a probability value. Next this function will provide us with the Confusion Matrix how many correct vs incorrect classifications have actually taken place within the given model Last up this function will churn out for us a Classification Report which details other important metrics such as Precision Recall the F1 score which is just the harmonic mean of the former two and support which is the classification count. Let s suppose you have a quirky classifier. Then I am going to add in at the end some summary statistics to understand per review The highest weighted word The weight of this word The total weighting per review. My sense is this is due to class imbalance. Lexicon Normalisation Noise RemovalAny piece of text which is not relevant to the context of the data and the end output can be specified as the noise. As you decrease the threshold you get more true positives but also more false positives. That s the whole point of using AUC it considers all possible thresholds. Cheap This word features in the top list but quite far down the list How important is price when reviewing a product at this retailer Precision Recall CurveFor each person the Random Forest algorithm classifies it computes a probability based on a function and it classifies the review as Recommended when the score is bigger than the threshold or as Not Recommended when the score is smaller than the threshold. With this dataset we re way off that. 7 for negative examples and 0. One is a two part process of using the CountVectorizer class to count how many times each term shows up in each document followed by the TfidfTransformer class generating the weight matrix. Set upTo help with Machine Learning I will define a function that will return the most prized statistics in one go. Data handling and processing Data visualisation Statistics NLP Machine Learning xa0Reading in data Inspecting the variables Replacing blank variables with unknown ready for processing Importing SKLearn s list of stopwords and then appending with my own words Basic text cleaning function Make lowercase xa0Remove whitespaces Remove special characters Remove punctuation Remove numbers Remove Stopwords Convert to string Applying noise removal function to data xa0Defining a sentiment analyser function Applying function to reviews Instantiate the Word tokenizer Word lemmatizer Define a word lemmatizer function Apply the word lemmatizer function to data Getting a count of words from the documents Ngram_range is set to 1 2 meaning either single or two word combination will be extracted Getting the total n gram count Creating the bag of words representation Instantiating the TfidfTransformer Fitting and transforming n grams Getting a list of all n grams Putting weighted n grams into a DataFrame and computing some summary statistics Merging td idf weight matrix with original DataFrame Printing the first 10 reviews left Getting a view of the top 20 occurring words Getting a view of the top 20 weights Plotting overall recommendations and getting value counts Visualising polarity between recommending and non recommending customers then getting value counts Get a list of columns for deletion Drop all columns not part of the text matrix Create X y variables for Machine Learning Create a train test split of these variables Defining a function to fit and predict ML algorithms 1. 8 would be just perfect. Best Model Random Forest ClassifierGiven that the Random Forest now heads the pack in terms of accuracy let s proceed with some further exploration and perhaps a spot of optimisation too. First up I ll import every library that will be used in this project is imported at the start. ConclusionJob done In this kernel we have Started with an unstructured table of 23 000 clothing text reviews and corresponding Recommend vs Not Recommend classifications. In terms of how this kernel could be extended or improved we could Take a more rigorous approach to mining the text data such as categorising products controlling for spelling errors using more advanced modelling techniques such as topic modelling Make better use of the Polarity metric within Machine Learning Apply a more extensive list of algorithms including Deep Learning to the text data Apply GridSearchCV or RandomisedSearchCV to optimise the final model Create an ensemble of models for better predictionPerhaps I will return to some or all of these points at a later date. I ve enjoyed working with this dataset and have learned heaps around using text data to predict an outcome given this has been my first exposure to working with text the buzz in being able to do just this has been great It has also been an enjoyable experience using this data to predict genuinely meaningful and useful insights such as knowing what consumers are looking for in a clothing product and what matters most to them. After that we ll take a look at the sparsity of this representation which lets us know how many nonzero values there are in the dataset. In this Kernel I will proceed with method one below is step one I am happy with that number as a starting point less than 1000 was my initial aim. We can now tackle the next step which is to turn this document into a bag of words representation. In this notebook I will use an Oversampling technique from the handy SMOTE library. I ll begin with visualising the top predictive features from the Random Forest Classifier Assessing feature importanceTaking the win by a clear margin as the strongest predictor of product recommendation is the word love. Algorithms round 2We re in business a drastic improvement on Precision Recall across all algorithms even complemented with improved overall acccuracy on a couple as well. For now however thank you for reading this kernel Please do feel free to share with me your thoughts feedback and any suggestions for improvement I am always willing to learn new or more efficient techniques Cheers. For example language stopwords commonly used words of a language is am the of in etc URLs or links punctuations and industry specific words. This usually comprises two key steps 1. g a coin flip so the aim is for our classifier represented by the blue ROC curve to be as far away from it as possible. Getting a text matrixTo analyse a preprocessed data it needs to be converted into features. After initiating a model this function will return an mean accuracy score following 5 folds of cross validation this is to ensure that we are getting a smoothed out representation of both the training and test sets. The reviews are looking good for this retailer currently Let s find out a little more about this by inspecting the chosen target variable. Logistic Regression 4. To compute accuracy from probabilities you need a threshold to decide when zero turns into one. ROC AUC CurveAUC is a metric for binary classification. I ll come back to this again later. We ll need to keep a note of this as this could throw up a few issues when training algorithms i ll touch on this in a little while. This bodes well for using our text matrix to predict recommendations. However there seems to be an issue with Class 0 would not recommend across all trained models looking at the LinearSVC both Precision Recall are low contributing towards a mediocre 0. For this Random Forest Classifier model the graph therefore indicates very strong model performance hoo rah ROC AUC scoreLastly we will compute the size of this AUC space known as the ROC AUC score. Let s apply this technique and then re run all four models to re assess precision recall performance. Now let s see if the 20 highest weighted words throws up a similar list Largely similar albeit a slight difference in ranking. Following is a python function to strip out noise throughout the reviews Before moving onto lexicon normalisation I want to gain a sense of the sentiment per review. Following is the code using python s scikit learn package to convert a text into tf idf vectors Scikit learn provides two methods to get to our end result a TD IDF weight matrix. Though they mean different things contextually they all are similar. Therefore I will dive straight into Machine Learning. Not just positive words but also negative ones such as disappointed and unfortunately have been useful in the prediction. This step converts all the disparities of a word into their normalized form also known as lemma. With the weighted text matrix already created there is no pressing need for any further preprocessing engineering on these features. This space under the curve is known as the AUC and therefore a larger AUC space is indicative of a better model. This creates a separate column for each term that contains the count within each document. Achieved a final accuracy score above 90 on the model s training dataset and an ROC AUC score above 99. Data ExplorationNow that we have a td idf weight matrix this can be fed directly into a predictive model. Combined these metrics will provide rich insight into individual model performance and will guide better selection towards the best performing model and how best to optimise it. The other does both steps in a single TfidfVectorizer class. However now exceeding the 90 bracket is the Random Forest Classifier also boasting great Precision Recall values too. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. 9 for positive examples. Before we do this let s explore the current data a little more Merging datasets Checking out the most salient wordsLet s see which words are most frequent throughout the matrix It looks as though there is a definite positive lean throughout these most popular words such as love perfect and flattering. IDF TF IDF formula gives the relative importance of a term in a corpus list of documents given by the following formula below. Linear SVC Creating a plot for feature importance Set a different title for each axes Make the grid horizontal instead of vertical Compile arrays of columns words and feature importances Bung these into a dataframe rank highest to lowest then slice top 20 Plot the graph Getting prediction probabilities Defining a new function to plot the precision recall curve Compute the true positive and false positive rate Plotting the true positive and false positive rate Computing the ROC AUC score. Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features N different features to the low dimensional space 1 feature which is an ideal ask for any ML model. Let s see how the categories stack up There are nearly four times as many recommendations than there are non recommendations. To overcome this problem we can either DownSample the majority Class or UpSample the minority class. ", "id": "josh24990/nlp-ml-which-words-predict-a-recommendation", "size": "15733", "language": "python", "html_url": "https://www.kaggle.com/code/josh24990/nlp-ml-which-words-predict-a-recommendation", "git_url": "https://www.kaggle.com/code/josh24990/nlp-ml-which-words-predict-a-recommendation", "script": "textblob TfidfTransformer classification_report train_test_split cross_val_predict model confusion_matrix SMOTE stats precision_recall_curve sklearn.svm CountVectorizer numpy cross_val_score seaborn TfidfVectorizer recall_score plot_roc_curve TextBlob imblearn.over_sampling randint f1_score LinearSVC sklearn.feature_extraction.text randint as sp_randint GaussianNB sklearn.naive_bayes roc_auc_score lemmatize_text scipy plot_precision_and_recall sklearn.linear_model matplotlib.pyplot precision_score statsmodels.api sklearn.model_selection pandas roc_curve RandomForestClassifier LogisticRegression importance_plotting stopwords time remove_noise nltk.corpus model_sm scipy.stats sentiment_analyser sklearn.metrics sklearn.ensemble sklearn.feature_extraction text ", "entities": "(('ROC AUC', 'binary classification'), 'be') (('when score', 'threshold'), 'feature') (('I', 'product recommendation'), 'begin') (('which', 'just harmonic former two'), 'provide') (('Precision Recall', 'low mediocre 0'), 'seem') (('algorithm', 'text typically well data'), 's') (('text weighted matrix', 'features'), 'create') (('many classifiers', 'probability value'), 'be') (('s', '23 binary target product 000 overall recommendation'), 'include') (('summary statistics', 'total review'), 'go') (('down we', 'Linear SVC'), 'let') (('end output', 'noise'), 'specify') (('Therefore I', 'Machine straight Learning'), 'dive') (('I', 'always new more techniques'), 'thank') (('Term Frequency Inverse Document Frequency TF IDF TF IDF', 'information retrieval weighted commonly problems'), 'know') (('IDF TF IDF formula', 'following formula'), 'give') (('which', 'review'), 'be') (('which', 'rather just suffixes'), 'opt') (('when zero', 'one'), 'compute') (('step', 'also lemma'), 'convert') (('other', 'TfidfVectorizer single class'), 'step') (('whereby modelling', 'target variable'), 'be') (('missing reviews', 'first inspection'), 'look') (('most natural threshold', 'course'), 'be') (('Create y variables', 'ML algorithms'), 'handling') (('Random Forest it', 'ROC AUC'), 'clean') (('currently s', 'target chosen variable'), 'look') (('1 which', 'ML ideal model'), 'be') (('it', 'possible thresholds'), 's') (('that', 'document'), 'create') (('we', 'DownSample'), 'can') (('how many times term', 'weight matrix'), 'be') (('scoreLastly we', 'ROC AUC score'), 'indicate') (('Quick peak', 'binary variable Machine Learning variableA prediction'), 'be') (('us', 'tailor more exact precision requirements'), 'display') (('what', 'most them'), 'enjoy') (('Scikit', 'TD IDF weight matrix'), 'be') (('Now s', '23 clothing 000 reviews'), 'let') (('it', '0'), 'be') (('it', 'text preprocessing'), 'know') (('you', 'more true positives'), 'decrease') (('one below one I', 'starting point'), 'proceed') (('Both', 'high product'), 'be') (('Precision Recall optimum threshold', '0'), 'generate') (('word', 'documents'), 'adjust') (('the', 'etc URLs punctuations'), 'use') (('I', 'review'), 'be') (('i', 'little while'), 'need') (('Various thresholds', 'different true positive false positive rates'), 'result') (('which', 'project'), 'm') (('drastic improvement', 'couple'), 're') (('I', 'SMOTE handy library'), 'use') (('s', 'precision recall assess performance'), 'let') (('I', 'text purely weighted matrix'), 'be') (('coin so aim', 'as far away it'), 'flip') (('Compile instead vertical arrays', 'ROC AUC score'), 'Set') (('20 highest weighted words', 'ranking'), 'let') (('that', 'one'), 'define') (('that', 'now DataFrame'), 'remove') (('contextually they', 'different things'), 'be') (('space', 'AUC therefore larger better model'), 'know') (('class label', 'ones'), 'deal') (('player', 'different word'), 'be') (('it', 'features'), 'get') (('Random Forest Classifier', 'Precision Recall also great values'), 'exceed') (('us', 'nonzero how many dataset'), 'take') (('data', 'pre readily processing'), 'mean') (('red line', 'classifier purely random e.'), 'plot') (('ROC AUC very strong Score', 'model'), 'achieve') (('I', 'statistical features'), 'construct') (('Linear SVC', 'SMOTE new variables'), 'Import') (('It', 'such love'), 'let') (('s', 'perhaps optimisation'), 'let') (('this', 'class imbalance'), 'be') (('who', 'those'), 'recall') (('therefore various types', 'it'), 'be') (('we', 'training sets'), 'be') (('alone customer', 'clothing'), 'Learning') (('we', 'classifications'), 'do') (('I', 'later date'), 'take') (('highly emotive that', 'example'), 'come') (('that', 'start'), 'import') (('this', 'directly predictive model'), 'ExplorationNow') (('you', 'quirky classifier'), 'let') (('Lexicon NormalisationAnother type', 'single word'), 'be') (('reviews', 'positivity'), 'don') (('Combined', 'how best it'), 'provide') (('which', 'words representation'), 'tackle') (('D TF', 'term'), 'let') (('It', 'exact ordering'), 'aim') (('step', 'present text'), 'deal') "}