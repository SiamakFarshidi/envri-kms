{"name": "everything in machine learning 1 3 ", "full_name": " h2 Distance Measurements Between Data Points h3 This parameter specifies how the distance between data points in the classification clustering input is measured The options are h1 Supervised Learning h1 Generating Synthetics Datasets h1 Classification h1 Regression h1 Linear Regression h1 Linear Regression Example Plot h1 Ridge Regression h2 Ridge regression with feature normalization h2 Ridge regression with regularization parameter alpha h1 Lasso regression h2 Lasso regression with regularization parameter alpha h2 Polynomial regression h2 Logistic regression h4 Logistic regression for binary classification on fruits dataset using height width features positive class apple negative class others h2 Logistic regression on simple synthetic dataset h2 Logistic regression regularization C parameter h2 Application to real dataset h1 Support Vector Machines h2 Linear Support Vector Machine h2 Linear Support Vector Machine C parameter h2 Application to real dataset h2 Multi class classification with linear models h2 LinearSVC with M classes generates M one vs rest classifiers h2 Multi class results on the fruit dataset h1 Kernelized Support Vector Machines h1 Classification h2 Support Vector Machine with RBF kernel gamma parameter h2 Support Vector Machine with RBF kernel using both C and gamma parameter h2 Application of SVMs to a real dataset unnormalized data h2 Application of SVMs to a real dataset normalized data with feature preprocessing using minmax scaling h1 Cross validation h1 Example based on k NN classifier with fruit dataset 2 features h3 A note on performing cross validation for more advanced scenarios h2 Validation curve example h2 Decision Trees h2 Setting max decision tree depth to help avoid overfitting h2 Visualizing decision trees h2 Feature importance h4 Decision Trees on a real world dataset Breast Cancer Dataset h3 Random Forests h3 Random forest Fruit dataset h4 Random Forests on a real world dataset h2 Gradient Boosted Decision Trees h4 Gradient boosted decision trees on the fruit dataset h4 Gradient boosted decision trees on a real world dataset h2 Neural Networks h4 Activation funcitons h3 Neural Networks Classification h4 Synthetic dataset 1 single hidden layer h4 Synthetic dataset 1 two hidden layers h4 Regularization parameter alpha h4 The effect of different choices of activation function h3 Neural networks Regression h4 Application to real world dataset for classification h2 Naive Bayes Classifiers h3 Application to real world dataset h1 Evaluation for Classification h2 Preamble h4 Binarization h2 Dummy Classifiers h1 Confusion matrices h2 Binary two class confusion matrix h1 Evaluation metrics for binary classification h1 Decision functions h1 Precision recall curves h1 ROC curves Area Under Curve AUC h1 Evaluation measures for multi class classification h2 Multi class confusion matrix h1 Multi class classification report h2 Micro vs macro averaged metrics h1 Regression evaluation metrics h1 Model selection using evaluation metrics h2 Cross validation example h1 Grid search example h2 Evaluation metrics supported for model selection h1 Two feature classification example using the digits dataset h2 Optimizing a classifier using different evaluation metrics h2 Precision recall curve for the default SVC classifier with balanced class weights h2 Getting Started with Unsupervised Learning h2 Dimentionality Reduction and Manifold Learning h3 Principal Componant Analysis PCA h4 Using PCA to find the first two principal componants in the Breast Cancer dataset h3 Before applying PCA each feature should be centered zero mean and with unit variance h4 plotting the magnitude of each feature value for the first two principal componants h4 PCA on the fruit dataset for comparison h3 Manifold Learning h4 Multidimensional scaling MDS on the fruits dataset h4 each feature should be centered zero mean and with unit variance h4 Multidimensional Scaling on Breast Cancer dataset h3 t SNE on the fruit dataset h4 t SNE on the breast cancer dataset h2 Clustering h3 K means h3 Agglomerative Clustering h4 Create a dendrogram using scipy h3 DBSCAN Clustering h3 This is just the first draft version Will include some more of my own code with lots of updates in parts 2 and 3 ", "stargazers_count": 0, "forks_count": 0, "description": "Here it doesn t work as well at finding structure in the small fruits dataset compared to other methods like MDS. k NN 4 which distance function https www. Chebychev Use Chebychev distance to cluster together genes that do not show dramatic expression differences in any samples genes with a large expression difference in at least one sample are assigned to different clusters. linspace 10 15 for w b color in zip clf. max x2 y2 numpy. htmlor the Pipeline section in the recommended textbook Introduction to Machine Learning with Python by Andreas C. score X_test y_test title title nTrain score. create a two feature input vector matching the example plot above each feature should be centered zero mean and with unit variance plot_labelled_scatter X_mds y_cancer malignant benign First MDS dimension Second MDS dimension Breast Cancer Dataset MDS n_components 2 plot_labelled_scatter X_tsne y_cancer malignant benign First t SNE feature Second t SNE feature Breast cancer dataset t SNE. org stable modules generated sklearn. Distance Measurements Between Data PointsThis parameter specifies how the distance between data points in the classification clustering input is measured. Confusion matrices Binary two class confusion matrix Evaluation metrics for binary classification Decision functions Precision recall curves ROC curves Area Under Curve AUC Evaluation measures for multi class classification Multi class confusion matrix Multi class classification report Micro vs. M\u00fcller and Sarah Guido O Reilly Media. increasing expression with time but whose expression levels may be very different. 5 cm second example a larger elongated fruit with mass 100g width 6. 7 x_0_range np. set_title title if target_names is not None legend_handles for i in range 0 len target_names patch mpatches. plot x_0_range x_0_range w 0 b w 1 c color alpha. set_ylim y_min y_plot_adjust y_max y_plot_adjust if X_test is not None subplot. Instead scaling normalizing must be computed and applied for each cross validation fold separately. Spearman Use Spearman Correlation to cluster together genes whose expression profiles have similar shapes or show similar general trends e. score X y test_score clf. Multi class results on the fruit dataset Kernelized Support Vector Machines Classification Support Vector Machine with RBF kernel gamma parameter Support Vector Machine with RBF kernel using both C and gamma parameter Application of SVMs to a real dataset unnormalized data Application of SVMs to a real dataset normalized data with feature preprocessing using minmax scaling Cross validation Example based on k NN classifier with fruit dataset 2 features A note on performing cross validation for more advanced scenarios. To do this the easiest way in scikit learn is to use pipelines. org stable auto_examples model_selection plot_validation_curve. figure figsize 6 6 colors r g b y cmap_fruits ListedColormap FF0000 00FF00 0000FF FFFF00 plt. The proper way to do cross validation when you need to scale the data is not to scale the entire dataset with a single transform since this will indirectly leak information into the training data about the whole dataset including the test data see the lecture on data leakage later in the course. The options are 1. 9 of the points corresponds to the index of the points in the X array above. macro averaged metrics Regression evaluation metrics Model selection using evaluation metrics Cross validation example Grid search example Evaluation metrics supported for model selection Two feature classification example using the digits dataset Optimizing a classifier using different evaluation metrics Precision recall curve for the default SVC classifier with balanced class weights Getting Started with Unsupervised Learning Dimentionality Reduction and Manifold Learning Principal Componant Analysis PCA Using PCA to find the first two principal componants in the Breast Cancer dataset Before applying PCA each feature should be centered zero mean and with unit variance plotting the magnitude of each feature value for the first two principal componants PCA on the fruit dataset for comparison Manifold Learning Multidimensional scaling MDS on the fruits dataset each feature should be centered zero mean and with unit variance Multidimensional Scaling on Breast Cancer dataset t_SNE on the fruit datasetSome dimensionality reduction methods may be less successful on some datasets. ylabel width plt. as_matrix Creating a dataset with imbalanced binary classes Negative class 0 not digit 1 Positive class 1 digit 1 Negative class 0 is the most frequent class Accuracy of Support Vector Machine classifier Negative class 0 is most frequent Therefore the dummy most_frequent classifier always predicts class 0 Negative class 0 is most frequent produces random predictions w same class proportion as training set Accuracy TP TN TP TN FP FN Precision TP TP FP Recall TP TP FN Also known as sensitivity or True Positive Rate F1 2 Precision Recall Precision Recall Combined report with all above metrics show the decision_function scores for first 20 instances show the probability of positive class for first 20 instances Plot outputs again making this a binary problem with digit 1 as positive class and not 1 as negative class accuracy is the default scoring metric use AUC as scoring metric use recall as scoring metric default metric to optimize over grid parameters accuracy alternative metric to optimize over grid parameters AUC Create a two feature input vector matching the example plot above We jitter the points add a small amount of random noise in case there are areas in feature space where many instances have the same features. append patch subplot. com watch v _EEcjn0UirwMinkowski Distance https en. Note that in general it s important to scale the individual features before applying k means clustering. arange x_min k x_max k h numpy. legend target_names_fruits plt. Euclidean Use the standard Euclidean as the crow flies distance. xlabel height plt. Pearson Correlation Use the Pearson Correlation coefficient to cluster together genes or samples with similar behavior genes or samples with opposite behavior are assigned to different clusters. legend loc 0 handles legend_handles if pandas. contourf x2 y2 P cmap cmap_light alpha 0. amax y 1 color_list_light FFFFAA EFEFEF AAFFAA AAAAFF color_list_bold EEEE00 000000 00CC00 0000CC cmap_light ListedColormap color_list_light 0 numClasses cmap_bold ListedColormap color_list_bold 0 numClasses h 0. shape if plot_decision_regions subplot. Euclidean Squared Use the Euclidean squared distance in cases where you would use regular Euclidean distance in Jarvis Patrick or K Means clustering. show The default SVC kernel is radial basis function RBF Compare decision boundries with polynomial kernel degree 3 This code based on scikit learn validation_plot example See http scikit learn. arange y_min k y_max k h P clf. Pearson Squared Use the squared Pearson Correlation coefficient to cluster together genes with similar or opposite behaviors i. org wiki Minkowski_distance Supervised Learning Generating Synthetics Datasets Classification Regression Linear Regression Linear Regression Example Plot Ridge Regression Ridge regression with feature normalization Ridge regression with regularization parameter alpha Lasso regression Lasso regression with regularization parameter alpha Polynomial regression Logistic regression Logistic regression for binary classification on fruits dataset using height width features positive class apple negative class others Logistic regression on simple synthetic dataset Logistic regression regularization C parameter Application to real dataset Support Vector Machines Linear Support Vector Machine Linear Support Vector Machine C parameter Application to real dataset Multi class classification with linear models LinearSVC with M classes generates M one vs rest classifiers. format train_score test_score subplot. genes that are highly correlated and those that are highly anti correlated are clustered together. intercept_ r g b y plt. scatter X_fruits_2d height X_fruits_2d width c y_fruits_2d cmap cmap_fruits edgecolor black alpha. Manhattan Use the Manhattan city block distance. Agglomerative Clustering Create a dendrogram using scipy This dendrogram plot is based on the dataset created in the previous step with make_blobs but only 10 samples have been selectedAnd here s the dendrogram corresponding to agglomerative clustering of the 10 points above using Ward s method. scatter X 0 X 1 c y cmap cmap_bold s plot_symbol_size edgecolor black subplot. ravel P P. While these are beyond the scope of this course further information is available in the scikit learn documentation here http scikit learn. scatter X_test 0 X_test 1 c y_test cmap cmap_bold s plot_symbol_size marker edgecolor black train_score clf. Validation curve example Decision Trees Setting max decision tree depth to help avoid overfitting Visualizing decision trees Feature importance Decision Trees on a real world dataset Breast Cancer Dataset Random Forests Random forest Fruit dataset Random Forests on a real world dataset Gradient Boosted Decision Trees Gradient boosted decision trees on the fruit dataset Gradient boosted decision trees on a real world dataset Neural Networks Activation funcitons Neural Networks Classification Synthetic dataset 1 single hidden layer Synthetic dataset 1 two hidden layers Regularization parameter alpha The effect of different choices of activation function Neural networks Regression Application to real world dataset for classification Naive Bayes Classifiers Application to real world dataset Evaluation for Classification Preamble Binarization Dummy ClassifiersDummyClassifier is a classifier that makes predictions using simple rules which can be useful as a baseline for comparison against actual classifiers especially with imbalanced classes. Patch color color_list_bold i label target_names i legend_handles. max y_min X 1. edu ml datasets Communities and Crime Unnormalized remove features with poor coverage or lower relevance and keep ViolentCrimesPerPop target column plot k NN regression on sample dataset for different values of K make into a binary problem apples vs everything else plt. DBSCAN Clustering This is just the first draft version. html def plot_class_regions_for_classifier_subplot_forndarray clf X y X_test y_test title subplot target_names None plot_decision_regions True numClasses numpy. DataFrame in str type X_fruits elif numpy. 5 cm we choose 5 nearest neighbors we must apply the scaling to the test set that we computed for the training set synthetic dataset for simple regression synthetic dataset for more complex regression synthetic dataset for classification binary more difficult synthetic dataset for classification binary with classes that are not linearly separable Breast cancer dataset for classification Communities and Crime dataset Communities and Crime dataset for regression https archive. when feature values have very different ranges we ve seen the need to scale or normalize the training and test sets before use with a classifier. Will include some more of my own code with lots of updates in parts 2 and 3 create a mapping from fruit label value to fruit name to make results easier to interpret plotting a scatter matrix plotting a 3D scatter plot For this example we use the mass width and height features of each fruit instance default is 75 25 train test split first example a small fruit with mass 20g width 4. set_xlim x_min x_plot_adjust x_max x_plot_adjust subplot. t SNE on the breast cancer dataset Clustering K meansExample showing k means used to find n clusters in the fruits dataset. 5 x_plot_adjust 0. For example point 0 5. 1 y_plot_adjust 0. 76 are the closest two points and are clustered first. min x_max X 0. min y_max X 1. ndarray in str type X_fruits. 1 plot_symbol_size 50 x_min X 0. ", "id": "akshayvaru103/everything-in-machine-learning-1-3", "size": "7655", "language": "python", "html_url": "https://www.kaggle.com/code/akshayvaru103/everything-in-machine-learning-1-3", "git_url": "https://www.kaggle.com/code/akshayvaru103/everything-in-machine-learning-1-3", "script": "plot_decision_tree cm Ridge make_classification MLPClassifier recall_score DBSCAN KNeighborsRegressor GaussianNB sklearn.neighbors sklearn.linear_model matplotlib.pyplot precision_score SCORERS AgglomerativeClustering DummyClassifier TSNE Axes3D Lasso libraryplot LinearRegression load_breast_cancer precision_recall_curve sklearn.svm cross_val_score accuracy_score sklearn.cluster dendrogram load_digits ListedColormap sklearn.manifold plot_fruit_knn sklearn.naive_bayes sklearn.dummy plot_labelled_scatter_modified copyfile sklearn MinMaxScaler pandas plot_two_class_knn make_regression LogisticRegression matplotlib GridSearchCV sklearn.ensemble ward confusion_matrix numpy load_crime_dataset shutil LinearSVC sklearn.tree make_friedman1 DecisionTreeClassifier mpl_toolkits.mplot3d roc_curve RandomForestClassifier MLPRegressor MDS PolynomialFeatures DummyRegressor sklearn.metrics StandardScaler classification_report train_test_split sklearn.metrics.scorer plot_class_regions_for_classifier_subplot_forndarray seaborn plot_class_regions_for_classifier sklearn.datasets f1_score SVC validation_curve datasets GradientBoostingClassifier make_blobs sklearn.neural_network plot_feature_importances roc_auc_score auc plot_class_regions_for_classifier_subplot PCA matplotlib.colors sklearn.model_selection KMeans r2_score scipy.cluster.hierarchy plot_labelled_scatter mean_squared_error KNeighborsClassifier load_iris sklearn.decomposition sklearn.preprocessing ", "entities": "(('DataFrame', 'str type'), 'numpy') (('which', 'especially imbalanced classes'), 'example') (('http scikit', '3 scikit'), 'show') (('0 target_names', 'range'), 'title') (('that', 'Crime dataset Crime regression https archive'), 'choose') (('Pearson Squared', 'behaviors similar i.'), 'Use') (('expression levels', 'time'), 'be') (('we', 'classifier'), 'have') (('i', 'legend_handles'), 'color_list_bold') (('figure', 'r g 6 6 b cmap_fruits'), 'figsize') (('n clusters', 'fruits'), 'SNE') (('ROC', 'class classification Multi class confusion matrix Multi class classification multi report'), 'curve') (('target column k NN regression', 'everything'), 'dataset') (('how distance', 'classification clustering input'), 'specify') (('Clustering', 'This'), 'be') (('we', 'train g 75 25 first small mass 20 width'), 'include') (('Support Vector Machines Linear Support Vector Machine Linear Support Vector Machine C parameter Application', 'rest one classifiers'), 'supervise') (('where many instances', 'same features'), 'create') (('Instead scaling normalizing', 'cross validation'), 'compute') (('Manhattan', 'Manhattan city block distance'), 'Use') (('where you', 'Jarvis Patrick'), 'Use') (('general it', 'k means'), 'note') (('Second t SNE feature Breast cancer', 't SNE'), 'create') (('expression profiles', 'trends similar general e.'), 'Spearman') (('cmap_light cmap_bold 000000 0 ListedColormap', 'AAFFAA AAAAFF'), 'EFEFEF') (('Euclidean', 'flies distance'), 'use') (('unit zero mean variance', 'less datasets'), 'average') (('9', 'X array'), 'correspond') (('dendrogram here corresponding', '10 points method'), 'create') (('expression dramatic differences', 'different clusters'), 'assign') (('Pearson Correlation', 'different clusters'), 'Use') (('this', 'later course'), 'be') (('width c y_fruits_2d cmap scatter X_fruits_2d height X_fruits_2d cmap_fruits', 'black alpha'), 'edgecolor') (('Here it', 'MDS'), 'doesn') (('http here scikit', 'scikit'), 'be') "}