{"name": "breast cancer prediction ", "full_name": " h1 About this Kernel h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations ", "stargazers_count": 0, "forks_count": 0, "description": "png attachment efa5748c 1e3b 44c4 a93d f37f0421fcc1. There s only ID column of int type. All feature values are recoded with four significant digits. Observations In order to conduct our analysis easily we have converted the target column as Malignant 1 Benignant 0 Observations The following columns are the one s that show the greatest correlation with our diagnosis column. We can either use only the columns which have greatest correlation or we can continue to use all the columns. Wohhoo We have finally built a model with an accuracy of 99. This would save our time from mapping the variables. There are two things that can be done. 058 and LogisticRegression has the highest of 98. 50 DecisionTreeClassifier 90. 50 GradientBoostingClassifier 97. The ID column would not help us contributing to predict about the cancer. 0 concavity severity of concave portions of the contour concave points number of concave portions of the contour symmetry fractal dimension coastline approximation 1 The mean standard error and worst or largest mean of the three largest values of these features were computed for each image resulting in 30 features. For instance field 3 is Mean Radius field13 is Radius SE field 23 is Worst Radius. Observations After dropping the two columns we are now left with 31 columns. Most of the columns seem to have a numeric entry. Observations So there are 3 columns that have outliers lets plot them and check them out Observations We can clearly see that all our models perform with more than 90 accuracy where DecisionTreeClassifier has the lowest of 90. We might as well drop it. We will probably drop it anyway. 67 AdaBoostClassifier 96 XGBClassifier 97. Let us see if there are any outliers present in the dataset About The Local Outlier Factor The Local Outlier Factor LOF algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. We might probably just drop it. Observations Only the diagnosis column which we have to predict is of object datatype. png Summary We used a total of 8 models in order to achieve our final result. The columns 3 32 contain 30 real value features that have been computed from digitized images of the cell nuclei which can be used to build a model to predict whether a tumor is benign or malignant. There are a total of 31 columns which are of float datatype. We have to select which of the attributes we want to use in building our model Observations We would need to eliminate the outliers so that it does not affects our model s accuracy. Let s try to see where exactly does this happen Observations From the above plot we may conclude that highest accuracy is achieved at 20th iteration. Let us see how well do they correlate with the diagnosis column. The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis M malignant B benign respectively. Let us see if we can further improve the accuracy of our model by adding a few changes to it From the above figure we can see that our model touches somewhere around 99 between 20 25. png attachment a530fc48 7806 483c 9049 9b86635fd7bd. png About the Dataset About this Kernel The Breast Cancer datasets is available UCI machine learning repository maintained by the University of California Irvine. The dataset contains 569 samples of malignant and benign tumor cells. I will be using all these columns to predict our result You can eliminate a few and see if the accuracy improves Observations Looks wonderful isn t it There are only a handful of columns that show negative correlation with the diagnosis column Around half of our columns are more than 50 positively correlated to diagnosis column. 1 Malignant Cancerous Present M 0 Benign Not Cancerous Absent B Ten real valued features are computed for each cell nucleus radius mean of distances from center to points on the perimeter texture standard deviation of gray scale values perimeter area smoothness local variation in radius lengths compactness perimeter 2 area 1. 67 Thanks a lot for checking this out till the end linear algebra data processing data visualization data visualization to ignore the warnings for model building Loading the data pair plot split the data to X and y before Local Outlier Factorization 1 inlier 1 outlier Dont fit the scaler while standardizate X_test. It considers as outliers the samples that have a substantially lower density than their neighbors. LogisticRegression 99. 12 KNeighborsClassifier 95. Missing attribute values none Class distribution 357 benign 212 malignant Observations The last column named Unaname 32 seems like an erronous coloumn in our dataset. 05 RandomForestClassifier 96. ", "id": "aditimulye/breast-cancer-prediction", "size": "5668", "language": "python", "html_url": "https://www.kaggle.com/code/aditimulye/breast-cancer-prediction", "git_url": "https://www.kaggle.com/code/aditimulye/breast-cancer-prediction", "script": "train_test_split confusion_matrix xgboost sklearn.svm accuracy_score numpy LocalOutlierFactor seaborn NeighborhoodComponentsAnalysis SVC sklearn.neighbors sklearn.tree GradientBoostingClassifier sklearn.linear_model matplotlib.pyplot DecisionTreeClassifier sklearn.model_selection pandas RandomForestClassifier LogisticRegression KNeighborsClassifier AdaBoostClassifier sklearn.metrics sklearn.ensemble StandardScaler sklearn.preprocessing ", "entities": "(('none Class 357 benign 212 malignant last column', 'dataset'), 'miss') (('how well they', 'diagnosis column'), 'let') (('we', 'object datatype'), 'be') (('highest accuracy', '20th iteration'), 'let') (('feature values', 'four significant digits'), 'recode') (('1 mean standard error', '30 features'), 'severity') (('LogisticRegression', '98'), '058') (('we', 'now 31 columns'), 'leave') (('it', 'accuracy'), 'have') (('we', 'columns'), 'use') (('one that', 'diagnosis column'), 'observation') (('Local Outlier Factor LOF detection unsupervised anomaly which', 'neighbors'), 'let') (('that', 'neighbors'), 'consider') (('Cancerous Present 1 Malignant M 0 Cancerous Absent B Ten real valued features', 'radius lengths'), 'benign') (('Mean Radius 3 field13', 'instance field'), 'be') (('us', 'cancer'), 'help') (('model', 'somewhere around 99 between 20 25'), 'let') (('Most', 'numeric entry'), 'seem') (('that', 'more than positively column'), 'use') (('Summary png We', 'final result'), 'use') (('Breast Cancer datasets', 'California Irvine'), 'png') (('tumor', 'model'), 'contain') (('dataset', 'tumor malignant cells'), 'contain') (('where DecisionTreeClassifier', '90'), 'observation') (('1 outlier', 'scaler'), 'thank') (('Wohhoo We', '99'), 'build') (('which', 'float datatype'), 'be') (('This', 'variables'), 'save') "}