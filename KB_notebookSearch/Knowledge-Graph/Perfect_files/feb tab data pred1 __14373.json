{"name": "feb tab data pred1 ", "full_name": " h2 1 Import relevant libraries h2 2 TPU Check h2 3 Read in The Data Files h2 4 Checking for missing Values h2 5 Let s see the summary info of the data h2 6 Check Cardinality of Categorical Variables h2 7 Pinpointing rare categories in categorical variables h2 8 Visualize the relationship between Categorical variables and Target h2 9 One Hot Encoding Categorical Variables h2 10 Drop categorical columns with 10 or less value contribution per variable h2 11 Visualizing The Relationship between Numerical Variables and Target h2 12 Correlation Strength h2 13 Checking for Multi collinearity h2 14 Applying Variance Inflation Factor h2 15 Dropping Target Variable h2 16 Applying Box Cox Transformation to Original Dataset h2 17 Visualizing Variable Normality using a Q Q Plot h2 18 Exploring Outliers h2 19 Applying Standardization to Train Copy dataset h4 train stdize data has less than 3 outliers to dataset ratio Let s try mean normalization h2 20 Applying Mean Normalization to Original Dataset h4 The decision is between Box Cox and Mean Norm They both have a perfect range across all features between 0 and 1 But Mean Norm balances the data better with zero deviation from the mean of 0 and slightly lower deviation from the std of 1 than Box Cox Also Box cox is just slightly better on outliers as compared to Mean norm But since I d treat outliers soon I so far prefer the centralised distribution of Mean norm h4 Standardization is not an option because even though it perfectly balances the distribution of all variables at a MEAN of 0 and STD of 1 it causes a chaotic range of distribution and an outlier ratio worse than Box cox and similar to Mean norm Let s try a couple more h2 21 Applying Robust Scaling to Train copy h3 X scaled X X median X quantile 0 75 X quantile 0 25 h2 22 Applying Scaling to Vector Unit Length L1 Norm h2 23 Applying Scaling to Vector Unit Length L2 Norm h2 24 Summarizing Feature Scaling Normalization Choices h2 25 Winsorization to Address Outliers h2 26 Kurtosis AKA The 4th Statistical Moment h2 27 Performing Data Transformations on Test set h4 We also need to transform the test set on values learnt on the training set The functions below perform all the transformations we ve been doing for specific activation functions in addition to reducing the train test and target datasets and returning these fit for machine learning h3 Note that each transformation function must only contain one key word amongst other words Key words are h3 Instantiate an instance of the FinalPrep class passing if we want One Hot Encoding or Not and the type of transformation for the final test and train data sets h2 28 Saving the pre processed train target and test sets ", "stargazers_count": 0, "forks_count": 0, "description": "We d apply the inter quartile range IQR proximity rule. com Lawrence Krukrubo Understanding_Multiple_Linear_Regression blob master coefficients_of_multiple_linear_regression. The functions below perform all the transformations we ve been doing for specific activation functions in addition to reducing the train test and target datasets and returning these fit for machine learning. MEAN for mean norm activation 3. BOX for boxcox activation 2. join dirname filename You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session Just incase we need it for median range or Robust Scaling for vector unit length norm for the Q Q plots to display the total number columns present in the dataset detect TPUs TPU detection detect GPUs for GPU or multi GPU machines For the 1st and 2nd cat vars For the 3rd and 4th cat vars For the 5th and 6th cat vars For the 7th and 8th cat vars For the 9th and 10th cat vars Average target for cat0 Average target for cat1 Average target for cat2 Average target for cat3 Average target for cat4 Average target for cat5 The original column names of Categorical columns drop former cat column names since O H E Double check for NAN values after join Let s delete cat_vars from memory plot both together to compare For cont0 and cont1 For cont2 and cont3 For cont4 and cont5 For cont6 and cont7 For cont8 and cont9 For cont10 and cont11 For cont12 and cont13 Let s remove the correlation of same to same columns 1. Winsorization to Address Outliers. we d drop Cat O H E columns that don t have significance up to a certain threshold we choose. Here we use the Power Transform Function to Normalize the data First let s visualize the current shapesBox_cox can t work with negative values so let s confirm if we have negative values in the training set So we shall use standard scaler with the box cox transformation Let s see the normalized features Box Cox has done a good transformation with the shape of the data but looking at the numeric variables they are not centred around zero. 0 to capture them all We can see that by removing outliers certain categorical variables basically have no more distribution. We can identify these by the much lighter colors in the matrix. Visualize the relationship between Categorical variables and Target Let s investigate the relationship between cat0 and Target As much as possible we want to have categorical variables in a column that seem to have distinct relationship with target not too similar hence they lose their predictive power. This procedure involves subtracting the MEAN from each observation and then dividing the result by the difference between the minimum and maximum values Let s see how far the numeric values are from the MEAN and STD The decision is between Box Cox and Mean Norm. Kurtosis AKA The 4th Statistical Moment Now just before we fit the Winsorizer on the dataset let s compute the Kurtosis score of the data which is a score that computes how much outliers are in the dataset. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. Robust scaling produces more robust estimates for the center and value range of the variable and is recommended if the data contains outliers just like our present data X_scaled X X_median X. Let s create a Winsorizer object to Cap outliers based on the same Inter Quantile Range we specified earlier Next we d fit_transform the training data using the winsorizer and then transform the test data soon with the learned parameters from the training data using the transform function of the Winsorizer But first let s sample a few variables and see the outliers before and after applying winsorization. Count the cardinality values per categorical column Let s see one of the variables Let s make a plot with the cardinality of each categorical variable Let s define a method that plots each Cat feature and a threshold of percentage importance per Cat feature value to each Cat feature. 5 the IQR Well it turns out that the percent of outliers to total data size is 3 for train boxcox. We can see the general bell curve shape of the distribution from the histogram. Typically we consider a label to be rare when it appears in less than 5 or 1 of the population 8. I d create a function that takes a dataframe and the factor default is 1. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. Since both dataframes have different columns using pd. While normal outliers are outliers 1. Finally reduce the target size and save it. When scaling to vector unit length we transform the components of a feature vector so that the transformed vector has a length of 1 or in other words a norm of 1. Winsorizing is different from trimming because the extreme values are not removed but are instead replaced by other values. Let s look at variables cont8 and cont2 of mean_norm_df. Summarizing Feature Scaling Normalization Choices Looking at the above charts for choice of Feature Norm Scaling let s create a function that basically summarizes the key similarities or differences of each choice in a table. 0 Let s remove the target column from numcols Select only Numeric cols first scale values to range 1 2 Then apply boxcox transform After boxcox scale back to range 0 1 convert the array back to a dataframe convert the array back to a dataframe confirm no missing values from transformation For example let s see the cont10 variable if it s normally distributed The blue dots should adjust to the 45 degree line For example let s see the cont10 variable if it s normally distributed The blue dots should adjust to the 45 degree line Select only numeric cols Select only numeric cols confirm no missing values from transformation For example let s see the cont10 variable if it s normally distributed The blue dots should adjust to the 45 degree line Select only numeric cols Let s learn the means Let s learn the ranges Fit the learned means and ranges to the train set If test also fit it on the test set confirm no missing values from transformation set up the scaler fit the scaler to the train set it will learn the parameters Fit to only numerical columns convert back to df transform testset convert back to df confirm no missing values from transformation norm takes a str value of l1 or l2 set up the scaler fit transform the scaler to the train set it will learn the parameters fit on only numeric columns convert back to df transform testset convert back to df confirm no missing values from transformation confirm no missing values from transformation function to create histogram Q Q plot and boxplot of specific variables function takes a dataframe df and the variable of interest as arguments define figure size histogram Q Q plot boxplot Let s look at variable cont8 before applying winsorization Let s look at variable cont8 before applying winsorization Let s see the data Let s see the normalized data ensure activation is all lower case Reducing Num_cols to Float 32 Reducing Cat_cols to Int 32 if OHE. Import relevant libraries 2. walk kaggle input for filename in filenames print os. They both have a perfect range across all features between 0 and 1. Saving the pre processed train target and test sets. Note that each transformation function must only contain one key word amongst other words. Let s see the summary info of the data 6. Let s try to reposition the data to have mean 0 and Std 1. But Mean Norm balances the data better with zero deviation from the mean of 0 and slightly lower deviation from the std of 1 than Box Cox. An outlier is a data point that is significantly different from the remaining data. This data does not have a general MEAN of 0 and STD of 1. read_csv Input data files are available in the read only. Let s re check the distance of means from 0 and stds from 1 in mean norm df The percentage of outliers to data size has also reduced from 3. When scaling to vector unit length we divide each feature vector by its norm using either l1 manhattan dist or l2 euclidean dist norm. Dropping Target VariableMost machine learning models work better with a normalized data set. 5 but can be changed to use in the IQR calculation and returns the IQR proximity rule boundaries Let s find the extreme outliers for train boxcox these are outliers 3 times the IQR rate. We can also see the 45 deg line and the blue dots that roughly keep to the red line of the Q Q Plot Probability Plot this also indicates presence of a normal distribution. Applying Variance Inflation Factor We shall use VIF to determine the overall columns with high multi collinearity and seive them out. Performing Data Transformations on Test set We also need to transform the test set on values learnt on the training set. The higher the Kurtosuis measure is the more outliers are present and the longer the tails in the distribution of the histogram are Let s return to the mean_norm_df data and apply kurtosis Now let s apply the winsorizer First let s calculate the kurtosis score and see if it s gone down. In other cases outliers are rare observations that do not add any additional value. We need to treat all such columns 13. We shall use the box cox normalization for the numerical columns 16. So let s use a simple join since they both have the same index col. So let s continue by keeping these variables. Check Cardinality of Categorical Variables. Kurtosis indicates the outlier content within the data. It seems these variables are just around 0 all through. Let s consider the Z score or Standardization method The MEAN distance and STD distance above show the sum total of how far the numeric features in the DataFrame in this case train boxcox are far away from a MEAN of 0 and STD of 1 Let s see the distribution range of values after box cox The range of distribution above from the box cox transformation is just awesomely perfect Even though the MEAN and STD are not around 0 or 1 17. Pinpointing rare categories in categorical variablesCategories that appear in a tiny proportion of the observations are rare. Let s see the average target score per categorical variable per column 9. Plus the MEAN dist and STD dist are worse than Mean norm but better than Boxcox. Double check for possible NAN values after join 11. 5 and this is not enough to drop the columns so we continue. One Hot Encoding Categorical Variables Each column of values seem too alike and it makes no sense to keep all. Finally we can see the thick dotted ouliers above point 4on the y axis of the Box plot indicating the presence of outliers. Visualizing Variable Normality using a Q Q Plot Normality can be also assessed by Q Q plots. According to the IQR proximity rule a value is an outlier if it falls outside these boundaries Upper boundary 75th quantile IQR 1. Checking for Multi collinearity 14. This makes Robust norm not yet an ideal choice over Mean Norm. ipynb First we make the unique high corr data a dataframe Next we standardize the data Next we apply the VIF The VIF is a measure of colinearity among predictor variables within a multiple regression. Applying Scaling to Vector Unit Length L1 Norm. Also Box cox is just slightly better on outliers as compared to Mean norm. Yep it s gone down from 245 to a mere 41 because the winsorizer has fixed a lot of the outliers in the data the only outlies left may warrant us to be more strict with our IQR rate down to 1. A feature vector contains the values of each variable for a single observation. 25 Let s see how far away values are from the MEAN and STD We Can see that the distribution range of pure numerical values for Robust norm is uneven and chaotic like Z Score norm going from 0 up to almost 5. Standardization is also called Z Score Norm Let s see how far away each numeric feature s MEAN and STD is from 0 and 1 We can see that there is virtually no distance between the features MEAN and STD of train_stdize as impress as this is let s see the range of distribution. Winsorization or winsorizing is the process of transforming the data by limiting the extreme values that is the outliers to a certain arbitrary value closer to the mean of the distribution. Here we want to grab the rare cat column names just as they would appear after the O H E so that we can drop them off as individual columns from cat_vars dataframe immediately after applying O H E. Applying Robust Scaling to Train copy This is also called scaling with median and quantiles. In this case all VIF scores are just between 1 and 2. Visualizing The Relationship between Numerical Variables and Target 12. Running the cell below repeatedly plots different pairs of variables. Though the data is more central around the mean the range is uneven and chaotic. Applying Scaling to Vector Unit Length L2 Norm. Read in The Data Files. Let s see the outliers train_stdize data has less than 3 outliers to dataset ratio. concat will return NAN values. Drop categorical columns with 10 or less value contribution per variable Let s first make a copy of the original train set and use this for the transformations Merge both dataframes. Let s investigate further with a correlation matrix Clearly there is no linear relationship between each numerical variable and TargetThe only way to learn any meaningful representation is to use a non linear style regression One more important thing we can learn from the corr matrix is that some columns may be highly correlated or multi collinearity issues. A typical strategy involves setting outliers to a specified percentile. L1 for vector unit length L1 5. Standardization is not an option because even though it perfectly balances the distribution of all variables at a MEAN of 0 and STD of 1 it causes a chaotic range of distribution and an outlier ratio worse than Box cox and similar to Mean norm. 5 Here IQR is given by the following equation IQR 75th quantile 25th quantile Let s randomly plot two variables we might see some outliers. Let s try a couple more. Note that this scaling technique scales the feature vector as opposed to each individual variable. Let s print their min and max values to be certain. We can see that the dotted outliers have disappeared after applying winsorization to the vectorl1_df data 26. 5 Lower boundary 25th quantile IQR 1. Just as suspected all have a min equal min values and max values. Let s try mean normalization. On occasions outliers are very informative for example when looking for credit card transactions an outlier may be an indication of fraud. Let s check the cardinality of each Cat variable and delete those with high cardinality. We observe this through the boxplot as well as the average comparison to the target variable done above for each cat column. When scaling variables to the median and quantiles the median value is deducted from the observations and the result is divided by the inter quartile range IQR. But since I d treat outliers soon I so far prefer the centralised distribution of Mean norm. ROBUST for robust scaling 4. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory import os for dirname _ filenames in os. STANDARD for standardization Instantiate an instance of the FinalPrep class passing if we want One Hot Encoding or Not and the type of transformation for the final test and train data sets. Checking for missing Values 5. Applying Mean Normalization to Original Dataset In mean normalization we center the variable at zero and rescale the distribution to the value range. Applying Standardization to Train Copy dataset. While those of categorical values are within 0 to 1. Correlation Strength Looking at the regplot for each numerical variable and Target there is a general weak linear relationship. Link https github. If the outcome is 1 it s okay. These are the values whose relative values were dropped for being smaller than the threshold we set earlier. 33 refers to the few original numerical variables that have moderate outliers around 1. L2 for vector unit length L2 6. If the variable follows a normal distribution the dots in the Q Q plot should fall in a 45 degree diagonal line. If it s between 1 and 5 it shows low to average colinearity and above 5 generally means highly redundant and variable should be dropped. This is the result of the mean norm we did earlier. In a Q Q plot we plot the quantiles of the variable in the y axis and the expected quantiles of the normal distribution in the x axis. Applying Box Cox Transformation to Original Dataset. ", "id": "black9t/feb-tab-data-pred1", "size": "14373", "language": "python", "html_url": "https://www.kaggle.com/code/black9t/feb-tab-data-pred1", "git_url": "https://www.kaggle.com/code/black9t/feb-tab-data-pred1", "script": "SparkSession box_plots pathlib train_test_split deepcopy standardize plot_range robust_scaler pyspark.sql mean_norm statsmodels.stats.outliers_influence numpy seaborn Path FinalPrep(object) train_test _reduce_datasets category_freq _apply_transform _winsorization _copy_test tensorflow drop_rare_cat_cols RobustScaler  # for median/range or Robust Scaling mean_std_distance matplotlib.pyplot MinMaxScaler copy outliers_toDF calc_kurtosis plot_hist standardize_numCols sklearn.model_selection Normalizer  # for vector unit-length-norm pandas PowerTransformer Winsorizer sklearn.pipeline find_boundaries mean_squared_error apply_boxcox_scaler scipy.stats reg_plots variance_inflation_factor Pipeline vector_unit_scaler norm_scale_summary __init__ diagnostic_plots feature_engine.outliers clean_winsorizer _keep_same_cols sklearn.metrics sklearn.preprocessing ", "entities": "(('s', 'same columns'), 'join') (('We', 'matrix'), 'identify') (('those', '1'), 'be') (('between it', '5'), 'show') (('Kurtosis', 'data'), 'indicate') (('MEAN dist', 'Boxcox'), 'be') (('decision', 'Box Cox'), 'involve') (('s', 'min values'), 'let') (('we', 'H immediately E.'), 'want') (('that', 'Cat feature'), 'count') (('data', 'ratio'), 'let') (('We', 'numerical columns'), 'use') (('too similar they', 'predictive power'), 'visualize') (('data that', 'significantly remaining data'), 'be') (('result', 'inter quartile range IQR'), 'deduct') (('activation', 'Int'), '0') (('they', 'index same col'), 'let') (('percent', 'train boxcox'), '5') (('FinalPrep class Instantiate we', 'data sets'), 'STANDARD') (('we', 'outliers'), 'give') (('when it', 'population'), 'consider') (('Running', 'variables'), 'plot') (('it', 'kurtosis score'), 'be') (('typical strategy', 'specified percentile'), 'involve') (('This', 'also median'), 'call') (('read_csv Input data files', 'read'), 'be') (('they', 'zero'), 'use') (('dotted outliers', 'data'), 'see') (('just awesomely Even MEAN', 'box cox above transformation'), 'let') (('Just all', 'min min equal values'), 'have') (('So s', 'variables'), 'let') (('This', 'Mean Robust yet ideal Norm'), 'make') (('dataframes', 'pd'), 'have') (('how much outliers', 'dataset'), 'AKA') (('We', 'histogram'), 'see') (('we', 'machine learning'), 'perform') (('we', 'certain threshold'), 'd') (('this', 'normal distribution'), 'see') (('that', 'distribution'), 'be') (('we', 'value range'), 'apply') (('range', 'more mean'), 'be') (('that', '1'), 'refer') (('Finally we', 'outliers'), 'see') (('s', 'data'), 'let') (('we', 'l1 manhattan dist'), 'divide') (('we', 'threshold'), 'be') (('Dropping', 'data better normalized set'), 'work') (('it', 'Mean norm'), 'be') (('percentage', 'also 3'), 'let') (('soon I', 'Mean norm'), 'prefer') (('certain categorical variables', 'basically more distribution'), '0') (('It', 'kaggle python Docker image https github'), 'come') (('only outlies', 'down 1'), 'go') (('s', 'column'), 'let') (('Box Also cox', 'Mean norm'), 'be') (('transformed vector', 'norm 1'), 'transform') (('we', 'mean norm'), 'be') (('input directory', '_ os'), 'list') (('s', '0'), 'let') (('data', 'just present data'), 'produce') (('that', 'table'), 'let') (('columns', 'corr matrix'), 'let') (('first s', 'before winsorization'), 'let') (('extreme values', 'instead other values'), 'be') (('Mean Norm', 'Box Cox'), 'balance') (('rare that', 'additional value'), 'be') (('we', 'columns'), 'be') (('dots', '45 degree diagonal line'), 'follow') (('s', 'mean_norm_df'), 'let') (('We', 'them'), 'apply') (('They', '0'), 'have') (('We d', 'inter quartile range IQR proximity rule'), 'apply') (('factor default', 'dataframe'), 'create') (('we', 'x axis'), 'plot') (('VIF', 'multiple regression'), 'make') (('transformation function', 'other words'), 'note') (('data', '1'), 'have') (('VIF scores', 'case'), 'be') (('Visualizing', 'Q Q also plots'), 'assess') (('that', 'observations'), 'be') (('too alike it', 'all'), 'variable') (('s', 'distribution'), 'call') (('it', 'Upper boundary IQR'), 'be') (('s', 'Merge dataframes'), 'drop') (('feature vector', 'single observation'), 'contain') (('outlier', 'fraud'), 'be') (('Z Score norm', 'up almost 5'), 'let') (('We', 'training set'), 'need') (('these', 'train boxcox'), 'change') (('scaling technique', 'individual variable'), 'note') (('We', 'cat above column'), 'observe') (('s', 'high cardinality'), 'let') "}