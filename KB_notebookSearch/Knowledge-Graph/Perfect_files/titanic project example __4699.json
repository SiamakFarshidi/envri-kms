{"name": "titanic project example ", "full_name": " h1 Titanic Project Example Walk Through h2 Overview h3 1 Understand the shape of the data Histograms box plots etc h3 2 Data Cleaning h3 3 Data Exploration h3 4 Feature Engineering h3 5 Data Preprocessing for Model h3 6 Basic Model Building h3 7 Model Tuning h3 8 Ensemble Modle Building h3 9 Results h2 Project Planning h2 Light Data Exploration h3 1 For numeric data h3 2 For Categorical Data h2 Feature Engineering h3 1 Cabin Simplify cabins evaluated if cabin letter cabin adv or the purchase of tickets across multiple cabins cabin multiple impacted survival h3 2 Tickets Do different ticket types impact survival rates h3 3 Does a person s title relate to survival rates h2 Data Preprocessing for Model h3 1 Drop null values from Embarked only 2 h3 2 Include only relevant variables Since we have limited data I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with h3 3 Do categorical transforms on all data Usually we would use a transformer but with this approach we can ensure that our traning and test data have the same colums We also may be able to infer something about the shape of the test data through this method I will stress this is generally not recommend outside of a competition use onehot encoder h3 4 Impute data with mean for fare and age Should also experiment with median h3 5 Normalized fare using logarithm to give more semblance of a normal distribution h3 6 Scaled data 0 1 with standard scaler h2 Model Building Baseline Validation Performance h2 Model Tuned Performance h2 Model Additional Ensemble Approaches ", "stargazers_count": 0, "forks_count": 0, "description": "5 Random Forest 80. With a validation set basline we can see how much tuning improves each of the models. 425 accuracy Top 12 Overview 1 Understand the shape of the data Histograms box plots etc. For a hard voting classifier each classifier gets 1 vote yes or no and the result is just a popular vote. For this you generally want odd numbers A soft classifier averages the confidence of each of the models. For this analysis we will be exclusively working with the Training set. Below is the rough outline that I created for this project using commented cells. The accompanying video is located here https www. This is for convenience difference in numeric vs non numeric tickets in survival rate survival rate across different tyicket types feature engineering on person s title mr. 9 4 Experimented with soft voting on all estimators including XGB KNN SVM RF LR XGB 83. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session Understand nature of the data. describe Histograms and boxplots Value counts Missing data Correlation between the metrics Explore interesting themes Wealthy survive By location Age scatterplot with ticket price Young and wealthy Variable Total spent Feature engineering preprocess data together or use a transformer use label for train and test Scaling Model Baseline Model comparison with CV quick look at our data types null counts to better understand the numeric data we want to use the. For our final submissions we will make predictions based on the test set. Model Baseline Tuned Performance Naive Bayes 72. 6 2 Experimented with a soft voting classifier of three estimators KNN SVM RF 82. 5 This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. This gives us an understanding of the central tendencies of the data quick way to separate numeric columns look at numeric and categorical values separately distributions for all numeric variables compare survival rate across Age SibSp Parch and Fare Comparing survival and each of these categorical variables after looking at this we may want to look at cabin by letter or by number. get_dummies created dummy variables from categories also can use OneHotEncoder Split to train test again Scale data I usually use Naive Bayes as a baseline for my classification tasks Voting classifier takes all of the inputs and averages the results. read_csv Input data files are available in the read only. 4 Impute data with mean for fare and age Should also experiment with median 5 Normalized fare using logarithm to give more semblance of a normal distribution 6 Scaled data 0 1 with standard scaler Model Building Baseline Validation Performance Before going further I like to see how various different models perform with default parameters. etc create all categorical variables that we did above for both training and test sets impute nulls for continuous data drop null embarked rows. Titanic Project Example Walk Through In this notebook I hope to show how a data scientist would go about working through a problem. 6 K Nearest Neighbor 80. 6 Logistic Regression 82. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. We also may be able to infer something about the shape of the test data through this method. Usually we would use a transformer but with this approach we can ensure that our traning and test data have the same colums. com watch v I3FBJdiExcg Best results 79. Only 2 instances of this in training and 0 in test tried log norm of sibsp not used log norm of fare used converted fare to category for pd. 6 NA K Nearest Neighbor 80. 0 Random Forest 80. 2 Data Cleaning 3 Data Exploration 4 Feature Engineering 5 Data Preprocessing for Model 6 Basic Model Building 7 Model Tuning 8 Ensemble Modle Building 9 Results Here we import the data. I tried the following models using 5 fold cross validation to get a baseline. I also used Randomized Search for the Random Forest and XG boosted model to simplify testing time. I took the best model from this and did a more granular search in a soft voting classifier you can weight some models more than others. 3 Best Performance 3 Experimented with soft voting on all estimators performing better than 80 except xgb KNN RF LR SVC 82. The goal is to correctly predict if someone survived the Titanic shipwreck. We will be validating based on data from the training set as well. 6 Support Vector Classifier 83. 2 Xtreme Gradient Boosting 81. This is an area where we may want to do some feature engineering Feature Engineering 1 Cabin Simplify cabins evaluated if cabin letter cabin_adv or the purchase of tickets across multiple cabins cabin_multiple impacted survival 2 Tickets Do different ticket types impact survival rates 3 Does a person s title relate to survival rates Data Preprocessing for Model 1 Drop null values from Embarked only 2 2 Include only relevant variables Since we have limited data I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with Variables Pclass Sex Age SibSp Parch Fare Embarked cabin_adv cabin_multiple numeric_ticket name_title 3 Do categorical transforms on all data. Let s create some categories for this letters multiple letters creates categories based on the cabin letter n stands for null in this case we will treat null values like it s own category comparing surivial rate by cabin understand ticket values better numeric vs non numeric lets us view all rows in dataframe through scrolling. Just because a model has a high basline on this validation set doesn t mean that it will actually do better on the eventual test set. 8 Model Tuned Performance After getting the baselines let s see if we can improve on the indivdual model results I mainly used grid search to tune the models. 6 Decision Tree 77. Light Data Exploration 1 For numeric data Made histograms to understand distributions Corrplot Pivot table comparing survival rate across numeric variables 2 For Categorical Data Made bar charts to understand balance of classes Made pivot tables to understand relationship with survival Perhaps we should take the non normal distributions and consider normalizing them Cabin and ticket graphs are very messy. 3 Model Additional Ensemble Approaches 1 Experimented with a hard voting classifier of three estimators KNN SVM RF 81. Project PlanningWhen starting any project I like to outline the steps that I plan to take. 6 NA Logistic Regression 82. I used a grid search to explore different weightings no new results here Make Predictions convert output to dataframe track differences between outputs prepare submission files. If a the average confidence is 50 that it is a 1 it will be counted as such simple performance reporting function Because the total feature space is so large I used a randomized search to narrow down the paramters for the model. I will stress this is generally not recommend outside of a competition use onehot encoder. 1 Decision Tree 77. 8 Soft Voting Classifier All Models 82. I thought it would be fun to see how well I could do in this competition without deep learning. ", "id": "kenjee/titanic-project-example", "size": "4699", "language": "python", "html_url": "https://www.kaggle.com/code/kenjee/titanic-project-example", "git_url": "https://www.kaggle.com/code/kenjee/titanic-project-example", "script": "xgboost sklearn.svm numpy cross_val_score XGBClassifier seaborn SVC tree GaussianNB sklearn.neighbors sklearn.naive_bayes clf_performance sklearn.linear_model sklearn matplotlib.pyplot sklearn.model_selection pandas RandomForestClassifier LogisticRegression VotingClassifier KNeighborsClassifier RandomizedSearchCV GridSearchCV sklearn.ensemble StandardScaler sklearn.preprocessing ", "entities": "(('soft classifier', 'models'), 'want') (('us', 'scrolling'), 'let') (('Voting classifier', 'results'), 'create') (('here Predictions', 'submission files'), 'use') (('Histograms', 'data'), 'Top') (('We', 'method'), 'be') (('read_csv Input data files', 'read'), 'be') (('non numeric tickets', 'engineering'), 'be') (('I', 'baseline'), 'try') (('6 2', 'three estimators'), 'experiment') (('correctly someone', 'Titanic shipwreck'), 'be') (('We', 'training'), 'validate') (('3 Model', 'three estimators'), 'experiment') (('you', 'more others'), 'take') (('I', 'models'), 'Model') (('I', 'that'), 'PlanningWhen') (('we', 'the'), 'describe') (('result', '1 vote'), 'get') (('t', 'data'), 'list') (('how well I', 'deep learning'), 'think') (('It', 'kaggle python Docker image https github'), '5') (('we', 'test set'), 'make') (('we', 'number'), 'give') (('Here we', 'data'), 'Preprocessing') (('numeric_ticket cabin_adv cabin_multiple name_title', 'data'), 'be') (('it', 'test actually better eventual set'), 'mean') (('I', 'competition generally outside onehot encoder'), 'stress') (('so I', 'model'), 'count') (('Only 2 instances', 'pd'), 'try') (('I', 'commented cells'), 'be') (('how much tuning', 'models'), 'see') (('traning data', 'same colums'), 'use') (('we', 'Training exclusively set'), 'work') (('Cabin graphs', 'them'), 'Exploration') (('we', 'null embarked rows'), 'create') (('I', 'testing time'), 'use') (('how various different models', 'default parameters'), 'experiment') (('data how scientist', 'problem'), 'walk') "}