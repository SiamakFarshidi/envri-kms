{"name": "yours sincerely core ", "full_name": " h2 Overview h3 Takebacks h2 MyNotes h1 Acknowledgements h1 Levers for different kernels h1 Section Utilities h2 Leverage HTML output for pretty printing logs h2 Profiler utility for performance monitoring h2 MultiProcessing Utility h2 Embeddings Utility h1 DataManager h1 Section Preprocessing Text h2 Text Handling h2 QuoraPreprocessor h1 Section Synthetic Data Generator h2 Overview h2 TopicalWords h2 InsincereVocabGenerator h2 InsincerityWithNER h2 QuestionsGenerator h1 Section Machine Learning h2 Attention Layer h2 Model Factory h2 NLPPatterns h1 The Flow h1 Main Where It Begins h1 Rough Work area ", "stargazers_count": 0, "forks_count": 0, "description": "words that are similar to the seed but not the words in the other 2 kinds. Find the right list of substitutes For each substitutable phrase Output Find multiple word lists with the largest phrase Create generated questions CSV that matches the train. Lots of focus on i believe what I see and so lots of print statements. OverviewMain focus Generating synthetic data because only 6 of data is tagged insincereGreat place to start code review would be to find the classic C style main function near the bottom. com nikhilroxtomar gru with kfold lb 0 689 Model FactoryTo try out different models since inputs and outputs from the model are the same. QuestionsGeneratorFinally the multi processor generation of questions using phrase substitutions from InsincereBoW above Section Machine Learning Attention LayerCopy pasted from https www. So multiprocessing seemed good to invest time and also for learning. One model factory per word embedding. read_csv Use join to get full file path. They are vertically matching. Similarity does not mean synonymity. I followed no external data sources so strictly that I didn t realize I could import intermediate results from other kernelsMyNotes Improve q generation questions not used for generating q. Taboo topics are Religion Ethnicity group of people Politics Sexuality Words with negative intense emotion expletives Generate taboo words from similarities to some set of seed words in word2vec for each topic above. This method changes the underlying rawdata and file names for processing training vs test data. Another is reuse for future projects. 4 x BatchNorm XXX channel 1 channel 2 channel 3 merge interpretation evaluator_cb F1Evaluation validation_data X_val y_val interval 1 verbose 1 Initialize a counter Combined generated cached file with input to create big training data cache. horizontally relevant to topical word Textually matching words Words that could be substituted for each other and the source word. InsincerityWithNER This is used to eyeball the highest frequency named entities that are involved in insincerity. It doesn t know anything about natural language. Pickled binary object If there is a CSV file then reduce. if not found and verbose log_list found_multi_words desc Words of MultiWord not found. words that partially match the seeds and 1. DataManagerData loader Section Preprocessing Text Text Handling So far we have loaded up the files provided by the competition and added functionality for their content. Mix embeddings GLOVE and PARA seem popular Add features by hand Glove only OOV hand fixes explore words in different embeddings Acknowledgements1. Helper function if gWIP log f Not found retval Reduces output files from worker processes to single file. Find matching words in related words to source word Add all related words that are not matching words to insincere_vocab category seed topical words by category Insincere_bow is an dictionary of categories. XXX culling requires more attention. For all insincere questions if there is a term that matches any list above create new questions by interchanging the terms from the appropriate list. word2count is a Counter dictionary Starting index at 1 so that index 0 is reserved. And similarly with other taboo topics. Assumes that each worker process will write output to a file map out and when they are all done the outputs will be reducedand written out. NLPPatterns The NLPPatterns only operates on numbers. cleanup related words that are not in word_list are newly discovered topics. GOOGLENEWS is done differently doesn t use mean std why XX phrase phrase. For manual review as well as saving dev time save generated questions to file. XXX what assumptions can be made about train. http The final result is cached to file for use by the QuestionGenerator. Code quality Lots of OOP not seen commonly with ML projects but no getters setters for dev speed and bloat. Sort list of tuples by the first element size. An expletive can probably be replaced with another similar expletive. Twice for newly discovered topical words Iterate categories PEOPLES Hindu None. PARAGRAM has all lowercase. The Flow Main Where It Begins Rough Work area This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. can make a question insincere. It is used for manually creating the seed for InsincerityVocabGenerator. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. Additionally words that belong in a category should match the type of Named Entity of the category. QuoraPreprocessorAssimilation of lots of good and partially good advice on the Kaggle kernels for this competition. Since we have created a combined file. Add newly discovered words to topical words for this category Find multiple word lists with the largest phrase Search for substitutes by matching the longest phrase in the question within each category. It means substitutable for the purposes of insincerity. Foresight suggested that I might be experimenting with many models and I wasn t clear how the limits on 2h GPU vs 6hr CPU would pan out. words that relate the most to the list of partially matched 1. Takebacks Editor issues Coding on Kaggle s Jupyter is painful primarily because lots of code means too much scrolling I am new to Kaggle and AI competitions. Most likely python has these facilities but easier to implement my intentions here than learn new stuff due to time. What I learnt is applicable for multi node processing in future. Just that bad things said about a group of people and what is said. Each category is mapped to an array of dictionaries of words. Get size and add to list of tuples. The similar words may fall under 3 kinds 1. Did it by using filesystem. Section Utilities Leverage HTML output for pretty printing logs Profiler utility for performance monitoringPut profile above any method and performance is automatically tracked. For each matching phrase there could be multiple lists of substitutions. Upper case word s index gets assigned to lower case word s vector For paragram let s give the vector for lowercase word to uppercase word Allow any form of a word that is in the word2index and if not present then find the lowercase and and if present but PARAGRAM then find all indices of all forms of the word in word2index. MultiProcessing UtilityI am new to multiprocessing especially on Python. Primary reason for OOP would be readability and avoiding huge function signatures. Reduction handles csv. Find related words to source word. It receives a model training data as padded sequences. For each insincere question If a word phrase in the question is in the list of taboo words that are similar then Replace that word phrase with other similar words or phrases. Returns embeddings matrix for one source. So this complies with the rule of no external data sources. How can we generate more insincere questions In essence Bad things said about one group of people are likely to be equally bad for another group. Get X Get y Get embeddings matrix Select Model Train initialize_cache remove_file sequences remove_file submission. Handles csv files if they exist. Raw cleaned data Clean Question Text load reduced csv Collect unique words vocab Combined training and test vocab into 1. TopicalWordsUsed GOOGLENEWS Word2Vec to generate words that are similar to each other such that they are substitutable. We don t need to teach model human history. can t use member function output_cache_name because GOOGLENEWS has compound words with underscore Allow underscore in single word and check if each component is also in the vocab. Memory efficient loading except for GOOGLENEWS Word2Vec which is loaded as a whole. Returns an array of words RawData Pre processing Word Count Raw cleaned data Produce vocab dictionary The maxlength of questions at 60 arrived at after counting words in training set. Lots of interim results written to filesystem. Iterate topical words in each category Hindu Sikhs. For training more data is better. Brevity not a focus. com theoviel improve your score with text preprocessing v2 Levers for different kernels Many of these have the biggest impact on compute time. Naming matters to me for clarity of thought and OOP indirections enable better naming. replace punct f punct log_list desc Raw Data Statistics Spelling errors tokenwise Does more than just split the phrase. These words can be used to expand the seed. The question generation takes a while and would have been much faster on 4 CPU but the training over 2DCNN is much faster on GPU. Now we need to Build some intuition about the entire dataset eyeballing stats and samples Preprocess the text Generate synthetic data see how below Create word indices Create training sequences each question is an array of indices which refer to words in the embeddings matrix To save developer time save to file embeddings matrix word indices training sequences Generate synthetic data Get external list of terms that are expletives ethnicities and politics using word2vec word similarities. Section Synthetic Data Generator OverviewObjective There are only 6 insincere questions. Collect unique words vocab This class was originally written to process some rawdata and write the results to file without focus on training vs test. InsincereVocabGeneratorUsing TopicalWords and an initial set of words this class uses similarity function from Word2Vec to discover similar sets of words that could be substituted into a question with each other. Combine all words that vertically belong to the source word which is the topic Topics that are more similar to textually matching words. Important insight in many published kernels the vocabulary is created from training data only. Embeddings Utility Memory and performance efficient way to load embeddings. All target methods must have a signature func self cpu_id cpu_count cache_file. This isn t correct because the model for training and predictions has the same embeddings matrix. Can explore merging word embeddings later. RawData word substitutes Multi process q generation To help improve generation algorithm. Explored multiprocessing on python first time. x SpatialDropout1D 0. ", "id": "takeseven/yours-sincerely-core", "size": "6498", "language": "python", "html_url": "https://www.kaggle.com/code/takeseven/yours-sincerely-core", "git_url": "https://www.kaggle.com/code/takeseven/yours-sincerely-core", "script": "QuoraSequence(Sequence) gen_from_one_q Profiler massage_data read_embeddings_file data_partition create_LSTMGRU_Model regularizers glove2word2vec get_tag contains layers keras.callbacks tensorflow.python.client datapath gensim.test.utils fillna make_clean_questions matplotlib.pyplot tensorflow.keras.callbacks ModelCheckpoint do_substitutes metrics output_cache_name make_vocab gen_NER QuestionsGenerator remove_file log_dir constraints tensorflow.keras multiprocessing with_profiling make_word2index log_list get_NER_label word2count generate_bow InsincereVocabGenerator collections __getitem__ find_patterns inspect DetectorFactory decontract mislabeled_sincerity move_files gensim.models get_tmpfile keras build process spacy tensorflow.keras.models initializers memclean despace test_method map_process instance save initialize_cache get_coefs make_path pad_sequences has_gpu clear_prof_data copyfile wraps TextualDataControl pos_phrase sklearn make_sequences class_method get_model validate displacy profile pandas filter_related_sources find_substitute_phrases langdetect call find_prediction_threshold is_vocab nltk.corpus TestProfiler synthesize_data create_CPU_LSTM_Model __len__ display nlp detect optimizers make_word2count IPython.core.display testme get_prof_data precompute_mean_std save_combined_data compute_output_shape denumber NLPPatterns functools numpy start find_matching_phrases train shutil main TestMPCWithFileCache gensim.scripts.glove2word2vec AttentionLayer(Layer) on_epoch_end cpu_cache_name Sequence is_substitutable save_binary Tokenizer ReduceLROnPlateau create_2DCNN_Model gen_q_proc ModelFactory find_string InsincerityWithNER defaultdict spawn KeyedVectors generate_similar_words keras.preprocessing.text sequences_proc rmtree calculate sent_capitalize apply_patterns log_current_memory HTML compute_mask tensorflow.keras.optimizers QuoraPreprocessor(TextualDataControl) create_GPU_GRU3_Model log_prof_data create_1DCNNNgram_Model sklearn.metrics pp_prof_data show_html create_2DCNN_ConcatModel log find_substitutes_list range_partition Counter MPHelper data_generator keras.preprocessing.sequence train_test_split predict tensorflow.keras.layers tokenize testme2 make_substitutes_bow seaborn combine_vocab clean f1_score Callback compilation backend as K embeddings load_embedding_index reduce filter_substitutes F1Evaluation(Callback) cleaning_proc tqdm TopicalWords nogen_q_proc create_GPU_LSTM_Model device_lib multi_word_in_vocab DataManager multi_gpu_model backend clean_cache EmbeddingsControl() sklearn.model_selection depunctuate make_tokens set_mode find_substitute_candidates EarlyStopping respell load stopwords get_topical_words display_cache tqdm.autonotebook keras.utils get_data __init__ fix_special_chars make_callbacks vocab_proc ", "entities": "(('Returns', 'one source'), 'embedding') (('that', 'expletives word2vec word similarities'), 'need') (('assumptions', 'train'), 'xxx') (('So multiprocessing', 'time'), 'seem') (('csv Collect unique words', 'test 1'), 'clean') (('index', 'Starting Counter dictionary 1'), 'be') (('retval Reduces', 'single file'), 'function') (('that', 'appropriate list'), 'create') (('what', 'people'), 'say') (('only 6', 'bottom'), 'focus') (('Most likely python', 'time'), 'have') (('It', 'insincerity'), 'mean') (('Add', 'category'), 'find') (('I', 'Kaggle competitions'), 'issue') (('I', 'print so statements'), 'believe') (('Raw Data Statistics Spelling errors', 'more just phrase'), 'replace') (('how limits', '6hr CPU'), 'suggest') (('Combined', 'training data big cache'), 'interpretation') (('that', 'category'), 'match') (('maxlength', 'training set'), 'return') (('Textually matching that', 'other'), 'relevant') (('about one group', 'equally group'), 'generate') (('Taboo topics', 'topic'), 'be') (('method', 'test data'), 'change') (('Words', 'MultiWord'), 'desc') (('Iterate categories', 'Hindu None'), 'twice') (('X', 'embeddings matrix Select Model Train initialize_cache remove_file sequences remove_file submission'), 'get') (('inputs', 'model'), 'com') (('NLPPatterns', 'only numbers'), 'nlppattern') (('OOP indirections', 'thought'), 'matter') (('It', 'python docker image https kaggle github'), 'Main') (('didn I', 'q.'), 'follow') (('final result', 'QuestionGenerator'), 'http') (('that', 'partially matched 1'), 'word') (('target methods', 'cpu_count func cache_file'), 'have') (('Section Utilities Leverage HTML output', 'method'), 'track') (('We', 'model human history'), 'don') (('getters', 'dev speed'), 'see') (('OOV hand only fixes', 'different embeddings'), 'seem') (('GOOGLENEWS', 'differently doesn'), 'do') (('It', 'padded sequences'), 'receive') (('t', 'natural language'), 'doesn') (('PARAGRAM', 'word2index'), 'assign') (('training', 'much GPU'), 'take') (('as well saving', 'generated questions'), 'save') (('It', 'manually seed'), 'use') (('Primary reason', 'function huge signatures'), 'be') (('Many', 'compute time'), 'have') (('that', 'other'), 'use') (('MultiProcessing UtilityI', 'especially Python'), 'be') (('Section Machine Learning Attention LayerCopy', 'https www'), 'paste') (('that', 'categories'), 'be') (('that', 'word_list'), 'be') (('that', 'insincerity'), 'insinceritywithner') (('expletive', 'probably similar expletive'), 'replace') (('that', 'other similar words'), 'be') (('which', 'whole'), 'loading') (('topic that', 'more textually matching words'), 'combine') (('similar words', '3 kinds'), 'fall') (('component', 'also vocab'), 'use') (('such they', 'other'), 'Word2Vec') (('RawData word Multi', 'generation algorithm'), 'substitute') (('So far we', 'added content'), 'DataManagerData') (('class', 'test'), 'write') (('that', 'partially seeds'), 'word') (('learnt', 'future'), 'be') (('category', 'words'), 'map') (('vocabulary', 'data'), 'create') (('model', 'embeddings same matrix'), 'correct') (('that', 'other 2 kinds'), 'word') (('when they', 'outputs'), 'be') (('that', 'train'), 'generate') (('words', 'seed'), 'use') "}