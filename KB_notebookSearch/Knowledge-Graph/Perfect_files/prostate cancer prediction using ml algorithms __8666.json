{"name": "prostate cancer prediction using ml algorithms ", "full_name": " h1 IMPORTING LIBRARIES h1 Exploratory Data Analysis h1 DIAGNOSIS Explanation h1 VISUALIZATION h1 1 Logistic regression classification h1 2 SVM Support Vector Machine classification h1 3 Naive bayes classification h1 4 Decision Tree classification h1 5 Random forest classification h1 6 K Nearest Neighbor classification h1 Comparison of classification techniques ", "stargazers_count": 0, "forks_count": 0, "description": "It is noteworthy that there is no information on the joint probability functions therefore a free distribution learning must be carried out. We define the number of trees in the forest in 100. In order to make the prediction of the probability the logistic function is used which allows us to obtain the log odds or the probit. Benign tumors may need to be removed by surgery. Logistic regression classificationLogistic regression is a technique that can be applied to binary classification problems. Naive bayes classificationThe naive Bayesian classifier is a probabilistic classifier based on Bayes theorem with strong independence assumptions between the features. In this sense the decision tree selects the best attribute using to divide the records converting that attribute into a decision node and dividing the data set into smaller subsets to finally start the construction of the tree repeating this process recursively. For each node of the tree randomly choose m variables on which to base the decision. Data points falling on either side of the hyperplane can be attributed to different classes 3. In this technique the number of neighbors k is usually an odd number if the number of classes is 2. SVM Support Vector Machine classificationSVMs Support Vector Machine have shown a rapid proliferation during the last years. Hence SVMs belong to the supervised learning techniques. From the classification approach the goal of SVM is to find a hyperplane in an N dimensional space that clearly classifies the data points. In Brief A cancer in a man s prostate a small walnut sized gland that produces seminal fluid. Decision Tree classificationA decision tree is a flowchart like tree structure where an internal node represents feature the branch represents a decision rule and each leaf node represents the outcome. All the training data used in the test phase. K Nearest Neighbor classificationK Nearest neighbors is a technique that stores all available cases and classifies new cases based on a similarity measure e. 1 Fibroids in the uterus or lipomas are examples of benign tumors. This technique is non parametric since there are no assumptions for the distribution of underlying data and it is lazy since it does not need any training data point model generation. The assumption made here is that the presence of one particular feature does not affect the other the predictors features are independent. IMPORTING LIBRARIES Exploratory Data Analysis DIAGNOSIS Explanation Diagnosis of prostate cancer is based on Type of tumour found in the medical test. Specify the inverse of the regularization strength in 10. This makes the training faster and the test phase slower and more costlier. sg images default source default album surgery prostate cancer. Distribution of diagnosis 68 benign 38 malignant with Cancer VISUALIZATIONAs you can see above we obtain the heatmap of correlation among the variables. Started from make an instance of the model setting the default values. Let m be the number of input variables to be used to determine the decision in a given node m M. For finding closest similar points find the distance between points using distance measures such as Euclidean distance Hamming distance Manhattan distance and Minkowski distance. Malignant Tumour Cancerous Malignant means that the tumor is made of cancer cells and it can invade nearby tissues. Symptoms include difficulty with urination but sometimes there are no symptoms at all. CANCER https www. read_csv Input data files are available in the read only. This technique uses the logistic function or sigmoid function which is an S shaped curve that can assume any real value number and assign it to a value between 0 and 1 but never exactly in those limits. Choose a training set for this tree and use the rest of the test cases to estimate the error. Some cancer cells can move into the bloodstream or lymph nodes where they can spread to other tissues within the body this is called metastasis. distance functions. Random forest classificationBased on the previous classification method random forest is a supervised learning algorithm that creates a forest randomly. The essential idea of bagging is to average many noisy but approximately impartial models and therefore reduce the variation. The color palette in the side represents the amount of correlation among the variables. The lighter shade represents a high correlation. Comparison of classification techniques ROC curve From a Comparison of classification techniques we plotting ROC to illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. A man s prostate produces the seminal fluid that nourishes and transports sperm. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. This process is iterated by all the trees in the assembly and the label that gets the most incidents is reported as the prediction. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session Subsetting the data Calculate the correlation of the above variables Plot the correlation as heat map We defining the model We train the model We predict target values The confusion matrix Test score We define the SVM model We train model We predict target values The confusion matrix Test score We define the model We train model We predict target values The confusion matrix Test score We define the model We train model We predict target values We define the model We train model We predict target values Test score We define the model We train model We predict target values The confusion matrix Test score We define the model We train model We predict target values The confusion matrix Test score Logistic Regression Classification SVM Classification Naive Bayes Classification Decision Tree Classification Random Forest Classification KNN Classification. Thus the model is a linear combination of the inputs but that this linear combination relates to the log odds of the default class. What is Prostate cancer Prostate cancer is a form of cancer that develops in the prostate gland. Here Y is the evidence and X is the hypothesis. Hence it is called naive. Now For Conveinent Purpose Diagnosis Result in the Table Can be Changed to following M 1 Which Indicates diagnosis of Prostate Cancer B 0 which Indicates patient doesn t have Prostate Cancer and not Harmful. The decision tree analyzes a set of data to construct a set of rules or questions which are used to predict a class i. This forest is a set of decision trees most of the times trained with the bagging method. Each tree is constructed using the following algorithm Let N be the number of test cases M is the number of variables in the classifier. For prediction a new case is pushed down the tree. The only information available is a training data set D xi yi X Y i 1 l where l stands for the number of the training data pairs and is therefore equal to the size of the training data set D additionally yi is denoted as di where d stands for a desired target value. In this case we will assume that we assume the values are sampled from a Gaussian distribution and therefore we consider a Gaussian Naive Bayes. Then it is assigned the label of the terminal node where it ends. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. Beningn Tumour Noncancerous If the cells are not cancerous the tumor is benign. A benign tumor is less worrisome unless it is pressing on nearby tissues nerves or blood vessels and causing damage. Thus logistic regression models the probability of the default class the probability that an input X belongs to the default class Y 1 P X P Y 1 X. The learning problem setting for SVMs corresponds to a some unknown and nonlinear dependency mapping function y f x between some high dimensional input vector x and scalar output y. It won t invade nearby tissues or spread to other areas of the body metastasize. Calculate the best partition of the training set from the m variables. Thus hyperplanes are decision boundaries that help classify the data points. For this study using a sample size of 20 assumed it ideal ratio between training and testing 1. Trained the logistic regression model with the training data and then applied such model to the test data. Thus using Bayes theorem P X Y P Y X P X P Y we can find the probability of X happening given that Y has occurred. the goal of decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Data for training and testing To select a set of training data that will be input in the Machine Learning algorithm to ensure that the classification algorithm training can be generalized well to new data. jpg sfvrsn e70a871e_4 In this study we tried to predict Prostate Cancer using 6 different algorithm Logistic regression classification SVM Support Vector Machine classification Naive bayes classification Decision tree classification Random forest classification K Nearest Neighbor classification Predictor variable use in classifying Prostate and its features radius mean of distances from center to points on the perimeter texture standard deviation of gray scale values perimeter area smoothness local variation in radius lengths We measure the study of 7 different algorithms using a confusion matrix. ", "id": "imkrkannan/prostate-cancer-prediction-using-ml-algorithms", "size": "8666", "language": "python", "html_url": "https://www.kaggle.com/code/imkrkannan/prostate-cancer-prediction-using-ml-algorithms", "git_url": "https://www.kaggle.com/code/imkrkannan/prostate-cancer-prediction-using-ml-algorithms", "script": "OneVsRestClassifier train_test_split confusion_matrix sklearn.svm numpy seaborn preprocessing SVC BaggingClassifier GaussianNB sklearn.neighbors sklearn.naive_bayes sklearn.tree sklearn sklearn.linear_model matplotlib.pyplot MinMaxScaler sklearn.multiclass DecisionTreeClassifier sklearn.model_selection pandas roc_curve RandomForestClassifier LogisticRegression average_precision_score KNeighborsClassifier sklearn.metrics sklearn.ensemble sklearn.preprocessing ", "entities": "(('that', 'classification binary problems'), 'be') (('Hence SVMs', 'learning supervised techniques'), 'belong') (('lighter shade', 'high correlation'), 'represent') (('we', 'discrimination threshold'), 'technique') (('classification algorithm training', 'well new data'), 'datum') (('that', 'prediction'), 'iterate') (('study', 'confusion matrix'), 'sfvrsn') (('leaf node', 'outcome'), 'be') (('therefore we', 'Naive Gaussian Bayes'), 'assume') (('read_csv Input data files', 'read'), 'be') (('Support Vector Machine', 'last years'), 'classificationSVMs') (('Y', 'X'), 'use') (('It', 'kaggle python Docker image https github'), 'come') (('less it', 'blood damage'), 'be') (('which', 'class i.'), 'analyze') (('decision that', 'data points'), 'be') (('IMPORTING LIBRARIES Exploratory Data Analysis DIAGNOSIS Explanation Diagnosis', 'medical test'), 'base') (('forest', 'bagging method'), 'be') (('distribution therefore free learning', 'probability joint functions'), 'be') (('m', 'given node'), 'let') (('tumor', 'Tumour Noncancerous'), 'Beningn') (('predictors', 'other'), 'be') (('that', 'process'), 'select') (('it', 'training data point model generation'), 'be') (('that', 'seminal fluid'), 'produce') (('learning supervised that', 'forest'), 'classificationbase') (('X', 'default class'), 'model') (('this', 'body'), 'call') (('Benign tumors', 'surgery'), 'need') (('0 which', 'Prostate Cancer'), 'change') (('color palette', 'variables'), 'represent') (('bayes Naive classificationThe naive Bayesian classifier', 'features'), 'be') (('that', 'similarity measure e.'), 'classificationK') (('essential idea', 'approximately impartial therefore variation'), 'be') (('linear combination', 'default class'), 'be') (('instance', 'default values'), 'start') (('it', 'nearby tissues'), 'mean') (('that', 'data clearly points'), 'be') (('Symptoms', 'urination'), 'include') (('us', 'log odds'), 'in') (('where it', 'terminal node'), 'assign') (('that', 'data features'), 'be') (('above we', 'variables'), 'distribution') (('that', 'prostate gland'), 'be') (('M', 'classifier'), 'construct') (('that', 'seminal fluid'), 'prostate') (('S shaped that', 'exactly limits'), 'use') (('We', '100'), 'define') (('sg', 'default source default album surgery prostate cancer'), 'image') (('where d', 'target desired value'), 'be') (('usually odd number', 'classes'), 'be') (('It', 'body metastasize'), 'win') (('Data points', 'different classes'), 'attribute') (('1 Fibroids', 'benign tumors'), 'be') (('new case', 'tree'), 'push') (('We', 'confusion matrix Test score Logistic Regression Classification SVM Naive Classification Bayes'), 'list') "}