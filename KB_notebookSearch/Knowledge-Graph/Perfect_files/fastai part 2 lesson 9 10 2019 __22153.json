{"name": "fastai part 2 lesson 9 10 2019 ", "full_name": " h2 This is last part of fastai part 1 lesson 9 though it is mixed with my notes eksperiments and what I found usefull if you want the pure version check fastai github or the following link https github com fastai course v3 tree master nbs dl2 h1 Initial setup h2 data h1 Cross entropy loss h1 Basic training loop h1 Using parameters and optim h1 Registering modules h1 nn ModuleList h1 nn Sequential h1 optim h1 Dataset and DataLoader h2 Dataset h1 DataLoader h1 Random sampling h1 PyTorch DataLoader h1 Validation h1 Part 2 04 callbacks h1 CallbackHandler h1 Runner h2 Third callback how to compute metrics h1 Part 3 05 anneal ipynb h1 Initial setup h1 Annealing h1 Part 4 05b early stopping h2 Early stopping h3 Better callback cancellation h3 Other callbacks h3 LR Finder h1 Part 5 06 cuda cnn hooks init h2 ConvNet h2 CUDA h2 Refactor model from above h1 Hooks h2 Manual insertion h2 Pytorch hooks h2 Hook class h2 Other statistics h2 Generalized ReLU h2 Uniform init may provide more useful initial weights normal distribution puts a lot of them at 0 h1 Part 6 07 batchnorm h1 ConvNet h1 Batchnorm h2 Custom h1 Builtin batchnorm from pythorch h1 With scheduler h1 More norms h2 Layer norm h2 Instance norm h2 Group norm h2 Fix small batch sizes h2 Running Batch Norm h3 What can we do in a single epoch ", "stargazers_count": 0, "forks_count": 0, "description": "the fist layer have a std not to far away from 1 but as we whould espect it gets eksponential worse and the last layer in this group is almost 0 which we dont want. With the AdaptiveAvgPool this model can now work on any size input image. note we do not use model. so lets try to fix it Generalized ReLUNow let s use our model with a generalized ReLU that can be shifted and with maximum value. n and now we dot hee same as above lets take 10 values from training set here we use shuffling where we set it to False so we dont shuffle and we see that it is ordered here we use shuffling where we set it to True so we do shuffle and we see that it is not ordered create tensorser grap the x and y s and stack them up. _modules with a list of all the modules l1 and l2 in this chase __setattr__ goes into self. but it is clear that the first pich leaves out model in a very sad place and we clealy see the result of it just look realy bad see the first 10 means here we see the means getting close or are realy close the 0 which is what we want see the std for the fiirst 10 batches we see a problem. It will behave a bit like a numpy array in the sense that we can index into it via a single index a slice like 1 5 a list of indices a mask of indices True False False True. pass it through our first layer f our model. And note that the learner has no logi itself now lets create the learn veriable where we pass in the functions from above training loop it is the same as before thought we replace model. so to fix this we do random sampling Random samplingWe want our training set to be in a random order and that order should differ each iteration. mom in the updaa_stats function above. lets look inside the model and try to get a higher acc. log because in Pytorch the negative log likelyhood expect a log softmax not just a softmax that start looking at our depended veriable and lets just look at the first 3 values so now we want find the proberbillety accosiated with 5 the proberbillety accosiated with 0 and the proberbillety accosiated with 4 this is how we index into one prediction this is how we index into the 3 numbers a list of dimensions 0 1 2 5 0 4 and we have 2 dimensions next is all he row indexses we want 0 1 2 and the 5 0 4 is a list of all he columns indexses we want. 15 to 1 which is much better though do note that the minibach is from the training set that we also do the prediction on so it dosent mean to much but we can see that our model is learning something so this is good. so we are now using nearly all of our activations by being cearful about out initialisation and our relu Uniform init may provide more useful initial weights normal distribution puts a lot of them at 0. Here we store the mean and std of the output in the correct position of our list. We will calculate and print the validation loss at the end of each epoch. The idea is to use the following formula log left sum_ j 1 n e x_ j right log left e a sum_ j 1 n e x_ j a right a log left sum_ j 1 n e x_ j a right where a is the maximum of the x_ j. zero_grad PyTorch already provides this exact functionality in optim. bs taking one batch size bs at the time and each time we go through we will yield our dataset at a index starting at i til i bacth size bs yield means it like a function that dosent return one thing ones. so it stops since our loss got much worse then the best worse i the graf aboveso we can se on the graf below that we dont hit the 100 iterations Part 5 06_cuda_cnn_hooks_init ConvNetHelper function to quickly normalize with the mean and standard deviation from our training set Let s check it behaved properly. Having given an __enter__ and __exit__ method to our Hooks class we can use it as a context manager. Because the data know how many activations it needs now lets store away the rest from the into above just storing them. in the callbacks defined in AvgStatsCallback. eval so the code rough dont do anything so it just says thise are the steps i need to run and the callbacks do the running. The __iter__ method is there to be able to do things like for x in. luckily there are another way image. but note we also have ListContainer of which we use and it does it. and because we used yield these sampler are only gonna be calulate when we call on them so we can use these on big dataset then by for i in s we are gonna grap all of the indexses in a given sample and we gonna grap the dataset at that index self. l1 dosent start with _ because if it does it maight be _modules or something in python. grad lr model. So this will end up returning 0 5 1 0 and 2 4 this is how we call on all the rows in our target negative log likelyhood input lets look into our predictions range target. we have though a problem with batchnorm since it cannot take batches of one or very small ones like 2 4 since at a point it will divide by itself and we will go to infinity. 5 and we can handle leaking leak. c is predefined since we from the beginning defined how many last activation layer need. we usally calulate the averge by taken the first 5 point and then the next 5 points and so on like the picture above. training same as before but where torch. And maybe we want a limit so we can use maximum value maxv so if you pass leak from GeneralRelu it will use leaky_relu otherwise we will use normal relu F. SGD it also handles stuff like momentum which we ll look at later except we ll be doing it in a more flexible way Randomized tests can be very useful. Then use PyTorch s implementation. First we can do it manually like this Now we can have a look at the means and stds of the activations at the beginning of training. soall we are doing is almost nothing since we are going from 9 to 8. This makes sure that onces we are out of the with block all the hooks have been removed and aren t there to pollute our memory. This is defined by hbox softmax x _ i frac e x_ i e x_ 0 e x_ 1 cdots e x_ n 1 or more concisely hbox softmax x _ i frac e x_ i sum_ 0 leq j leq n 1 e x_ j In practice we will need the log of the softmax when we calculate the loss. we will accumulate the statisstics so we need a class that does the accumulate which is defined above at the end of an epoch we will print out the statistics create our learner add our argstatscallback partial is a function that returns a function so we go in the AvgStatsCallback class and get the functions and pass it to the veriable now we can set it in our runner in the cb_funcs which are for new callbacks defoult a learing rate at 0. in other words we averge over all the batches 0 and we averge over all x 2 and y 3 koordinates. 2 cbfs callbacks functions and sched from above just a class CancelTrainException that inhaied from Exception class pass means that it will not get more opf\u00f8relser then the parant class except so if we call on CancelBatchException this will run. so now lets add callbacks CallbackHandlerThis was our training loop without validation from the previous notebook with the inner loop contents factored out Add callbacks so we can remove complexity from loop and make it flexible This is roughly how fastai does it now except that the handler can also change and return xb yb and loss. note here the training set is shuffled and the validation is not RandomSampler shuffle them as above SequentialSampler order it as above another way of doing the same as RandomSampler another way of doing the same as SequentialSampler same training loop as before Handle batchnorm dropout print model. so it will fire of more processes and each one wil seperated grap stuff from the dataset and then it will collate them afterwoodit usefull if you are opening big jpeg files and all kind of images transformations ValidationYou always should also have a validation set in order to identify if you are overfitting. count dona reprer so it prints it out gonna add up the total loss times the batch size count the size of the batch and for each matrix. This can be done using numpy style integer array indexing. com fastai course v3 tree master nbs dl2 Initial setup dataSo now we need a loss function Cross entropy lossFirst we will need to compute the softmax of our activations. so we use 5 or 7 to slide over the full matrix for the fist layer instead. stop defined in the CallBackHandler so lets test it this dictat what order your callbacks run in like this does that you can use self. its a Conv2d from the filter nfs i current filter to the next filter nfs i 1 and then the kernel size depends it 5 for the fist layer otherwise it is 3 this is explaned in the picture below last few layers are averge pulling AdaptiveAvgPool2d flattening and a linear layer we can now remove the mnist_view from the previus model since it is a callback. This is last part of fastai part 1 lesson 9 though it is mixed with my notes eksperiments and what I found usefull if you want the pure version check fastai github or the following link https github. Sequential is a convenient class which does the same as the above optimLet s replace our previous manually coded optimization step with torch. zero_grad and instead use just opt. E X 2 E X 2 This solves the small batch size issue What can we do in a single epoch Now let s see with a decent batch size what result we can get. we want to take the mean of that but in order to do that in pytorch we have to convert it to a float. 09 when we try to use in a model but when your doing RNN u have to use something like this same as layernorm but only using 2 3 and not 1 2 3. In order to combine the effects of instance specific normalization and batch normalization we propose to replace the latter by the instance normalization also known as contrast normalization layer label eq inorm y_ tijk frac x_ tijk mu_ ti sqrt sigma_ ti 2 epsilon quad mu_ ti frac 1 HW sum_ l 1 W sum_ m 1 H x_ tilm quad sigma_ ti 2 frac 1 HW sum_ l 1 W sum_ m 1 H x_ tilm mu_ ti 2. Shape Input N num_channels Output N num_channels same shape as input Examples input torch. GroupNorm 3 6 Separate 6 channels into 6 groups equivalent with InstanceNorm m nn. The mean and standard deviation are calculated separately over the each group. since we dont pass pos it just takes position a decorator is a function that returns a function the annealer decorator does the same as the partial shift tab works too in Jupyter sched_lin linear schedular and pass the learining as 1 to 2 and ask what should the learning rate be 30 through training note pos 0. and we dont want to save a history of all of them just to get the averge. And therefor have a combined class for both let define training and validation sets and store them away and thats it we make it optinal to pass in c and we pass None right now because we will use it later but for conviense let make it eseay to grap them from the class now lets use DataBunch and put all our data in a veriable called data and we pass in c which will be used later the input number is size of the training data data. So how does that look like and now since we have created it we can use pytorchs dataloader PyTorch DataLoaderNote that PyTorch s DataLoader if you pass num_workers will use multiple threads to call your Dataset. this gives us a new matrix that is 8 by 9. ds so loop through from 0 to the size of the dataset so from 0 til 50000. 3 no schedular return always start This monkey patch is there to be able to plot tensors dont think to much about the below code for the combined schedular we are gonna pass in the fases we want so fase one wil be a learning rate from 0. at the start of an epoch we will reset the statistics and after we got the loss calulated. so if you are training you model like this it might look like it is training nicely without you knowing that 90 procent o you activations are totaly tabt. so you are never gonna get great result by wheasting 90 procent of your actiovations. check if the key like self. since we created a validations set that we used in the fit and so we got 50 hidden layer and batch size on 64 we define c as the last layer of activattions we need. so there a 9 input activations to do out dot product with. now we can go and fit the model and we do 5 epochs and we just chech that the acc. Pytorch hooksHooks are PyTorch object you can add to any nn. its gonna add up the total matrixces times the batch size we aare gonna use some object to keep track of matricses ans losses one for training and one for valid. so after each module it detaltes and call def remove goes through each layer and removes each registed hook. relu x if you want to subrat something go do that if you want to use maksimum value go do that uniform boolien so some people say uniform is better then normal so it is an optinal see buttom of this part when used since our relu now can be negative since we can subrat and use leak argurments in GeneralRelu we set in 7 note we now use the middel two histrogram bins instead of the first to since we can take negative numbers so now lets use shedular look longer up in this notebook if you dont rember what it is grap our data as before same as before learning rate 0. the name in self doesnt matter except so if we call on CancelBatchException this will run. hook when we are done using a hook module we should call hook. we don t keep a moving average2. for the model there are only to things the parameters the things we are updating and the activations the things we calculate and the thing we want on the GPU is the parameters and inputs Now that s definitely faster Refactor model from aboveFirst we can regroup all the conv relu in a single function Another thing is that we can do the mnist resize in a batch transform that we can do with a Callback. png the the picture above is the first layer. Args num_groups int number of groups to separate the channels intonum_channels int number of channels expected in inputeps a value added to the denominator for numerical stability. so by makking eps bigger we make sure we get further and further away for our fomular to crachs. ds i so now we got a list of tensors and we need a way to collate them together into a singel per of tensors and the collate function above does this now we can create our to samplers now we can create our to dataloaders. register_buffer vars since when train on the GPU everything called buffer will be aoutomatic places there aswell start the means on zero tensor and variance on ones tensor caluelate teh mean note 0 2 3 means we averge our these axsis 0 2 and 3. png picture from article about batchnorm. It returns a function that takes a pos argument going from 0 to 1 such that this function goes from start at pos 0 to end at pos 1 in a linear fashion. which is our maxisinum y value lets create a class that put together training set and validation set. doesnt realy work for classification batch size 2 note we get an acc. note this is an explanation to the code above the fist filter is 8 and thereby we make a 3 by 3 matrix to slide over the full matrix. our problem is though that our training loop always loop through our training set in order. we don t average over the batches dimension but over the hidden dimension so it s independent of the batch sizeThought experiment can this distinguish foggy days from sunny days assuming you re using it before the first conv Instance normFrom the paper The key difference between contrast and batch normalization is that the latter applies the normalization to a whole batch of images instead for single ones label eq bnorm y_ tijk frac x_ tijk mu_ i sqrt sigma_i 2 epsilon quad mu_i frac 1 HWT sum_ t 1 T sum_ l 1 W sum_ m 1 H x_ tilm quad sigma_i 2 frac 1 HWT sum_ t 1 T sum_ l 1 W sum_ m 1 H x_ tilm mu_i 2. Question why can t this classify anything Lost in all those norms The authors from the group norm paper have you covered image. but still there are a better solution and its shown in the code below which is running batch norm Running Batch NormTo solve this problem we introduce a Running BatchNorm that uses smoother running mean and variance for the mean and std. 2 still following a cosine. But let s see if we can make things simpler and more flexible so that a single class has access to everything and can change anything at any time. Note that the formula log left frac a b right log a log b gives a simplification when we compute the log softmax which was previously defined as x. So we can use it for our log_softmax function. Question Are these validation results correct if batch size varies get_dls returns dataloaders for the training and validation sets Answer it is an incorrect way it is done since if we try to devide with a batch size of first a 10000 and then 1 we get differeent outcomes. but to realy know whats going on we have to see in the activations. in_train it will append the current learning rate since there are many learning rates we just take the last one therefor we use 1 in param_groups and lr to get the learning rate and append the current loss then we tell it to plot the learning rates and plot the losses it is a good idea to schadule all the things we want to apply on the model like dropouts momentun and learning ratess since we want to change them as the model improves pass in a sched_func and a parameter pname to shedule param_groups same as layer groups in fastai so it group different layers together so they can get a different learning rate the sched_func takes a singel argument which number of epochs divided by total epochs note this will be a float and the result of this will set the hyper parameter pg self. to realy be sure we see the next grafand look at that we now see that in the last layer is lost less then 20 procent of the activations. note we use the outp since it is the last layer we wnat for each layer to calulate the std for the last layer or any module m register forward hook register_forward_hook and pass in a function partial append_stats i which is a call back which will be called when the m. and which we can transform to the 8 1. to about 20 because if the small batch size SO THERE ARE TREE CONCEPT TO THIS IDEA 1. nb_01 import since we know about boardcasting we make a funktion that our tensor x subrat with the mean m and divided with standard diviation s export test if the floats are neer since we cant just compare floats with eachother we made i in last lesson make the model this is the None 1 keepdim True it is to make sure the result will broadcast correctly against x. Dropout to ensure appropriate behaviour for these different phases. NB if you use a Lambda layer with a lambda function your model won t pickle so you won t be able to save it with PyTorch. pname in this chase learing rate so if we are trianing in_train we will run our scheduler and set parameters set_param for learning rate in differetn layer groups for linear schedule starting learning rate to end learning like from 1e 01 to 1e 10 now return the learning rate so the start position time the different in the end and start so now to only get posistion pos we return partial and pass the function _inner start and end. has been calulated we can also use register_backward_hook which can be used after the backward are calulated note we use i to nkow which layer we are at. plot means plot stds so what we can do is to modufy our append_stats so it also got a histogram histc of our activations histc isn t implemented on the GPU pop them into 40 bins between 0 and 10 note there are no negatives becouse of Relu same as before look up for detais same same Thanks to ste for initial version of histgram plotting code note that most of the histogram is where the yellow is and that is what we realy care about to see how much ylleo there are lets take the first 2 histragrams bins are near 0 so lets get the sum of those two divided byt he sum of all if the bins this gonna tell us what procent of the activations are near or close to 0. the first 4 layers and it is bautiful where we see that it doesnt collapse so it should use the fullnes of the activations. 4 NB pytorch bn mom is opposite of what you d expect note mults are a parameter note we are multiply by tensor of 1 in the last line in forward function which does nothing note adds are a parameter note we are add by a tensor of 0 in the last line in the forward function which does nothing but the are parameters so they can learn they are being updated and note that adds is the same as bias in a linear model. A hook is attached to a layer and needs to have a function that takes three arguments module input output. train so we call model. the name in self doesnt matter raise CancelTrainException so we go back and activiate it in Runner class using an exponential curve to set the learning rate current iteration divided by the maxisimum iteration find learning rate set the leaerning rate in parameters same as we did in run scedular above after each step check if we have done more then maximum iterations 100 and if the loss is much worse then the best we have had this far if either of those happens raise CancelTrainException and stop training check if the current loss is better then the best loss and if it is set the current loss to the best loss the only thing it does is 7 4 2 averge pooling Somewhat more flexible way we create a device be calling torch. Here is an example use 30 of the budget to go from 0. for this will be practising godd anniling AnnealingWe define two new callbacks the Recorder to save track of the loss and our scheduled learning rate and a ParamScheduler that can schedule any hyperparameter as long as it s registered in the state_dict of the optimizer. this happens since the gradient was so fast that it just falling of a clift klippe and having to star agian this we will ty to fix you can see hooks as the same as hooks create global verible for list for every layer create global verible for list for every layer create a function to use for register_forward_hook and if we look at the docs for it it takes 3 things the modeule that do the callback mod the input to the module inp and the output to the module outp note i is the layer number we are using to caluelate the mean of the last layer. To refactor layers it s useful to have a Lambda layer that can take a basic function and convert it to a layer you can put in nn. remove function defined here. The cross entropy loss for some target x and some prediction p x is given by sum x log p x But since our x s are 1 hot encoded this can be rewritten as log p_ i where i is the index of the desired target. Basic callbacks from the previous notebook CUDAThis took a long time to run so it s time to use a GPU. Part 4 05b_early_stopping A hidden layer in an artificial neural network is a layer in between input layers and output layers where artificial neurons take in a set of weighted inputs and produce an output through an activation function Early stopping Better callback cancellationso the code is almost the same as before but with a few different things se the comment Other callbacks LR FinderNB You may want to also add something that saves the model before running this and loads it back after running otherwise you ll lose your weights NB In fastai we also use exponential smoothing on the loss. A hook will be called when a layer it is registered to is executed during the forward pass forward hook or the backward pass backward hook. we use the formular E X 2 E X 2 to get the varaince 2. now we can just use the ListContainer here to get teh features we want and this means also we now have hooks in it so now we can just go for hooks in each layer m in children of our model ms. sub_ means 2 note fomular is used in chase we get the first minibatch that is close to 0 and we dont want that to destoy everything so if you having seem more them 20 items yet just clamp the variance to be no smaller then 0. stack just take a bunch of tensor and grue them together we created samplers before so now are gonna use them. since it s equal to its mean. We ll also re create our TestCallbackThird callback how to compute metrics. used in below code but its what happens when you starts the with block defined longer done used in below code when you finish the with block but when called on it will use. Bot we see a lot better learning of the fist 10 layers. A simple Callback can make sure the model inputs and targets are all on the same device. sqrt so if we just set eps to 0. sum 1 keepdim True. on about 94 and this what we will try to improve this by batchnorm Batchnorm CustomLet s start by building our own BatchNorm layer from scratch. log just takes the log then sum then the exponeltial so in Pytorch we have LogSumExp which we implement as so and the reason we need this trick is because when we take somthe to the power of x. And here are other scheduler functions this above graf shows the four different schedulars but this enought since we want to get first a higher leairning rate and then a lower oone se grafs a the bottum. In PyTorch this is already implemented for us. now we can call fit and we will see the same gra as before where the layers means build up and then collapses when we initize it register a forward hook and some function called f we also use self. From the histograms we can easily get more informations like the min or max of the activationsand we see in the last layer that over 90 procent of the activation actual zero. so now we will try to fix it More norms Layer normFrom the paper batch normalization cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small. NB In fastai we use a bool param to choose whether to make it a forward or backward hook. We will also use it in the next notebook as a container for our objects in the data block API. note we did it after the keiming inisilatation of the first layer so now we dont see the first pich where everything went to shit before. So all we are left with is a mean for each chanel filter keepdim True means its gonna leave an empty unit axisis in position 0 2 3 so it will still boardcast nicely calulate the veriance var save the mean of the last few running batches deeper explornation for lerp_ see pictures below mom momentum save the veriance an of the last few running batches we save those since we doing training chould lose some importen information so we just save the lave few runnings the be save we get the means and veriance from update_stats function above subrat by the mean and divid by the kvadratrod of the veriance and now we multiply mults by the x and add it with adds note we are usin the bottum formular in the picture above create new conv layer note bn BatchNorm layer No bias needed if using bn if we do use batchnorm we dont need bias since adds is a bias uses the class batchnorm above same as before so layernorm is almost the same as batchnorm just without the running averge so instead we make an averge on one image and not call of them though it doesnt realy work this can be seem by the acc. because there aren t many samples the variance of one thing is 0. Here the class above TrainEvalCallback use the n_iters in the after_batch function note True is returned so it stops cb_funcs None any callback functions you pass in that is not being defined here new callbacks assign the new callbacks to a attribute with that name. export from exp. train before training and model. 6 following a cosine then the last 70 of the budget to go from 0. tHis givea an out of 9. which means we can acces it in append_stats note self. Here we store it in opt and now we can use it and here we can see that we get a acc at 0. in other words if we get the first batches right the model will get a much better scc score. Default 1e 5affine a boolean value that when set to True this module has learnable per channel affine parameters initialized to ones for weights and zeros for biases. GroupNorm 1 6 Activating the module output m input Fix small batch sizesWhat s the problem When we compute the statistics mean and std for a BatchNorm Layer on a small batch it is possible that we get a standard deviation very close to 0. when usin RNN batch can also be a problem. this is now a generaric get cnn model function that return s sequential model with some arbitory set of layers and some arbitory layers nfs lets put the model and the layers intot he function below grap optmisation function grap the optimazer grap our learner note we see both our filters 8 16 32 32 and out kernel size where the first sis 5 and the for the rest it is 3 veiw model train model new model class make a list to use for act_means for _ in layers means a list for every layer make a list to use for act_stds for _ in layers means a list for every layer we have built a SequentialModel before so we will reuse the first to lines go through each layer set x to the current layer of x graps the mean of the output and put it in act_means grap the std of the output and put it in act_stds makae model learning model fit model so the means for every layers hit a pich in the beginning. _modules k v from he code above it would be esayer of pytorch did it for us so the nn. __setattr__ and move relu to functional Registering modulesWe can use the original layers approach but we have to register the modules. 3 insted create a function so it is easayer to change the learning rate Recorder records the learning rate given in the hyper parameters given in the below code set the learning rates and losses to emty at the beginning so after each batch as long as we are training will return False if training and True if note training there we use if not self. Hook classWe can refactor this in a Hook class. 1 we can never divide by zero even though v variance is 0. to is from pytorch and it means that it moves some parameters to a device remember that run is a class longer up in the code and xb and yb are defined also but ere we just move them to the device another way of doing this is use pytorchs version but here we use only one device Somewhat less flexible but quite convenient note we dont have to move it to a device we just use. modules so we just go through all of the layers self. We can use it to write a Hooks class that contains several hooks. so this means that our final layer gets alomost no activations and basic no gradiants. but what we do instead is to take each current point multiply it with a momentum mom and add it to the secound point multiplied the 1 mom 1 momentum and therefor we just have one point all the time and we can forget about the rest. add_module is the same as self. log_softmax and we can see it is the same as our loss so we make a function that can find our accuracy we did in part 1 by hand but we use argmax here that means that we take the highest number in out softmax and the index of that is out prediction and we check if that is to the accuracal yb. for s in sampler means that we are gonna loop through the samples in above like 6 1 7 8 2 9 and so on. since most of them culd have a zero gradient that this point. So it s best to give a name to the function you re using inside your Lambda like flatten below. the batch size can be different from minibatch to minibatch 3. so we combine the different schedularsIn practice we ll often want to combine different schedulers the following function does that it uses scheds i for pcts i of the training. parameters for hooks soeverything in out model with the append_stats print it out to see all the hooks grap a batch of data chech the one batchs of datas mean and std and it is about mean 0 and std about 1 whcich is what we want. training print model. randn 20 6 10 10 Separate 6 channels into 3 groups m nn. Pytorch also have a class that does the same as DummyModule defined below super setup the _modules in the DummyModule class. train every time we have data. But here we just see it as a very good hyperparameter that we should use. device 0 is the GPU number u wonna use if you only have one GPU it is device 0 initiate a model move the model to the device. SGD which does the same as our Optimazer in this chase lr 0. 5 becouse of the Relu and std is very close to 1 do our hooks note that we use a with block which means make this object Hooks model append_stats and give it this name hooks and when it is finishs it will call something which in our case is __exit__ from Hooks class above do a fit with 2 epochs do our first 10 mean. In the above version we re only supporting forward hooks. General equation for a norm layer with learnable affine y frac x mathrm E x sqrt mathrm Var x epsilon gamma beta The difference with BatchNorm is1. gamma and beta are learnable per channel affine transform parameter vectorss of size num_channels if affine is True. bool mask note ater a 100 it is no longer printing them all just 9. note that the mean starts on 0 and the std starts on 1 and note also that the training has gotten rid of the pich and collapses from before Builtin batchnorm from pythorch With schedulerNow let s add the usual warm up annealing. 5 its defined above check the loss function training loop we just test if the acc is better then 70 we create places for x and y take the lenth of x donda getitem means that when we index into len it returns x i and y i creating a validation and training dataset check that the lenth is right check that the lenth is right check the first 5 values make sure the shape is right make sure the shape is right printing values creating model and we can set them in model and opt since those are the things we return in get_model class creating trainig loop and now we have replaced the to lines of code with one checking the acc score takes a dataset ds and a batch size bs and stores them away iterate through for i in range 0 len self. see it like a subroutine now we can create aa training dataloader and a validation one iter create the co routine and next grap the next thing yielded out of that co routine check shape check shape now we cant fit the model or in other words its a traiing loop go through each epoch go through each batch in the independed and depended veriables in train data calulate the preditiction calulate the gradients update with a learning rate reset the gradients samler class so we take in the dataset ds and the batch size bs and set the shuffle to False and then we store them away but here note we dont store the dataset away just the lenth of the dataset so we know how many items to sample so remember this is the one we can call the next on a lot of times so iif we are shuffling if self. note model 0 mean the first means the first layer note our mean is 0 and the std isnt 1 so now we just go ahead and inisiate it with keiming and other that our mean is very close to 0. ModuleList goes through all the layers and even this can be written esayer so this does the same as SequentialModel class we wrote above pass in some parameter params and a learning rate lr and store those away making step we use no_grad since this is not a part of gradient calulation but just the result from it go through each parameter and update it with the gradient time the learning rate for all of the parameters making zero_grad going through the list of parameters you asked to update and zero those gradients creating model call optimizer class and pass the models paramters. shuffle then let us take a random permutation from n til minus 1 and if we are ot shuffling then lets grap the number in order torch. Part 3 05_anneal. so the mean get eksponential bigger to they suddenly colaps other that it get slidtly better but not much. This way we will avoid an overflow when taking the exponential of a big activation. Let s start with a simple linear schedule going from start to end. The fact that we re passing cb to so many functions is a strong hint they should all be in the same class Runnernew better version of the above code This first callback is reponsible to switch the model back and forth in training or validation mode as well as maintaining a count of the iterations or the percentage of iterations ellapsed in the epoch. png no memory for more pictures so that. but since every singel activation has one there can be hundred of millions of activations. _modules k v we put this in our _modules dirsonary and call it k do whatever the superclass does and here the superclass is an object defalt mode when it isnt defined printing the list if called apon the class now we can define a method called parameters go through everything in _modules in tthis chase is it l1 and l2 and then go through all of their parameters making it a list of layers pytorch are only going to accept something it accept as proper nn. So this is being done in TrainEvalCallback class above keep track of the numbers of epochs we are doing and kepping track of the learner and note that the learner has not logic in it now we tell each callback what runner they are working with call begin fit go through each epoch set the epoch and call all batches if not means here to keep going because it return False and for no_grad we use begin_validate call all batches if not means here to keep going because it return False it will first stop when and only when True is returned last thing we call after_epoch and after fit to get rid of repetations from CallBackHandler we use __Call__ which note let us tried a object as if it was a function after self after_epoch we are going through all out callbacks getattr get attribute means look inside this object cb and try to find something of the this name cb_name and if you cant find it defound it to None and if it can find cb_name like begin_validate then you can call it note True is returned so it stops and los and metrixces to give all statistic making a property that goes through losses and matrixces and give us the avgerge gennemsnitlige o self. register a buffer for sum reister a buffer for kvadratrod so check for minibatches we make a register for counts we do the sum over 0 2 3 dimensions dims and we do x x. this gives us a eksponential aftagende function and all of this is called linear interpolation and in code is lerp_ We can then use it in training and see how it helps keep the activations means to 0 and the std to 1. sum so 2 note formular is used so we have to devide the total number in the minibatch numel byt the number of chanels nc and we take the lerp_ the eksponential moving averga of the sums and we take the lerp_ the eksponential moving averga of the sqrs and take a eksponential moving averge of the counts of the batch sizes in the forward function we dont divide by the batchs std and we dont subrat the batch mean but use the moving averge statistic at training time aswell note we divide by the dbias amount and for the variance sqrs divided by counts minus. A Hooks classLet s design our own class that can contain a list of objects. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link. 5 see code above in _04 try a learning rate at 0. nb_02 import from exp. so to see if something is bad we will see in the activaitions and see how many of them are realy realy small and how well is it realy getting everything activated Other statisticsLet s store more than the means and stds and plot histograms of our activations now. Note also here that we only add a little bit of the secound point each time. ModuleList does this for us. 9375 which is okay and now since have impimentet the optimazer class we can just use pytorchs optim. And this helper function will quickly give us everything needed to run the training. step here we can see how they implemented it thoug note that they alson have momentum and weight decay and more of which we will implement later greate a get model where we create the model with optim. But most people do it this wayNow our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code Part 2 04_callbacks Factor out the connected pieces of info out of the fit argument list fit epochs model loss_func opt train_dl valid_dl Let s replace it with something that looks like this fit 1 learn This will allow us to tweak what s happening inside the training loop in other places of the code because the Learner object will be mutable so changing any of its attribute elsewhere will be seen in our training loop. This one takes the flat vector of size bs x 784 and puts it back as a batch of images of 28 by 28 pixels We can now define a simple CNN. parameters from the DummyModule below so we start by defineing tha every time we define a veriable like l1 or l2 in this chase to a linear so we now want to update the self. we are gonna try a few things to fix the above problem note we use kwargs since we can get extra arguments from GeneralRelu class note we use kwargs since we can get extra arguments from GeneralRelu class note we use kwargs since we can get extra arguments from GeneralRelu class note we use kwargs since we can get extra arguments from GeneralRelu class better Relu so now we can use subret from the relu which we found was good about 0. train the rest is the same as before here means cb CallBack so all_batches go through every batch the trainig loopp has to loop through every batch and the validation loop has to go throguh every batch these are all the CallBack and for the callback calls we need a callback handler go through the call back and se if we resive a False it means dont keep going any more and then return it we do this for all the callbacks this is just an exsemple of a callback it will use the fit callback set the iterations to 0 and then after every step it will set number of iterations to 1 and print it out and if we get pass 10 iterations we tell the learner to stop here we use the callbacj learn. So it will use the Callback class above and then use the function called name and use this. eval before inference because these are used by layers such as nn. We can refactor this with a decorator. so this is why we need it this does the updates and the parameters use use the function. nll_loss are combined in one optimized function F. Note that we always call model. shape 0 and for our row indexses it is every singel row we takes so from 0 50000 and target is each column we then take the mean to it all the reason we use input is because it is the negative log likelyhood now we can find the loss by taking the negative log likelyhood with the accatual prediction on the target note we can rewrite the softmax function with the knowledge of reforming the log a b to log a log b and we can see they are the same so the to formulars does the same we can see from above that the code x. cuda add it to our callback functions grap our model since we use relu and conv2 alot we just make a function for it kernel size 3 ks note this ans stride on 2 are defoults previus when we wrote the transforming of mnist it will only work with it but here we will make a more generel transformation of the data transform the indepened verible X training data for a batch you pass it som etransformation function tfm which its stores away begin batch just replaces the current batch xb with the result of the transformation tfm view_tfm takes and transform the size just to view something at 1 by 28 28 now we can simply append anotheer callback hte numbers of filters per layer now we make a model again the first few layers are for everyone of the filters. Hooks don t require us to rewrite the model. png attachment image. other then that we use. since we cant take the mean to an int in pytorch lets check the accuracy function so we grap our first batch so minibatch batch size a mini batch from x predictions now we can take the same size of the dependents veriables and calulate our loss now we can calulate our accuracy and it is so low because we havent trained our model yet so this is just a random answer so now we will train it learning rate how many epochs to train for training loop lets go through i up to n 1 where n is 50000 number of rows in training data we divide it with bs because we wanna do a batch at the time set_trace then we grap everything i times the batch the start of the minibatcj and ending at the start_i the bacth the end of the minibatcj so the to lines above is our first i th minibatch then lets grap one x minibatch one y minibatch and let pass that through out loss fucntion then lets do backward then we do our update and we use no_grad since this is not a part of gradient calulation but just the result from it now we have to update the model so we use only layers that have paramenters weight in them. note we can say for h in self even thogh h is not in a iteration list in this cell block. but insted of that we if the averge moving is 1 we divide it by the momentum and the secound time we take two points divided it by momentum 2 and so on. But it can return a lot of things and you can ask for it a lots of times. sothe consurn is that there are a lot of parameters and we dont know if all of the layers get better or just some of them. this will be explained in more detail in a later. png Group normFrom the PyTorch docs GroupNorm num_groups num_channels eps 1e 5 affine True The input channels are separated into num_groups groups each containing num_channels num_groups channels. GroupNorm 6 6 Put all 6 channels into a single group equivalent with LayerNorm m nn. is above 90 and the above is actuel acc. dbiasing we want to make sure at each point that no observation is rated to highly and the normal problem with moving averges is that the first point gets far to much weight because it appers in the firsst moving averge and secound and the third and the fourth so like before when we toke the curretn point multiplied with the momentum added with the next point minus the momentum. Hooks Manual insertionLet s say we want to do some telemetry and want the mean and standard deviation of each activations in the model. But the validation set shouldn t be randomized. It s very important to remove the hooks when they are deleted otherwise there will be references kept and the memory won t be properly released when your model is deleted. log Then there is a way to compute the log of the sum of exponentials in a more stable way called the LogSumExp trick. This means also we have four tensors to deal with we are gonna go through all of our layers we do a check to make sure the layer have weight in it update by taking the the gradient to the weight times the learning rate update by taking the the gradient to the bias times the learning rate now we zero those gradients so they are ready to next run the next thing we will do is to make the updata part better and use less code and now we can see the accuracy improved from 0. ipynb Initial setuphere we will se that the first batches is every. eval so the n_iters which will stop it when we get to 10 iterations wil have to stop. Dataset and DataLoader DatasetIt s clunky to iterate through minibatches of x and y values separately xb x_train start_i end_i yb y_train start_i end_i Instead let s do these two steps together by introducing a Dataset class xb yb train_ds i bs i bs bs DataLoaderPreviously our loop iterated over batches xb yb like this for i in range n 1 bs 1 xb yb train_ds i bs i bs bs. and so we loss the randomness of sjoveling it each time. lets try to use batchnorm on a small batch sizeso to fix this is to use epsilon eps which just is a very small number u define its normally used to avoid float roundings and we see it typically in the bottum at a division. note this is our pre batchnorm we can now see the acc. remove since otherwise if we keep registering more hooks on the same module they are all gonna get called and eventually you are gonna run out of memory note we do this pythosch will outomatically call __del__ for delate when it is done with a module which in return will remove the hook note hook so in our hook we can pop in our to list to store away our mean and std name the to list one for means and one for stds and now we can just append the means for each layer and now we can just append the stds for each layer so now we can just go for hook layer ind children of our model and we are just gonna grap the first 4 layers since it its the conv2d layers that are the most interresting and not hte linear layers and we see it does the same thing as all the other grafs export just a list that contains a lot of usefull features not inparticular needed being clled when we are useing firkant parantesterne for correct operation usage bla bla bla. exp we can get exstreme big numbers and every big number we have as a float will be inacurrat but the compter think it is the same even though they can be 1000 or more apart lets find the maxisimum lets subrat it from all of w\u00f3ur x ses and then lets do the LogSumExp and in the end we add the maxsimum number to the ligning so it gives the same number pytorchs LogSumExp compare the selfwritten log_softmax with pytorch this is called F. 6 and take up 30 procent of our bacthes and fase to will take 70 of our bathes and be a learning from 0. Part 6 07_batchnorm remember that there are to types of numbers in a neural network paramteres thing we learn and activations tings we calulate ConvNetLet s get the data and training interface from where we left in the last notebook. so optinen one is to use a higher epsilon value in BatchNorm because he formular is x m v self. train we replace it with learn. For that reason we check for best_loss 3 instead of best_loss 10. So we will start merging some of the parameters the things we want to keep is epochs the rests we want to put in one. This layer uses statistics computed from input data in both training and evaluation modes. Here Relu is an activation layer and have no parameters so we just have to linear layers in our difined model above. Let s make our loop much cleaner using a data loader for xb yb in train_dl. We will implement an other way later. cross_entrop and is F. So this is done because we a object that contains another object so in the runner object is there a object for callback name function takes the name on fx a class and make it lowerchase and put a _ as a seperation on the ords see eksampel below with function name camel2snake and then the Callbakc are removed keep track of how many epochs it has done and how many iterations it has done so we call model. no_grad for p in model. no_grad goes through the validation set note also that we dont use backward since we isnt training and the validation set insted we just keep track on the losses and track on the accuracy how big is our dataloader an dhow many batches are there and we devide the nv with the total loss since we dont have to do the backward pass so we have twice as much space therefor we can multipy the batch size with 2. Basic training loopBasically the training loop repeats over the following steps get the output of the model on a batch of inputs compare the output to the labels we have and compute a loss calculate the gradients of the loss with respect to every parameter of the model update said parameters with those gradients to make them a little bit better Using parameters and optimParametersUse nn. ", "id": "nickteim/fastai-part-2-lesson-9-10-2019", "size": "22153", "language": "python", "html_url": "https://www.kaggle.com/code/nickteim/fastai-part-2-lesson-9-10-2019", "git_url": "https://www.kaggle.com/code/nickteim/fastai-part-2-lesson-9-10-2019", "script": "accumulate exp.nb_01 TrainEvalCallback(Callback) sched_exp CancelTrainException(Exception) DataLoader Sampler() pathlib parameters model loss_func test_near after_fit step Path __iter__ combine_scheds Runner() append_stats begin_epoch conv2d collate get_cnn_layers matplotlib.pyplot test_near_zero conv_layer plot BatchNorm(nn.Module) partial CancelBatchException(Exception) BatchTransformXCallback(Callback) gzip avg_stats LayerNorm(nn.Module) normalize __enter__ InstanceNorm(nn.Module) torch conv_in init __setattr__ __getitem__ cos_1cycle_anneal AvgStats() torch.nn.functional Optimizer() name update_stats RunningBatchNorm(nn.Module) set_param IPython.core.debugger _inner all_stats begin_validate plot_loss do_stop SequentialSampler DataLoader() logsumexp get_learn_run view_tfm get_cnn_model torch.nn GeneralRelu(nn.Module) LR_Find(Callback) fastai flatten get_min Lambda(nn.Module) create_learner normalize_to get_model_func plot_lr get_model AvgStatsCallback(Callback) CudaCallback(Callback) after_backward valid_ds __delitem__ tensor Recorder(Callback) matplotlib DummyModule() __len__ camel2snake Hook() opt Dataset() math CallbackHandler() begin_fit sched_no CancelEpochException(Exception) all_batches TestCallback(Callback) functools sched_cos after_batch __exit__ __repr__ RandomSampler ListContainer() remove get_dls get_runner exp.nb_02 mnist_resize annealer typing after_loss test_eq Learner() __setitem__ set_trace Model(nn.Module) after_step torch.utils.data init_cnn begin_batch children after_epoch __call__ __del__ near data SequentialModel(nn.Module) train_ds one_batch Hooks(ListContainer) set_runner log_softmax init_cnn_ DataBunch() nn conv_rbn Callback() datasets pickle zero_grad accuracy conv_ln nll forward ParamScheduler(Callback) reset optim #and now since have impimentet the optimazer class we can just use pytorchs fit listify sched_lin get_data __init__ __getattr__ get_hist ", "entities": "(('2 we', 'acc'), 'realy') (('v even variance', 'zero'), 'divide') (('much cleaner', 'train_dl'), 'let') (('we', 'relu'), 'go') (('we', 'hook'), 'hook') (('we', 'E E X 2 2 varaince'), 'use') (('we', 'what'), 'print') (('thogh even h', 'cell block'), 'note') (('this', 'CancelBatchException'), 'matter') (('you', 'order'), 'fire') (('learning rate', 'training note pos'), 'take') (('time we', 'rest'), 'be') (('so this', 'something'), 'note') (('are', '0'), 'accumulate') (('everything', 'activations'), 'see') (('it', 'nn'), 'modules') (('90 procent', 'model this'), 'look') (('this', 'hyper parameter pg self'), 'in_train') (('nll_loss', 'one optimized function'), 'combine') (('then lower oone se', 'bottum'), 'be') (('We', 'data block API'), 'use') (('pre we', 'now acc'), 'note') (('so you', 'actiovations'), 'go') (('GroupNorm', 'LayerNorm m equivalent nn'), 'put') (('this', 'CancelBatchException'), 'mean') (('we', 'device'), 'be') (('so we', 'now self'), 'parameter') (('validation', 'shouldn t'), 'randomize') (('where i', 'desired target'), 'loss') (('This', 'style integer array numpy indexing'), 'do') (('he', 'BatchNorm'), 'be') (('we', 'activattions'), 'create') (('0 we', '2'), 'averge') (('so when we', 'momentum'), 'want') (('we', 'just averge'), 'want') (('right model', 'scc much better score'), 'get') (('it', 'registed hook'), 'go') (('that', 'arguments module input three output'), 'attach') (('needs', 'just them'), 'know') (('way we', 'big activation'), 'avoid') (('layer we', 'last layer'), 'happen') (('we', 'training'), 'go') (('singel activation', 'activations'), 'have') (('here we', 'callbacj'), 'be') (('we', 'mask indices'), 'behave') (('as long it', 'optimizer'), 'practise') (('DummyModule', 'DummyModule class'), 'have') (('s', 'end'), 'let') (('pure version', 'fastai github'), 'be') (('model inputs', 'same device'), 'make') (('this', 'pytorch'), 'get') (('5', '0'), 'see') (('Hook', 'Hook class'), 'refactor') (('more single class', 'time'), 'let') (('so we', 'it'), 'loss') (('we', 'self'), 'note') (('We', 'how metrics'), 'create') (('we', 'crachs'), 'make') (('we', '1 7 2 9'), 'mean') (('We', 'epoch'), 'calculate') (('Now we', 'training'), 'do') (('there we', 'training'), 'create') (('We', 'now simple CNN'), 'take') (('batch size', '3'), 'be') (('Learner object', 'training elsewhere loop'), 'do') (('lossFirst we', 'activations'), 'course') (('this', 'later'), 'explain') (('i', 'range'), 's') (('parameters', 'function'), 'be') (('we', 'problem'), 'be') (('backward', 'backward hook'), 'call') (('we', 'Callback'), 'be') (('input later number', 'training data data'), 'have') (('aren', 'there memory'), 'make') (('variance', 'one thing'), 'sample') (('we', 'counts minus'), 'note') (('we', 'loss'), 'part') (('Bot we', 'fist 10 layers'), 'see') (('we', 'one'), 'start') (('we', 'model'), 'say') (('where everything', 'first pich'), 'note') (('we', 'float'), 'want') (('we', 'also self'), 'call') (('back which', 'i'), 'note') (('it', 'activations'), 'layer') (('we', '_ inner start'), 'pname') (('callbacks', 'running'), 'do') (('when it', 'block'), 'use') (('training though loop', 'order'), 'be') (('each', 'num_channels num_groups channels'), 'normFrom') (('zero_grad PyTorch', 'optim'), 'provide') (('we', 'pytorchs just optim'), '9375') (('we', '0'), 'store') (('we', 'division'), 'be') (('we', 'scratch'), 'on') (('where minibatches', 'extremely large distributed models'), 'try') (('where we', 'last notebook'), 'remember') (('_ _ setattr _ _', 'self'), 'module') (('it', '_ maight python'), 'start') (('that', '9'), 'give') (('that', 'accuracal yb'), 'see') (('properly when model', 'very hooks'), 's') (('you', 'nn'), 's') (('we', 'infinity'), 'have') (('So it', 'this'), 'use') (('lets', 'higher acc'), 'look') (('that', 'several hooks'), 'use') (('s', 'usual warm annealing'), 'note') (('layers', 'beginning'), 'be') (('Hooks don t', 'model'), 'require') (('now we', 'dataloaders'), 'ds') (('we', '3 instead best_loss'), 'for') (('you', 'models paramters'), 'go') (('General equation', 'BatchNorm is1'), 'beta') (('DataLoaderNote you', 'Dataset'), 'look') (('this', 'acc'), 'be') (('_ _ iter _ _ method', 'in'), 'be') (('PyTorch you', 'nn'), 'be') (('mean deviation', 'separately group'), 'calculate') (('we', '2 dimensions 0 3 dims'), 'register') (('it', 'bool param'), 'NB') (('where we', 'optim'), 'see') (('secound we', 'momentum'), 'inste') (('Handle', 'dropout print model'), 'note') (('Shape N num_channels input Output N num_channels same Examples', 'input torch'), 'Input') (('so now we', 'model'), 'use') (('thereby we', 'full matrix'), 'note') (('so we', 'layers'), 'module') (('sigma_i 2 epsilon quad mu_i', 'eq bnorm'), 'don') (('here new callbacks', 'name'), 'use') (('it', 'previus model'), 'conv2d') (('100 it', 'longer them'), 'ater') (('it', 'device'), 'be') (('so fase one wil', 'learning 0'), 'start') (('you', 'self'), 'stop') (('we', 'code'), 'shape') (('loss', 'little bit better parameters'), 'repeat') (('it', 'suddenly other'), 'get') (('it', 'as before rate'), 'relu') (('so we', 'fist layer'), 'use') (('we', 'activations'), 'be') (('Here we', 'list'), 'store') (('mean', 'very 0'), 'mean') (('it', 'GPU'), 'take') (('first callback', 'epoch'), 'be') (('all', 'just them'), 'be') (('that', 'them'), 'check') (('we', 'modules'), 'use') (('relu Uniform more useful initial weights normal distribution', '0'), 'provide') (('PyTorch', 'link'), 'note') (('that', 'avgerge gennemsnitlige'), 'do') (('when we', '10 iterations'), 'have') (('order', 'iteration'), 'so') (('final layer', 'activations'), 'mean') (('training together validation', 'class'), 'be') (('now handler', 'xb also yb'), 'let') (('again first few layers', 'filters'), 'add') (('it', 'training set'), 'stop') (('batch small size', 'TREE SO IDEA'), 'are') (('we', 'layer'), 'calulate') (('how it', '1'), 'give') (('log input negative likelyhood lets', 'predictions range target'), 'end') (('that', 'objects'), 'design') (('that', 'maximum value'), 'try') (('we', 'indexses'), 'log') (('layer', 'training modes'), 'use') (('result', 'correctly x.'), 'import') (('you', 'more 20 yet just variance'), 'mean') (('module', 'biases'), '5affine') (('we', 'activations'), 'have') (('0 2 3 we', 'axsis'), 'start') (('we', '2'), 'go') (('ds', '50000'), 'loop') (('you', 'Lambda'), 's') (('convenient which', 'torch'), 'be') (('which', 'previously x.'), 'note') (('together we', 'before so now them'), 'take') (('this', 'already us'), 'implement') (('then last 70', '0'), 'go') (('that', 'mean'), 'be') (('inparticular when we', 'operation usage bla bla correct bla'), 'go') (('procent', '0'), 'mean') (('So we', 'log_softmax function'), 'use') (('loss', 'statistics'), 'reset') (('we', 'activation actual zero'), 'get') (('which', '2 epochs'), 'note') (('we', 'very close 0'), 'GroupNorm') (('model', 'size input now image'), 'work') (('GroupNorm', 'InstanceNorm m equivalent nn'), 'channel') (('you', 'image'), 'question') (('we', 'that'), 'see') (('when we', 'loss'), 'define') (('function', '1 linear fashion'), 'return') (('that', 'linear model'), 'be') (('which', 'chase'), 'SGD') (('we', 'only forward hooks'), 'support') (('when we', 'x.'), 'take') (('also here we', 'secound point'), 'note') (('_ _ log sum _ _ 1 right left 1 where a', 'x _ j.'), 'be') (('1 we', 'order torch'), 'let') (('we', 'then next 5 so picture'), 'calulate') (('we', 'result'), 'x') (('we', 'just that'), 'go') (('it', 'proper nn'), 'modules') (('when your', 'only 2 3'), '09') (('affine', 'size num_channels'), 'be') (('we', 'index self'), 'go') (('acc score', 'len range 0 self'), 'check') (('so we', 'difined model'), 'be') (('activation how many last layer', 'beginning'), 'predefine') (('we', 'model'), 'note') (('it', 'it'), 'note') (('we', 'almost which'), 'have') (('almost we', '8'), 'be') (('then we', 'differeent outcomes'), 'be') (('you', 'times'), 'return') (('it', 'them'), 'n') (('so we', 'self'), 'create') (('accuracy', '0'), 'mean') (('these', 'such nn'), 'eval') (('most', 'zero gradient'), 'have') (('so we', 'model'), 'do') (('batch size', 'matrix'), 'reprer') (('we', 'context manager'), 'use') (('it', 'training'), 'combine') (('otherwise we', 'normal relu'), 'want') (('Randomized tests', 'more flexible way'), 'handle') (('4 7 2 Somewhat more flexible way we', 'torch'), 'matter') (('dosent', 'thing one ones'), 'bs') (('helper function', 'training'), 'give') (('t', 'PyTorch'), 'nb') (('HW sum _ l W _ H _ 1 1 m 1 tilm', 'eq inorm'), 'label') (('example Here use', '0'), 'be') "}