{"name": "quantile reg lr schedulers checkpoints ", "full_name": " h1 Quick intro h2 Competition description h3 What should I expect the data format to be what am I predicting h1 Update history h2 New Notebook for CV backed Random Grid Hyperparameter Search click here h2 Domain knowledge h3 Some domain knowledge can be gained from watching the following video and from reading here h2 Load all dependencies you need h1 First glimpse at the data h1 Data Wrangling h2 Getting the format right h2 Preparing the data for the Neural Network h3 The first apporach is using sklearn as it is super famous and used frequently h3 The 2nd approach is doing all the legwork ourselfs h1 Model Loss h1 CONFIG Section h3 Loss Function h2 Neural Network Model h3 Normalization Regularization h3 Activation function h3 What about those mysterious quantiles q1 and quantiles adjustment q adjust layers h1 Training h1 Evaluation submission h3 Check loss and accuracy h3 OOF Evaluation h2 Thanks a lot for reading I hope you could gain as much insights from reading this as I got from writing it ", "stargazers_count": 0, "forks_count": 0, "description": "The idea on how to do that is coming from PAB97 Pierre s great notebook CHECK IT OUT https www. Those plots can tell us whether our model training is working as expected or if it strongly overfits. If for the column SmokingStatus we only have the values Smoker and Never Smoked we have two resulting columns if there are additional possible values like Ex_Smoker we get more columns. 8 default choose ADAM or SGD higher batch size higher lr 30 of all epochs are used for ramping up the LR and then declining starts how many epochs shall L_RMAX be sustained rate of decay get the Keras required callback with our LR for training plot check the LR Scheulder for sanity check logging saving defining custom callbacks defining a class variable which is used in the following to ensure LogPrinting evaluation saving is consitent choose between val_score and val_loss get index of best epoch get score in best epoch create constants for the loss function define competition metric Python is automatically broadcasting y_true with shape 1 0 to shape 3 0 in order to make this subtraction work define pinball loss IMPORTANT define quartiles feel free to change here combine competition metric and pinball loss to a joint loss function instantiate optimizer create model predicting the 3 quantiles generating another output for quantile adjusting the quantile predictions adding the tf. Months later you were finally diagnosed with pulmonary fibrosis a disorder with no known cause and no known cure created by scarring of the lungs. iterrows is roughly 8 times slower than using for idx in df. cumsum q1 then we would guarantee that we have an increasing order but we have less degrees of freedom and the OOF Score and LB score is worse. First we are taking care of the loss. We are going to use dropout for regularization and not a too broad and deep network as the training data is very limited. As you can see in the loss function and in the neural network output we expect 3 values one value for each of the defined quantiles. CONFIG Section In this section you can configure the following Features used for training Basic training setup BATCH_SIZE and EPOCHS Configuration for the loss function Optimizers Learning Rate Schedulers incl. Your help and data science may be able to aid in this prediction which would dramatically help both patients and clinicians. The upside is it s easy to add some additional features and pump them through the Pipeline or to change the Pipeline itself e. the relative number of weeks pre post the baseline CT may be negative We can eighter drop all except the first or last duplicate or average them. The good thing is we dont need a NoTransformer here as we simply can work in the DataFrame itself and not change any data which we want to preserve. We need to use pd. In addition the wide range of varied prognoses create issues organizing clinical trials. The first big challenge is data wrangling We could see that some patients take FVE measurements only after their baseline CT Images and some took measurements before that. Then the patient took the next FVC measurement 9 weeks later. The below EXAMPLE IMAGE is an example of a strongly overfitting model image. com chrisden 6 82x cv backed hyperp gridsearch quantile reg V39 Optimized Wording Hyperparameters reduced NFOLDs V33 Improved loggingV30 31 enhanced model explanations q1 and q_adjust layers and outputs. Reduce the network s size width andor dept by removing layers or reducing the number of neurons in the hidden layers Use regularization like LASSO Least Absolute Shrinkage and Selection Operator aka L1 regularization or Ridge aka L2 regularization which results in adding a cost term to the loss function Use higher dropout rate in the Dropout Layers which will randomly remove more connections by setting them to zero and forcing the network to generalize better avoid relying on a limitied number of strong influence neurons. Produces negative outputs which helps the network nudge weights and biases in the right directions for negative inputs too. What about those mysterious quantiles q1 and quantiles_adjustment q_adjust layers The idea of the q1 layer is to actually predict the 3 quantiles. Its mission is to bring together radiologists clinicians and computational scientists from around the world to improve imaging based treatments. But wait we only get back a series element How to put that in a Dataframe again Sadly not as easy as I had hoped. Does anybody have an idea on HOW to improve this The 2nd approach is doing all the legwork ourselfs. If that happened to you you would want to know your prognosis. As there is less randomness in a huge averaged batch compared with for example Stochastic Gradient Decent SGD with batch size 1 and our confidence in the direction is higher the learning rate can be bigger to advance fast to the optimum. Preparing the data for the Neural Network The first apporach is using sklearn as it is super famous and used frequently. Cited from page 3 Empirically we find that the ability to grow the norm v makes optimization of neural networks with weight normalization very robust to the value of the learning rate If the learning rate is too large the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached. So the number of columns now depends on how many different values categories a categorical value has because for each unique value we get a separate column e. In this competition you ll predict a patient s severity of decline in lung function based on a CT scan of their lungs. That s where a troubling disease becomes frightening for the patient outcomes can range from long term stability to rapid deterioration but doctors aren t easily able to tell where an individual may fall on that spectrum. We subtract the lower quartile from the upper quartile defined in the loss function and average it. The group enables rapid advances in the fight against Idiopathic Pulmonary Fibrosis IPF fibrosing interstitial lung diseases ILDs and other respiratory diseases including emphysematous conditions. So I looked up how other people solved it and I found a rough equivalent to the following function The second apporach is using transform which is not as known as apply but faster for basic operations not involving multiple columns of a dataframe. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Getting our model to predict the Confidence and FVC values which is what we need is not working fine so far as you can read here https www. png The global minimum of this function is achieved for delta 0 and sigma 70 which results in a loss of roughly 4. com activation functions explained elu below you can find a short summary Pros Avoids the dead ReLu problem ReLus provides activation values gradients of 0 for negative input values Produces activations for negative inputs instead of letting them be zero when calculating the gradient. https stackoverflow. com ulrich07 great notebook https www. mean val_score limit y values for beter zoom scale get rid of unused data and show some non empty data. com ulrich07 osic multiple quantile regression starter which also inspired me to change my loss to the above coded version. exchanging MiNmaxScaler with StdScaler RobustScaler etc. In the next step we need to baseline the FVC values. Btw there is an even worse approach Using for row in df. com c osic pulmonary fibrosis progression discussion 179033 I wanted to know how much this speeds up the processing you can find the results in the following Clearly the third approach works best and I have learned a very clean way of processing data. Okay wow that was not as easy as I expected. Activation functionAs the given task without the usage of images is not very compute intensive you don t need a GPU CPU will do we will change the activation function from relu to elu. It adds additional degrees of freedom more neurons more trainable weights which provides better results. Remember that roughly 4. If successful patients and their families would better understand their prognosis when they are first diagnosed with this incurable lung disease. Data Wrangling Getting the format rightIn this section we are going to do all the Data Wrangling and pre processing. com c osic pulmonary fibrosis progression discussion 168469. We need to get this into a format which we can easily use for making our predictions so let s split up the Patient_Week column into a Patient and a Weeks column to align it with the train test data format. The code of this section is derived from Ulrich s https www. TrainingIn the following we want to create leak free folds to get a robust cross validation strategy in order to evaluate all our models our training. com cdeotte triple stratified kfold with tfrecords. Those weeks which are not in the final three visits are ignored in scoring. Make sure to not call it MinMaxScaler and shadow the already important MinMaxScaler from Sklearn Okay so the second apporach using our own implementation was more straightforward and less code. The challenge is to use machine learning techniques to make a prediction with the image metadata and baseline FVC as input. com c osic pulmonary fibrosis progression discussion 167764. Note that the BASELINE FVC it not the minimum FVC but the first measurement meaning the measurement taken in the min_week or baselined_week 0. To avoid potential leakage in the timing of follow up visits you are asked to predict every patient s FVC measurement for every possible week. You ll determine lung function based on output from a spirometer which measures the volume of air inhaled and exhaled. The idea is to avoid having the same patient PatientID in training and in validation Data as this might lead to evaluate a higher CV score for a model which is luckily learning memorizing the data for a particular patientID which is also frequently occuring in the validation data. Feel free to experiment with the scheduler and it s max min and decay values. png Long before we hit the 10th epoch the validation loss is increasing again while the training loss keeps decreasing. Does not avoid the exploding gradient problem. Downside is we need to implement the MinMaxScaler by hand. For more info you can read here. com rftexas osic eda leak free kfold cv lgb baseline kln 440 Please note that we still don t use propoer stratification based on Age Sex SmokingStatus. In the next section we go trough two possibilities on how to normalize standardize and prepare the data for the neural Network. com questions 27517425 apply vs transform on a group object It still wasn t perfect and I found a third super clean approach here https www. Model LossIn this section we are going to define the loss a first model. The authors describe the method like this Weight normalization a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. But we used OneHot Encoding. But now check our model s result and evaluate it OOF EvaluationIn the next section we are going to use the train_preds to calculate the optimized sigma which is a measure for certainty or rather uncertainty. Open Source Imaging Consortium OSIC is a not for profit co operative effort between academia industry and philanthropy. There are 2 reasons for the q_adjust layer First With adding the cumulative sum on top of q1 we ensure or at least very strongly support that the output preds are in increasing order. Then our output preds are q1 tf. Okay now we encoded all the data and want to have a look at it. Normalization RegularizationWe are going to use weight normalization link to the paper click here https arxiv. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. Current methods make fibrotic lung diseases difficult to treat even with access to a chest CT scan. test_df only contains male Ex smokers. The final submission file should contain a header and have the following format Patient_Week FVC ConfidenceID00002637202176704235138_1 2000 100ID00002637202176704235138_2 2000 100ID00002637202176704235138_3 2000 100 Update history New Notebook for CV backed Random Grid Hyperparameter Search click here https www. Not the data has the format we need to work with. Ignore future warnings and Data Frame Slicing warnings. First glimpse at the dataTo get an idea of the meaning of the weeks column let s check some of their values. As we can clearly see some patients took the first measure of FVC before and some after their baseline CT images. V23 Minor optimizations wording hyperparametersV19 Introduced plotting evaluation of results V18 Introduced usage of tensorflow. com c osic pulmonary fibrosis progression discussion 165727 Load all dependencies you need from coffee import Let s start seeding everything to make results somewhat reproducible. We start with the baseline week What we can see here is that the Patient with ID ending on 430 had his first FVC measure 4 weeks before the first baseline CT images Weeks column 4 were taken. As there are only a few duplicates we can drop them without a bad conciousness for loosing to much data for our first apporach. We can do that as we have both the model s estimate and the real data. Please support the original Notebook creators The chosen quartiles are simply derived by testing using 0. addons tfa WeightNormalization V16 Code optimizations https www. MinMax Scaling in test_df will not have the same range of values e. Currently the way to go seems to be pinball loss. The following code is getting our data back to a Dataframe and preserving the correct order. Evaluation submission Check loss and accuracyOkay we made it Let s evaluate our model check our stats Out Of Fold log loss and submit it Let s start with some plots. So let s concatinate all our data first and then start with the transformations. The categorical features might have different categories in test_df than in train_df e. Standardization or Normalization e. The downside is it s not intuitive at least not for me and takes quite some lines of code. CHECK FOR DUPLICATES DEAL WITH THEM keep False All duplicates will be shown split Patient_Week Column and re arrage columns introduce a column to indicate the source train test for the data make a copy to not change original df ensure all Weeks values are INT and not accidentaly saved as string as test data is containing all weeks copy the DF to not in place change the original one get only the rows containing the baseline min_weeks and therefore the baseline FVC fill the df with the baseline FVC values same as above add a row which contains the cumulated sum of rows for each patient drop all except the first row for each patient unique rows containing the min_week merge the rows containing the base_FVC on the original _df import the necessary Encoders Transformers define which attributes shall not be transformed are numeric or categorical create an instance of the ColumnTransformer the No Transformer does not change the data and is applied to all no_transform_attribs Apply MinMax to the numerical attributes here you can change to e. If you don t do that you need to be careful with some steps e. 75 leads to worse results. Loss Function Neural Network ModelIn this section we build an initial neural Network. This class needs a fit and a transform method so that the ColumnTransformer itself can use fit_transform like for the numerical and categorical attributes. StdScaler OneHotEncoder all categorical attributes. It s good practice to concatinate all tabular data train test submission to ensure all data get s the same correct treatment. We can also clearly ovserve that there is no improvement in our accuracy anymore. We are trying to minimize the following image. What can we do against strongly overfitting models We could do the following Collect more training data or use augmentation to generate new data Sadly I have no brilliant idea on how to do this for this specific Kaggle competition. Learning Rate start endpoint Custom Logging Callback Checkpoint Saving CallbackThe Learning Rate scheduler below is inspired by Chris great Melanoma detection notebook https www. Finally patients suffer extreme anxiety in addition to fibrosis related symptoms from the disease s opaque path of progression. Anyway in keras it is quite hard to get 100 reproducible results. Here is an interesting post about it for those who want to learn more Apply vs transform. Cons Introduces longer computation time because of the exponential operation included. Let s consider two arrays vectors q1 a b c q_adjust e f g. Improved severity detection would also positively impact treatment trial design and accelerate the clinical development of novel treatments. What should I expect the data format to be what am I predicting Each row in the dataset contains a Patiend_ID and a week you must predict the FVC and a confidence. Ever wondered why lr_max is scaled by BATCH_SIZE and therefore bigger for larger batches The reason for this is the following the larger the BATCH_SIZE the more averaged smoothened a step of gradient decent is and the bigger our confidence in the direction of the step is. png attachment image. Downside if you want to replace the MinMaxScaler with another scaling method RobustScaler StdScaler you need to implement it first. cumsum q_adjust a e b e f c e f g Second We can see q_adjust as a baseline which is added to the q1 layers. For getting the baselined FVC I first wrote the following straightforward function This apporach works fine but as it contains a lot of look ups its slow and didn t feel right. 5 is the best possible score plt. V24 29 Minor adoptions testing multiple hyperparameters correcting MONITORING during training now correctly monitoring val_score instead of score which improves OOF score. To get a ColumnTransformer which outputs the whole DataFrame compatible format we can create a class that takes attributes that we don t want to change and simply passes them through. get column names for non categorical data extract possible values from the fitted transformer create Dataframe based on the extracted Column Names get back original data split be careful the resulsts are very SEED DEPENDEND you can exclude and include features by extending this feature list 0. So let s first find out what the actual baseline week and baseline FVC for each Patient is. Then we merge our info from the test data to the submission_df that s the fastest way of getting the correct format for predictions and submissions later on. For this we are going to define some functions and transformations which then are applied to the data. DataFrame data columns column_names function for which we need the column names. Thanks a lot for reading I hope you could gain as much insights from reading this as I got from writing it. min max values and therefore scaling than in train_df. 07868 from tensorflow_addons to support faster convergence. Additionally we gain more robustness for the choosing of the hyperparameter learning rate. Quick intro Competition descriptionImagine one day your breathing became consistently labored and shallow. https mlfromscratch. As I learned in a comment from frederiklaubisch sklearns ColumnTransformer has a remainder parameter that can be set to passtrough which would eliminate the need for the NoTranformer. Okay we made it Let s finally check our stats and submit it In the following last step we overwrite our predictions with the known data from the orginal submission file to not waste known data. Theoretically we could also use only q1 and add tf. com c osic pulmonary fibrosis progression discussion 179033 see third approach in the Data Wrangling section V13 14 small corrections edited some links V12 Corrected bug for saving models correctly see comment section enhanced readabilty V11 Introduced model checkpoints saving V10 Introduced configurable Learning Rate Schedulers V8 9 Introduced GroupKFolds to get leak free cross validation strategy to evaluate models training Domain knowledge Some domain knowledge can be gained from watching the following video and from reading here. For the architecture It s good practice to use numbers of units following the schema 2 x with x element of N resulting in 1 2 4 8 16 32 64 128. And we also should not get things wrong and mix the columns up. cumsum of q_adjust to the output q1 to ensure increasing values a a a b a b c create neural Network get target value get training test data instantiate target arrays extract Patient IDs for ensuring callbacks logging model saving with checkpoints each fold callbacks get_lr_callback BATCH_SIZE un comment for using LRScheduler build and train model evaluate append OOF evaluation to calculate OFF_Score predict on test set and average the predictions over all folds fetch results from history create subplots limit y values for better zoom scale. ", "id": "ChristianDenich/quantile-reg-lr-schedulers-checkpoints", "size": "19221", "language": "python", "html_url": "https://www.kaggle.com/code/ChristianDenich/quantile-reg-lr-schedulers-checkpoints", "git_url": "https://www.kaggle.com/code/ChristianDenich/quantile-reg-lr-schedulers-checkpoints", "script": "RobustScaler ColumnTransformer OneHotEncoder Style GroupKFold LogPrintingCallback(tf.keras.callbacks.Callback) mloss get_baseline_FVC tensorflow_addons Fore IPython.display tensorflow.keras.layers get_checkpoint_saver_callback old_baseline_FVC Image numpy get_baseline_FVC_new Back on_train_end sklearn.compose qloss sklearn.preprocessing on_epoch_end own_OneHotColumnCreator mean_absolute_error on_train_begin tqdm get_lr_callback tensorflow transform StratifiedKFold tensorflow.keras.backend KFold loss matplotlib.pyplot own_MinMaxColumnScaler MinMaxScaler baseline_FVC_new get_baseline_week TransformerMixin) get_model PIL sklearn.model_selection pandas sklearn.base NoTransformer(BaseEstimator baseline_FVC timeit BaseEstimator get_baseline_FVC_old fit seed_everything HTML TransformerMixin lr_scheduler score sklearn.metrics StandardScaler colorama ", "entities": "(('code', 'https www'), 'derive') (('only a few we', 'first apporach'), 'be') (('the bigger confidence', 'step'), 'wonder') (('more neurons more trainable which', 'better results'), 'add') (('we', 'freedom'), 'guarantee') (('SEED very you', 'feature list'), 'get') (('following code', 'correct order'), 'get') (('It', '2 16'), 's') (('2nd approach', 'legwork ourselfs'), 'have') (('it', 'code'), 'be') (('here you', 'e.'), 'CHECK') (('which', 'influence strong neurons'), 'avoid') (('which', 'certainty uncertainty'), 'check') (('which', 'NoTranformer'), 'have') (('you', 'FVC'), 'expect') (('that', 'direction'), 'describe') (('I', 'it'), 'thank') (('wide range', 'clinical trials'), 'create') (('440 we', 'Age Sex SmokingStatus'), 'cv') (('We', 'also clearly accuracy'), 'ovserve') (('again Sadly as I', 'Dataframe'), 'wait') (('which', 'scoring'), 'ignore') (('which', 'then data'), 'go') (('4', 'Weeks column'), 'be') (('it', 'Pipeline'), 'be') (('we', 'elu'), 'be') (('output at least very strongly preds', 'increasing order'), 'be') (('BASELINE FVC', 'min_week'), 'note') (('we', 'training'), 'trainingin') (('so s', 'train test data format'), 'need') (('Finally patients', 'progression'), 'suffer') (('Additionally we', 'hyperparameter learning rate'), 'gain') (('severity Improved detection', 'novel treatments'), 'impact') (('which', 'validation also frequently data'), 'be') (('now we', 'it'), 'encode') (('challenge', 'input'), 'be') (('we', 'neural Network'), 'go') (('Sadly I', 'Kaggle specific competition'), 'do') (('reparameterization', 'minibatch'), 'inspire') (('Optimizers Learning Rate Schedulers', 'EPOCHS loss function'), 'Section') (('we', 'gradient stochastic descent'), 'improve') (('FVC Update history New 2000 2000 100 Notebook', 'Random Grid Hyperparameter Search click'), 'contain') (('We', 'it'), 'subtract') (('we', 'don simply them'), 'pass') (('fetch results', 'zoom better scale'), 'cumsum') (('Then patient', 'FVC next measurement'), 'take') (('we', 'hand'), 'be') (('0 70 which', 'roughly 4'), 'png') (('it', 'quite 100 reproducible results'), 'be') (('Loss Function Neural Network we', 'initial neural Network'), 'ModelIn') (('how that', 'PAB97 great notebook'), 'idea') (('We', 'following image'), 'try') (('we', 'column separate e.'), 'depend') (('data', 'data train test good tabular submission'), 's') (('Learning Rate', 'Custom Logging Callback Checkpoint Saving CallbackThe Learning Rate Melanoma detection notebook https scheduler below Chris great www'), 'start') (('CT some', 'that'), 'be') (('I', 'third super clean approach'), 'apply') (('we', 'defined quantiles'), 'expect') (('it', 'sklearn'), 'prepare') (('when they', 'lung first incurable disease'), 'understand') (('which', 'negative inputs'), 'produce') (('we', 'more columns'), 'have') (('we', 'known data'), 'make') (('learning quickly appropriate effective rate', 'too large unnormalized weights'), 'find') (('lung fibrotic diseases', 'chest CT scan'), 'make') (('arrays vectors two q1', 'e f'), 'let') (('s', 'plots'), 'loss') (('which', 'q1 layers'), 'cumsum') (('which', 'dataframe'), 'look') (('training too broad data', 'regularization'), 'go') (('slow t', 'look'), 'write') (('RobustScaler you', 'it'), 'downside') (('that', 'predictions'), 'merge') (('which', 'OOF score'), 'V24') (('domain knowledge', 'reading'), 'see') (('y values', 'non empty data'), 'mean') (('we', 'format'), 'have') (('we', 'estimate'), 'do') (('learning rate', 'fast optimum'), 'be') (('So s', 'Patient'), 'let') (('we', 'column names'), 'function') (('q_adjust idea', 'actually 3 quantiles'), 'about') (('Months later you', 'lungs'), 'diagnose') (('best I', 'data'), 'discussion') (('group', 'other respiratory emphysematous conditions'), 'enable') (('Model section we', 'first model'), 'LossIn') (('which', 'air'), 'determine') (('you', 'possible week'), 'ask') (('s', 'values'), 'get') (('you', 'prognosis'), 'want') (('you', 'lungs'), 'predict') (('results', 'everything'), 'Load') (('So s', 'first then transformations'), 'let') (('Optimized Wording V39 Hyperparameters', 'model explanations 31 enhanced q1'), 'chrisden') (('it', 'us'), 'tell') (('iterrows', 'df'), 'be') (('We', 'them'), 'post') (('Theoretically we', 'tf'), 'use') (('pinball loss IMPORTANT define quartiles', 'tf'), 'choose') (('chosen quartiles', '0'), 'support') (('profit', 'academia industry'), 'be') (('you', 'steps e.'), 'do') (('we', 'Data Wrangling'), 'get') (('we', 'FVC values'), 'need') (('EXAMPLE below IMAGE', 'model strongly overfitting image'), 'be') (('which', 'above coded version'), 'com') (('it', 'scheduler'), 'feel') (('who', 'transform'), 'be') (('we', 'which'), 'be') (('transform ColumnTransformer', 'numerical attributes'), 'need') (('so second apporach', 'own implementation'), 'make') (('clearly patients', 'baseline CT before images'), 'see') (('easily where individual', 'spectrum'), 's') (('fine so far you', 'what'), 'work') (('Normalization RegularizationWe', 'paper click'), 'go') (('mission', 'based treatments'), 'be') (('which', 'dramatically patients'), 'be') (('training again loss', 'decreasing'), 'png') (('also things', 'columns'), 'get') (('instead them', 'when gradient'), 'explain') (('categorical features', 'e.'), 'have') (('Scaling', 'values e.'), 'minmax') "}