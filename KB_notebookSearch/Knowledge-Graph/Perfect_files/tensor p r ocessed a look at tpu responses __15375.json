{"name": "tensor p r ocessed a look at tpu responses ", "full_name": " h1 Profiling the TPU Users h1 Executive Summary h3 Importing required packages h3 Reading the data h2 Conclusion h2 References ", "stargazers_count": 0, "forks_count": 0, "description": "40 of the TPU user would be either a Student or a data scientist. GPUs and TPUs are both hardware accelerators which are used to speed up complex math computations in a deep learning problem. But we see precisely that regression and tree based models are adopted widely and models are built using those models before jumping into neural network based models. com kabure extensive usa youtube eda3. Interactive notebooks rich datasets to try out our concepts and availability of resources like TPU and GPU might be the reasons why the Kaggle learn courses are very popular among data scientists students. That might be the reason for SQL being used by 50 of TPU users. Importing required packages Reading the data The percentage of people using tpu is very nascent and it contributes to 4. A quarter of these users are from India 3 and an typical user is likely to have used TPU 2 3 times 6. 9 GANs for CV Embeddings for NLP Go back to Executive summary42 of the data scientist frequently used GANs for computer vision followed by equal number of people using image segmentation and classification methods. The purpose of the notebook is to get an insight of the TPU users and answer few questions like Who are the TPU users Are they from all over the world or they belong to typical age group come from a specific background etc Are TPU Users also use GPU If so what percentage From which country the majority of TPU usage comes from What are the common problems for which the TPUs are used How does it differ from a regular user After reading this notebook I hope that you will be inspired to try out interesting problems using TPUs. 1 Young Professionals Students are the main users of TPUGo back to Executive summaryTPU hardware is relatively new and came to widespread usage approximately an year ago. 5 Python C C are the go to languages for TPU Compute Go back to Executive summary When it comes to programming language of choice we have a clear winner for TPU specific problems. 2 Minimum TPU Usage is between 2 5 times Go back to Executive summary It is not very clear from the question definition as to what is the time interval used for this frequency is it monthly weekly daily. While Sklearn might be the most used framework 10 for machine learning problems Tensorflow Keras Pytorch frameworks are the go to frameworks for deep learning problems and close to 90 of data scientist would prefer to use Python 9 as a language to code. While the data used for deep learning tasks are typically big running to few GBs they could be stored either in enterprise cloud databases and one might require SQL to query and fetch them. When it comes to TPU specific users the years of experience is usually around 1 2 years which means professionals and young graduates who are starting their career as a data scientist are adopting the hardware accelerator for their ML requirements. More than 80 5 of them would also have used GPU for hardware acclearation along with the TPUs who would have completed courses 11 in Coursera and Kaggle Learn to pick up deep learning skills and lay their hands on with either TPU GPU. Python frameworks like Tensorflow and Pytorch have XLAs and enable mixed percision which makes it easier to adapt and code for using TPUs. Also to troubleshoot problems and help the community Kaggle has created a separate forum for TPU https www. This has also acclerated the developments in various fields like training on huge text corpus computer vision problems in medicine and biology. Inline with this it is no surprise that the highest percentage of response comes from people with 1 2 years of experience. This percentage may grow in the coming years with more and more TPU specific competitions datasets are added to the Kaggle datasets. Time and passion are the only investment for learning. 4 More than half of TPU users have either Bachelors or a Masters degree Go back to Executive summary Irrespective of the education qualification TPUs are used for solving the problems. GANs are a clever way of training a generative model by framing the problem as a supervised learning problem with two sub models the generator model that we train to generate new examples and the discriminator model that tries to classify examples as either real from the domain or fake generated 6. This might be because of inherent skewness in the survey which is tilted towards bachelors and masters degree. It is interesting to note that almost an equal percentage of users are using SKLearn for their modelling using TPU compute. They were first developed and used by Google to process large image databases such as extracting all the text from street view. This is the reason we see that 82 of the responses is shared between GPU and TPU compute. They have a wide application in image to image translation text to image translation and wherever the data is less to train an adversarial model could be developed to synthesis new images for training. To approximate the results I have considered this response to the question as minumum frequency of using a TPU. 4 Data Scientist prefer free cloud resources over setting up their own DL machine Go back to Executive summary TPU Prototype Coral Accelearator https coral. The graph seems to support our earlier hypothesis that young professionals having 1 2 years of experience are adopting TPU for their ML projects. The adoption is higher among the young professionals 1 between 1 3 years of experience and students who might have completed either Masters or Bachelors degree 4 who are keen to learn new technology in the field and stay relavant. Demographic Analysis1. While the survey has provided a peek into the world of data scientist in this notebook I am going to specifically focus on TPU users those who have selected TPU s as answer for the question Which type of specialization hardware do you use on a regular basis. 6 Tensorflow Keras has a edge over Pytorch for TPU Compute Go back to Executive summary Tensorflow Keras has an slight edge over Pytorch when it comes to framework of choice for TPU users. com product feedback 129828 TPU s tensor proccessing units are hardware acclerators specialized in deep learning tasks and provide significantly more computational power for mixed percision and matrix multiplications. Since both are here to solve the same problem it is natural to think GPU as an alternative and use them. Data analytics is a rapidly changing field with lot of concepts techniques discovered everyday. GPUs dont have this advantage and run out of memory while doing the calculations. Easy to use interface free compute resources less downtime might be few reasons for this preference. Therefore lets look specifically at computer vision algorithms to get a clear picture. 7 Coursera and Kaggle Learn are the preferred MOOC Go back to Executive summaryIn line with the global results Coursera is preferred MOOC among the TPU Users. It has already got some amazing topics to explore and do not forget to check it out. In terms of percentage Colab slightly has an edge over Kaggle followed by Google Cloud AI notebooks. Executive SummaryAccording to TPU launch page https www. 8 Regression and Tree based models remains the top choice of algorithms Go back to Executive summaryTPUs are used for training deep neural networks and we expect either RNNs or CNN based networks to be prefered over either regression or tree based networks. While the difference is narrow for TPU users a margin of 20 is noticed when overall response is considered between SKlearn and TF. It is also possible that the responses would have been provided keeping in mind the overall use of the algorithm rather than being TPU specific. com tpu utm_source google utm_medium cpc utm_campaign japac IN all en dr bkws all pkws trial e dr 1009137 utm_content text ad none none DEV_c CRE_396375661581 ADGP_Hybrid 20 7C 20AW 20SEM 20 7C 20BKWS 20 20T2 20 7C 20EXA 20 7C 20AI 20Platform 20 7C 20M 3A1 20 7C 20IN 20 7C 20en 20 7C 20cloud 20tpu 20 7C 20pricing 20 20PKWS KWID_43700049545260967 kwd 842288534372 userloc_1007809 network_g utm_term KW_gcp 20cloud 20tpu 20price gclid EAIaIQobChMIr8XCtMOh7QIV6INLBR1XPgs6EAAYASAAEgJtbfD_BwE While the percentage of TPU users remains at 4 it is to be noted that more than half of the respondents have also used GPU computing for their data science projects which suggests that the choice of hardware acclerators depends upon the problem at hand. ConclusionRelatively new to the deep learning world TPUs appear to be giving deep learning researchers ML engineers and students the required computational efficiency for problems that would have taken days to train on a CPU or even a GPU. Using TPUs in Kaggle comes with a time restriction of 30 hrs per week and going by this calculation it is seen that the minimum number of times a data scientist has used the tpu is between 2 5. In our algorithmic analysis one this is clear that data scientists prefer to start with good old established approaches like tree based regression embeddings before adopting neural network based models like CNNs BERT. While SQL C and C make it in top 5. com paultimothymooney 2020 kaggle data science machine learning survey5. com what are generative adversarial networks gans Q12_Part_2 Which types of specialized hardware do you use on a regular basis Select all that apply Selected Choice TPUs For how many years have you used machine learning methods Select the title most similar to your current role or most recent title if retired Selected Choice Approximately how much money have you or your team spent on machine learning and or cloud computing services at home or at work in the past 5 years approximate USD Which of the following machine learning frameworks do you use on a regular basis Select all that apply On which platforms have you begun or completed data science courses Select all that apply Selected Choice Coursera Which categories of computer vision methods do you use on a regular basis Select all that apply Selected Choice General purpose image video tools PIL cv2 skimage etc Which of the following natural language processing NLP methods do you use on a regular basis Select all that apply Selected Choice Word embeddings vectors GLoVe fastText word2vec. This has enabled begineers and students alike to access these resourses with zero investment 8. Both have the capability to increase the computational performance of problems involving large martrices and hence could be used depending upon the size and type of problem. 3 Colab and Kaggle are the choice for TPU Compute Go back to Executive summaryTPU is a Google designed hardware acclerators and are available in all its cloud platforms like Colab AI notebooks. There is a marginal difference between the users who have used GPU s to those users who have not used either a GPU or TPUs. Both Kaggle and Google colab offers eight TPU cores. Kaggle learn courses are small micro courses on a specific skill topic with notebooks available for reading and practise. 2 TPU Usage is spread across all Age Group Go back to Executive summaryTPU Users are spread out across all age group and we have 5 of kagglers in age group above 55 years who have adopted TPU for their deep learning requirements. Therefore it is important to keep up with pace adopt and practise new technology. 1 TPU Users have also worked on problems with GPU compute Go back to Executive summaryLets filter out only the TPU users for this notebook and analyse them. The result is consistent and reflects the overall trend when it comes to language of choice Python and SQL are used by data scientist. These two countries contribute to a significant number of active Kaggle users and hence its very natural to expect them in top 5 list. On text based problems pre BERT technique of word embeddings are widely used while a quarter of data scientists prefer BERT GPT 3 as their first choice. Currenly the learning stack ranges from basic introduction to Python to Reinforcement learning. com tags tpu to encourage its growth and build better optimized networks. The main advantage of using TPU is that it is run on cloud server and it lets to scale operations across different machines. The percentage of TPU users might be very low 4 according to the survey for this year but it will keep growing in the coming years and it is set to transform the field forever. Though there are newer and better models growing day by day Data scientists always prefer to start with either a regression 12 or tree based models before trying their hands on neural networks. The change comes in the 2nd position where Kaggle learn courses are prefered over the other MOOC platforms. com python choropleth maps 6. This could be the reason why a data scientist using TPU might prefer C or C over R whereas when it is seen for overall data scientist R is preferred over C and C. 3 India is word 1 in TPU adoption Go back to Executive summaryAccording to the survey India has seen a rapid adoption of the TPU s with over 33 of the response followed by US. This also shows that irrespective of the profession and education data scientists are not restricted to a specific technology and show interest to expand outside their formal education. But this could not turn our to be true for either a computer vision or a NLP problem 13 where techniques like GANs and BERT are increasingly used. This custom designed machine learning ASIC also powers Google products like Translate Photos Search Assistant and Gmail. https machinelearningmastery. Availability of GPU s in Colab Kaggle and a surge in ML competition related to image classification NLP makes it a popular tool of choice for kagglers. Therefore it is no surprise to note that the top 4 platforms are from Google. Profiling the TPU UsersIn yet another year of the comprehensive kaggle data science and machine learning survey there were 20 036 responses from 171 countries. 93 of the data scientist code using Python. Being a product of Google TPU s are widely available 7 and used in Google platforms like Colab and Kaggle by majority of data scientist where one is provided with a limited time 30 hrs weeky in Kaggle while in Colab free account should not exceed more than 12 hrs of continuous usage. This indicates that kagglers are willing to adopt and use new technology and are keeping up with latest technology advancements in the field. Source https cloud. ai products accelerator costs around 60 but due to the availablity TPU in cloud platforms like Kaggle and Colab most of the data scientist who are just starting their journey remember that our target TPU users are either students or a data scientist starting their career use these platforms rather than buying and setting up the hardware. This is consistent with the overall view as well. com rohanrao a deep learning of deep learning4. 8 whereas on the other hand it is seen that 42 of the users have used GPU s. Those with masters degree and above 50 have shown an increasing adoption compared to other degrees. com jpmiller some best practices for analytics reporting2. Python bindings enable one to use C or a C code to run in Python if there are no built in modules available in Python or to speed up this. ", "id": "gsdeepakkumar/tensor-p-r-ocessed-a-look-at-tpu-responses", "size": "15375", "language": "python", "html_url": "https://www.kaggle.com/code/gsdeepakkumar/tensor-p-r-ocessed-a-look-at-tpu-responses", "git_url": "https://www.kaggle.com/code/gsdeepakkumar/tensor-p-r-ocessed-a-look-at-tpu-responses", "script": "seaborn matplotlib.pyplot plotly.express pandas tpu_plot_bar_graph create_dict numpy ", "entities": "(('This', 'medicine'), 'acclerate') (('129828 tensor', 'mixed percision'), 'feedback') (('less downtime', 'few preference'), 'be') (('Therefore lets', 'clear picture'), 'look') (('TPU 1 Users', 'them'), 'work') (('machine learning ASIC', 'Translate Photos Search Assistant'), 'design') (('modelling', 'TPU compute'), 'be') (('Data analytics', 'concepts techniques'), 'be') (('one', 'them'), 'store') (('embeddings vectors', 'Selected Choice Word'), 'com') (('They', 'street view'), 'develop') (('Kaggle colab', 'TPU eight cores'), 'offer') (('two countries', 'top 5 list'), 'contribute') (('it', 'different machines'), 'be') (('widely quarter', '3 first choice'), 'use') (('It', 'it'), 'get') (('Colab', 'Google Cloud AI notebooks'), 'have') (('who', 'field'), 'be') (('young who', 'ML requirements'), 'be') (('discriminator that', '6'), 'be') (('Therefore it', 'new technology'), 'be') (('40', 'TPU user'), 'be') (('I', 'TPU'), 'approximate') (('This', 'zero investment'), 'enable') (('courses', 'data scientists very students'), 'be') (('based widely models', 'based models'), 'see') (('very it', '4'), 'import') (('when it', 'TPU users'), 'have') (('rapid adoption', 'US'), 'be') (('Coursera', 'TPU preferred Users'), 'be') (('9 GANs', 'image segmentation methods'), 'go') (('you', 'regular basis'), 'go') (('who', 'TPU'), 'use') (('when it', 'C'), 'be') (('42', 'GPU s.'), '8') (('kagglers', 'field'), 'indicate') (('close to 90', '9 language'), 'be') (('also irrespective', 'formal education'), 'show') (('learning Currenly stack', 'Reinforcement learning'), 'range') (('3 typical user', 'TPU'), 'be') (('Those', 'other degrees'), 'show') (('it', 'field'), 'be') (('it', 'them'), 'be') (('GPUs', 'calculations'), 'have') (('wherever data', 'training'), 'have') (('NLP 13 where techniques', 'GANs'), 'turn') (('Data 4 Scientist', 'Executive summary TPU Prototype Coral Accelearator https back coral'), 'prefer') (('community', 'TPU https www'), 'troubleshoot') (('data scientists', 'CNNs BERT'), 'be') (('RNNs based networks', 'regression'), 'remain') (('Both', 'problem'), 'have') (('top 4 platforms', 'Google'), 'be') (('that', 'CPU'), 'appear') (('it', 'kagglers'), 's') (('also responses', 'algorithm'), 'be') (('TPUs', 'problems'), 'have') (('target', 'rather hardware'), 'cost') (('young professionals', 'ML projects'), 'seem') (('82', 'GPU'), 'be') (('you', 'TPUs'), 'be') (('free account', 'continuous usage'), 'be') (('Python bindings', 'this'), 'enable') (('when overall response', 'SKlearn'), 'notice') (('choice', 'hand'), 'cpc') (('SQL', 'TPU users'), 'be') (('learn Kaggle courses', 'available reading'), 'be') (('learn courses', 'MOOC other platforms'), 'come') (('hardware which', 'learning deep problem'), 'be') (('which', 'bachelors'), 'be') (('Python', 'data scientist'), 'be') (('Young Professionals 1 Students', 'relatively widespread usage'), 'be') (('it', 'frequency'), 'go') (('who', 'GPU'), 'be') (('Data scientists', 'neural networks'), 'prefer') (('competitions more TPU specific datasets', 'Kaggle datasets'), 'grow') (('we', 'TPU specific problems'), 'be') (('TPU Compute', 'Colab AI notebooks'), 'be') (('who', 'learning deep requirements'), 'spread') (('it', 'TPUs'), 'have') (('Time', 'only learning'), 'be') (('highest percentage', 'experience'), 'be') (('minimum number', 'data tpu'), 'come') (('This', 'overall view'), 'be') (('com tags', 'better optimized networks'), 'tpu') "}