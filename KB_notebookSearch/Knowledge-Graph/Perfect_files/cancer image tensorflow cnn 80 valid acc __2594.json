{"name": "cancer image tensorflow cnn 80 valid acc ", "full_name": " h1 1 Libraries and settings h1 2 Analyze data h1 3 Manipulate data h1 4 Try out sklearn base models h1 5 Build neural network with TensorFlow h1 6 Train and validate the neural network ", "stargazers_count": 0, "forks_count": 0, "description": "layer fully connected 7 7 36 1024 1024. Analyze data 2 bullet 3. After that we implement the following neural network architecture input layer. 50 50 3 layer Conv1 ReLu MaxPool. In a first step we analyze the images and look at the distribution of the pixel intensities. Manipulate data 4. Still I think there is a lot of room for improvement here since I have not spend much time on tuning the model. 25 25 36 layer Conv2 ReLu MaxPool. Then the images are normalized and we try out some basic classification algorithms like logistic regregession random forest decision tree and so on. Build neural network with TensorFlow 6. 576 output layer FC ReLu. Train and validate the neural network 6 bullet Reference Predicting IDC in Breast Cancer Histology Images by paultimothymooney https www. layer convolution max pooling. 1 cost function optimisation function predicted probabilities in one hot encoding tensor of correct predictions accuracy tensors to save intermediate accuracies and losses during training number of weights and biases create summary tensors for tensorboard merge all summaries for tensorboard initialize summary writer initialize tensorflow saver function to train the graph train on original or augmented data training and validation data use augmented data parameters start timer looping over mini batches adapt learn_rate get new batch run the graph store losses and accuracies for logging the results summary for tensorboard concatenate losses and accuracies and assign to tensor variables save tensors summaries tf saver tb summary forward prediction of current graph function to load tensors from a saved graph input tensors weights and bias tensors activation tensors training and prediction tensors tensor of stored losses and accuricies during training get losses of training and validation sets get accuracies of training and validation sets get weights get biases load session from file restore graph and load tensors receive activations given the input cross validations cross validations default 20 5 validation set start timer train and validation data of original images create neural network graph instance of nn_class create graph attach saver tensors start tensorflow session attach summaries variable initialization of the default graph training on original data training on augmented data save tensors and summaries of model choose neural network choose neural network confusion matrix normalize plot. Try out sklearn base models 4 bullet 5. Results Using 10 fold cross validation the best base models can achieve an accuracy of roughly 76 on the validation sets. Implementing a 90 10 split for the training and validation data and training the neural network for 30 epochs while using data augmentation we can achieve an accuracy of 80 on the validation sets. The goal is to classify cancerous images IDC invasive ductal carcinoma vs non IDC images. training and validation data use one hot encoding for labels 0 1 dictionaries for saving results generate new images via rotations translations zoom using keras rotations translations zoom get transformed images check image generation computet the accuracy of label predictions store models in dictionary choose models for out of folds predictions start timer cross validations cross validations default 20 5 validation set random_state 123 start timer train and validation data of original images create cloned model from base models predictions accuracies normalized confusion matrix choose model plot boxplot algorithm comparison class that implements the neural network constructor tunable hyperparameters for nn architecture filter size of first convolution layer default 3 number of features of first convolution layer default 36 filter size of second convolution layer default 3 number of features of second convolution layer default 36 filter size of third convolution layer default 3 number of features of third convolution layer default 36 number of neurons of first fully connected layer default 576 tunable hyperparameters for training mini batch size keeping probability with dropout regularization in terms of epochs helper variables current position pointing to current learning rate log results in terms of epochs counting current number of mini batches trained on True use tensorboard visualization True use saver to save the model name of the neural network permutation array function to get the next mini batch adapt length of permutation array shuffle once at the start of epoch at the end of the epoch shuffle data start next epoch set index to mini batch size use augmented data for the next epoch use augmented data use original data generate new images via rotations translations zoom using keras rotations translations zoom get transformed images weight initialization bias initialization positive bias 2D convolution max pooling attach summaries to a tensor for TensorBoard visualization function to create the graph reset default graph variables for input and output 1. Build TensorFlow Graph 5 bullet 6. We also implement tensor summaries which can be visualized with TensorBoard. This takes roughly 45 minutes on the kaggle hardware. com paultimothymooney predicting idc in breast cancer histology images notebook 1. Manipulate data 3 bullet 4. Libraries and settings 1 bullet 2. 7 7 36 layer FC ReLu. Libraries and settings 2. Train and validate the neural network load images of shape 5547 50 50 3 load labels of shape 5547 1 0 no cancer 1 cancer shuffle data 0 no cancer 1 cancer convert one hot encodings into labels convert class labels from scalars to one hot vectors e. layer convolution max pooling 3 3 3 36 36. Try out sklearn base models 5. 1 0 1 0 1 0 function to normalize data scale features using statistics that are robust to outliers convert from 0 255 to 0. 1024 add dropout 5. We validate and compare each of these base models. 2 The neural network is implemented as a python class and the complete TensorFlow session can be saved to or restored from a file. The output layer gives for each image a probability for IDC 0 and IDC 1. In order to prevent overfitting the training data we generate new images by rotations translations and zoom. 13 13 36 layer Conv3 ReLu MaxPool. layer fully connected 1024 1 1024. Author Raoul Malm Abstract The dataset consists of 5547 breast histology images each of pixel size 50 x 50 x 3. ", "id": "raoulma/cancer-image-tensorflow-cnn-80-valid-acc", "size": "2594", "language": "python", "html_url": "https://www.kaggle.com/code/raoulma/cancer-image-tensorflow-cnn-80-valid-acc", "git_url": "https://www.kaggle.com/code/raoulma/cancer-image-tensorflow-cnn-80-valid-acc", "script": "one_hot_to_dense nn_class create_graph attach_saver save_model dense_to_one_hot numpy load_tensors seaborn get_loss get_biases weight_variable summary_variable get_accuracy generate_images train_graph accuracy_from_dense_labels tensorflow bias_variable matplotlib.cm conv2d matplotlib.pyplot load_session_from_file forward pandas attach_summary next_mini_batch get_weights normalize_data __init__ max_pool_2x2 get_activations ", "entities": "(('This', 'kaggle hardware'), 'take') (('Author Raoul Malm dataset', 'pixel each size'), 'Abstract') (('that', '255 0'), 'convert') (('here I', 'model'), 'think') (('neural network', 'neural network confusion matrix normalize plot'), 'predict') (('we', 'rotations translations'), 'in') (('we', 'network architecture input following neural layer'), 'implement') (('we', 'regregession forest decision logistic random tree'), 'normalized') (('com paultimothymooney', '1'), 'notebook') (('images weight transformed initialization', 'input'), 'use') (('Results', 'validation sets'), 'achieve') (('goal', 'non IDC images'), 'be') (('we', 'pixel intensities'), 'analyze') (('0 cancer 1 cancer', 'vectors one hot e.'), 'train') (('output layer', 'IDC'), 'give') (('we', 'validation sets'), 'achieve') (('layer convolution max', '3 3 36'), 'pool') (('neural network', 'TensorFlow complete file'), '2') (('We', 'base models'), 'validate') (('which', 'TensorBoard'), 'implement') "}