{"name": "fine tuning bert text classification ", "full_name": " h1 Fine Tune BERT for Text Classification with TensorFlow h2 Prerequisites h2 Initial Set Up h3 Install TensorFlow and TensorFlow Model Garden h2 Some Initial Imports and Checks h4 Logging into wandb h2 Lets Get the Dataset h3 Get to Know your data Some Basic EDA h1 Taming the Data h2 Lets BERT Get the Pre trained BERT Model from TensorFlow Hub h3 Checking out some of the training samples and their tokenized ids h2 Lets Get That Data Ready Tokenize and Preprocess Text for BERT h3 Wrapping the Python Function into a TensorFlow op for Eager Execution h2 Let the Data Flow Creating the final input pipeline using tf data h1 Lets Model Our Way to Glory h2 Create The Model h2 Let Us Train h4 W B Experiment Tracking h3 Lets Evaluate h3 Lets Look at some Graphs h3 Saving the models and model Versioning h4 W B Artifacts h3 Quick Sneak Peek into the W B Dashboard h3 References ", "stargazers_count": 0, "forks_count": 0, "description": "In order to prepare the text to be given to the BERT layer we need to first tokenize our words. csv quora_dataset_train. Artifacts Model versioning and storage. The official tfhub page states that All parameters in the module are trainable and fine tuning all parameters is the recommended practice. Get to Know your data Some Basic EDASo it looks like the train and validation set are similar in terms of class imbalance and the various lengths in the question texts. A sequence_output of shape batch_size max_seq_length 768 with representations for each input token in context. ai site for experiment tracking. Taming the Data Lets BERT Get the Pre trained BERT Model from TensorFlow HubWe will be using the uncased BERT present in the tfhub. Add your model file to the Artifact with wandb. Saving the models and model Versioning W B ArtifactsFor saving the models and making it easier to track different experiments I will be using wandb. ai akshayuppal12 Finetune BERT Text Classification artifacts model BERT_EN_UNCASED 48ffa3e14aba242a5113 Model versioning and more. Graph tensors do not have a value. Input Masks Since we are padding all the sequences to 128 max sequence length it is important that we create some sort of mask to make sure those paddings do not interfere with the actual text tokens. the dictionary have keys which should obv match Now we will simply apply the transformation to our train and test datasets train valid train data spec we can finally see the input datapoint is now converted to the BERT specific input tensor valid data spec Building the model input BERT Layer Classification Head for classification we only care about the pooled output at this point we can play around with the classification head based on the downstream tasks and its complexity inputs coming from the function Calling the create model function to get the keras based functional model using adam with a lr of 2 10 5 loss as binary cross entropy as only 2 classes and similarly binary accuracy Update CONFIG dict with the name of the model. If this was helpful consider sharing it with more people so they can also learn about it. Therefore we need a generate input mask blocking the paddings. You want to use Dataset. Here are Four main things that W B offers Experiment Tracking Tracking ML Experiments and logging various parameters and metrics on a clean dashboard. Initialize W B run Train model setting low epochs as It starts to overfit with this limited data please feel free to change Initialize a new run for the evaluation job Model Evaluation on validation set Log scores using wandb. given two sentences which sentence came first. log Log a dict of scalars metrics like accuracy and loss and any other type of wandb object. Hopefully this was useful for you and by now you have a small kickstart on training and utilizing BERT for a variety of downstream tasks like classification Named Entity Recognition Sentence filling and many more. Visualizations for system metrics could be useful when training on cloud instances or physical GPU machines Hyperparmeter tracking in the tabular form. org api_docs python tf data Dataset map to apply this function to each element of the dataset. csv from that competition https archive. Fine Tune BERT for Text Classification with TensorFlow Figure 1 BERT Classification ModelWe will be using GPU accelerated Kernel for this tutorial as we would require a GPU to fine tune BERT. Reports we can create reports on experiments and project levels. Data preprocessing consists of transforming text to BERT input features input_word_ids input_mask segment_ids input_type_ids Input Word Ids Output of our tokenizer converting each sentence into a set of token ids. Datasets return features labels pairs as expected by keras. You can run multiple experiments with different hyper parameters and track them. data The resulting tf. com ayuraj experiment tracking with weights and biases install requirements to use tensorflow models repository you may have to restart the runtime afterwards also ignore any ERRORS popping up at this step TO LOAD DATA FROM ARCHIVE LINK import numpy as np import pandas as pd from sklearn. Quick Sneak Peek into the W B Dashboard Things to note Grouping of experiments and runs. In graph mode you can only use TensorFlow Ops and functions. shape TO LOAD DATA FROM KAGGLE label 0 non toxic label 1 toxic Since the dataset is very imbalanced we will keep the same distribution in both train and test set by stratifying it based on the labels using small portions of the data as the over all dataset would take ages to train feel free to include more data by changing train_size TRAIN SET VALIDATION SET TRAIN SET VALIDATION SET we want the dataset to be created and processed on the cpu lets look at 3 samples from train set Setting some parameters Label categories maximum length of token input sequences Get BERT layer and tokenizer All details here https tfhub. com au1206 Fine_Tuning_BERT. ai akshayuppal12 Finetune BERT Text Classification runs 29thnm00 workspace user akshayuppal12 created for this run. init It initializes the run with basic project informationparameters project The project name this will create a new project tab where all the experiments for this project will be tracked config A dictionary of all parameters and hyper parameters we wish to track group optional but would help us to group by different parameters later on job_type to describe the job type it would help in grouping different experiments later. zip compression zip low_memory False print df. dev tensorflow bert_en_uncased_L 12_H 768_A 12 2 checks if the bert layer we are using is uncased or not This provides a function to convert row to input features and label this uses the classifier_data_lib which is a class defined in the tensorflow model garden we installed earlier since we only have 1 sentence for classification purpose textr_b is None since only 1 example the index 0 py_func doesn t set the shape of the returned tensors. Let us decompress and read the data into a pandas DataFrame. Since we just trained for a very limited data and less epoch these graphs generated here are not as represenetative so leaving this here more interms of a place holder for the reader to experiment with. Only realtokens are attended to. ai guides integrations kerasYes Its as simple as adding a callback D Lets EvaluateLet us do an evaluation on the validation set and log the scores using weights and biases. the final datapoint passed to the model is of the format a dictionary as x and labels. More details about Kaggle s Secret key feature on https www. Setting all parameters in form of a dictionary so any changes if needed can be made here Checking out some of the training samples and their tokenized ids Lets Get That Data Ready Tokenize and Preprocess Text for BERTEach line of the dataset is composed of the review text and its label. org api_docs python tf keras Model fit Lets Model Our Way to Glory Create The ModelThere are two outputs from the BERT Layer A pooled_output of shape batch_size 768 with representations for the entire input sequences. Bert was trained on two tasks fill in randomly masked words from a sentence. org api_docs python tf data Dataset map runs in graph mode. First things first we need to create a free account on W B https wandb. UPDATE BERT Annotated Paper and Beyond. ai authorize and add it to kaggle s secret key for hassle free authentication. org api_docs python tf py_function will pass regular tensors with a value and a. eg train evaluate etcIn order to Log all the different metrics we will use a simple callback provided by W B WandCallback https docs. map this function directly You need to wrap it in a tf. Within a run there are three steps for creating and saving a model Artifact. All these graphs are actually directly logged on the wandb dashboard https wandb. You can check out and get the entire code as a notebook and run it on colab from this Github Repo https github. log Finish the run Save model Initialize a new W B run for saving the model changing the job_type Save model as Model Artifact Finish W B run. For this kernel we will be looking into Weights and Biases https wandb. NOTE ANYTHING BEFORE THIS CELL SHOULD ONLY BE RUN ONCE ONLY DURING THE INITIAL SETUP Some Initial Imports and ChecksA Healthy practice for any ML practioner is to do a clean experiment tracking such that reasults are reproducable and trackable. Here we will pass the evaluation dictionary as it is and log it. Artifacts Storing Datasets models and other files for version tracking. For the classification task we are only concerned with the pooled_output. If you made it this far please consider leaving feedback so I can improve and also if you liked it consider upvoting. io annotated 20paper BERT References W B usage and intro https www. Create an empty Artifact with wandb. read_csv https archive. The mask has 1 for real tokens and 0 for padding tokens. Therefore we will go ahead and train teh entire model without freezing anything W B Experiment TrackingIn order to start the expirment tracking we will be creating runs on W B wandb. b lets us clone a specific branch only. Prerequisites Willingness to learn Growth Mindset is all you need Some basic idea about Tensorflow Keras Some Python to follow along with the code Initial Set Up Install TensorFlow and TensorFlow Model GardenCloning the Github Repo for tensorflow models depth 1 during cloning Git will only get the latest copy of the relevant files. Segment Ids For out task of text classification since there is only one sequence the segment_ids input_type_ids is essentially just a vector of 0s. To still give out a method to generate graphs explicitly here is some very basic code. Visualizations of all training logs and metrics. com product feedback 114053 Lets Get the Dataset The data we will use is the dataset provided on the Quora Insincere Questions Classification competition on Kaggle https www. ai site Then let us access our authorization API key https wandb. Even the distribution of question length in words and characters is very similar. Lets Look at some GraphsThese Graphs will mainly be useful when we are training for more epochs and more data. Coming up Next BERT Annotated Paper Write up on Transformers and its workings For some annotated reader friendly research papers on advanced concepts and tutorials like these please visit https au1206. It can save you a lot of space and time. log_artifact to save the Artifact Artifact dashboard https wandb. Wrapping the Python Function into a TensorFlow op for Eager Execution Let the Data Flow Creating the final input pipeline using tf. Please feel free to download the train set from kaggle or use the link below to download the train. model_selection import train_test_split df pd. Let Us Train One drawback of the tf hub is that we import the entire module as a layer in keras as a result of which we dont see the parameters and layers in the model summary. com c quora insincere questions classification data. W B Artifacts are a way to save your datasets and models. The tokenizer here is present as a model asset and will do uncasing for us as well. Sweeps Hyper parameter tuning. numpy method to access it to the wrapped python function. It looks like a good train test split so far. org download quora_dataset_train. Please match it with your tensorflow 2. Note After installing the required Python packages you ll need to restart the Colab Runtime Engine Run Restart and clear all cell outputs. py_function https www. org api_docs python tf py_function. We will be looking into Experiment Trancking and Artifacts Logging into wandb. ", "id": "au1206/fine-tuning-bert-text-classification", "size": "10124", "language": "python", "html_url": "https://www.kaggle.com/code/au1206/fine-tuning-bert-text-classification", "git_url": "https://www.kaggle.com/code/au1206/fine-tuning-bert-text-classification", "script": "official.nlp.data train_test_split create_model optimization wandb.keras numpy seaborn create_feature create_feature_map tokenization official.nlp create_graphs tensorflow matplotlib.pyplot kaggle_secrets sklearn.model_selection pandas classifier_data_lib official.nlp.bert UserSecretsClient WandbCallback tensorflow_hub ", "entities": "(('still give', 'graphs'), 'be') (('we', 'Kaggle https www'), 'feedback') (('sentence', 'two sentences'), 'give') (('we', 'only pooled_output'), 'be') (('it', 'it'), 'pass') (('ai Then us', 'authorization https API key wandb'), 'site') (('W B', 'clean dashboard'), 'be') (('train set', 'question various texts'), 'get') (('Therefore we', 'paddings'), 'need') (('np', 'sklearn'), 'install') (('Bert', 'sentence'), 'train') (('directly You', 'tf'), 'map') (('You', 'them'), 'run') (('Pre', 'present tfhub'), 'use') (('it', 'far feedback'), 'improve') (('org api_docs python', 'value'), 'pass') (('Datasets return features', 'labels keras'), 'pair') (('mask', 'tokens'), 'have') (('We', 'wandb'), 'look') (('paddings', 'text actual tokens'), 'Masks') (('we', 'https Weights wandb'), 'look') (('You', 'Github Repo https github'), 'check') (('mainly when we', 'more epochs data'), 'be') (('W B Artifacts', 'datasets'), 'be') (('we', 'experiments'), 'report') (('you', 'classification Named Entity Recognition Sentence filling'), 'be') (('org api_docs', 'dataset'), 'python') (('reader', 'place holder'), 'be') (('0 py_func doesn', 'returned tensors'), 'bert_en_uncased_L') (('characters', 'words'), 'be') (('Artifacts', 'version other tracking'), 'Datasets') (('workings', 'https au1206'), 'come') (('us', 'pandas'), 'let') (('we', 'first words'), 'in') (('It', 'wandb'), 'run') (('you', 'TensorFlow only Ops'), 'use') (('we', 'W B WandCallback https docs'), 'evaluate') (('Data Flow', 'tf'), 'let') (('It', 'train good test'), 'look') (('they', 'also it'), 'consider') (('Visualizations', 'tabular form'), 'be') (('keras', 'model'), 'have') (('log', 'other wandb'), 'log') (('such reasults', 'experiment clean tracking'), 'NOTE') (('graphs', 'wandb dashboard https actually directly wandb'), 'log') (('you', 'cell outputs'), 'note') (('Label maximum length', 'BERT layer'), 'take') (('we', 'model summary'), 'let') (('we', 'W B wandb'), 'go') (('log_artifact', 'https wandb'), 'dashboard') (('Model Artifact Finish W B', 'job_type Save model'), 'log') (('Data', 'token ids'), 'feature') (('Data Ready Tokenize', 'review text'), 'make') (('BERT Text Classification', 'run'), 'ai') (('EvaluateLet us', 'weights'), 'guide') (('ModelThere', 'input entire sequences'), 'python') (('tokenizer', 'us'), 'be') (('I', 'wandb'), 'save') (('we', 'tune fine BERT'), 'use') (('it', 'different experiments'), 'init') (('final datapoint', 'dictionary x'), 'pass') (('First things first we', 'W B https wandb'), 'need') (('io', 'References W B 20paper BERT usage'), 'annotate') (('TensorFlow GardenCloning', 'relevant files'), 'be') (('parameters', 'module'), 'state') (('org api_docs', 'graph mode'), 'python') (('segment_ids only one input_type_ids', 'essentially just 0s'), 'Ids') (('ai', 'hassle free authentication'), 'authorize') "}