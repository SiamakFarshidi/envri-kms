{"name": "plant disease classification resnet 99 2 ", "full_name": " h1 PLANT DISEASE CLASSIFICATION USING RESNET 9 h1 Description of the dataset h1 Our goal h2 Importing necessary libraries h1 Exploring the data h4 Visualizing the above information on a graph h4 Images available for training h1 Data Preparation for training h4 Image shape h2 Some Images from training dataset h1 Modelling h4 Some helper functions h2 Building the model architecture h4 Residual Block code implementation h2 Defining the final architecture of our model h1 Training the model h3 We got an accuracy of 99 2 h1 Plotting h4 Helper functions for plotting h2 Validation Accuracy h2 Validation loss h2 Learning Rate overtime h1 Testing model on test data h1 Saving the model h1 Conclusion h1 References h4 Hope you all learned something from this kernel Do upvote if you find this useful h4 Happy Learning h4 Catch you guys on the next one h4 Peace ", "stargazers_count": 0, "forks_count": 0, "description": "The reason for this is because pickle does not save the model class itself. Here is a simple residual block image https www. A new directory containing 33 test images is created later for prediction purpose. datasets is a class which helps in loading all common and famous datasets. It also helps in loading custom datasets. You will get a good overview of how to use PyTorch for image classification problems. com understanding and visualizing resnets 442284831be8 text ResNet 20Layers layers 20remains 20the 20same 20 E2 80 94 204. Saving a model in this way will save the entire module using Python s pickle https docs. Image shape We can see the shape 3 256 256 of the image. The algorithm takes the first 100 samples from 1st to 100th from the training dataset and trains the network. 2 Plotting Helper functions for plotting Validation Accuracy Validation loss Learning Rate overtime Testing model on test data We only have 33 images in test data so let s check the model on all images We can see that the model predicted all the test images perfectly Saving the model There are several ways to save the model in Pytorch following are the two most common ways 1. Since Kaggle provides a 2 core CPU I have set it to 2 Modelling It is advisable to use GPU instead of CPU when dealing with images dataset because CPUs are generalized for general purpose and GPUs are optimized for training deep learning models as they can process multiple computations simultaneously. Additionally computations in deep learning need to handle huge amounts of data this makes a GPU s memory bandwidth most suitable. 9 chance of getting the right answer or you can say model randomly chooses a class. So we need to install the torchsummary library discussed earlier Training the model Before we train the model Let s define a utility functionan evaluate function which will perform the validation phase and a fit_one_cycle function which will perform the entire training process. Getting a nicely formatted summary of our model like in Keras. It helps in loading large and memory consuming datasets. Even if you have used TensorFlow in the past and are new to PyTorch hang in there everything is explained clearly and concisely. This simple yet effective technique is called gradient clipping. The disadvantage of this approach is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved. Importing necessary librariesLet s import required modulesWe would require torchsummary library to print the model s summary in keras style nicely formatted and pretty to look as Pytorch natively doesn t support that Exploring the data Loading the data The above cell extract the number of unique plants and number of unique diseasesSo we have images of leaves of 14 plants and while excluding healthy leaves we have 26 types of images that show a particular disease in a particular plant. Note The following cell may take 15 mins to 45 mins to run depending on your GPU. Let s start training our model. In fit_one_cycle we have use some techniques Learning Rate Scheduling Instead of using a fixed learning rate we will use a learning rate scheduler which will change the learning rate after every batch of training. validation_step Because an accuracy metric can t be used while training the model doesn t mean it shouldn t be implemented Accuracy in this case would be measured by a threshold and counted if the difference between the model s prediction and the actual label is lower than that threshold. 3 is the number of channels RGB and 256 x 256 is the width and height of the image Some Images from training dataset batch_size is the total number of images given as input at once in forward propagation of the CNN. In kaggle P100 GPU it took around 20 mins of Wall Time. com remotesensing remotesensing 11 01896 article_deploy html images remotesensing 11 01896 g001. Save Load state_dict Recommended When saving a model for inference it is only necessary to save the trained model s learned parameters. com spMohanty PlantVillage Dataset. They have a large number of cores which allows for better computation of multiple parallel processes. com the vanishing gradient problem 69bf08b15484 and allow us to train deep neural networks. There are many strategies for varying the learning rate during training and the one we ll use is called the One Cycle Learning Rate Policy which involves starting with a low learning rate gradually increasing it batch by batch to a high learning rate for about 30 of epochs then gradually decreasing it to a very low value for the remaining epochs. org Hope you all learned something from this kernel. We need to build a model which can classify between healthy and diseased crop leaves and also if the crop have any disease predict which disease is it. Basically batch size defines the number of samples that will be propagated through the network. Now declare some hyper parameters for the training of the model. validation_epoch_end We want to track the validation losses accuracies and train losses after each epoch and every time we do so we have to make sure the gradient is not being tracked. Next it takes the second 100 samples from 101st to 200th and trains the network again. If you want to learn more about ResNets read the following articles Understanding and Visualizing ResNets https towardsdatascience. Weight Decay We also use weight decay which is a regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function. Note This description is given in the dataset itself Our goal Goal is clear and simple. org tutorials beginner examples_tensor two_layer_net_tensor. We can change it if result is not satisfactory. The entire array of pixel values is converted to torch tensor https pytorch. num_workers denotes the number of processes that generate batches in parallel. eval to set dropout and batch normalization layers to evaluation mode before running inference. The original PlantVillage Dataset can be found here https github. I have used subclass torchvision. We can keep doing this procedure until we have propagated all samples through of the network. Remember that you must call model. DataLoader is a subclass which comes from torch. Catch you guys on the next one Peace for working with files for numerical computationss for working with dataframes Pytorch module for plotting informations on graph and images using tensors for creating neural networks for dataloaders for checking images for functions for calculating loss for transforming images into tensors for data checking for working with classes and images for getting the summary of our model printing the disease names unique plants in the dataset number of unique plants number of unique diseases Number of images for each disease converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column plotting number of images available for each disease datasets for validation and training total number of classes in train set for checking some images from training dataset Setting the seed value setting the batch size DataLoaders for training and validation helper function to show a batch of training instances Images for first batch of training for moving data into GPU if available for moving data to device CPU or GPU for loading in the device GPU if available else CPU Moving data into GPU ReLU can be applied before or after adding the input for calculating the accuracy base class for the model Generate predictions Calculate loss Generate prediction Calculate loss Calculate accuracy Combine loss Combine accuracies Architecture for training convolution block with BatchNormalization resnet architecture out_dim 128 x 64 x 64 out_dim 256 x 16 x 16 out_dim 512 x 4 x 44 xb is the loaded batch defining the model and moving it to the GPU getting summary of the model for training scheduler for one cycle learniing rate Training gradient clipping recording and updating learning rates validation since images in test folder are in alphabetical order Convert to a batch of 1 Get predictions from model Pick index with highest probability Retrieve the class label predicting first image getting all predictions actual label vs predicted saving to the kaggle working directory saving the entire model to working directory. epoch_end We also want to print validation losses accuracies train losses and learning rate too because we are using learning rate scheduler which will change the learning rate after every batch of training after each epoch. Failing to do this will yield inconsistent inference results. The total dataset is divided into 80 20 ratio of training and validation set preserving the directory structure. Setting shuffle True shuffles the dataset. If you are not familiar why normalizing inputs help neural network read this https towardsdatascience. ai aakashns 05b cifar10 resnet PyTorch docs https pytorch. Save Load Entire Model This save load process uses the most intuitive syntax and involves the least amount of code. Because of this your code can break in various ways when used in other projects or after refactors. ImageFolder which helps in loading the image data when the data is arranged in this way root dog xxx. com why data should be normalized before training a neural network c626b7f66c7d post. It is heplful so that batches between epochs do not look alike. We also define an accuracy function which calculates the overall accuracy of the model on an entire batch of outputs so that we can use it as a metric in fit_one_cycle Defining the final architecture of our model Now we define a model object and transfer it into the device with which we are working. This also helps in preventing vanishing gradient problem https towardsdatascience. Do upvote if you find this useful. html text A 20PyTorch 20Tensor 20is 20basically used 20for 20arbitrary 20numeric 20computation. The model is able to predict every image in test set perfectly without any errors References CIFAR10 ResNet Implementation https jovian. A common PyTorch convention is to save models using either a. Gradient Clipping Apart from the layer weights and outputs it also helpful to limit the values of gradients to a small range to prevent undesirable changes in parameters due to large gradient values. Description of the dataset This dataset is created using offline augmentation from the original dataset. PLANT DISEASE CLASSIFICATION USING RESNET 9 DISCLAIMER This notebook is beginner friendly so don t worry if you don t know much about CNNs and Pytorch. Doing so will eventually make our model more robust. Let s check our validation loss and accuracySince there are randomly initialized weights that is why accuracy come to near 0. To seamlessly use a GPU if one is available we define a couple of helper functions get_default_device to_device and a helper class DeviceDataLoader to move our model data to the GPU as required Some helper functionsChecking the device we are working withWrap up our training and validation data loaders using DeviceDataLoader for automatically transferring batches of data to the GPU if available Building the model architecture We are going to use ResNet which have been one of the major breakthrough in computer vision since they were introduced in 2015. Pytorch doesn t support it natively. We ll also record the learning rate used for each batch. Visualizing the above information on a graphWe can see that the dataset is almost balanced for all classes so we are good to go forward Images available for training Data Preparation for training torchvision. For instance let s say you have 1050 training samples and you want to set up a batch_size equal to 100. Rather it saves a path to the file containing the class which is used during load time. png Next after loading the data we need to transform the pixel values of each image 0 255 to 0 1 as neural networks works quite good with normalized data. com method resnet In ResNets unlike in traditional neural networks each layer feeds into the next layer we use a network with residual blocks each layer feeds into the next layer and directly into the layers about 2 3 hops away to avoid over fitting a situation when validation loss stop decreasing at a point and then keeps increasing while training loss still decreases. We got an accuracy of 99. This dataset consists of about 87K rgb images of healthy and diseased crop leaves which is categorized into 38 different classes. It takes in batch_size which denotes the number of samples contained in each generated batch. We are using this function other than just an accuracy metric that is likely not going to be differentiable this would mean that the gradient can t be determined which is necessary for the model to improve during training A quick look at the PyTorch docs that yields the cost function cross_entropy https pytorch. png Residual Block code implementation Then we define our ImageClassificationBase class whose functions are training_step To figure out how wrong the model is going after training or validation step. com an overview of resnet and its variants 5281e2f56035 Paper with code implementation https paperswithcode. Saving the model s state_dict with the torch. save function will give you the most flexibility for restoring the model later which is why it is the recommended method for saving models. If you have more cores in your CPU you can set it to number of cores in your CPU. ConclusionResNets perform significantly well for image classification when some of the parameters are tweaked and techniques like scheduling learning rate gradient clipping and weight decay are applied. Overview of ResNet and its variants https towardsdatascience. Let s get started. and then divided by 255. ", "id": "atharvaingle/plant-disease-classification-resnet-99-2", "size": "12582", "language": "python", "html_url": "https://www.kaggle.com/code/atharvaingle/plant-disease-classification-resnet-99-2", "git_url": "https://www.kaggle.com/code/atharvaingle/plant-disease-classification-resnet-99-2", "script": "torch.nn.functional make_grid       # for data checking torchvision.transforms ImageFolder  # for working with classes and images plot_accuracies fit_OneCycle validation_step to_device numpy ImageClassificationBase(nn.Module) get_lr evaluate summary              # for getting the summary of our model __iter__ validation_epoch_end get_default_device training_step torch.nn epoch_end Image           # for checking images predict_image accuracy matplotlib.pyplot show_image plot_losses DeviceDataLoader() forward PIL pandas torchsummary SimpleResidualBlock(nn.Module) plot_lrs ResNet9(ImageClassificationBase) DataLoader # for dataloaders torch.utils.data __len__ show_batch __init__ ConvBlock torchvision.datasets torchvision.utils ", "entities": "(('you', 'CNNs'), 'classification') (('PlantVillage original Dataset', 'https here github'), 'find') (('Image We', 'image'), 'shape') (('we', 'training torchvision'), 'see') (('why it', 'saving recommended models'), 'give') (('they', 'multiple computations'), 'provide') (('model', 'randomly class'), 'chance') (('why accuracy', '0'), 'be') (('difference', 'actual threshold'), 'validation_step') (('code', 'refactors'), 'break') (('goal Goal', 'dataset'), 'note') (('you', 'equal 100'), 'let') (('that', 'network'), 'define') (('which', 'generated batch'), 'take') (('dataset', 'original dataset'), 'description') (('which', 'multiple parallel processes'), 'have') (('that', 'cross_entropy https pytorch'), 'mean') (('We', 'batch'), 'record') (('Cycle Learning Rate One which', 'remaining epochs'), 'be') (('training batch_size', 'CNN'), 'be') (('memory bandwidth', 'data'), 'make') (('entire array', 'torch tensor https pytorch'), 'convert') (('algorithm', 'network'), 'take') (('load process', 'code'), 'Model') (('everything', 'PyTorch'), 'use') (('training_step how model', 'training'), 'implementation') (('it', 'only trained model'), 'recommend') (('we', 'which'), 'define') (('you', 'CPU'), 'set') (('It', 'custom also datasets'), 'help') (('training then loss', 'point'), 'resnet') (('you', 'ResNets https towardsdatascience'), 'read') (('that', 'parallel'), 'denote') (('following cell', 'GPU'), 'note') (('It', 'consuming large datasets'), 'help') (('which', 'training'), 'use') (('You', 'image classification problems'), 'get') (('total dataset', 'directory structure'), 'divide') (('actual label', 'working directory'), 'catch') (('20Tensor 20is 20PyTorch 20basically', '20for'), 'use') (('which', 'torch'), 'be') (('model', 'Pytorch'), 'have') (('Setting', 'dataset'), 'shuffle') (('they', '2015'), 'define') (('it', 'Wall Time'), 'take') (('that', 'particular plant'), 'require') (('model', 'perfectly errors'), 'be') (('which', 'epoch'), 'want') (('1 neural networks', 'quite normalized data'), 'png') (('when data', 'way'), 'ImageFolder') (('why data', 'network c626b7f66c7d neural post'), 'com') (('Saving', 'pickle https docs'), 'save') (('us', 'deep neural networks'), 'com') (('gradient', 'train epoch'), 'want') (('which', 'load time'), 'save') (('PyTorch common convention', 'a.'), 'be') (('which', '38 different classes'), 'consist') (('it', 'large gradient values'), 'clip') (('we', 'network'), 'keep') (('pickle', 'model class'), 'be') (('Failing', 'inference inconsistent results'), 'yield') (('which', 'common datasets'), 'be') (('result', 'it'), 'change') (('techniques', 'learning rate gradient scheduling clipping'), 'perform') (('neural network', 'https towardsdatascience'), 'help') (('This', 'gradient problem https also vanishing towardsdatascience'), 'help') (('new directory', 'prediction later purpose'), 'create') (('org you', 'kernel'), 'Hope') (('directory exact when model', 'specific classes'), 'be') (('Next it', 'network'), 'take') (('fit_one_cycle which', 'training entire process'), 'need') (('batches', 'epochs'), 'be') (('disease', 'crop healthy leaves'), 'need') (('regularization which', 'loss function'), 'Decay') "}