{"name": "fork of hubmap submission experts raster i o ", "full_name": " h1 Introduction h1 Training Tricks h2 Disclamer h2 Color spaces augmentation trick h2 Multi scale input tiles h2 Pseudo labelling h2 Short training pipeline h1 Inference h2 Inference in a nutshell h2 Parameters h3 Models h1 Detailed Description of our Inference Approach h2 Data Preprocessing h3 Loading and Downsampling Whole Slide Images h2 Making Predictions h3 Padding for the first pass h3 Dividing the Image into Tiles h3 Generating the Segmentation Mask h3 Saving the Segmentation Mask h2 Generating Final Predictions h1 Submission h1 Insights ", "stargazers_count": 0, "forks_count": 0, "description": "ParametersHere are the parameters that we use for inference. This is only necessary during the first pass as the second pass will operate on pre selected tiles. com bguberfain memory aware rle encoding watch out for the bug This simplified method requires first and last pixel to be zero. For that purpose we apply the following _reconstruct_img function. You can see normal tiles with the cutmix augs color spaced without cutmix and color spaced with cutmix. com iafoss 256x256 images Models for first pass Models for second pass Tiles selection https www. The median area perimeter\u00b2 describing how complex the shape of the glomeruli were has been found to be correlated to the weight 0. Eventually the obtained mask is converted into RLE format. import cv2cspaces cv2. We copy our group finding code for information. In order to handle the WSI images size we used memmap to load the images and also used temporary saving of processed patches to perform the ensembling. COLOR_RGB2HSV cv2. COLOR_BGR2LUV cv2. Data PreprocessingData preprocessing consists in downsampling images. 071 57512b7f1 38216 741 0. Loading and Downsampling Whole Slide ImagesThe main issue with WSI is that they are very big. COLOR_RGB2YUV In the batch sampling part if self. Before the data update the best one was of 1024 and after 512 while the training is performed on 256x256. COLOR_BGR2Luv cv2. However by some reasons we did not discover which this stopped working after the data update at least for the public LB. We import them as follows. All put together the result is the following load_resize function. All these functions are combined in make_one_prediction_1st and make_one_prediction_2nd that aim at saving segmentation mask tiles based on an image and a given set of padding parameters for the first pass. On the images below one could see how the naturally looked tiles are transformed being augmented with this method. Inference in a nutshell We make an inference on the bigger patch size than was used for training. ModelsThe models that we can use for inference are defined below. 070 aa05346ff 55608 1122 0. Additionally we observed interesting behaviour such as robust segmentation even of some suspicious glomeruli in the d48 sample from public test set. 069 b9a3865fc 15859 483 0. 33 that wasn t used because of the inference time constraints. Very important point that we wanted to highlight that we were able to split all test images into two groups with respect to the ffpe ff type using the PhysicalSizeY from the tiff. csv file containing all our predictions. Padding for the first passAs images are very big we must divide them into tiles. sum 0 keepdims True if random. 068 8242609fa 32320 683 0. 071 1e2425f28 8768 356 0. csv file that is provided. We also share the code as we implemented this idea. id median area \u00b5m\u00b2 median perimeter \u00b5m median area perimeter\u00b2 0486052bb 5610 309 0. com iafoss 256x256 images Saturation blancking threshold Threshold for the minimum number of pixels Size of center check box Inference Make predictions only on public LB Overlap between tiles during prediction X axis Overlap between tiles during prediction Y axis Reduction for two types of models Threshold for first pass Threshold for second pass Number of bins when saving mask tiles Final prediction Size of saved mask tiles m torch. Then for the first pass we select tiles on which we will make predictions based on their color saturations. That approach was giving us very good results on the public leaderboard and on cross validation before the change of test data but unfortunately for some reason we could not see that improvement afterwards. 070 We found that the median area of glomeruli was negatively correlated to the weight correlation coefficient of 0. Then we must reconstruct the image from resampled tiles that we saved. This also gave us significant boost both in CV and LB. com bguberfain memory aware rle encoding https www. COLOR_BGR2RGB cv2. During the second pass predictions are upsampled to match the original image. The aforementioned DataLoader is generated using the following _make_tiles_dataloader_1st and _make_tiles_dataloader_1st functions. Therefore we initially tried to apply different models with respect to the type of WSI that we were processing. InferenceIn this section we describe how we make predictions. Image names from public and private test sets are retrieved from the sample_submission. png attachment c3ebe340 3998 4698 a504 39214ecbfa4f. COLOR_BGR2HLS cv2. png attachment 9bb3d996 0be8 44ec 9a9a 05d8135be713. Then we generate masks for each image in the test set. 069 aaa6a05cc 13466 436 0. shape Iterator like wrapper that returns predicted masks List of models Dataloader Half precision Reduction on reduced image Prepare input Make predictions Quantize probablities to save memory Output predictions List of models Dataloader Half precision Reduction on reduced image Prepare input Make predictions Upsample to initial shape py new_py Output predictions Add padding to make the image dividable into tiles Split image into tiles using the reshape transpose trick Final shape nb_x nb_y TILE_SZ TILE_SZ 3 Select tiles for running the model Make tiles dataset Generate masks Reshape tiled masks into a single mask and crop padding A little bit of cleaning. Detailed Description of our Inference ApproachIn this section we describe our approach version 1. COLOR_RGB2HLS cv2. Training Tricks DisclamerWe used NO hand labelling and manual annotation since in our vision it is not consistent with the initial goal and objective of the competition. 067 e79de561c 33504 704 0. The goal of the competition was to provide with automated glomeruli segmentation models for WSI of human kidneys. COLOR_BGR2Lab cv2. COLOR_RGB2Lab cv2. 069 54f2eec69 32392 684 0. In this competition these WSI can be divided into two categories those containing fresh frozen FFPE tissues and those containing formalin fixed paraffin embedded tissues. Multi scale input tilesWe used multiple image reduction rates 3 4 5 6 and 8 in order to create the tiles from original images. In particular we were interested in the median area per patient the median perimeter per patient and the median ratio area perimeter\u00b2 per patient. cvtColor img cspace if self. Pseudo labellingPublic test set pseudo labeled with own models and included in the training procedure Short training pipelineUnet with resnext50_32x4d resnext101_32x16d efficientnet b7 b5 backbones ASPP and FPN modules complex loss combination of FocalLoss DiceLoss SymmetricLovasz OneCycleLR Heavy Augmentations CutMix ColorSpaced augs 5 fold patient based CV. If PUBLIC_ONLY True then we only make the prediction for public test data. 070 3589adb90 28081 630 0. As said before the HuBMAPTestDataset2nd does not need the tile selection process we mentioned. This DataLoader is based on the HuBMAPTestDataset1st and the HuBMAPTestDataset2nd class below. After discussion we decided not to follow that approach in our final submissions. But it still demonstrated higher CV when models were trained independently for each group. The Model_pred_2nd class makes predictions on pre selected tiles and then only keeps glomeruli whose intersection with a center square of predefined dimensions is non empty this is to avoid problems encountered with detections close to edges that are often wrong. Here are the values we obtained based on the GT for images in the training set and based on our predictions for images in the test set. png Tiles with Color Space augmentation image. 069 26dc41664 28316 649 0. 065 c68fe75ea 34348 720 0. Saving the Segmentation MaskThe segmentation mask is then saved as tiles containing scores it is not binarized yet. The second version using a two pass segmentation is described in a separate notebook. exp K K elif random. After loading the image using the load_image function above we apply the _tile_resize_save function below. Such approach naturally increases the training dataset and enriches variation of glomeruli size and its proportion in the considered tile of fixed size. Here is how we load an image using NumPy MemMap. COLOR_RGB2LAB cv2. png Tiles with both augmentations image. 067 4ef6695ce 22636 574 0. 068 2ec3f1bb9 19719 529 0. InsightsThanks to our predictions we could compute some metrics about glomeruli. png This methodology demonstrated its effectiveness and power during both stages of the competition but mostly with the first set of the data maybe due to the different test sets since we dont know yet results on the private test data. Segmentation mask tiles are generated for all possible sets of padding parameters for the first pass using the following get_mask_tiles_1st and get_mask_tiles_2nd functions. The RLE conversion is done using the following rle_encode_less_memory function obtained from https www. However these findings must be confirmed on more patients as they are based on a very small sample of people that is not necessarily representative of the general population. 044 d488c759a 11376 403 0. Locally we used smaller step of 0. It is not yet binarized. load facebookresearch semi supervised ImageNet1K models resnext50_32x4d_ssl aspp with customized dilatations aspp with customized dilatations m torch. Making PredictionsIn this section we describe how we make predictions. Imports TODO remove PARAMETERS Printing parameters Data processing Input data directory Reduce the original images by x times Size of tiles on which inference is done https www. IntroductionThis kernel contains the inference code of the submission_experts team for the 2021 HuBMAP competition version 1. COLOR_RGB2LUV cv2. Eventually we need a DataLoader to feed data to our model. 65 in the standard pipeline up to 0. This version makes a two pass segmentation a first pass aims at finding where glomeruli are detection threshold is very low and a second pass aims at refining segmentations by applying our models on tiles centered on glomeruli found during the first pass. 071 b2dc8411c 24430 594 0. Generating the Segmentation MaskThe segmentation mask is then generated. The median perimeter was similarly correlated respectively 0. Color spaces augmentation trick For the training we found and decided to use various Color Space Augmentations in combination with custom stochastic kernel this is one of the most important things that allowed our models to stay robust and segment glomeruli despite the data source type of the images FFPE fresh frozen even others and color related variations. COLOR_RGB2XYZ cv2. We also used classical TTA based on transformations of the D4 group but we were limited by the kernel inference time. The _generate_masks_1st and _generate_masks_2nd function outputs a zero padded mask. Tiles with CutMix augmentation image. Save tiles in HDD memory Make tiles dataset Generate masks First generate mask tiles Then reconstruct mask from tiles Get bounding boxes Eventually convert to rle https www. We added zero padding to make the dimensions of the image dividable by the tile size. 071 095bf7a1f 29992 651 0. The next step for the first pass is to remove padding. 068 cb2d976f4 13378 435 0. We used overlapping step of 0. COLOR_BGR2XYZ cv2. COLOR_RGB2Luv cv2. This information was crucial before the data update and allowed us to get different models that worked for each of the groups. The _generate_masks_1st and _generate_masks_2nd functions are based on the Model_pred_1st and Model_pred_2nd iterator like class that is used to generate predictions. COLOR_RGB2BGR cv2. einsum ijk kl ijl img stoch This method also allowed to obtain much more focused predictions we observed very high soft dice metrics even without proper binarization threshold. load facebookresearch semi supervised ImageNet1K models resnet50_swsl aspp with customized dilatations aspp with customized dilatations Model name to class Import models Get tile Reduce if needed Save tile Return dimension after reduction Tiles are deleted when read X overlap padding Y overlap padding Check that width is OK Check that height is OK Generate tmp directory if needed Remove black or gray images based on saturation check print x1 x2 y1 y2 p. png attachment af404411 8d4e 4df3 9bb8 311a51da9d32. random 1 len cspaces cspace random. Generating Final PredictionsFinal predictions are generated stacking the outputs of the make_predictions_1st function and the make_predictions_2nd function through the get_mask_tiles_1st and get_mask_tiles_2nd functions. And finally we generate a submission. Obvious visual differences can be noticed in both types of WSI the structure of FFPE tissues is better preserved than fresh frozen ones. COLOR_BGR2YUV cv2. 069 afa5e8098 21632 577 0. 89 Dice depending on the fold. We also initially wanted to find to which group fresh frozen vs FFPE they belonged to but as we said before we finally did not follow that approach. SubmissionHere is the final part of this kernel dealing with the actual submission of a CSV file containing generated masks. Some additional insights are provided at the end of the notebook. Dealing with such big images is not convenient we found that we should downsample data for better performance. Ensembling predictions with different padding dimensions allows us to avoid bad predictions due to glomeruli located on edges of the tiles. 5xtile_size in both X and Y directions to cut the patches from test images. Final submissions are ensembles of models trained with and without Color Space Augmenatations and Cutmix in order to increase the robustness. Only after the second pass function _generate_masks_2nd the mask is upsampled to match the original image. choice cspaces img cv2. For example improvement is from 0. COLOR_BGR2LAB cv2. Dividing the Image into TilesThe following _split_image_1st function allows us to divide images into tiles. 059 2f6ecfcdf 34158 708 0. Additionally we used only non empty tiles of the bigger reduction rates to balance more the training dataset. com bguberfain memory aware rle encoding. COLOR_BGR2HSV cv2. rand 3 3 K stoch. sqrt K stoch stoch K img np. ", "id": "dannyelkallab/fork-of-hubmap-submission-experts-raster-i-o", "size": "13858", "language": "python", "html_url": "https://www.kaggle.com/code/dannyelkallab/fork-of-hubmap-submission-experts-raster-i-o", "git_url": "https://www.kaggle.com/code/dannyelkallab/fork-of-hubmap-submission-experts-raster-i-o", "script": "torch.nn.functional ConvLayer # TODO: remove = {\"ux50\" _make_tiles_dataloader_1st DataLoader _reshape_depad_mask_1st Unet50(nn.Module) Effb7Unet _reconstruct_img _get_nored_pads_1st Effb5Unet make_one_prediction_2nd make_predictions_1st UnetBlock(nn.Module) _add_padding_1st segmentation_models_pytorch load_resize _generate_masks_1st Image fastai.vision.all numpy UneSt200(nn.Module) Model_pred_1st UneXt101(nn.Module) UneXt50(nn.Module) __iter__ FPN(nn.Module) _ASPPModule(nn.Module) Model_pred_2nd _init_weight torch.nn get_mask_tiles_2nd tqdm resnest.torch ResNet ASPP(nn.Module) HuBMAPTestDataset1st(Dataset) rle_encode_less_memory matplotlib.pyplot make_predictions_2nd PixelShuffle_ICNR _split_image_1st __getitem__ forward PIL HuBMAPTestDataset2nd(Dataset) pandas _generate_masks_2nd tqdm.notebook _make_tiles_dataloader_2nd get_mask_tiles_1st Bottleneck torch.utils.data __len__ _save_mask_tiles_1st Dataset make_one_prediction_1st __init__ img2tensor _tile_resize_save _select_tiles_1st read_tiff torchvision.models.resnet load_image ", "entities": "(('WSI', 'formalin fixed paraffin embedded tissues'), 'divide') (('Eventually we', 'model'), 'need') (('Additionally we', 'training more dataset'), 'use') (('median area', '0'), '070') (('how shape', 'weight'), 'describe') (('This', 'CV'), 'give') (('Final submissions', 'robustness'), 'be') (('Such approach', 'fixed size'), 'increase') (('it', 'scores'), 'save') (('we', 'better performance'), 'be') (('when models', 'independently group'), 'demonstrate') (('training', '256x256'), 'update') (('we', 'improvement'), 'give') (('we', 'idea'), 'share') (('how we', 'predictions'), 'inferencein') (('that', 'groups'), 'be') (('section we', 'approach version'), 'description') (('we', 'inference'), 'be') (('we', 'ensembling'), 'use') (('us', 'tiles'), 'allow') (('You', 'cutmix'), 'see') (('scale', 'original images'), 'use') (('put', 'together result'), 'be') (('Segmentation mask tiles', 'following get_mask_tiles_1st functions'), 'generate') (('Generating', 'make_predictions_2nd get_mask_tiles_1st functions'), 'generate') (('goal', 'human kidneys'), 'be') (('we', '_ tile_resize_save function'), 'apply') (('group', 'information'), 'copy') (('we', 'kernel inference time'), 'use') (('_ generate_masks_1st', '_ generate_masks_2nd zero padded mask'), 'output') (('we', 'that'), 'reconstruct') (('inference', 'training'), 'inference') (('that', 'first pass'), 'combine') (('dimensions', 'tile size'), 'add') (('how naturally looked tiles', 'method'), 'see') (('Eventually obtained mask', 'RLE format'), 'convert') (('how we', 'predictions'), 'make') (('very we', 'tiles'), 'divide') (('Get boxes', 'rle https Eventually www'), 'save') (('PUBLIC_ONLY then we', 'test public data'), 'make') (('we', 'final submissions'), 'decide') (('load', 'NumPy MemMap'), 'be') (('Then we', 'test set'), 'generate') (('RLE conversion', 'https www'), 'do') (('SubmissionHere', 'generated masks'), 'be') (('they', 'WSI'), 'Slide') (('Additionally we', 'test public set'), 'observe') (('Image names', 'sample_submission'), 'retrieve') (('we', 'tile selection process'), 'say') (('we', 'tiff'), 'point') (('we', 'glomeruli'), 'InsightsThanks') (('load facebookresearch semi', 'dilatations m customized torch'), 'supervise') (('particular we', 'ratio median patient'), 'be') (('models', 'fresh even others'), 'trick') (('that', 'predictions'), 'base') (('that', 'close edges'), 'make') (('we', '_ reconstruct_img following function'), 'apply') (('second pass', 'selected tiles'), 'be') (('IntroductionThis kernel', 'HuBMAP competition 2021 version'), 'contain') (('height', 'saturation check print x1 x2 y1 y2 p.'), 'supervise') (('additional insights', 'notebook'), 'provide') (('we', 'test set'), 'be') (('we', 'finally approach'), 'want') (('DataLoader', 'HuBMAPTestDataset1st'), 'base') (('we', 'inference'), 'define') (('we', 'color saturations'), 'select') (('where glomeruli', 'first pass'), 'make') (('this', 'at least public LB'), 'discover') (('_ mask', 'original image'), 'after') (('we', 'that'), 'try') (('next step', 'padding'), 'be') (('second version', 'separate notebook'), 'describe') (('simplified method', 'first pixel'), 'aware') (('it', 'competition'), 'use') (('that', 'necessarily general population'), 'confirm') (('inference', 'https www'), 'remove') (('Generating', 'Segmentation MaskThe segmentation mask'), 'generate') (('Generate masks Reshape', 'cleaning'), 'shape') (('wasn t', 'inference time constraints'), '33') (('we', 'binarization even proper threshold'), 'stoch') (('we', 'test private data'), 'demonstrate') (('structure', 'better fresh frozen ones'), 'notice') (('com 256x256 images', 'Tiles selection https www'), 'iafoss') (('aforementioned DataLoader', '_ following make_tiles_dataloader_1st'), 'generate') (('ColorSpaced CutMix augs', '5 patient based CV'), 'set') "}