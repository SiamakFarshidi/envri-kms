{"name": "lab 1 introduction to ml ", "full_name": " h1 Creators h1 xsub train h2 Our xsub col choices h2 Notes Discussion h4 max vals h1 kNN Tests h1 Discussion h2 All cols and subsetting h2 kNN Results ", "stargazers_count": 0, "forks_count": 0, "description": "However it can be said that subsetting improved our performance with a good subset. When setting k 5 however or accuracy improved greatly. In general it seems that with this dataset subsetting the data can improve accuracy. For example at one point the initial split saw 98 accuracy. Class 0 was now classified 99 of the time and class 1 was classified 98 of the time. read_csv get rid of NaN entries select all entries then get from col1 and select up until the end exlcuding it select all entries then get col 10 selecting rows to increase accuracy eyeballed 96 accuracy experimenting getting bad accuracy came out with 92 cols 5 4 8 1 when k value is 1 when k value is 1. When k 1 it could be seen that the accuracy improve for classifying class 1 92 95 however the accuracy for finding class 0 dropped 100 97. In conclusion it looks like a nearest neighbor model with k 5 is the best approach for the provided dataset. linear algebra data processing CSV file I O e. Creators Morgan Dally 1313361 Reece Breebaart 1314828 xsub train Our xsub col choices nuclei size or shape these two will be related features so try either or adhesion single Notes Discussion Class 0 looks to be easier to distinguish from class 1 We therefore want to select in a way that makes class 1 features more prominent We experimented and eyeballed the accuaracy tried to determine which classes were detremental to perfomance it seemed that selecting columns with a dominant class provided the largest accuracy in the end we were able to make a subset that contradicted any results we found trying to find any patterns we were not able to determine good or bad individual columns we ended up with 5 2 3 4 as they improved accuracy max vals 1 139 2 373 5 3 346 4 393 3 5 376 4 seems to influence results badly contradicted as we made a good subset with it improving performance 6 402 2 7 161 8 432 1 kNN Tests DiscussionFrom the above tests our results were x_train 100 accuracy classifying class 0 85 accuracy classifying class 1 xsub_train 100 accuracy classifying class 0 92 accuracy classifying class 1 Nearest neighbor model k 1 97 accuracy classifying class 0 95 accuracy classifying class 1 Nearest neighbor model k 5 99 accuracy classifying class 0 98 accuracy classifying class 1 All cols and subsettingModifying the test and train set to only use a subset of columns saw varying results. After reloading the page the accuracy then dropped to around 95. kNN Results k 1 97 accuracy classifying class 0 95 accuracy classifying class 1 k 5 99 accuracy classifying class 0 98 accuracy classifying class 1Using the neighbor classifier model improved the performance over the original SVC model used. For more conclusive results we would need to test with a larger dataset. The largest accuracy difference was actually found by modifying the random_state value when initially splitting into the train and test sets. In both cases subsetting the data could improve the accuracy of classifying the classes moreso classifying class 1. This can be seen with the two subsets 5 code blocks up one is commented out. However it was possible to also degrade the performance of our model. In this case subsetting was able to improve the accuracy of classifying class 1 from 85 to 92 a significant difference. To compensate for this the random state used for splitting data has been pinned. ", "id": "mjdall/lab-1-introduction-to-ml", "size": "3265", "language": "python", "html_url": "https://www.kaggle.com/code/mjdall/lab-1-introduction-to-ml", "git_url": "https://www.kaggle.com/code/mjdall/lab-1-introduction-to-ml", "script": "seaborn print_predicted_stats classification_report train_test_split SVC confusion_matrix sklearn.model_selection pandas sklearn.metrics sklearn.svm numpy ", "entities": "(('random state', 'data'), 'pin') (('however accuracy', 'When 5'), 'set') (('we', 'larger dataset'), 'need') (('1 0 5 0 neighbor classifier Results k 97 accuracy classifying class 95 accuracy classifying class 1 k 99 accuracy classifying class 98 accuracy classifying 1Using model', 'SVC original model'), 'class') (('accuracy largest difference', 'when initially train sets'), 'find') (('k when value', '92 cols'), 'rid') (('accuracy', 'then around 95'), 'drop') (('initial split', '98 accuracy'), 'see') (('subsetting', 'significant difference'), 'be') (('This', 'one'), 'comment') (('However it', 'model'), 'be') (('Nearest neighbor 5 0 1 k 99 accuracy classifying class 98 accuracy classifying 1 cols', 'varying results'), 'dally') (('neighbor nearest model', 'best provided dataset'), 'look') (('dataset', 'accuracy'), 'seem') (('subsetting', 'classes moreso classifying class'), 'improve') (('subsetting', 'good subset'), 'say') (('class', '1 98 time'), 'classify') (('however accuracy', 'class'), 'see') "}