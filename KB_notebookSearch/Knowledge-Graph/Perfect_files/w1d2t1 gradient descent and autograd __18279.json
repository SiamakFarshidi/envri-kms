{"name": "w1d2t1 gradient descent and autograd ", "full_name": " h1 Tutorial 1 Gradient Descent and AutoGrad h1 Tutorial Objectives h2 Tutorial slides h1 Setup h2 Install dependencies h2 Figure settings h2 Plotting functions h2 Set random seed h2 Set device GPU or CPU Execute set device h1 Section 0 Introduction h2 Video 0 Introduction h1 Section 1 Gradient Descent Algorithm h2 Section 1 1 Gradients Steepest Ascent h3 Video 1 Gradient Descent h3 Analytical Exercise 1 1 Gradient vector Optional h4 Solution h3 Coding Exercise 1 1 Gradient Vector h4 Video 2 Gradient Descent Discussion h2 Section 1 2 Gradient Descent Algorithm h3 Analytical Exercise 1 2 Gradients h2 Section 1 3 Computational Graphs and Backprop h3 Video 3 Computational Graph h3 Analytical Exercise 1 3 Chain Rule Optional h4 Solution h1 Section 2 PyTorch AutoGrad h2 Video 4 Auto Differentiation h2 Section 2 1 Forward Propagation h3 Coding Exercise 2 1 Buiding a Computational Graph h2 Section 2 2 Backward Propagation h2 AUTOMATIC DIFFERENTIATION WITH TORCH AUTOGRAD h1 Section 3 PyTorch s Neural Net module nn Module h2 Video 5 PyTorch nn module h2 Section 3 1 Training loop in PyTorch h3 Coding Exercise 3 1 Training Loop h1 Summary h2 Video 6 Tutorial 1 Wrap up h2 Airtable Submission Link ", "stargazers_count": 0, "forks_count": 0, "description": "AutoGrad is PyTorch s automatic differentiation engine. backward on the tensor we wish to initiate the backpropagation from. backward is called on the loss which is the last node on the graph. Here we start by covering the essentials of AutoGrad and you will learn more in the coming days. Read more here https pytorch. For gradient descent it is only required to have the gradients of cost function with respect to the variables we wish to learn. parameters on the model. nn layers the weights and biases are already in requires_grad mode and will be registered as model parameters. PyTorch rebuilds the graph every time we iterate or change it or simply put PyTorch uses a dynamic graph. 3 Chain Rule Optional For the function above calculate the dfrac partial f partial y using the computational graph and chain rule. com NeuromatchAcademy course content dl tree main tutorials W1D2_LinearDeepLearning solutions W1D2_Tutorial1_Solution_6668feea. Coding Exercise 3. Gradient Descent is an iterative algorithm for minimizing the function f starting with an initial value for variables mathbf w taking steps of size eta learning rate in the direction of the negative gradient at the current point to update the variables mathbf w. Let s perform one training iteration. 2 is an example of how overwhelming the derivation of gradients can get as the number of variables and nested functions increases. Optimizer holds the current state of the model and by calling the step method will update the parameters based on the computed gradients. zero_grad on the loss or optimizer to zero out all. Parameter https pytorch. Starting with Gradient Descent the workhorse of deep learning algorithms in this tutorial. For more complex functions printing the grad_fn would only show the last operation even though the object tracks all the operations up to that point Now let s kick off the backward pass to calculate the gradients by calling. com NeuromatchAcademy course content dl tree main tutorials W1D2_LinearDeepLearning solutions W1D2_Tutorial1_Solution_5204c053. In PyTorch Tensor and Function are interconnected and build up an acyclic graph that encodes a complete history of computation. html A kind of Tensor that is to be considered a module parameter. forward inputs Compute the loss Perform backpropagation to build the graph and compute the gradients Optimizer takes a tiny step in the steepest direction negative of gradient and updates the weights and biases of the network keeping recods of loss Complete the function and remove or comment the line below set gradients to 0 Compute model prediction output Compute the loss Compute gradients backward pass update parameters optimizer takes a step add event to airtable Cauchy Exercices d analyse et de physique mathematique 1847 title Video 6 Tutorial 1 Wrap up add event to airtable title Airtable Submission Link. html AUTOGRAD MECHANICS https pytorch. Vanilla Algorithm inputs initial guess mathbf w 0 step size eta 0 number of steps T For t 0 2 dots T 1 do qquad mathbf w t 1 mathbf w t eta nabla f mathbf w t end return mathbf w t 1 Hence all we need is to calculate the gradient of the loss function with respect to the learnable parameters i. In this notebook we will cover the key concepts and ideas of Gradient descent PyTorch Autograd PyTorch nn module Tutorial slides These are the slides for the videos in this tutorial SetupThis a GPU Free tutorial Install dependencies Figure settings Plotting functions Set random seed Executing set_seed seed seed you are setting the seed Set device GPU or CPU. This is called the forward pass. The second tutorial will help us build a better intuition about neural networks and basic hyper parameters. Wide neural networks are thought to be really good at generalization. inform the user if the notebook uses GPU or CPU. 1 Training LoopUsing everything we ve learned so far we ask you to complete the train function below. Tutorial 1 Gradient Descent and AutoGrad Week 1 Day 2 Linear Deep Learning By Neuromatch Academy __Content creators __ Saeed Salehi Vladimir Haltakov Andrew Saxe__Content reviewers __ Polina Turishcheva Antoine De Comite Kelson Shilling Scrivo__Content editors __ Anoop Kulkarni Spiros Chavlis__Production editors __ Khalid Almubarak Spiros Chavlis Our 2021 Sponsors including Presenting Sponsor Facebook Reality Labs Tutorial ObjectivesDay 2 Tutorial 1 will continue on buiding PyTorch skillset and motivate its core functionality Autograd. begin equation mathbf w t 1 mathbf w t eta nabla f mathbf w t end equation where eta 0 and nabla f mathbf w left frac partial f mathbf w partial w_1. Video 6 Tutorial 1 Wrap up Airtable Submission Link title Tutorial slides markdown These are the slides for the videos in this tutorial title Install dependencies init airtable form Imports title Figure settings interactive display title Plotting functions title Set random seed markdown Executing set_seed seed seed you are setting the seed for DL its critical to set the random seed so that students can have a baseline to compare their results to expected results. org docs stable notes randomness. reguires_grad tensors are contagious. grad on the relevant tensors. Now let s start from f and work our way against the arrows while calculating the gradient of each expression as we go. In 1847 Augustin Louis Cauchy used negative of gradients to develop the Gradient Descent algorithm as an iterative method to minimize a continuous and ideally differentiable function of many variables. Since negative gradients always point locally in the direction of steepest descent the algorithm makes small steps at each point towards the minimum. 1 Gradients Steepest Ascent Video 1 Gradient DescentBefore introducing the gradient descent algorithm let s review a very important property of gradients. t weights we calculate the derivative w. html AUTOMATIC DIFFERENTIATION WITH TORCH. The training process in PyTorch is interactive you can perform training iterations as you wish and inspect the results after each iteration. py Example output We can see from the plot that for any given x_0 and y_0 the gradient vector left dfrac partial z partial x dfrac partial z partial y right top _ x_0 y_0 points in the direction of x and y for which z increases the most. Each variable has a grad_fn attribute that references a function that has created the Tensor except for Tensors created by the user these have None as grad_fn. backward https pytorch. For this exercise use the provided tensors to build the following graph which implements a single neuron with scalar input and output. the input nodes to the node of interest. Finally in tutorial 3 we learn about the learning dynamics what the a good deep network is learning and why sometimes they may perform poorly. In neural nets weights and biases are often the learnable parameters. org docs stable notes autograd. It is important to note that gradient vectors only see their local values not the whole landscape Also length size of each vector which indicates the steepness of the function can be very small near local plateaus i. To create a custom model parameter we can use nn. 1 Gradient vector Optional Given the following function begin equation z h x y sin x 2 y 2 end equation find the gradient vector begin equation begin bmatrix dfrac partial z partial x dfrac partial z partial y end bmatrix end equation hint use the chain rule Chain rule For a composite function F x g h x equiv g circ h x begin equation F x g h x cdot h x end equation or differently denoted begin equation frac dF dx frac dg dh frac dh dx end equation Solution We can rewrite the function as a composite function begin equation z f left g x y right f u sin u g x y x 2 y 2 end equation Using chain rule begin align dfrac partial z partial x dfrac partial f partial g dfrac partial g partial x cos g x y 2x cos x 2 y 2 cdot 2x dfrac partial z partial y dfrac partial f partial g dfrac partial g partial y cos g x y 2y cos x 2 y 2 cdot 2y end align Coding Exercise 1. It does so by calculating intermediate variables a b c d and e. com NeuromatchAcademy course content dl tree main tutorials W1D2_LinearDeepLearning solutions W1D2_Tutorial1_Solution_0c8e3872. The task is to train a wide nonlinear using tanh activation function neural net for a simple sin regression task. title Video 0 Introduction add event to airtable title Video 1 Gradient Descent add event to airtable Complete the function and remove or comment the line below add event to airtable title Video 2 Gradient Descent Discussion add event to airtable title Video 3 Computational Graph add event to airtable title Video 4 Auto Differentiation add event to airtable add event to airtable Complete the function and remove or comment the line below Complete the function and remove or comment the line below input tensor target tensor analytical gradients remember detaching first we should call the backward to build the graph we calculate the derivative w. Important Notes Learnable parameters i. 1 Buiding a Computational GraphIn PyTorch to indicate that a certain tensor contains learnable parameters we can set the optional argument requires_grad to True. org tutorials beginner basics autogradqs_tutorial. Click for solution https github. grad attributes see autograd. Optimizer PyTorch provides us with many optimization methods different versions of gradient descent. Generate the sample datasetLet s define a very wide 512 neurons neural net with one hidden layer and Tanh activation function. detach method on that tensor. If we build a neural network using torch. For training we need three things Model parameters Model parameters refer to all the learnable parameters of the model which are accessible by calling. weights begin equation dfrac partial Loss partial mathbf w left dfrac partial Loss partial w_1 dfrac partial Loss partial w_2. The gradient of a function always points in the direction of the steepest ascent. 2 Gradient Descent AlgorithmLet f mathbf w mathbb R d rightarrow mathbb R be a differentiable function. In case that DataLoader is used title Set device GPU or CPU. By breaking the computation into simple operations on intermediate variables we can use the chain rule to calculate any gradient begin equation dfrac partial f partial x dfrac partial f partial e dfrac partial e partial d dfrac partial d partial c dfrac partial c partial a dfrac partial a partial x left 1 tanh 2 e right cdot frac 1 d 1 cdot z cdot frac 1 b cdot 2 end equation Conveniently the values for e b and d are available to us from when we did the forward pass through the graph. Module Time estimate 30 mins Video 5 PyTorch nn modulePyTorch provides us with ready to use neural network building blocks such as layers e. html AUTOMATIC DIFFERENTIATION PACKAGE TORCH. 1 Forward PropagationEverything starts with the forward propagation pass. detach it from the graph by calling the. So how can we as well as PyTorch and similar frameworks approach such beasts Let s look at the function again begin equation f x y z tanh left ln left 1 z frac 2x sin y right right end equation We can build a so called computational graph shown below to break the original function into smaller and more approachable expressions. This is called the backward pass from which the backpropagation of errors algorithm gets its name. py It is important to appreciate the fact that PyTorch can follow our operations as we arbitrarily go through classes and functions. org docs stable generated torch. html Call set_seed function in the exercises to ensure reproducibility. Thus we can simply use the aforementioned formula to find the local minima. org docs stable autograd. Execute set_device especially if torch modules used. We have seen all of these using PyTorch modules and we compared the analytical solutions with the ones provided directly by the PyTorch module. PyTorch will then track every operation using this tensor while configuring the computational graph. These variables are often called learnable trainable parameters or simply parameters in PyTorch. 2 Backward PropagationHere is where all the magic lies. Solution begin equation dfrac partial f partial y dfrac partial f partial e dfrac partial e partial d dfrac partial d partial c dfrac partial c partial b dfrac partial b partial y left 1 tanh 2 e right cdot frac 1 d 1 cdot z cdot frac a b 2 cdot cos y end equation For more Calculus on Computational Graphs Backpropagation https colah. backward accumulates gradients in the leaf nodes i. org tutorials beginner blitz autograd_tutorial. Starting from x y and z and following the arrows and expressions you would see that our graph returns the same function as f. Replace with other single operations e. We can then compare it to PyTorch gradients which can be obtained by calling. You can use the command dir my_object to observe all variables and associated methods to your object e. c a b or c torch. 1 Training loop in PyTorchWe use a regression problem to study the training loop in PyTorch. The example below shows that the tensor c a b is created by the Add operation and the gradient function is the object. The following exercise will help clarify this. This code block is the core of everything to come please make sure you go line by line through all the commands and discuss their purpose with the pod. html Section 3 PyTorch s Neural Net module nn. AUTOGRAD https pytorch. Please note that NOT all the requires_grad tensors are seen as model parameters. Coding Exercise 2. dfrac partial Loss partial w_d right top end equation Analytical Exercise 1. Let s look at a simple example Y W X where X is the feature tensors and W is the weight tensor learnable parameters reguires_grad the newly generated output tensor Y will be also reguires_grad. Before doing that let s calculate the loss gradients by hand frac partial loss partial w 2 x y_t y_p 1 y_p 2 frac partial loss partial b 2 y_t y_p 1 y_p 2 Where y_t is the target true label and y_p is the prediction model output. Video 0 Introduction Section 1 Gradient Descent Algorithm Time estimate 30 45 mins Since the goal of most learning algorithms is minimizing the risk also known as the cost or loss function optimization is often the core of most machine learning techniques The gradient descent algorithm along with its variations such as stochastic gradient descent is one of the most powerful and popular optimization methods used for deep learning. Today we will introduce the basics but you will learn much more about Optimization in the coming days Week 1 Day 4. That is the partial derivatives have simple expressions in terms of the intermediate variables a b c d e that we calculated and stored during the forward pass. io posts 2015 08 Backprop Section 2 PyTorch AutoGrad Time estimate 30 45 mins Video 4 Auto DifferentiationDeep learning frameworks such as PyTorch JAX and TensorFlow come with a very efficient and sophisticated set of algorithms commonly known as Automatic Differentiation. 3 Computational Graphs and Backprop Video 3 Computational Graph Exercise 1. Video 2 Gradient Descent Discussion Section 1. Execute set_device Section 0 IntroductionToday we will go through 3 tutorials. Analytical Exercise 1. 2 GradientsGiven f x y z tanh left ln left 1 z frac 2x sin y right right how easy is it to derive dfrac partial f partial x dfrac partial f partial y and dfrac partial f partial z hint you don t have to actually calculate them Section 1. Therefore if we need to plot or store a tensor that is reguires_grad we must first. Recall that in python we can access variables and associated methods with. different activation and loss functions and much more packed in the torch. sin a and examine the results. This function is still extraordinarily simple compared to the loss functions of modern neural networks. py Example output SummaryIn this tutorial we covered one of the most basic concepts of deep learning the computational graph and how a network learns via gradient descent and the backpropagation algorithm. References and more A GENTLE INTRODUCTION TO TORCH. You will learn more details about choosing the right model architecture loss function and optimizer later in the course. We can now create an instance of our neural net and print its parameters. t bias title Video 5 PyTorch nn module add event to airtable markdown Generate the sample dataset creating an instance Create a mse loss function Stochstic Gradient Descent optimizer you will learn about momentum soon learning rate Reset all gradients to zero Forward pass Compute the output of the model on the features inputs like wide_net. So any operation that is applied to Y will be part of the computational graph. You can run the cell multiple times and see how the parameters are being updated and the loss is reducing. 1 Gradient VectorImplement complete the function which returns the gradient vector for z sin x 2 y 2. PyTorch tracks all the instructions as we declare the variables and operations and it builds the graph when we call the. frac partial f mathbf w partial w_d right. Loss function The loss that we are going to be optimizing which is often combined with regularization terms conming up in few days. ", "id": "joseguzman/w1d2t1-gradient-descent-and-autograd", "size": "18279", "language": "python", "html_url": "https://www.kaggle.com/code/joseguzman/w1d2t1-gradient-descent-and-autograd", "git_url": "https://www.kaggle.com/code/joseguzman/w1d2t1-gradient-descent-and-autograd", "script": "math set_device display as IPyDisplay mpl_toolkits.axes_grid1 IPython.display fun_z seed_worker numpy sq_loss WideNet(nn.Module) train ex3_plot set_seed BiliVideo(IFrame) nn IPython evaltools.airtable AirtableForm ex1_plot matplotlib.pyplot IFrame forward ipywidgets SimpleGraph pi display widgets __init__ fun_dz torch make_axes_locatable YouTubeVideo ", "entities": "(('which', 'PyTorch gradients'), 'compare') (('It', 'b c d'), 'do') (('nabla mathbf 0 w', 'f mathbf w partial partial w_1'), 'begin') (('we', 'variables'), 'recall') (('torch especially modules', 'set_device'), 'Execute') (('you', 'seed'), 'cover') (('we', 'True'), '1') (('end right right We', 'smaller more expressions'), 'approach') (('we', 'variables'), 'require') (('weight learnable parameters', 'output newly generated tensor'), 'let') (('you', 'iteration'), 'be') (('that', 'computation'), 'interconnect') (('function', 'modern neural networks'), 'be') (('We', 'parameters'), 'create') (('30 Auto DifferentiationDeep learning 45 mins Video 4 frameworks', 'Automatic commonly Differentiation'), 'posts') (('Execute set_device 0 IntroductionToday we', '3 tutorials'), 'Section') (('notebook', 'GPU'), 'inform') (('which', 'plateaus very local i.'), 'be') (('Gradient Descent', 'mathbf w.'), 'be') (('why sometimes they', 'what'), 'learn') (('PyTorch', 'computational graph'), 'track') (('which', 'last graph'), 'call') (('we', 'PyTorch directly module'), 'see') (('requires_grad tensors', 'model parameters'), 'note') (('gradient function', 'Add c operation'), 'show') (('when we', 'graph'), 'use') (('when we', 'the'), 'track') (('gradient', 'steepest ascent'), 'point') (('gradient descent', 'deep learning'), 'section') (('us', 'neural networks'), 'help') (('that', 'html Tensor'), 'kind') (('we', 'learnable parameters'), 'input') (('w', 'dfrac partial Loss partial w_1 dfrac partial Loss partial w_2'), 'begin') (('we', 'tensor'), 'first') (('these', 'grad_fn'), 'have') (('students', 'expected results'), 'wrap') (('you', 'coming days'), 'introduce') (('true y_p', 'hand frac partial loss'), 'let') (('you', 'coming days'), 'start') (('DataLoader', 'case'), 'in') (('Training 1 loop', 'PyTorch'), 'use') (('variables', 'often learnable trainable simply PyTorch'), 'call') (('Forward 1 PropagationEverything', 'propagation forward pass'), 'start') (('You', 'associated object'), 'use') (('algorithm', 'minimum'), 'make') (('You', 'later course'), 'learn') (('you', 'pod'), 'be') (('task', 'sin regression simple task'), 'be') (('weights', 'model parameters'), 'be') (('Chain Rule 3 function', 'computational graph rule'), 'Optional') (('output', 'wide_net'), 'add') (('Optimizer PyTorch', 'gradient descent'), 'provide') (('different activation functions', 'much more torch'), 'packed') (('Optimizer', 'computed gradients'), 'hold') (('how network', 'gradient descent'), 'output') (('loss', 'cell'), 'run') (('cdot z 1 cdot', 'Computational Graphs Backpropagation https colah'), 'begin') (('1847 title Video 6 Tutorial 1 Wrap', 'title Airtable Submission airtable Link'), 'input') (('we', 'expression'), 'let') (('which', 'few days'), 'function') (('z dfrac partial f partial you', 'don actually them'), 'leave') (('that', 'computational graph'), 'be') (('so far we', 'train function'), '1') (('nn Video 5 modulePyTorch', 'layers such e.'), 'estimate') (('number', 'variables'), 'be') (('s', 'gradients'), '1') (('following exercise', 'this'), 'help') (('ObjectivesDay 2 Tutorial', 'core functionality'), 'Descent') (('which', 'scalar input'), 'use') (('we', 'arbitrarily classes'), 'py') (('Augustin Louis Cauchy', 'many variables'), 'use') (('Thus we', 'local minima'), 'use') (('we', 'backpropagation'), 'backward') (('z', 'most'), 'py') (('Wide neural networks', 'really generalization'), 'think') (('we', 'nn'), 'use') (('Now s', 'gradients'), 'show') (('we', 'torch'), 'build') (('backpropagation', 'name'), 'call') (('b c d we', 'forward pass'), 'be') (('which', 'z sin'), 'complete') (('frac', 'f partial w partial right'), 'mathbf') (('right f u end u g y 2 2 equation', 'align dfrac partial z partial dfrac partial f partial g dfrac partial g partial'), 'begin') (('html AUTOMATIC DIFFERENTIATION', 'TORCH'), 'package') (('we', 'derivative w.'), 'add') (('which', 'model'), 'need') (('graph', 'f.'), 'see') (('time we', 'dynamic graph'), 'rebuild') "}