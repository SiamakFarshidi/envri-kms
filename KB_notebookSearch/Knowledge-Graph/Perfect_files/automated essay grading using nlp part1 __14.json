{"name": "automated essay grading using nlp part1 ", "full_name": " h3 The Automated Student Assessment Prize s ASAP Dataset sponsored by Hewitt Packard comprised of 13 000 essays 8 different datasets of different genre Each dataset was a collection of responses to its own prompt Each essay set has it s own rubric marking scheme to decide the final score of the essay Each essay has one or more human scores and a final resolved score h3 Part 1 Text Preprocessing on the Data h1 Preprocessing the Data h4 A Regular Expression is a text string that describes a search pattern which can be used to match or replace patterns inside a string with a minimal amount of code h3 Currently I am learning NLP by practicing it on various datasets This is all I have done by far in text preprocessing If there are certain things that should be changed updated or I have missed out please comment and let me know Also If there could be some inputs on Semantic Analysis like what should be the approach and algorithms that could be used for Semantic Analysis please share h3 Thankyou ", "stargazers_count": 0, "forks_count": 0, "description": "The Automated Student Assessment Prize s ASAP Dataset sponsored by Hewitt Packard comprised of 13 000 essays 8 different datasets of different genre. Cleaning the Data using regex function2. Importing Libraries Importing Data Finding the number of records for each column for each of the eight essay sets to know that data is consistent. Each essay has one or more human scores and a final resolved score Part 1 Text Preprocessing on the Data Preprocessing the Data1. Cleaning the text using regex function removing url remove numbers and lowercase the text Eg caps1 will be removed remove punctuation After cleaning the data Here we are using ascii encoding on the string ignoring the ones that can t be converted and then again decoding it. Tokenization Word Tokenize and Sentence Tokenize 3. Hence count of lemmas is less than count of noun adj verb and adverb. This is all I have done by far in text preprocessing If there are certain things that should be changed updated or I have missed out please comment and let me know. create a list of tuples Only for essay_set 1. Orthography Spelling Mistakes and Punctuation A Regular Expression is a text string that describes a search pattern which can be used to match or replace patterns inside a string with a minimal amount of code. Each essay set has it s own rubric marking scheme to decide the final score of the essay. Each dataset was a collection of responses to its own prompt. Lemmatization using POS_Tagging5. Numeric Features like word_count char_count sentence_count etc. Calculating number of nouns adjectives verbs and adverbs in an essay this will give the real count. Also If there could be some inputs on Semantic Analysis like what should be the approach and algorithms that could be used for Semantic Analysis please share. Tokenizing the sentences to words For Splitting sentences in the paragraph using PunktSentenceTokenizer Data After tokenizing Data after removing stopwords After removing the words having length 1 calculating number of words in an essay equivalent to a zA Z0 9 Calculating the number of characters matches a single whitespace character space newline return tab form Number of sentences using sent_tokenize to convert paragraph into sentences Average word length Lemmatization using POS Tagging Here we are appending all the POS tags in the lemma create list of tuples Lemmatizing the wprds We only need POS_tags not the word the words that are neither of above are by default tagged as NOUN. Currently I am learning NLP by practicing it on various datasets. ", "id": "sakshisaku3000/automated-essay-grading-using-nlp-part1", "size": "14", "language": "python", "html_url": "https://www.kaggle.com/code/sakshisaku3000/automated-essay-grading-using-nlp-part1", "git_url": "https://www.kaggle.com/code/sakshisaku3000/automated-essay-grading-using-nlp-part1", "script": "average_word_length extract_features count_pos count_lemmas nltk.tokenize numpy seaborn clean_length char_count nltk.stem remove_stopwords convert_essay_to_wordlist decode_essay wordnet process_text nltk pandas spellchecker sent_count WordNetLemmatizer stopwords tokenize_essay nltk.corpus SpellChecker matplotlib pos_tag spell_count word_tokenize word_count ", "entities": "(('t', 'then again it'), 'clean') (('Currently I', 'various datasets'), 'learn') (('me', 'by far text'), 'be') (('Hence count', 'noun adj verb'), 'be') (('Text final resolved Part 1 Preprocessing', 'Data1'), 'have') (('that', 'Semantic Analysis'), 'share') (('this', 'real count'), 'adjective') (('data', 'essay eight sets'), 'import') (('it', 'essay'), 'have') (('ASAP Dataset', 'different genre'), 's') (('dataset', 'own prompt'), 'be') (('that', 'NOUN'), 'tokenize') (('which', 'code'), 'be') "}