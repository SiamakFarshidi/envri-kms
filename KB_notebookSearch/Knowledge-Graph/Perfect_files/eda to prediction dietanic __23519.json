{"name": "eda to prediction dietanic ", "full_name": " h1 EDA To Prediction DieTanic h3 Ashwin h3 Sometimes life has a cruel sense of humor giving you the thing you always wanted at the worst time possible h2 Contents of the Notebook h4 Part1 Exploratory Data Analysis EDA h4 Part2 Feature Engineering and Data Cleaning h4 Part3 Predictive Modeling h2 Part1 Exploratory Data Analysis EDA h3 How many Survived h2 Types Of Features h3 Categorical Features h3 Ordinal Features h3 Continous Feature h2 Analysing The Features h2 Sex Categorical Feature h2 Pclass Ordinal Feature h2 Age Continous Feature h4 Observations h3 Filling NaN Ages h3 Observations h2 Embarked Categorical Value h3 Chances for Survival by Port Of Embarkation h3 Observations h3 Observations h3 Filling Embarked NaN h2 SibSip Discrete Feature h3 Observations h2 Parch h3 Observations h2 Fare Continous Feature h2 Observations in a Nutshell for all features h2 Correlation Between The Features h3 Interpreting The Heatmap h2 Part2 Feature Engineering and Data Cleaning h2 Age band h4 Problem With Age Feature h2 Family Size and Alone h2 Fare Range h2 Converting String Values into Numeric h3 Dropping UnNeeded Features h1 Part3 Predictive Modeling h3 Radial Support Vector Machines rbf SVM h3 Linear Support Vector Machine linear SVM h3 Logistic Regression h3 Decision Tree h3 K Nearest Neighbours KNN h3 Gaussian Naive Bayes h3 Random Forests h1 Cross Validation h2 Confusion Matrix h3 Interpreting Confusion Matrix h3 Hyper Parameters Tuning h4 SVM h4 Random Forests h1 Ensembling h2 Voting Classifier h2 Bagging h4 Bagged KNN h4 Bagged DecisionTree h2 Boosting h4 AdaBoost Adaptive Boosting h4 Stochastic Gradient Boosting h4 XGBoost h4 Hyper Parameter Tuning for AdaBoost h3 Confusion Matrix for the Best Model h2 Feature Importance h4 Observations h3 Thanks a lot for having a look at this notebook If you found this notebook useful Do Upvote ", "stargazers_count": 0, "forks_count": 0, "description": "Correlation Between The Features Interpreting The HeatmapThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. If you found this notebook useful Do Upvote. It is evident that irrespective of Pclass Women were given first priority while rescue. 2 Survival chances for Passenegers aged 20 50 from Pclass1 is high and is even better for Women. Due to the averaging there is reduction in variance. Fare We have the Fare_cat feature so unneeded Cabin A lot of NaN values and also many passengers have multiple cabins. A value 1 means perfect negative correlation. SVM Random ForestsThe best score for Rbf Svm is 82. So lets divide the range from 0 80 into 5 bins. If You Like the notebook and think that it helped you. The chances of survival is good for somebody who has 1 3 parents on the ship. 3 For males the survival chances decreases with an increase in age. But with that we cannot accurately predict or tell whether a passenger will survive or die. So what it does is it looks for strings which lie between A Z or a z and followed by a. NEGATIVE CORRELATION If an increase in feature A leads to decrease in feature B then they are negatively correlated. 3 Maximum number of deaths were in the age group of 30 40. Lets check survival rate with Sex and Pclass Together. But we can clearly see that Passenegers Of Pclass 1 were given a very high priority while rescue. To overcome this and get a generalized model we use Cross Validation. Now this is problematic. It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family. Now this iterative process continous and new classifers are added to the model until the limit is reached on the accuracy. For this we will use pandas. As this is also continous we can convert into discrete values by using binning. The number of men on the ship is lot more than the number of women. Observations in a Nutshell for all features Sex The chance of survival for women is high as compared to men. Alone will denote whether a passenger is alone or not. Also we will tranform the existing relevant features to suitable form for Predictive Modeling. Dropping UnNeeded Features Name We don t need name feature as it cannot be converted into any categorical value. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. A value 1 means perfect positive correlation. Okay so there are some misspelled Initials like Mlle or Mme that stand for Miss. Now in the next iteration the learner will focus more on the wrongly predicted instances or give more weight to it. So this is a useless feature. Such a materialistic world. Majority of them being from Pclass3. BaggingBagging is a general ensemble method. Thus it has made more mistakes by predicting dead as survived. We use FactorPlot in this case because they make the seperation of categorical values easy. Lets analyse other features. Following are the algorithms I will use to make the model 1 Logistic Regression2 Support Vector Machines Linear and radial 3 Random Forest4 K Nearest Neighbours5 Naive Bayes6 Decision Tree7 Logistic Regression Radial Support Vector Machines rbf SVM Linear Support Vector Machine linear SVM Logistic Regression Decision Tree K Nearest Neighbours KNN Now the accuracy for the KNN model changes as we change the values for n_neighbours attribute. We had already seen the positive correlation between Sex and Initial so they both refer to the gender. Clearly if you are alone or family_size 0 then chances for survival is very low. 5 million to build the Titanic and it sunk under the ocean due to collision. I will replace them with Miss and same thing for other values. The graph roughly decreases if the number of siblings increase. 2 Errors Wrongly Classified 58 dead people as survived and 95 survived as dead. Stochastic Gradient BoostingHere too the weak learner is a Decision Tree. For women the chance of survival from Pclass1 is almost 1 and is high too for those from Pclass2. e children looks to be good irrespective of the Pclass. The Women and Child first policy thus holds true irrespective of the class. Pclass Ordinal FeaturePeople say Money Can t Buy Everything. Family_Size and AloneAt this point we can create a new feature called Family_size and Alone and analyse it. We should convert it into singleton values same as we did in Age_Band Clearly as the Fare_cat increases the survival chances increases. An example for this can be Decision Tree or Random Forests. How do we check features how do we add new features and some Machine Learning Concepts. Contents of the Notebook Part1 Exploratory Data Analysis EDA 1 Analysis of the features. com pliptor divide and conquer 0 82297 notebook 2 For Python Pytanic by Heads and Tails https www. It is imminent that all the large families in Pclass3 3 died. The survival rate for Pclass3 is very low. 2 Port S looks to be very unlucky for Pclass3 Passenegers as the survival rate for both men and women is very low. Still the chances for survival is low here that is because many passengers from Pclass3 around 81 didn t survive. Confusion MatrixIt gives the number of correct and incorrect classifications made by the classifier. I will be using binning i. Ticket It is any random string that cannot be categorised. 3 Converting features into suitable form for modeling. Gaussian Naive Bayes Random ForestsThe accuracy of a model is not the only factor that determines the robustness of the classifier. So then we can make a strong judgement about a single product after analysing all different parameters. For eg If we have a feature like Height with values Tall Medium Short then Height is a ordinal variable. That is if I have a family on board I will try to save them instead of saving myself first. 8 which we did get earlier. 2 The oldest Passenger was saved 80 years. Now if I say to group them by their Age then how would you do it If there are 30 Persons there may be 30 age values. This is known as MultiColinearity as both of them contains almost the same information. Out of 891 passengers in training set only around 350 survived i. Filling NaN Ages Observations 1 The Toddlers age 5 were saved in large numbers The Women and Child First Policy. But is it the best Lets check other features. Observations 1 The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Pclass. Now we cannot pass the Fare_Range values as it is. com arthurtok introduction to ensembling stacking in python Thanks a lot for having a look at this notebook. The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S. Lets say we want to buy a phone and ask many people about it based on various parameters. But we can change the dafault base_estimator to any algorithm of our choice. We will try to increase it with Hyper Parameter Tuning Hyper Parameter Tuning for AdaBoostThe maximum accuracy we can get with AdaBoost is 83. Thus we should train and test our algorithm on each and every instance of the dataset. Here we can have a relative sort in the variable. Before understanding the plot let us see what exactly correlation is. Thus with cross validation we can achieve a generalised model. This is known as Hyper Parameter Tuning. It is a step by step enhancement of a weak model. For example gender is a categorical variable having two categories male and female. As the training and testing data changes the accuracy will also change. It will keep me motivated. 3 The Embark S looks to the port from where majority of the rich people boarded. Ensembling can be done in ways like 1 Voting Classifier2 Bagging3 Boosting. SibSip Discrete FeatureThis feature represents whether a person is alone or with his family members. We can use KNN with small value of n_neighbours as small value of n_neighbours. Looking at the CrossTab and the FactorPlot we can easily infer that survival for Women from Pclass1 is about 95 96 as only 3 out of 94 Women from Pclass1 died. Cross ValidationMany a times the data is imbalanced i. So money and status matters. So if we pass for 5 bins it will arrange the values equally spaced into 5 seperate bins or value ranges. In simple words it is the combination of various simple models to create a single powerful model. 3 Similarly the Pclass and Fare_cat refer to the status of the passengers and Family_Size with Alone Parch and SibSp. First let us understand the different types of features. This feature is the summation of Parch and SibSp. Voting ClassifierIt is the simplest way of combining predictions from many different simple machine learning models. Even though the the number of Passengers in Pclass 3 were a lot higher still the number of survival from them is very low somewhere around 25. Lisa Kleypas The sinking of the Titanic is one of the most infamous shipwrecks in history. This means that both the features are containing highly similar information and there is very little or no variance in information. This is called K Fold Cross Validation. e there may be a high number of class1 instances but less number of other class instances. of correct predictions are 491 for dead 247 for survived with the mean CV accuracy being 491 247 891 82. Lets Dive in little bit more and check for other interesting observations. Okay so the maximum age of a passenger was 80. Now the model will get some instances right while some wrong. Categorical Features in the dataset Sex Embarked. e group a range of ages into a single bin or assign them a single value. Ordinal Features in the dataset PClass Continous Feature A feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column. Being alone also proves to be fatal and the chances for survival decreases when somebody has 4 parents on the ship. Age We have the Age_band feature so no need of this. com headsortails pytanic 3 For Python Introduction to Ensembling Stacking by Anisotropic https www. 2 Removing redundant features. Sibling brother sister stepbrother stepsisterSpouse husband wife Observations The barplot and factorplot shows that if a passenger is alone onboard with no siblings he have 34. For RandomForest score is abt 81. It gives an average prediction result based on the prediction of all the submodels. Fare Continous FeatureThe lowest fare is 0. So we successfully extract the Initials from the Name. Lets check the accuracies over various values of n_neighbours. Still the number of women saved is almost twice the number of males saved. 4 of the total training set survived the crash. 2 The Sex feature doesn t seem to give any importance which is shocking as we had seen earlier that Sex combined with Pclass was giving a very good differentiating factor. They are also known as Nominal Variables. PassengerId Cannot be categorised. For family size 4 the chances decrease too. It however reduces as the number goes up. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it. Continous Features in the dataset Age Analysing The Features Sex Categorical FeatureThis looks interesting. On April 15 1912 during her maiden voyage the Titanic sank after colliding with an iceberg killing 1502 out of 2224 passengers and crew. Lets consider the first plot for rbf SVM 1 The no. Fare_Range We have the fare_cat feature. 4 Port Q had almost 95 of the passengers were from Pclass3. Passengers between age group 15 to 35 died a lot. Age Continous Feature Observations 1 The number of children increases with Pclass and the survival rate for passenegers below Age 10 i. Then we can take an average of all the noted accuracies over the dataset. I will try to fix them. The crosstab shows that Person with SibSp 3 were all in Pclass3. 16 with n_estimators 200 and learning_rate 0. Bagged KNNBagging works best with models with high variance. For Pclass 1 survived is around 63 while for Pclass2 is around 48. Money Matters 3 Port Q looks looks to be unlukiest for Men as almost all were from Pclass 3. This is a very unforgetable disaster that no one in the world can forget. This is known as model variance. 55 while it is lowest for S. 2 The Passengers from C look to be lucky as a good proportion of them survived. Eg If I say to group or arrange Sports Person by Sex We can easily segregate them by Male and Female. 05 Confusion Matrix for the Best Model Feature ImportanceWe can see the important features for various classifiers like RandomForests AdaBoost etc. Filling Embarked NaNAs we saw that maximum passengers boarded from Port S we replace NaN with S. Parch SibSp Having 1 2 siblings spouse on board or 1 3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with you. This is Ensembling which improves the stability of the model. This looks to be a very important feature for modeling. 2 Let s say we divide the dataset into k 5 parts. We will tune the hyper parameters for the 2 best classifiers i. As we had seen earlier the Age feature has 177 null values. We can get a summarized result with the help of confusion matrix which shows where did the model go wrong or which class did the model predict wrong. XGBoostWe got the highest accuracy for AdaBoost. So do you think we should use both of them as one of them is redundant. There maybe be many redundant features which should be eliminated. Hyper Parameters TuningThe machine learning models are like a Black Box. The default value is 5. Ordinal Features An ordinal variable is similar to categorical values but the difference between them is that we can have relative ordering or sorting between the values. Observations 1 Some of the common important features are Initial Fare_cat Pclass Family_Size. AdaBoost Adaptive Boosting The weak learner or estimator in this case is a Decsion Tree. However we can see the feature Initial which is at the top in many classifiers. Some of them being SibSp andd Family_Size and Parch and Family_Size and some negative ones like Alone and Family_Size. Surprisingly the survival for families with 5 8 members is 0. Observations 1 Maximum passenegers boarded from S. Part2 Feature Engineering and Data Cleaning 1 Adding any few features. Observations Here too the results are quite similar. 2 Finding any relations or trends considering multiple features. EnsemblingEnsembling is a good way to increase the accuracy or performance of a model. Thus it will try to predict the wrong instance correctly. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers. There are some default parameter values for this Black Box which we can tune or change to get a better model. The survival rates for a women on the ship is around 75 while that for men in around 18 19. Lets see if we can get any new features and eliminate a few. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Now the above correlation plot we can see some positively related features. This feature may become an important feature during modeling along with the Sex. POSITIVE CORRELATION If an increase in feature A leads to increase in feature B then they are positively correlated. Family_Size 0 means that the passeneger is alone. Like the C and gamma in the SVM model and similarly different parameters for different classifiers are called the hyper parameters which we can tune to change the learning rate of the algorithm and get a better model. Age_band Problem With Age Feature As I have mentioned earlier that Age is a continous feature there is a problem with Continous Variables in Machine Learning Models. The submodels or the basemodels are all of diiferent types. We will try to check the survival rate by using the different features of the dataset. Unlike Voting Classifier Bagging makes use of similar classifiers. There looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the standards reduces. Now lets say that two features are highly or perfectly correlated so the increase in one leads to increase in the other. So now we will predict the whether the Passenger will survive or not using some great Classification Algorithms. The accuracies and errors are then averaged to get a average accuracy of the algorithm. e the SVM and RandomForests. Part1 Exploratory Data Analysis EDA The Age Cabin and Embarked have null values. Part3 Predictive Modeling1 Running Basic Algorithms. That s why the name DieTanic. Lets examine this further. Fare_RangeSince fare is also a continous feature we need to convert it into ordinal value. As discussed above we can clearly see that as the fare_range increases the chances of survival increases. To replace these NaN values we can assign them the mean age of the dataset. 1 The K Fold Cross Validation works by first dividing the dataset into k subsets. I hope all of you did gain some insights to Machine Learning. Thus we can assign the mean values of Mr and Mrs to the respective groups. 4 An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Some other great notebooks for Machine Learning are 1 For R Divide and Conquer by Oscar Takeshita https www. We just cant assign a 4 year kid with the mean age that is 29 years. Looking upon the feature we can see that the names have a salutation like Mr or Mrs. The classification accuracy can be sometimes misleading due to imbalance. The reason may be Pclass The reason is Pclass. Passengers at Q were all from Pclass3. Interpreting Confusion MatrixThe left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong prredictions made. The highest correlation is between SibSp and Parch i. So we can carry on with all features. What s In A Name Feature pOkay so here we are using the Regex A Za z. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn t. Pclass There is a visible trend that being a 1st class passenger gives you better chances of survival. Now this seems to be very good accuracy for a classifier but can we confirm that it will be 90 for all the new test sets that come over. We reserve 1 part for testing and train the algorithm over the 4 parts. Now from the above heatmap we can see that the features are not much correlated. This also looks to be an important feature for the model. Boosting works as follows A model is first trained on the complete dataset. How many Survived It is evident that not many passengers survived the accident. Part2 Feature Engineering and Data CleaningNow what is Feature Engineering Whenever we are given a dataset with features it is not necessary that all the features will be important. An example would be getting the Initals feature using the Name Feature. Now we cannot sort or give any ordering to such variables. Embarked This is a very interesting feature. We need to convert these continous values into categorical values by either Binning or Normalisation. Looks like Pclass is also an important feature. ParchThe crosstab again shows that larger families were in Pclass3. The answer is No because we can t determine which all instances will the classifier will use to train itself. Let s say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90. 8 with n_estimators 900. But the problem is there were many people with many different ages. Also we can get or add new features by observing or extracting information from other features. 4 Important Features Extraction. Sex looks to be important only in RandomForests. Even Men from Pclass1 have a very low survival rate. 3 We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. Some of the features being Sex Port Of Embarcation Age etc. The Objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. Part3 Predictive ModelingWe have gained some insights from the EDA part. Types Of Features Categorical Features A categorical variable is one that has two or more categories and each value in that feature can be categorised by them. EDA To Prediction DieTanic Ashwin Sometimes life has a cruel sense of humor giving you the thing you always wanted at the worst time possible. By looking at all the matrices we can say that rbf SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived. Bagged DecisionTree BoostingBoosting is an ensembling technique which uses sequential learning of classifiers. the survival rate decreases as the age increases irrespective of the Pclass. It may increase or decrease. So what qcut does is it splits or arranges the values according the number of bins we have passed. While making or training models we should try to eliminate redundant features as it reduces training time and many such advantages. Is there any way to find out what age band does the passenger lie Bingo we can check the Name feature. Passengers with their parents onboard have greater chance of survival. Converting String Values into NumericSince we cannot pass strings to a machine learning model we need to convert features loke Sex Embarked etc into numeric values. Wow a free luxorious ride. Age Children less than 5 10 years do have a high chance of survival. Embarked Categorical Value Chances for Survival by Port Of EmbarkationThe chances for survival for Port C is highest around 0. ", "id": "ash316/eda-to-prediction-dietanic", "size": "23519", "language": "python", "html_url": "https://www.kaggle.com/code/ash316/eda-to-prediction-dietanic", "git_url": "https://www.kaggle.com/code/ash316/eda-to-prediction-dietanic", "script": "sklearn.ensemble xgboost numpy seaborn GaussianNB #Naive bayes svm #support vector Machine cross_val_predict #prediction metrics #accuracy measure BaggingClassifier train_test_split #training and testing data split sklearn.neighbors sklearn.naive_bayes sklearn.tree GradientBoostingClassifier sklearn.linear_model sklearn cross_val_score #score evaluation DecisionTreeClassifier #Decision Tree matplotlib.pyplot confusion_matrix #for confusion matrix KFold #for K-fold cross validation sklearn.model_selection pandas KNeighborsClassifier #KNN LogisticRegression #logistic regression VotingClassifier AdaBoostClassifier GridSearchCV sklearn.metrics RandomForestClassifier #Random Forest ", "entities": "(('notebook', 'Upvote'), 'find') (('it', 'features column'), 'say') (('all', 'Machine Learning'), 'hope') (('learner', 'it'), 'focus') (('Surprisingly survival', '5 8 members'), 'be') (('names', 'Mr'), 'see') (('classification accuracy', 'sometimes imbalance'), 'be') (('Sex', 'only RandomForests'), 'look') (('example', 'this'), 'be') (('3 lot still number', 'very somewhere 25'), 'be') (('4 chances', 'family size'), 'decrease') (('3 Maximum number', '30 40'), 'be') (('one', 'world'), 'be') (('ensembling which', 'classifiers'), 'be') (('Also we', 'other features'), 'get') (('Lets', 'rbf SVM'), 'consider') (('who', 'correctly passengers'), 'have') (('Voting Classifier Bagging', 'similar classifiers'), 'make') (('feature', 'important Sex'), 'become') (('Port Money Matters 3 Q', 'almost all Pclass'), 'look') (('good proportion', 'them'), '2') (('Women first policy', 'class'), 'hold') (('Age Continous Feature 1 number', 'Age'), 'observation') (('seperation', 'categorical values'), 'use') (('model', 'confusion matrix'), 'get') (('it', 'Fare_Range values'), 'pass') (('They', 'Nominal also Variables'), 'know') (('where majority', 'rich people'), '3') (('Thus it', 'wrong instance'), 'try') (('then how you', 'it'), 'do') (('rather large family', 'you'), 'show') (('which', 'many classifiers'), 'see') (('value', '1 perfect positive correlation'), 'mean') (('exactly correlation', 'plot'), 'let') (('4', 'crash'), 'survive') (('Survival 2 chances', 'even Women'), 'be') (('number', 'women'), 'be') (('easily survival', 'Pclass1'), 'infer') (('survival rate', 'passengers'), 'give') (('very good begineers', 'Kaggle'), 'be') (('that', 'Miss.'), 'be') (('chance', 'Pclass2'), 'be') (('he', '34'), 'sible') (('graph', 'siblings roughly increase'), 'decrease') (('they', 'gender'), 'see') (('Age We', 'so this'), 'have') (('Thus we', 'dataset'), 'train') (('Then we', 'dataset'), 'take') (('So then we', 'different parameters'), 'make') (('example', 'Name Feature'), 'get') (('we', 'alphabets'), 'be') (('survival rate', 'men'), 'look') (('95', 'Wrongly 58 dead people'), 'classify') (('First us', 'features'), 'let') (('which', 'a.'), 'be') (('Exploratory Data Analysis Age Cabin', 'null values'), 'Part1') (('We', 'Male'), 'segregate') (('Pclass visible being', 'survival'), 'be') (('CV mean accuracy', 'dead 247'), 'be') (('then they', 'feature B'), 'CORRELATION') (('age', 'Pclass'), 'decrease') (('Lets', 'n_neighbours'), 'check') (('correlation Now above we', 'positively related features'), 'plot') (('891 passengers', 'training'), 'survive') (('then chances', 'survival'), 'be') (('This', 'very important modeling'), 'look') (('who', 'ship'), 'be') (('it', 'single powerful model'), 'be') (('accuracies', 'algorithm'), 'average') (('Lisa sinking', 'history'), 'Kleypas') (('Age Children', 'survival'), 'have') (('we', 'Name feature'), 'be') (('I', 'instead myself'), 'be') (('Exploratory Data Part1 Analysis', 'features'), 'content') (('submodels', 'diiferent types'), 'be') (('two features', 'other'), 'say') (('many passengers', 'accident'), 'be') (('again larger families', 'Pclass3'), 'show') (('Predictive Part3 ModelingWe', 'EDA part'), 'gain') (('it', 'training time'), 'try') (('so here we', 'Regex A Za z.'), 's') (('It', 'weak model'), 'be') (('Ensembling', 'ways'), 'do') (('survival rate', 'Pclass3'), 'be') (('Even Men', 'survival very low rate'), 'have') (('Cross Validation', 'k subsets'), '1') (('Sex chance', 'men'), 'observation') (('t', 'Everything'), 'say') (('only that', 'classifier'), 'be') (('1 Toddlers', '5 large numbers'), 'fill') (('we', 'n_neighbours attribute'), 'be') (('We', 'n_neighbours'), 'use') (('Hyper TuningThe machine learning models', 'Black Box'), 'Parameters') (('even majority', 'S.'), 'look') (('features', 'very little information'), 'mean') (('Sex Categorical FeatureThis', 'Features'), 'Features') (('here that', 'didn t many Pclass3 around 81 survive'), 'be') (('also many passengers', 'multiple cabins'), 'fare') (('Sex', 'Categorical dataset'), 'Features') (('AdaBoost Adaptive', 'case'), 'be') (('point we', 'it'), 'family_size') (('we', 'various parameters'), 'say') (('feature', 'Parch'), 'be') (('classifier', 'itself'), 'be') (('Lets', 'Sex'), 'check') (('which', 'model'), 'be') (('XGBoostWe', 'AdaBoost'), 'get') (('Bagged KNNBagging', 'high variance'), 'work') (('one', 'them'), 'think') (('limit', 'accuracy'), 'add') (('We', 'Binning'), 'need') (('clearly Passenegers', '1 very high priority'), 'see') (('We', '4 parts'), 'reserve') (('both', 'almost same information'), 'know') (('problem', 'many many different ages'), 'be') (('Person', '3 all Pclass3'), 'show') (('how workflow', 'modeling predictive problem'), 'be') (('earlier Sex', 'differentiating very good factor'), '2') (('we', 'values'), 'feature') (('we', 'choice'), 'change') (('Thus it', 'dead'), 'make') (('Lets', 'other features'), 'be') (('we', 'better model'), 'be') (('we', '5 parts'), 'let') (('Confusion MatrixIt', 'classifier'), 'give') (('3 We', 'other parts'), 'continue') (('right diagonal', 'wrong prredictions'), 'leave') (('also we', 'binning'), 'be') (('Thus we', 'respective groups'), 'assign') (('EnsemblingEnsembling', 'model'), 'be') (('I', 'other values'), 'replace') (('when somebody', 'ship'), 'prove') (('Here we', 'variable'), 'have') (('fare_range', 'survival increases'), 'see') (('Voting ClassifierIt', 'machine learning many different simple models'), 'be') (('This', 'also important model'), 'look') (('standards', 'Pclass1'), 'look') (('It', 'predictions'), 'work') (('features', 'Now above heatmap'), 'see') (('Boosting', 'first complete dataset'), 'train') (('Still number', 'almost twice males'), 'be') (('highest correlation', 'SibSp'), 'be') (('so maximum age', 'passenger'), 'be') (('SVM Random ForestsThe best score', 'Rbf Svm'), 'be') (('Confusion Matrix', 'RandomForests AdaBoost etc'), '05') (('we', 'few'), 'see') (('large families', 'Pclass3'), 'be') (('Passengers', 'survival'), 'have') (('1 Some', 'common important features'), 'observation') (('who', 'passengers'), 'need') (('We', 'classifiers 2 best i.'), 'tune') (('it', 'collision'), 'million') (('we', 'better model'), 'call') (('we', 'Cross Validation'), 'overcome') (('also continous we', 'ordinal value'), 'be') (('who', 'family'), 'be') (('irrespective', 'first priority'), 'be') (('we', 'AdaBoost'), 'try') (('we', 'generalised model'), 'achieve') (('So we', 'Name'), 'extract') (('as possible even newbies', 'it'), 'try') (('that', 'mean age'), 'assign') (('chances', 'age'), '3') (('Observations 1 Maximum passenegers', 'S.'), 'board') (('It', 'submodels'), 'give') (('survival 1 chances', 'irrespective Pclass'), 'observation') (('how we', 'new features'), 'check') (('almost 95', 'Pclass3'), 'have') (('Also we', 'Predictive Modeling'), 'tranform') (('features', 'numeric values'), 'pass') (('Tall Medium then Height', 'values'), 'for') (('value', '1 perfect negative correlation'), 'mean') (('So lets', '80 5 bins'), 'divide') (('it', 'categorical value'), 'drop') (('other great notebooks', 'Oscar Takeshita https www'), 'be') (('person', 'family members'), 'represent') (('two categories', 'them'), 'type') (('We', 'dataset'), 'try') (('features', 'features'), 'CleaningNow') (('algorithm', 'training other set'), '4') (('Passengers', 'age group'), 'die') (('1 survived', '48'), 'be') (('we', 'dataset'), 'assign') (('children', 'good Pclass'), 'look') (('it', 'you'), 'like') (('we', 'S.'), 'Embarked') (('Now model', 'instances'), 'get') (('Looks', 'Pclass'), 'be') (('Passenger', 'Classification great Algorithms'), 'predict') (('survival rates', 'around 18 19'), 'be') (('we', 'bins'), 'be') (('values', 'seperate equally 5 bins'), 'arrange') (('it', '90'), 'let') (('categorical two categories', 'example'), 'be') (('that', 'test new sets'), 'seem') (('Now we', 'such variables'), 'sort') (('you', 'always worst time'), 'EDA') (('accurately passenger', 'that'), 'predict') (('Age earlier feature', '177 null values'), 'have') (('Some', 'SibSp negative Alone'), 'be') (('chances', 'survival'), 'convert') (('earlier Age', 'Machine Learning Models'), 'Problem') "}