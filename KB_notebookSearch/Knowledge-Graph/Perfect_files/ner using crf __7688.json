{"name": "ner using crf ", "full_name": " h1 NER USING CRF h2 Exploring Visualizing our data h2 Modeling the Data h3 Random Forest Classifier h3 Conditional Random Fields classifier ", "stargazers_count": 0, "forks_count": 0, "description": "metrics import flat_classification_report The dataset does not have any header currently. We can either work on this model alone by improving the features or ensembling it with a more contextual model or use a different model altogether. Also we add new features such as upper lower digit title etc. Lets check the dataset again without the O tags. It is a supervised learning method which has been proven to be better than the tree based models when it comes to NER. Named entity recognition is a task that is well suited to the type of classifier based approach. However the precision and recall metrics of the classes individually have improved but not much. False Positives FP When actual class is no and predicted class is yes. com shoumikgoswami ner using random forest and crf Libraries Exploring Visualizing our dataBefore going further we will try to understand what the dataset is all about and what all the features mean. The model is basically memorizing words and tags which will not suffice. Modeling the Data With the basic EDA done and understanding the dataset we can move to the modeling stage. The precision and recall values of most of the classes were 0. NER USING CRFThe goal of a named entity recognition NER system is to identify all textual mentions of the named entities. So our dataset mostly contains words related to geopolitical entities geographical locations and person names. Precision TP TP FP Recall Sensitivity Recall is the ratio of correctly predicted positive observations to the all observations in actual class yes. We divide the dataset into train and test sets Lets see how the input array looks like Random Forest classifier Lets check the performance Feature set Creating the train and test set Creating the CRF model We predcit using the same 5 fold cross validation Lets evaluate the mode Tuning the parameters manually setting c1 10. This will help us in understanding what each tag type and sub type represents. Since the problem statement is a simple classification problem we will start with a simple tree based model Random Forest using a simple feature map. We can use the first row as a header as it has the relevant headings. Whereas a discrete classifier predicts a label for a single sample without considering neighboring samples a CRF can take context into account e. These words can be considered as fillers and their presence might impact the classifier performance as well. It is important that the classifier has proper features fed in to improve the performance. True Positives TP These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. Classification reports are used to obtain the values of these metrics in a text format per class. True Negatives TN These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. It is essential that the model is evaluated by these metrics per class to make sure we have a good model. read_csv Input data files are available in the read only. pdf Future Scope we can do the hyperparameter tuning This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. Compared to the Random Forest classifier the CRF classifier did better as the scores have improved. In short we try to provide a sequence of features to the model for each word the sequence containing POS tags capitalisations type of word title etc. reference link https www. The dataset has the following columns or features Index Index numbers for each word Numeric type Sentence The number of sentences in the dataset We will find the number of sentences below Numeric type Word The words in the sentence Character type POS Parts Of Speech tags these are tags given to the type of words as per the Penn TreeBank Tagset Categorical type Tag The tags given to each word based on the IOB tagging system described above Target variable Categorical type so this is dataset is for learning purpose there we are having a same length of sentenceNow that we know the words and sentences lets try to understand what sort of words each tag contains. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. Lets find the number of words in the dataset Lets visualize how the sentences are distributed by their length Lets find out the longest sentence length in the dataset Words tagged as B org Words tagged as I org Words tagged as B per Words tagged as I per Words tagged as B geo Words tagged as I geo Words tagged as I geo Words distribution across Tags Words distribution across Tags without O tag Words distribution across POS Simple feature map to feed arrays into the classifier. It seems the features which require the model to take proper decisions are missing. It is the harmonic mean of the both Precision and RecallF1 Score 2 Recall Precision Recall Precision For a decent classifier we would prefer high precision and recall values. This is important in order to understand how the classifiers will perform and help us interpret the results. Since we need to take into account the context as well we create features which will provide consecutive POS tags for each word. Precision Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. We will make the first row as the heading remove the first row and re index the dataset so we are basically having only those rows where sentence column is not null A class to retrieve the sentences from the dataset ths is how a sentence will look like. Since we are dealing with Information Extraction we will use the following metrics to evaluate the models Precision Recall F1 scoreThe metrics mentioned above are calculated using True False positives and True False negatives respectively. we will divide the dataset into 5 subsets and train test on them. In particular a tagger can be built that labels each word in a sentence using the IOB format where chunks are labelled by their appropriate type. Random Forest ClassifierWe will use 5 fold cross validation as an input parameter to the classifier i. Conditional Random Fields classifierA Conditional Random Field CRF is a standard model for predicting the most likely sequence of labels that correspond to a sequence of inputs. eu tagsets penn treebank tagset text The 20English 20Penn 20Treebank 20tagset Sketch 20Engine 20 earlier 20version. Some models like decision trees and neural networks are often be able to get 100 accuracy on the training data but perform much worse on new data. In order to use CRF we will enhance the feature set and create more features which can be used by the model to predict the tags correctly. for each word and also consider the consecutive words in the list. We will try tuning the model manually to see if we can improve it. Therefore this score takes both false positives and false negatives into account. the linear chain CRF which is popular in natural language processing predicts sequences of labels for sequences of input samples. The context information behind each word needs to be fed to the model as well so that the predictions are more accurate. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session Data analysis Data visualisation from sklearn_crfsuite import CRF scorers metrics import sklearn_crfsuite from sklearn_crfsuite import scorers from sklearn_crfsuite import metrics from sklearn_crfsuite. Recall TP TP FN F1 score F1 Score is the weighted average of Precision and Recall. Simple tree based models have been proven to provide decent performance in building NERC systems. This can be broken down into two sub tasks identifying the boundaries of the NE and identifying its type. False Negatives FN When actual class is yes but predicted class in no. Quite surprising most of the words are tagged as outside of any chunk. Therefore we will train on one subset and test on the other and repeat for every subset so that the classifier classifies correctly on average and the performance estimate is not overly optimisticAlthough we have a good average score the model performed quite badly. The IOB Tagging system contains tags of the form B CHUNK_TYPE for the word in the Beginning chunk I CHUNK_TYPE for words Inside the chunk O Outside any chunkThe IOB tags are further classified into the following classes geo Geographical Entity org Organization per Person gpe Geopolitical Entity tim Time indicator art Artifact eve Event nat Natural Phenomenon Penn Treebank tagset https www. Random Forest being one of the most popular tree based models can learn the underlying rules according to which terms are tagged. Maybe the model is again remembering words and not taking into the context information completely. Study material CRF http homepages. Performance metricsBefore we move to the modeling part it is important to understand the performance metrics on the basis of which the models will be evaluated. uk csutton publications crftut fnt. ", "id": "bavalpreet26/ner-using-crf", "size": "7688", "language": "python", "html_url": "https://www.kaggle.com/code/bavalpreet26/ner-using-crf", "git_url": "https://www.kaggle.com/code/bavalpreet26/ner-using-crf", "script": "classification_report cross_val_predict sent2features cross_val_score numpy seaborn flat_classification_report chain word2features matplotlib.pyplot sent2labels scorers metrics sklearn.model_selection pandas sklearn_crfsuite.metrics feature_map RandomForestClassifier getsentence(object) sklearn_crfsuite RandomizedSearchCV __init__ sklearn.metrics GridSearchCV make_scorer CRF sklearn.ensemble itertools ", "entities": "(('Quite surprising most', 'as outside chunk'), 'tag') (('So dataset', 'geopolitical entities geographical locations'), 'contain') (('value', 'predicted class'), 'TN') (('read_csv Input data files', 'read'), 'be') (('we', 'high precision values'), 'be') (('fillers', 'classifier performance'), 'consider') (('we', 'word title etc'), 'try') (('Therefore score', 'false account'), 'take') (('tag', 'words'), 'have') (('O', 'Geopolitical tim Time indicator Artifact eve Natural Phenomenon Penn Treebank tagset https Entity art www'), 'contain') (('models', 'which'), 'move') (('It', 'kaggle python Docker image https github'), 'Scope') (('precision values', 'classes'), 'be') (('tag type', 'what'), 'help') (('sentence', 'how'), 'make') (('Also we', 'digit title such upper lower etc'), 'add') (('terms', 'which'), 'learn') (('Maybe model', 'context information'), 'remember') (('which', 'basically words'), 'memorize') (('as well predictions', 'model'), 'need') (('metrics import dataset', 'header'), 'flat_classification_report') (('NER goal', 'named entities'), 'USING') (('classifier', 'performance'), 'be') (('which', 'proper decisions'), 'seem') (('when it', 'NER'), 'be') (('Precision TP TP FP Recall Sensitivity Recall', 'actual class'), 'be') (('We', 'manually c1'), 'divide') (('We', 'different model'), 'work') (('classification simple we', 'Random feature simple map'), 'be') (('which', 'tags'), 'enhance') (('we', 'train them'), 'divide') (('we', 'good model'), 'be') (('value', 'predicted class'), 'tp') (('that', 'classifier based approach'), 'be') (('However precision metrics', 'classes'), 'improve') (('which', 'word'), 'create') (('us', 'results'), 'be') (('t', 'sklearn_crfsuite'), 'list') (('Simple tree based models', 'NERC systems'), 'prove') (('manually we', 'it'), 'try') (('which', 'input samples'), 'predict') (('Random Forest ClassifierWe', 'classifier i.'), 'use') (('model', 'good average score'), 'train') (('better scores', 'Random Forest classifier'), 'do') (('where chunks', 'appropriate type'), 'build') (('that', 'inputs'), 'be') (('Lets', 'O again tags'), 'check') (('I', 'classifier'), 'find') (('features', 'all what'), 'com') (('Precision Recall F1 scoreThe metrics', 'True False negatives'), 'use') (('CRF', 'account e.'), 'predict') (('This', 'type'), 'break') (('Negatives False When actual class', 'no'), 'fn') (('Classification reports', 'class'), 'use') (('it', 'relevant headings'), 'use') (('Recall TP TP FN F1 score F1 Score', 'weighted Precision'), 'be') (('Precision Precision', 'positive observations'), 'be') (('models', 'new data'), 'be') (('we', 'modeling stage'), 'model') "}