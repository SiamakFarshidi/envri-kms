{"name": "introduction to cnn keras 0 997 top 6 ", "full_name": " h1 Introduction to CNN Keras Acc 0 997 top 8 h3 Yassine Ghouzam PhD h4 18 07 2017 h1 1 Introduction h1 2 Data preparation h2 2 1 Load data h2 2 2 Check for null and missing values h2 2 3 Normalization h2 2 3 Reshape h2 2 5 Label encoding h2 2 6 Split training and valdiation set h1 3 CNN h2 3 1 Define the model h2 3 2 Set the optimizer and annealer h2 3 3 Data augmentation h1 4 Evaluate the model h2 4 1 Training and validation curves h2 4 2 Confusion matrix ", "stargazers_count": 0, "forks_count": 0, "description": "e the area size pooled each time more the pooling dimension is high more the downsampling is important. For those who have a 3. The CNN can isolate features that are useful everywhere from these transformed images feature maps. 1 Training and validation curves 4. Since i set epochs 2 on this notebook. However it seems that our CNN has some little troubles with the 4 digits hey are misclassified as 9. The second important layer in CNN is the pooling MaxPool2D layer. The rectifier activation function is used to add non linearity to the network. We can get a better sense for one of these examples by visualising the image and looking at the label. Here we can see that our CNN performs very well on all digits with few errors considering the size of the validation set 4 200 images. 1 Load data a random split of the train set doesn t cause some labels to be over represented in the validation set. 3 Data augmentation 4. Data preparation 2. 2 Confusion matrixConfusion matrix can be very helpfull to see your model drawbacks. fit X_train Y_train batch_size batch_size epochs epochs validation_data X_val Y_val verbose 2 With data augmentation to prevent overfitting accuracy 0. We have to choose the pooling size i. This metric function is similar to the loss function except that the results from the metric evaluation are not used when training the model only for evaluation. 2 Set the optimizer and annealer 3. 2 Confusion matrix 5. Introduction to CNN Keras Acc 0. Its better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function. The last 9 is also very misleading it seems for me that is a 0. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit. The most important errors are also the most intrigous. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima. Be carefull with some unbalanced dataset a simple random split could cause inaccurate evaluation during the validation. 6 Split training and valdiation set 3. 17 sklearn versions. In the end i used the features in two fully connected Dense layers which is just artificial an neural networks ANN classifier. I ll show you the training and validation curves i obtained from the model i build with 30 epochs 2h30 The model reaches almost 99 98. In order to make the optimizer converge faster and closest to the global minimum of the loss function i used an annealing method of the learning rate LR. It is like a set of learnable filters. Keras requires an extra dimension in the end which correspond to channels. 3 ReshapeTrain and test images 28px x 28px has been stock into pandas. 3 Normalization 2. Firstly I will prepare the data handwritten digits images then i will focus on the CNN modeling and evaluation. Our model is very well trained 4. So we can safely go ahead. Once our model is ready we fit the training dataset. The LR is the step by which the optimizer walks through the loss landscape. The validation accuracy is greater than the training accuracy almost evry time during the training. Sometime it is very difficult to catch the difference between 4 and 9 when curves are smooth. Dataframe as 1D vectors of 784 values. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive monotonically decreasing learning rate. 1 Load dataWe have similar counts for the 10 digits. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. IntroductionThis is a 5 layers Sequential Convolutional Neural Network for digits recognition trained on MNIST dataset. 2 Set the optimizer and annealerOnce our layers are added to the model we need to set up a score function a loss function and an optimisation algorithm. 0 GPU capabilites from GTX 650 to recent GPUs you can use tensorflow gpu with keras. The Flatten layer is use to convert the final feature maps into a one single 1D vector. 67 of accuracyFor the data augmentation i choosed to Randomly rotate some training images by 10 degrees Randomly Zoom by 10 some training images Randomly shift images horizontally by 10 of the width Randomly shift images vertically by 10 of the height I did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9. In the last layer Dense 10 activation softmax the net outputs distribution of probability of each class. Moreover the CNN converg faster on 0. I choosed to build it with keras API Tensorflow backend which is very intuitive. With the ReduceLROnPlateau function from Keras. 114 With data augmentation i achieved 99. 5 Label encodingLabels are 10 digits numbers from 0 to 9. callbacks i choose to reduce the LR by half if the accuracy is not improved after 3 epochs. This technique also improves generalization and reduces the overfitting. This function will iteratively improve parameters filters kernel values weights and bias of neurons. Evaluate the model 4. The improvement is important Without data augmentation i obtained an accuracy of 98. 1 Training and validation curvesThe code below is for plotting loss and accuracy curves for training and validation. By applying just a couple of these transformations to our training data we can easily double or triple the number of training examples and create a very robust model. 2 Check for null and missing values 2. It is the error rate between the oberved labels and the predicted ones. It combines all the found local features of the previous convolutional layers. Dropout is a regularization method where a proportion of nodes in the layer are randomly ignored setting their wieghts to zero for each training sample. To keep the advantage of the fast computation time with a high LR i decreased the LR dynamically every X steps epochs depending if it is necessary when accuracy is not improved. 7 accuracy on the validation dataset after 2 epochs. It looks at the 2 neighboring pixels and picks the maximal value. 98114 history model. 1 Define the modelI used the Keras Sequential API where you have just to add one layer at a time starting from the input. Computation will be much much faster For computational reasons i set the number of steps epochs to 2 if you want to achieve 99 of accuracy set it to 30. Some of these errors can also be made by humans especially for one the 9 that is very close to a 4. 6 Split training and valdiation set I choosed to split the train set in two parts a small fraction 10 became the validation set which the model is evaluated and the rest 90 is used to train the model. in order to minimise the loss. We define the loss function to measure how poorly our model performs on images with known labels. Let s investigate for errors. This flattening step is needed so that you can make use of fully connected layers after some convolutional maxpool layers. I want to see the most important errors. 671 of accuracy with this CNN trained in 2h30 on a single CPU i5 2500k. Each filter transforms a part of the image defined by the kernel size using the kernel filter. We use a specific form for categorical classifications 2 classes called the categorical_crossentropy. These are used to reduce computational cost and to some extent also reduce overfitting. you found this notebook helpful or you just liked it some upvotes would be very much appreciated That will keep me motivated convert to one hot encoding Load the data Drop label column free some space Check the data Normalize the data Reshape image in 3 dimensions height 28px width 28px canal 1 Encode labels to one hot vectors ex 2 0 0 1 0 0 0 0 0 0 0 Set the random seed Split the train and the validation set for the fitting Some examples Set the CNN model my CNN architechture is In Conv2D relu 2 MaxPool2D Dropout 2 Flatten Dense Dropout Out Define the optimizer Compile the model Set a learning rate annealer Turn epochs to 30 to get 0. For those six case the model is not ridiculous. To avoid that you could use stratify True option in train_test_split function Only for 0. We need to encode these lables to one hot vectors ex 2 0 0 1 0 0 0 0 0 0 0. 2 Check for null and missing valuesI check for corrupted images missing values inside. We reshape all data to 28x28x1 3D matrices. I choosed RMSprop with default values it is a very effective optimizer. 3 Data augmentation In order to avoid overfitting problem we need to expand artificially our handwritten digit dataset. Prediction and submition 5. This Notebook follows three main parts The data preparation The CNN modeling and evaluation The results prediction and submission 2. For RGB images there is 3 channels we would have reshaped 784px vectors to 28x28x3 3D matrices. 3 NormalizationWe perform a grayscale normalization to reduce the effect of illumination s differences. Filters can be seen as a transformation of the image. 9967 accuracy Without data augmentation i obtained an accuracy of 0. Some popular augmentations people use are grayscales horizontal flips vertical flips random crops color jitters translations rotations and much more. The kernel filter matrix is applied on the whole image. For that purpose i need to get the difference between the probabilities of real value and the predicted ones in the results. 99286 set input mean to 0 over the dataset set each sample mean to 0 divide inputs by std of the dataset divide each input by its std apply ZCA whitening randomly rotate images in the range degrees 0 to 180 Randomly zoom image randomly shift images horizontally fraction of total width randomly shift images vertically fraction of total height randomly flip images randomly flip images Fit the model Plot the loss and accuracy curves for training and validation Look at confusion matrix Predict the values from the validation dataset Convert predictions classes to one hot vectors Convert validation observations to one hot vectors compute the confusion matrix plot the confusion matrix Display some error results Errors are difference between predicted labels and true labels Probabilities of the wrong predicted numbers Predicted probabilities of the true values in the error set Difference between the probability of the predicted label and the true label Sorted list of the delta prob errors Top 6 errors Show the top 6 errors predict results select the indix with the maximum probability. I plot the confusion matrix of the validation results. The higher LR the bigger are the steps and the quicker is the convergence. Combining convolutional and pooling layers CNN are able to combine local features and learn more global features of the image. 1 Define the model 3. The most important function is the optimizer. There is no missing values in the train and test dataset. Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. 1 Predict and Submit results 1. 5 Label encoding 2. That means that our model dosen t not overfit the training set. For example the number is not centered The scale is not the same some who write with big small numbers The image is rotated. MNIST images are gray scaled so it use only one channel. The first is the convolutional Conv2D layer. We could also have used Stochastic Gradient Descent sgd optimizer but it is slower than RMSprop. I choosed to set 32 filters for the two firsts conv2D layers and 64 filters for the two last ones. The metric function accuracy is used is to evaluate the performance our model. relu is the rectifier activation function max 0 x. Since we have 42 000 training images of balanced labels see 2. We can make your existing dataset even larger. This layer simply acts as a downsampling filter. 997 top 8 Yassine Ghouzam PhD 18 07 2017 1. ", "id": "yassineghouzam/introduction-to-cnn-keras-0-997-top-6", "size": "10898", "language": "python", "html_url": "https://www.kaggle.com/code/yassineghouzam/introduction-to-cnn-keras-0-997-top-6", "git_url": "https://www.kaggle.com/code/yassineghouzam/introduction-to-cnn-keras-0-997-top-6", "script": "matplotlib.image keras.layers keras.models train_test_split confusion_matrix keras.utils.np_utils numpy keras.preprocessing.image seaborn Dropout ImageDataGenerator Dense keras.callbacks MaxPool2D plot_confusion_matrix RMSprop keras.optimizers ReduceLROnPlateau matplotlib.pyplot Sequential sklearn.model_selection pandas Conv2D to_categorical # convert to one-hot-encoding display_errors sklearn.metrics Flatten ", "entities": "(('We', 'label'), 'get') (('It', 'maximal value'), 'look') (('7 accuracy', '2 epochs'), 'dataset') (('you', 'keras'), '0') (('function metric accuracy', 'performance'), 'use') (('we', 'training dataset'), 'be') (('We', 'one hot vectors'), 'need') (('when someone', 'digit'), 'be') (('activation rectifier function', 'network'), 'use') (('optimizer', 'loss landscape'), 'be') (('gray it', 'only one channel'), 'be') (('image', 'big small numbers'), 'center') (('accuracy', '3 epochs'), 'callback') (('which', 'channels'), 'require') (('technique', 'overfitting'), 'improve') (('i', '99'), '114') (('which', 'Dense two fully connected layers'), 'use') (('Label 5 encodingLabels', '9'), 'be') (('RMSProp update', 'learning aggressive monotonically decreasing rate'), 'adjust') (('It', 'error oberved labels'), 'be') (('that', 'data augmentation techniques'), 'approach') (('IntroductionThis', 'MNIST dataset'), 'be') (('test 3 ReshapeTrain 28px 28px', 'pandas'), 'image') (('I', '64 two last ones'), 'choose') (('model', 'almost 99 98'), 'show') (('you', 'Only 0'), 'avoid') (('regularization where proportion', 'training sample'), 'be') (('model', 'six case'), 'be') (('it', 'default values'), 'choose') (('Flatten layer', '1D one single vector'), 'be') (('671', 'CPU i5 single 2500k'), 'train') (('which', 'keras API Tensorflow backend'), 'choose') (('grayscales horizontal vertical flips', 'crops color jitters translations rotations'), 'be') (('i', 'learning rate LR'), 'use') (('fit X_train batch_size epochs epochs validation_data X_val batch_size Y_val', 'overfitting accuracy'), 'Y_train') (('that', 'me'), 'seem') (('you', 'maxpool convolutional layers'), 'need') (('This', 'distributed way'), 'drop') (('validation accuracy', 'evry almost time training'), 'be') (('i', 'predicted results'), 'need') (('We', 'categorical classifications 2 classes'), 'use') (('90', 'model'), 'set') (('filter', 'kernel filter'), 'transform') (('when accuracy', 'X steps dynamically epochs'), 'decrease') (('simple random split', 'validation'), 'cause') (('we', 'very robust model'), 'double') (('Combining', 'image'), 'be') (('optimizer', 'probably local minima'), 'be') (('that', 'very 4'), 'make') (('then i', 'CNN modeling'), 'focus') (('kernel filter matrix', 'whole image'), 'apply') (('function', 'neurons'), 'improve') (('3 we', '28x28x3 3D matrices'), 'be') (('second important layer', 'CNN'), 'be') (('you', '30'), 'be') (('model dosen', 'training set'), 'mean') (('We', '28x28x1 3D matrices'), 'reshape') (('I', 'validation results'), 'plot') (('3 NormalizationWe', 'differences'), 'perform') (('Load', '10 digits'), 'have') (('we', 'digit artificially handwritten dataset'), 'augmentation') (('labels', 'validation set'), 'datum') (('i', '98'), 'be') (('how poorly model', 'known labels'), 'define') (('hey', '9'), 'seem') (('it', 'such 6'), 'rotate') (('i', '0'), 'accuracy') (('These', 'also overfitting'), 'use') (('it', 'RMSprop'), 'use') (('CNN', '4 200 images'), 'see') (('we', 'loss function'), 'set') (('results', 'only evaluation'), 'be') (('curvesThe 1 Training code', 'training'), 'be') (('Filters', 'image'), 'see') (('layer', 'downsampling simply filter'), 'act') (('Notebook', 'data CNN results prediction'), 'follow') (('Confusion matrixConfusion 2 matrix', 'model very drawbacks'), 'be') (('42 training 000 images', '2'), 'see') (('when curves', '4'), 'be') (('where you', 'input'), 'use') (('It', 'previous convolutional layers'), 'combine') (('model', '0'), 'find') (('results', 'maximum probability'), 'mean') (('that', 'images feature everywhere transformed maps'), 'isolate') "}