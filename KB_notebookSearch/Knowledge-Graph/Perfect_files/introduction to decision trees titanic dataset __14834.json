{"name": "introduction to decision trees titanic dataset ", "full_name": " h2 Abstract h2 Preparing the Titanic dataset h2 Visualising processed data h2 Gini Impurity h2 Finding best tree depth with the help of Cross Validation h2 Final Tree ", "stargazers_count": 0, "forks_count": 0, "description": "org wiki Deep_learning 2 https en. It s not intended to be the most accurate Titanic survival model out there but to explain how to create visualise and understand Classification Trees. Title VS Sex You can easily compare features and their relationship with the class by grouping them and calculating some basic statistics for each group. 89 Title seems therefore to be more useful than Sex for our purpose. org wiki Cross validation_ statistics The best max_depth parameter seems therefore to be 3 82. html 6 https en. The goal of their learning algorithms is always to find the best split for each node of the tree. Thanks to these rules we can infer some insights about the shipwreck. Finally we can observe that 3rd class passengers had also less chances to survive so probably passengers belonging to upper social social classes were privileged or simply 3rd class cabins may have been further away of the lifeboats. The data shows that less Mr survived 15 67 than men in general 18. Our model can therefore be summarised with 4 simple rules If our observation includes de Mr Title then we classify it as not survived all the branches in the left side of the tree lead to an orange node If it doesn t include Mr Title and FamilySize is 4 or less then we classify it as survived. But sometimes we need to actually get insights from the available data and in these cases transparent easy to understand models like Decision Trees will greatly simplify our task. But the absolute correlation between both is also very high 0. Cross Validation is a model validation technique that splits the training dataset in a given number of folds. Samples is simply the number of observations contained in the node. png to allow display in web notebook Annotating chart with PIL Drawing offset position Text to draw RGB desired color ImageFont object with desired font Code to check available fonts and respective paths import matplotlib. font_manager matplotlib. We re now going to simulate both splits calculate the impurity of resulting nodes and then obtain the weighted Gini Impurity after the split to measure how much each split has actually reduced impurity. We use copy again to prevent modifications in out original_train dataset Create Title feature Map Sex as binary feature Table with Sex distribution grouped by Title Since Sex is a binary feature this metrics grouped by the Title feature represent MEAN percentage of men COUNT total observations SUM number of men Define function to calculate Gini Impurity Gini Impurity of starting node Gini Impurity decrease of node for male observations Gini Impurity decrease if node splited for female observations Gini Impurity decrease if node splited by Sex Gini Impurity decrease of node for observations with Title 1 Mr Gini Impurity decrease if node splited for observations with Title 1 Mr Gini Impurity decrease if node splited for observations with Title 1 Mr Desired number of Cross Validation folds Testing max_depths from 1 to max attributes Uncomment prints for details about each Cross Validation pass print Current max depth depth n Extract train data with cv indices Extract valid data with cv indices We fit the model with the fold train data We calculate accuracy with the fold validation data print Accuracy per fold fold_accuracy n print Average accuracy avg print n Just to show results conveniently Create Numpy arrays of train test and target Survived dataframes to feed into our models Create Decision Tree with max_depth 3 Predicting results for test dataset Export our trained model as a. Sex will therefore be neglected since the information is already included in the Title feature. If it doesn t include Mr Title FamilySize is more than 4 and Pclass is more than 2 then we classify it as not survived. Next we find the Gini Impurity of the node already explained in this kernel. We can also note that smaller families had better chances to survive maybe because bigger families tried to stick together or look for missing members and therefore didn t had places left in the lifeboats. 8 average accuracy across the 10 folds and feeding the model with more data results in worst results probably due to over fitting. Of course a complex classification algorithm will do better at identifying the customers who bought a tie by taking into account more features but is that really useful for the supermarket Decision Trees can also help a lot when we need to understanding the data. A good way to find the best value for this parameter is just iterating through all the possible depths and measure the accuracy with a robust method such as Cross Validation 1. Decision Trees will try to find the split which decreases Gini Impurity the most across the two resulting nodes. But remember the KISS principle Keep It Simple Stupid Always consider the complexity accuracy trade off complex techniques should only be used if they offer significant improvements. That s why advanced techniques such as Deep Learning 1 or Ensemble Learning 2 cf. Anisotropic Kernel 3 are commonly used for complex tasks. Lastly class correspond to the predominant class of each node and this is how our model will classify an observation. Let s load the data and get an overview. The code below does exactly this in one line and explains the meaning of each metric when working with a binary class. Over fitting a model excessively adapted to the train data is a common reason. org wiki Information_gain_in_decision_trees 2 https en. A good example is the traditional problem of classifying Iris flowers included in the sklearn documentation 5 were we can learn about the characteristics of each flower type in the resulting tree. It s true that by regrouping rare titles into a single category we are losing some information regarding Sex. First we re going to prepare the dataset and discuss the most relevant features. org wiki Random_forest Preparing the Titanic dataset For the Titanic challenge we need to guess wheter the individuals from the test dataset had survived or not. Gini Impurity Before start working with Decision Trees let s briefly explain how they work. This may be because Title implicitly includes information about Sex in most cases. There s already extended work on this so we re just using one the best approches out there credit to Sina 1 Anisotropic 2 and also Megan Risdal 3 for the suggestion of the Title feature. The main aspects covered are Learning from the data with Decision Trees Dataset exploration and processing Relevant features for Decision Trees Gini Impurity Finding best tree depth with the help of cross validation Generating and visualising the final modelThis is my first Kernel so please feel free to include any suggestions comments or critics 1 https en. If we split by Sex we ll have the two following nodes Node with men 577 observations with only 109 survived Node with women 314 observations with 233 survivedIf we split by Title 1 Mr we ll have the two following nodes Node with only Mr 517 observations with only 81 survived Node with other titles 374 observations with 261 survivedWe find that the Title feature is slightly better at reducing the Gini Impurity than Sex. After this short introduction to Decision Trees and their place in Machine Learning let s see how to apply them for the Titanic challenge. Misters seem to have honoured their title and sacrificed themselves in favour on women and men with more exotic titles like Master or Dr. Let s now explore the relationship between our variables by plotting the Pearson Correlation between all the attributes in our dataset credit to Anisotropic 1 for this beautiful plot 1 https www. Therefore the feature Title is capturing all the information present in Sex. 6 23 out of 891 samples. It is calculated as the probability of mislabelling an element assuming that the element is randomly labelled according the the distribution of all the classes in the set. The main downsides of Decision Trees are their tendency to over fit their inability to grasp relationships between features and the use of greedy learning algorithms not guaranteed to find the global optimal model. Value shows the class distribution of the samples count non_survived count survived. Each split uses different data for training and testing purposes allowing the model to be trained and tested with different data each time. org wiki Apriori_algorithm 5 http scikit learn. If we need to build a model that will be directly used for some task and only show it s end results then we don t really care about building some kind of blackbox if it s accurate enough image or speech recognition for example. We could create two categories Rare Male and Rare Female but the separation will be almost meaningless due to the low occurrence of Rare Titles 2. Let s take the case of a supermarket looking to better understand customer behaviour the straightforward Apriori 4 algorithm can quickly offer relevant insights like 80 of customers who bought a suit also bought a tie so they may try to increase tie sales by offering a discount to clients buying a suit. com mrisdal titanic exploring survival on the titanic Visualising processed data Our dataset is now much cleaner than before with only numerical values and potentially meaningful features. We ll therefore use 3 as the max_depth parameter for our final model. Final Tree Finally here we have our Decision Tree It achieves an accuracy of 82. org stable modules tree. First we need to calculate the Gini Impurity of the starting node including all 891 observations in our train dataset. High chances are one of them will be used for the first node in our final decision tree so let s first explore further these features and compare them. org wiki Decision_tree_learning Gini_impurityLet s use our Sex and Title features as an example and calculate how much each split will decrease the overall weighted Gini Impurity. Since only 342 observations survived the survival probability is around 38 38 342 891. com c titanic discussion 10169 Finding best tree depth with the help of Cross Validation After exploring the data we re going to find of much of it can be relevant for our decision tree. This is a critical point for every Data Science project since too much train data can easily result in bad model generalisation accuracy on test real unseen observations. Let s begin explaining how to read the graph. In other cases too much data can also hide meaningful relationships either because they evolve with time or because highly correlated features prevent the model from capturing properly the value of each single one. The sklearn library we re gonna use implements Gini Impurity 2 another common measure so let s explain it. Therefore is very likely that Title is going to be the first feature in our final decision tree making Sex useless after this initial split. Thanks to this overview we can see that our dataset needs some treatment. com arthurtok titanic introduction to ensembling stacking in python 4 https en. Our submission to the Titanic competition results in scoring 2234 out of 5672 competition entries. To verify this we can use the copy we made of the original training data without mappings and check the distribution of Sex grouped by Title. But for our current purpose let s also find out what can the data tell us about the shipwreck with the help of a Classification Tree. org wiki Decision_tree_learningIntroduction When applying Machine Learning algorithms it s critical to always keep in mind the problem we re trying to solve. The class Survived is already in binary format so no additional formatting is necessary but features like Name Ticket or Cabin need to be adapted for the problem we re trying to solve and we can also engineer some new features by merging or regrouping existing ones. The colour also represents the class the opacity increasing with the actual distribution of samples. We ll then find the best tree depth to avoid over fitting generate the final model and explain how to visualise the resulting tree. Gini Impurity measures the disorder of a set of elements. They re helpful for checking the quality of engineered features and identifying the most relevant ones by visualising the resulting tree. If it doesn t include Mr Title FamilySize is more than 4 and Pclass is 2 or less then we classify it as survived. This confirms our previous analysis and we re now sure that Title will be used for the first split. Simpler models are also less prone to over fitting and tend to generalise better. Abstract In this Kernel we re going to take a look at Decision Trees 1 using Python and the Titanic dataset. In most cases the most accurate and robust model might be what you re looking for. In addition Title may be more valuable to our task by capturing other characteristics of the individuals like age social class personality. If you want to learn more about how Decision Trees work I recommend you to follow the links in this Kaggle discussion 3. findSystemFonts fontpaths None fontext ttf. Group all non common titles into one single grouping Rare Mapping Sex Mapping titles Mapping Embarked Mapping Fare Mapping Age Feature selection remove variables no longer containing relevant information Since Survived is a binary class 0 or 1 these metrics grouped by the Title feature represent MEAN survival rate COUNT total observations SUM people survived title_mapping Mr 1 Miss 2 Mrs 3 Master 4 Rare 5 Since Survived is a binary feature this metrics grouped by the Sex feature represent MEAN survival rate COUNT total observations SUM people survived sex_mapping female 0 male 1 Let s use our original_train dataframe to check the sex distribution for each title. The first line of each node except those of the final row shows the splitting condition in the form feature value. We find that excepting for a single observation a female with Dr title all the observations for a given Title share the same Sex. org wiki Ensemble_learning 3 https www. com arthurtok titanic introduction to ensembling stacking in pythonThis heatmap is very useful as an initial observation because you can easily get an idea of the predictive value of each feature. Thanks to this in depth analysis of the Sex and Title features we ve seen that even if the correlation of the feature Sex with the class Survived was higher Title is a richer feature because it carries the Sex information but also adds other characteristics. Given their transparency and relatively low computational cost Decision Trees are also very useful for exploring your data before applying other algorithms. This allows the algorithm to be trained and tested with all available data across all folds avoiding any splitting bias and giving a good idea of the generalisation of the chosen model. But measuring the goodness of a given split is a subjective question so in practice different metrics are used for evaluating splits. In the case of decision trees the max_depth parameter determines the maximum number of attributes the model is going to use for each prediction up to the number of available features in the dataset. For the titanic example it can be calculated as follows code should be explicit enough 1 https en. Not bad for a simple Decision Tree And remember any suggestions comments or critics are welcome Thanks for reading Diego Imports needed for the script Loading the data Store our test passenger IDs for easy access Showing overview of the train dataset Copy original dataset in case we need it later when digging into interesting features WARNING Beware of actually copying the dataframe instead of just referencing it original_train train will create a reference to the train variable changes in train will apply to original_train Using copy allows to clone the dataset creating a different object with the same values Feature engineering steps taken from Sina and Anisotropic with minor changes to avoid warnings Feature that tells whether a passenger had a cabin on the Titanic Create new feature FamilySize as a combination of SibSp and Parch Create new feature IsAlone from FamilySize Remove all NULLS in the Embarked column Remove all NULLS in the Fare column Remove all NULLS in the Age column Next line has been improved to avoid warning Define function to extract titles from passenger names If the title exists extract and return it. 86 the highest in our dataset so they are probably carrying the same information and using the two as inputs for the same model wouldn t be a good idea. One commonly used metric is Information Gain 1. This result only accounts for part of the submission dataset and is indicative while the competition is running. But if we re using Machine Learning to actually get insights from the data blackbox models are almost useless and it s best to stick with simpler transparent techniques. com sinakhorami titanic titanic best working classifier 2 https www. 38 across the training dataset. In this case Sex and Title show the highest correlations in absolute terms with the class Survived 0. The main downside is that Cross Validation requires the model to be trained for each fold so the computational cost can be very high for complex models or huge datasets. com arthurtok titanic introduction to ensembling stacking in python 3 https www. Using them in a Random Forest 6 helps mitigate some of this issues. ", "id": "dmilla/introduction-to-decision-trees-titanic-dataset", "size": "14834", "language": "python", "html_url": "https://www.kaggle.com/code/dmilla/introduction-to-decision-trees-titanic-dataset", "git_url": "https://www.kaggle.com/code/dmilla/introduction-to-decision-trees-titanic-dataset", "script": "IPython.display get_gini_impurity these metrics grouped by the Title feature represent ImageDraw xgboost accuracy_score cross_val_score Image numpy seaborn check_call subprocess tree Image as PImage sklearn plotly.tools KFold matplotlib.pyplot PIL plotly.offline plotly.graph_objs pandas sklearn.model_selection (0 or 1) ImageFont get_title sklearn.metrics ", "entities": "(('feature Therefore Title', 'present Sex'), 'capture') (('Simpler models', 'also less fitting'), 'be') (('png', 'available fonts'), 'offset') (('we', 'Title'), 'use') (('how much split', 'actually impurity'), 'go') (('how model', 'observation'), 'correspond') (('model validation that', 'folds'), 'be') (('briefly how they', 'Gini Decision Trees'), 'Impurity') (('We', 'a.'), 'use') (('then we', 'it'), 'include') (('computational cost', 'very complex models'), 'be') (('s', 'Titanic challenge'), 'let') (('It', '82'), 'Tree') (('Using', 'issues'), 'help') (('title', 'it'), 'bad') (('they', 'suit'), 'let') (('calculated', 'follows code'), 'be') (('less Mr', 'general'), 'show') (('higher richer it', 'also other characteristics'), 'see') (('Misters', 'Master'), 'seem') (('dataset', 'treatment'), 'see') (('dataset', 'now much only numerical values'), 'survival') (('They', 'resulting tree'), 're') (('we', 'Sex'), 's') (('so we', 'Title feature'), 's') (('competition', 'submission dataset'), 'result') (('when we', 'data'), 'do') (('then we', 'it'), 'summarise') (('non_survived count', 'samples count'), 'show') (('we', 'existing ones'), 'be') (('class simply cabins', 'further away lifeboats'), 'observe') (('Sex', 'initial split'), 'be') (('wouldn t', 'same model'), '86') (('how much split', 'Gini overall weighted Impurity'), 'use') (('sex_mapping female 0 1 s', 'title'), 'group') (('you', 'what'), 'be') (('opacity', 'samples'), 'represent') (('First we', 'train dataset'), 'need') (('s', 'how graph'), 'let') (('only they', 'significant improvements'), 'remember') (('different metrics', 'splits'), 'be') (('good way', 'Cross such Validation'), 'iterate') (('model', 'dataset'), 'determine') (('highly correlated features', 'single one'), 'hide') (('therefore information', 'Title already feature'), 'neglect') (('now Title', 'first split'), 'confirm') (('model', 'train excessively data'), 'be') (('5 we', 'resulting tree'), 'be') (('First we', 'most relevant features'), 'go') (('Relevant', 'suggestions first so comments'), 'learn') (('Samples', 'node'), 'be') (('Title', 'age class social personality'), 'be') (('Gini Impurity', 'already kernel'), 'find') (('Gini 2 common so s', 'it'), 'library') (('individuals', 'test dataset'), 'Random_forest') (('89 Title', 'purpose'), 'seem') (('so s', 'them'), 'be') (('That', 'Deep why advanced such Learning'), 's') (('Rare Rare separation', 'Rare Titles'), 'create') (('we', '1 Python'), 'go') (('Title feature', 'Sex'), 'have') (('Sex', 'class'), 'show') (('We', 'how resulting tree'), 'find') (('first line', 'form feature value'), 'show') (('Anisotropic Kernel', '3 commonly complex tasks'), 'use') (('sometimes we', 'greatly task'), 'need') (('we', 'decision tree'), 'be') (('8 average accuracy', 'probably fitting'), 'due') (('We', 'final model'), 'use') (('you', 'feature'), 'be') (('It', 'Classification Trees'), 'intend') (('which', 'most two resulting nodes'), 'try') (('main downsides', 'global optimal model'), 'be') (('it', 'speech accurate enough example'), 'need') (('didn therefore t', 'lifeboats'), 'note') (('Gini Impurity', 'elements'), 'measure') (('we', 'problem'), 'decision_tree_learningintroduction') (('goal', 'tree'), 'be') (('code', 'when binary class'), 'do') (('s', '1 beautiful plot'), 'let') (('that', 'same Sex'), 'find') (('model', 'different data'), 'use') (('also data', 'Classification Tree'), 'let') (('I', 'Kaggle discussion'), 'want') (('absolute correlation', 'both'), 'be') (('cost Decision relatively computational Trees', 'other algorithms'), 'be') (('Title VS Sex You', 'group'), 'compare') (('we', 'shipwreck'), 'infer') (('train too much data', 'unseen test real observations'), 'be') (('element', 'set'), 'calculate') (('almost it', 'simpler transparent techniques'), 'be') (('more than then we', 'it'), 'include') (('Title', 'most cases'), 'be') (('algorithm', 'chosen model'), 'allow') "}