{"name": "amazon deep composer ar cnn music augmentation ", "full_name": " h1 Amazon Deep Composer h2 Training a custom AR CNN model h3 The AWS DeepComposer approach to generating music h1 Start Here h2 Installing dependencies h3 Download and Unzip Training DataSet h3 Optional Download and Unzip pre trained model h1 Preprocess and Parse Training DataSet h2 Preprocessing the data into the piano roll format h3 Reviewing sample piano rolls h3 Why do we use 128 timesteps h3 Creating samples of uniform size shape for model training h4 In the code cells below h3 Augment the data for better results and training h1 Create Setup the Model Setup all Hyper Parameters h1 Generate and Evaluate Plot from the resulting model h2 Performing inference h3 How to change the inference parameters when you perform inference h2 Submitting to the Spin the Model Chartbusters challenge ", "stargazers_count": 0, "forks_count": 0, "description": "Setting this value to 0 means no notes will be added to your input melody NOTE If you restrict your model s ability to add and remove notes you risk creating poor compositions. The model has been trained to both remove and add notes so it can improve the input melody and correct mistakes that it may have made in earlier iterations. Piano Roll Input Dimensions Number of Filters In The Convolution Growth Rate Of Number Of Filters At Each Convolution Number Of Encoder And Decoder Layers A List Of Dropout Values At Each Encoder Layer A List Of Dropout Values At Each Decoder Layer A List Of Flags To Ensure If batch_normalization Should be performed At Each Encoder A List Of Flags To Ensure If batch_normalization Should be performed At Each Decoder Path to Pretrained Model If You Want To Initialize Weights Of The Network With The Pretrained Model Learning Rate Of The Model Optimizer To Use While Training The Model Batch Size Number Of Epochs title Calculate the number of Batch Iterations Before A Training Epoch Is Considered Finished Total Number Of Time Steps title Creating the data generators that perform data augmentation. A higher number of sampling iterations gives the model more time to improve the input melody. As you saw when we used the play_midi function each sample isn t the same length. Now that you understand the basic theory behind our approach let s dive into the code. Training Data Generator Validation Data Generator title Creating callbacks for the model and performing initial intitialization Callback For Loss Plots Callback For Saving Model Checkpoints Create A List Of Callbacks Create A Model Instance title Tensorboard Graphs and Stats Load the TensorBoard notebook extension title Main Training Loop. Download and Unzip Training DataSet Optional Download and Unzip pre trained model Preprocess and Parse Training DataSet Preprocessing the data into the piano roll format Reviewing sample piano rolls Why do we use 128 timesteps In this tutorial we use 8 bar https en. Please be patient as it may take a while. Creativity temperature To create the output probability distribution the final layer uses a softmax activation. We subdivide those 8 bars into 128 timesteps. We do this by treating music generation as a series of edit events which can be either the addition or removal of a note. For our purposes we use a custom data generator to perform data augmentation. And in some cases Runtime also needs to be restarted The MIT Zero License Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files the Software to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and or sell copies of the Software and to permit persons to whom the Software is furnished to do so. By training our model to view the problem as edit events rather than as an entire image or just the addition of notes we found that it can offset the accumulation of errors and generate higher quality music. Create the environment and install required packages title Import all modules and other necessary code Imports Make it a multiple of the batch size for best balanced performance Number of Bars Number of Beats Per Bar number of bars to be shifted Total number of pitches in a Pianoroll Total number of Tracks 100 bpm title The Best Choice Works best stand alone Super Piano Original 2500 MIDIs title Second Best Choice Works best stand alone Alex Piano Only Drafts Original 1500 MIDIs unzip data JSB Chorales. You may also need to run this section twice or trice if there are any errors. param type string Create An Inference Object Load The Checkpoint title MODEL INFERENCE GENERATION CONTROLS run auto param type slider min 0. In the code cells below generate_samples is a function used to ingest the midi files and break the files down into a uniform shape plot_pianoroll uses a built in function plot_track from the pypianoroll https salu133445. We explain how to acquire the data that you use for this project provide some exploratory data analysis EDA and show how we augment the data during training. These functions are wrapped in a larger function generate_samples which also takes in constants that are related to subdividing the. Setting this value to 0 prevents the model from removing notes from your input melody. title Generate MIDI file samples and shuffle the DataSet Parse the MIDI file and get the piano roll Saving the generated samples into a dataset variable Shuffle the dataset title Sampling from a uniform distribution title Calculate Training and Validation sampling lengths title Specifying training hyperparameters. load json_file dict to string string to json title Main Generation Loop param type string Generate The Composition set the src and play title Plot and Graph the Output Only first batch MIDI file is plotted and displayed param type slider min 0 max 20 step 1 param type slider min 0 max 20 step 1 param type slider min 1 max 128 step 1 param type slider min 1 max 128 step 1 For plotting Use librosa s specshow function for displaying the piano roll Plot the output title Additional Input Output Comparison Metrics of the specified MIDI file Input Midi Metrics Generated Output Midi Metrics Convert The Input and Generated Midi To Tensors a matrix Plot Input Piano Roll Plot Output Piano Roll. Start Training title Optional Save your model manually if necessary title Inference Code Initialization Routine truncate Apply temperature and softmax Mask all pixels that both have a note and were once part of the original input Mask all pixels that both do not have a note and were not once part of the original input Check if the note being removed is from the original input Check if the note being added is not in original input title Loading a saved checkpoint file. NOTE If you want to test that your model is training on your custom dataset you can decrease the number of epochs down to 1 in this cell. Augment the data for better results and trainingWe are going to do the following things in this section 1 Adding or removing notes during training2 Removing random notes from a target piano roll to create input piano rolls3 Adding random notes to the target piano roll to create input piano rolls Create Setup the Model. Maximum notes to add maxNotesAdded The maximum percentage of notes that can be added during inference. How to change the inference parameters when you perform inference The model performs inference by sampling from its predicted probability distribution across the entire piano roll. You also can change the inference parameters to observe differences in the quality of the music generated Sampling iterations samplingIterations The number of iterations performed during inference. Then download your notebook checkpoint files and compositions from SageMaker and upload them to your public repository. THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. NOTE Training times can vary greatly based on the parameters that you have chosen and the notebook instance type that you chose when launching this notebook. You can change the temperature for the softmax to produce different levels of creativity in the outputs generated by the model. 5k training steps title Import the MIDI files from the data_dir and save them with the midi_files variable Finds our random MIDI file from the midi_files variable and then plays it Note To listen to multiple samples from the Bach dataset you can run this cell over and over again. In the following cell you start training your model. zip d data title Alternative Choice Alex Piano Only Original 450 MIDIs title Super Piano 2 Pre Trained Model floss 1. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. io pypianoroll visualization. Submitting to the Spin the Model Chartbusters challengeTo submit your composition s and model to the Spin the model chartbusters challenge you will first need to create a public repository on GitHub https github. Creating samples of uniform size shape for model training For model training the input piano rolls must be the same size. Start Here Installing dependenciesFirst let s install and import all of the Python packages that we will use in this tutorial. In the next section we show examples of the piano roll format that we use for training the model. Every edit sequence can directly correspond to a piano roll. json as json_file inference_params json. Setup all Hyper Parameters Generate and Evaluate Plot from the resulting model Performing inference Congratulations You have now trained your very own AR CNN model to generate music. Inference is an iterative process. 1 param type slider min 0 max 100 step 1 param type slider min 0 max 100 step 1 param type slider min 0 max 100 step 1 with open inference_parameters. To create the input piano rolls during training we need data generators for both the training and validation samples. This yields 128 timesteps frac 4 timesteps 1 beat frac 4 beats 1 bar frac 8 bars 1 128 timesteps We found that this level of resolution is sufficient to capture the musical details in our dataset. An edit sequence is a series of edit events. We use two functions to create target piano rolls that are the same size process_midi and process_pianoroll. Maximum notes to remove maxPercentageOfInitialNotesRemoved The maximum percentage of notes that can be removed during inference. The AWS DeepComposer approach to generating music Autoregressive based approaches are prone to accumulate errors during training. We further divide each beat into 4 timesteps. To help mitigate this problem we train our AR CNN model so that it can detect and then fix mistakes including those made by the model itself. org wiki Bar_ music samples from the dataset. Use the link from your public repository to make your submission to the Chartbusters challenge title Environment and Dependencies Setup. Amazon Deep Composer Training a custom AR CNN model In this Jupyter notebook we guide you through several steps of the data science life cycle. To use your trained model you will need to update the PATH variable in the cell below. After adding or removing a note from the input the model feeds this new input back into itself. html library to plot a piano roll track from the dataset. That s because each of the 8 bars contains 4 beats. Now you can see how well your model will perform with an input melody. ", "id": "aleksandrsigalov/amazon-deep-composer-ar-cnn-music-augmentation", "size": "5754", "language": "python", "html_url": "https://www.kaggle.com/code/aleksandrsigalov/amazon-deep-composer-ar-cnn-music-augmentation", "git_url": "https://www.kaggle.com/code/aleksandrsigalov/amazon-deep-composer-ar-cnn-music-augmentation", "script": "Track ArCnnModel data_generator pypianoroll keras.layers generate_composition keras.models OptimizerType utils.generate_training_plots IPython.display keras model get_sampled_index losses plot_pianoroll AddAndRemoveAPercentageOfNotes Enum PianoRollGenerator get_indices numpy mask_not_allowed_notes MaxPooling2D Adam Dropout process_pianoroll process_midi Multitrack BatchNormalization randrange convert_midi_to_tensor inference sample_notes_from_model backend as K sample_multiple Inference plot_piano_roll RMSprop keras.optimizers tensorflow output random Loss google.colab matplotlib.pyplot files midi2audio generate_samples backend convert_tensor_to_midi Conv2D load_model get_music_metrics enum Model concatenate UpSampling2D Input play_midi augmentation GenerateTrainingPlots FluidSynth Constants() display HTML utils.midi_utils __init__ get_softmax Audio Javascript ", "entities": "(('it', 'while'), 'be') (('edit sequence', 'piano directly roll'), 'correspond') (('output probability final layer', 'softmax activation'), 'use') (('plot_pianoroll', 'pypianoroll https salu133445'), 'be') (('that', 'target piano rolls'), 'use') (('which', 'note'), 'do') (('model', 'piano entire roll'), 'change') (('we', 'training samples'), 'need') (('further each', '4 timesteps'), 'divide') (('level', 'dataset'), 'yield') (('We', '128 timesteps'), 'subdivide') (('softmax', 'model'), 'change') (('MIDIs title Second Best Choice best alone Super Piano 2500 Works', 'Alex best alone Piano'), 'create') (('it', 'model'), 'train') (('you', 'cell'), 'NOTE') (('data', 'input piano rolls Create Setup'), 'augment') (('you', 'cell'), 'title') (('edit sequence', 'edit events'), 'be') (('how well model', 'input melody'), 'see') (('higher number', 'input melody'), 'give') (('submission', 'title Environment'), 'use') (('Software', 'whom'), 'need') (('Performing You', 'music'), 'Generate') (('You', 'also section'), 'need') (('that', 'inference'), 'note') (('number', 'inference'), 'change') (('you', 'cell'), 'need') (('added', 'checkpoint saved file'), 'start') (('COPYRIGHT HOLDERS', 'OR OTHER SOFTWARE'), 'SHALL') (('you', 'when notebook'), 'vary') (('we', 'tutorial'), 'start') (('that', 'data augmentation'), 'number') (('s', 'code'), 'now') (('each', '4 beats'), 's') (('batch MIDI Only first file', 'Input Midi Output Midi Metrics Generated Tensors'), 'dict') (('Checkpoint MODEL GENERATION CONTROLS', 'auto param type slider'), 'create') (('model', 'back itself'), 'after') (('you', 'model'), 'start') (('piano rolls', 'input'), 'be') (('when we', 'same length'), 'see') (('that', 'inference'), 'maxPercentageOfInitialNotesRemoved') (('Model Instance title Tensorboard Graphs', 'TensorBoard notebook extension title'), 'create') (('Autoregressive based approaches', 'training'), 'approach') (('PROVIDED AS', 'PARTICULAR PURPOSE'), 'be') (('we', 'data science life cycle'), 'Training') (('you', 'GitHub https github'), 'submit') (('we', 'bar 8 https'), 'train') (('we', 'data augmentation'), 'use') (('it', 'earlier iterations'), 'train') (('it', 'quality higher music'), 'find') (('how we', 'training'), 'explain') (('we', 'model'), 'show') (('MIDI piano roll', 'training hyperparameters'), 'sample') (('you', 'poor compositions'), 'mean') (('that', 'the'), 'wrap') "}