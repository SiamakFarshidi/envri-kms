{"name": "deeplearninginbme assignment2 ", "full_name": " h1 Deep Learning in Biomedical Engineering h2 Assignment 2 h3 Due date 11 59 pm Febraury 25 2021 h2 Please enter your full name and UNI here h2 Introduction h2 Instructions h2 How to submit h3 1 5 pts According to the CIFAR10 dataset descriptions and other online resources please identify the quantities below h3 2 0 pts Import the required packages in the following code block h3 3 5 pts Load train and test sets using Pytorch datasets functions h3 4 10 pts Using the matplotlib library make a figure with N times4 grid cells where N is the number of classes Display one random sample of each class pulled from the train set in its corresponding row that depends on its class index and the first column and its histogram in the second column Repeat this for the third and fourth columns but pull images from the test set h3 5 5 pts Split up the train set into new train and validation sets so that the number of samples in the validation set is equal to the number of samples in the test set Then create a DataLoader for each set including the test set with a batch size of 32 h3 6 5 pts Display the number of samples for each class in the train validation and test sets as a stacked bar plot similar to the FirstTutorial h3 7 10 pts According to the LeNet architecture below create a fully connected model Also identify the architeture s hyper parameters activation functions and tensor shapes h3 8 5 pts Create an instance of ADAM optimizer with an initial learning rate of 0 0001 and an instance of Mean Squared Error MSE loss function Briefly explain the ADAM optimizer algorithm and the MSE loss function h3 9 15 pts Train the model for 200 epochs using the train set and validate your model s performance at the end of each epoch using the validation sets h3 10 5 pts Display the learning curve and illustrate the best epoch Explain your criteria for the best epoch h3 11 10 pts Load the model s weights at the best epoch and test your model performance on the test set Display the confusion matrix and classification report h3 12 5 pts Display five random samples of each class titled with the true label and the predicted label Comment on your model s performance h3 13 20 pts Repeat the training validation and testing with the Cross Entropy loss function and initial learning rate of 0 005 Explain how the model s performance changed h1 Reference ", "stargazers_count": 0, "forks_count": 0, "description": "MSE is the average squared difference between the estimated values and the true values. format epoch train_epoch_loss 1 Choose the best epoch Get the test image Covert list to tensor define the prediction function. 0 get the inputs data is a list of inputs labels Convert labels to one hot vector y_onehot labels. The input has dimension 6x28x28. Please remove the comments before filling the blocks. 8 vdots end bmatrix shows the probability of belonging to each class for the same sample and predicted by the model. Truck may be classified as car or ship which are transportations. plot img plt. To avoid overwriting your previously trained model change the save directories in the training loop. dataset save models Loss curve Accuracy curve Choose the best epoch call our best model and use that for inference on the test set Get the test image Covert list to tensor define the prediction function Display the confusion matrix Classification Report Reload the testset 5 test images of 10 labels for each category. ipynb file File Download 5. The maxpooling layer has 2x2 filter. The Pytorch CIFAR10 https pytorch. item accuracy num_correct1 len validationloader. MSELoss loss function. The title is in the format of True_label Predicted_label Get the predicted label img np. com drvaibhavkumar alexnet in pytorch cifar10 clas 83 test accuracy Preprocess the images 10 train images of 10 labels 10 test images of 10 labels Get the train image list full of images Get the test image list full of images Takes one color image and intensity range then returns the histogram and edges values as a list that each row will have two elements the edges and histogram values. Load train and test sets using Pytorch datasets functions. numpy y_onehot np. dataset reset the gradient backpropagate the loss update the parameters print statistics y_onehot labels. According to the CIFAR10 http www. Otherwise the weights would be the same as the last epoch or the best epoch in the last part. Just remember they are using a different architecture and they are using TensorFlow for implementations. If you are using an online code or paper make sure you cite their work properly to avoid plagiarism. Display the number of samples for each class in the train validation and test sets as a stacked bar plot similar to the FirstTutorial https www. The first convolution layer has 6 channels with a kernel size of 5 zero padding and a stride of 1. The first convolution layer has 16 channels with a kernel size of 5 zero padding and a stride of 1. dataset backpropagate the loss update the parameters print statistics Get the accuracy num_correct1 torch. com roblexnana cifar10 with cnn for beginer you may find useful information about how your outputs must look. Using other students works is absolutely forbidden and considered cheating. org docs stable optim. dataset save models print Epoch train_loss. Download your completed notebook as a. The output should be 16x10x10. Briefly explain the ADAM optimizer algorithm and the MSE loss function. item accuracy num_correct len trainloader. Define the display function fig axs plt. Import the required packages in the following code block. I choose the epoch which has the lowest validation loss as the best epoch. Although you may use other available online resources such as GitHub Kaggle Notebooks it is highly recommended to try your best to do it yourself. You can always add more blocks if you need to or it just makes your answers well organized. For other optimization algorithms and loss functions check the links below Optimizers list https pytorch. Obviously you don t need to re import the dataset and the libraries. The input has dimension 6x14x14. transpose img 1 2 0 img torch. Note I trained 100 epoch to save GPU Quota and time. With the same loss function we can clearly see the more complicated Alexnet offers better performance than Lenet with a test accuracy of 65. InstructionsDepending on each question there are empty blocks you need to fill. To increase the training speed use the GPU accelerator. subplot 5 4 4 i 4 plt. You can always come back here and import another package please keep them all in the following block to make your code well organized. The result is epoch 63 11. Copy the Public URL 4. xlabel Intensity plt. The output is 6x14x14. cuda Get the accuracy num_correct1 torch. Make the saved version public Share Public 3. The criteria for the best epoch can be the minimum loss or maximum accuracy or other criteria. ipynb files on the CourseWorks https courseworks2. The input has dimension 16x10x10. org wiki LeNet model to classify CIFAR10 http www. The model has severe overfitting issue. Repeat the training validation and testing with the Cross Entropy https pytorch. loop over the dataset multiple times get the inputs data is a list of inputs labels Convert labels to one hot vector zero the parameter gradients forward backward optimize Get the accuracy num_correct torch. The markdown blocks are only for plain or laTex text that you may use for answering descriptive questions. Fully connected layer with an input of 120 neurons and an output of 84 neurons. The MSE loss function is L y hat y frac 1 N sum_ i 1 N y_i hat y _i 2 Where y is the true value hat y is the predicted value N is the number of classes. arange 10 y_onehot None. Look at the FirstTutorial https www. Do not forget to save the model at the end of each epoch. LeNet Architecture https raw. Architecture hyper parameter includes the number of layers number of kernels in each layer size of the kernels stride zero padding size. loop over the dataset multiple times running_loss 0. html loss functions Explaine ADAM and MSE here 1. vt and st are estimates of the first moment the mean and the second moment of the gradients respectively. Some regularization methods should be applied to the architecture to solve the issue. html algorithms Loss function list https pytorch. format epoch train_epoch_loss 1 Loss curve Accuracy curve Choose the best epoch Get the test image Covert list to tensor define the prediction function Extra credit Define the model First phase Second phase Third phase fully connected layer AlexNet\u4e0a\u9762\u662f1000. The output should be 6x28x28. Train the model for 200 epochs using the train set and validate your model s performance at the end of each epoch using the validation sets. However you need to create a new instance of the architecture. The test accuracy is lower than the model with MSE loss function. Display five random samples of each class titled with the true label and the predicted label. Comment about the model s performance The model has an overall test accuracy of 62. This is normal but it is understandable since lenet5 is very simple network. From the graph we can see the generalization gap becomes larger and larger. It is used if the dataset is normally distributed and you want to penalize outliers a lot. Just by looking at the architecture itself you should be able to identify the hyper parameters. One comment line at the top of each code block is necessary to explain what you did in that block. Split up the train set into new train and validation sets Get the testset OPtimizer and loss function Train our CNN with Alexnet architecture for classification. Save a version You can use Quick Save to avoid re running the whole notebook 2. Both architectures experience serious overfitting issue and generalization gap enlarges as the training proceed. In this Kaggle Notebook https www. Comment on your model s performance. reshape images 3 32 32 img plt. org tutorials beginner blitz cifar10_tutorial. Load the model s weights at the best epoch and test your model performance on the test set. Define the optimizer and MSE loss function Create the directory Train our CNN for classification. float32 y_onehot torch. Essentially you need to copy all the codes above and just change the loss function and edit the learning rate. dataset print Epoch train_loss. html dataset descriptions and other online resources please identify the quantities below a Total Number of samples b Number of classes c Image size d Write class names and their corresponding label index e. Display the learning curve and illustrate the best epoch. com soroush361 deeplearninginbme firsttutorial. Adam with an initial learning rate of 0. Make sure none of the samples in the validation set exists in the new train set. The output is 16x5x5. png Describe the model s architecture hyper parameters 1. from_numpy y_onehot loss loss_CrossEntropyLoss validation_label_predicted y_onehot. tensor y_onehot dtype torch. Preprocess the image and enlarge the images to 96 96 since this size better fits the Alexnet. org docs stable generated torch. subplots 10 2 PLot the train image and its histgram. For more help look at this implementation https github. e Intensity range your answers here a 60000 samples 50000 for training and 10000 for testing b 10 classses c 32x32 color images d airplane 0 automobile 1 bird 2 cat 3 deer 4 dog 5 frog 6 horse 7 ship 8 truck 9 e 0 255 2. Create an instance of ADAM optimizer https pytorch. 0001 and an instance of Mean Squared Error MSE https pytorch. You may use transformers while downloading the dataset to do the job. Name your variables properly that represent the data they are holding such as test_set. Using the matplotlib library make a figure with N times4 grid cells where N is the number of classes. Also identify the architeture s hyper parameters activation functions and tensor shapes. 98 which is not bad. Upload the Public URL and the. com soroush361 deeplearninginbme firsttutorial for more help. Implementing and reporting results using other architectures than LeNet will grant you an extra 20 on grade. Explain your criteria for the best epoch. plot img PLot the test image and its histgram. Split up the train set into new train and validation sets so that the number of samples in the validation set is equal to the number of samples in the test set. Make sure the intensity range is between 0 1 and images are stored as tensor type. Don t comment on every detail. ylabel Abundance Architecture Define the model x x. For your information here is the mathematics behind the ADAM optimizer For each parameter w j v_t beta_1v_ t 1 1 beta_1 g_t s_t beta_2s_ t 1 1 beta_2 g_t 2 Delta w j eta frac v_t sqrt s_t epsilon g_t w j_ t 1 w j_t Delta w j Where eta is the initial learning rate g_t is the gradient at time t along w j v_t is the exponential average of gradients along w j s_t is the exponential average of squares of gradients along w j beta_1 beta_2 are the hyper parameters and epsilon is a small number to avoid dividing by zero. \u5982\u679c\u6d4b\u8bd5\u7684\u8bdd\u7528MNIST\u5219\u53ef\u4ee5\u4f7f\u752810 \u5411\u524d\u4f20\u64ad in_channel class_num If GPU available move the model to GPU. dataset backpropagate the loss update the parameters print statistics y_onehot labels. Look at this tutorial Pytorch CIFAR10 https pytorch. Keep in mind that you identified the W H and N Which refers to the number of classes in the first question. The input has dimension 3x32x32. The output has dimension 16x5x5. The learning curve shows the model s loss and accuracy at the end of each epoch for all epochs 200 epochs. html tutorial will also help you here. How to submit After you have completed the assignment 1. com soroush361 deeplearninginbme firsttutorial 2. format colors i plt. zero_grad forward backward optimize loss loss_CrossEntropyLoss outputs y_onehot. Explain how the model s performance changed. We also observe that the loss and accuracy fluctuate which may the result of higher learning rate. from_numpy y_onehot y_onehot torch. They are initialized as zero vectors and they are biased towards zero. Write comments for your codes in the big picture mode. title Histogram colors Red Green Blue histograms GetHistograms img display_range for i in range 3 plt. Repeat this for the third and fourth columns but pull images from the test set. cuda Get the accuracy num_correct torch. For ADAM optimizer keep other arguments as default. com icpm pytorch cifar10 blob master models LeNet. DataLoader for each set including the test set with a batch size of 32. Define the new optimizer and CroessEntropy loss function Train our CNN for classification. transpose img 0 1 2 0 axs i 0. long zero the parameter gradients optimizer. size 0 1 If GPU available move the model to GPU. CrossEntropyLoss loss function and initial learning rate of 0. Currently the markdown blocks have a comment like your answer here. Display one random sample of each class pulled from the train set in its corresponding row that depends on its class index and the first column and its histogram in the second column. Keep in mind that y is a one hot vector like y begin bmatrix 0 0 1 vdots end bmatrix This example of y indicates that the sample belongs to class ID 2 remember it is zero indexed and hat y begin bmatrix 0. plot histograms i 0 histograms i 1 c colors i label Channel 0. Deep Learning in Biomedical Engineering Assignment 2 Due date 11 59 pm Febraury 25 2021 Please enter your full name and UNI here Mingyang Zang mz2846 IntroductionIn this assignment you will implement train and test LeNet https en. Samples must be pulled from the test set. com soroush361 DeepLearningInBME main Ass1_Arch1. According to the LeNet architecture below create a fully connected model. long zero the parameter gradients forward backward optimize loss loss_CrossEntropyLoss outputs y_onehot. The code blocks are only for Python codes and comments and currently have a comment like Your code here. For those misclassification labels it seems like the model misclassify labels under the same super category For example dogs may be classified as horse or cat which are animals in general. Reload the dataset. Display the confusion matrix and classification report. The Adam optimizer calculates an exponential moving average of the gradient and the squared gradient. Fully connected layer with an input of 84 neurons and an output of 10 neurons. Then create a DataLoader https pytorch. Altogether it is a 7 layers network. plot img Display the images and histograms Split up the train set into new train and validation sets Get the testset Get the label from trainset Get the label from validationset Get the label from testset Count the number Plot the figure Check GPU availablity Define the model x x. ", "id": "mz2846/deeplearninginbme-assignment2", "size": "11862", "language": "python", "html_url": "https://www.kaggle.com/code/mz2846/deeplearninginbme-assignment2", "git_url": "https://www.kaggle.com/code/mz2846/deeplearninginbme-assignment2", "script": "torch.nn.functional torchvision.transforms DataLoader classification_report test10 mpl_toolkits.axes_grid1 train_test_split tensorflow.keras.datasets.mnist confusion_matrix to_categorical DisplayFunction accuracy_score numpy TensorDataset seaborn Adam SGD torch.nn MyDisplayFunction matplotlib.pyplot predict_with_pytorch forward sklearn.model_selection pandas torch.optim Model(nn.Module) torch.utils.data keras.utils __init__ GetHistograms ImageGrid sklearn.metrics AlextNet(nn.Module) ", "entities": "(('update', 'statistics y_onehot labels'), 'backpropagate') (('Covert list', 'prediction function'), 'epoch') (('you', 'block'), 'be') (('title', 'label img predicted np'), 'get') (('test accuracy', 'MSE loss function'), 'be') (('GPU', 'GPU'), '\u5411\u524d\u4f20\u64ad') (('row', 'two elements'), 'Preprocess') (('e Intensity', 'c color images 10 32x32 airplane 0 automobile 1 bird 2 3 deer 4 5 6 horse'), 'range') (('clearly more complicated Alexnet', '65'), 'see') (('parameters', 'statistics y_onehot labels'), 'print') (('Adam optimizer', 'gradient'), 'calculate') (('which', 'best epoch'), 'choose') (('Covert list', 'category'), 'choose') (('code', 'following block'), 'come') (('normally you', 'outliers'), 'use') (('which', 'car'), 'classify') (('empty you', 'question'), 'be') (('criteria', 'best epoch'), 'be') (('code blocks', 'code'), 'be') (('You', 'job'), 'use') (('none', 'train new set'), 'make') (('generalization training', 'serious overfitting issue'), 'experience') (('Obviously you', 'don dataset'), 'need') (('loss functions', 'https pytorch'), 'check') (('MSE', 'average squared estimated values'), 'be') (('works', 'other students'), 'forbid') (('Samples', 'test set'), 'pull') (('regularization methods', 'issue'), 'apply') (('dataset save models', 'Epoch train_loss'), 'print') (('number', 'test set'), 'split') (('they', 'such test_set'), 'name') (('How you', 'assignment'), 'submit') (('Third phase', 'layer fully AlexNet\u4e0a\u9762\u662f1000'), 'epoch') (('they', 'zero'), 'initialize') (('you', 'descriptive questions'), 'be') (('Otherwise weights', 'best last part'), 'be') (('how outputs', 'useful information'), 'roblexnana') (('generalization gap', 'graph'), 'see') (('inputs multiple times data', 'parameter zero forward backward accuracy'), 'get') (('it', 'it'), 'use') (('which', 'horse'), 'seem') (('0 1 GPU', 'GPU'), 'size') (('You', 'whole notebook'), 'save') (('hyper parameters', 'small zero'), 'be') (('also loss which', 'learning higher rate'), 'observe') (('Mingyang Zang mz2846 UNI here assignment you', 'LeNet https'), 'Learning') (('size', 'better Alexnet'), 'preprocess') (('testset function', 'classification'), 'split') (('they', 'implementations'), 'remember') (('just answers', 'always more blocks'), 'add') (('Architecture hyper parameter', 'kernels stride zero padding size'), 'include') (('model', '62'), 'comment') (('LeNet', 'grade'), 'grant') (('html dataset', 'classes c Image size d Write class names'), 'identify') (('vt', 'mean second gradients'), 'be') (('you', 'properly plagiarism'), 'make') (('markdown Currently blocks', 'answer'), 'have') (('Which', 'first question'), 'keep') (('title Histogram Red Green Blue histograms', 'range'), 'color') (('hat zero y', 'bmatrix'), 'keep') (('1 images', 'tensor type'), 'make') (('you', 'hyper parameters'), 'be') (('I', 'GPU Quota'), 'note') (('Load train', 'functions'), 'dataset') (('vdots end 8 bmatrix', 'model'), 'show') (('Essentially you', 'learning rate'), 'need') (('parameter', 'optimizer'), 'gradient') (('it', 'lenet5'), 'be') (('long parameter', 'optimize loss loss_CrossEntropyLoss forward backward outputs'), 'zero') (('However you', 'architecture'), 'need') (('convolution first layer', '1'), 'have') (('1 i c i', 'Channel'), 'histogram') (('that', 'first second column'), 'display') (('predicted N', 'classes'), 'be') (('learning curve', 'epochs 200 epochs'), 'show') (('inputs data', 'one hot vector y_onehot labels'), 'get') (('where N', 'classes'), 'make') (('number', 'Check availablity model'), 'Display') (('update', 'print statistics'), 'backpropagate') "}