{"name": "lyft level5 eda training inference ", "full_name": " h1 Lyft Motion Prediction for Autonomous Vehicles n Build motion prediction models for self driving vehicles n Table of contents n n 0 Introduction 0 n n 1 Import Packages 1 n n 2 Utility Functions 2 n 3 Basic EDA 3 n 3 1 Data Formats 3 1 n 3 1 1 Introduction 3 1 1 n 3 1 2 Zarr Format 3 1 2 n 3 1 3 Train Validation And Test Zarr 3 1 3 n 3 2 Checking Configuration Fields 3 2 n 3 3 Loading Data 3 3 n 3 4 Data Overview 3 4 n 3 4 1 Agents 3 4 1 n 3 4 2 Scenes 3 4 2 n 3 4 3 Frames 3 4 3 n 3 5 Dataset Package 3 5 n 3 5 1 ChunckedDataset 3 5 1 n 3 6 Visualize Autonomous Vehicle 3 6 n 3 6 1 Visualizing Various Rasterizer Objects 3 6 1 n 3 6 2 Visualize Trajectory Semantic View 3 6 2 n 3 6 3 Visualize Trajectory Satellite View 3 6 3 n 3 6 4 Visualize Agent 3 6 4 n 3 6 5 Visualize Individual Scene Semantic 3 6 5 n 3 6 6 Visualize Individual Scene Satellite 3 6 6 n n 4 Pytorch Baseline 4 n 4 1 Configuration 4 1 n 4 2 Loading Training Data 4 2 n 4 3 Training DataLoader 4 3 n 4 4 Model resnet50 Pytorch 4 4 n 4 5 Compilation 4 5 n 4 6 Training 4 6 n 4 7 Saving Model 4 7 n n 5 Prediction and Results 5 n 5 1 Test DataLoader 5 1 n 5 2 Getting Predictions 5 2 n 5 2 Submission 5 3 n n 6 Reference 6 n 0 Introduction n n Table of Contents 0 1 n nThis competition is hosted by ridesharing company Lyft https www lyft com which started Level 5 https self driving lyft com level5 self driving division to tackle the challenges in the field of self driving cars n nIn this competition our task is to build motion prediction models for self driving vehicles Model which can predict the movement of traffic agents around the Autonomous Vehicles such as cars cyclists and pedestrians etc We are required to predict how these different agents move in Autonomous Vehicles s environment n n What is Autonomous Vehicle n nAn autonomous vehicle or a driverless vehicle is one that is able to operate itself and perform necessary functions without any human intervention through ability to sense its surroundings n nAn autonomous vehicle utilises a fully automated driving system in order to allow the vehicle to respond to external conditions that a human driver would manage n n Competition Data n nThis dataset https www kaggle com c lyft motion prediction autonomous vehicles data is the largest collection of the traffic agent motion data This dataset includes the logs of movement of cars cyclists pedestrians and other traffic agents encountered by Lyft s autonomous fleet These logs come from processing raw lidar camera and radar data through our team s perception systems and are ideal for training motion prediction models The dataset includes n n 1000 hours of traffic agent movement n 16k miles of data from 23 vehicle n 15k semantic map annotations n nHere is the reserach paper of Prediction Dataset https paperswithcode com paper one thousand and one hours self driving n nThe dataset consists of 170 000 scenes capturing the environment around the autonomous vehicle Each scene encodes the state of the vehicle s surroundings at a given point in time source https self driving lyft com level5 prediction n n n n n n n nThis baseline solution is trained on over 2 million samples from the agent locations contained within the dataset The model predicts a single agent at a time First a raster generates a bird s eye view BEV top down raster which encodes all agents and the map The network infers the future coordinates of the agent based upon this raster n n n n n What we are predicting n nOur task in the competition is to predict the motion of external objects such as cars cyclist pedestrains etc in order to assist the self driving car We have to predict the location of objects agents in the next 50 frames n n Evaluation Metric Negative log likelihood n nWe calculate the negative log likelihood of the ground truth data given the multi modal predictions You can get more information here https www kaggle com c lyft motion prediction autonomous vehicles overview evaluation n n n https camo githubusercontent com b3634eea5be5501318957e21086781666018efa1 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239 n https camo githubusercontent com 8048a110a20827715a17eb76f8039302a576d503 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302b25323025354373756d5f742532302535436c6f672532302535436d61746863616c2537424e253744253238785f74253743253543626172253742782537445f742535456b2532432532302535437369676d61253344312532392532302535436d61746863616c2537424e253744253238795f74253743253543626172253742792537445f742535456b2532432532302535437369676d6125334431253239253744 n https camo githubusercontent com 9ba94f5c0c40666d66b93fba994cc5f7623ebd98 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302d25354366726163253742312537442537423225374425323025354373756d5f74253230253238253543626172253742782537445f742535456b2532302d253230785f74253239253545322532302b253230253238253543626172253742792537445f742535456b2532302d253230795f7425323925354532253744 n Lyft s Autonomous Vehicle AV n n Lyft s AV Introducton video n 1 Import Packages n n Table of Contents 0 1 n nWe are required to use L5Kit toolkit https github com lyft l5kit provided by the competition host to prepare preprocess data trian and evaluate the model Please add this utility script https www kaggle com pestipeti lyft l5kit unofficial fix provided by Peter s https www kaggle com pestipeti first by clicking on the Add data section inside your notebook before starting The L5Kit toolkit https github com lyft l5kit has some issue right now You can check this https www kaggle com c lyft motion prediction autonomous vehicles discussion 177125 discussion here for more information n n L5Kit is a library which lets you n n Load driving scenes from zarr files n Read semantic maps n Read aerial maps n Create birds eye view BEV images which represent a scene around an AV or another vehicle n Sample data n Train neural networks n Visualize results n 2 Utility Functions n nThanks to Trigram https www kaggle com nxrprime for providing this function n 3 EDA n n Table of Contents 0 1 n nThe data is huge around 22 GB We will use Lyft s L5Kit https github com lyft l5kit to process our data loading and visualization and training n n 3 1 Data Formats n nWe need to be familiar with the data we are using Let s dive in n n 3 1 1 Introduction n nThe L5Kit toolkit uses data format that consists of a set of numpy structured arrays Structured arrays https numpy org doc stable user basics rec html are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields The structured array can store various types of features Structured arrays are stored in memory in an interleaved format this means that one row or sample is grouped together in memory Let us take an example to understand this nlet s add some data in this structured array nAs we can see structured arrays allow us to mix different data types into a single array We will develop more intuition ahead n 3 1 2 Zarr Format n n Table of Contents 0 1 n nThe L5Kit uses zarr format to store and read these structured numpy arrays The data is available in zarr file format which can be easily load using the L5Kit https github com lyft l5kit Most of the traditional numpy operations can be handled using zarr files The zarr files are flat compact and highly performant for loading Each of the zarr file contains n n scenes driving episodes acquired from a given vehicle n frames snapshots in time of the pose of the vehicle A frame is a snapshot in time which consists of ego pose time and multiple agent states n agents a generic entity captured by the vehicle s sensors Note that only 4 of the 17 possible agent label probabilities are present in this dataset Each agent state describes the position orientation bounds and type n agents mask a mask that for train and validation masks out objects that aren t useful for training In test the mask provided in files as mask npz masks out any test object for which predictions are NOT required n traffic light faces traffic light information n n 3 1 3 Train Validation And Test Zarr n n Table of Contents 0 1 nWe can see that our train zarr file has set of 4 arrays All this 4 fields are described above in the Data Format section above Let us check these fields We will check 1 observation from each nThe data is expected to live in a folder that can be configured using the L5KIT DATA FOLDER env variable We will now develop some intuition about the data n 3 2 Checking Configuration Fields n n Table of Contents 0 1 n nWe will look at this yaml file from an external dataset provided in L5kit examples Let s look the raster params filed It contains information related to the transformation of the 3D world onto image plane You can also check for various other information n n NOTE We can make our own configuration file n 3 3 Loading Data n n Table of Contents 0 1 n nWe re building a LocalDataManager object This will resolve relative paths from the config using the L5KIT DATA FOLDER env variable we have just set We will work with sample zarr for developing intuition regarding the data Please use train zarr validate zarr and test zarr for actual model training validation and prediction nWe are going to load our sample zarr file The 2020 Lyft competition dataset is stored in four structured arrays scenes frames agents and tl faces n 3 4 Data Overview n n Table of Contents 0 1 n nWe will see each field inside the zarr files train validation and test We will use sample zarr here n 3 4 1 Agents n nAn agent is an observation by the AV of some other detected object Each entry describes the object in terms of its attributes such as position and velocity gives the agent a tracking number to track it over multiple frames but only within the same scene and its most probable label n Points to note n centroid position of agent n extent agent dimension n yaw rotation of an agent with respect to vertical axis A yaw rotation is a movement around the yaw axis of a rigid body that changes the direction it is pointing to the left or right of its direction of motion n velocity speed of the agent n track id unique id to track agent in different frames n label probabilities prabability an agent belong to one of the 17 classes We are only given three labels cyclist pedestrians and cars n Centroid Distribution nWe will see the distribution of centroid now Since centroid column consist of list per sample we will make two new columns centroid x and centroid y n Extent Distribution n nFirst we need to make new columns for extent x extent y and extent z as we have one extent column n Points to note n We can see extent distributions are right skewed n We can see long tails in positive direction n Extent Distribution Scatterplot n Yaw Distribution n Velocity Distribution nAs we can see indeed there are only four types of agents provided in the dataset such as Cars Cyclists Pedestrians and Unknown We can see that Unknown label is more as compared to other three agent labels n 3 4 2 Scenes n Table of contents 0 1 n nA scene is identified by the host i e which car was used to collect it and a start and end time It consists of multiple frames snapshots at discretized time intervals The scene datatype stores references to its corresponding frames in terms of the start and end index within the frames array described in dataframe below The frames in between these indices all correspond to the scene including start index excluding end index n Points to note n frame index interval frame index including start index excluding end index n host car used to collect data n start time start time of scene n end time end time of scene n Frame Index n Points to note n We can see linear trend here n Both host cars collected data within particular time intervals n Host Count n nWe will now see the host counts Ego vehicle used to collect the data n Points to note n We have two host cars which were used to collect the data host a013 and host a101 n 3 4 3 Frames n Table of contents 0 1 n nA frame captures all information that was observed at a time This includes the following fields as mentioned in dataframe below n Points to note n timestamp frame s timestamp n agent index interval agents vehicles cyclists and pedestrians that were captured by the ego s sensors n traffic light faces index interval traffic light index n ego translation position of host car n ego rotation rotation of host car which is collecting data using ego sensors n Ego Translation Distribution n Points to note n We can see the distributions are multi models here n Ego Translation Scatterplot n Ego Rotations Distribution n 3 4 3 Traffic Light Faces n Table of contents 0 1 n nThe traffic light bulbs red green yellow are refered as face n face id unique id for traffic light bulb n traffic light id traffic light status n traffic light face status out of red green yellow which face is active unactive unknown n 3 5 Dataset Package n n Table of Contents 0 1 n nThe dataset package for example already implements PyTorch ready datasets so you can hit the ground running and start coding immediately There are two classes in the dataset package We will be using below two datasets classes to generate inputs and targets n n EgoDataset this dataset iterates over the AV annotations n AgentDataset this dataset iterates over other agents annotations n nBoth of them can be iterated and return multi channel images from the rasterizer along with future trajectories offsets and other information We will see ahead about rasterizer and trajectories soon n nWe need cfg Zarr Dataset ChunkedDataset and rasterizer object to instantiate these n n 3 5 1 ChunckedDataset n n Table of Contents 0 1 n nThe ChunckedDataset 0 2 click class as you can see above returned four structured arrays scenes frames agents and tl faces all are described above in detail Both the EgoDataset and AgentDataset are using zarr dataset object which is made using ChunckedDataset 0 2 n n 3 6 Visualize Autonomous Vehicle n n Table of Contents 0 1 n nNow we will look into the visualisation utility of L5Kit Toolkit https github com lyft l5kit There are two core packages for visualisation n n rasterization contains classes for getting visual data as multi channel tensors and turning them into interpretable RGB images We will talk about these classes in detail shortly Each class inside this rasterization https github com lyft l5kit tree master l5kit l5kit rasterization package conatin has at least a rasterize method to get the tensor and a to rgb method to convert it into an image n n BoxRasterizer this object renders agents e g vehicles or pedestrians as oriented 2D boxes n StubRasterizer this object doesn t do anything It return all black image and can be used for testing n SatelliteRasterizer this object renders an oriented crop from a satellite map n SatBoxRasterizer this object combine a Satellite and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a satellite image n SemanticRasterizer this object renders semantic map which contains lane crosswalk information n SemBoxRasterizer this object combine a Semantic Map and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a semantic image n nTo instantiate each of these object we will use build rasterizer method n n n visualization contains utilities to draw additional information e g trajectories onto RGB images These utilities are commonly used after a to rgb call to add other information to the final visualisation Following utilities are available n n draw arrowed line Draw a single arrowed line in an RGB image n draw trajectory Draw a trajectory on oriented arrow onto an RGB image n draw reference trajectory Draw a trajectory as points onto the image n 3 6 1 Visualizing Various Rasterizer Objects n nWe will visualize different raster objects credits https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n n Table of Contents 0 1 n 3 6 2 Visualize Trajectory Semantic View n n Table of Contents 0 1 n 3 6 3 Visualize Trajectory Satellite View n n Table of Contents 0 1 n nWe can get the satellite view by changing the parameter map type inside raster params in our configuration file cfg n 3 6 4 Visualize Agent n n Table of Contents 0 1 n nWe will visualize our agents using AgentDataset class nThe green box is our AV agent The blue boxes are entities which we are captured by the sensors We want to predict the motion of these entities so that our AV can more effectively predict its path n 3 6 5 Visualize Individual Scene Semantic n n Table of Contents 0 1 n nWe will visualize the scene in depth n The green box is our AV agent and the arrow on top of it represent its motion n The blue boxes are agents cars cyclists predestrians n We can see intersection of roads n In animation we can see the AV is moving on straight path n 3 6 6 Visualize Individual Scene Satellite n n Table of Contents 0 1 n nWe are now going to visualize the satellite view for more detailed understanding n We can see the green AV agent and blue entities cars bicycles and pedestrians n We can see our agent in motion in realtion to the movement of other agents vehicles n 4 Pytorch Baseline n n Table of Contents 0 1 n n 4 1 Configuration n nLet us first make our configurations for training and testing then we will load our data n 4 2 Loading Training Data n n Table of Contents 0 1 n 4 3 Training DataLoader n n Table of Contents 0 1 n n 4 4 Model resnet50 Pytorch n n Table of Contents 0 1 nThis is for the purpose of demonstration only You can set pretrained True and train your model in separate notebook keeping on internet connection for downloading pretrained weights and make inference in separate notebook keeping internet off since subission requires you to turn off your internet connection I will use my trained weights for making inference n 4 5 Compilation n n Table of Contents 0 1 n 4 6 Training n n Table of Contents 0 1 n 4 7 Saving Model n n Table of Contents 0 1 n 5 Prediction and Results n n Table of Contents 0 1 n nBe careful you have turned of your internet connection in order to make submission Please make separate notebook for the inference This notebook is for the purpose of demonstration Train using pretrained resnet weights in one notebook and make inference using another notebook n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 300px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 246 349 n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 350px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 169 83 n n 5 1 Test DataLoader n n Table of Contents 0 1 n 5 2 Getting Predictions n n Table of Contents 0 1 n For loading checkpoint n nWEIGHT FILE kaggle input lyft l5 weights resnet34 300x300 model state 15000 pth n nmodel state torch load WEIGHT FILE map location device n nmodel load state dict model state model state dict nWe need to run below two cell to make predictions and generate submission csv I am going to use my submission csv here since I am getting memory error while making predictions n 5 3 Submission n n Table of Contents 0 1 n 6 References n n Table of Contents 0 1 n n https www twi global com technical knowledge faqs what is an autonomous vehicle n Negative log likelihood https github com lyft l5kit blob master competition md n https www kaggle com pestipeti pytorch baseline train n https self driving lyft com level5 prediction n https www kaggle com corochann lyft deep into the l5kit library n https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n https www kaggle com gpreda lyft first data exploration h1 n Build motion prediction models for self driving vehicles n Table of contents n n 0 Introduction 0 n n 1 Import Packages 1 n n 2 Utility Functions 2 n 3 Basic EDA 3 n 3 1 Data Formats 3 1 n 3 1 1 Introduction 3 1 1 n 3 1 2 Zarr Format 3 1 2 n 3 1 3 Train Validation And Test Zarr 3 1 3 n 3 2 Checking Configuration Fields 3 2 n 3 3 Loading Data 3 3 n 3 4 Data Overview 3 4 n 3 4 1 Agents 3 4 1 n 3 4 2 Scenes 3 4 2 n 3 4 3 Frames 3 4 3 n 3 5 Dataset Package 3 5 n 3 5 1 ChunckedDataset 3 5 1 n 3 6 Visualize Autonomous Vehicle 3 6 n 3 6 1 Visualizing Various Rasterizer Objects 3 6 1 n 3 6 2 Visualize Trajectory Semantic View 3 6 2 n 3 6 3 Visualize Trajectory Satellite View 3 6 3 n 3 6 4 Visualize Agent 3 6 4 n 3 6 5 Visualize Individual Scene Semantic 3 6 5 n 3 6 6 Visualize Individual Scene Satellite 3 6 6 n n 4 Pytorch Baseline 4 n 4 1 Configuration 4 1 n 4 2 Loading Training Data 4 2 n 4 3 Training DataLoader 4 3 n 4 4 Model resnet50 Pytorch 4 4 n 4 5 Compilation 4 5 n 4 6 Training 4 6 n 4 7 Saving Model 4 7 n n 5 Prediction and Results 5 n 5 1 Test DataLoader 5 1 n 5 2 Getting Predictions 5 2 n 5 2 Submission 5 3 n n 6 Reference 6 n 0 Introduction n n Table of Contents 0 1 n nThis competition is hosted by ridesharing company Lyft https www lyft com which started Level 5 https self driving lyft com level5 self driving division to tackle the challenges in the field of self driving cars n nIn this competition our task is to build motion prediction models for self driving vehicles Model which can predict the movement of traffic agents around the Autonomous Vehicles such as cars cyclists and pedestrians etc We are required to predict how these different agents move in Autonomous Vehicles s environment n n What is Autonomous Vehicle n nAn autonomous vehicle or a driverless vehicle is one that is able to operate itself and perform necessary functions without any human intervention through ability to sense its surroundings n nAn autonomous vehicle utilises a fully automated driving system in order to allow the vehicle to respond to external conditions that a human driver would manage n n Competition Data n nThis dataset https www kaggle com c lyft motion prediction autonomous vehicles data is the largest collection of the traffic agent motion data This dataset includes the logs of movement of cars cyclists pedestrians and other traffic agents encountered by Lyft s autonomous fleet These logs come from processing raw lidar camera and radar data through our team s perception systems and are ideal for training motion prediction models The dataset includes n n 1000 hours of traffic agent movement n 16k miles of data from 23 vehicle n 15k semantic map annotations n nHere is the reserach paper of Prediction Dataset https paperswithcode com paper one thousand and one hours self driving n nThe dataset consists of 170 000 scenes capturing the environment around the autonomous vehicle Each scene encodes the state of the vehicle s surroundings at a given point in time source https self driving lyft com level5 prediction n n n n n n n nThis baseline solution is trained on over 2 million samples from the agent locations contained within the dataset The model predicts a single agent at a time First a raster generates a bird s eye view BEV top down raster which encodes all agents and the map The network infers the future coordinates of the agent based upon this raster n n n n n What we are predicting n nOur task in the competition is to predict the motion of external objects such as cars cyclist pedestrains etc in order to assist the self driving car We have to predict the location of objects agents in the next 50 frames n n Evaluation Metric Negative log likelihood n nWe calculate the negative log likelihood of the ground truth data given the multi modal predictions You can get more information here https www kaggle com c lyft motion prediction autonomous vehicles overview evaluation n n n https camo githubusercontent com b3634eea5be5501318957e21086781666018efa1 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239 n https camo githubusercontent com 8048a110a20827715a17eb76f8039302a576d503 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302b25323025354373756d5f742532302535436c6f672532302535436d61746863616c2537424e253744253238785f74253743253543626172253742782537445f742535456b2532432532302535437369676d61253344312532392532302535436d61746863616c2537424e253744253238795f74253743253543626172253742792537445f742535456b2532432532302535437369676d6125334431253239253744 n https camo githubusercontent com 9ba94f5c0c40666d66b93fba994cc5f7623ebd98 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302d25354366726163253742312537442537423225374425323025354373756d5f74253230253238253543626172253742782537445f742535456b2532302d253230785f74253239253545322532302b253230253238253543626172253742792537445f742535456b2532302d253230795f7425323925354532253744 n Lyft s Autonomous Vehicle AV n n Lyft s AV Introducton video n 1 Import Packages n n Table of Contents 0 1 n nWe are required to use L5Kit toolkit https github com lyft l5kit provided by the competition host to prepare preprocess data trian and evaluate the model Please add this utility script https www kaggle com pestipeti lyft l5kit unofficial fix provided by Peter s https www kaggle com pestipeti first by clicking on the Add data section inside your notebook before starting The L5Kit toolkit https github com lyft l5kit has some issue right now You can check this https www kaggle com c lyft motion prediction autonomous vehicles discussion 177125 discussion here for more information n n L5Kit is a library which lets you n n Load driving scenes from zarr files n Read semantic maps n Read aerial maps n Create birds eye view BEV images which represent a scene around an AV or another vehicle n Sample data n Train neural networks n Visualize results n 2 Utility Functions n nThanks to Trigram https www kaggle com nxrprime for providing this function n 3 EDA n n Table of Contents 0 1 n nThe data is huge around 22 GB We will use Lyft s L5Kit https github com lyft l5kit to process our data loading and visualization and training n n 3 1 Data Formats n nWe need to be familiar with the data we are using Let s dive in n n 3 1 1 Introduction n nThe L5Kit toolkit uses data format that consists of a set of numpy structured arrays Structured arrays https numpy org doc stable user basics rec html are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields The structured array can store various types of features Structured arrays are stored in memory in an interleaved format this means that one row or sample is grouped together in memory Let us take an example to understand this nlet s add some data in this structured array nAs we can see structured arrays allow us to mix different data types into a single array We will develop more intuition ahead n 3 1 2 Zarr Format n n Table of Contents 0 1 n nThe L5Kit uses zarr format to store and read these structured numpy arrays The data is available in zarr file format which can be easily load using the L5Kit https github com lyft l5kit Most of the traditional numpy operations can be handled using zarr files The zarr files are flat compact and highly performant for loading Each of the zarr file contains n n scenes driving episodes acquired from a given vehicle n frames snapshots in time of the pose of the vehicle A frame is a snapshot in time which consists of ego pose time and multiple agent states n agents a generic entity captured by the vehicle s sensors Note that only 4 of the 17 possible agent label probabilities are present in this dataset Each agent state describes the position orientation bounds and type n agents mask a mask that for train and validation masks out objects that aren t useful for training In test the mask provided in files as mask npz masks out any test object for which predictions are NOT required n traffic light faces traffic light information n n 3 1 3 Train Validation And Test Zarr n n Table of Contents 0 1 nWe can see that our train zarr file has set of 4 arrays All this 4 fields are described above in the Data Format section above Let us check these fields We will check 1 observation from each nThe data is expected to live in a folder that can be configured using the L5KIT DATA FOLDER env variable We will now develop some intuition about the data n 3 2 Checking Configuration Fields n n Table of Contents 0 1 n nWe will look at this yaml file from an external dataset provided in L5kit examples Let s look the raster params filed It contains information related to the transformation of the 3D world onto image plane You can also check for various other information n n NOTE We can make our own configuration file n 3 3 Loading Data n n Table of Contents 0 1 n nWe re building a LocalDataManager object This will resolve relative paths from the config using the L5KIT DATA FOLDER env variable we have just set We will work with sample zarr for developing intuition regarding the data Please use train zarr validate zarr and test zarr for actual model training validation and prediction nWe are going to load our sample zarr file The 2020 Lyft competition dataset is stored in four structured arrays scenes frames agents and tl faces n 3 4 Data Overview n n Table of Contents 0 1 n nWe will see each field inside the zarr files train validation and test We will use sample zarr here n 3 4 1 Agents n nAn agent is an observation by the AV of some other detected object Each entry describes the object in terms of its attributes such as position and velocity gives the agent a tracking number to track it over multiple frames but only within the same scene and its most probable label n Points to note n centroid position of agent n extent agent dimension n yaw rotation of an agent with respect to vertical axis A yaw rotation is a movement around the yaw axis of a rigid body that changes the direction it is pointing to the left or right of its direction of motion n velocity speed of the agent n track id unique id to track agent in different frames n label probabilities prabability an agent belong to one of the 17 classes We are only given three labels cyclist pedestrians and cars n Centroid Distribution nWe will see the distribution of centroid now Since centroid column consist of list per sample we will make two new columns centroid x and centroid y n Extent Distribution n nFirst we need to make new columns for extent x extent y and extent z as we have one extent column n Points to note n We can see extent distributions are right skewed n We can see long tails in positive direction n Extent Distribution Scatterplot n Yaw Distribution n Velocity Distribution nAs we can see indeed there are only four types of agents provided in the dataset such as Cars Cyclists Pedestrians and Unknown We can see that Unknown label is more as compared to other three agent labels n 3 4 2 Scenes n Table of contents 0 1 n nA scene is identified by the host i e which car was used to collect it and a start and end time It consists of multiple frames snapshots at discretized time intervals The scene datatype stores references to its corresponding frames in terms of the start and end index within the frames array described in dataframe below The frames in between these indices all correspond to the scene including start index excluding end index n Points to note n frame index interval frame index including start index excluding end index n host car used to collect data n start time start time of scene n end time end time of scene n Frame Index n Points to note n We can see linear trend here n Both host cars collected data within particular time intervals n Host Count n nWe will now see the host counts Ego vehicle used to collect the data n Points to note n We have two host cars which were used to collect the data host a013 and host a101 n 3 4 3 Frames n Table of contents 0 1 n nA frame captures all information that was observed at a time This includes the following fields as mentioned in dataframe below n Points to note n timestamp frame s timestamp n agent index interval agents vehicles cyclists and pedestrians that were captured by the ego s sensors n traffic light faces index interval traffic light index n ego translation position of host car n ego rotation rotation of host car which is collecting data using ego sensors n Ego Translation Distribution n Points to note n We can see the distributions are multi models here n Ego Translation Scatterplot n Ego Rotations Distribution n 3 4 3 Traffic Light Faces n Table of contents 0 1 n nThe traffic light bulbs red green yellow are refered as face n face id unique id for traffic light bulb n traffic light id traffic light status n traffic light face status out of red green yellow which face is active unactive unknown n 3 5 Dataset Package n n Table of Contents 0 1 n nThe dataset package for example already implements PyTorch ready datasets so you can hit the ground running and start coding immediately There are two classes in the dataset package We will be using below two datasets classes to generate inputs and targets n n EgoDataset this dataset iterates over the AV annotations n AgentDataset this dataset iterates over other agents annotations n nBoth of them can be iterated and return multi channel images from the rasterizer along with future trajectories offsets and other information We will see ahead about rasterizer and trajectories soon n nWe need cfg Zarr Dataset ChunkedDataset and rasterizer object to instantiate these n n 3 5 1 ChunckedDataset n n Table of Contents 0 1 n nThe ChunckedDataset 0 2 click class as you can see above returned four structured arrays scenes frames agents and tl faces all are described above in detail Both the EgoDataset and AgentDataset are using zarr dataset object which is made using ChunckedDataset 0 2 n n 3 6 Visualize Autonomous Vehicle n n Table of Contents 0 1 n nNow we will look into the visualisation utility of L5Kit Toolkit https github com lyft l5kit There are two core packages for visualisation n n rasterization contains classes for getting visual data as multi channel tensors and turning them into interpretable RGB images We will talk about these classes in detail shortly Each class inside this rasterization https github com lyft l5kit tree master l5kit l5kit rasterization package conatin has at least a rasterize method to get the tensor and a to rgb method to convert it into an image n n BoxRasterizer this object renders agents e g vehicles or pedestrians as oriented 2D boxes n StubRasterizer this object doesn t do anything It return all black image and can be used for testing n SatelliteRasterizer this object renders an oriented crop from a satellite map n SatBoxRasterizer this object combine a Satellite and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a satellite image n SemanticRasterizer this object renders semantic map which contains lane crosswalk information n SemBoxRasterizer this object combine a Semantic Map and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a semantic image n nTo instantiate each of these object we will use build rasterizer method n n n visualization contains utilities to draw additional information e g trajectories onto RGB images These utilities are commonly used after a to rgb call to add other information to the final visualisation Following utilities are available n n draw arrowed line Draw a single arrowed line in an RGB image n draw trajectory Draw a trajectory on oriented arrow onto an RGB image n draw reference trajectory Draw a trajectory as points onto the image n 3 6 1 Visualizing Various Rasterizer Objects n nWe will visualize different raster objects credits https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n n Table of Contents 0 1 n 3 6 2 Visualize Trajectory Semantic View n n Table of Contents 0 1 n 3 6 3 Visualize Trajectory Satellite View n n Table of Contents 0 1 n nWe can get the satellite view by changing the parameter map type inside raster params in our configuration file cfg n 3 6 4 Visualize Agent n n Table of Contents 0 1 n nWe will visualize our agents using AgentDataset class nThe green box is our AV agent The blue boxes are entities which we are captured by the sensors We want to predict the motion of these entities so that our AV can more effectively predict its path n 3 6 5 Visualize Individual Scene Semantic n n Table of Contents 0 1 n nWe will visualize the scene in depth n The green box is our AV agent and the arrow on top of it represent its motion n The blue boxes are agents cars cyclists predestrians n We can see intersection of roads n In animation we can see the AV is moving on straight path n 3 6 6 Visualize Individual Scene Satellite n n Table of Contents 0 1 n nWe are now going to visualize the satellite view for more detailed understanding n We can see the green AV agent and blue entities cars bicycles and pedestrians n We can see our agent in motion in realtion to the movement of other agents vehicles n 4 Pytorch Baseline n n Table of Contents 0 1 n n 4 1 Configuration n nLet us first make our configurations for training and testing then we will load our data n 4 2 Loading Training Data n n Table of Contents 0 1 n 4 3 Training DataLoader n n Table of Contents 0 1 n n 4 4 Model resnet50 Pytorch n n Table of Contents 0 1 nThis is for the purpose of demonstration only You can set pretrained True and train your model in separate notebook keeping on internet connection for downloading pretrained weights and make inference in separate notebook keeping internet off since subission requires you to turn off your internet connection I will use my trained weights for making inference n 4 5 Compilation n n Table of Contents 0 1 n 4 6 Training n n Table of Contents 0 1 n 4 7 Saving Model n n Table of Contents 0 1 n 5 Prediction and Results n n Table of Contents 0 1 n nBe careful you have turned of your internet connection in order to make submission Please make separate notebook for the inference This notebook is for the purpose of demonstration Train using pretrained resnet weights in one notebook and make inference using another notebook n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 300px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 246 349 n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 350px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 169 83 n n 5 1 Test DataLoader n n Table of Contents 0 1 n 5 2 Getting Predictions n n Table of Contents 0 1 n For loading checkpoint n nWEIGHT FILE kaggle input lyft l5 weights resnet34 300x300 model state 15000 pth n nmodel state torch load WEIGHT FILE map location device n nmodel load state dict model state model state dict nWe need to run below two cell to make predictions and generate submission csv I am going to use my submission csv here since I am getting memory error while making predictions n 5 3 Submission n n Table of Contents 0 1 n 6 References n n Table of Contents 0 1 n n https www twi global com technical knowledge faqs what is an autonomous vehicle n Negative log likelihood https github com lyft l5kit blob master competition md n https www kaggle com pestipeti pytorch baseline train n https self driving lyft com level5 prediction n https www kaggle com corochann lyft deep into the l5kit library n https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n https www kaggle com gpreda lyft first data exploration h3 Build motion prediction models for self driving vehicles n Table of contents n n 0 Introduction 0 n n 1 Import Packages 1 n n 2 Utility Functions 2 n 3 Basic EDA 3 n 3 1 Data Formats 3 1 n 3 1 1 Introduction 3 1 1 n 3 1 2 Zarr Format 3 1 2 n 3 1 3 Train Validation And Test Zarr 3 1 3 n 3 2 Checking Configuration Fields 3 2 n 3 3 Loading Data 3 3 n 3 4 Data Overview 3 4 n 3 4 1 Agents 3 4 1 n 3 4 2 Scenes 3 4 2 n 3 4 3 Frames 3 4 3 n 3 5 Dataset Package 3 5 n 3 5 1 ChunckedDataset 3 5 1 n 3 6 Visualize Autonomous Vehicle 3 6 n 3 6 1 Visualizing Various Rasterizer Objects 3 6 1 n 3 6 2 Visualize Trajectory Semantic View 3 6 2 n 3 6 3 Visualize Trajectory Satellite View 3 6 3 n 3 6 4 Visualize Agent 3 6 4 n 3 6 5 Visualize Individual Scene Semantic 3 6 5 n 3 6 6 Visualize Individual Scene Satellite 3 6 6 n n 4 Pytorch Baseline 4 n 4 1 Configuration 4 1 n 4 2 Loading Training Data 4 2 n 4 3 Training DataLoader 4 3 n 4 4 Model resnet50 Pytorch 4 4 n 4 5 Compilation 4 5 n 4 6 Training 4 6 n 4 7 Saving Model 4 7 n n 5 Prediction and Results 5 n 5 1 Test DataLoader 5 1 n 5 2 Getting Predictions 5 2 n 5 2 Submission 5 3 n n 6 Reference 6 n 0 Introduction n n Table of Contents 0 1 n nThis competition is hosted by ridesharing company Lyft https www lyft com which started Level 5 https self driving lyft com level5 self driving division to tackle the challenges in the field of self driving cars n nIn this competition our task is to build motion prediction models for self driving vehicles Model which can predict the movement of traffic agents around the Autonomous Vehicles such as cars cyclists and pedestrians etc We are required to predict how these different agents move in Autonomous Vehicles s environment n n What is Autonomous Vehicle n nAn autonomous vehicle or a driverless vehicle is one that is able to operate itself and perform necessary functions without any human intervention through ability to sense its surroundings n nAn autonomous vehicle utilises a fully automated driving system in order to allow the vehicle to respond to external conditions that a human driver would manage n n Competition Data n nThis dataset https www kaggle com c lyft motion prediction autonomous vehicles data is the largest collection of the traffic agent motion data This dataset includes the logs of movement of cars cyclists pedestrians and other traffic agents encountered by Lyft s autonomous fleet These logs come from processing raw lidar camera and radar data through our team s perception systems and are ideal for training motion prediction models The dataset includes n n 1000 hours of traffic agent movement n 16k miles of data from 23 vehicle n 15k semantic map annotations n nHere is the reserach paper of Prediction Dataset https paperswithcode com paper one thousand and one hours self driving n nThe dataset consists of 170 000 scenes capturing the environment around the autonomous vehicle Each scene encodes the state of the vehicle s surroundings at a given point in time source https self driving lyft com level5 prediction n n n n n n n nThis baseline solution is trained on over 2 million samples from the agent locations contained within the dataset The model predicts a single agent at a time First a raster generates a bird s eye view BEV top down raster which encodes all agents and the map The network infers the future coordinates of the agent based upon this raster n n n n n What we are predicting n nOur task in the competition is to predict the motion of external objects such as cars cyclist pedestrains etc in order to assist the self driving car We have to predict the location of objects agents in the next 50 frames n n Evaluation Metric Negative log likelihood n nWe calculate the negative log likelihood of the ground truth data given the multi modal predictions You can get more information here https www kaggle com c lyft motion prediction autonomous vehicles overview evaluation n n n https camo githubusercontent com b3634eea5be5501318957e21086781666018efa1 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239 n https camo githubusercontent com 8048a110a20827715a17eb76f8039302a576d503 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302b25323025354373756d5f742532302535436c6f672532302535436d61746863616c2537424e253744253238785f74253743253543626172253742782537445f742535456b2532432532302535437369676d61253344312532392532302535436d61746863616c2537424e253744253238795f74253743253543626172253742792537445f742535456b2532432532302535437369676d6125334431253239253744 n https camo githubusercontent com 9ba94f5c0c40666d66b93fba994cc5f7623ebd98 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302d25354366726163253742312537442537423225374425323025354373756d5f74253230253238253543626172253742782537445f742535456b2532302d253230785f74253239253545322532302b253230253238253543626172253742792537445f742535456b2532302d253230795f7425323925354532253744 n Lyft s Autonomous Vehicle AV n n Lyft s AV Introducton video n 1 Import Packages n n Table of Contents 0 1 n nWe are required to use L5Kit toolkit https github com lyft l5kit provided by the competition host to prepare preprocess data trian and evaluate the model Please add this utility script https www kaggle com pestipeti lyft l5kit unofficial fix provided by Peter s https www kaggle com pestipeti first by clicking on the Add data section inside your notebook before starting The L5Kit toolkit https github com lyft l5kit has some issue right now You can check this https www kaggle com c lyft motion prediction autonomous vehicles discussion 177125 discussion here for more information n n L5Kit is a library which lets you n n Load driving scenes from zarr files n Read semantic maps n Read aerial maps n Create birds eye view BEV images which represent a scene around an AV or another vehicle n Sample data n Train neural networks n Visualize results n 2 Utility Functions n nThanks to Trigram https www kaggle com nxrprime for providing this function n 3 EDA n n Table of Contents 0 1 n nThe data is huge around 22 GB We will use Lyft s L5Kit https github com lyft l5kit to process our data loading and visualization and training n n 3 1 Data Formats n nWe need to be familiar with the data we are using Let s dive in n n 3 1 1 Introduction n nThe L5Kit toolkit uses data format that consists of a set of numpy structured arrays Structured arrays https numpy org doc stable user basics rec html are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields The structured array can store various types of features Structured arrays are stored in memory in an interleaved format this means that one row or sample is grouped together in memory Let us take an example to understand this nlet s add some data in this structured array nAs we can see structured arrays allow us to mix different data types into a single array We will develop more intuition ahead n 3 1 2 Zarr Format n n Table of Contents 0 1 n nThe L5Kit uses zarr format to store and read these structured numpy arrays The data is available in zarr file format which can be easily load using the L5Kit https github com lyft l5kit Most of the traditional numpy operations can be handled using zarr files The zarr files are flat compact and highly performant for loading Each of the zarr file contains n n scenes driving episodes acquired from a given vehicle n frames snapshots in time of the pose of the vehicle A frame is a snapshot in time which consists of ego pose time and multiple agent states n agents a generic entity captured by the vehicle s sensors Note that only 4 of the 17 possible agent label probabilities are present in this dataset Each agent state describes the position orientation bounds and type n agents mask a mask that for train and validation masks out objects that aren t useful for training In test the mask provided in files as mask npz masks out any test object for which predictions are NOT required n traffic light faces traffic light information n n 3 1 3 Train Validation And Test Zarr n n Table of Contents 0 1 nWe can see that our train zarr file has set of 4 arrays All this 4 fields are described above in the Data Format section above Let us check these fields We will check 1 observation from each nThe data is expected to live in a folder that can be configured using the L5KIT DATA FOLDER env variable We will now develop some intuition about the data n 3 2 Checking Configuration Fields n n Table of Contents 0 1 n nWe will look at this yaml file from an external dataset provided in L5kit examples Let s look the raster params filed It contains information related to the transformation of the 3D world onto image plane You can also check for various other information n n NOTE We can make our own configuration file n 3 3 Loading Data n n Table of Contents 0 1 n nWe re building a LocalDataManager object This will resolve relative paths from the config using the L5KIT DATA FOLDER env variable we have just set We will work with sample zarr for developing intuition regarding the data Please use train zarr validate zarr and test zarr for actual model training validation and prediction nWe are going to load our sample zarr file The 2020 Lyft competition dataset is stored in four structured arrays scenes frames agents and tl faces n 3 4 Data Overview n n Table of Contents 0 1 n nWe will see each field inside the zarr files train validation and test We will use sample zarr here n 3 4 1 Agents n nAn agent is an observation by the AV of some other detected object Each entry describes the object in terms of its attributes such as position and velocity gives the agent a tracking number to track it over multiple frames but only within the same scene and its most probable label n Points to note n centroid position of agent n extent agent dimension n yaw rotation of an agent with respect to vertical axis A yaw rotation is a movement around the yaw axis of a rigid body that changes the direction it is pointing to the left or right of its direction of motion n velocity speed of the agent n track id unique id to track agent in different frames n label probabilities prabability an agent belong to one of the 17 classes We are only given three labels cyclist pedestrians and cars n Centroid Distribution nWe will see the distribution of centroid now Since centroid column consist of list per sample we will make two new columns centroid x and centroid y n Extent Distribution n nFirst we need to make new columns for extent x extent y and extent z as we have one extent column n Points to note n We can see extent distributions are right skewed n We can see long tails in positive direction n Extent Distribution Scatterplot n Yaw Distribution n Velocity Distribution nAs we can see indeed there are only four types of agents provided in the dataset such as Cars Cyclists Pedestrians and Unknown We can see that Unknown label is more as compared to other three agent labels n 3 4 2 Scenes n Table of contents 0 1 n nA scene is identified by the host i e which car was used to collect it and a start and end time It consists of multiple frames snapshots at discretized time intervals The scene datatype stores references to its corresponding frames in terms of the start and end index within the frames array described in dataframe below The frames in between these indices all correspond to the scene including start index excluding end index n Points to note n frame index interval frame index including start index excluding end index n host car used to collect data n start time start time of scene n end time end time of scene n Frame Index n Points to note n We can see linear trend here n Both host cars collected data within particular time intervals n Host Count n nWe will now see the host counts Ego vehicle used to collect the data n Points to note n We have two host cars which were used to collect the data host a013 and host a101 n 3 4 3 Frames n Table of contents 0 1 n nA frame captures all information that was observed at a time This includes the following fields as mentioned in dataframe below n Points to note n timestamp frame s timestamp n agent index interval agents vehicles cyclists and pedestrians that were captured by the ego s sensors n traffic light faces index interval traffic light index n ego translation position of host car n ego rotation rotation of host car which is collecting data using ego sensors n Ego Translation Distribution n Points to note n We can see the distributions are multi models here n Ego Translation Scatterplot n Ego Rotations Distribution n 3 4 3 Traffic Light Faces n Table of contents 0 1 n nThe traffic light bulbs red green yellow are refered as face n face id unique id for traffic light bulb n traffic light id traffic light status n traffic light face status out of red green yellow which face is active unactive unknown n 3 5 Dataset Package n n Table of Contents 0 1 n nThe dataset package for example already implements PyTorch ready datasets so you can hit the ground running and start coding immediately There are two classes in the dataset package We will be using below two datasets classes to generate inputs and targets n n EgoDataset this dataset iterates over the AV annotations n AgentDataset this dataset iterates over other agents annotations n nBoth of them can be iterated and return multi channel images from the rasterizer along with future trajectories offsets and other information We will see ahead about rasterizer and trajectories soon n nWe need cfg Zarr Dataset ChunkedDataset and rasterizer object to instantiate these n n 3 5 1 ChunckedDataset n n Table of Contents 0 1 n nThe ChunckedDataset 0 2 click class as you can see above returned four structured arrays scenes frames agents and tl faces all are described above in detail Both the EgoDataset and AgentDataset are using zarr dataset object which is made using ChunckedDataset 0 2 n n 3 6 Visualize Autonomous Vehicle n n Table of Contents 0 1 n nNow we will look into the visualisation utility of L5Kit Toolkit https github com lyft l5kit There are two core packages for visualisation n n rasterization contains classes for getting visual data as multi channel tensors and turning them into interpretable RGB images We will talk about these classes in detail shortly Each class inside this rasterization https github com lyft l5kit tree master l5kit l5kit rasterization package conatin has at least a rasterize method to get the tensor and a to rgb method to convert it into an image n n BoxRasterizer this object renders agents e g vehicles or pedestrians as oriented 2D boxes n StubRasterizer this object doesn t do anything It return all black image and can be used for testing n SatelliteRasterizer this object renders an oriented crop from a satellite map n SatBoxRasterizer this object combine a Satellite and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a satellite image n SemanticRasterizer this object renders semantic map which contains lane crosswalk information n SemBoxRasterizer this object combine a Semantic Map and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a semantic image n nTo instantiate each of these object we will use build rasterizer method n n n visualization contains utilities to draw additional information e g trajectories onto RGB images These utilities are commonly used after a to rgb call to add other information to the final visualisation Following utilities are available n n draw arrowed line Draw a single arrowed line in an RGB image n draw trajectory Draw a trajectory on oriented arrow onto an RGB image n draw reference trajectory Draw a trajectory as points onto the image n 3 6 1 Visualizing Various Rasterizer Objects n nWe will visualize different raster objects credits https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n n Table of Contents 0 1 n 3 6 2 Visualize Trajectory Semantic View n n Table of Contents 0 1 n 3 6 3 Visualize Trajectory Satellite View n n Table of Contents 0 1 n nWe can get the satellite view by changing the parameter map type inside raster params in our configuration file cfg n 3 6 4 Visualize Agent n n Table of Contents 0 1 n nWe will visualize our agents using AgentDataset class nThe green box is our AV agent The blue boxes are entities which we are captured by the sensors We want to predict the motion of these entities so that our AV can more effectively predict its path n 3 6 5 Visualize Individual Scene Semantic n n Table of Contents 0 1 n nWe will visualize the scene in depth n The green box is our AV agent and the arrow on top of it represent its motion n The blue boxes are agents cars cyclists predestrians n We can see intersection of roads n In animation we can see the AV is moving on straight path n 3 6 6 Visualize Individual Scene Satellite n n Table of Contents 0 1 n nWe are now going to visualize the satellite view for more detailed understanding n We can see the green AV agent and blue entities cars bicycles and pedestrians n We can see our agent in motion in realtion to the movement of other agents vehicles n 4 Pytorch Baseline n n Table of Contents 0 1 n n 4 1 Configuration n nLet us first make our configurations for training and testing then we will load our data n 4 2 Loading Training Data n n Table of Contents 0 1 n 4 3 Training DataLoader n n Table of Contents 0 1 n n 4 4 Model resnet50 Pytorch n n Table of Contents 0 1 nThis is for the purpose of demonstration only You can set pretrained True and train your model in separate notebook keeping on internet connection for downloading pretrained weights and make inference in separate notebook keeping internet off since subission requires you to turn off your internet connection I will use my trained weights for making inference n 4 5 Compilation n n Table of Contents 0 1 n 4 6 Training n n Table of Contents 0 1 n 4 7 Saving Model n n Table of Contents 0 1 n 5 Prediction and Results n n Table of Contents 0 1 n nBe careful you have turned of your internet connection in order to make submission Please make separate notebook for the inference This notebook is for the purpose of demonstration Train using pretrained resnet weights in one notebook and make inference using another notebook n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 300px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 246 349 n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 350px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 169 83 n n 5 1 Test DataLoader n n Table of Contents 0 1 n 5 2 Getting Predictions n n Table of Contents 0 1 n For loading checkpoint n nWEIGHT FILE kaggle input lyft l5 weights resnet34 300x300 model state 15000 pth n nmodel state torch load WEIGHT FILE map location device n nmodel load state dict model state model state dict nWe need to run below two cell to make predictions and generate submission csv I am going to use my submission csv here since I am getting memory error while making predictions n 5 3 Submission n n Table of Contents 0 1 n 6 References n n Table of Contents 0 1 n n https www twi global com technical knowledge faqs what is an autonomous vehicle n Negative log likelihood https github com lyft l5kit blob master competition md n https www kaggle com pestipeti pytorch baseline train n https self driving lyft com level5 prediction n https www kaggle com corochann lyft deep into the l5kit library n https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n https www kaggle com gpreda lyft first data exploration h3 n Table of contents n n 0 Introduction 0 n n 1 Import Packages 1 n n 2 Utility Functions 2 n 3 Basic EDA 3 n 3 1 Data Formats 3 1 n 3 1 1 Introduction 3 1 1 n 3 1 2 Zarr Format 3 1 2 n 3 1 3 Train Validation And Test Zarr 3 1 3 n 3 2 Checking Configuration Fields 3 2 n 3 3 Loading Data 3 3 n 3 4 Data Overview 3 4 n 3 4 1 Agents 3 4 1 n 3 4 2 Scenes 3 4 2 n 3 4 3 Frames 3 4 3 n 3 5 Dataset Package 3 5 n 3 5 1 ChunckedDataset 3 5 1 n 3 6 Visualize Autonomous Vehicle 3 6 n 3 6 1 Visualizing Various Rasterizer Objects 3 6 1 n 3 6 2 Visualize Trajectory Semantic View 3 6 2 n 3 6 3 Visualize Trajectory Satellite View 3 6 3 n 3 6 4 Visualize Agent 3 6 4 n 3 6 5 Visualize Individual Scene Semantic 3 6 5 n 3 6 6 Visualize Individual Scene Satellite 3 6 6 n n 4 Pytorch Baseline 4 n 4 1 Configuration 4 1 n 4 2 Loading Training Data 4 2 n 4 3 Training DataLoader 4 3 n 4 4 Model resnet50 Pytorch 4 4 n 4 5 Compilation 4 5 n 4 6 Training 4 6 n 4 7 Saving Model 4 7 n n 5 Prediction and Results 5 n 5 1 Test DataLoader 5 1 n 5 2 Getting Predictions 5 2 n 5 2 Submission 5 3 n n 6 Reference 6 n 0 Introduction n n Table of Contents 0 1 n nThis competition is hosted by ridesharing company Lyft https www lyft com which started Level 5 https self driving lyft com level5 self driving division to tackle the challenges in the field of self driving cars n nIn this competition our task is to build motion prediction models for self driving vehicles Model which can predict the movement of traffic agents around the Autonomous Vehicles such as cars cyclists and pedestrians etc We are required to predict how these different agents move in Autonomous Vehicles s environment n n What is Autonomous Vehicle n nAn autonomous vehicle or a driverless vehicle is one that is able to operate itself and perform necessary functions without any human intervention through ability to sense its surroundings n nAn autonomous vehicle utilises a fully automated driving system in order to allow the vehicle to respond to external conditions that a human driver would manage n n Competition Data n nThis dataset https www kaggle com c lyft motion prediction autonomous vehicles data is the largest collection of the traffic agent motion data This dataset includes the logs of movement of cars cyclists pedestrians and other traffic agents encountered by Lyft s autonomous fleet These logs come from processing raw lidar camera and radar data through our team s perception systems and are ideal for training motion prediction models The dataset includes n n 1000 hours of traffic agent movement n 16k miles of data from 23 vehicle n 15k semantic map annotations n nHere is the reserach paper of Prediction Dataset https paperswithcode com paper one thousand and one hours self driving n nThe dataset consists of 170 000 scenes capturing the environment around the autonomous vehicle Each scene encodes the state of the vehicle s surroundings at a given point in time source https self driving lyft com level5 prediction n n n n n n n nThis baseline solution is trained on over 2 million samples from the agent locations contained within the dataset The model predicts a single agent at a time First a raster generates a bird s eye view BEV top down raster which encodes all agents and the map The network infers the future coordinates of the agent based upon this raster n n n n n What we are predicting n nOur task in the competition is to predict the motion of external objects such as cars cyclist pedestrains etc in order to assist the self driving car We have to predict the location of objects agents in the next 50 frames n n Evaluation Metric Negative log likelihood n nWe calculate the negative log likelihood of the ground truth data given the multi modal predictions You can get more information here https www kaggle com c lyft motion prediction autonomous vehicles overview evaluation n n n https camo githubusercontent com b3634eea5be5501318957e21086781666018efa1 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239 n https camo githubusercontent com 8048a110a20827715a17eb76f8039302a576d503 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302b25323025354373756d5f742532302535436c6f672532302535436d61746863616c2537424e253744253238785f74253743253543626172253742782537445f742535456b2532432532302535437369676d61253344312532392532302535436d61746863616c2537424e253744253238795f74253743253543626172253742792537445f742535456b2532432532302535437369676d6125334431253239253744 n https camo githubusercontent com 9ba94f5c0c40666d66b93fba994cc5f7623ebd98 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302d25354366726163253742312537442537423225374425323025354373756d5f74253230253238253543626172253742782537445f742535456b2532302d253230785f74253239253545322532302b253230253238253543626172253742792537445f742535456b2532302d253230795f7425323925354532253744 n Lyft s Autonomous Vehicle AV n n Lyft s AV Introducton video n 1 Import Packages n n Table of Contents 0 1 n nWe are required to use L5Kit toolkit https github com lyft l5kit provided by the competition host to prepare preprocess data trian and evaluate the model Please add this utility script https www kaggle com pestipeti lyft l5kit unofficial fix provided by Peter s https www kaggle com pestipeti first by clicking on the Add data section inside your notebook before starting The L5Kit toolkit https github com lyft l5kit has some issue right now You can check this https www kaggle com c lyft motion prediction autonomous vehicles discussion 177125 discussion here for more information n n L5Kit is a library which lets you n n Load driving scenes from zarr files n Read semantic maps n Read aerial maps n Create birds eye view BEV images which represent a scene around an AV or another vehicle n Sample data n Train neural networks n Visualize results n 2 Utility Functions n nThanks to Trigram https www kaggle com nxrprime for providing this function n 3 EDA n n Table of Contents 0 1 n nThe data is huge around 22 GB We will use Lyft s L5Kit https github com lyft l5kit to process our data loading and visualization and training n n 3 1 Data Formats n nWe need to be familiar with the data we are using Let s dive in n n 3 1 1 Introduction n nThe L5Kit toolkit uses data format that consists of a set of numpy structured arrays Structured arrays https numpy org doc stable user basics rec html are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields The structured array can store various types of features Structured arrays are stored in memory in an interleaved format this means that one row or sample is grouped together in memory Let us take an example to understand this nlet s add some data in this structured array nAs we can see structured arrays allow us to mix different data types into a single array We will develop more intuition ahead n 3 1 2 Zarr Format n n Table of Contents 0 1 n nThe L5Kit uses zarr format to store and read these structured numpy arrays The data is available in zarr file format which can be easily load using the L5Kit https github com lyft l5kit Most of the traditional numpy operations can be handled using zarr files The zarr files are flat compact and highly performant for loading Each of the zarr file contains n n scenes driving episodes acquired from a given vehicle n frames snapshots in time of the pose of the vehicle A frame is a snapshot in time which consists of ego pose time and multiple agent states n agents a generic entity captured by the vehicle s sensors Note that only 4 of the 17 possible agent label probabilities are present in this dataset Each agent state describes the position orientation bounds and type n agents mask a mask that for train and validation masks out objects that aren t useful for training In test the mask provided in files as mask npz masks out any test object for which predictions are NOT required n traffic light faces traffic light information n n 3 1 3 Train Validation And Test Zarr n n Table of Contents 0 1 nWe can see that our train zarr file has set of 4 arrays All this 4 fields are described above in the Data Format section above Let us check these fields We will check 1 observation from each nThe data is expected to live in a folder that can be configured using the L5KIT DATA FOLDER env variable We will now develop some intuition about the data n 3 2 Checking Configuration Fields n n Table of Contents 0 1 n nWe will look at this yaml file from an external dataset provided in L5kit examples Let s look the raster params filed It contains information related to the transformation of the 3D world onto image plane You can also check for various other information n n NOTE We can make our own configuration file n 3 3 Loading Data n n Table of Contents 0 1 n nWe re building a LocalDataManager object This will resolve relative paths from the config using the L5KIT DATA FOLDER env variable we have just set We will work with sample zarr for developing intuition regarding the data Please use train zarr validate zarr and test zarr for actual model training validation and prediction nWe are going to load our sample zarr file The 2020 Lyft competition dataset is stored in four structured arrays scenes frames agents and tl faces n 3 4 Data Overview n n Table of Contents 0 1 n nWe will see each field inside the zarr files train validation and test We will use sample zarr here n 3 4 1 Agents n nAn agent is an observation by the AV of some other detected object Each entry describes the object in terms of its attributes such as position and velocity gives the agent a tracking number to track it over multiple frames but only within the same scene and its most probable label n Points to note n centroid position of agent n extent agent dimension n yaw rotation of an agent with respect to vertical axis A yaw rotation is a movement around the yaw axis of a rigid body that changes the direction it is pointing to the left or right of its direction of motion n velocity speed of the agent n track id unique id to track agent in different frames n label probabilities prabability an agent belong to one of the 17 classes We are only given three labels cyclist pedestrians and cars n Centroid Distribution nWe will see the distribution of centroid now Since centroid column consist of list per sample we will make two new columns centroid x and centroid y n Extent Distribution n nFirst we need to make new columns for extent x extent y and extent z as we have one extent column n Points to note n We can see extent distributions are right skewed n We can see long tails in positive direction n Extent Distribution Scatterplot n Yaw Distribution n Velocity Distribution nAs we can see indeed there are only four types of agents provided in the dataset such as Cars Cyclists Pedestrians and Unknown We can see that Unknown label is more as compared to other three agent labels n 3 4 2 Scenes n Table of contents 0 1 n nA scene is identified by the host i e which car was used to collect it and a start and end time It consists of multiple frames snapshots at discretized time intervals The scene datatype stores references to its corresponding frames in terms of the start and end index within the frames array described in dataframe below The frames in between these indices all correspond to the scene including start index excluding end index n Points to note n frame index interval frame index including start index excluding end index n host car used to collect data n start time start time of scene n end time end time of scene n Frame Index n Points to note n We can see linear trend here n Both host cars collected data within particular time intervals n Host Count n nWe will now see the host counts Ego vehicle used to collect the data n Points to note n We have two host cars which were used to collect the data host a013 and host a101 n 3 4 3 Frames n Table of contents 0 1 n nA frame captures all information that was observed at a time This includes the following fields as mentioned in dataframe below n Points to note n timestamp frame s timestamp n agent index interval agents vehicles cyclists and pedestrians that were captured by the ego s sensors n traffic light faces index interval traffic light index n ego translation position of host car n ego rotation rotation of host car which is collecting data using ego sensors n Ego Translation Distribution n Points to note n We can see the distributions are multi models here n Ego Translation Scatterplot n Ego Rotations Distribution n 3 4 3 Traffic Light Faces n Table of contents 0 1 n nThe traffic light bulbs red green yellow are refered as face n face id unique id for traffic light bulb n traffic light id traffic light status n traffic light face status out of red green yellow which face is active unactive unknown n 3 5 Dataset Package n n Table of Contents 0 1 n nThe dataset package for example already implements PyTorch ready datasets so you can hit the ground running and start coding immediately There are two classes in the dataset package We will be using below two datasets classes to generate inputs and targets n n EgoDataset this dataset iterates over the AV annotations n AgentDataset this dataset iterates over other agents annotations n nBoth of them can be iterated and return multi channel images from the rasterizer along with future trajectories offsets and other information We will see ahead about rasterizer and trajectories soon n nWe need cfg Zarr Dataset ChunkedDataset and rasterizer object to instantiate these n n 3 5 1 ChunckedDataset n n Table of Contents 0 1 n nThe ChunckedDataset 0 2 click class as you can see above returned four structured arrays scenes frames agents and tl faces all are described above in detail Both the EgoDataset and AgentDataset are using zarr dataset object which is made using ChunckedDataset 0 2 n n 3 6 Visualize Autonomous Vehicle n n Table of Contents 0 1 n nNow we will look into the visualisation utility of L5Kit Toolkit https github com lyft l5kit There are two core packages for visualisation n n rasterization contains classes for getting visual data as multi channel tensors and turning them into interpretable RGB images We will talk about these classes in detail shortly Each class inside this rasterization https github com lyft l5kit tree master l5kit l5kit rasterization package conatin has at least a rasterize method to get the tensor and a to rgb method to convert it into an image n n BoxRasterizer this object renders agents e g vehicles or pedestrians as oriented 2D boxes n StubRasterizer this object doesn t do anything It return all black image and can be used for testing n SatelliteRasterizer this object renders an oriented crop from a satellite map n SatBoxRasterizer this object combine a Satellite and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a satellite image n SemanticRasterizer this object renders semantic map which contains lane crosswalk information n SemBoxRasterizer this object combine a Semantic Map and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a semantic image n nTo instantiate each of these object we will use build rasterizer method n n n visualization contains utilities to draw additional information e g trajectories onto RGB images These utilities are commonly used after a to rgb call to add other information to the final visualisation Following utilities are available n n draw arrowed line Draw a single arrowed line in an RGB image n draw trajectory Draw a trajectory on oriented arrow onto an RGB image n draw reference trajectory Draw a trajectory as points onto the image n 3 6 1 Visualizing Various Rasterizer Objects n nWe will visualize different raster objects credits https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n n Table of Contents 0 1 n 3 6 2 Visualize Trajectory Semantic View n n Table of Contents 0 1 n 3 6 3 Visualize Trajectory Satellite View n n Table of Contents 0 1 n nWe can get the satellite view by changing the parameter map type inside raster params in our configuration file cfg n 3 6 4 Visualize Agent n n Table of Contents 0 1 n nWe will visualize our agents using AgentDataset class nThe green box is our AV agent The blue boxes are entities which we are captured by the sensors We want to predict the motion of these entities so that our AV can more effectively predict its path n 3 6 5 Visualize Individual Scene Semantic n n Table of Contents 0 1 n nWe will visualize the scene in depth n The green box is our AV agent and the arrow on top of it represent its motion n The blue boxes are agents cars cyclists predestrians n We can see intersection of roads n In animation we can see the AV is moving on straight path n 3 6 6 Visualize Individual Scene Satellite n n Table of Contents 0 1 n nWe are now going to visualize the satellite view for more detailed understanding n We can see the green AV agent and blue entities cars bicycles and pedestrians n We can see our agent in motion in realtion to the movement of other agents vehicles n 4 Pytorch Baseline n n Table of Contents 0 1 n n 4 1 Configuration n nLet us first make our configurations for training and testing then we will load our data n 4 2 Loading Training Data n n Table of Contents 0 1 n 4 3 Training DataLoader n n Table of Contents 0 1 n n 4 4 Model resnet50 Pytorch n n Table of Contents 0 1 nThis is for the purpose of demonstration only You can set pretrained True and train your model in separate notebook keeping on internet connection for downloading pretrained weights and make inference in separate notebook keeping internet off since subission requires you to turn off your internet connection I will use my trained weights for making inference n 4 5 Compilation n n Table of Contents 0 1 n 4 6 Training n n Table of Contents 0 1 n 4 7 Saving Model n n Table of Contents 0 1 n 5 Prediction and Results n n Table of Contents 0 1 n nBe careful you have turned of your internet connection in order to make submission Please make separate notebook for the inference This notebook is for the purpose of demonstration Train using pretrained resnet weights in one notebook and make inference using another notebook n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 300px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 246 349 n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 350px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 169 83 n n 5 1 Test DataLoader n n Table of Contents 0 1 n 5 2 Getting Predictions n n Table of Contents 0 1 n For loading checkpoint n nWEIGHT FILE kaggle input lyft l5 weights resnet34 300x300 model state 15000 pth n nmodel state torch load WEIGHT FILE map location device n nmodel load state dict model state model state dict nWe need to run below two cell to make predictions and generate submission csv I am going to use my submission csv here since I am getting memory error while making predictions n 5 3 Submission n n Table of Contents 0 1 n 6 References n n Table of Contents 0 1 n n https www twi global com technical knowledge faqs what is an autonomous vehicle n Negative log likelihood https github com lyft l5kit blob master competition md n https www kaggle com pestipeti pytorch baseline train n https self driving lyft com level5 prediction n https www kaggle com corochann lyft deep into the l5kit library n https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n https www kaggle com gpreda lyft first data exploration ", "stargazers_count": 0, "forks_count": 0, "description": "com lyft l5kit to process our data loading and visualization and training. As we can see structured arrays allow us to mix different data types into a single array. 6 Visualize Autonomous Vehicle Table of Contents 0. 1 Visualizing Various Rasterizer Objects 3 6 1 3. 1 Configuration 4 1 4. 5 Visualize Individual Scene Semantic Table of Contents 0. Understanding Rasterizer class Table of Contents 0. Each entry describes the object in terms of its attributes such as position and velocity gives the agent a tracking number to track it over multiple frames but only within the same scene and its most probable label. 1 We can see that our train. The scene datatype stores references to its corresponding frames in terms of the start and end index within the frames array described in dataframe below. There are two classes in the dataset package. There are two core packages for visualisation rasterization contains classes for getting visual data as multi channel tensors and turning them into interpretable RGB images. com gpreda lyft first data exploration import packages level5 toolkit level5 toolkit visualization deep learning check files in directory animation for scene set env variable for data get configuration yaml Raster Parameters Prepare all rasterizer and EgoDataset for each rasterizer print key key raster object for visualization EgoDataset object select one example from our dataset plot ground truth trajectory EgoDataset object AgentDataset object animation satellite view animation training cfg raster s spatial resolution meters per pixel the size in the real world one pixel corresponds to. vehicles or pedestrians as oriented 2D boxes. We can see the green AV agent and blue entities cars bicycles and pedestrians. Structured arrays are stored in memory in an interleaved format this means that one row or sample is grouped together in memory. The 2020 Lyft competition dataset is stored in four structured arrays scenes frames agents and tl_faces. 4 Model resnet50 Pytorch Table of Contents 0. source https self driving. Let s look the raster_params filed. SatBoxRasterizer this object combine a Satellite and a Box Rasterizers into a single class. Both the EgoDataset and AgentDataset are using zarr dataset object which is made using ChunckedDataset 0. This notebook is for the purpose of demonstration. I will use my trained weights for making inference. com nxrprime for providing this function. A yaw rotation is a movement around the yaw axis of a rigid body that changes the direction it is pointing to the left or right of its direction of motion. Note that only 4 of the 17 possible agent label_probabilities are present in this dataset. 2 Submission 5 3 6. com lyft l5kit provided by the competition host to prepare preprocess data trian and evaluate the model. 1 We re building a LocalDataManager object. 2 Visualize Trajectory Semantic View 3 6 2 3. Pytorch Baseline 4 4. You can get more information here https www. 1 Now we will look into the visualisation utility of L5Kit Toolkit https github. draw_trajectory Draw a trajectory on oriented arrow onto an RGB image. This will resolve relative paths from the config using the L5KIT_DATA_FOLDER env variable we have just set. 1 Be careful you have turned of your internet connection in order to make submission. 5 show those obstacles with 0. An autonomous vehicle utilises a fully automated driving system in order to allow the vehicle to respond to external conditions that a human driver would manage. 5 Dataset Package Table of Contents 0. We are required to predict how these different agents move in Autonomous Vehicles s environment. The green box is our AV agent and the arrow on top of it represent its motion. 2 Getting Predictions 5 2 5. 1 Visualizing Various Rasterizer Objects We will visualize different raster objects. Ego Translation Scatterplot Ego Rotations Distribution 3. 1 This is for the purpose of demonstration only. Pytorch Baseline Table of Contents 0. 1 Data FormatsWe need to be familiar with the data we are using. It contains information related to the transformation of the 3D world onto image plane. The frames in between these indices all correspond to the scene including start index excluding end index. 1 For loading checkpoint WEIGHT_FILE kaggle input lyft l5 weights resnet34_300x300_model_state_15000. Please add this utility script https www. We will be using below two datasets classes to generate inputs and targets. Following utilities are available. 3 Visualize Trajectory Satellite View Table of Contents 0. Points to note frame_index_interval frame index including start index excluding end index host car used to collect data start_time start time of scene end_time end time of scene Frame Index Points to note We can see linear trend here. 2 Loading Training Data 4 2 4. 2 Checking Configuration Fields 3 2 3. load_state_dict model_state model_state_dict We need to run below two cell to make predictions and generate submission. 3 Train Validation And Test Zarr Table of Contents 0. 3 Traffic Light Faces Table of contents 0. 1 We are required to use L5Kit toolkit https github. html are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields. 3 Train Validation And Test Zarr 3 1 3 3. StubRasterizer this object doesn t do anything. 2 Getting Predictions Table of Contents 0. EDA Table of Contents 0. 2 Loading Training Data Table of Contents 0. 5 Compilation Table of Contents 0. The structured array can store various types of features. NOTE We can make our own configuration file. 1 A scene is identified by the host i. We can see our agent in motion in realtion to the movement of other agents vehicles. Points to note timestamp frame s timestamp agent_index_interval agents vehicles cyclists and pedestrians that were captured by the ego s sensors traffic_light_faces_index_interval traffic light index ego_translation position of host car. Import Packages Table of Contents 0. The data is available in. You can check this https www. First a raster generates a bird s eye view BEV top down raster which encodes all agents and the map. All this 4 fields are described above in the Data Format section above. The zarr files are flat compact and highly performant for loading. What we are predicting Our task in the competition is to predict the motion of external objects such as cars cyclist pedestrains etc in order to assist the self driving car. zarr file has set of 4 arrays. Let us check these fields. Extent DistributionFirst we need to make new columns for extent_x extent_y and extent_z as we have one extent column. 1 We can get the satellite view by changing the parameter map_type inside raster_params in our configuration file cfg. 1 ConfigurationLet us first make our configurations for training and testing then we will load our data. 3 Loading Data Table of Contents 0. csv here since I am getting memory error while making predictions. 3 Loading Data 3 3 3. 2 click class as you can see above returned four structured arrays scenes frames agents and tl_faces all are described above in detail. Host CountWe will now see the host counts Ego vehicle used to collect the data. train validation and test. 2 Scenes Table of contents 0. EgoDataset this dataset iterates over the AV annotations AgentDataset this dataset iterates over other agents annotations Both of them can be iterated and return multi channel images from the rasterizer along with future trajectories offsets and other information. com b3634eea5be5501318957e21086781666018efa1 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239 https camo. The dataset consists of 170 000 scenes capturing the environment around the autonomous vehicle. The blue boxes are entities which we are captured by the sensors. Import Packages 1 2. 1 The data is huge around 22 GB. Let us take an example to understand this. 349Inference Trained using Google Colab Model Single mode baseline resnet18 Steps Trained for 30000 iterations batch 16 Size Input size 350px history 1s 10 frames Optimizer Adam 1e 3 Loss MSE Loss LB 169. 1 Test DataLoader 5 1 5. Structured arrays https numpy. 1 The ChunckedDataset 0. 1 Data Formats 3 1 3. Each scene encodes the state of the vehicle s surroundings at a given point in time. 2 Zarr Format Table of Contents 0. We will talk about these classes in detail shortly. Most of the traditional numpy operations can be handled using. We will develop more intuition ahead. 1 We will see each field inside the zarr files. 4 Data Overview 3 4 3. We can see that Unknown label is more as compared to other three agent labels. In animation we can see the AV is moving on straight path. Each class inside this rasterization https github. load WEIGHT_FILE map_location device model. In test the mask provided in files as mask. vehicles or pedestrians as oriented 2D boxes in a satellite image. Basic EDA 3 3. agents_mask a mask that for train and validation masks out objects that aren t useful for training. com c lyft motion prediction autonomous vehicles discussion 177125 discussion here for more information. You can visualize agents e. 6 Visualize Individual Scene Satellite Table of Contents 0. com pestipeti first by clicking on the Add data section inside your notebook before starting. com pestipeti pytorch baseline train https self driving. Points to note We have two host cars which were used to collect the data host a013 and host a101. We can see intersection of roads. I am going to use my submission. com which started Level 5 https self driving. Understanding Rasterizer class https www. We are going to load our sample. SemanticRasterizer this object renders semantic map which contains lane crosswalk information. 1 A frame captures all information that was observed at a time. Competition DataThis dataset https www. 1 This competition is hosted by ridesharing company Lyft https www. com pestipeti lyft l5kit unofficial fix provided by Peter s https www. com lyft l5kit has some issue right now. Extent Distribution Scatterplot Yaw Distribution Velocity DistributionAs we can see indeed there are only four types of agents provided in the dataset such as Cars Cyclists Pedestrians and Unknown. 3 Frames Table of contents 0. 1 The traffic light bulbs red green yellow are refered as face. The model predicts a single agent at a time. 1 AgentsAn agent is an observation by the AV of some other detected object. These utilities are commonly used after a to_rgb call to add other information to the final visualisation. Points to note We can see extent distributions are right skewed. com technical knowledge faqs what is an autonomous vehicle Negative log likelihood https github. com corochann lyft deep into the l5kit library 1. 0 filter all other agents. 2 Zarr Format 3 1 2 3. 1 The L5Kit uses zarr format to store and read these structured numpy arrays. 6 Training Table of Contents 0. We are only given three labels cyclist pedestrians and cars Centroid DistributionWe will see the distribution of centroid now. Introduction Table of Contents 0. com level5 prediction https www. face_id unique id for traffic light bulb traffic_light_id traffic light status traffic_light_face_status out of red green yellow which face is active unactive unknown 3. Points to note centroid position of agent extent agent dimension yaw rotation of an agent with respect to vertical axis. 5 Visualize Individual Scene Semantic 3 6 5 3. 2 Checking Configuration Fields Table of Contents 0. draw_reference_trajectory Draw a trajectory as points onto the image. 4 Data Overview Table of Contents 0. L5Kit is a library which lets you Load driving scenes from zarr files Read semantic maps Read aerial maps Create birds eye view BEV images which represent a scene around an AV or another vehicle Sample data Train neural networks Visualize results 2. The L5Kit toolkit https github. zarr file contains scenes driving episodes acquired from a given vehicle. It return all black image and can be used for testing. traffic_light_faces traffic light information. 6 Visualize Autonomous Vehicle 3 6 3. velocity speed of the agent track_id unique id to track agent in different frames label_probabilities prabability an agent belong to one of the 17 classes. 1 Agents 3 4 1 3. This includes the following fields as mentioned in dataframe below. We need cfg Zarr Dataset ChunkedDataset and rasterizer object to instantiate these. Utility Functions 2 3. 7 Saving Model 4 7 5. trajectories onto RGB images. inference cfg root directory set env variable for data training cfg rasterizer dataloader set pretrained True while training This is 512 for resnet18 and resnet34 And it is 2048 for the other resnets X Y coords for the future positions output shape Bx50x2 You can add more layers here. frames snapshots in time of the pose of the vehicle. BoxRasterizer this object renders agents e. 6 Training 4 6 4. pth model_state torch. 6 Visualize Individual Scene Satellite 3 6 6 4. 5 probability of being one of the classes we care about cars bikes peds etc. 5 Compilation 4 5 4. You can also check for various other information. 7 Saving Model Table of Contents 0. References Table of Contents 0. com paper one thousand and one hours self driving. 3 Visualize Trajectory Satellite View 3 6 3 3. The dataset includes 1000 hours of traffic agent movement 16k miles of data from 23 vehicle 15k semantic map annotationsHere is the reserach paper of Prediction Dataset https paperswithcode. the keys are relative to the dataset environment variable e. 3 Submission Table of Contents 0. 5 would show the ego centered in the image. We will check 1 observation from each. 4 Model resnet50 Pytorch 4 4 4. zarr file format which can be easily load using the L5Kit https github. We will now develop some intuition about the data. 1 We are now going to visualize the satellite view for more detailed understanding. 5 Dataset Package 3 5 3. zarr for developing intuition regarding the data. 1 We will visualize our agents using AgentDataset class. Introduction 0 1. ego_rotation rotation of host car which is collecting data using ego sensors Ego Translation Distribution Points to note We can see the distributions are multi models here. Model which can predict the movement of traffic agents around the Autonomous Vehicles such as cars cyclists and pedestrians etc. Lyft Motion Prediction for Autonomous Vehicles 128663 Build motion prediction models for self driving vehicles 128663 Table of contents 0. Please make separate notebook for the inference. com 8048a110a20827715a17eb76f8039302a576d503 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302b25323025354373756d5f742532302535436c6f672532302535436d61746863616c2537424e253744253238785f74253743253543626172253742782537445f742535456b2532432532302535437369676d61253344312532392532302535436d61746863616c2537424e253744253238795f74253743253543626172253742792537445f742535456b2532432532302535437369676d6125334431253239253744 https camo. What is Autonomous Vehicle An autonomous vehicle or a driverless vehicle is one that is able to operate itself and perform necessary functions without any human intervention through ability to sense its surroundings. We have to predict the location of objects agents in the next 50 frames. Both host cars collected data within particular time intervals. vehicles or pedestrians as oriented 2D boxes in a semantic image. Prediction and Results 5 5. 2 Visualize Trajectory Semantic View Table of Contents 0. Evaluation Metric Negative log likelihoodWe calculate the negative log likelihood of the ground truth data given the multi modal predictions. We can see long tails in positive direction. This dataset includes the logs of movement of cars cyclists pedestrians and other traffic agents encountered by Lyft s autonomous fleet. It consists of multiple frames snapshots at discretized time intervals. com 9ba94f5c0c40666d66b93fba994cc5f7623ebd98 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302d25354366726163253742312537442537423225374425323025354373756d5f74253230253238253543626172253742782537445f742535456b2532302d253230785f74253239253545322532302b253230253238253543626172253742792537445f742535456b2532302d253230795f7425323925354532253744 Lyft s Autonomous Vehicle AV Lyft s AV Introducton video. Each agent state describes the position orientation bounds and type. 4 Visualize Agent 3 6 4 3. The network infers the future coordinates of the agent based upon this raster. We will see ahead about rasterizer and trajectories soon. credits https www. 1 ChunckedDataset 3 5 1 3. To instantiate each of these object we will use build_rasterizer method. 4 Visualize Agent Table of Contents 0. com level5 self driving division to tackle the challenges in the field of self driving cars. Train using pretrained resnet weights in one notebook and make inference using another notebook. 3 Training DataLoader Table of Contents 0. com lyft l5kit tree master l5kit l5kit rasterization package conatin has at least a rasterize method to get the tensor and a to_rgb method to convert it into an image. In this competition our task is to build motion prediction models for self driving vehicles. com lyft l5kit blob master competition. 1 Test DataLoader Table of Contents 0. draw_arrowed_line Draw a single arrowed line in an RGB image. We will use Lyft s L5Kit https github. The data is expected to live in a folder that can be configured using the L5KIT_DATA_FOLDER env variable. SatelliteRasterizer this object renders an oriented crop from a satellite map. Inference Trained using Google Colab Model Single mode baseline resnet18 Steps Trained for 30000 iterations batch 16 Size Input size 300px history 1s 10 frames Optimizer Adam 1e 3 Loss MSE Loss LB 246. From 0 to 1 per axis 0. 1 Introduction 3 1 1 3. 1 We will visualize the scene in depth. let s add some data in this structured array. npz masks out any test object for which predictions are NOT required. We will use sample. 1 The dataset package for example already implements PyTorch ready datasets so you can hit the ground running and start coding immediately. 1 We will look at this yaml file from an external dataset provided in L5kit examples. 1 IntroductionThe L5Kit toolkit uses data format that consists of a set of numpy structured arrays. com c lyft motion prediction autonomous vehicles overview evaluation. Utility Functions Thanks to Trigram https www. The green box is our AV agent. 0 include every obstacle 0. org doc stable user basics. com level5 prediction This baseline solution is trained on over 2 million samples from the agent locations contained within the dataset. com corochann lyft deep into the l5kit library https www. agents a generic entity captured by the vehicle s sensors. We will work with sample. Prediction and Results Table of Contents 0. 2 Scenes 3 4 2 3. which car was used to collect it and a start and end time. com c lyft motion prediction autonomous vehicles data is the largest collection of the traffic agent motion data. A frame is a snapshot in time which consists of ego pose time and multiple agent states. These logs come from processing raw lidar camera and radar data through our team s perception systems and are ideal for training motion prediction models. zarr for actual model training validation and prediction. The blue boxes are agents cars cyclists predestrians. visualization contains utilities to draw additional information e. 2 compiling model get hardware type CPU GPU TPU training loop forward pass not all the output steps are valid but we can filter them out from the loss using availabilities Backward pass save full trained model test configuration Rasterizer Test dataset dataloader Saved state dict from the training notebook submission. 3 Frames 3 4 3 3. SemBoxRasterizer this object combine a Semantic Map and a Box Rasterizers into a single class. Since centroid column consist of list per sample we will make two new columns centroid_x and centroid_y. We want to predict the motion of these entities so that our AV can more effectively predict its path. You can set pretrained True and train your model in separate notebook keeping on internet connection for downloading pretrained weights and make inference in separate notebook keeping internet off since subission requires you to turn off your internet connection. 3 Training DataLoader 4 3 4. 1 ChunckedDataset Table of Contents 0. ", "id": "kool777/lyft-level5-eda-training-inference", "size": "23434", "language": "python", "html_url": "https://www.kaggle.com/code/kool777/lyft-level5-eda-training-inference", "git_url": "https://www.kaggle.com/code/kool777/lyft-level5-eda-training-inference", "script": "l5kit.evaluation create_chopped_dataset Counter Style load_config_data DataLoader visualize_rgb_image Fore resnet34 IPython.display read_gt_csv animate build_rasterizer torchvision.models.resnet ChunkedDataset PERCEPTION_LABELS l5kit.geometry numpy TARGET_POINTS_COLOR write_pred_csv compute_metrics_csv seaborn animation Back animate_solution resnet50 l5kit.visualization resnet18 draw_trajectory nn tqdm draw_reference_trajectory l5kit.data transform_points Dict matplotlib.pyplot typing l5kit.configs LyftModel(nn.Module) LocalDataManager forward pandas PrettyTable AgentDataset EgoDataset matplotlib torch.utils.data HTML display clear_output __init__ l5kit.rasterization optim torch collections l5kit.dataset prettytable colorama ", "entities": "(('notebook', 'demonstration'), 'be') (('agent state', 'position orientation bounds'), 'describe') (('We', 'inputs'), 'use') (('4 fields', 'Data Format above section'), 'describe') (('keys', 'variable e.'), 'be') (('which', 'ChunckedDataset'), 'use') (('that', 'time'), 'capture') (('We', 'more detailed understanding'), '1') (('distributions', 'ego sensors Ego Translation Distribution Points'), 'rotation') (('us', 'single array'), 'allow') (('We', 'ahead rasterizer'), 'see') (('We', 'detail'), 'talk') (('that', 'L5KIT_DATA_FOLDER env variable'), 'expect') (('scene', 'time'), 'encode') (('predicting', 'self driving car'), 'be') (('We', 'raster different objects'), 'visualize') (('only 4', 'dataset'), 'note') (('structured array', 'features'), 'store') (('1 We', 'L5Kit toolkit https github'), 'require') (('object', 'Box single class'), 'combine') (('visualization', 'information additional e.'), 'contain') (('labels cyclist only three pedestrians', 'centroid'), 'give') (('AV', 'straight path'), 'see') (('which', 'cars such cyclists'), 'model') (('face', 'red green yellow'), 'face_id') (('It', 'testing'), 'return') (('We', 'agents other vehicles'), 'see') (('frames', 'end index'), 'correspond') (('utilities', 'final visualisation'), 'use') (('file zarr which', 'L5Kit https easily github'), 'format') (('It', 'time discretized intervals'), 'consist') (('red green yellow', 'face'), '1') (('entry', 'only same scene'), 'describe') (('host cars', 'time particular intervals'), 'collect') (('one pixel', 'real world'), 'com') (('This', 'demonstration'), '1') (('which', 'a101'), 'point') (('cfg Zarr Dataset ChunkedDataset', 'these'), 'need') (('We', 'configuration own file'), 'NOTE') (('how different agents', 'Autonomous environment'), 'require') (('model', 'time'), 'predict') (('we', 'etc'), 'ped') (('car', 'it'), 'use') (('we', 'build_rasterizer method'), 'use') (('agents cars', 'predestrians'), 'be') (('that', 'numpy structured arrays'), '1') (('Evaluation Metric Negative log likelihoodWe', 'modal multi predictions'), 'calculate') (('AV', 'more effectively path'), 'want') (('We', 'positive direction'), 'see') (('here I', 'predictions'), 'csv') (('We', 'next 50 frames'), 'have') (('competition', 'company Lyft https www'), '1') (('draw_arrowed_line', 'RGB image'), 'draw') (('AV green agent', 'cars blue bicycles'), 'see') (('you', 'submission'), 'be') (('1 We', 'configuration file cfg'), 'get') (('Both', 'trajectories future offsets'), 'iterate') (('we', 'one extent column'), 'need') (('which', 'crosswalk lane information'), 'SemanticRasterizer') (('1 We', 'AgentDataset class'), 'visualize') (('one row', 'together memory'), 'mean') (('Most', 'traditional numpy operations'), 'handle') (('agent', '17 classes'), 'speed') (('SatelliteRasterizer', 'satellite map'), 'render') (('you', 'ground'), '1') (('we', 'L5KIT_DATA_FOLDER env variable'), 'resolve') (('we', 'sensors'), 'be') (('AgentsAn 1 agent', 'other detected object'), 'be') (('us', 'this'), 'let') (('that', 'traffic index ego_translation host light car'), 'point') (('baseline solution', 'dataset'), 'com') (('Unknown label', 'agent other three labels'), 'see') (('object doesn', 'anything'), 'do') (('Backward', 'training notebook submission'), 'get') (('Bx50x2 You', 'more layers'), 'set') (('This', 'dataframe'), 'include') (('You', 'https here www'), 'get') (('predictions', 'which'), 'mask') (('then we', 'data'), '1') (('WEIGHT_FILE kaggle lyft input l5', 'resnet34_300x300_model_state_15000'), '1') (('We', 'LocalDataManager object'), 'build') (('Sample data Train neural Visualize', 'AV'), 'be') (('dataset', 'autonomous vehicle'), 'consist') (('dataset', 'reserach Prediction Dataset https paperswithcode'), 'include') (('now host', 'data'), 'see') (('I', 'inference'), 'use') (('ego', 'image'), 'show') (('scene datatype stores', 'dataframe'), 'reference') (('com c lyft motion vehicles prediction autonomous data', 'traffic agent motion largest data'), 'be') (('load_state_dict model_state We', 'submission'), 'model_state_dict') (('Prediction', 'Contents'), '0') (('Inference', 'Size Input size 300px 16 history'), 'Trained') (('task', 'self driving vehicles'), 'be') (('driverless that', 'surroundings'), 'be') (('1 We', 'zarr files'), 'see') (('AV agent', 'motion'), 'be') (('it', 'motion'), 'be') (('We', 'L5kit examples'), '1') (('datatype', 'named fields'), 'be') (('subission', 'internet connection'), 'set') (('We', 'data'), 'develop') (('dataset', 'traffic other autonomous fleet'), 'include') (('L5Kit', 'numpy structured arrays'), '1') (('We', 'linear trend'), 'start') (('we', 'data'), 'need') (('mask', 'mask'), 'provide') (('draw_reference_trajectory', 'image'), 'draw') (('s', 'structured array'), 'let') (('logs', 'motion prediction models'), 'come') (('Extent Distribution Scatterplot Yaw Distribution Velocity we', 'Cars Cyclists such Pedestrians'), 'distributionas') (('network', 'raster'), 'infer') (('lyft l5kit tree master l5kit l5kit rasterization package conatin', 'image'), 'com') (('You', 'also various other information'), 'check') (('which', 'https self Level 5 driving'), 'com') (('It', 'image plane'), 'contain') (('which', 'pose ego time'), 'be') (('zarr file', 'given vehicle'), 'contain') (('BoxRasterizer', 'agents e.'), 'render') (('tl_faces all', 'above detail'), 'return') (('which', 'agents'), 'generate') (('that', 'useful training'), 'agents_mask') (('Now we', 'L5Kit Toolkit https github'), 'look') (('zarr files', 'flat highly loading'), 'be') (('we', 'columns two new centroid_x'), 'make') (('Lyft competition 2020 dataset', 'arrays scenes frames four structured agents'), 'store') (('human driver', 'that'), 'utilise') "}