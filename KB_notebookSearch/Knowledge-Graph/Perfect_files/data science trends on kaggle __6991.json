{"name": "data science trends on kaggle ", "full_name": " h2 Historical Data Science Trends on Kaggle h2 1 Linear Vs Logistic Regression h2 2 The dominance of xgboost h2 3 Trends of Neural Networks and Deep Learning h2 4 ML Tools used on Kaggle h2 5 XgBoost vs Keras h2 6 What Kagglers are using for Data Visualizations h2 7 Important Data Science Techniques h2 8 Kaggle Components What people talks about the most ", "stargazers_count": 0, "forks_count": 0, "description": "However with the arrival of Lightgbm in 2016 the useage of xgboost dipped to some extent and popularity of lightgbm started rising very quickly. The second best is seaborn which is used extensively as well. But when XgBoost was open sourced in 2014 it gained popularty quickly and dominated the kaggle competitions and kernels. Also many cloud instance providers such as Amazon AWS Google cloud etc showcases their capabilities of training very deep neural networks on clouds. People tend to forget that ensembling is only the last stage of any modelling process but a considerable amount of time should be given to feature engineering and model tuning tasks. Some examples are Otto Group Classification Competition in which first place solution made use of xgboost. From the above graph we can observe that there were always been more discussions related to logistic regression than linear regression. The number of logistic regression discussions on forums kernel comments and replies boomed to high numbers in October 2017 and March 2018. Kaggle is the largest and the most popular data science community across the globe. Also transfer learning and pre trained models have shown great results in competitions. Also a number of Data Science for Good Challenges and Kernels only competitions have been launched on kaggle which are one of the reason of kernels popularity. Kaggle can launch more competitions playgrounds related to Image Classification Modelling as people wants to learn from them alot. XgBoost vs Keras Among both the popular techniques on Kaggle xgboost and deeplearning xgboost has remained on top because it is faster and requires less computational infrastructure than very complex and deeper neural networks. These two tasks have the most important significance in the best and accurate models. There is a regression competition as well House Prices advanced regression but people more often start it after titanic only. The dominance of xgboost Before 2014 Linear Models Decision Trees and Random Forests were very popular. Today xgboost is still used exhaustively in compeitions and is the part of the winning models of many competitions. The number of discussions related to deep learning is increasing regularly and are always more than neural networks. In almost every regression or classification kernels one can notice the ensemblling kernels. For instance Catboost was recently released and is starting gaining popularity. Deep learning is also become populary every month because of different variants of models such as RNNs CNNs have shown great improvements in the kernels. Not to forget that Kaggle have added the GPU support in kernels which facilitates the Deep Learning useage on kaggle. Among the ML tools Keras is the most popular because of the simplistic deep learning implementation. What Kagglers are using for Data Visualizations Plotly has gained so much popularity since 2017 and is one of the most used data visualization library among the kernels. This competition has most number of discussions and is one of the longest running compeition on Kaggle. One indication is that there are more number of classification problems than regression problems on Kaggle including the most popular Titanic Survival Prediction competition. Surprizing to see that discussions related to Feature Engineering and Model Tuninig are less than Ensembling. get relevant text cleaning ignore tokenize. Important Data Science Techniques Among the important data science steps kagglers focus alot on Model Ensembling since many winning solutions on kaggle competitions are ensemble models the blends and stacked models. Linear Vs Logistic RegressionLets look at the comparison of linear regression and logistic regression discussions on forums kernels and replies on kaggle. The generel trend is that number of discussions are increasing every month. With the launch of kernels in 2016 their useage increased to a great extent. Kaggle also launched the awesome Kaggle Learn section which is becoming popular and popular but still it is behind than the compeitions kernels and discussions. ML Tools used on Kaggle Scikit Learn was the only library used on kaggle for machine learning tasks but since 2015 tensorflow gained populartiy. Also deeplearning models became popular for text classification problems for example Quora Duplicate Questions Classification. One of the reason is the the Toxic Comments Classification Competition in which a number of authors shared excellent information related to classification models including logistic regression. Some of the high quality visualization kernels by kaggle grandmasters such as SRK and Anistropic are created with plotly. In this kernel I am using Kaggle Meta Data to explore the Data Science trends over the years. Just for an example in Toxic Comment Classification Competition massively large number of ensemling kernels were shared. Data Exploration is the important technique and people have started stressing on the importance of exploration in the EDA kernels. Firstly kagglers shared kernels in competitions only but with a more focus on kaggle datasets kernel awards the number of discussions related to kernels started rising and have surpassed the discussions related to competitions. Based on the recent increasing trend of lightgbm shown in red one can forecast that it will dominate next few years as well unless any other company opensources a better model. For example lightgbm was used in the winning solution of Porto Seguro s Safe Driver Prediction. Trends of Neural Networks and Deep Learning Neural networks were present in the industry since the decades but in recent years trends changed because of the access to much larger data and computational power. The deeplearning models also became popular because of a number of Image Classification competitions on Kaggle such as Data Science Bowl competitions from Google etc. Personally I am a big fan of plotly as well. Historical Data Science Trends on Kaggle A number of trends have changed over the years in the field of Data Science. This is because its primarily audience is the novice and begineers but for sure in coming years and with the more addition of courses kaggle learn section will reach the similar levels as competitions and kernels. Kaggle Components What people talks about the most Kaggle communitiy has shared a number of competition related discussions in fourms and are increasing in general. One of the reason for light gbm popularity is the faster implementation and simple interface as compared to xgboost. The era of deep learning started in 2014 with the arrival of libraries such as theano tensorflow in 2015 and keras in 2016. ", "id": "shivamb/data-science-trends-on-kaggle", "size": "6991", "language": "python", "html_url": "https://www.kaggle.com/code/shivamb/data-science-trends-on-kaggle", "git_url": "https://www.kaggle.com/code/shivamb/data-science-trends-on-kaggle", "script": "tools Counter plotthem init_notebook_mode numpy seaborn plotly.plotly plotly ngrams get_top_ngrams matplotlib.pyplot striphtml plotly.offline plotly.graph_objs pandas clntxt iplot stopwords nltk.corpus nltk.util check_presence zip_longest collections itertools ", "entities": "(('primarily audience', 'competitions'), 'be') (('important people', 'EDA kernels'), 'be') (('also month different variants', 'kernels'), 'become') (('One indication', 'Titanic Survival Prediction most popular competition'), 'be') (('we', 'linear regression'), 'observe') (('Keras', 'learning most simplistic deep implementation'), 'be') (('which', 'kernels popularity'), 'number') (('tensorflow', 'populartiy'), 'be') (('Linear Vs Logistic RegressionLets', 'kaggle'), 'look') (('massively large number', 'ensemling kernels'), 'for') (('using', 'kernels'), 'gain') (('regression as well House Prices advanced people', 'titanic'), 'be') (('lightgbm', 'Porto Safe Driver Prediction'), 'use') (('still it', 'compeitions behind kernels'), 'launch') (('discussions', 'Model Ensembling'), 'surprize') (('Historical Data Science Kaggle number', 'Data Science'), 'Trends') (('number', 'regularly always neural networks'), 'increase') (('number', 'discussions'), 'be') (('it', 'very complex neural networks'), 'remain') (('which', 'kaggle'), 'forget') (('Kaggle', 'data science most popular globe'), 'be') (('trends', 'much larger data'), 'be') (('dominance', '2014'), 'be') (('as well other company', 'better model'), 'forecast') (('Some', 'such SRK'), 'create') (('deeplearning models', 'Google etc'), 'become') (('number', 'logistic regression'), 'be') (('many winning solutions', 'ensemble blends'), 'Techniques') (('era', '2016'), 'start') (('number', 'competitions'), 'kaggler') (('useage', 'great extent'), 'increase') (('useage', 'lightgbm'), 'dip') (('two tasks', 'best models'), 'have') (('One', 'faster simple xgboost'), 'be') (('classification one', 'ensemblling kernels'), 'notice') (('it', 'quickly kaggle competitions'), 'gain') (('place first solution', 'xgboost'), 'be') (('considerable amount', 'tuning engineering tasks'), 'tend') (('Also trained models', 'competitions'), 'transfer') (('competition', 'Kaggle'), 'have') (('people', 'alot'), 'launch') (('Also deeplearning', 'example'), 'become') (('Catboost', 'recently popularity'), 'release') (('Today xgboost', 'many competitions'), 'use') (('cloud instance Also many providers', 'clouds'), 'showcase') (('I', 'years'), 'use') (('talks', 'fourms'), 'share') "}