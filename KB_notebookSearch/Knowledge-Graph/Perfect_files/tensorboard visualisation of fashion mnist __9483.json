{"name": "tensorboard visualisation of fashion mnist ", "full_name": " h1 Tensorboard Visualisation of Fashion Items h3 What is an Embedding h3 Workflow h4 To do a basic embedding visualisation we need to follow the below workflow steps h4 Creating the embedding variable with all the images defined above under x test h2 How to run h2 Visualizing Embeddings h3 Projections h4 PCA Principal Component Analysis h4 t SNE T distributed stochastic neighbor embedding h4 Exploration h1 If you like this kernel greatly appreciate an UPVOTE ", "stargazers_count": 0, "forks_count": 0, "description": "Implementations can give unexpected behavior otherwise. The Embedding Projector computes the top 10 principal components from which you can choose two or three to view. Tensorboard Visualisation of Fashion Items This Kernel is about Data Visualisation of high dimensional data to a lower dimension using Tensorboard with t SNE and PCA dimensionality reduction techniques and exploration of data points with multiple parameter tuning. Its typically values are between 5 and 50. The Embedding Projector provides three ways to reduce the dimensionality of a data set. To be clear this is a different effect than the run of the mill fact that any dimensionality reduction technique will distort distances. You should see this after navigating to the Projector Tab as shown below Visualizing EmbeddingsVisualising embeddings is a powerful technique It helps you understand what your algorithm learned and if this is what you expected it to learn. The perplexity value has a complex effect on the resulting pictures. gif t SNE T distributed stochastic neighbor embedding A nonlinear nondeterministic algorithm T distributed stochastic neighbor embedding that tries to preserve local neighborhoods in the data often at the expense of distorting global structure. ProjectionsA projection is a method for taking these high dimensional vections and project them into a lower dimensional space. global_variables_initializer saver tf. What is an Embedding Embedding is a way to map discrete objects images words etc. Import Libraries. metadata_path w as meta meta. write Index tLabel n for index label in enumerate labels meta. PCA tends to highlight large scale structure in the data but can distort local neighborhoods. Session as sesh sesh. shape 0 sprite_image np. ckpt Create the sprite imagerows 28cols 28label T shirt top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Ankle boot sprite_dim int np. You can choose whether to compute two or three dimensional projections. Topological information off a t SNE plot can be read with views at multiple perplexities. tensor_name embedding_var. Recognizing these clumps as random noise is an important part of reading t SNE plots. You can also inspect nearest neighbor subsets. FileWriter r C FashionMNIST logs Creating the embedding variable with all the images defined above under x_test embedding_var tf. ProjectorConfig embedding config. TensorBoard includes the Embedding Projector a tool that lets you interactively visualize embeddings. Getting the most from t SNE may mean analyzing multiple plots with different perplexities. imshow sprite_image cmap gray plt. If you like this kernel greatly appreciate an UPVOTE. Embeddings are important for input to machine learning. After a search the points matching the query are selected. They train best on dense vectors where all values contribute to define an object. Workflow To do a basic embedding visualisation we need to follow the below workflow steps Read the Fashion MNIST data and create an X image and Y label batch Create a Summary Writer Configure the projector Create the embedding Tensor from X Run the TF session and create a model check point Create the sprite image Create the metadata labels file Read the Fashion MNIST test data and create an X image and Y label batchFirst we will load the fashion mnist test data into a pandas dataframe and subsequently convert into a numpy array with datatype as float32Lets consider around 2500 images as part of the embeddingNow we will split our data into x_test for storing images and y_test for storing labels. visualize_embeddings summary_writer config Run the TF session and create a model check pointwith tf. ones cols sprite_dim rows sprite_dim index 0labels for i in range sprite_dim for j in range sprite_dim labels. The Embedding Projector finds all points whose label matches the Left pattern and computes the centroid of that set similarly for Right. extend 28 28 Create the embedding Tensor from Xprojector. Rather density equalization happens by design and is a predictable feature of t SNE. png How to runWe saved our MNIST fashion images time to visualise it Follow the steps below Step 1 Go to command prompt and type the following C Users Pavan tensorboard logdir C FashionMNIST logs Step 2 Now open a browser and navigate to http 127. This tool can read embeddings from your model and render them in two or three dimensions. You define the horizontal axis for instance by giving text patterns for Left and Right. reshape 28 28 1 1 index 1 Create the metadata labels filewith open embedding. To store all the events in a log directory for tensorboard view we need to mention the path as below Create a Summary Writersummary_writer tf. format index label plt. to high dimensional vectors. Clicking on a point causes the right pane to list the nearest neighbors along with distances to the current point. Inspector panel on the right side where you can search for particular points and see a list of nearest neighbors. png ExplorationYou can explore visually by zooming rotating and panning using natural click and drag gestures. Hovering your mouse over a point will show any metadata for that point. The nearest neighbor points are also highlighted in the projection. The vertical axis is likewise computed from the centroids for points matching the Up and Down text patterns. Epsilon is another parameter used to measure the learning rate The t SNE algorithm naturally expands dense clusters and contracts sparse ones evening out cluster sizes. A second feature of t SNE is a tuneable parameter perplexity which says loosely how to balance attention between local and global aspects of your data. To do so you can select points in multiple ways After clicking on a point its nearest neighbors are also selected. Variable x_test name fmnist_embedding Configure the projectorThis is the important part of embedding visualisation. Source https distill. Let us look more closer by zooming the picture as below https i. join logdir metadata. PCA Principal Component Analysis A linear deterministic algorithm principal component analysis that tries to capture as much of the data variability in as few dimensions as possible. image_path sprite_image cmap gray plt. 1 6006 note this can change depending on your computer setup. show The above figure represents the grayscale fashion products representation in lower dimension. Since all images are ranging from 0 255 we will need to rescale all of them by dividing with 255 so that it reflects between 0 and 1. png Custom A linear projection onto horizontal and vertical axes that you specify using labels in the data. One common use is to find nearest neighbors. Instead it s the overall patterns of location and distance between vectors that machine learning takes advantage of. Low perplexity levels often show cloud of points generated randomly which has no statistically interesting clusters clumps. It is sometimes useful to restrict the view to a subset of points and perform projections only on those points. The line passing through these two centroids defines the horizontal axis. Enabling selection clicking on a point and dragging defines a selection sphere. Classifiers and neural networks more generally work on vectors of real numbers. Distances between well separated clusters in a t SNE plot may mean nothing. Here we specify what variable we want for the project what the metadata path is the names and classes and where to save the sprites. Embeddings map objects to vectors applications can use similarity in vector space for instance Euclidean distance or the angle between vectors as a robust and flexible measure of object similarity. Projections panel on the bottom left where you can choose the type of projection. For the algorithm to operate properly the perplexity really should be smaller than the number of points. The algorithm is non linear and adapts to the underlying data performing different transformations on different regions. The individual dimensions in these vectors typically have no inherent meaning. pub 2016 misread tsne https i. The Embedding Projector has three panels Data panel on the top left where you can choose the run the embedding variable and data columns to color and label points by. append label int y_test index sprite_image i cols i 1 cols j rows j 1 rows x_test index. The parameter is in a sense a guess about the number of close neighbors each point has. ", "id": "pavansanagapati/tensorboard-visualisation-of-fashion-mnist", "size": "9483", "language": "python", "html_url": "https://www.kaggle.com/code/pavansanagapati/tensorboard-visualisation-of-fashion-mnist", "git_url": "https://www.kaggle.com/code/pavansanagapati/tensorboard-visualisation-of-fashion-mnist", "script": "numpy tensorflow projector matplotlib.pyplot HTML IPython.core.display tensorflow.contrib.tensorboard.plugins IPython.display input_data tensorflow.examples.tutorials.mnist pandas Image ", "entities": "(('nearest neighbors', 'point'), 'select') (('that', 'global structure'), 'distribute') (('ckpt', 'sprite'), 'create') (('it', 'what'), 'see') (('individual dimensions', 'typically inherent meaning'), 'have') (('randomly which', 'clusters statistically interesting clumps'), 'show') (('Recognizing', 'SNE important t plots'), 'be') (('Implementations', 'unexpected behavior'), 'give') (('parameter tuneable which', 'data'), 'be') (('points', 'query'), 'select') (('steps', '127'), 'png') (('vertical axis', 'text Up patterns'), 'compute') (('perplexity value', 'resulting pictures'), 'have') (('t SNE algorithm', 'cluster sizes'), 'be') (('Enabling', 'point'), 'define') (('where you', 'projection'), 'leave') (('dimensionality reduction technique', 'distances'), 'be') (('neighbor nearest points', 'also projection'), 'highlight') (('Topological information', 'multiple perplexities'), 'read') (('You', 'Left'), 'define') (('right pane', 'current point'), 'cause') (('One common use', 'nearest neighbors'), 'be') (('Embedding Embedding', 'words'), 'be') (('above figure', 'lower dimension'), 'show') (('fmnist_embedding Variable projectorThis', 'embedding important visualisation'), 'name') (('algorithm', 'points'), 'be') (('where values', 'object'), 'train') (('you', 'interactively embeddings'), 'include') (('density Rather equalization', 't predictable SNE'), 'happen') (('vectors applications', 'object similarity'), 'map') (('you', 'greatly UPVOTE'), 'appreciate') (('j', 'j 1 rows x_test index'), 'int') (('metadata path', 'where sprites'), 'specify') (('tool', 'two dimensions'), 'read') (('Embedding Projector', 'data set'), 'provide') (('it', '0'), 'need') (('machine learning', 'advantage'), 's') (('It', 'only points'), 'be') (('visualize_embeddings summary_writer config', 'model check pointwith'), 'Run') (('ExplorationYou', 'visually natural click'), 'explore') (('parameter', 'close neighbors'), 'be') (('Kernel', 'parameter multiple tuning'), 'visualisation') (('label', 'similarly Right'), 'find') (('Distances', 'nothing'), 'mean') (('typically values', '5'), 'be') (('algorithm', 'different regions'), 'be') (('Embeddings', 'machine learning'), 'be') (('you', 'two'), 'compute') (('Getting', 'different perplexities'), 'mean') (('where you', 'nearest neighbors'), 'panel') (('this', 'computer setup'), 'note') (('Classifiers', 'real numbers'), 'work') (('line', 'horizontal axis'), 'define') (('You', 'two dimensional projections'), 'choose') (('us', 'https i.'), 'let') (('ProjectionsA projection', 'lower dimensional space'), 'be') (('Hovering', 'point'), 'show') (('You', 'neighbor also nearest subsets'), 'inspect') (('sprite_dim', 'range sprite_dim labels'), 'col') (('top where you', 'color points'), 'have') (('PCA Principal Component algorithm component linear deterministic principal that', 'as few dimensions'), 'Analysis') (('we', 'Summary Writersummary_writer'), 'store') (('you', 'data'), 'projection') (('PCA', 'local neighborhoods'), 'tend') (('we', 'labels'), 'do') "}