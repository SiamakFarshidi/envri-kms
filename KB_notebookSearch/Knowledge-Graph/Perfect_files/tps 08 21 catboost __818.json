{"name": "tps 08 21 catboost ", "full_name": " h2 Data import h1 EDA h1 Data preparation h1 Hyperparameters optimization h1 Model training h2 Feature importances h2 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "Model training Feature importances Submission linear algebra data processing CSV file I O e. copy df custom_feat_0 df_copy. As you can see f1 feature has the smallest amount of unique values 289. to_datetime test date_time format Y m d H M S Colors to be used for plots Plot dataframe Mask to hide upper right part of plot as it is a duplicate Making a plot Calculating edges of target bins to be used for stratified split x_scaler MinMaxScaler X pd. flatten Scaling data def add_new_features df Adds custom features to a given dataframe df_copy df. 060436568484918 depth 2 grow_policy Depthwise leaf_estimation_method Gradient fold_rmse np. split X target_bins X_train X_valid X. Data import EDA There are no missing value in the both datasets. reshape 1 1 print f Trees model. 578784014838337 bagging_temperature 0. loc valid_idx y_train y_valid y. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session nrows 10000 train date_time pd. Lets check feature values distribution in the both datasets. array oof_preds valid_idx. Lets look at feature correlation. As you can see the correlation is between 0. There are some features with relatively low correlation with target value even comparing with other features Lets visualize each feature vs loss. sum axis 1 df custom_feat_1 df_copy. So I don t think any feature should be treated as categorical. optimize lambda trial train_model_optuna trial X_train X_valid y_train y_valid n_trials 100 print Number of finished trials len study. 03 which is pretty small. drop id axis 1 columns test. params print Best score study. Lets check target distribution. 5813008056988401 random_strength 1. best_value Hyperparameters optimized by Optuna cb_params iterations 10000 learning_rate 0. mean axis 1 df custom_feat_2 df_copy. reshape 1 1 y_scaler. 2 random_state 42 for train_idx valid_idx in split. copy A set of hyperparameters to optimize by optuna Exact time split StratifiedShuffleSplit n_splits 1 test_size 0. Data preparation Hyperparameters optimization The code below is commented in order to save runtime. columns y_scaler MinMaxScaler y pd. The datasets are pretty well balanced. 030108080370377578 l2_leaf_reg 4. loc valid_idx study optuna. So the features are weakly correlated. 518949583881732 random_strength 1. read_csv Pandas setting to display more dataset rows and columns Input data files are available in the read only. 010526847803225213 l2_leaf_reg 7. drop id loss axis 1 columns train. max axis 1 df custom_feat_4 df_copy. median axis 1 return df X add_new_features X. 7705601193056997 depth 7 grow_policy Depthwise leaf_estimation_method Gradient cb_params iterations 10000 learning_rate 0. sqrt mean_squared_error y_scaler. create_study direction minimize study. columns X_test pd. copy X_test add_new_features X_test. trials print Best trial parameters study. to_datetime train date_time format Y m d H M S test date_time pd. 606620127979116 bagging_temperature 7. min axis 1 df custom_feat_3 df_copy. drop id loss axis 1. ", "id": "maximkazantsev/tps-08-21-catboost", "size": "818", "language": "python", "html_url": "https://www.kaggle.com/code/maximkazantsev/tps-08-21-catboost", "git_url": "https://www.kaggle.com/code/maximkazantsev/tps-08-21-catboost", "script": "seaborn catboost CatBoostRegressor mean_squared_error StratifiedKFold StratifiedShuffleSplit matplotlib.pyplot sklearn.preprocessing MinMaxScaler train_model_optuna add_new_features sklearn.model_selection pandas sklearn.metrics StandardScaler numpy ", "entities": "(('f1 feature', 'unique values'), 'have') (('best_value Hyperparameters', 'Optuna'), 'optimize') (('Lets', 'loss'), 'be') (('read_csv Pandas', 'columns Input data read'), 'be') (('code', 'runtime'), 'optimization') (('it', 'stratified split x_scaler MinMaxScaler X pd'), 'format') (('Lets', 'datasets'), 'check') (('correlation', '0'), 'be') (('Submission', 'linear algebra data CSV file'), 'importance') (('Scaling data flatten df', 'given df'), 'def') (('t', 'train date_time 10000 pd'), 'list') "}