{"name": "bayesian optimization of xgboost parameters ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "stdout out yield out finally sys. Large diff values may indicate that a particular set of parameters is overfitting especially if you check the CV portion of it in the log file and find out that train scores were improving much faster than validation scores. Obviously this is done to make sure that this notebook can run to completion on Kaggle. That s what the XGB_BO. Each parameter has its own line so it is easy to comment something out if you wish. If you repeat this run new output will be added to it Load data set and target values We really didn t need to load the test data in the first place unless you are planning to make a prediction at the end of this run. In a production version you should uncomment the first line in the section below and comment out or delete everything else. 05 range but beware that it will significantly slow down the process because more iterations will be required to get to early stopping. Keep in mind that in such a case you must comment out the matching lines in optimization and explore sections below. However I like to try couple of high and low end values for each parameter as a starting point and after that fewer random points are needed. The exact combination of parameters determines exploitation vs. DMatrix train label target XGB_BO. contextmanager def capture import sys from cStringIO import StringIO olderr oldout sys. Make sure to install the superb __Bayesian Optimization__ https github. Note that the learning rate eta is set to 0. They must match the parameters that are passed above to the XGB_CV function. com fmfn BayesianOptimization blob master examples exploitation 20vs 20exploration. Commenting out the capture so there will be no record of xgb. If you commented out any of them above you should do the same here. exploration https github. stdout olderr oldout out 0 out 0. Train and validation scores are also extracted in this section. Note the diff part in the printout below which is the difference between the two scores. collect dtrain xgb. These are the parameters and their ranges that will be used during optimization. This line is needed for python 2. com fmfn BayesianOptimization library. It is tough to know which would work better without actually trying though in my hands exploitation with expected improvement usually works the best. We can go with separately defined gini scorer and use feval gini below but I don t think it makes any difference because AUC and gini are directly correlated. cv is done this section puts its output into log file. maximize init_points 10 n_iter 50 acq ucb kappa 10 XGB_BO. XGBoost outputs lots of interesting info but it is not very helpful and clutters the screen when doing grid search. For n_iter 25 50 is usually enough. I am doing a stratified split and using only 25 of the data. Scaling is really not needed for XGBoost but I leave it here in case if you do the optimization using ML approaches that need it. Note that these are pretty wide ranges for most parameters. maximize init_points 10 n_iter 50 acq ucb kappa 1. You can simply specify that 10 20 random parameter combinations init_points below be used. AUC will be optimized here. If you have a special relationship with your computer and want to know everything it is saying back you d probably want to remove the two warnings lines and slide the XGB_BO line all the way left. I d say that 15 20 is usually adequate. Define cross validation variables that are used for parameter search. splitlines Comment out any parameter you don t want to test Define all XGboost parameters verbose_eval 10 This line would have been on top of this section with capture as result After xgb. maximize line below is specifying. stdout try out StringIO StringIO sys. In my version of sklearn there are many warning thrown out by the GP portion of this code. This will be used to capture stderr and stdout without having anything print on screen. This portions gives the summary and creates a CSV file with results. maximize init_points 10 n_iter 50 acq ei xi 0. This portion of the code is not necessary. 7 probably not for python 3 contextlib. print file log_file for line in result 1 print line file log_file log_file. splitlines out 1 out 1. The real code starts here. Good luck Let me know how it works. Doing 10 fold instead of 5 fold cross validation will also result in a small gain but will double the search time. You will probably want to experiment with values in 0. We will extract the relevant info from these variables later and will print the record of each CV run into a log file. flush Define the log file. This is set to prevent them from showing on screen. Note that a number of options must be the same for each parameter and they are applied vertically. It turns out that Kaggle does not have cStringIO so I will comment out this portion. I am doing only 2 initial points which along with 8 exploratory points above makes it 10 random parameter combinations. That is certainly not optimal but it will make the search go faster. So we will run XGboost CV with verbose turned on but will capture stderr in result 0 and stdout in result 1. There are several commented out maximize lines that could be worth exploring. ", "id": "tilii7/bayesian-optimization-of-xgboost-parameters", "size": "3874", "language": "python", "html_url": "https://www.kaggle.com/code/tilii7/bayesian-optimization-of-xgboost-parameters", "git_url": "https://www.kaggle.com/code/tilii7/bayesian-optimization-of-xgboost-parameters", "script": "bayes_opt load_data scale_data cStringIO xgboost cross_val_score numpy matthews_corrcoef roc_auc_score StratifiedKFold StratifiedShuffleSplit MinMaxScaler StringIO capture pandas print_function BayesianOptimization __future__ sklearn.cross_validation XGB_CV sklearn.metrics log_loss sklearn.preprocessing ", "entities": "(('more iterations', 'early stopping'), 'range') (('section', 'log file'), 'do') (('AUC', 'difference'), 'go') (('you', 'something'), 'have') (('you', 'sections'), 'keep') (('that', 'XGB_CV above function'), 'match') (('these', 'pretty wide most parameters'), 'note') (('XGboost 10 line', 'xgb'), 'comment') (('We', 'log file'), 'extract') (('This', 'screen'), 'set') (('Train scores', 'also section'), 'extract') (('fewer random points', 'starting point'), 'like') (('it', '8 exploratory points'), 'do') (('you', 'run'), 'add') (('work', 'usually best'), 'be') (('they', 'parameter'), 'note') (('portions', 'results'), 'give') (('You', '0'), 'want') (('notebook', 'Kaggle'), 'do') (('so I', 'portion'), 'turn') (('diff part', 'two scores'), 'note') (('you', 'below everything'), 'uncomment') (('Doing', 'search time'), 'result') (('you back d', 'XGB_BO line'), 'have') (('that', 'parameter search'), 'cross') (('that', 'it'), 'need') (('you', 'same'), 'do') (('that', 'optimization'), 'be') (('it', 'grid when search'), 'output') (('So we', '0 result'), 'run') (('train scores', 'validation much faster scores'), 'indicate') (('portion', 'code'), 'be') (('learning rate', '0'), 'note') (('anything', 'screen'), 'use') (('exact combination', 'exploitation'), 'determine') (('I', 'data'), 'do') "}