{"name": "unsupervised learning ", "full_name": " h1 Clustering for dataset exploration h1 Visualization with hierarchical clustering and t SNE h1 Decorrelating your data and dimension reduction h1 Discovering interpretable features h2 Non negative matrix factorization h2 Using scikit learn NMF h2 NMF components h2 NMF features h2 Sample reconstruction h2 NMF learns interpretable parts h3 NMF learns topics of documents h2 Which articles are similar to Cristiano Ronaldo ", "stargazers_count": 0, "forks_count": 0, "description": "In this exercise identify the topic of the corresponding NMF component. DataFrame label labels article titles print df. Ex Clustering Customers by their purchases Compressing the data using purchase pattern dimentionality reduction Difference between Supervised and Unsupervised Learning Supervised learning finds the pattern for a prediction task unsupervised learning find patterns but without specific prediction in mind. Datasets used Iris K Means Clustering Finds clusters of samples Number of clusters must be specified in advance. 75 WINE DATA Samples contain two wine features Seeds width length Data Scatter plot width vs length Calculate the Pearson correlation Display the correlation Apply the fit_transform method of model to grains pca_features Assign 0th column of pca_features xs Assign 1st column of pca_features ys Scatter plot xs vs ys Calculate the Pearson correlation of xs and ys Display the correlation Make a scatter plot of the untransformed points the coordinates of the mean of the data Get the first principal component first_pc Data varies the most along this direction Plot first_pc as an arrow starting at mean Keep axes on same scale FISH DATA Plot the explained variances from sklearn. For example you ll employ a variant of PCA will allow you to cluster Wikipedia articles by their content Visualizing the PCA Transformation Dimension Reduction Removes less informative noise features Principal Component Analysis Fundamental Dimension Reduction TechniqueFirst step is decorrealtion second is reducing dimension PCA aligns data with axes Rotates data samples to be aligned with axesShifts data samples so they have mean 0No information is lostPCA follows the fit transform pattern PCA Features Rows of tranformed corresponds to samplescolumns are PCA featuresFeatures of the data are correlated but but PCA Features are not linearly correlated. plotting sepal length and petal length Preparing seeds for clustering by dropping the target column Inertia decreases from 3 to 4 very slowly so 3 can be a good choice A vs LKG P vs C WK vs A_Coeff Variance comparison between Proline and OD280 After scaling we get tight clusters TSNE on Seed Data Create a TSNE instance model Apply fit_transform to samples tsne_features Select the 0th feature xs Select the 1st feature ys Scatter plot coloring by variety_numbers the t SNE visualization manages to separate the 3 varieties of grain samples. Principal Components Principal components is the direction of variance PCA Aligns principal componenets with the axesAvailable as components_ attribute of PCA objectEach row defines displacement from mean Decorrelating the grain measurements with PCA You observed in the previous exercise that the width and length measurements of the grain are correlated. After you are done take a moment to recognise the topic that the articles about Anne Hathaway and Denzel Washington have in common Which articles are similar to Cristiano Ronaldo You learned how to use NMF features and the cosine similarity to find similar articles. Decorrelating your data and dimension reductionDimension reduction summarizes a dataset using its common occuring patterns. In this section you ll learn about the most fundamental of dimension reduction techniques Principal Component Analysis PCA. Cant extend the map to new data. Now you ll use PCA to decorrelate these measurements then plot the decorrelated points and measure their Pearson correlation. annotate seedlabel x y fontsize 5 alpha 0. Previously you saw that the 3rd NMF feature value was high for the articles about actors Anne Hathaway and Denzel Washington. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. note that This will be a row corresponding to these topics from the NMF Features matrix which is of topic component dimension Print the row for Anne Hathaway Print the row for Denzel Washington Compute the dot products similarities dot product is cosine hence cosine simialrity. For example it expresses documents as combinations of topics and images in terms of commonly occurring visual patterns. Example Cluster labeled 2 has all the three class of Wines in it. For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. Apply this to your NMF model for popular Wikipedia articles by finding the articles most similar to the article about the footballer Cristiano Ronaldo. PCA is often used before supervised learning to improve model performance and generalization. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. Clustering for dataset exploration Definition Unsupervised learning is all about finding patterns in the data. Verify this for yourself for the NMF model that you built earlier using the Wikipedia articles. fit articles labels pipeline. You ll also learn to use NMF to build recommender systems that can find you similar articles to read or musical artists that match your listening history Non negative matrix factorizationNMF is non negative matrix factorization Dimension reduction technique NMF models are interpretable unlike PCA All features must be non negative Using scikit learn NMFFollows fit transfrom pattern must specify number of components works with numpy array and with csr_matrix NMF componentsNMF has components just like PCA has principal componentsDimension of components dimension of samples Entries are non negative NMF featuresNMF Feature values are non negative Can be used to reconstruct the samples combine feature values with components Sample reconstructionMultiply feature values with components and add upSample can be expressed as the product of matrices hence the term Matrix Factorization NMF learns interpretable parts NMF learns topics of documentsWhen NMF is applied to documents the components correspond to topics of documents and the NMF features reconstruct the documents from the topics. Applying Standard Scaler then KMeans in sklearn Pipeline Scaling without pipeline Visualization with hierarchical clustering and t SNE Hierarchical Clustering Cluster Labels in Hierarchical Clustering Cluster labels at any intermediate stage can be recoveredThen they can be used in cross tabulationHeight can be used to extract intermediate clustersHeight on the dendrogram is the distance between merging clusters Distance Between Clusters Defined by linkage methodSpecified via method parameter complete means distance between clusters is maximum distance between their samples Extraxcting Cluster Labels Use the fcluster methodReturns numpy array of cluster labels t SNE for 2 dimentional maps t distributed stochastic neighbor embeddingMaps samples to 2D or 3D spaceMap approximately preserves nearness of samplesGreat for inspecting datasets t SNE on iris data Iris samples are 4Dt SNE maps samples to 2D space Interpreting t SNE scatter plots versicolor and virginica harder to distinguishConsistent with intertia plot from KMeans both 2 and 3 clusters can be viewed t SNE in sklearn It has a fit_transform method no separate fit and transform methods exist. t sne features are different every time running again gives a different scatter plot although the relative position of clusters remain the same. decomposition import TruncatedSVD svd TruncatedSVD n_components 50 kmeans KMeans n_clusters 6 pipeline make_pipeline svd kmeans import pandas as pd pipeline. sort_values label nmf_features is a numpy array with shape Topics Components Lets consider the Topic Anne Hathaway and Denzel Washington and see what their NMF Features has to say. predict articles df pd. It can also be useful for unsupervised learning. New samples can be assigned to existing cluster K means remembers the mean of each cluster Cluster Centroids Finds the nearest centroid for each new sample Evaluating a Cluster Inertia measures cluster quality Measures how spread the clusters are the lower the betterDistance from each sample to centroid of its clusterAfter fit available as attribute inertia_k means attemps to minimize the inertia when choosing the clusters The number of clusters More clusters means lower inertia A good clustering has tight clusters so low inertiabut not too many clustersChoose an elbow in the inertia plot where inertia begins to decrease more slowly Analysing seeds data from UCI Tranforming Features for Better Clustering Piedmont wine Dataset Feature Variances The wine features have very different variances spread of its values Hence the cross tabulation results has loose clusters too many values overlapping. read_csv Input data files are available in the. Intrinsic dimensions Intrinsic dimension is the number of features needed to approximate the datasetIt is the essential idea behind dimension reductionIntrinsic dimention number of PCA Features with significant variance Dimension Reduction with PCA Dimension reduction of the fish measurementsIn a previous exercise you saw that 2 was a reasonable choice for the intrinsic dimension of the fish measurements. Learning rate should be carefully chosen wrong choice can bunch points together values between 50 and 200 can be chosen. Annotate the points for x y seedlabel in zip xs ys seed_labels plt. Discovering interpretable featuresIn this chapter you ll learn about a dimension reduction technique called Non negative matrix factorization NMF that expresses samples as combinations of interpretable parts. Now use PCA for dimensionality reduction of the fish measurements retaining only the 2 most important components. ", "id": "impratiksingh/unsupervised-learning", "size": "7496", "language": "python", "html_url": "https://www.kaggle.com/code/impratiksingh/unsupervised-learning", "git_url": "https://www.kaggle.com/code/impratiksingh/unsupervised-learning", "script": "linkage pearsonr scipy.sparse dendrogram numpy sklearn.cluster seaborn make_pipeline sklearn.datasets sklearn.manifold NMF csr_matrix PCA fcluster matplotlib.pyplot pandas KMeans scipy.cluster.hierarchy TruncatedSVD sklearn.pipeline scipy.stats normalize TSNE load_iris sklearn.decomposition StandardScaler sklearn.preprocessing ", "entities": "(('NMF Features', 'Denzel what'), 'label') (('it', 'commonly occurring visual patterns'), 'express') (('It', 'also unsupervised learning'), 'be') (('features', 'topics'), 'learn') (('together values', '50'), 'choose') (('Clustering', 'data'), 'be') (('NMF feature 3rd value', 'actors Anne Hathaway'), 'see') (('relative position', 'clusters'), 'be') (('separate fit methods', 'fit_transform method'), 'apply') (('2', 'fish measurements'), 'be') (('Now you', 'Pearson correlation'), 'use') (('You', 'cosine similar articles'), 'take') (('read_csv Input data files', 'the'), 'be') (('matrix factorization Non negative that', 'interpretable parts'), 'learn') (('SNE t visualization', 'grain samples'), 'be') (('pattern', 'mind'), 'find') (('too many values', 'loose clusters'), 'mean') (('It', 'python docker image https kaggle github'), 'come') (('reductionDimension data reduction', 'common occuring patterns'), 'summarize') (('you', 'output'), 'list') (('width measurements', 'grain'), 'be') (('PCA Features', 'data'), 'employ') (('you', 'Principal Component Analysis PCA'), 'learn') (('Data', 'sklearn'), 'contain') (('dot products similarities', 'Denzel Washington Compute'), 'note') (('decomposition TruncatedSVD pipeline make_pipeline svd import 50 KMeans 6 kmeans', 'pd pipeline'), 'svd') (('Example Cluster', 'it'), 'label') (('you', 'Wikipedia earlier articles'), 'verify') (('Finds clusters', 'advance'), 'use') (('PCA', 'model performance'), 'use') "}