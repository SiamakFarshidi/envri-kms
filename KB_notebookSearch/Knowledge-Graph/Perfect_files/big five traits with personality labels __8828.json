{"name": "big five traits with personality labels ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "Unlike agreeableness it seems that extraversion could be somewhat useful with regards to prediction. I ll therefore impute the missing value with 0. That is we might as well just choose our labels randomly and get results that are about as accurate So as our correlation heatmap has foretold it seems that our datapoints don t seem to have any clear relationship with their accompanying labels. I ll now go back to do some more exploration let s take a look at the correlation matrix of our data. As we already understand the relationship between the Big Five scores and the accompanying labels is very weak. The given data is composed of features that represent age in years and the Big Five personality traits each datapoint has ratings for openness conscientiousness extroversion agreeableness and neuroticism and is labeled by overall personality labels that don t seem to be directly related to Big Five psychometrics each datapoint is labeled as either dependable extraverted lively responsible or serious. So from first glance some Big Five traits seem potentially useful for prediction on their own neuroticism extraversion some traits seem potentially quite useless on their own openness agreeableness while conscientiousness looks to be somewhere in between. Still let us press on For our models to work well we should make sure that our data isn t too skewed openness and agreeableness have a negative skew below 0. First I ll give Gender and Personality numerical values Now I ll check for rows with null values Only one row with a null value in Gender. This does not discount the possibility of non linear relationships that could allow a model to make good predictions but it s not an encouraging start. Let s see if this assumption holds in our case by checking the distributions of personality labels in both our datasets As we can see the training and test sets given to us seem to come from very different distributions. We can see that extraversion has only somewhat different distributions across the personality labels and a rough divide can be seen between some of the labels. Can we try to make some lemonade from this quite sour lemon Maybe. Let s relabel Y and Y_test accordingly and fit some new models These results are better than flipping a coin but still quite mediocre. Still let s continue and see what we can do. Let s take a look Indeed we can see an imbalance in both sets. Whereas our training set is a bit imbalanced with regards to its personality labels our test set is even more imbalanced e. In this notebook I will explore the data and see how well it can be used for prediction while relying on a stacked metamodel. Still I ll define a few functions that will allow me to build a stacking metamodel and then see how well such a metamodel does on our test set a test set that has comes from a different distribution than our training set. This will allow us to see if there are any clear linear relationships between any of our values and more importantly between any of our features and the label. Perhaps if you limit yourself to the following labels you can achieve some sort of predictive capability by giving candidates Big 5 personality tests either a candidate is responsible or dependable OR he is not either of these. I ll use a square root transformation over a reflection of these columns At this point I will split the unified database back into the training and the test set and then scale them separately When we write machine learning models we very often start with the i. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session. read_csv Input data files are available in the read only. But before we jump to showing the predictive capabilites of a stacking metamodel let s get acquainted with the data. csv together so that the data can be first preprocessed and explored as a whole although I m not going to assume that these two datasets are similarly distributed they aren t which will result in some strange outcomes later on. It is unclear how these labels were created to begin with and as we shall soon see these labels are at most weakly related to the Big Five ratings. This isn t a good situation it means that we shouldn t expect any model trained on our training set to do very well on our test set. I ll start by defining the models and the parameter grids I want GridSearchCV to search through Now I ll define a function that will be able to return the best models and then use that function to get these best models and their scores What we have here is a pretty bad situation the best models we could find each have an accuracy of about 25 percent. d assumption that is we assume that our datapoints come from the same data generating process which also means that our training and test sets are assumed to be identically distributed. First I want to find the best hyperparameters for a number of different models. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. 5 so it s probably a good idea to unskew these columns. get_oof_predictions trains a model on all of a training set that is split into some number of cross validation sets each time making predictions on the part of the training set that was not trained on. Imagine you re an employer and want to hire people who are specifically responsible or dependable more than anything else. The best that the metamodel learned to do was to predict 0 for almost every datapoint of the training set and the same behavior applied to the test set ends up with an even higher accuracy So while the test accuracy is high were we to measure the F score of our predictions which also takes into account precision and recall it wouldn t be terribly good at all. train_metamodel uses get_oof_predictions to get all of the out of fold oof predictions for each model and then chooses the best hyperparameters for the metamodel that uses all of the oof predictions to predict the labels. This is likely due to an imbalance in the labels of both the training and the test set. Another possible approach to the data that I have not shown in this notebook would be to balance the given training data using an algorithm such as Synthetic Minority Oversampling Technique before fitting the models and metamodel. closer to the male numerical value. I ll group our dataframe by Personality and Age and see what the mean age value is for serious 21 year olds The mean is closer to 0 i. conscientiousness is distributed quite similarly within each personality label although the extraverted and responsible labels have more individuals with a below average rating. Before I preprocess anything I ll do some exploration. Still some limited predictions can ultimately be made. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. neuroticism looks a bit more useful for us in as much as it has somewhat different distributions within each personality label. Before I do a bit more data exploration some preprocessing is required. agreeableness has basically identical distributions for each personality label and is thus unlikely to be very useful when making predictions. Alas it seems that our data is simply not terribly suitable for good predictions as we were afraid from our preliminary exploration of the data especially if it is divided into differently distributed training and test sets. Finally I will use predict_with_models to prepare all of the predictions that the metamodel requires to make a final prediction on the test set. Each personality label has very similar distributions of openness besides lively which has a distribution which seems a bit more concentrated at above average values. I ll quickly impute it in a somewhat ad hoc manner. Alas it s now clear that there aren t any strong linear relationships at play the strongest correlations with regards to our label are with Gender and neuroticism and even those are quite weak. Whereas the metamodel shows a slight improvement in prediction accuracy over even the best submodel when it comes to the training set which is to be expected it is surprisingly accurate when it comes to the test set this is even more surprising due to the fact that the training and the test sets are differently distributed. I ll start by concatenating train. I will draw a box and whiskers plot for each Big Five trait to roughly see the distribution of each trait within each of the personality label categories. label 4 serious makes up almost half of the labels in the test set while labels 0 and 2 each make up less than ten percent. Still such an approach would not solve the problem of differently distributed training and test data so I have chosen not to show it here. We already have a rough idea from our box and whiskers plots that no Big Five trait seems to have a strong relationship with any of the personality labels but looking at a correlation matrix may still be useful. ", "id": "yonatanilan/big-five-traits-with-personality-labels", "size": "8828", "language": "python", "html_url": "https://www.kaggle.com/code/yonatanilan/big-five-traits-with-personality-labels", "git_url": "https://www.kaggle.com/code/yonatanilan/big-five-traits-with-personality-labels", "script": "RobustScaler sklearn.svm numpy seaborn SVC sklearn.neighbors sklearn.linear_model label)' KFold sklearn.model_selection pandas train_metamodel RandomForestClassifier LogisticRegression predict_with_models KNeighborsClassifier AdaBoostClassifier choose_hyperparameters get_oof_predictions GridSearchCV relabel_personality sklearn.ensemble sklearn.preprocessing ", "entities": "(('so it', 'probably good columns'), '5') (('it', 'good predictions'), 'discount') (('First I', 'different models'), 'want') (('s', 'data'), 'let') (('soon labels', 'Big Five ratings'), 'be') (('I', 'exploration'), 'preprocess') (('s', 'data'), 'go') (('read_csv Input data files', 'read'), 'be') (('each', 'about 25 percent'), 'start') (('how well it', 'stacked metamodel'), 'explore') (('also training sets', 'data generating same process'), 'assumption') (('who', 'specifically more anything'), 're') (('it', 'personality label'), 'look') (('I', '0'), 'impute') (('It', 'kaggle python Docker image https github'), 'come') (('agreeableness', 'thus very when predictions'), 'have') (('that', 'training set'), 'define') (('t which', 'strange outcomes'), 'csv') (('model', 'test very well set'), 't') (('datapoint', 'Big directly Five psychometrics'), 'compose') (('results', 'coin'), 'let') (('Big Five trait', 'correlation matrix'), 'have') (('we', 'what'), 'let') (('preprocessing', 'bit more'), 'require') (('metamodel', 'test set'), 'use') (('we', 'quite sour lemon'), 'try') (('which', 'it'), 'be') (('I', 'personality label categories'), 'draw') (('I', 'hoc somewhat ad manner'), 'impute') (('t', 'outside current session'), 'list') (('conscientiousness', 'openness potentially quite own agreeableness'), 'seem') (('he', 'either these'), 'be') (('mean', '0 i.'), 'will') (('that', 'labels'), 'use') (('I', 'models'), 'be') (('training', 'even more fact'), 'be') (('extraverted labels', 'below average rating'), 'distribute') (('especially it', 'differently distributed training'), 'seem') (('test set', 'personality'), 'be') (('extraversion', 'prediction'), 'seem') (('we', 'very often i.'), 'use') (('Indeed we', 'sets'), 'let') (('already relationship', 'Big Five scores'), 'be') (('label', 'less than ten percent'), 'make') (('Now I', 'Gender'), 'give') (('which', 'bit more average values'), 'have') (('test so I', 'it'), 'solve') (('that', 'training set'), 'train') (('us', 'features'), 'allow') (('datapoints', 'accompanying labels'), 'is') (('rough divide', 'labels'), 'see') (('data', '0'), 'let') (('This', 'training'), 'be') (('even those', 'Gender'), 's') (('training sets', 'very different distributions'), 'let') "}