{"name": "practical model evaluation day 2 ", "full_name": " h2 Load in data h2 Data preparation h1 XGBoost Baseline h1 Cloud AutoML h3 Prepare your account and project h3 Creating your dataset h3 Training our model h1 TPOT h1 H20 ai AutoML h1 Check that we ve saved each of our models h1 Exercise ", "stargazers_count": 0, "forks_count": 0, "description": "com rtatman practical model evaluation day 2 Day 3 Notebook Evaluating our models https www. ai AutoML http docs. Automated machine learning or AutoML for short is the task of removing human labor from the process of training machine learning models. com practical model evaluation event which ran from December 3 5 2019. aiLet s get started Load in dataFirst let s load in our pre cleaned data. Remember to save your models You ll need them tomorrow and since it takes a while to run AutoML code you don t want to have to run it multiple times. ai s open source AutoML library http docs. ai h2o latest stable h2o docs automl. Actually for Cloud AutoML we don t even need to split our data but we ll look at that in a minute. Day 1 Notebook Figuring out what matters for you https www. Give your dataset a name and make sure the region is US CENTRAL1. com watch v Rsg_XzgGqZw utm_medium notebook utm_source kaggle utm_campaign automl event goes into more details. I ve already done some data cleaning https www. Creating your datasetFor this workshop we re going to create our AutoML datasets using the GCP console. We ll be using ordinal label encoding https contrib. com appengine docs standard nodejs building app creating project utm_medium notebook utm_source kaggle utm_campaign automl event. This may take a while. Important Make sure in the Choose where to store your data step you pick Region and set the location as us central1 Iowa. We re all set ExerciseNow it s your turn Following the steps above use the df_2019 dataframe and Prepare your data split into testing and training encode variables Train your models XGBoost Cloud AutoML TPOT and H20 AutoML Note if you can t or would prefer not to set up billing on order to use Cloud AutoML feel free to skip training that model. com signup v2 webcreateaccount service cloudconsole continue https 3A 2F 2Fconsole. Select the bucket where you d like to store your data. This is an academic library built on top of scikit learn and my favorite thing about it is that when you export a model you re actually exporting all the Python code you need to train that model. be 7RdKnACscjA list PLqFaTIg4myu HA1VGJi_7IGFkKRoZeOFt. If you have the code in your notebook to import your dataset right before the code to create your model when you run your notebook top to bottom it ll give you an error because the modelling code was run before the dataset was done importing. Prepare your account and project First you ll need to create a GCP account https accounts. com watch v uINleRduCWM utm_medium notebook utm_source kaggle utm_campaign automl event. com compute docs regions zones utm_medium notebook utm_source kaggle utm_campaign automl event to us central1. com rtatman practical model evaluation day 1 Day 2 Notebook Training models with automated machine learning https www. com kaggle kaggle survey 2018 and 2019 https www. You can find the livestreams for this event here https youtu. html a second open source automated machine learning library developed by researchers at H20. Select Upload files from your computer and select the file with the dataset you want. To create a new bucket click on the icon that looks like a shopping basket with a plus sign in it. Have fun training your models and I ll see you all tomorrow for our final model evaluation Importing libraries set a seed for reproducability read in our data split into predictor target variables Splitting data into training and test set save out the split training data to use with Cloud AutoML encode all features using ordinal encoding you ll need to use a different encoder for each dataframe split encoded dataset train XGBoost model with default parameters and save our model don t change this value don t change this is the only region that works currently these you ll change based on your GCP project data this will come from your specific GCP project name of your uploaded dataset from GCP console column with feature you re trying to predict these can be whatever you like what you want to call your model max time to train model in milli hours from 1000 72000 first you ll need to make sure your model is predicting the right column let our model know that input columns may have missing values and then you ll need to kick off your model training check if it s done yet it won t be create fit TPOT classifier with save our model code print the model code to see what it says initilaize an H20 instance running locally convert our data to h20Frame an alternative to pandas datatables Run AutoML for 20 base models limited to 1 hour max runtime by default View the top five models from the AutoML Leaderboard The leader model can be access with aml. Click on BROWSE under the Select Files button and a side panel will pop up. Let s just double check that that s the case Alright we ve got three models and the code for the notebook. For TPOT and XGBoost however we need to make sure that all our input data is numeric. The reason for this is that importing datasets can take a while. be xP99eh6nQN0 utm_medium notebook utm_source kaggle utm_campaign automl event. Import your dataset. We cover XGBoost in more detail in the Intro To Machine Learning course https www. For now you do need to have a credit card in order to enable billing and you need billing enabled to use Cloud AutoML. Data preparationWe do have an additional step of preperation. This video https www. com rtatman practical model evaluation day 3 For today s exercise we re going to be working on classifying roles into job titles based on information about the role. Training our modelIn order to train an AutoML model from inside Kaggle Notebooks you ll need to attach a notebook to your Google Cloud Account. If you haven t created any buckets you ll see the text No buckets found. You should set the region of your project https cloud. Kaggle competitors are very fond of stacking and H20 is known for hiring a lot of top Kagglers so it s nice to have that automated for us. com learn intro to machine learning so I m not going to talk about it here. The data will be from the 2018 https www. I ll be using the 2018 data as an example and then have you work through the 2019 data as your exercise. TPOTAlright now we ll move onto TPOT https epistasislab. Check that we ve saved each of our modelsBefore we wrap up for the day we want to make sure we ve saved all of our models for tomorrow The Cloud AutoML model is saved automatically on GCP but we ve saved each of the other models in our current working directory. Follow the prompts to create your bucket. One thing that I like about this library is that as each model is trained its evaluated both on its own and as part of a stacked ensemble. com automl tables utm_medium notebook utm_source kaggle utm_campaign automl event page in the Google Cloud Console and click enable API. Cloud AutoMLNow let s train our Cloud AutoML model We ll be using both the GCP console and notebook code here so you ll probably want to open those in separate tabs or windows. Currently most AutoML research is focused on automating model selection and hyperparameter tuning. com 2F dsh S 385463669 3A1575309184770524 gmb exp biz false flowName GlifWebSignIn flowEntry SignUp nogm true utm_medium notebook utm_source kaggle utm_campaign automl event if you already have a Google account you can use that one and enable billing https www. If you re not able to enable billing you can still follow along with the rest of the workshop just skip the Cloud AutoML parts. com c kaggle survey 2019 Kaggle data science survey. First we ll split into training and testing sets For H20 AutoML and Cloud AutoML we don t need to do anything else. org categorical encoding ordinal. XGBoost BaselineFirst we re going to train a basic XGBoost model using the default arguments. Click on Datasets in the list on the left hand side of your screen and then click on the blue New Dataset text near the top of your screen. The libraries we ll be using are XGBoost https xgboost. These notebooks are part of Kaggle s Practical Model Evaluation https www. This video goes into more detail https youtu. Once your dataset is done importing take a close look at your imported data and make sure it looks the way you d expect. com automl utm_medium notebook utm_source kaggle utm_campaign automl event an enterprise focused automated machine learning product TPOT https epistasislab. Go to the AutoML Tables https console. This will let you train an AutoML Tables model in your current project. com rebeccaturner data prep for job title classification but if you d like to do your own or do some additional feature engineering feel free Today we ll be building four different models using four different libraries including some automated machine learning libraries. Then you can modify the following code to start your AutoML model training Once our model starts training we don t need to do anything else it s already saved in our GCP account and good to go for tomorrow. leader save the model out we ll need to for tomorrow check to see that we ve saved all of our models. ai AutoMLFor our final model we ll be using H20. io tpot an open source automated machine learning library developed at the University of Pennsylvania H20. From there create a new project https cloud. io en latest not automated machine learning we ll be using this as a baseline Cloud AutoML https cloud. ", "id": "rtatman/practical-model-evaluation-day-2", "size": "8531", "language": "python", "html_url": "https://www.kaggle.com/code/rtatman/practical-model-evaluation-day-2", "git_url": "https://www.kaggle.com/code/rtatman/practical-model-evaluation-day-2", "script": "automl_v1beta1 train_test_split h2o.automl confusion_matrix tpot xgboost accuracy_score XGBClassifier category_encoders auc KaggleKernelCredentials kaggle_secrets sklearn.model_selection pandas TPOTClassifier google.cloud H2OAutoML automl_v1beta1 as automl storage GcpTarget sklearn.metrics kaggle.gcp ", "entities": "(('model evaluation com rtatman practical day 2 3 Notebook', 'models https www'), 'evaluate') (('I', 'data cleaning https ve already www'), 'do') (('where you', 'data'), 'select') (('data', 'https 2018 www'), 'be') (('you', 'Google Cloud Account'), 'need') (('leader model', 'aml'), 'have') (('final we', 'H20'), 'ai') (('model', 'stacked ensemble'), 'be') (('we', 'notebook'), 'let') (('which', 'December'), 'com') (('we', 'minute'), 'don') (('what', 'https www'), 'day') (('You', 'event'), 'find') (('we', 'role'), 'day') (('you', 'dataset'), 'file') (('you way d', 'imported data'), 'take') (('com watch utm_medium utm_source utm_campaign automl Rsg_XzgGqZw event', 'more details'), 'v') (('we', 'models'), 'save') (('that', 'it'), 'create') (('workshop we', 'GCP console'), 'create') (('importing datasets', 'while'), 'be') (('AutoML Currently most research', 'model selection'), 'focus') (('s', 'our'), 'start') (('then you', 'exercise'), 'use') (('We', 'https www'), 'cover') (('Today we', 'machine learning automated libraries'), 'prep') (('else it', 'tomorrow'), 'modify') (('TPOTAlright now we', 'TPOT https epistasislab'), 'move') (('input data', 'TPOT'), 'need') (('buckets', 'text'), 'create') (('that', 'us'), 'be') (('so I', 'it'), 'learn') (('Data preparationWe', 'preperation'), 'have') (('you', 'billing https www'), 'S') (('region', 'name'), 'give') (('com signup v2 webcreateaccount service cloudconsole', 'https 3A 2F'), 'continue') (('billing', 'Cloud AutoML'), 'need') (('you', 'Cloud AutoML just parts'), 'skip') (('you', 'current project'), 'let') (('We', 'label encoding https ordinal contrib'), 'use') (('you', 'us'), 'make') (('Click', 'Select Files button'), 'pop') (('enterprise', 'product TPOT https automated machine learning epistasislab'), 'automl') (('AutoML open library', 'docs'), 's') (('we', 'Cloud AutoML https baseline cloud'), 'en') (('dataset', 'error'), 'give') (('video', 'detail https more youtu'), 'go') (('you', 'model'), 'be') (('don t', 'anything'), 'split') (('we', 'working current directory'), 'check') (('you', 'don it'), 'remember') (('here so you', 'separate tabs'), 'let') (('You', 'project https cloud'), 'set') (('notebooks', 'Practical Model Evaluation https www'), 'be') (('First you', 'GCP account https accounts'), 'prepare') (('XGBoost Cloud AutoML H20 AutoML you', 'model'), 'set') (('machine Automated learning', 'machine learning models'), 'be') (('we', 'default arguments'), 'go') "}