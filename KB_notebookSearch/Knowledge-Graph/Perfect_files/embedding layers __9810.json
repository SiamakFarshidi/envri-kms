{"name": "embedding layers ", "full_name": " h1 Sparse Categorical Variables h1 MovieLens h1 Building a rating prediction model in Keras h2 Bad idea 1 Use user ids and movie ids as numerical inputs h2 Bad idea 2 One hot encoded user and movie inputs h2 Good idea Embedding layers h2 Implementing it h2 Training it h2 Example predictions h1 Your turn h3 P S ", "stargazers_count": 0, "forks_count": 0, "description": "There are a few movies in the validation set not present in the training set. The column y is just a copy of the rating column with the mean subtracted this will be useful later. 5 5 representing how many stars we think this user would give that movie. Your turn Head over to the Exercises notebook https www. User 26556 has given out two perfect ratings to the movies Airplane and Airplane II The Sequel. But the actual numerical values of the ids assigned to users and movies are meaningless. Here s the code Training itWe ll compile our model to minimize squared error MSE. In this lesson I ll be using the MovieLens dataset https www. com dansbecker using categorical data with one hot encoding. One hot encoding is fine for categorical variables with a small number of possible values like Red Yellow Green or Monday Tuesday Wednesday Friday Saturday Sunday. Word embeddings are a crucial technique for applying deep learning to natural language. This is much more efficient than taking a one hot vector and doing a huge matrix multiplication As an example if we learn embeddings of size 8 for movies the embedding for Legally Blonde index 4352 might look like 1. To translate our model s output values to the original 0. y as my target variable rather than df. com kernels fork 1598432 to get some hands on practice working with embedding layers. Bad idea 1 Use user ids and movie ids as numerical inputsWhy not feed in user ids and movie ids as inputs then add on some dense layers and call it a day i. To judge whether our model is any good it d be helpful to have a baseline. Let s also throw in a couple examples of movies that this user seems unlikely to ever watch according to their rating history. Schindler s List has id 527 and The Usual Suspects has id 50 but that doesn t mean Schindler s List is ten times bigger than The Usual Suspects. Why Let s review some alternatives and see why they don t work. Aside A key implementation detail is that embedding layers take as input the index of the entity being embedded i. They have many possible values Building a rating prediction model in KerasWe want to build a model that takes a user u_i and a movie m_j and outputs a number from 0. For more details feel free to check out this kernel https www. com grouplens movielens 20m dataset includes information about each movie such as its title its year of release a set of genres and user assigned tags. Import libraries and load dataframes for Movielens data. Sparse Categorical VariablesBy this I mean a categorical variable with lots of possible values high cardinality with a small number of them often just 1 present in any given observation. 1 stars or about 15. We ll need to turn to the more powerful functional API using the keras. Bad idea 2 One hot encoded user and movie inputsIf you re not familiar with one hot encoding you may want to check out our lesson Using Categorical Data with One Hot Encoding https www. A single input to our model is a vector of 165 237 numbers of which we know that 165 235 will be zeros. Something to think about We know that ratings can only take on the values 0. com learn deep learning won t work. The y column is just a centered version of the rating i. com 82826168584267 I d greatly appreciate it. But it s not so great in cases like our movie recommendation problem where variables have tens or hundreds of thousands of possible values. Sequential https www. com colinmorris movielens preprocessing with all the preprocessing I performed on the MovieLens dataset. We ll start by picking out a specific user from the dataset at random. Our goal will be to predict the rating a given user u_i will give a particular movie m_j. In this lesson I ll show how to implement a model with embedding layers using the tf. We ll also include absolute error MAE as a metric to report during training since it s a bit easier to interpret. the rating column minus its mean over the training set. io getting started functional api guide. 761 Where do these come from We initialize an embedding for each user and movie using random noise then we train them as part of the process of training the overall rating prediction model. Implementing itI want my model to look something like this Imgur https i. png A key thing to note is that this network is not simply a stack of layers from input to output. We re treating the user and the movie as separate inputs which come together only after each has gone through its own embedding layer. com meganrisdal la county restaurant inspections and violations has several sparse categorical variables including employee_id which of the health department s employees performed this inspection 250 distinct values facility_zip what zip code is the restaurant located in 3 000 distinct values owner_name who owns the restaurant 35 000 distinct values An embedding layer would be a good idea for using any of these variables as inputs to a network. org api_guides python train Optimizers Passing in a string like adam or SGD will load one of keras s optimizers found under tf. org wiki The_Naked_Gun series another series of spoof films starring Leslie Nielsen. Save training history for later comparison User ids Movie ids NB Remember we trained on y which was a version of the rating column centered on 0. We ll just use the global mean rating in their case. So why is it a bad idea here Let s see what a model would look like that took one hot encoded users and movies. In the cell below we calculate the error of a couple dumb baselines always predicting the global average rating and predicting the average rating per movie Here s a plot of our embedding model s absolute error over time. For more detail on the Functional API check out Keras s guide here https keras. For example if the overall average rating in the training set was 3 stars then we would translate 3 star ratings to 0 5 star ratings to 2. Aside You may have noticed that the MovieLens dataset https www. You can also leave public feedback in the comments below or on the Learn Forum https www. You can think of it as a sort of lookup table. userId and movieId are both sparse categorical variables. 5 5 so why not treat this as a multiclass classification problem with 10 classes one for each possible star rating Let s train the model. org api_docs python tf keras Sequential class which you may be familiar with from our course on deep learning with image data https www. Not bad Example predictionsLet s try some example predictions as a sanity check. This course is still in beta so I d love to get your feedback. If you have a moment to fill out a super short survey about this lesson https form. But other examples abound. Good luck fitting that all into memory at once Also doing training and inference on our model will be inefficient. I claim we need an embedding layer to handle these inputs. In that lesson we claim that one hot encoding is The Standard Approach for Categorical Data. Welcome to our first lesson on the topic of embeddings. MovieLensThe MovieLens dataset consists of ratings assigned to movies by users. Sound mysterious In later lessons I ll show some techniques for interpreting learned embeddings such as visualizing them with the t SNE algorithm. Embeddings are a technique that enable deep neural nets to work with sparse categorical variables. They seem to be much slower on problems like this because they don t efficiently handle sparse gradient updates. To calculate the activations of our first hidden layer we ll need to multiply our 165k inputs through about 21 million weights but the vast vast majority of those products will just be zero. This means that the keras. A basic issue here is scaling and efficiency. We don t have as much evidence about what this user hates. Great choices Perhaps they d also enjoy the The Naked Gun https en. It s up to the model to discover whatever properties of the entities are useful for the prediction task and encode them in the embedding space. There are hundreds of thousands of them in the English language but a single tweet might only have a dozen. The feature data for our whole dataset of 20 million rating instances will require a 2 d array of size 20 000 000 x 165 237 or about 3 trillion numbers. What do they mean An object s embedding if it s any good should capture some useful latent properties of that object. But the key word here is latent AKA hidden. Rather than extrapolating from their few low ratings a better indication of this user s dislikes might be the kinds of movies they haven t even rated. org api_docs python tf keras Model class. In the simplest terms neural nets work by doing math on their inputs. Looks pretty reasonable For each of the movies in the The Naked Gun series our predicted ratings for this user are around a full star above the average rating in the dataset and our out of left field picks have their predicted ratings downgraded compared to average. Good idea Embedding layersIn short an embedding layer maps each element in a set of discrete things like words users or movies to a dense vector of real numbers its embedding. com grouplens movielens 20m dataset as an example. Here s a sample Ratings range from 0. Aside I m passing in df. we can give it our userIds and movieIds as input. This is a common practice in deep learning and tends to help achieve better results in fewer epochs. 5 5 star rating scale we need to uncenter the values by adding the mean back The difference between rating and y will be the same for all rows so we can just use the first Add a column with the difference between our predicted rating for this user and the movie s overall average rating across all users in the dataset. For comparison our best baseline predicting the average rating per movie is marked with a dotted line Compared to the baseline we were able to get our average error down by more than. One good example is words. For example this dataset of LA county restaurant inspections https www. But for now we re not going to try to exploit any of that extra information. Set random seeds for reproducibility Shuffle 2 input values user id and movie id A single output node containing the predicted rating One hidden layer with 128 units A single output node containing the predicted rating Each instance will consist of two inputs a single user id and a single movie id Concatenate the embeddings and remove the useless extra dimension Add one or more hidden layers A single output our predicted rating Technical note when using embedding layers I highly recommend using one of the optimizers found in tf. ", "id": "colinmorris/embedding-layers", "size": "9810", "language": "python", "html_url": "https://www.kaggle.com/code/colinmorris/embedding-layers", "git_url": "https://www.kaggle.com/code/colinmorris/embedding-layers", "script": "sklearn get_metrics tensorflow matplotlib train_test_split pyplot pyplot as plt keras metrics sklearn.model_selection pandas numpy ", "entities": "(('One hot encoding', 'Red Yellow Green'), 'be') (('com grouplens', 'example'), 'movielen') (('t', 'mean ten times Usual Suspects'), 'have') (('predicted ratings', 'average'), 'look') (('I', 'MovieLens dataset'), 'movielens') (('embedding layer', 'network'), 'com') (('embedding Good layersIn short layer', 'embedding'), 'idea') (('We', 'keras'), 'need') (('I', 'https dataset www'), 'use') (('4352', '1'), 'be') (('network', 'output'), 'png') (('t', 'gradient efficiently sparse updates'), 'seem') (('Word embeddings', 'natural language'), 'be') (('This', 'fewer epochs'), 'be') (('they', 't'), 'be') (('165 235', 'which'), 'be') (('movie inputs', 'it'), 'idea') (('org api_guides python train Optimizers', 'tf'), 'load') (('that', 'one hot encoded users'), 'be') (('course', 'feedback'), 'be') (('one hot encoding', 'Standard Categorical Data'), 'claim') (('it any d', 'baseline'), 'judge') (('why they', 't work'), 'let') (('user', 'rating ever history'), 'let') (('together only each', 'embedding own layer'), 'treat') (('ratings', 'only values'), 'something') (('it', 'object'), 'capture') (('you', 'lesson https form'), 'have') (('s', 'model'), '5') (('we', 'dataset'), 'be') (('rating column', 'training set'), 'mean') (('deep neural nets', 'sparse categorical variables'), 'be') (('it', 'training'), 'include') (('We', 'case'), 'use') (('that', '0'), 'have') (('you', 'Hot Encoding https One www'), 'idea') (('mean this', 'rating just column'), 'be') (('user', 'movie'), '5') (('feature data', 'size'), 'require') (('then we', 'rating prediction overall model'), '761') (('MovieLensThe MovieLens', 'users'), 'dataset') (('User', 'movies'), 'give') (('You', 'Learn Forum https www'), 'leave') (('we', 'extra information'), 'go') (('embedding layers', 'entity'), 'be') (('You', 'lookup table'), 'think') (('properties', 'embedding space'), 's') (('single tweet', 'only dozen'), 'be') (('embedding layers', 'tf'), 'show') (('we', 'inputs'), 'claim') (('neural nets', 'inputs'), 'work') (('model', 'Imgur https i.'), 'want') (('vast vast majority', 'products'), 'be') (('org api_docs', 'tf keras Model class'), 'python') (('m 20 dataset', 'tags'), 'movielen') (('3 then we', '2'), 'translate') (('we', 'input'), 'give') (('MovieLens', 'https www'), 'notice') (('given user', 'u_i particular movie'), 'be') (('Training itWe', 'error squared MSE'), 's') (('I', 't SNE algorithm'), 'mysterious') (('average error', 'more'), 'mark') (('Example bad s', 'sanity check'), 'predictionsLet') (('d', 'greatly it'), 'com') (('y column', 'rating just centered i.'), 'be') (('where variables', 'possible values'), 's') (('Here plot', 'time'), 'calculate') (('you', 'image data https www'), 'python') (('user', 'what'), 'don') (('actual numerical values', 'users'), 'be') (('which', '0'), 'save') (('Perhaps they', 'Naked Gun also https'), 'enjoy') (('I', 'tf'), 'Set') (('Sparse Categorical I', 'often just 1 given observation'), 'VariablesBy') (('We', 'random'), 'start') "}