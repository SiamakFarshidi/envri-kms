{"name": "principal component analysis pca ", "full_name": " h1 Principal Component Analysis PCA h3 It is a data transformation technique The final objective of PCA is DIMENSIONALITY REDUCTION h1 Using Kmeans on the transformed data using PCA h1 Kmeans on original data ", "stargazers_count": 0, "forks_count": 0, "description": "You can see the plot below FIGURE 1 alt text https i. Out these there are 2 features which are highly correlated to each other. There is a chance that those weakly correlated features have high correlation among themselves eventually adding redundancy and multicolliearity to our models. On the other hand there is a lot of redundant overlapping data due to high correlation in these 2 features which gives rise to multicollinearity. So most of the information essential to differentiate the feature from each other has been captured by the 1st two PCs. So we would need to transpose our selected PCs to do the dot product. png This redundant data causes multicolliearity. Consider we have 15 features in a dataset. Reducing high number of PCs could lead to introducing bias error in our model. Eigen Vectors here are the principal components PCs. This would mean that we are considering using only the important information from the high variance zone FIGURE 2 and eliminating all the redundant noise less important information by elimininating the PCs which explain the low variance region FIGURE 3. abnormal bp range abnormal rbc wbc level bcoz it could lead to a loss of extensive information and our model could behave unexpectedly. cov calculates the covariance row wise. So we can say that we are not sacrificing the feature but we are sacrificing the information content at the low frequency low variance zone. t their corresponding eigen values. So we transpose the dataset to represent the features as rows. Please UpVote if you like the work Principal Component Analysis PCA It is a data transformation technique. The final objective of PCA is DIMENSIONALITY REDUCTION. So the intention of PCA is to reduce that noise. The following plot represents this redundant data FIGURE 3 alt text https i. Thus it gives us an improved performance after transforming the data using PCA. Consider an example of predicting a cancerous patient. png The important information which these 2 features will provide us will be those data points which are not redundant non overlapping and lie at the extreme ends of the histograms as shown in the plot below FIGURE 2 alt text https i. We need to sort these eigen vectors in descending order w. To know more about why we compare inertia values for comparing models please do checkout my kernel Kmeans Clustering Guide https www. Overdoing PCA is not advisable. The actual representation of principal components will be the transpose of eigen vectors as follows PC1 0. Now to transform our data to a data which has its dimensionality reduced we need to do a dot product between our originally scaled data and the PCs. Lets have a look at it into with an example more intuitively. So if we plot a histogram of these 2 features a majority of the part will be overlapping as they are highly correlated. The least percentage advisable is around 90 percent. This multicolliearity effect is the NOISE in PCA. So even if we lose some data from this part there won t be much information loss. com pratikasarkar kmeans clustering guide Please UpVote if you like the work. The general myth that the idea of dimensionality reduction means that PCA will drop some of the weak features is WRONG. But on the other hand we can t affort to lose the information from a cancerous patient eg. png This is the information which we cannot afford to lose because the information in these records are responsible for significantly differentiating the 2 features from each other. 52354627 So as we can see that most of the variance in data has been explained by the 1st two principal components that is about 96 percent. Consider we have some features which are highly significant in our data out of these some are highly correlated to out target variable and some a weakly correlated. normal bp range normal rbc wbc level as it will not affect our prediction result to an extreme level. Our scaled data is 150x4 and our selected PCs are 2X4. Using Kmeans on the transformed data using PCA Kmeans on original dataAs we can see that the inertia value on transformed data using PCA is less as compared to the inertia value on the original data. We can afford to lose some information from a healthy patient eg. ", "id": "pratikasarkar/principal-component-analysis-pca", "size": "4645", "language": "python", "html_url": "https://www.kaggle.com/code/pratikasarkar/principal-component-analysis-pca", "git_url": "https://www.kaggle.com/code/pratikasarkar/principal-component-analysis-pca", "script": "sklearn.cluster seaborn numpy pandas KMeans StandardScaler sklearn.preprocessing ", "entities": "(('You', 'alt text https FIGURE 1 i.'), 'see') (('model', 'extensive information'), 'behave') (('It', 'work Principal Component Analysis PCA'), 'please') (('a', 'some'), 'correlate') (('which', 'PCs'), 'mean') (('So intention', 'noise'), 'be') (('which', 'multicollinearity'), 'be') (('we', 'originally scaled data'), 'transform') (('Reducing', 'model'), 'lead') (('actual representation', 'PC1'), 'be') (('So even we', 'there t'), 'win') (('So we', 'rows'), 'transpose') (('2 which', 'highly other'), 'be') (('Kmeans', 'Guide https www'), 'checkout') (('they', 'part'), 'feature') (('it', 'extreme level'), 'result') (('So we', 'dot product'), 'need') (('final objective', 'PCA'), 'be') (('we', 'dataset'), 'consider') (('data which', 'alt text https FIGURE 2 i.'), 'be') (('information', 'other'), 'png') (('We', 'order descending w.'), 'need') (('multicolliearity effect', 'PCA'), 'be') (('you', 'work'), 'cluster') (('png redundant data', 'multicolliearity'), 'cause') (('some', 'weak features'), 'be') (('inertia value', 'original data'), 'see') (('So most', '1st two PCs'), 'capture') (('weakly correlated features', 'models'), 'be') (('We', 'healthy patient eg'), 'afford') (('we', 'patient cancerous eg'), 'can') (('we', 'frequency variance low low zone'), 'say') (('that', '1st two principal components'), '52354627') (('Thus it', 'PCA'), 'give') (('Lets', 'example'), 'have') (('following plot', 'data redundant FIGURE'), 'represent') "}