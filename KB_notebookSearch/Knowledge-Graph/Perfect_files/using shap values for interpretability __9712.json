{"name": "using shap values for interpretability ", "full_name": " h1 Using SHAP values for XGBoost interpretability h2 The data Wisconsin Breast Cancer Dataset h2 Explaining the model with SHAP values h2 Conclusions ", "stargazers_count": 0, "forks_count": 0, "description": "Using SHAP values for XGBoost interpretabilityMost of the algorithms whose popularity has risen over the recent years are considered black box methods. Another interesting dependence plot is the one of radius_worst that shows two clearly differentiated segments. High values of both radius_worst and concavity_worse indicate healthy tissue. However SHAP values are especially useful when interpreting individual results. This can be confusing now but will get easier with the examples below. We can see that along with the name of a variable there is a plot. Also this will make the tree return continuous probabilities which are always useful for this problemas. The example of this are force plots where the horizontal axis are the SHAP value of the final prediction and each feature s contribution is shown a block left of the result the positive SHAP values pushing the probability higher and lower SHAP values in the right pushing the probability lower. We can see how the final SHAP value which can be transformed from a continuous domain to probability using a sigmoid is 2. Therefore a general interpretability of the model is hard and prone to miss plenty of information. Creating a binary benign columns indicating if the tumor was benign or not should suffice. These values are a representation of the impact of a certain feature in a prediction positive values contribute to increase the final probability and negative values contribute to decrease it. Once the eye is trained enough in a glimpse it is possible to see much more than that perimeter points_worst values are correlated with their SHAP values colors seem to advance in order from left to right and it can be a really good indicator of benign tissue the points in the right most side of the graph are further from zero than in other variables. Below we can see the dependence plot of perimeter_worst as seen in the dependence plot higher values of the variable are related to higher SHAP values. Note putting it simply in this models red means good and blue means bad. Hopefully this kernel helps you find the motivation to look for interpretability of your models and exploit the information that it can provide. We can also see how the color denotes that when the perimeter_worst has a low value texture_worst makes the SHAP value lower the higher it is. We will be using a 5 folds cross validation to train the model. On the other hand we can see that in an example of malign tissue the 371st sample the concavity texture and perimeter of the cells make the model predict it as a negative case. It is obvious for most data scientist that we will not be able to understand a neural network inner workings as trivially as the ones of a decision tree but studying the relation between its input and outputs using an unified approach as SHAP can help the community and with some pedagogy a client understand how a ML model is making predictions. We can see an example of it in a case of benign tissue the first one in the training data. It also contains the diagnosis column as the label of the samples which is B for benign results and M for malign. This matrix shap_values is related with the training data The element shap_values i j is the SHAP value of the i sample s j feature. Refer to the repository https github. In this case only transforming the diagnosis columns is necessary. Similarly gradient techniques like the ones used in XGBoost may use dozens of trees for regularization rendering their interpretability as useless for most users. The main thing to highlight is that using binary logistic as objective function and auc as the evaluation metric seem to be adequeate for the task. That way the library helps us find multi dimensional dependences in the data. This is however out of the scope of the kernel. The first artifact that can be generated with these values is a summary plot which plots the most important features for the model along with a visual intuition on how they influence it. The highest contribution is the one by concave points_worst. In the confusion matrix is also possible to see how the discrete predictions out of the probabilities have a great performance an average precision of around 95. In the summary plot we can see how the most important features for the model are perimeter_worst concave points_mean concave points_worst texture_worst and area_se among others. That can be misleading at times so if you are struggling with it you can change the color of it. We can take a step down the abstraction scale and see individual features dependence plots. 29 and higher than the base value therefore predicted as benign. These values are not exclusive to XGBoost plenty of other models have optimized explainers in SHAP and the technique itself is model agnostic. For XGBoost to properly manage the data we must transform all columns to numeric values. On the other hand the interpretability of a single sample can be as straightforward as a force plot which provides a lot of visual information. This time it is clear that low values of radius_worst imply a lower probability of it benign and the samples in that segment with high concavity_worst seem to indicate a higher risk of malign tumors. This notebook aims to be both an introduction to SHAP https github. On the other hand neural networks seem to magically fit to training data and then use their obscure weights to spit a prediction. The main cross validation loop runs computing also the scale_pos_weight parameter which compensates unbalanceness in the training data per fold. Explaining the model with SHAP valuesUsing the SHAP module we can generate different explainers objects used to compute the SHAP values for each sample. The final results are quite satisfactory. These features represent different characterstics like the texture or the simmetry computed for each cell and presented as the mean standard deviation and worst sum of the three largest values of each of them. Lowering the abstraction from the general model to a single prediction makes the interpretation much more intuitive resulting in more useful explanations. We can drop the id and the original diagnosis column we will not be using them anymore in this notebook. com slundberg shap for more details. In it each point being a sample and its color the value of the feature in it we can see its SHAP value in the horizontal axis. The representation shows how despite the texture having anomalous values the concavity and perimeter features finally turn the prediction to the positive side. Take into account that high SHAP values contribute to the sample being benign and low values contributing to it being malign. The data Wisconsin Breast Cancer DatasetFor this demo we will use the Wisconsin Breast Cancer Dataset which contains 30 features extracted from an image of a fine needle aspirate FNA of a breast mass. Try running the function with the keyword argument plot_cmap 00AA00 EE2222 to turn it into green means good red means bad. We can round later if needed. Interpretability is the only way to demystify ML DL algorithms for non specialized people and clear its inclusion as a general tool to solve different problems in industry or society. Remember kids always cross validate your results This one is a relatively well behaved dataset and we are not in a competition so a minimal parametrization will be performed. com slundberg shap which presents itself as a unified approach to explain the output of any machine learning model and a motivation to understand interpretability in a different way than it is popularly imagined. Also let s split the data in the conventional names X for the feature matrix and y for the labels vector. In the second variable the perimeter_worst variable is not as disperse in its SHAP values but the majority of values are not in zero which implies no influence in the prediction. ConclusionsIt is important to understand that while Linear Models exploit the average of all the training samples Machine Learning usually is able to exploit individual charactersitics to perform a non uniform prediction. Contrary to interpretable methods that produce predictions that can be explained by taking a look at the inner workings of the model black box methods present the prediction as a mistery to the user. These graphs plot the feature s value respect to its SHAP value letting us understand easily how they are related. One of the most trivially interpretable methods decision trees provide a trivial way of interpreting generating a binary decission tree if X then A else B is enough to traverse the tree checking each sample to understand their prediction. Still take into account that no parameter search or feature engineering has been performed so there is plenty of room for improvement. We can also see that this relation is not linear but closer to having a threshold around 110. Due to these model s nature each sample has to be studied individually so most of these graphs are just an aggregation of all these samples. This model is able to make accurate predictions but thanks to these SHAP interpretations also its explanations are available which provides practitioners with useful information to understand the model and data scientists with valuable information to tune it. We will compute these values for all the samples X. SHAP also plots a second variables value which is automatically chosen depending its interaction with the feature at hand. ", "id": "diegovicente/using-shap-values-for-interpretability", "size": "9712", "language": "python", "html_url": "https://www.kaggle.com/code/diegovicente/using-shap-values-for-interpretability", "git_url": "https://www.kaggle.com/code/diegovicente/using-shap-values-for-interpretability", "script": "seaborn KFold matplotlib.pyplot matplotlib.pylab confusion_matrix sklearn.model_selection pandas sklearn.metrics xgboost roc_auc_score numpy ", "entities": "(('individual', 'dependence plots'), 'take') (('higher values', 'SHAP higher values'), 'see') (('only transforming', 'diagnosis columns'), 'be') (('individually most', 'just samples'), 'have') (('it', 'other variables'), 'be') (('which', 'always problemas'), 'make') (('features', 'them'), 'represent') (('we', 'sample'), 'explain') (('concavity', 'positive side'), 'show') (('easily how they', 'SHAP value'), 'plot') (('highest contribution', 'concave points_worst'), 'be') (('tumor', 'binary benign columns'), 'create') (('also how discrete predictions', 'around 95'), 'be') (('the it', 'low value'), 'see') (('which', 'prediction'), 'be') (('you', 'it'), 'be') (('Interpretability', 'industry'), 'be') (('also relation', '110'), 'see') (('We', 'samples'), 'compute') (('ML how model', 'predictions'), 'be') (('High values', 'healthy tissue'), 'indicate') (('diagnosis original we', 'anymore notebook'), 'drop') (('us', 'data'), 'help') (('which', 'hand'), 'plot') (('We', 'model'), 'use') (('model', 'negative case'), 'see') (('then else B', 'prediction'), 'provide') (('which', 'fold'), 'run') (('evaluation metric', 'task'), 'be') (('minimal parametrization', 'competition'), 'remember') (('low values', 'malign tumors'), 'be') (('which', 'it'), 'be') (('that', 'user'), 'present') (('we', 'horizontal axis'), 'be') (('which', 'breast mass'), 'use') (('which', 'sigmoid'), 'see') (('Also s', 'labels vector'), 'let') (('it', 'different way'), 'shap') (('it', 'that'), 'help') (('Try', 'good red means'), 'mean') (('Therefore general interpretability', 'information'), 'be') (('which', 'malign'), 'contain') (('This', 'kernel'), 'be') (('This', 'now examples'), 'confuse') (('notebook', 'SHAP https github'), 'aim') (('popularity', 'recent years'), 'consider') (('j', 'SHAP i j feature'), 'relate') (('technique', 'SHAP'), 'be') (('SHAP However values', 'especially when individual results'), 'be') (('how they', 'it'), 'be') (('putting', 'simply models'), 'mean') (('which', 'visual information'), 'be') (('values', 'negative it'), 'be') (('Machine Learning', 'non uniform prediction'), 'be') (('that', 'two clearly differentiated segments'), 'be') (('parameter search', 'improvement'), 'take') (('We', 'variable'), 'see') (('interpretation', 'much more more useful explanations'), 'make') (('we', 'numeric values'), 'manage') (('perimeter_worst points_mean', 'others'), 'see') (('SHAP positive values', 'probability'), 'be') (('We', 'training data'), 'see') (('benign values', 'it'), 'take') (('Similarly gradient techniques', 'useless most users'), 'use') "}