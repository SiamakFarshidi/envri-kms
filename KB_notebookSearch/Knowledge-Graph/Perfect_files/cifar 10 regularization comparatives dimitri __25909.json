{"name": "cifar 10 regularization comparatives dimitri ", "full_name": " h1 5th Project Image Classification CIFAR 10 h3 Author Arthur Dimitri Brito Oliveira h1 Introduction h1 Dataset Description h1 Objectives h2 Dependencies h1 Loading Data h1 Preprocessing h3 Feature Scaling h1 Neural Network Architecture h3 Theoretical Fundaments h3 Activation Functions h3 z w 1x 1 w 2x 2 w nx n h3 y g z g w 1x 1 w 2x 2 w nx n h4 Relu h4 Softmax h3 Z L w L a L 1 b L h2 a L frac e z L sum j 1 n t i h3 Convolution Layers h2 y n sum n infty infty x k h n k h3 Pooling Layers h2 Flatten Layer h2 Dense Layer h2 Regularization Layers h3 Dropout Layers h1 Training the Neural Network h2 Regularization of Parameters h3 Weight Decay h3 min w b J w b w in R N x b in R h2 J w b sum i 1 N x L hat y i y i frac lambda 2m w 2 h2 dw L from back prop frac lambda m w L h2 w L w L alpha dw L h3 Batch Normalization h2 Z XW h2 Z Z sum i 1 m Z i h2 hat Z frac Z sqrt epsilon frac 1 m sum i 1 m Z 2 h2 a hat Z beta h1 Metrics For Evaluating the Classifier h2 Confusion Matrix h2 Metrics Precision Recall and F1 score h3 Precision h2 Precision frac TP TP FP h3 Recall h2 Recall frac TP TP FN h3 F1 score h2 F1 2 frac Precision Recall Precision Recall h2 Baseline Model h2 Baseline Model Regularization Techniques h1 More Complex Model Regularization Techniques h3 ImageDataGenerator width shift range 0 1 height shift range 0 1 horizontal flip True zoom range 0 2 rotation range 10 h1 Deeper Network h1 Best Model Loading h1 Confusion Matrix h1 Classification Metrics h1 Save Best Model h1 Conclusions h4 1 Overfitting was adressed but underfitting came out Our model seems to be stuck on 90 on both training and validation sets Analyzing the model we suppose that it is sufficiently complex to solve the problem Nevertheless we applied agressive dropout rates at the main convolution layers this is the layers with more neurons Reading some approaches in the original paper that introduced the dropout layers by Hinton 2012 a dropout layer with k prop 0 5 is used at the end of the Dense layer It was not applied on convolutional layers A more recent research by Park 2017 points that it is useful for the network combining other regularization techniques with dropouts after convolutional layers obtaining aproximately 8 of error on the validation set For further improvements we plan to test more variations of the network with lower dropout rates at hidden units and other tests following Hinton 2012 approach to compare them h4 2 In most of the cases it seems that the model shows a trending of improvement If they could be tested for more epochs until convergence using early stoppings as the main regularizer there might be advantages for the model accuracy on unseen data h4 3 The long time taken to converge might be related to few hyperparameter tunning Since we found Nadam as the best choice to solve the problem we could experimentally find an optimal learning rate by doing more experiments We also could fine tune the weight decay regularization We just used one type of the possible kernel regularizers and we set it with the default value so it might be non optimal for our problem h4 4 By analyzing our confusion matrix we see that we need to improve classification of the 3rd and 5th classes We plan to analyze better these classes on the dataset and maybe engineer some features to help the classifier identify more patterns on these classes Apllying more advanced preprocessing techniques could also help the training such as making some patterns stand out more h4 5 The main objective here was developing a neural network from scratch It means that it would probably be not really optimized For a better model transfer learning could also be applied There are many architectures consolidated for solving computer vision problem such as VGG16 Resnet etc with way more complex chained layers Since the first layers capture generic patterns from images we could train just the final layers and get a more accurate network h4 6 For aditional implementation we also recommend other techniques for scaling the image such as pixel scaling For hyperparameter tunning we plan to use GridSearchCV from sklearn to automate some parameter comparison and find the most promising model to begin the experiments h1 References h3 1 Aur\u00e9lien G\u00e9ron Hands on Machine Learning with Scikit Learn and TensorFlow 2017 O Reilly h3 2 http deeplearningbook com br h3 3 https blog exsilio com all accuracy precision recall f1 score interpretation of performance h3 4 https www deeplearning ai tensorflow in practice h3 5 https machinelearningmastery com dropout for regularizing deep neural networks h3 6 https machinelearningmastery com how to reduce overfitting in deep learning with weight regularization h3 7 https medium com thevatsalsaglani multi class image classification using cnn over pytorch and the basics of cnn fdf425a11dc0 h3 8 https stats stackexchange com questions 173663 increasing number of neurons in convolutional net h3 9 https www kaggle com roblexnana cifar10 with cnn for beginer h3 10 https www quora com Why would one use larger strides in convolutional NNs as opposed to smaller strides ", "stargazers_count": 0, "forks_count": 0, "description": "In general it reaches a higher recall and precision for the problematic classes previously mentioned. In order to prevent that without taking out features we can apply what is called weight decay. At them we could adress a more agressive dropout rate. It means that our network fails to generalize satisfactorily. Baseline Model Regularization TechniquesConsidering the overfitting seen in the previous model we could apply some regularization techniques to keep the model learning and generalizing after some epochs of training. It was not applied on convolutional layers. Besides it every layer has a weight decay regularization. With that the behavior of the network is expected to be more generalistic by extracting features that are more useful in general cases and not in a combination isolated example. False Positives FP wrongly predicted element. It also reaches a lower val_loss. The 5th class has a slightly better performance on recall but also lacks in correctly predicting true positives correctly. This combination of techniques might cause a shortage of information to keep the model learning and it might slowe its convergence. This indicates that we should apply some regularization techniques and improve the model. Batch NormalizationTo increase the stability of a neural network batch normalization normalizes the output of a previous activation layer. It means that some neurons are stochastically dropped out of the network with a certain rate. It reduces overfitting because it has a slight regularization effects. For further improvements we plan to test more variations of the network with lower dropout rates at hidden units and other tests following Hinton 2012 approach to compare them. There are 6 000 images of each class. In the case of images the function being slided is called a filter or kernel. Since we found Nadam as the best choice to solve the problem we could experimentally find an optimal learning rate by doing more experiments. Our model has a better f1 score for all of the bad performance classes in the previous report. br 3 https blog. Regularization Layers Dropout LayersHow could we deal with overfitting with a simple approach Dropout is the answer. However there might be some underfitting on the data since there isn t much of improvement from the 100th epoch. A not predicted as A Precision Precision measures the amount of correctly predicted labels in respect to the total positive observations. From here we see that there is some pattern on the diagonal. All the techniques applied seemed to be useful to solve the initial overfitting problem. For hyperparameter tunning we plan to use GridSearchCV from sklearn to automate some parameter comparison and find the most promising model to begin the experiments. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. The worst results can be observed in the classes 3 and 5. For the sake of GPU time remaining we are just showing the results obtained with this approach. The pictures below show the results of previous tests. It seems that a higher value makes the algorithm get lost finding the mininum value on the cost function contour plot. The training and validation accuracy curves sticked to each other demonstrating that the model is making good generalizations. It can be compared to the sigmoid function but in this case it gives probabilities for multiple classes. The efficiency of theses methods was detailed in the previous theoretical sections. In most of the cases it seems that the model shows a trending of improvement. Besides it the model overfits fast. As may be seen in the figures above the behavior on the test set changed substatially. The Deer class is mostly misclassified as 6. ReluFor the negative input values the result is zero that means the neuron does not get activated. Precision frac TP TP FP Recall Recall measures the percentage of elements positively labeled in respect to all the observations in the actual class. As one can see the optimizers had a similar behavior searching for the global minimum of the cost function on the training set. Nevertheless we applied agressive dropout rates at the main convolution layers this is the layers with more neurons. Let s that we have a cost function that we want to minimize min_ w b J w b w in R N_x b in R J w b sum_ i 1 N_x L hat y i y i frac lambda 2m w 2 To update the weights from backpropagation in the layer L we also add a penalty dw L from back prop frac lambda m w L w L w L alpha dw L So that s why it is called weight decay. We apply an activation function to these feature maps and keep convolving them with more and more filters applying pooling techniques to get more sparse representations and catch even more details from the image. Neurons in the first convolutional layer are not connected to every single pixel in the input image but only to pixels in their receptive fields. It is about knowing how many elements predicted as a label really belong to it. It is one of the most widely used datasets for machine learning research. Looking just at the macro average can disguise the poor performance on these classes. Save Best Model ConclusionsWe can conclude that the main objective previously outlied at the begining was reached. com Why would one use larger strides in convolutional NNs as opposed to smaller strides This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. It gives even more sparse representation of the features in the image. Example A not predidcted as B3. We just used one type of the possible kernel regularizers and we set it with the default value so it might be non optimal for our problem. Combining these two techniques may give the model a good capacity of generalization. w_nx_n So the summing junction a linear combination of the inputs is transformed in a non linear output. This indicates that the model is learning a more generalized representation of the problem but it could have its performance increased by applying more severe penalties or adding some dropout layers or having a more complex model. com roblexnana cifar10 with cnn for beginer 10 https www. Normally using a small dropout probability from 20 to 50 is recommended for the hyperparameter testing. If we zoom the Nadam optimizer we can see that it makes the model converge a little faster than the others reaching 90 of accuracy on the training set close to the 10th epoch. If we want to keep the same size after the convolution layer we should apply a zero padding at the edges of the image. Testing the model with three optimizers RMSprop Adam and Nadam with the same learning rate gives some initial results. Analyzing the model we suppose that it is sufficiently complex to solve the problem. 2 in the first layer to 0. z w_1x_1 w_2x_2. Recall frac TP TP FN F1 scoreF1 Score is the weighted average of Precision and Recall. The results are down sampled or pooled feature maps that highlight the most present feature in the patch. A similar analysis is made in A Comparative Analysis of Gradient Descent Based Optimization Algorithms on Convolutional Neural Networks Dogo et. read_csv Input data files are available in the read only. 1 height_shift_range 0. com dropout for regularizing deep neural networks 6 https machinelearningmastery. At the end of this project we want a model that can predict satisfactorily the inputs with a relatively low error. The recall metric reveals that the model isn t having a good performance on classifying the actual elements from its belonging classes. However after 15 epochs the loss starts not to decrease and the validation accuracy starts not to evolve and this could be seen as an overfitting behavior. com thevatsalsaglani multi class image classification using cnn over pytorch and the basics of cnn fdf425a11dc0 8 https stats. Example A predicted as A2. We also could fine tune the weight decay regularization. At some epochs even before the 300th epoch the model reaches 90 but at the end it reaches only 87. This can guide us to apply softer regularization techniques or have a more complex model in the next steps. Since the network is getting deeper we should take care with more agressive regularization techniques batch norm weight decay data augmentation and dropout to prevent the model from overfitting. al where they find Nadam the best option among nine optimizers in terms of accuracy and loss values. We ommited here the calling of the function not to have aditional GPU time. It means that it would probably be not really optimized. Training the Neural Network Regularization of Parameters Weight DecayIf our network is too complex deeper there is an overfitting trend. Here we see some of the worst mistakes made by our model. Since the first layers capture generic patterns from images we could train just the final layers and get a more accurate network. For a better model transfer learning could also be applied. y n sum_ n infty infty x k h n k As a simple explanation what a convolution layer does is extract features depending on the filter applied and convert it into a lower dimension mantaining the main features of the original input. This is useful when we are reaching the end of the network before the fully connected layers. This shows that although a high accuracy on test set our model have a good amount of true positives diagonals but it lacks accuracy on some classes. For aditional implementation we also recommend other techniques for scaling the image such as pixel scaling. As one can see an aditional convolutional layer didn t make much of improvement. Seeking to get an even more complex feature detection we added a layer with 128 neurons. The darkest ones on the diagonal reveal the bad behavior on test set fot classes 3 and 5 mostly as mentioned before. Overfitting was adressed but underfitting came out. Reading some approaches in the original paper that introduced the dropout layers by Hinton 2012 a dropout layer with k prop 0. Moreover the model seems to solve the problem pretty well for a build from scratch approach. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session load weights into new model Errors are difference between predicted labels and true labels lists all downloadable files on server. We have a model 90 of accuracy on the validation set that doesn t overfit much. The aim is to take the inputs from the feature analysis and apply weights to classify the initial input. Subtract it from its mean Z Z sum_ i 1 m Z_i 3. w_nx_n Then the output of the neuron taking into consideration a certain level of activity treshold is y g z g w_1x_1 w_2x_2. L1 gives a simples solution and more interpretable and can t deal with complex patterns whilst L2 does the opposite. Our model seems to be stuck on 90 on both training and validation sets. However for further enhancements we list 1. com all accuracy precision recall f1 score interpretation of performance 4 https www. If it is 1 for a 2x2 kernel there will be always an overlap. The final model is loaded from disk. The long time taken to converge might be related to few hyperparameter tunning. Normalize the output of the layer hat Z frac Z sqrt epsilon frac 1 m sum_ i 1 m Z 2 4. It is useful to compare two models and a more powerful metric than simply accuracy. com questions 173663 increasing number of neurons in convolutional net 9 https www. Most of the efforts should be spent on improving classification for animal labels specially Cats Dogs and Deers. The layers that have most of the parameters should be the ones that we are concerned about overfitting the model. This makes us discard the hypothesis of having a model that was too simple for the problen. Most squares have cleares white color and it means the classifier predicts correctly most of the labels. Softmax When we want to describe a probability distribution over a discrete variable consisting of n classes the softmax function should be applied. In L1 whe just sum de absolute values of the weights whilst in L2 whe sum the square value of those weights. Although applying the model makes the model slower to train using a higher and controlled dropout gives the model an opportunity to acquire more independent representations of the data making the ability of predicting unseen data more accurate. Classification Metrics Comparing this with the previous model the bigger one we can infer some informations. The difference between L1 and L2 regularizations is only how we sum the penalties. The basic steps of the method are 1. Our recall for the 3rd class is the worst of all meaning that our model is dealing with Cats too much as false negatives in comparison to the true positives. Recall Precision Recall Baseline ModelSince convolutional neural networks are a kind of feedfoward networks specialized in images let s take some generic CNN design pattern as a guideline Input Convolution Pooling Convolution Pooling Fully Conected Layer OutputStructuring this with Keras gives For instance we have 32x32x3 input images and a first convolutional layer with 32 3x3 filters and stride 1 and 32 32x32 feature maps at the output of this layer since we applied a zero padding to keep the original input dimensions. The decay values were set as the default ones. For a layer L the summing junction would be a product of the previous activation with the weights of the layer plus the biases associated with the layer L Z L w L a L 1 b L The activation on the softmax layer is a L frac e z L sum_ j 1 n t_i So it gives a probability for each class taking into consideration all the class elements. The main objective is to know how much of the actual label we ve correctly predicted. More Complex Model Regularization TechniquesGiven that we have achieved a relatively good result with the baseline model around 80 on test set we could test adding one more convolutional layer. It is very popular in convolutional networks because it is proved to cause a faster convergence of the model and it prevents the vanishing gradients problem in the context of backpropagation. The intuition behind it is that it can t rely on any feature so it has to spread out the weights. True Positives TP correctly predicted classes. ai tensorflow in practice 5 https machinelearningmastery. Besides all these evidences of not enhancement our strategies of regularization seem to be sufficient to avoid the overfitting but maybe it was too much affecting the model s learning even in the training set. Here we apply categorical transformation to the class labels Neural Network Architecture Theoretical Fundaments Activation FunctionsThere is a representation of a neuron with its inputs The summing junction is given by the product of the input by a wheight which indicates the influence of the given input to the output. True Negatives TN correctly predicted element from its not original class. 1 horizontal_flip True zoom_range 0. The class of Cats is very misclassified as Dog or Frog. It forces the network to minimize the cost and make the weights have a lower influence on the final function and thus making the decision boundaries more generic and thus prevents overfitting. There are many architectures consolidated for solving computer vision problem such as VGG16 Resnet etc with way more complex chained layers. Dependencies Loading Data Preprocessing Feature ScalingThe main objective with this preprocessing technique is to keep the pixels in a controlled range 0 to 1 making the distribution look like a gaussian centered at zero. Some theoretical fundaments are presented and the steps that led to the final model are detailed. From it one can see how many classes have been misclassified and how good the model behaved for each one Metrics Precision Recall and F1 scoreFirst we need to define some terms 1. br IntroductionThis work describes the process of designing a Convolutional Neural Network for multiclass classification. From these plots we have some insights to improve our classifier. Wikipedia Objectives We aim to load this dataset apply one hot encoding to the labels and normalize the image pixels before we dive into the machine learning pipeline. Pooling LayersMaximum pooling or max pooling calculates the maximum value in each window sliding over a feature map. If they could be tested for more epochs until convergence using early stoppings as the main regularizer there might be advantages for the model accuracy on unseen data. The way the penalties are applied make these two methods distinct in practice. There s a plateau around 90 of accuracy on the test set. Multiply the normalized output by some arbitrary parameter and add some random parameter too a hat Z beta In practice what it does is make the cost function contour plot more simetric making the path for the gradient descend based learning more uniform. As can be seen the performance on the test set was dramatically improved. Probably our model is good at catching basic visual patterns for theses species but fails on detecting more specific ones. Consulting some discussions on Stack Exchange we find that adding dropout layers after each convolutional layer isn t a recommended strategy. Each filter applied to the input will generate what is called as feature map that gives a representation of the input in respect to one aspect. F1 2 frac Precision. This motivated an addition of a new convolutional layer with the double number of neurons aiming to catch more patterns and train for more epochs applying the same data augmentation procedures and a higher dropout rate. Convolution Layers A convolution is a mathematical operation that slides one function over another and measures the integral or the area behind the curve oftheir pointwise multiplication. The last dense layer gets these operations and by applying an activation function gives the probabilities for each class. The model seems to have controlled the overfitting behavior and we see a trend of improvement on the accuracy value on the test set. L1 is robust dealing with outliers while L2 have trouble with that. The figure shows a plateau on the validation set for all of the optimizers and an increasing loss value. The loss curve also behaves better getting lower at each subsequent epoch. The main objective here was developing a neural network from scratch. Normalizing causes a faster convergence of the model. Dense LayerConsists in a linear operation between the weights in which all the inputs are connected to the ouput. Some changes on the hyperparameters were made such as increasing learning rate but a high overshoot was noticed on the loss value. When it comes to the confusion matrix some classes seem to be very misclassified. 5th Project Image Classification CIFAR 10 Author Arthur Dimitri Brito Oliveira arthur. Flatten LayerFlattening means to reshape the previous matrix into one dimension. In turn each neuron in the second convolutional layer is connectedonly to neurons located within a small rectangle in the first layer. False Negatives FN when an element is not classified as its belonging class. Apllying more advanced preprocessing techniques could also help the training such as making some patterns stand out more. 1 2 The CIFAR 10 dataset contains 60 000 32x32 color images in 10 different classes. 5 is used at the end of the Dense layer. By analyzing our confusion matrix we see that we need to improve classification of the 3rd and 5th classes. References 1 Aur\u00e9lien G\u00e9ron Hands on Machine Learning with Scikit Learn and TensorFlow 2017 O Reilly 2 http deeplearningbook. We hope that it reduces overfitting. This is very useful since it adds non linear properties to the inputs. So a flatten layer in keras reshapes the tensor into a 1xN matrix with N being the number of elements in that tensor. Best Model Loading Confusion MatrixAs can be seen specially for the classes 3 cat and 5 dog there are a lot of false positives. The augmentation procedures were made the same way as before. Besides it we want a model that manages addressing the overfitting underfitting trade off. At the end we add two dense layers to reshape the tensors and apply some activation functions to predict the labels. From now on we are going to use Nadam as our optimizer. Deeper Network Since it took many hours to run the training we are just showing the architecture and the results obtained. In the cell below there is the model that we used. Some dark gray squares outside the main diagonal show the classes that were misclassified. 2 rotation_range 10 It is a common pattern on deep learning CNNs to increase the dropout rate for large layers and reduce it for small ones Hands on Machine Learning with Scikit Learn and TensorFlow 2017 O Reilly. Notice that due to time limitation in respect to the Kaggle GPU we are just showing pictures of the loss and accuracy curves. A very low probability doesn t have a impactful effect on the model results. 3 The 10 different classes represent airplanes cars birds cats deer dogs frogs horses ships and trucks. com how to reduce overfitting in deep learning with weight regularization 7 https medium. So we tried a pattern of increasing from 0. Here we are using a previous explained Batch Norm technique after the activation of each convolutional layer and another penalization l1_l2 which is a combination of the two classical weight decay techniques at each layer. This principle is show above after the first convolutional layer What controls the filter sliding and hence the output representation of the image is a parameter called stride. Compute the previous layer inputs operating with biases Z XW 2. These dropout rates are common in these kind of CNN problems and we are being more agressive adressing overfitting aiming to have enough hypothesis to trade off between overfitting and underfitting. We plan to analyze better these classes on the dataset and maybe engineer some features to help the classifier identify more patterns on these classes. Metrics For Evaluating the Classifier Confusion MatrixIt shows visually how many labels were predicted correctly. A more recent research by Park 2017 points that it is useful for the network combining other regularization techniques with dropouts after convolutional layers obtaining aproximately 8 of error on the validation set. Dataset Description The CIFAR 10 dataset Canadian Institute For Advanced Research is a collection of images that are commonly used to train machine learning and computer vision algorithms. For most part of the training the validation has higher accuracy rates even applying shuffling techniques on both datasets. For data augmentation we used a keras preprocessing function applying shift ranges in both dimensions and image mirroring ImageDataGenerator width_shift_range 0. ", "id": "arthurdimitri/cifar-10-regularization-comparatives-dimitri", "size": "25909", "language": "python", "html_url": "https://www.kaggle.com/code/arthurdimitri/cifar-10-regularization-comparatives-dimitri", "git_url": "https://www.kaggle.com/code/arthurdimitri/cifar-10-regularization-comparatives-dimitri", "script": "keras.datasets Counter classification_report keras.layers Activation keras.models IPython.display cifar10 keras confusion_matrix keras.preprocessing.image numpy Image MaxPooling2D seaborn Adam Dropout Adadelta ImageDataGenerator BatchNormalization Dense Callback regularizers on_epoch_end backend as K keras.callbacks MyThresholdCallback(tf.keras.callbacks.Callback) RMSprop model_from_json keras.optimizers tensorflow matplotlib.pyplot Sequential backend FileLinks pandas np_utils Conv2D FileLink CnnModel Adamax keras.utils __init__ sklearn.metrics Flatten collections Nadam ", "entities": "(('that', 'k prop'), 'read') (('high overshoot', 'loss value'), 'make') (('Project Image 5th Classification', 'Author Arthur Dimitri Brito Oliveira 10 arthur'), 'CIFAR') (('that', 'patch'), 'be') (('we', 'more accurate network'), 'capture') (('efficiency', 'previous theoretical sections'), 'detail') (('Precision frac TP TP FP Recall Recall', 'actual class'), 'measure') (('they', 'unseen data'), 'be') (('we', 'optimizer'), 'go') (('which', 'output'), 'apply') (('we', 'classifier'), 'have') (('it', 'multiple classes'), 'compare') (('neurons', 'certain rate'), 'mean') (('Looking', 'classes'), 'disguise') (('Most', 'Cats specially Dogs'), 'spend') (('it', 'regularization slight effects'), 'reduce') (('model', 'good generalizations'), 'stick') (('we', 'them'), 'plan') (('aim', 'initial input'), 'be') (('specially classes', '5 false positives'), 'see') (('Dropout', 'simple approach'), 'LayersHow') (('Nadam', 'accuracy'), 'al') (('t', 'server'), 'list') (('linear combination', 'non linear output'), 'w_nx_n') (('5th class', 'also correctly true positives'), 'have') (('model', 'unseen data'), 'give') (('that', 'one aspect'), 'generate') (('It', 'more powerful simply accuracy'), 'be') (('algorithm', 'cost function contour plot'), 'seem') (('more filters', 'image'), 'apply') (('we', 'image'), 'apply') (('L2', 'that'), 'deal') (('Recall frac TP TP FN scoreF1 Score', 'weighted Precision'), 'f1') (('convolutional layers', 'validation set'), 'research') (('we', 'input original dimensions'), 'be') (('bad behavior', '3'), 'one') (('L1', 'opposite'), 'give') (('Batch NormalizationTo', 'activation previous layer'), 'increase') (('we', 'that'), 'be') (('performance', 'more complex model'), 'indicate') (('We', 'weight decay also fine regularization'), 'tune') (('deer dogs frogs', 'ships'), 'represent') (('Deer class', 'mostly 6'), 'misclassifie') (('we', 'loss curves'), 'notice') (('what', 'features'), 'in') (('we', 'training'), 'Regularization') (('network', 'Parameters Weight DecayIf'), 'train') (('recall metric', 'belonging classes'), 'reveal') (('read_csv Input data files', 'read'), 'be') (('we', 'overfitting'), 'take') (('It', 'convolutional layers'), 'apply') (('Testing', 'initial results'), 'give') (('visually how many labels', 'Classifier Confusion MatrixIt'), 'show') (('It', 'machine learning research'), 'be') (('We', 'GPU aditional time'), 'ommite') (('it', 'weights'), 'be') (('What', 'output hence image'), 'show') (('it', 'non problem'), 'use') (('it', '2x2 kernel'), 'be') (('neuron', 'first layer'), 'be') (('Negatives False when element', 'belonging class'), 'fn') (('validation', 'datasets'), 'have') (('loss curve', 'also better subsequent epoch'), 'behave') (('it', 'backpropagation'), 'be') (('performance', 'test set'), 'improve') (('softmax function', 'n classes'), 'apply') (('figure', 'optimizers'), 'show') (('we', 'actual label'), 'be') (('taking', 'activity'), 'w_nx_n') (('Combining', 'generalization'), 'give') (('Pooling LayersMaximum pooling', 'feature map'), 'calculate') (('Here we', 'model'), 'see') (('maybe it', 'training even set'), 'seem') (('layer didn aditional convolutional t', 'improvement'), 'make') (('So we', '0'), 'try') (('just sum de absolute values', 'weights'), 'sum') (('general it', 'problematic classes'), 'reach') (('this', 'more neurons'), 'be') (('we', 'pixel such scaling'), 'recommend') (('It', 'image'), 'give') (('inputs', 'ouput'), 'LayerConsists') (('only how we', 'penalties'), 'be') (('we', 'machine learning pipeline'), 'aim') (('which', 'layer'), 'use') (('how many elements', 'really it'), 'be') (('we', 'labels'), 'add') (('mathematical that', 'curve pointwise oftheir multiplication'), 'Layers') (('This', 'next steps'), 'guide') (('long time', 'hyperparameter few tunning'), 'relate') (('we', '1'), 'list') (('br IntroductionThis work', 'multiclass classification'), 'describe') (('classifier', 'classes'), 'plan') (('why it', 'L'), 'let') (('we', 'one more convolutional layer'), 'TechniquesGiven') (('Moreover model', 'scratch approach'), 'seem') (('results', 'just architecture'), 'Network') (('previous layer', 'biases'), 'compute') (('we', 'approach'), 'show') (('main objective', 'previously begining'), 'conclude') (('model', 'close 10th epoch'), 'see') (('that', 'machine commonly learning'), 'description') (('we', 'model'), 'be') (('It', 'kaggle python Docker image https github'), 'com') (('Positives True TP', 'correctly classes'), 'predict') (('This', 'data augmentation same procedures'), 'motivate') (('we', 'overfitting'), 'be') (('we', 'experiments'), 'plan') (('class', 'very Dog'), 'be') (('class', 'class elements'), 'be') (('model', 'training sets'), 'seem') (('we', '3rd classes'), 'see') (('it', 'classes'), 'show') (('decision thus boundaries', 'more thus overfitting'), 'force') (('model', 'true positives'), 'be') (('optimizers', 'training set'), 'see') (('decay values', 'default ones'), 'set') (('classifier', 'labels'), 'cleare') (('probability very low doesn', 'model results'), 'have') (('this', 'overfitting behavior'), 'start') (('we', 'informations'), 'compare') (('pictures', 'previous tests'), 'show') (('doesn', 'validation'), 'have') (('when we', 'fully connected layers'), 'be') (('Positives False FP', 'wrongly element'), 'predict') (('worst results', 'classes'), 'observe') (('hat Z', 'epsilon'), 'normalize') (('it', 'convergence'), 'cause') (('that', 'relatively low error'), 'want') (('Normalizing', 'model'), 'cause') (('techniques', 'overfitting initial problem'), 'seem') (('last dense layer', 'class'), 'get') (('model', 'improvement'), 'seem') (('N', 'tensor'), 'reshape') (('we', 'test set'), 'seem') (('we', 'dropout more agressive rate'), 'adress') (('very it', 'inputs'), 'be') (('main objective', 'scratch'), 'develop') (('Neurons', 'only receptive fields'), 'connect') (('we', 'ImageDataGenerator width_shift_range'), 'use') (('that', 'final model'), 'present') (('layer', 'weight decay regularization'), 'have') (('two methods', 'practice'), 'make') (('such patterns', 'also training'), 'help') (('we', 'diagonal'), 'see') (('Normally using', 'hyperparameter testing'), 'recommend') (('more path', 'more uniform'), 'multiply') (('2 10 It', 'Scikit Learn'), 'rotation_range') (('that', 'too problen'), 'make') (('that', 'dark gray main diagonal show'), 'square') (('similar analysis', 'et'), 'make') (('we', 'isn recommended strategy'), 'find') (('function', 'images'), 'call') (('we', 'more experiments'), 'find') (('Probably model', 'more specific ones'), 'be') (('it', 'sufficiently problem'), 'analyze') (('Flatten LayerFlattening', 'one dimension'), 'mean') (('that', 'example'), 'expect') (('we', '128 neurons'), 'add') (('model', 'previous report'), 'have') (('neuron', 'result'), 'be') (('1 10 dataset', '10 different classes'), '2') (('classes', 'confusion matrix'), 'seem') (('Negatives True TN', 'original class'), 'predict') (('scoreFirst we', 'terms'), 'see') (('we', 'model'), 'indicate') (('distribution', 'zero'), 'Data') (('augmentation procedures', 'same way'), 'make') (('that', 'overfitting underfitting trade'), 'want') (('it', 'only 87'), 'reach') (('convolution layer', 'original input'), 'infty') "}