{"name": "wheat detection finalnb ", "full_name": " h1 Check data anamolies h1 Validating images in the directory h1 Make a label to classify data based on availability h1 Applying Faster RCNN Credit to the training code goes to RockyXu66 Git Repository h1 Model Inspection Graphs h1 Submission Code h2 All are JPGs h1 Couldn t complete from here ", "stargazers_count": 0, "forks_count": 0, "description": "groupby image_id width. format mean_overlapping_bboxes epoch_length Generate X x_img and label Y y_rpn_cls y_rpn_regr Train rpn model and get loss value _ loss_rpn_cls loss_rpn_regr Get predicted rpn from rpn model rpn_cls rpn_regr R bboxes shape 300 4 Convert rpn layer to roi bboxes note calc_iou converts from x1 y1 x2 y2 to x y w h format X2 bboxes that iou C. tolist classes_count np. npz allow_pickle True arr_0. Create the config This step will spend some time to load the data train_imgs np. shape 18 25 Y. Don t need rpn probs in the later process img cv2. arange 0 r_epochs record_df loss_class_regr c plt. shape 18 25 Calculate anchor position and size for each feature map point Top left x coordinate Top left y coordinate width of current anchor height of current anchor Apply regression to x y w and h if there is rpn regression layer Avoid width and height exceeding 1 Convert x y w h to x1 y1 x2 y2 x1 y1 is top left coordinate x2 y2 is bottom right coordinate Avoid bboxes drawn outside the feature map shape 4050 4 shape 4050 Find out the bboxes which is illegal and delete them from bboxes list Apply non_max_suppression Only extract the bboxes. com 9 this is a model that holds both the RPN and the classifier used to load save weights for the models Because the google colab can only run the session several hours one time then you need to connect again we need to save the model and load the model to continue training If this is the begin of the training load the pre traind base network such as vgg 16 Create the record. npz allow_pickle True arr_0 class_mapping np. arange 0 r_epochs record_df curr_loss r plt. Check data anamolies Validating images in the directory Make a label to classify data based on availability. Config setting Model Inspection Graphs Submission Code All are JPGs Couldn t complete from here. classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes Y1 one hot code for bboxes from above x_roi X Y2 corresponding labels and corresponding gt bboxes If X2 is None means there are no matching bboxes Find out the positive anchors and negative anchors If number of positive anchors is larger than 4 2 2 randomly choose 2 pos samples Randomly choose num_rois num_pos neg samples Save all the pos and neg samples in sel_samples in the extreme case where num_rois 1 we pick a random pos or neg sample training_data X X2 sel_samples labels Y1 sel_samples Y2 sel_samples X img_data resized image X2 sel_samples num_rois 4 in here bboxes which contains selected neg and pos Y1 sel_samples one hot encode for num_rois bboxes which contains selected neg and pos Y2 sel_samples labels and gt bboxes for num_rois bboxes which contains selected neg and pos plt. figure figsize 15 5 plt. arange 0 r_epochs record_df loss_class_cls r plt. savefig main_title Print the process or not Name of base network Setting for data augmentation Anchor box scales Note that if im_size is smaller anchor_box_scales should be scaled Original anchor_box_scales in the paper is 128 256 512 Anchor box ratios Size to resize the smallest side of the image Original setting in paper is 600. title total_loss plt. imread row image_id sys. format cls pos_cls 0 0 pos_cls 1 0 print y_rpn_regr for positive anchor. Set to 300 in here to save training time image channel wise mean to subtract number of ROIs at once stride at the RPN this depends on the network configuration scaling the stdev overlaps for RPN overlaps for classifier ROIs placeholder for the class mapping automatically generated by the parser x 0 is image with shape rows cols channels x 1 is roi with shape num_rois 4 with ordering x y w h Resized roi of the image to pooling size 7x7 Reshape to 1 num_rois pool_size pool_size nb_channels Might be 1 4 7 7 3 permute_dimensions is similar to transpose print Parsing annotation files Print process Make sure the info saved in annotation file matching the format path_filename x1 y1 x2 y2 class_name Note tOne path_filename might has several classes class_name tx1 y1 x2 y2 are the pixel value of the origial image not the ratio value t x1 y1 top left coordinates x2 y2 bottom right coordinates x1 y1 t t t t t t t t t t t t t t t t t t t t t t t t t t t t t x2 y2 if np. putText img gt bbox gt_x1 gt_y1 5 cv2. read_csv Input data files are available in the read only. Applying Faster RCNN Credit to the training code goes to RockyXu66 Git Repository. sort_values by count ascending False. csv file to record losses acc and mAP If this is a continued training load the trained model from before Load the records Training setting Just of sharing the kernel running with 2 epoch you try with min 20 epochs print Average number of overlapping bounding boxes from RPN for previous iterations. subplot 1 2 2 plt. title loss plt. 5 ya downscale iy 0. flush img cv2. arange 0 r_epochs record_df loss_rpn_cls b plt. shape 4 feature_map. width num_anchors Might be 4 18 25 18 if resized image is 400 width and 300 A is the coordinates for 9 anchors for every point in the feature map all 18x25x9 4050 anchors cooridnates anchor_x 128 1 16 8 width of current anchor anchor_y 128 2 16 16 height of current anchor curr_layer 0 8 9 anchors the Kth anchor of all position in the feature map 9th in total shape 18 25 4 shape 4 18 25 Create 18x25 mesh grid For every point in x there are all the y points and vice versa X. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. format regr pos_regr 0 0 pos_regr 1 0 cv2. arange 0 r_epochs record_df elapsed_time r plt. putText img pos anchor bbox str i 1 center 0 int anc_w 2 center 1 int anc_h 2 5 cv2. rename columns width count. randint 0 6 0 tall_imgs filename imageset trainval else tall_imgs filename imageset test make sure the bg class is last in the list Block 1 Block 2 Block 3 Block 4 Block 5 x MaxPooling2D 2 2 strides 2 2 name block5_pool x out_roi_pool. flush Augment with horizontal flips in training. classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes one hot code for bboxes from above x_roi X corresponding labels and corresponding gt bboxes 3 in here 3 in here A. Augment with 90 degree rotations in training. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session pip install upgrade tensorflow df_train. to_csv modified_train. 3 and RGB x is the difference between true value and predicted vaue absolute value of x If x_abs C. shape 1 num_rois channels pool_size pool_size num_rois 4 7x7 roi pooling Flatten the convlutional layer and connected to 2 FC and 2 dropout There are two output layer out_class softmax acivation function for classify the class name of the object out_regr linear activation function for bboxes coordinates regression note no regression target for bg class a and b should be x1 y1 x2 y2 128 256 512 1 1 1 2 sqrt 2 2 sqrt 2 1 3x3 9 calculate the output map size based on the network architecture 3 initialise empty output objectives get the GT box coordinates and resize to account for image resizing get the GT box coordinates and resize to account for image resizing rpn ground truth x coordinates of the current anchor box t ignore boxes that go across image boundaries t t t t t y coordinates of the current anchor box ignore boxes that go across image boundaries bbox_type indicates whether an anchor should be a target Initialize with negative this is the best IOU for the x y coord and the current anchor note that this is different from the best IOU for a GT bbox get IOU of the current GT box and the current anchor box calculate the regression targets if they will be needed x y are the center point of ground truth bbox xa ya are the center point of anchor bbox xa downscale ix 0. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. subplot 1 2 1 plt. Augment with vertical flips in training. FONT_HERSHEY_DUPLEX 0. 5 color 1 define the base network VGG here can be Resnet50 Inception etc rm r github. csv index False plt. npz allow_pickle True arr_0 e. arange 0 r_epochs record_df loss_rpn_regr g plt. title elapsed_time plt. show a and b should be x1 y1 x2 y2. classes_count Car 2383 Mobile phone 1108 Person 3745 bg 0 class_mapping Person 0 Car 1 Mobile phone 2 bg 3 Save the configuration Shuffle the images with seed print y_rpn_cls for possible pos anchor. write str idx r sys. 5 w h are the width and height of ground truth bbox wa ha are the width and height of anchor bboxe tx x xa wa ty y ya ha tw log w wa th log h ha all GT boxes should be mapped to an anchor box so we keep track of which anchor box was best we set the anchor to positive if the IOU is 0. 7 it does not matter if there was another better box it just indicates overlap we update the regression layer target if this IOU is the best for the current x y and anchor position if the IOU is 0. reset_index drop True. 7 color 1 Add text Draw positive anchors according to the y_rpn_regr print Center position of positive anchor center cv2. ", "id": "hpant438/wheat-detection-finalnb", "size": "344", "language": "python", "html_url": "https://www.kaggle.com/code/hpant438/wheat-detection-finalnb", "git_url": "https://www.kaggle.com/code/hpant438/wheat-detection-finalnb", "script": "keras.layers pyplot non_max_suppression_fast calc_iou TimeDistributed SGD rpn_loss_cls_fixed_num get_file regularizers categorical_crossentropy layer_utils apply_regr draw_rect Layer * K.mean(categorical_crossentropy(y_true[0 matplotlib.pyplot rpn_layer InputSpec Model X2[ apply_regr_np classifier_layer scipy.stats class_loss_regr rpn_loss_regr_fixed_num collections rpn_loss_regr * K.sum(y_true[ keras.models keras augment build initializers MaxPooling2D Adam Dropout nn_base GlobalAveragePooling2D rpn_loss_cls RMSprop rpn_to_roi tensorflow plot_img check_image_size pandas call average_precision_score = model_classifier.train_on_batch([X matplotlib InteractiveShell keras.engine math OptionParser draw_random_image_and_box intersection compute_output_shape union numpy keras.objectives get_output_length keras.engine.topology Config get_img_output_length keras.optimizers GlobalMaxPooling2D get_source_inputs generic_utils class_loss_regr_fixed_num Conv2D tqdm.notebook IPython.core.interactiveshell get_config sklearn.metrics Flatten get_anchor_gt pandas.util.testing Counter keras.utils.data_utils pyplot as plt class_loss_cls iou RoiPoolingConv(Layer) seaborn Dense optparse backend as K tqdm get_new_img_size ceil backend pprint Input data_preperation keras.utils get_data __init__ calc_rpn ", "entities": "(('Original setting', 'paper'), 'Print') (('ratio t x1 y1 top', 'x2 y2 x1 y1 t t t t t t t t t t t t t t t t t t t t t t t t t t t t t x2 left bottom right y2'), 'Set') (('Don t', 'later process'), 'need') (('read_csv Input data files', 'read'), 'be') (('It', 'kaggle python Docker image https github'), 'come') (('Config', 'Model Inspection Graphs Submission Code'), 'be') (('that', 'C.'), 'Generate') (('which', 'Only bboxes'), 'shape') (('which', 'selected neg'), 'y1') (('Applying', 'RockyXu66 Git Repository'), 'go') (('1 16 8 width', 'x'), 'be') (('y', 'anchor bbox center xa'), 'channel') (('classes_count Mobile 2383 phone', 'pos possible anchor'), 'Car') (('Check data', 'availability'), 'anamolie') (('you', 'previous iterations'), 'file') (('bg class', 'list'), 'imageset') (('base pre network', '16 record'), 'com') (('5 color 1 define', 'Inception rm r here github'), 'be') (('IOU', 'anchor'), 'be') (('IOU', 'current x y position'), '7') (('3', 'x'), 'be') (('step', 'data train_imgs np'), 'spend') (('t', 'pip df_train'), 'list') "}