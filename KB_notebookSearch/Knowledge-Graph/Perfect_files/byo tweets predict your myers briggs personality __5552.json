{"name": "byo tweets predict your myers briggs personality ", "full_name": " h1 BYO Tweets and predict your Myers Briggs Personality Type h3 Outline h2 Data preview h3 List of posts h2 Distribution of the MBTI personality types h4 unbalanced occurrences h3 Add columns for the type Indicators h3 Pearson Features Correlation h3 Prep data h3 Preprocessing posts h3 Vectorize with count and tf idf h2 Train XGBoost classifiers h3 X Y data h2 First XGBoost model for MBTI dataset h2 Monitor Performance and Early Stopping h2 Feature Importance with XGBoost h4 Show feature importance plot and list for the first indicator h2 How to Configure Gradient Boosting h2 XGBoost Hyperparameter Tuning h2 Predict own Myers Briggs Personality Type h4 Prep data h4 Fit and predict the 4 type indicators h4 Show result ", "stargazers_count": 0, "forks_count": 0, "description": "Train XGBoost classifiersPracticing XGBoost With Python Mini Course by jason machinelearningmastery. BYO Tweets and predict your Myers Briggs Personality Type Why yet another kernel that explores the Myers Briggs Personality Type Dataset Exploring use of NLTK and XGBoost. We want to remove these from the psosts Lemmatize Cache the stop words for speed Posts to a matrix of token counts Learn the vocabulary dictionary and return term document matrix Transform the count matrix to a normalized tf or tf idf representation Learn the idf vector fit and transform a count matrix to a tf idf representation First XGBoost model for MBTI dataset Posts in tf idf representation Let s train type indicator individually Let s train type indicator individually split data into train and test sets fit model on training data make predictions for test data evaluate predictions Let s train type indicator individually split data into train and test sets fit model on training data make predictions for test data evaluate predictions Only the 1st indicator fit model on training data plot feature importance Save xgb_params for late discussuin Save xgb_params for later discussuin setup parameters for xgboost Let s train type indicator individually split data into train and test sets fit model on training data make predictions for test data evaluate predictions Tune learning_rate Posts in tf idf representation setup parameters for xgboost Let s train type indicator individually learning_rate 0. Show result Prep data Fit and predict the 4 type indicators Show resultWow the result is very very close to the real Myers Briggs assessment that I did a few years back. com xgboost python mini course https machinelearningmastery. 3 max_depth 2 3 4 summarize results A few few tweets and blog post The type is just a dummy so that the data prep fucntion can be reused setup parameters for xgboost Let s train type indicator individually split data into train and test sets fit model on training data make predictions for my data print s prediction s type_indicators l y_pred. com blog 2016 03 complete guide parameter tuning xgboost with codes python XGBoost Hyperparameter Tuning The scikit learn framework provides the capability to search combinations of parameters. Prep dataBinarize Type Indicator better implemenation than mine above Preprocessing posts Remove urls Keep only words and put everything lowercase Lemmatize each word __Remove MBTI profiles strings. Too many appear in the posts __ Vectorize with count and tf idfKeep words appearing in 10 to 70 of the posts. com xgboost python mini course X Y data X Posts in tf idf representation Y Binarized MBTI Note here I trying out to build a model for each type indicator individually because I recall the multi class multi out models in other notebooks weren t that accurate right First XGBoost model for MBTI dataset Monitor Performance and Early Stopping XGBoost model can evaluate and report on the performance on a test set for the model during training. Fit and predict the 4 type indicators3. The depth of trees tree_depth in XGBoost should be configured in the range of 2 to 8 where not much benefit is seen with deeper trees. This capability is provided in the GridSearchCV class and can be used to discover the best way to configure the model for top performance on your problem. 3 param_grid dict learning_rate learning_rate learning_rate 0. Any guess plotting read data the x locations for the groups the width of the bars can also be len x sequence transform mbti to binary vector transform binary vector to mbti personality Check. Predict own Myers Briggs Personality Type base on a few tweets and blog post. Data preview distribution and correlation. Data preparation process posts vectorize with count and tf idf. Apply steps of the XGBoost With Python Mini Course https machinelearningmastery. My playground Utilizing the Kaggle Python Docker Container image https github. For example we can report on the binary classification error rate error on a standalone test set eval_set while training an XGBoost model. com xgboost python mini course Feature Importance with XGBoost A benefit of using ensembles of decision tree methods like gradient boosting is that they can automatically provide estimates of feature importance from a trained predictive model. Practicing with the learnings from XGBoost With Python Mini Course https machinelearningmastery. Trying out to build a model for each type indicator individually. They can be summarized as Learning rate or shrinkage learning_rate in XGBoost should be set to 0. 1 https machinelearningmastery. com depture multiclass and multi output classification RNN mbti predictor https www. The parameters to consider tuning are The number and size of trees n_estimators and max_depth. it en introduction to gradient boosted trees and xgboost hyperparameters tuning with python Predict own Myers Briggs Personality Type Using a few tweets and blog post let s try to predict my own Myers Briggs Personality Type. com prnvk05 rnn mbti predictor and MBTI Study personality https www. apprendimentoautomatico. com xgboost python mini course Show feature importance plot and list for the first indicator How to Configure Gradient Boosting A number of configuration heuristics were published in the original gradient boosting papers. com laowingkin mbti study personality. It supports this capability by specifying both a test dataset and an evaluation metric on the call to model. Data preview List of posts Distribution of the MBTI personality types. Only one indicator is different. Row sampling subsample in XGBoost should be configured in the range of 30 to 80 of the training dataset and compared to a value of 100 for no sampling. A trained XGBoost model automatically calculates feature importance on your predictive modeling problem. The learning rate and number of trees learning_rate and n_estimators. com stefan bergstein Utilizing the Kaggle Python Docker Container image This notebook is a fork from Multiclass and multi output classification https www. 1 or lower and smaller values will require the addition of more trees. And finally predicting my own Myers Briggs Personality Type. The row and column subsampling rates subsample colsample_bytree and colsample_bylevel. com 1 https machinelearningmastery. com xgboost python mini course. Add columns for the type Indicators Pearson Features CorrelationUnclear if the matrix shows anything valuable for interpretation. unbalanced occurrences. com xgboost python mini course See also Complete Guide to Parameter Tuning in XGBoost https www. fit when training the model and specifying verbose output verbose True. com xgboost python mini course See also Introduction to gradient boosted trees and XGBoost hyperparameters tuning https www. ", "id": "stefanbergstein/byo-tweets-predict-your-myers-briggs-personality", "size": "5552", "language": "python", "html_url": "https://www.kaggle.com/code/stefanbergstein/byo-tweets-predict-your-myers-briggs-personality", "git_url": "https://www.kaggle.com/code/stefanbergstein/byo-tweets-predict-your-myers-briggs-personality", "script": "TfidfTransformer get_types train_test_split plot_importance xgboost accuracy_score CountVectorizer numpy XGBClassifier seaborn sklearn.manifold nltk.stem translate_back sklearn.feature_extraction.text StratifiedKFold matplotlib.pyplot nltk PorterStemmer sklearn.model_selection pandas WordNetLemmatizer stopwords translate_personality nltk.corpus loadtxt word_tokenize pre_process_data TSNE GridSearchCV sklearn.metrics ", "entities": "(('XGBoost trained model', 'modeling predictive problem'), 'calculate') (('where much benefit', 'deeper trees'), 'configure') (('number', 'gradient boosting original papers'), 'plot') (('Data preparation process posts', 'count'), 'vectorize') (('width', 'personality Check'), 'be') (('notebook', 'output classification https Multiclass www'), 'bergstein') (('First XGBoost accurate right model', 'training'), 'course') (('fit', 'True'), 'verbose') (('predictions', 'data prediction'), 'result') (('Too many', 'posts'), 'appear') (('I', 'Myers Briggs very very real assessment'), 'result') (('blog s', 'Myers Briggs Personality own Type'), 'en') (('framework', 'parameters'), 'blog') (('we', 'XGBoost model'), 'report') (('_ _ Remove MBTI', 'strings'), 'implemenation') (('com xgboost mini python course', 'XGBoost https www'), 'see') (('they', 'trained predictive model'), 'be') (('parameters', 'trees'), 'be') (('Indicators Pearson Features matrix', 'valuable interpretation'), 'column') (('capability', 'problem'), 'provide') (('Why yet that', 'NLTK'), 'Tweets') (('1 values', 'more trees'), 'require') (('xgboost train type indicator', 'representation setup tf idf parameters'), 'want') (('subsampling row rates', 'colsample_bytree'), 'subsample') (('It', 'metric call'), 'support') (('Row sampling subsample', 'sampling'), 'configure') (('Learning rate', '0'), 'summarize') (('also Introduction', 'XGBoost https www'), 'see') "}