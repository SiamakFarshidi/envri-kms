{"name": "gan introduction explained ", "full_name": " h1 Introduction to Generative Adversarial Networks GANs h1 Application h3 Image Generation h3 Image to Image Translation h3 Text to Image Translation h3 Image Manipulation h3 Section Summary h1 Concept h3 What is a GAN h3 Why GAN works h3 Objective function h1 Algorithm h1 Architecture h1 Evaluation h1 DCGAN Code Practice ", "stargazers_count": 0, "forks_count": 0, "description": "2a is equivalent to minimizing the negative objective function which is called loss function Eq. sigmoid x equiv dfrac 1 1 e x Discriminator with softmax sigmoid output. Without paired datasets CycleGAN can translate landscape photos into a particular painting style such as Monet Van Gogh. If we input a high quality image which cannot be classified into ImageNet classes to InceptionV3 network then the softmax output will not be a sharp value. For the following discussion we use sigmoid output in D by default. We hope the output probability of real images close to 1 D x_r rightarrow 1 while the probability of fake images close to 0 D x_f rightarrow 0. Both of these two measurements are based on the Inception V3 network which is pretrained on ImageNet dataset. 2b is still based on mean absolute error. Generative Adversarial Networks GANs is a cutting edge technique of deep neural networks which was first come up by Ian Goodfellow in 2014. 5b back propagating gradients to update the parameters in Generator theta_G. 5a and L D of Eq. GAN applies a zero sum game to help G to simulate the real distribution. IS X_g exp left mathbb E _ x_g sim X_g left D_ textrm KL left p left y x_g right parallel p left y right right right right where x_g sim X_g indicates that x_g is an image sampled from X_g D_ textrm KL P parallel Q is the KL divergence between the distributions P and Q p y x_g is the conditional class distribution and p y mathbb E _ x_g sim X_g left p y x_g right is the marginal class distribution. 3 together the aggregate objective function for GAN L GAN can be derived as textrm minimax underset theta_G min underset theta_D max L GAN mathbb E _ x_r sim X_r log D x_r mathbb E _ x_g sim X_g log 1 D x_g tag 6. i The output is just selecting the maximum input value within the height width window of input values. How to move frac 1 2 step Inserting zero columns and rows to the original input. max J D mathbb E _ x_r sim X_r D x_r mathbb E _ x_f sim X_f 1 D x_f tag 2. This notebook contains background application concept algorithm architecture evaluation and a code demo of typical GANs. Investigator Found the new fake dollars and successfully figured all 1st version fake dollars out. For notation consistency we replace x_f X_f by x_g X_g because generated images x_g are exactly identified as fake images x_f by D. Besides the diversity of softmax output indicates the diversity of images. when t 1 loss_mae t a e. To note CycleGAN does not require these two datasets to be paired. The cross entropy of the distribution q relative to a distribution p over a given set is defined as follows CE p q mathbb E _p log q Let s apply the CE loss into the loss function for D. begin align texttt Maxpooling2D 2 2 begin bmatrix 1 2 3 4 5 6 7 8 8 7 4 3 6 5 2 1 end bmatrix mapsto begin bmatrix 6 8 8 4 end bmatrix end align Convolution2D stride 2. StackGAN is a conditional GAN which can generate images based on text description. i The height width window of output values is just repeating the corresponding input value. In 2016 Yann LeCun Facebook AI research director described GAN as the most interesting idea in the last 10 years in Machine Learning. ii There is no weight and no trainable parameters introduced by this operation. Stride here is the reciprocal of the moving step. stride 2 Rightarrow moving step frac 1 2. FID Vert mu_r mu_g Vert 2 Tr left Sigma_r Sigma_g 2 left Sigma_r Sigma_g right 1 2 right where mu_r is the mean of the real features mu_g is the mean of the generated features Sigma_r is the covariance matrix of the real features Sigma_g is the covariance matrix of the generated features. If domain X is a text data distribution GAN is going to generate images based on the input text query. 0 Fix Seeds Data Preparation change as channel last n dim dim channel Set channel It is suggested to use 1 1 input for GAN training Get image size Get number of classes Hyperparameters optimizer Adam lr 0. There is a website called The GAN Zoo https github. However Saturating loss function cannot provide sufficient gradient for G to learn. Besides feature level information far surpasses classification level information. Investigator Found the new fake dollars and successfully figured all 2nd version fake dollars out. In each training iteration we train D and G sequentially. GAN technique is going to let the machine learn to generate new stuff. In mathematical statistics the Kullback Leibler KL divergence also called relative entropy is a measure of how one probability distribution is different from a second reference probability distribution. t the generator loss Update the weights of the generator using the generator optimizer Compile Models Optimizer for both the networks learning_rate 0. These two models compete with each other and eventually reach to a Nash equilibrium where both G and D cannot get better or worse any more. Thus D x_g is close to 0 log 1 D x_g will saturate to 0. Next we freeze theta_D and going to update theta_G. begin align D_ textrm KL P parallel Q sum_ x in mathcal X P x log left frac P x Q x right underbrace left sum_ x in mathcal X P x log Q x right _ textrm Cross entropy P Q underbrace left sum_ x in mathcal X P x log P x right _ textrm Cross entropy P P end align This equation is a discrete case of KL divergence where P and Q are two probability distributions mathcal X is the probability space. Then we modify the objective function Eq. begin align texttt Upsampling2D 2 2 begin bmatrix 1 2 3 4 end bmatrix mapsto begin bmatrix 1 1 2 2 1 1 2 2 3 3 4 4 3 3 4 4 end bmatrix end align ConvTranspose2D Deconvolution2D stride 2. For the continuous case P x Q x are probability density functions. Criminal An expert in making fake dollars. As we have trained D to classify real fake images. ii There is no weight and no trainable parameters introduced by this operation. 5b as Non Saturating loss function. For example in the early learning step D can easily distinguish generated images and real images. In practice we can only train these two models alternatively. We can calculate the CE of generated images independently p is 1 0 T q is D x_g 1 D x_g T. a It is the objective function of original GAN with a minimax game. Architecture Discriminator Downsampling networks Input images mapsto Output probability Dense Fully connected layer i Decreasing number of neurons for layers. Image to Image TranslationGAN can learn the features of two image collections and translate one images from one domain to another. Then we can sample any random points from this distribution as the generated data. b In practice Non Saturating L G is better than Saturating L G. Apart from comparing the generated images with real images by our eyes there are two common mathematical metrics to evaluate the quality of the generated images Inception Score IS and Fr\u00e8chet Inception Distance FID. com hindupuravinash the gan zoo where includes hundreds variants of GAN. We only focus on two typical GANs original GANs and Deep Convolutional GAN DCGAN. In a typical GAN the Criminal is named as Generator G while the Investigator is Discriminator D. b The loss function Eq. Similarly we can scratch an objective function for G max J G mathbb E _ x_g sim X_g textrm score x_g tag 4 where textrm score still denotes the D function x_g is sampled from the generated distribution X_g. Comparing with these generative models GAN has the state of the art performance in the filed of image generation. Investigator An expert in figuring fake dollars. Super resolution recovering the photo realistic texture for the low resolution image. Up to now we only update D in current iteration. In other words we feed a noise vector z sim N 0 1 into the G function and get a generated image x_g G z. DCGAN Code Practice a sigmoid wp b error t a loss_mse e 2 dL dw 2 e de dw 2 e 1 da dw 2 e 1 a a dn dw 2 e 1 a a p 2 e e 1 e p. A model can generate new data samples when it learns the whole distribution. MAE MSE loss gradient vanishing when the error is large. ii It cannot be deep and is not good at generating features. i It is also named as fractional strided convolution. ii It cannot be deep and is not good at extracting features. 3 we also use CE loss on the generated images. Here we use G to map a normal distribution with the generated distribution N 0 1 overset G mapsto X_g. Then we feed a batch of generated images x_g into the updated D. With this loss function log D x_g is very large and non saturating at the early learning period. when t 1 loss_ce t log a dL dw t 1 a da dw log a t 1 a a 1 a dn dw t 1 a p e p. Fr\u00e8chet Inception Distance IS totally depends on the InceptionV3 knowledge on ImageNet. Photo inpainting filling the missing area in a given image. After that we complete one training iteration for GAN. Although it is an infant technique there are bunch of models proposed with the suffix GAN such as ACGAN DCGAN WGAN BEGAN CycleGAN StackGAN. There is a trick that L G can simply borrow the second term from L D. Introduction to Generative Adversarial Networks GANs In this notebook we will be familiar with an advanced deep learning technique named Generative Adversarial Networks GANs. different style images lower resolution images or masked images GAN is going to transform input images X target images Y. Generally we measure the generated images in two dimensions quality of images and diversity of images. Algorithm 1 Training Discriminator. Objective function Discriminator Discriminator D is going to distinguish all fake images. Criminal Produced 2nd version fake dollars and the investigator cannot figure them out. Text to Image Translation If we embed text information as the label of corresponding images GAN will learn the mapping between sentiment vectors and image features. D_ textrm KL calculation will output a small number. D is maximizing mathbb E _ x_g sim X_g log 1 D x_g and G can minimizing this term to compete with D. Then we feed half batch of generated images x_g and half batch of real images x_r into the D from previous iteration. Section SummaryThe underlying functionality of GAN is transforming data distribution from domain X to domain Y. However the objective J D rightarrow infty is not applicable in optimization. 3 CE loss can mitigate the gradient vanishing problem when we apply the sigmoid to the output layer. when t 1 dL dw 1 da dw 1 a 1 a dn dw 1 a 1 a p 1 e e p Load Packages tf. If we define domain X as a normal noise distribution GAN is going to generate images. As for the loss functions in classification we often prefer to use cross entropy CE loss rather than mean absolute error MAE L1 loss and mean squared error MSE L2 loss. t the discriminator loss Update the weights of the discriminator using the discriminator optimizer Get the latent vector Generate fake images using the generator Get the discriminator logits for fake images Calculate the generator loss Get the gradients w. Rightarrow underset theta_D min L D mathbb E _ x_r sim X_r log D x_r mathbb E _ x_g sim X_g log 1 D x_g tag 3 Any loss consisting of a negative log likelihood is a cross entropy between the empirical distribution defined by the training set and the probability distribution defined by model. Input noise vectors mapsto Output images Dense Fully connected layer i Increasing number of neurons for layers. Page 132 Deep Learning 2016The nature of Eq. begin align BCE mathbb E _ x_r sim X_r left begin bmatrix 1 0 end bmatrix begin bmatrix log D x_r log 1 D x_r end bmatrix right mathbb E _ x_f sim X_f left begin bmatrix 0 1 end bmatrix begin bmatrix log D x_f log 1 D x_f end bmatrix right end align With algebraic operations the loss function for D can be derived as below. Let s replace the textrm score function by D. The exp in the expression is there to make the values easier to compare so it will be ignored if we use ln IS without loss of generality. The objective of G is to make D classify the generated images into real type. If the output D x is a real number we hope D x_r rightarrow infty and D x_f rightarrow infty to achieve the maximum J D. For example in SummerWinter translation we need to collect photos of the same scene in different seasons. Generation is always harder than recognition. Especially in the recent two years GAN was developed with an exponential increment Fig. 4 textrm Non Saturating underset theta_G min L G mathbb E _ x_g sim X_g log D x_g tag 5. This method is exactly a minimax game and is proposed in the original GAN paper. In other words GAN is the most powerful image generative model. First we feed noise vectors z into the G from previous iteration to generate images x_g G z. With the sigmoid function the output denotes the probability of real D x p real. EvaluationGAN is a very new topic it is still an open problem to find a perfect evaluation of GAN or generated images. For example mean squared error is the cross entropy between the empirical distribution and a Gaussian model. 9 are recommended Define the loss functions to be used for discrimiator Define the loss functions to be used for generator Start Training Plot save generated images through training display original display generation. Besides maximizing the objective function Eq. In the objective function we add a sigmoid transfer function to the output layer. If the softmax output is sharp p textrm one class rightarrow 1 p textrm other classes rightarrow 0 the IS will be small and the quality of this image is high. 9 trainRatio 5 Models Setup Build Generator with convolution layer remove padding same to scale 6x6 up to 7x7 padding same Build Discriminator with convolution layer 28 x 28 x channel 14 x 14 x 32 7 x 7 x 64 3 x 3 x 128 GAN Part Build our GAN Get the batch size training train_ratio times on D while training once on G Get the latent vector Generate fake images from the latent vector Get the logits for the fake images Get the logits for real images Calculate loss of D Get the gradients w. However paired datasets are very expensive and often not available. Besides it can also translate zebras into horses. Our goal is to find the optimal weights to minimize the following BCE loss function. n th version fake dollars. As discussed in Chapter 20 of Deep Learning there are many other generative models such as Restricted Boltzmann Machine RBM Generative Stochastic Networks GSNs and Variational auto encoder VAE. However we cannot optimize this combined loss function by changing G and D simultaneously. The following equation is a typical binary cross entropy BCE loss. GAN is a very new stuff and has a promising future. ii Similar to convolution operation these weights become trainable parameters in your model. CycleGAN is one representative unsupervised GAN with such functionality. For fake images p is 0 1 T q is D x_f 1 D x_f T. In other words IS can only evaluate the generated images which should belong to ImageNet dataset. To note GAN does not simply memorize the given dataset. Thus we prefer to map the range to probability space infty infty mapsto 0 1. For example we first learned how to recognize digits like 0 9 then we tried to mimic the shape of digits and created digits in our styles which is called generation. In binary classification these two methods have the same effect. Actually Discriminator D is like a binary classifier to distinguish real or fake. theta_D denotes the parameters in D model which are the weights in neural networks. For real images p is 1 0 T q is D x_r 1 D x_r T. i The output is a linear combination of the input values times a weight for each cell in the height width kernel filter. 3 Cross entropy is a divergence to measure the difference between two distributions. In this game G is trying to generate real like images to fool D while D is trying to figure all fake images out. Intuitively it would be better to set a large learning rate at the early stage of gradient descent while a small learning rate when closing to the optima. The later one can also be reformulated as 1 D x_f rightarrow 1. Corresponding to L D Eq. Intuitively we can scratch an objective function of D max J D mathbb E _ x_r sim X_r textrm score x_r mathbb E _ x_f sim X_f textrm score x_f tag 1 where x_r is sampled from the real distribution X_r x_f is sampled from the fake distribution X_f textrm score denotes an evaluation function which gives high scores on real images and low scores on fake images. 3 back propagating gradients to update the parameters in Discriminator theta_D. In comparison FID is a more general evaluation and applicable to new datasets. Why GAN works If a model only learns the data points rather than the whole distribution this model can only memorize the dataset. With the loss function Eq. To note the parameters in Generator theta_G is frozen. 4 textrm Saturating underset theta_G min L G mathbb E _ x_g sim X_g log 1 D x_g tag 5. Inception Score IS measures the KL divergence similar to cross entropy see Def. Generator Upsampling networks. Image ManipulationThere are many other applications in image manipulation. CE loss the norm of gradient nabla is positively related to the error a large error leads to a fast learning speed. 5a as Saturating loss function. 2 between the generated sample distribution and the ImageNet distribution whereas FID calculates the feature level distance between the generated sample distribution and the real sample distribution. InceptionV3 network is trained on ImageNet and used to predict the class of given images. a Later Ian Goodfellow proposed a more stable and efficient loss function for G. IS is derived from the classification output while FID is derived from the feature layer. One becomes better means the opponent must be worse. 1 scratched at the beginning. How to be a master of producing fake dollars Criminal Produced 1st version fake dollars and the investigator cannot figure them out. Similarly we feed noise vectors z into the G from previous iteration to generate images x_g G z. Application Image GenerationMost of deep learning techniques we learned previously are used for recognition. After we learn the GAN technique you are highly encouraged to propose some creative ideas on further applications. AlgorithmIf we combine L G of Eq. Concept What is a GAN It is like a zero sum game in Game Theory Example 1. Generator Generator G is going to generate real like images to fool D. mathbb E _ x_r sim X_r log D x_r is minimizing the divergence between real distribution X_r in the training set and the probability distribution defined by D model and mathbb E _ x_f sim X_f log 1 D x_f is maximizing the divergence between generated distribution X_f and the probability distribution defined by D model. If domain X is another image distribution e. ii These weights become trainable parameters in your model. a Leftrightarrow min L D mathbb E _ x_r sim X_r D x_r mathbb E _ x_f sim X_f 1 D x_f tag 2. ", "id": "together/gan-introduction-explained", "size": "19494", "language": "python", "html_url": "https://www.kaggle.com/code/together/gan-introduction-explained", "git_url": "https://www.kaggle.com/code/together/gan-introduction-explained", "script": "matplotlib.image show_img DCGAN(Model) Reshape load_data tensorflow.keras.layers discriminator_loss numpy glorot_normal plt_img Adam Dropout generator_loss Dense change_image_shape generator_conv tensorflow.keras.initializers tensorflow train_step matplotlib.pyplot Sequential discriminator_conv Model Input tensorflow.keras \\ compile __init__ tensorflow.keras.optimizers tensorflow.keras.datasets.fashion_mnist ", "entities": "(('We', 'only two typical GANs original GANs'), 'focus') (('loss functions', 'display original generation'), 'recommend') (('However we', 'G'), 'optimize') (('GAN', 'other words'), 'be') (('X_g 1 G', 'D.'), 'maximize') (('which', 'neural networks'), 'denote') (('_ textrm KL D calculation', 'small number'), 'output') (('we', 'generality'), 'be') (('G', 'sufficient gradient'), 'provide') (('output', 'D p real real'), 'denote') (('Non Saturating L G', 'Saturating L G.'), 'b') (('you', 'further applications'), 'learn') (('probability relative how one distribution', 'reference probability second distribution'), 'be') (('conditional which', 'text description'), 'be') (('Then we', 'updated D.'), 'feed') (('1 Thus close to 0 D', '0'), 'be') (('Intuitively it', 'when optima'), 'be') (('probability two mathcal X', 'KL discrete divergence'), 'begin') (('which', 'low fake images'), 'scratch') (('weights', 'trainable model'), 'become') (('we', 'image G generated z.'), 'feed') (('underlying functionality', 'Y.'), 'section') (('CycleGAN', 'Monet Van such Gogh'), 'translate') (('output', 'height width kernel filter'), 'be') (('goal', 'BCE loss following function'), 'be') (('height width window', 'input just corresponding value'), 'repeat') (('Stride', 'here moving step'), 'be') (('where G', 'Nash eventually equilibrium'), 'compete') (('we', 'real fake images'), 'train') (('learning step early D', 'easily generated images'), 'distinguish') (('discriminator logits', 'gradients'), 'update') (('Investigator', 'successfully 2nd version fake dollars'), 'find') (('D', 'real type'), 'be') (('GAN', 'image generation'), 'have') (('which', 'styles'), 'learn') (('T 1 0 q', 'images real p'), 'be') (('we', 'current iteration'), 'update') (('we', 'default'), 'use') (('s', 'D.'), 'define') (('WGAN', 'ACGAN such DCGAN'), 'be') (('First we', 'images'), 'feed') (('KL divergence', 'similar entropy Def'), 'be') (('we', 'error MAE L1 error MSE L2 rather absolute loss squared loss'), 'prefer') (('text data GAN', 'input text query'), 'go') (('feature level information', 'classification level far information'), 'surpass') (('large error', 'learning fast speed'), 'lead') (('EvaluationGAN very new it', 'GAN generated images'), 'be') (('Then we', 'objective function'), 'modify') (('class p y mathbb E _ p conditional X_g left y right', 'KL distributions'), 'leave') (('two methods', 'same effect'), 'have') (('noise vectors Output Dense Fully connected i', 'layers'), 'mapsto') (('when we', 'output layer'), 'mitigate') (('s', 'D.'), 'let') (('Image', 'another'), 'learn') (('It', 'generating features'), 'ii') (('It', 'Game Theory Example'), 'concept') (('It', 'classes'), 'change') (('Investigator', 'successfully 1st version fake dollars'), 'find') (('P x Q x', 'continuous case'), 'be') (('we', 'different seasons'), 'need') (('we', 'D'), 'train') (('We', 'close 0'), 'hope') (('align texttt 2 end bmatrix Maxpooling2D 1 2 4 5 8 8 7 4 6 5 2 1 mapsto', 'bmatrix'), 'begin') (('It', 'minimax game'), 'be') (('T independently p 1 0 q', 'generated images'), 'calculate') (('output', 'input values'), 'select') (('method', 'minimax GAN exactly original paper'), 'be') (('bmatrix 1 2 3 4 mapsto', 'bmatrix'), 'begin') (('Thus we', 'infty mapsto'), 'prefer') (('ImageNet FID', 'sample generated distribution'), '2') (('T 0 1 q', 'images fake p'), 'be') (('Yann LeCun Facebook AI research director', 'Machine Learning'), 'describe') (('we', 'GAN'), 'complete') (('together aggregate objective function', 'L GAN E _ mathbb E _ log'), 'derive') (('3 we', 'generated images'), 'use') (('loss function', 'log learning D very large early period'), 'be') (('note', 'Generator theta_G'), 'freeze') (('GAN', 'simply given dataset'), 'memorize') (('L G', 'L D.'), 'be') (('It', 'extracting features'), 'ii') (('Generally we', 'images'), 'measure') (('Criminal Produced version 1st fake dollars', 'them'), 'be') (('Generation', 'always recognition'), 'be') (('Investigator', 'Generator G'), 'name') (('2b', 'still mean absolute error'), 'base') (('CycleGAN', 'such functionality'), 'be') (('we', 'previously recognition'), 'use') (('quality', 'image'), 'be') (('stride 2 Rightarrow', 'step'), 'move') (('Then we', 'previous iteration'), 'feed') (('generator loss', 'networks'), 'update') (('Load Packages', 'dn e e 1 a 1 1 a a p 1 p'), 'when') (('covariance matrix', 'covariance generated features'), 'Vert') (('squared error', 'cross empirical distribution'), 'be') (('Image ManipulationThere', 'image many other manipulation'), 'be') (('1 da', 't 1'), 'log') (('Calculate loss', 'gradients'), 'Build') (('Generator Generator G', 'D.'), 'go') (('which', 'ImageNet dataset'), 'base') (('Fr\u00e8chet Inception Distance', 'ImageNet'), 'depend') (('GAN', 'sentiment vectors'), 'text') (('softmax then output', 'ImageNet classes'), 'input') (('rather whole model', 'only dataset'), 'work') (('GAN', 'real distribution'), 'apply') (('when it', 'whole distribution'), 'generate') (('Similarly we', 'G z.'), 'feed') (('It', 'also fractional strided convolution'), 'name') (('which', '2014'), 'be') (('we', 'learning advanced deep technique'), 'introduction') (('FID', 'more general new datasets'), 'be') (('notebook', 'code typical GANs'), 'contain') (('GAN', 'very new promising future'), 'be') (('Besides it', 'horses'), 'translate') (('machine', 'new stuff'), 'go') (('Inception Score', 'generated images'), 'be') (('we', 'only two models'), 'train') (('network', 'given images'), 'InceptionV3') (('Ian Later Goodfellow', 'G.'), 'propose') (('textrm 4 where score', 'generated distribution'), 'scratch') (('which', 'negative objective function'), 'be') (('Criminal Produced version 2nd fake dollars', 'them'), 'figure') (('note', 'two datasets'), 'require') (('GAN', 'increment exponential Fig'), 'develop') (('loss function', 'D'), 'begin') (('Here we', 'generated distribution'), 'use') (('Cross 3 entropy', 'two distributions'), 'be') (('images', 'layers'), 'network') (('which', 'ImageNet dataset'), 'evaluate') (('generated images', 'D.'), 'replace') (('masked GAN', 'X target Y.'), 'low') (('Then we', 'generated data'), 'sample') (('3 loss', 'probability model'), 'underset') (('D', 'fake images'), 'try') (('Next we', 'theta_G.'), 'freeze') (('rightarrow infty', 'optimization'), 'be') (('FID', 'feature layer'), 'derive') (('1 x_f', 'probability D model'), 'sim') (('function Discriminator Discriminator Objective D', 'fake images'), 'go') (('rightarrow', 'J maximum D.'), 'be') (('we', 'output layer'), 'add') (('GAN', 'images'), 'go') (('Discriminator Actually D', 'real'), 'be') "}