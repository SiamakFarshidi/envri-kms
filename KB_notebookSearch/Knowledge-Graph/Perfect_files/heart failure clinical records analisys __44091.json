{"name": "heart failure clinical records analisys ", "full_name": " h1 Heart Failure clinical records h3 Index h1 Exploratory data analisys h2 Imports h2 Dataset h2 Feature distributions h2 Standardization h2 Correlation matrix h2 Normality assumptions h2 Validate the models h2 Feature selection h3 Mutual information h3 Chi squared test h4 Results h3 KFold case h2 Class imbalance h3 Random oversampling h3 Smote h3 Class weight parameter h1 Classification models h2 Decision tree h2 Random forest h2 Linear Regression h2 Logistic Regression h2 Support Vector Machine h4 Hard margin h4 Soft margin h4 Kernel trick h3 Linear kernel h3 Polynomial and RBF kernels h4 Gridsearch on C and gamma h3 Polynomial kernel best models h3 RBF kernel best models h2 K Nearest Neighbors h3 weights original h3 weights distance h2 Naive Bayes h3 Kernel density estimation h4 Bandwidth rule of thumb h3 Naive bayes with KDE and bernoulli Flexible bayes h3 Gaussian Naive Bayes h1 Results and conclusions h1 References ", "stargazers_count": 0, "forks_count": 0, "description": "the target with a chi squared test. Considering a sample z_i x_i y_i the out of bag error is the average error for each z_i evaluated using only trees that do not contain z_i in their training set. Bandwidth rule of thumbThere is also a rule of thumb for the selection of the best bandwidth in the case of gaussian kernel and gaussian estimated distribution. 95 anaemia Decrease of red blood cells or hemoglobin haematocrit levels were lower than 36 Boolean 0 1 creatinine_phosphokinase Level of the CPK enzyme in the blood mcg L 23. the line is a kde Smote Synthetic Minority Oversampling Technique or SMOTE is a useful technique used to deal with unbalanced datasets. This technique has been proposed in 1995 by John and Langley 3 references under the name of Flexible Bayes. Chi squared testFor the categorical features we can further test the dependence w. In fact one very nice positive aspect of Naive Bayes in general is the good performances even in cases in which some of the hypothesis are not respected. Smaller values of C allows more errors in exchange of a bigger margin while higher values can be used where it s needed to be less permissive regarding misclassificatons with an higher risk of overfitting. In the visualization below SMOTE is performed on the whole dataset while then when we use it during classification we perform it only on training samples as stated for random oversampling using it on the whole dataset leads to data leakage. Linear Regression Linear regression is one of the simplest models in machine learning. png Generalized Linear Models are based on the following equation Y X beta epsilon in which Y is the response vector X is the matrix of predictors beta is a set of unknown parameters and epsilon is a set of unobservable random variables called errors. The most used kernels are the polynomial kernel degree k langle textbf x textbf x prime rangle 1 k the radial basis function gaussian kernel e gamma textbf x textbf x prime 2 Linear kernel means that no mapping is done and the kernel is simply langle textbf x textbf x prime rangle To better see how soft margin svm works with different kernels we can see the classification according to serum_creatinine and ejection_fraction. Then will follow a number of machine learning models trained on the preprocessed dataset aiming to predict the survival of patients that suffered HF. weights distance Now let s apply it on our training set considering all features weights original weights distance Naive Bayes Naive bayes classifier is a probabilistic model based on the Bayes theorem in which is assumed a strong independence between the features. Another approach is K Fold that consists in diving the dataset into K parts with an equal number of samples using K 1 for training and the last one as validation. This represents our prior knowledge on the class distribution. In some cases SMOTE performs better with respect to random oversampling and the opposite in others. 05 interpret p value in ms categorical attributes need to be fixed categorical attributes need to be fixed KFOLD caching smote results for each fold we have different features categorical attributes need to be fixed cache smoted folds use cached folds initialize kfold object in ms random oversampling SMOTE not resampled weighted classes accuracy precision recall f1 score KFOLD random oversampling SMOTE not resampled weighted classes value are float variables due to class_weight balanced in ms oob_score Whether to use out of bag samples to estimate the generalization accuracy square root of number of features random oversampling SMOTE no random oversampling class weight accuracy precision recall f1 score in ms KFOLD random oversampling SMOTE no random oversampling class weight holdout oversampled SMOTE not oversampled oversampled SMOTE not oversampled https python graph gallery. 7861 diabetes If the patient has diabetes Boolean 0 1 ejection_fraction Percentage of blood leaving the heart at each contraction Percentage 14. x k psi x is the polynomial mapping functionThe problems arise in the case of multiple predictors. Soft marginIn this implementation a relaxationis added to the constraint. The main drawback of decision tree is the fact that sometimes is a too simple model that provides lower values of accuracy and it s easy to overfit. 606 We can clearly see how using some rebalancing techniques the f1 score increase substancially. 05 all the numerical features could be considered non normal. The idea is that not being able to sample more samples from the true distribution we sample them from the empirical distribution coming from the samples that we already have. click here to go to the end Results_and_conclusions Index Exploratory Data analisys Exploratory_data_analisys Imports Imports Dataset Dataset Feature distributions Feature_distributions Standardization Standardization Correlation matrix Correlation_matrix Normality Assumptions normality_assumptions Validate the models validate_the_models Feature selection Feature_selection Mutual information Mutual_information Chi squared test Chi_squared_test KFold case KFold_case Class imbalance Class_imbalance Random oversampling oversampling Smote Smote Class weight parameter Class_weight Classification models Classification_models Decision tree Decision_tree Random forest Random_forest Linear regression Linear_regression Logistic regression Logistic_regression Support vector machine Support_vector_machine Linear kernel Linear_kernel Polynomial and RBF kernels Polynomial_and_RBF_kernels Polynomial kernel Polynomial_kernel RBF kernel RBF_kernel K nearest neighbors K_nearest_neighbors weights original weights_original weights distance weights_distance Naive bayes Naive_bayes Kernel density estimation Kernel_density_estimation Naive bayes with KDE and bernoulli Flexible bayes naive_bayes_with_KDE_and_bernoulli Gaussian Naive Bayes Gaussian_naive_bayes Results and conclusions Results_and_conclusions References references Exploratory data analisys Imports Here are listed the main libraries used Numpy standard library for math operations Scipy used to compute test statistics and distributions Pandas used to manipulate data inside dataframes and for basic computations Sklearn used to apply different ML models to the data Pyplot to plot visualizations Seaborn built on top of pyplot nicer visualizations Other libraries random used to generate random numbers HTML and matplotlib. To handle class inbalance it s possible to re balance the dataset with different techniques. We can see how it works when the sample rate is higher. png One common example of kernel is a normal kernel but also other functions can be used such as triangular Epanechnikov or uniform. In this case we have 299 records so the same subset is used both for validation and testing. Here we can see the distribution of ejection_fraction for only under represented class samples before and after oversampling. The dataset is collected in 2015 at the Allied Hospital in Faisalabad Punjab Pakistan Feature Explanation Measurement Range age Age of the patient Years 40. stands for ejection fraction. Kernel trickTo solve the svm optimization problem computing inner products it s needed. A lower value of the statistic means a stronger independence. com 11 grouped barplot linear poly rbf in ms original oversampled SMOTE class weight balanced original oversampled SMOTE class weight balanced https python graph gallery. Estimating Continuous Distributions in Bayesian Classifiers 1995 PALETTE fade linear interpolate from color c1 at mix 0 to c2 mix 1 alpha 0. The null hypothesis is that the samples are taken from a normal distribution so with a sufficiently low value of the p value we can consider the features as not normal. The decision trees are trained with data sampled with repetition from the original dataset bagging. Here are reported the categorical features Standardization One important step is standardization for numerical features that is performed via the standard sklearn function removing the mean and scaling to unit variance. Results and conclusions Here we can see the results obtained with different models and different rebalancing techniques for the Hearth Disease dataset. begin align z frac x bar x hat sigma end align where bar x is the mean of the training samples and hat sigma is the sample standard deviation. png attachment kfold. org stable modules generated sklearn. X has as many rows as the number of training samples and p 1 columns where p is the number of predictiors that in this case after feature selection is 7 plus one first column that is an all one column that is used for the intercept beta_ 0. Considering the way KNN learns it is possible to re train an already trained model on new data. For the first ones a bernoulli distribution is the easy pick while for the others an analisys has to be done. This allows us to do a sort of vaidation during training. The results are presented at the end of the notebook. Iteration Feature 1 Feature 2 Iteration 1 anaemia diabetes Iteration 2 sex smoking Iteration 3 anaemia sex Iteration 4 anaemia smoking Iteration 5 sex smoking Class imbalance One thing to take into account is the possible class imbalance. Feature distributions We can see how features are distributed according to label. Normality assumptions Speaking of the numerical features it s interesting to notice whether or not they come from a normal distribution. We can see that for the mutual information the vast majority of the cases the features are kept or dropped analogously to the holdout feature election so we stick with the original feature selection removing platelets. Is in fact possible to plot the tree to see how the dataset is split at every step and it s easy to describe how the prediction works. Usually linear regression is not suggested for binary classification because it can output values below 0 or above 1 nonsense assuming probabilistic meaning. The dataset contains 11 clinical features some of them are binary others are numerical the follow up period and the label DEATH_EVENT that indicates whether or not the patient has died. In this way we can see how models handle a mix of continouos and binary features KFold caseWhen kfold crossvalidation is done feature selection needs to be performed inside every iteration in fact performing it before would mean considering the whole training set and this can lead to an overestimation of the accuracy during cross validation. 40 serum_sodium Level of sodium in the blood mEq L 114. less than 5 samples in the subset. Random forest Random forest is an ensemble model based on a number of decision trees. Furthermore where is possible to apply it also the use of the class weight parameter increases the performances sometimes outperforming the other techniques. The same happens in the kfold case in which resampling methods are applied inside each iteration and not before. 06 hat sigma n frac 1 n end align Even if gaussian assumptions seems to be not respected normality_assumptions we can see the results for our features Despite the rule of thumb different bandwidths will be evaluated. animation used for the animations Dataset Heart failure clinical records Data Set contains the medical records of 299 patients who had heart failure. This is confirmed by the fact that in the original paper by Chicco and Jurman 1 references the analisys has been conducted taking into account only these two features. Considering the mapping this operation could be unfeasible. In this case feature selection consists in dropping features that have a very low mutual information and chi squared statistic so it s possible to evaluate those for each iteration of KFold. My implementation of SMOTE is based on the original paper by N. The configuration tested is class weight balanced that according to the sklearn documentation https scikit learn. In fact according to representer theorem w can be written as w sum_ i alpha_i psi x_i space so it follows that space w 2 langle sum_j alpha_j psi x_j sum_j alpha_j psi x_j rangle sum_ i j 1 m alpha_i alpha_j langle psi x_i psi x_j rangle and the problem now consists in maximizing the margin over alpha. 148 sex Woman or man Binary 0 1 smoking If the patient smokes Boolean 0 1 time Follow up period Days 4. html and definition https en. Naive bayes can achieve good results with categorical features so one approach could be binning the numerical features into bins and then treat those bins as categories but the problem is that in that way you risk to lose some important information. Then is also possible to weight the neighbors according to their distance from the new point. space space forall i space space y_i big langle w x_i rangle b big 1 color red xi_i space space and space space xi_i 0 end align xi_i is called slack variable it is the distance of x_i from the corresponding class s margin if x_i is on the wrong side of the margin and 0 otherwise. Basically if the label and the prediction have the same sign it means that the prediction is on the correct side of the margin. png Hard marginThe simplest implementation is the hard margin SVM in which data needs to be linearly separable to allow the algorithm to converge. Bootstrapping training data in fact allow us to decrease the variance of the model without increasing the bias average of many trees is less sensitive to noise with respect to single tree and helps us to train trees that are less correlated. the bigger the better Decision tree Decision trees are one of the most widely known machine learning models. So for each sample we compute the error considering only trees not trained on that sample. Being able to respect this constraint means having a linearly separable problem. Therefore it s possible to map our features in another higher dimensional space in which hopefully it will be easier to learn. Best models are re trained on the whole training set and then evaluated on the validation set. We can train decision trees considering the rebalanced datasets and the original one. Kernel density estimationKDE is a technique used to estimate the probability distribution of a continouos random variable starting from a finite set of observations. Then we noticed how using Gaussian Naive Bayes even without respecting the hypothesis leads to good results and also with a Bayes Classifier with KDE the results are in line. On the top left plot there is an example of a normal kernel centered in zero with the given bandwidth. The best model seems to be the decision tree on the original dataset with class_weight balanced with a depth of 6. png attachment regression. Logistic Regression Logistic regression is a generalized linear model in which the link function is not the identity as in the case of linear regression but is the logit. Once the class probabilities given the sample are computed one common strategy is to pick the most probable one begin align hat y argmax_ k in 1 dots K space p C k prod_ i 1 n p X_i x_i C k end align where the evidence is ignored because is the same for every class. It consists in trying to fit an hyperplane that best divides the dataset into the two classes by maximizing the margin the distance between the hyperplane and the closest points. As we can see the results are still quite comparable with the other models. 80 high_blood_pressure If a patient has hypertension Boolean 0 1 platelets Platelets in the blood kiloplatelets mL 25. The inverse of the logit is called sigmoid begin align displaystyle S x frac 1 1 e x frac e x e x 1. Best overall model seems to be the random forest trained on the oversampled dataset that delivers the best results in terms of accuracy and f1 score. 614 Logistic Regression 0. For the models that allow it it s possible to evaluate the ROC curve to select a threshold according to the main goal minimize false positives or maximize true positives but the results in the table are obtained by fixing the threshold at 0. Altough sex and anaemia produce still very low values of the statistic they are kept. An online version of this notebook is available here https nbviewer. 664 Random Forest 0. com 11 grouped barplot distance in ms distance in ms oversampling SMOTE no oversampling k fold oversampling SMOTE no oversampling accuracy precision recall f1 score oversampling SMOTE no oversampling k fold oversampling SMOTE no oversampling accuracy precision recall f1 score in ms original oversampling SMOTE original oversampling smote in ms original oversampling smote k fold original oversampling smote original oversampling smote. Correlation matrix To see how features are correlated is useful to look at the correlation matrix that is a matrix in which are showed the correlation values of each couple of features according to the Pearson s correlation coefficient displaystyle rho _ X_1 X_2 frac operatorname cov X_1 X_2 sigma _ X_1 sigma _ X_2 Where cov stands for the covariance measure displaystyle operatorname cov X_1 X_2 operatorname E big X_1 operatorname E X_1 X_2 operatorname E X_2 big That is computed for every pair of features X_1 and X_2 From the heatmap we can see that in general features are quite uncorrelated with the exception of sex and smoking that seems to be slightly positively correlated. This is useful when we have lots of data. During training random oversampling needs to be done after the subdivision into train validation and test to avoid data leakage. In this case the imbalance is not so strong so just with a sample rate of 1 we obtain a good balance between classes. In our case the weights will be Classification models Now will follow a series of different models used to perform classification of the DEATH_EVENT Decision tree Decision_tree Random forest Random_forest Linear regression Linear_regression Logistic regression Logistic_regression Support vector machine linear poly rbf Support_vector_machine K nearest neighbors K_nearest_neighbors Naive bayes Naive_bayes All models are evaluated considering the following metrics accuracy frac TP TN TP TN FP FN quad precision frac TP TP FP quad recall frac TP TP FN quad F_1 2 times frac precision times recall precision recall Moreover ROC curve is also evaluated. In this case we plot the kernel density estimation with a kdeplot to better see the distribution along with the boxplot. 285 DEATH_EVENT If the patient died during the follow up period Boolean 0 1 To be consistent with the feature description let s represent the platelets as kiloplatelets mLLet s take a look to the datasetFor brevity creatinine_phosphokinase will be renamed CPK. begin align logit p ln frac p 1 p end align The link function is a function connecting the expected value of the response and the linear combination of predictors. Here we can see the results on our dataset Support Vector Machine Support vector machine is a powerful model used for both classification and regression. This could be helpful for some models Gaussian Naive Bayes Gaussian_naive_bayes in which is assumed normality conditional to the class. Having trained the tree with unnormalized features from the visualization we can see the split policies on the original feature values providing so a clearer explaination. This means that SVM can scale well. For this reason we can add a term on the constraints to relax them. 604 KNN distance 0. When the algorithm has converged then the model can be described using only the points on the margin called support vectors. The concept is that every feature independently contribute to the final prediction. org github lorenzodenisi Heart Failure Clinical Records blob master Heart 20Failures. 2 of training data Feature bagging is also performed this means that at each candidate split only a subset of features traditionally the square root or the log2 of the total number is considered. The parameters taken into account are of course the number of neighbors but also the distance metric that is the minkowski distance Big sum_ i 1 n x_i y_i p Big 1 p in which the parameter p is changed. In fact to predict a new point is necessary to store the entire dataset so when the number of features or the number of records is very high the computation could be heavy. The null hypothesis is that anaemia and DEATH_EVENT are independent so we compute the expected values considering them as under the null hypothesis so begin align P anaemia 0 DEATH _EVENT 0 P anaemia 0 times P DEATH _EVENT 0 frac 128 times 159 224 times 224 0. A matrix is semidefinite if and only if textbf x T G textbf x 0 space space forall textbf x in R n backslash 0 and the aigenvalues are non negative. We can find some features strictly related to medical aspects like levels of enzymes sodium creatinine and platelets in the blood and others that are more common like age sex or smoking. We will see that changing the prior the results may be very different. This is done K times and each part is used as validation exactly once. The test statistic is begin align W frac Big sum_ i 1 n a_i x_ i Big 2 sum_ i 1 n big x_i bar x big 2 end align where x_ i is the i th smallest number in the sample bar x is the sample mean and a_i are coefficients derived from a normal distributions. The logit stretches the interval 0 1 into the whole real line. png When the number on samples for each part is 1 the method is called leave 1 out. 619 Linear SVM 0. Both accuracy and f1 score inside parenthesis are showed. org doc scipy reference generated scipy. p C k is the prior. Our dataset is composed by a mix of binary and continouos features. Now that we have the statistic we could compare it with a chi2 distribution with those given degrees of freedom fixing an alpha value and keeping only the features that produce a lower p value rejected. Here we can see how the tree grows according to the depth constraint. Then model selection can be also performed according to the Area Under the Curve AUC that is the area under the roc curve. The probabilities are then multiplied to obtain the class probability given the sample. Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone 2020 2 https arxiv. SMOTE Synthetic Minority Over sampling Technique 2002 3 https dl. 00 serum_creatinine Level of creatinine in the blood mg dL 0. Feature selection Mutual informationMutual information can be useful when it s needed to assess the dependence of a feature with respect to the target. Also RBF SVM with class weights balanced provides some good results on KFold. This is a parameter present in many models that allows to weight samples during training according to the imbalance. begin align hat f_h x frac 1 n sum_ i 1 n K_h x x_i frac 1 nh sum_ i 1 n frac K x x_i h end align Practically it consists in summing for each observation the same function K called kernel centered on that observation. So to evaluate Naive Bayes classifier a model based on bernoulli and kde distributions is implemented. MI is always 0 and higher values indicate stronger dependence. Prediction is then done by majority voting. Out Of Bag score OOB Out of bag score is equal to 1 the out of bag error. This model is quite simple but it doesn t scale well. Since we assume feature independence p X x C k prod_ i 1 n p X_i x_i C k To evaluate the probability of a sample given the class it s important to know the feature distributions. png attachment simple_linear_regression_scaled2. For the chi squared test the results are quite different so it s decided to keep the top 2 for each iteration. Random oversamplingRandom oversampling is a resample technique that consists in taking the under represented class samples and sampling new samples from them until the classes are balanced. From the correlation matrices we can see that apart from some pairs of features anaemia CPK sex ejection_fraction the vast majority seems to be strongly uncorrelated. We already tested the linear independence of the features with the correlation matrix now we can perform the same correlation matrix but considering the samples given the target label. Naive bayes with KDE and bernoulli Flexible bayes As stated before the conditional distributions are a mix of bernoulli and kde estimations. A symmetric function K X times X rightarrow rm I R can be a kernel function if and only if it respects the Mercer theorem that says that the Gram matrix that is the matrix such that G_ i j K textbf x _i textbf x _j needs to be positive semidefinite. This holds for the samples conditional on the class but also considering all of them as we can see from the table. As we can see already from the unnormalized distribution plot of the features the most informative ones seem to be ejection_fraction and serum_creatinine. A KDE weights a defined density around each observation. com 11 grouped barplot Original dataset poly 3 degree rbf random oversampling dataset poly 3 degree rbf SMOTE dataset poly 3 degree rbf class weight poly 3 degree rbf original oversampling SMOTE class weight balanced https python graph gallery. Receiver operating characteristic is a plot that shows the True Positive and False positive rates applying different thresholds on the prediction that needs to be a number between 0 and 1. Mutual information is estimated with the function mutual_info_classif from sklearn that according to the documentation https scikit learn. For this reason logistic regression is very suitable for binary classification. Class weight parameterAnother way to handle the class imbalance is class weighting. space space forall i space space y_i big langle w x_i rangle b big 1 end align where y_i is the true label and the prediction is the evaluated distance of the sample from the hyperplane. Before applying Naive Bayes we need to test that all the features are mutually independent conditional on the target. Such measure in this case is the Gini Index in this sklearn implementation the unnormalized gini index is used G 1 sum_ j p_j 2 where p is the ratio between number of samples of class j and total number of samples. Each tree will be trained on average on 63. As we can see even if not so strong there is a class imbalance. html sets the weights in this way w_i frac N m times n_i where N is the total number of samples m is the number of classes and n_i is the number of samples belonging to class i. This is a fundamental prior knowledge on features picking the wrong distributions can leads to wrong results. Altough some of them seems to be gaussians others are not ejection_fractions seems to be bimodal. As the number of samples goes to infinity X 2 tends to a chi 2 distribution with columns 1 rows 1 degrees of freedom so in this case 1. com 11 grouped barplot original oversampling SMOTE class weight balanced https python graph gallery. c stands for serum creatinine while e. One further model can be trained with class_weight balanced For decision trees and random forests normalization is not necessary so for the sake of visualization I use the unnormalized features. begin align Y begin bmatrix y_ 0 y_ 1 vdots y_ n end bmatrix space space space space X begin bmatrix x_ 11 x_ 12 x_ 13 dots x_ 1p x_ 21 x_ 22 x_ 23 dots x_ 2p vdots vdots vdots ddots vdots x_ n1 x_ n2 x_ n3 dots x_ np end bmatrix space space space space beta begin bmatrix beta_ 0 beta_ 1 vdots beta_ p end bmatrix space space space space epsilon begin bmatrix epsilon_ 0 epsilon_ 1 vdots epsilon_ n end bmatrix end align Our goal is to find beta such that Y X beta 2 is minimized. A jittering is added on the x axis to better see the distribution. DecisionTreeClassifier. com 11 grouped barplot area under the curve evaluation oversampled SMOTE not oversampled class weight oversampled SMOTE not oversampled class weight https python graph gallery. 617 Linear Regression 0. A kernel function is a function that implements the inner product in the new feature space given psi K textbf x textbf x prime langle psi textbf x psi textbf x prime rangle In this way we don t have to explicitly apply psi on our data and then compute the inner product. simple_linear_regression_scaled2. Our goal is to find begin align p C k x_1 dots x_n end align that is the probability of having the class k given the sample features considered independent. Practically begin align logit p X X cdot beta end align begin align p X logit 1 X cdot beta end align begin align p X S X cdot beta end align where p is the expected value of the prediction that in this case binary can be modeled as a bernoulli. Considering the hyperplane described by the vector w such that begin align L v langle w v rangle b 0 space space w 1 end align the distance of a point fom the hyperplane L can be evaluated in that way begin align d x L langle w x rangle b end align while the distance between two points of two different classes on the margin is begin align frac left W right frac space b space b left W right frac 2 left W right end align So we need to find w and b such that that distance is maximized considering the whole training set and at the same time all the points are classified correctly. For RBF kernels gamma is the coefficient multiplied to the squared norm of x x prime while for polynomial kernel in the libsvm implementation the one used by sklearn library the kernel function is gamma langle textbf x textbf x prime rangle 1 k Gridsearch model selection is done with KFold crossvalidation. One simple example is the polynomial mapping A degree k polynomial is p x sum_ j 0 k w_jx j space that can also be seen as space langle w psi x rangle space space where space space psi x 1 x x 2. 406 end align begin align E anaemia 0 DEATH _EVENT 0 P anaemia 0 DEATH _EVENT 0 times N 0. html exploits entropy estimation from k nearest neighbors distances. Firstly we plot the numerical features omitting time because is not used in the prediction. Polynomial kernel best models RBF kernel best models K Nearest Neighbors KNN model tries to classify new points according to the class of the nearest neighbors. begin align p X_i x_i C k frac 1 sqrt 2 pi sigma_ k 2 e frac x_i mu_k 2 2 sigma_k 2 end align where x_i is the observation value and mu_k and sigma_k 2 are respectively the sample mean and sample variance of the values in X_i associated with class k. 574 Poly SVM 0. Model Holdout Original Holdout Oversampling Holdout SMOTE Holdout class weight balanced KFold Original KFold Oversampling KFold SMOTE KFold class weight balanced Decision Tree 0. Linear kernel Polynomial and RBF kernels Gridsearch on C and gammaGamma is a kernel coefficient used in both polynomial and RBF kernels. 9 end align Then we know that begin align X 2 sum_ i 1 k frac o_i e_i 2 e_i end align where o_i is the observed value and e_i is the expected value of the i th combination of features. In this case I decided to perform it anyway to compare the results with another regression technique called logistic regression that instead is very suitable for binary classification. Bayes theorem tells us that begin align p C k X x frac p C k p X x C k p X x end align Basically we have to compute the feature distributions given the class p X x C k while p X x in this case is called evidence and is evaluated in this way p X x sum_k p C k p X x C k. To tackle this problem it s possible to estimate the distributions without trying to fit an already known one using kernel density estimation. Then the results are combined averaged. Chi squared test is performed starting from the contingency table for instance that indicates how samples are distributed among these two features. Gaussian Naive BayesAnyway even if the hypothesis seems to be not respected a Gaussian Naive Bayes is performed. In this way the statistic for anaemia is 0. The first one is holdout that consists in dividing the dataset into subsets dedicated to training validation and test. Equivalently begin align min_ w b frac 1 2 w 2 space space s. This forces the models to select different features increasing uncorrelation. Once alpha vector is obtained support vectors correspond to samples that have an alpha value that is greater than zero so to describe the model only those samples are needed. org wiki Chi squared_test Yates s_correction_for_continuity consists in subtracting 0. ipynb Heart Failure clinical records Heart failure occurs when the heart is not able to pump enough blood to the body. To tackle this problem it s possible to train an ensemble of decision trees called random forest. Now let s apply it on our actual classification task. 1186 s12911 020 1023 5 D. 536 RBF SVM 0. Here we can see how the bandwidth works. end align so begin align p X frac e X beta 1 e X beta end align regression. Validate the models To validate and test our models we can have different approaches. Here it s possible to see how the metrics change according the prior on X axis and bandwidth on the slider. In the Gaussian Naive Bayes we assume that our continouos values associated with the class are distributed according to a normal distribution. To test it a Shapiro Wilk test is performed on all numerical features. png In this way every prediction is bounded between 0 and 1 assuming a probabilistic meaning. Then to compare the results also the classic Gaussian Naive Bayes is tested. In this notebook the analisys will be done starting from an EDA to understand the dataset and applying some preprocessing to be able to learn properly from it. As we will see during feature selection this is not a problem since one of the two will be dropped. Due to the nature of this model the explainability is quite high. This can leads to biased results that can be noticed by measures such as recall precision or f1. This is an example of how a two feature decisison tree work. ResultsFrom the results we can say that smoking high_blood_pressure diabetes and platelets can be easily dropped. 611 Gaussian Naive Bayes 0. 693 KNN original 0. Machine learning applied to medical records can be useful to predict the survival of a patient highlighting patterns and even ranking the features to understand which are risk factors possibly undetectable by doctors. In fact in a polynomial mapping on a rm I R 2 space psi textbf x x_1 2 x_2 2 sqrt2x_1x_2 considering only the quadratic features This means that in the cases in which there are a lot of features mapping every time from the original to the new space could be costly second order features in 1000 dimensions are around 5 cdot10 5 numbers The solution is using kernel functions. Then the same algorithm will be tested on all the 7 features of the original oversampled and SMOTE dataset. It is a generalized linear model capable of fitting a linear equation to observed data. begin align min_ w b Big frac 1 2 W 2 color red C sum_ i 1 m xi_i Big space spaces. In the scipy implementation the statistic is further corrected with Yates correction for continuity that according to the documentation https docs. A further parameter h called bandwidth is used to control the smothing effect of the kde. Nearest neighbors are evaluated according to a distance metric function and for each new point only a fixed number of neighbors are taken into account. They are non parametric models that learn by recursively split the predictor space and so the train samples according to the best feature greedy approach until the tree reaches a constrained depth the subsets contain elements of only one class or it meets another stopping criterion e. HF are only a subgroup of all the cardiovascular diseases that comprehend also coronary heart diseases heart attacks cerebrovascular diseases strokes and other pathologies that altogether kill every year approximately 17 million people around the world. mutual_info_regression. begin align h bigg frac 4 hat sigma 5 3n bigg frac 1 5 approx 1. In some cases problems are not well separable on the original feature space but they are separable on another space. It consists in taking for each sample of the minority class the k nearest neighbours an then synthesize new samples starting from the sample and one of the nearest neighbours chosen randomly To show how SMOTE works let s plot the samples according to ejection_fraction and serum_creatinine. Formally begin align I X Y sum_ y in Y sum_ x in X p_ X Y x y log Bigg frac p_ X Y x y p_X x p_Y y Bigg end align begin align I X Y int_ y int_ x p_ X Y x y log Bigg frac p_ X Y x y p_X x p_Y y Bigg dx dy end align In this case the mutual information is estimated for each feature with respect to the class label DEATH_EVENT. Gini index is evaluated on both the splits weighted by the number of samples in each split and the feature that gives us the lower overall Gini index is chosen. 601 Flexible Bayes 0. References 1 https bmcmedinformdecismak. 5 from the absolute difference in the numerator. The overall results seem in line with the ones obtained in the reference paper 1 references but it s needed to keep in mind that the metrics are highly influenced by the small dimension of the dataset 75 samples in holdout validation set. Another approach consists in estimating the continouos distributions. The main drawback is that in the real world the vast majority of the problems are not linearly separable and an algorithm like this one would not converge. The best feature is the feature that if used to discriminate samples allows us to obtain the best possible split according to a measure. Basically each prediction in X beta is made by a linear combination of the features weighted by beta y_i sum_ j 0 p x_j beta_j that is the prediction for the i th observation according to the p predictors. In this case applying an hypotethical alpha value equal to 0. ", "id": "lorenzodenisi/heart-failure-clinical-records-analisys", "size": "44091", "language": "python", "html_url": "https://www.kaggle.com/code/lorenzodenisi/heart-failure-clinical-records-analisys", "git_url": "https://www.kaggle.com/code/lorenzodenisi/heart-failure-clinical-records-analisys", "script": "chi2_contingency chi2_test sklearn.feature_selection matplotlib.image train_test_split bernoulli IPython.core.display LinearRegression predict SMOTE animate_func SVR KFold() numpy sklearn.svm fit_predict seaborn accuracy_score LinearSegmentedColormap gridsearch recall_score mutual_info_classif evaluate gridspec chi2 colorFader f1_score SVC tree matplotlib.animation _make_combinations GaussianNB sklearn.neighbors norm sklearn.naive_bayes sklearn.tree sklearn sklearn.linear_model StratifiedKFold random sample make_meshgrid matplotlib.pyplot predict_proba DecisionTreeClassifier precision_score rand_jitter matplotlib.colors sklearn.model_selection pandas shapiro gaussian_kde roc_curve RandomForestClassifier LogisticRegression make_combinations MyNaiveBayes export_graphviz fit matplotlib scipy.stats KNeighborsClassifier HTML __init__ plot_contours sklearn.metrics sklearn.ensemble StandardScaler roc_area sklearn.preprocessing ", "entities": "(('we', 'different approaches'), 'validate') (('that', 'trees'), 'allow') (('that', 'validation'), 'be') (('we', 'classes'), 'be') (('mutual information', 'class label DEATH_EVENT'), 'begin') (('how tree', 'depth constraint'), 'see') (('that', 'accuracy'), 'seem') (('results', 'still quite other models'), 'see') (('it', 'kernel density estimation'), 's') (('that', 'p lower value'), 'compare') (('same subset', 'validation'), 'have') (('metrics', 'holdout validation set'), 'seem') (('results', '0'), 'minimize') (('how samples', 'two features'), 'perform') (('best model', '6'), 'seem') (('results', 'notebook'), 'present') (('C k p end Basically we', 'p X p k p X C sum_k C k.'), 'tell') (('dataset', 'patient Years'), 'collect') (('analisys', 'easy others'), 'be') (('when it', 'target'), 'be') (('true prediction', 'hyperplane'), 'forall') (('Now s', 'classification actual task'), 'let') (('implementation', 'N.'), 'base') (('Estimating', '1 alpha'), 'mix') (('we', 'them'), 'add') (('then model', 'margin'), 'describe') (('logistic that', 'instead very binary classification'), 'decide') (('Here we', 'rebalancing Hearth Disease different dataset'), 'result') (('we', 'thumb different bandwidths'), 'sigma') (('that', 'hyperplane'), 'consist') (('One thing', 'account'), 'diabetes') (('problem', 'alpha'), 'follow') (('results', 'line'), 'notice') (('statistic', 'documentation https docs'), 'correct') (('other that', 'world'), 'be') (('Other libraries', 'numbers random HTML'), 'click') (('Bandwidth rule', 'gaussian kernel'), 'be') (('80 patient', 'blood Boolean 0 1 kiloplatelets'), 'high_blood_pressure') (('risk factors', 'possibly doctors'), 'be') (('com 11 grouped barplot', 'SMOTE class SMOTE https python graph ms original oversampled weight balanced original oversampled class weight balanced gallery'), 'linear') (('linearly algorithm', 'margin hard which'), 'be') (('Being', 'linearly separable problem'), 'mean') (('prediction', 'probabilistic meaning'), 'png') (('we', 'features'), 'be') (('K that', 'last validation'), 'be') (('we', 'that'), 'be') (('This', 'wrong results'), 'be') (('T only textbf textbf', 'space space forall R n backslash'), 'be') (('KDE', 'observation'), 'weight') (('Machine learning', 'https alone 2020 2 arxiv'), 'predict') (('It', 'observed data'), 'be') (('only fixed number', 'account'), 'evaluate') (('x_j that', 'p predictors'), 'make') (('Naive Bayes Naive bayes classifier', 'features'), 'let') (('methods', 'iteration'), 'happen') (('K', 'observation'), 'begin') (('that', 'roc curve'), 'perform') (('statistic', 'anaemia'), 'be') (('linearly one', 'problems'), 'be') (('patient', 'them'), 'contain') (('smoking', 'high_blood_pressure diabetes'), 'resultsfrom') (('Decision tree Decision the bigger better trees', 'machine learning most widely known models'), 'be') (('conditional distributions', 'bernoulli estimations'), 'baye') (('that', 'HF'), 'follow') (('jittering', 'better distribution'), 'add') (('that', 'recall such precision'), 'lead') (('explainability', 'model'), 'be') (('observation 2 respectively sample', 'class k.'), 'begin') (('results', 'prior'), 'see') (('it', 'criterion stopping e.'), 'be') (('number', 'so case'), 'tend') (('This', 'normality class'), 'be') (('ejection_fractions', 'them'), 'seem') (('they', 'normal distribution'), 'assumption') (('align displaystyle S x', 'logit'), 'call') (('link function', 'linear regression'), 'be') (('only samples', 'model'), 'obtain') (('one', 'two'), 'be') (('online version', 'notebook'), 'be') (('so it', 'KFold'), 'consist') (('vast majority', 'features anaemia CPK sex ejection_fraction'), 'see') (('where it', 'overfitting'), 'allow') (('they', 'space'), 'be') (('Then same algorithm', 'original oversampled'), 'test') (('end 406 align', 'E P align anaemia 0 DEATH _ EVENT 0 anaemia'), 'begin') (('7861 patient', 'contraction'), 'diabetes') (('features', 'mutually independent target'), 'need') (('some', 'hypothesis'), 'be') (('kiloplatelets mLLet', 'datasetFor brevity creatinine_phosphokinase'), 'death_event') (('logit', '0 1 whole real line'), 'stretch') (('quite it', 't scale'), 'be') (('bar where x', 'training samples'), 'begin') (('forest Random Random forest', 'decision trees'), 'be') (('sum _ j G 1 2 where p', 'total samples'), 'be') (('it', 'accuracy'), 'be') (('tree', 'average 63'), 'train') (('always higher values', 'stronger dependence'), 'be') (('j K _ _ j', 'G _ such i'), 'function') (('prediction', 'margin'), 'have') (('they', 'statistic'), 'produce') (('a_i', 'normal distributions'), 'begin') (('that', 'imbalance'), 'be') (('RBF K Nearest Neighbors KNN Polynomial kernel best best model', 'nearest neighbors'), 'model') (('We', 'f1 score increase'), '606') (('results', 'powerful classification'), 'see') (('it', 'feature distributions'), 'assume') (('around 5 cdot10 5 solution', 'kernel functions'), 'in') (('normal also other functions', 'such triangular Epanechnikov'), 'be') (('one that', 'beta intercept _'), 'have') (('approach', 'continouos distributions'), 'consist') (('binary', 'bernoulli'), 'begin') (('model', 'bernoulli distributions'), 'implement') (('This', 'feature decisison tree how two work'), 'be') (('1 analisys', 'only two features'), 'confirm') (('recall precision ROC Moreover curve', 'metrics following accuracy'), 'be') (('how features', 'label'), 'distribution') (('org Chi squared_test s_correction_for_continuity', '0'), 'wiki') (('dataset', 'binary features'), 'compose') (('technique', 'Flexible Bayes'), 'propose') (('that', 'Gini lower overall index'), 'evaluate') (('Smote Synthetic Minority Oversampling kde Technique', 'useful unbalanced datasets'), 'be') (('Here we', 'class only represented samples'), 'see') (('it', 'probabilistic meaning'), 'suggest') (('This', 'class distribution'), 'represent') (('class', 'sample features'), 'be') (('us', 'measure'), 'be') (('when we', 'data'), 'be') (('functionThe polynomial problems', 'multiple predictors'), 'psi') (('Mutual information', 'documentation https scikit'), 'estimate') (('n_i', 'class i.'), 'set') (('k', 'original oversampling smote original oversampling smote'), 'com') (('most informative ones', 'features'), 'seem') (('this', 'cross validation'), 'see') (('who', 'heart failure'), 'animation') (('x_i', 'margin'), 'forall') (('it', 'new data'), 'be') (('quite it', 'iteration'), 'be') (('we', 'then inner product'), 'be') (('classes', 'them'), 'be') (('k end 1 C where evidence', 'class'), 'be') (('that', '0'), 'be') (('P 0 DEATH _ 0 anaemia', 'so align P anaemia'), 'be') (('RBF Gridsearch', 'kernel polynomial kernels'), 'kernel') (('parameterAnother way', 'class imbalance'), 'be') (('you', 'important information'), 'achieve') (('class weight random oversampling holdout', 'https python graph oversampled gallery'), 'need') (('Linear Regression Linear regression', 'machine learning'), 'be') (('us', 'training'), 'allow') (('anaemia 95 Decrease', 'blood mcg'), 'be') (('we', 'boxplot'), 'plot') (('it', 'inner products'), 'solve') (('we', 'further dependence'), 'square') (('parameter p', 'minkowski _ 1 Big p Big 1 which'), 'be') (('This', 'uncorrelation'), 'force') (('Also RBF', 'KFold'), 'provide') (('we', 'serum_creatinine'), 'be') (('Naive Bayes', 'Then results'), 'test') (('hopefully it', 'which'), 's') (('Prediction', 'majority then voting'), 'do') (('continouos values', 'normal distribution'), 'assume') (('space where space', 'space w psi rangle space also langle space'), 'be') (('how metrics', 'slider'), 's') (('analisys', 'properly it'), 'do') (('that', 'unit variance'), 'report') (('operation', 'mapping'), 'be') (('we', 'platelets'), 'see') (('K times part', 'validation'), 'do') (('feature', 'independently final prediction'), 'be') (('that', 'age more sex'), 'find') (('it', 'different techniques'), 's') (('Shapiro Wilk test', 'numerical features'), 'perform') (('only subset', 'total number'), 'consider') (('gamma langle k Gridsearch model textbf textbf prime 1 selection', 'KFold crossvalidation'), 'be') (('Furthermore where is', 'sometimes other techniques'), 'increase') (('now we', 'target label'), 'test') (('very computation', 'records'), 'in') (('lower value', 'stronger independence'), 'mean') (('when heart', 'body'), 'occur') (('we', 'table'), 'hold') (('link function', 'linear predictors'), 'begin') (('I', 'unnormalized features'), 'train') (('observed e_i', 'features'), 'align') (('split policies', 'so clearer explaination'), 'see') (('accuracy', 'f1 parenthesis'), 'show') (('we', 'sample'), 'compute') (('logistic regression', 'very binary classification'), 'be') (('decision trees', 'dataset original bagging'), 'train') (('points', 'same time'), 'consider') (('SMOTE', 'others'), 'perform') (('that', 'training set'), 'consider') (('sex 148 1 patient', '0 1 time period'), 'woman') (('probabilities', 'sample'), 'multiply') (('Firstly we', 'prediction'), 'plot') (('configuration', 'class sklearn documentation https scikit'), 'be') (('Kernel density', 'observations'), 'be') (('method', '1'), 'call') (('set', 'unobservable random variables'), 'base') (('how prediction', 'step'), 'be') (('Y X such beta', 'beta'), 'begin') (('it', 'decision trees'), 's') (('s', 'ejection_fraction'), 'consist') (('Best models', 'validation then set'), 'train') (('parameter further h', 'kde'), 'use') (('align min _ w Equivalently b', 'w space space frac 1 2 2 s.'), 'begin') (('that', 'sex'), 'matrix') (('we', 'data leakage'), 'perform') (('We', 'rebalanced datasets'), 'train') "}