{"name": "build your first deep neural network ", "full_name": " h3 This notebook is a tutorial to get beginners started with building first deep neural network using Keras and Tensorflow Feel free to suggest edits and shortcomings of the notebook Do show your support if you like the notebook h1 About Keras h1 Table of contents h1 Building a single layer perceptron h3 Some most commonly used activation functions are shown here h4 By now we have created our first single layer perceptron But the work might not be visible yet h2 Building a multilayer perceptron h4 We will build a MLP with 2 hidden dense layers and another dense layer for the output h3 Configurations of a model h4 Listed below are different optimizers loss functions and metrics For more information you can always refer to the documentations here here and here h2 Voila We have created our first mutiple layer deep neural network which is actually learning from the data and giving you results h1 Deep Neural Network on Fashion MNIST data h3 Plot some sample images from the dataset h3 Build a deep neural network on this dataset h3 Compile the model h3 Train your model h3 Plot the training history visualisation h4 Seems the model is overfitting here An ideal loss plot should look similar to this h4 But at least we have managed to build our first deep neural network Hurray We have come a long way h1 Managing Model Overfitting h3 Weight Regularization h3 Dropout h3 Early Stopping h1 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "Early Stopping The training could be stopped at an epoch where there isn t any improvement of accuracy or loss for a specific number of rounds. Overfitting could be handled in many ways. 001 as regularization value as kernel regularizer Add l2 regularizer with 0. Dropout applied as a hidden layer randomly drops out a number of output features of the layer during training. 1 Weight Regularization wr 4. Configurations of a model Loss functions are used to compare the network s predicted output with the real output in each pass of the backpropagations algorithmCommon loss functions are mean squared error cross entropy and so on. Generate some random data Input dimension should be equal to the number of features The output should be a single outcome so one Dense layer is defined with a single unit. Fit the model created above to training and validation sets. MLPs are fully connected with each node connected each node in the other layer. org api_docs python tf metrics Voila We have created our first mutiple layer deep neural network which is actually learning from the data and giving you results. For more information you can always refer to the documentations here http www. 3 probability Add a dropout layer with 0. Compile the above created model Optimises the learning by updating the weights with Stochastic Gradient Descent method. The Optimizer determines the update rules of the weights. 4 Train the model tm 3. Define a hidden layer with single perceptron. But at least we have managed to build our first deep neural network. Build a deep neural network on this dataset. Weight regularization could be done with 2 different techniques L1 regularization or Lasso Regression L2 regularization or Ridge Regression In tf. The number of units in the last layer should always be the number of classes in which we have to classify our input data. Plot some sample images from the dataset. An activation function in a neural network provides non linearity to the data which is important for learning features from the input data else the learning will stop at a particular stage and leads to a dying neuron problem. Conclusion conc Building a single layer perceptron A sigle layer perceptron can be thought of as a single learning unit in the network. io losses and here http www. The parameters haven t been tuned yet which could open doors to more accurate models. org api_docs python tf keras optimizers here http keras. Dropout Dropout is another most commonly used regularization techniques for neural networks. This notebook is a tutorial to get beginners started with building first deep neural network using Keras and Tensorflow. This is called weight regularization and it is done by adding to the loss function of the network a cost associated with having large weights. At test time no units are dropped out and instead the layer s output values are scaled down by a factor equal to the dropout rate so as to balance for the fact that more units are active than at training time. 001 as regularization value as kernel regularizer Add a Dense layer with number of neurons equal to the number of classes with softmax as activation function Compile the model created above. Deep Neural Network on Fashion MNIST data. A diagram below gives a taste of what an MLP looks like. 1 Activation Functions af 2. You can set verbose to 1 to get the status of your model training 2 to get one line per epoch here I kept it 0 to keep the notebook precise. Plot training validation accuracy values Plot training validation loss values Build the model Add l2 regularizer with 0. 5 Plot the training history visualization pthv 4. Feel free to suggest edits and shortcomings of the notebook. By now we have created our first single layer perceptron. Managing Model Overfitting mmo 4. Fashion MNIST is a dataset of Zalando s article images consisting of a training set of 60 000 examples and a test set of 10 000 examples. It allows for easy and fast prototyping and supports both convolutional networks and recurrent networks. But the work might not be visible yet. This node will get inputs from the input layer perform the learning task along with activation function. keras weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Building a multilayer perceptron. The dropout rate is the fraction of the features that are being dropped it is usually set between 0. Building a multilayer Perceptron mlp 2. We will use the Fashion MNIST dataset for our purpose. Deep Neural Network on Fashion MNIST data dnn 3. Image Courtesy University of Genoa Listed below are different optimizers loss functions and metrics. This is stored in a variable because the output of fit function is a history class which consists of 4 key value pairs for accuracy val_accuracy loss val_loss split training set into training set and validation set using train_test_split provided by scikit learn The items in the dataset are to be classified into 1 of the 10 classes. Its most important features are user friendliness modularity easy extensibility Table of contents1. 1 Plot sample images from the dataset psi 3. Still there are a lot of things which could be done to improve the accuracy of the model since the current model only gives an accuracy of 87 on training set and 85 on the test set. io callbacks The callbacks parameter of the fit function is responsible to handle the Early Stopping. Now it s time to work on a real data and build a more robust deep neural network. 2 Build a DNN bdnn 3. We will build a MLP with 2 hidden dense layers and another dense layer for the output. Building a single Layer Perceptron slp 1. These specific number of rounds is called patience. 3 Compile the model ctm 3. But we will end this notebook here and leave you all to play with the parameters and improve the scores achieved above Cheers Define the number of inputs and outputs This function allows you to create a sequantial model a stack to which you can add as many dense layers as you wish. Metrics are used to evaluate a model common metrics are precision recall accuracy auc. Hurray We have come a long way. Managing Model Overfitting Now that we have trained our very first model it s time to optimise the model training dealing with the problem of overfitting. Compile the model Train your model Plot the training history visualisation Seems the model is overfitting here. Call the plot_history function to plot the obtained results Evaluate the results Build the model Add a dropout layer with 0. Each example is a 28 28 grayscale image associated with a label from 10 classes. Below is an idea on how different optimizers work. Some most common ways we will see here include Weight Regularization Dropout Early Stopping or Callbacks Weight Regularization One of the most common way is to put constraints on the complexity of a network by forcing its weights only to take small values which makes the distribution of weight values more regular. Train the model by fitting the train data to the model we compiled in the above line. Even if we declare a total of 1 000 epochs the training will stop according to the patience once it finds no improvement in the accuracy. Some most commonly used activation functions are shown here. 2 Dropout drop 4. 3 Early Stopping es 5. Conclusion There we go We have been able to optimise this model to reduce the problem of over fitting as much as possible. 1 Configurations of a model cm 3. 3 probability Early stopping for more refer documentation here https keras. About KerasKeras is a high level neural networks API written in Python and capable of running on top of TensorFlow. Do show your support if you like the notebook. An ideal loss plot should look similar to this. This function flattens the input data Feel free to play around with different parameters here like number of units in each layer or switching the activation function or increasing decreasing the number of layers. ", "id": "divyansh22/build-your-first-deep-neural-network", "size": "7377", "language": "python", "html_url": "https://www.kaggle.com/code/divyansh22/build-your-first-deep-neural-network", "git_url": "https://www.kaggle.com/code/divyansh22/build-your-first-deep-neural-network", "script": "train_test_split tensorflow.keras.layers tensorflow.keras.models numpy SGD Adam Adadelta Dropout Dense regularizers RMSprop tensorflow matplotlib.pyplot tensorflow.keras.callbacks ModelCheckpoint Sequential sklearn.model_selection Adagrad EarlyStopping Adamax tensorflow.keras tensorflow.keras.optimizers plot_history Flatten Nadam ", "entities": "(('deep neural which', 'results'), 'python') (('you', 'here www'), 'refer') (('specific number', 'rounds'), 'call') (('We', 'purpose'), 'use') (('loss ideal plot', 'this'), 'look') (('Weight regularization', 'L1 Lasso Regression L2 Ridge tf'), 'do') (('Now it', 'more robust deep neural network'), 's') (('model', 'training history visualisation'), 'compile') (('more units', 'training time'), 'drop') (('it', 'usually 0'), 'set') (('We', 'over fitting'), 'conclusion') (('it', 'accuracy'), 'stop') (('here I', 'notebook'), 'set') (('most important features', 'user friendliness extensibility modularity easy contents1'), 'be') (('Compile', 'Stochastic Gradient Descent method'), 'optimise') (('haven which', 'more accurate models'), 'tune') (('inputs', 'activation function'), 'get') (('About KerasKeras', 'TensorFlow'), 'be') (('distribution', 'weight values'), 'include') (('perceptron sigle layer perceptron', 'network'), 'conc') (('example', '10 classes'), 'be') (('Early training', 'rounds'), 'stop') (('It', 'convolutional networks'), 'allow') (('activation function', 'model'), 'as') (('you', 'as many dense layers'), 'end') (('it', 'overfitting'), 'overfitte') (('Overfitting', 'many ways'), 'handle') (('Plot sample 1 images', '3'), 'psi') (('io callbacks', 'Early Stopping'), 'callback') (('MLP', 'what'), 'give') (('current model', 'test set'), 'be') (('Plot training validation loss values', '0'), 'value') (('Dense single so one layer', 'single unit'), 'be') (('we', 'first single layer'), 'create') (('Fashion MNIST', 'test 10 000 examples'), 'be') (('Configurations', 'loss functions'), 'use') (('weight it', 'large weights'), 'call') (('items', '10 classes'), 'store') (('we', 'above line'), 'train') (('we', 'input data'), 'be') (('common metrics', 'model'), 'use') (('We', 'dense output'), 'build') (('input data', 'layers'), 'flatten') (('Dropout', 'training'), 'apply') (('beginners', 'Keras'), 'be') (('Optimizer', 'weights'), 'determine') (('at least we', 'first deep neural network'), 'manage') (('3 probability', '0'), 'add') (('Dropout Dropout', 'regularization most commonly used neural networks'), 'be') (('keras weight regularization', 'keyword arguments'), 'add') (('else learning', 'neuron dying problem'), 'provide') (('you', 'notebook'), 'show') (('fully node', 'other layer'), 'connect') (('Image Courtesy University', 'Genoa'), 'be') (('python', 'here keras'), 'http') "}