{"name": "notebook1cc9bab3f3 ", "full_name": " h1 Deep Learning in Biomedical Engineering h2 Assignment 2 h3 Due date 11 59 pm Febraury 25 2021 h2 Seyedhamidreza Alaie sa3724 h2 Introduction h2 Instructions h2 How to submit h3 1 5 pts According to the CIFAR10 dataset descriptions and other online resources please identify the quantities below h1 1 B Therefore total number of classes are 10 shown above h2 c therefore the size of each image is 32x32 above h3 2 0 pts Import the required packages in the following code block h3 3 5 pts Load train and test sets using Pytorch datasets functions h3 4 10 pts Using the matplotlib library make a figure with N times4 grid cells where N is the number of classes Display one random sample of each class pulled from the train set in its corresponding row that depends on its class index and the first column and its histogram in the second column Repeat this for the third and fourth columns but pull images from the test set h3 5 5 pts Split up the train set into new train and validation sets so that the number of samples in the validation set is equal to the number of samples in the test set Then create a DataLoader for each set including the test set with a batch size of 32 h3 6 5 pts Display the number of samples for each class in the train validation and test sets as a stacked bar plot similar to the FirstTutorial h3 7 10 pts According to the LeNet architecture below create a fully connected model Also identify the architeture s hyper parameters activation functions and tensor shapes h3 8 5 pts Create an instance of ADAM optimizer with an initial learning rate of 0 0001 and an instance of Mean Squared Error MSE loss function Briefly explain the ADAM optimizer algorithm and the MSE loss function h3 9 15 pts Train the model for 200 epochs using the train set and validate your model s performance at the end of each epoch using the validation sets h3 10 5 pts Display the learning curve and illustrate the best epoch Explain your criteria for the best epoch h3 12 5 pts Display five random samples of each class titled with the true label and the predicted label Comment on your model s performance h3 13 20 pts Repeat the training validation and testing with the Cross Entropy loss function and initial learning rate of 0 005 Explain how the model s performance changed ", "stargazers_count": 0, "forks_count": 0, "description": "html We check if the datasets are transformed to tensors with 0 1 value rangeTherefore they are stored as tensr see above. 8 vdots end bmatrix shows the probability of belonging to each class for the same sample and predicted by the model. Please remove the comments before filling the blocks. plot 24 images from each category seacrhing through the images and finding the lowest and highest value in each image array loading the datasets CIFAR10 checking the min max values in tensors fro sample 1 to 100 hh2 is a list of 10 lists each listing the images for the gaegories 0 to 9 hh2t is a list of 10 lists each listing the images for the gaegories 0 to 9 in the test data calculate the histogram of a numpy array function plot on random image from training and one from test data for each category and plot their histograms reshaping the labels in the train dataset from bacth 1 to bacth 10 reshaping the labels in the validation dataset from bacth 1 to bacth 10 loading the training and test datasets loading the test dataset looking at a label in he train dataset. To avoid overwriting your previously trained model change the save directories in the training loop. ipynb file File Download 5. The Pytorch CIFAR10 https pytorch. MSELoss loss function. Looking at the misclassified samples their classification is difficult with for human. Load train and test sets using Pytorch datasets functions. According to the CIFAR10 http www. Otherwise the weights would be the same as the last epoch or the best epoch in the last part. Just remember they are using a different architecture and they are using TensorFlow for implementations. e Intensity rangeA total Number of samples are 10k and 50k for test and training data as shown bellow 1 B Therefore total number of classes are 10 shown above c therefore the size of each image is 32x32 above 1 d based on the data plotted bellow it looks that class 0 airplane 1 car 2 bird 3 cat 4 elk deer 5 dog 6 frog 7 horse 8 ship 9 Truck bold text bold text 1 e searching through pixel values of 100 images shows that the value ranges from 0 o 255 bellow 2. If you are using an online code or paper make sure you cite their work properly to avoid plagiarism. Display the number of samples for each class in the train validation and test sets as a stacked bar plot similar to the FirstTutorial https www. com roblexnana cifar10 with cnn for beginer you may find useful information about how your outputs must look. Using other students works is absolutely forbidden and considered cheating. First I take a look at the images to ensure that they look fine 10. org docs stable optim. Download your completed notebook as a. Briefly explain the ADAM optimizer algorithm and the MSE loss function. Import the required packages in the following code block. Although you may use other available online resources such as GitHub Kaggle Notebooks it is highly recommended to try your best to do it yourself. However the model with cross entropy trains faster and slightly oytperforms accuracy 60. You can always add more blocks if you need to or it just makes your answers well organized. Using this formt we can define the MSE loss L2 distance between the labels and output tensors. og for learning and code writen here IntroductionIn this assignment you will implement train and test LeNet https en. For other optimization algorithms and loss functions check the links below Optimizers list https pytorch. Obviously you don t need to re import the dataset and the libraries. Based on pytorch s help CrossEntropyLoss also act as a softmax layer therefore a softmax layer is not required In this case I observe similar performance for both models. InstructionsDepending on each question there are empty blocks you need to fill. Deep Learning in Biomedical Engineering Assignment 2 Due date 11 59 pm Febraury 25 20211. This is consitent with the knowledge that logistic regressions and therefore classification is advantageous for classification of linearly seperable data. To increase the training speed use the GPU accelerator. The performance is worse 43 accuracy while the model has more weights than LeNEt 57 accuracy. You can always come back here and import another package please keep them all in the following block to make your code well organized. Copy the Public URL 4. Make the saved version public Share Public 3. The criteria for the best epoch can be the minimum loss or maximum accuracy or other criteria. ipynb files on the CourseWorks https courseworks2. org wiki LeNet model to classify CIFAR10 http www. It also includes a dropout layer to minimize overfit. Repeat the training validation and testing with the Cross Entropy https pytorch. Seyedhamidreza Alaie sa3724In preparing this notebook I used the following resources https pytorch. The markdown blocks are only for plain or laTex text that you may use for answering descriptive questions. if cuda our_CNN. The MSE loss function is L y hat y frac 1 N sum_ i 1 N y_i hat y _i 2 Where y is the true value hat y is the predicted value N is the number of classes. Although in the training process softmax activation function was used based on the type of loss the network needs a softmax function for classification. Look at the FirstTutorial https www. Do not forget to save the model at the end of each epoch. To fix this issue once solution is to reshape the labels such that the lables and the LeNET s output have the same shape i. LeNet Architecture https raw. Architecture hyper parameter includes the number of layers number of kernels in each layer size of the kernels stride zero padding size. Alternatively I could have change the CNN output s shape to batch 1 to match the lables. This is consistent with the knowledge that densenets are inefficent and inferior to CNNs for image classification. html algorithms Loss function list https pytorch. Both this methods should give similar results. Train the model for 200 epochs using the train set and validate your model s performance at the end of each epoch using the validation sets. However you need to create a new instance of the architecture. Display five random samples of each class titled with the true label and the predicted label. Therefore min and max value of each tensor is between 0 and 1 above http 4. Just by looking at the architecture itself you should be able to identify the hyper parameters. One comment line at the top of each code block is necessary to explain what you did in that block. hh is a list of 10 lists each listing the images for the gaegories 0 to 9 hh is a list of 10 lists each listing the images for the gaegories 0 to 9 looking at sample 2 in the category 1. Save a version You can use Quick Save to avoid re running the whole notebook 2. In this Kaggle Notebook https www. Comment on your model s performance. png Note that a model with kernel size of 3 in the first convolution layer also works well 62 accuracy. org tutorials beginner blitz cifar10_tutorial. Load the model s weights at the best epoch and test your model performance on the test set. Describe your criteria for choosing the best epoch I choose the epoch with the lowest validation loss MSE as the optimal choice. Essentially you need to copy all the codes above and just change the loss function and edit the learning rate. com soroush361 deeplearninginbme firsttutorialI also used pytorch. In the majority random samples from the test dataset the model performed well. html dataset descriptions and other online resources please identify the quantities below a Total Number of samples b Number of classes c Image size d Write class names and their corresponding label index e. Display the learning curve and illustrate the best epoch. com soroush361 deeplearninginbme firsttutorial. Adam with an initial learning rate of 0. Make sure none of the samples in the validation set exists in the new train set. Moreover these models are older models than CNNs. org docs stable generated torch. The model misclasify birds the most 13. For more help look at this implementation https github. For evaluation the L2 distance between the labels and the CNN output is the metric for classification. Create an instance of ADAM optimizer https pytorch. 0001 and an instance of Mean Squared Error MSE https pytorch. You may use transformers while downloading the dataset to do the job. Name your variables properly that represent the data they are holding such as test_set. Using the matplotlib library make a figure with N times4 grid cells where N is the number of classes. Also identify the architeture s hyper parameters activation functions and tensor shapes. Extra credit 3rd architecture I eimplemented the densenet archrictecture 5 dense layers without any CNNs. Upload the Public URL and the. com soroush361 deeplearninginbme firsttutorial for more help. Implementing and reporting results using other architectures than LeNet will grant you an extra 20 on grade. Explain your criteria for the best epoch. I imlemenetd this distance bellow for evaluation of the test data. Split up the train set into new train and validation sets so that the number of samples in the validation set is equal to the number of samples in the test set. Make sure the intensity range is between 0 1 and images are stored as tensor type. Don t comment on every detail. For your information here is the mathematics behind the ADAM optimizer For each parameter w j v_t beta_1v_ t 1 1 beta_1 g_t s_t beta_2s_ t 1 1 beta_2 g_t 2 Delta w j eta frac v_t sqrt s_t epsilon g_t w j_ t 1 w j_t Delta w j Where eta is the initial learning rate g_t is the gradient at time t along w j v_t is the exponential average of gradients along w j s_t is the exponential average of squares of gradients along w j beta_1 beta_2 are the hyper parameters and epsilon is a small number to avoid dividing by zero. The same function was also used in the training loss. To evaluate the proformance on the test data I used the following. The output shape of labels is bacth 1 whilethe output of the CNN is batch 10. Look at this tutorial Pytorch CIFAR10 https pytorch. Keep in mind that you identified the W H and N Which refers to the number of classes in the first question. The learning curve shows the model s loss and accuracy at the end of each epoch for all epochs 200 epochs. Although this model is old and known to be worse than CNNs comparison is insightful. html tutorial will also help you here. How to submit After you have completed the assignment 1. Howevr I implemented the first oneTest dataset does not need the aforementioned reshaping because is not used in the training loop. However It is my understanding that kernel 5 is what is asked for given the input size is 32x32x3. cuda functions to show an image get some random training images show images create a directory for the first model saving generate the CNN model train the model loop over the dataset multiple times get the inputs data is a list of inputs labels zero the parameter gradients forward backward optimize plot the validation and training loss find the epoch woth the lowesr validation loss and load that model weights load all test data into one bacth for evaluation take the first only bactch from the loader calculated the pridction lowesr L2 distance from the labels and store them in Prediction and truelabels from the test data print the confusion tableand acuracy of the test data take one batch of 256 samples from the test data plot 5 test images from each category and print the prediction and theground truth get some random training images show images create another CNN with the same architecture but differnt learning rate loop over the dataset multiple times get the inputs data is a list of inputs labels zero the parameter gradients forward backward optimize plot the validation and train loss choose the best epoch with the lowes validation loss using the lowest logit distance from the labels calassify the images and store the predictions in Prediction print confusion matrix extranet is a densnet of 5 layes without concolutional layers as the extra credit use the defult learning rate and use the crossentroppy loss train the 3rd network loop over the dataset multiple times get the inputs data is a list of inputs labels zero the parameter gradients forward backward optimize plot training and validation loss choose the poch with the lowest validation loss using the lowest logit distance from the labels calassify the images and store the predictions in Prediction print confusion matrix and accuracy. Explain how the model s performance changed. As such the model performs resonably well. Write comments for your codes in the big picture mode. Repeat this for the third and fourth columns but pull images from the test set. List item List item Kernel Size activation functions and maxpool kernel and bias are some of the hyper parameters that I dentified 8. For ADAM optimizer keep other arguments as default. com icpm pytorch cifar10 blob master models LeNet. DataLoader for each set including the test set with a batch size of 32. CrossEntropyLoss loss function and initial learning rate of 0. Currently the markdown blocks have a comment like your answer here. Display one random sample of each class pulled from the train set in its corresponding row that depends on its class index and the first column and its histogram in the second column. Keep in mind that y is a one hot vector like y begin bmatrix 0 0 1 vdots end bmatrix This example of y indicates that the sample belongs to class ID 2 remember it is zero indexed and hat y begin bmatrix 0. html loss functions Explaine ADAM and MSE here 9. Samples must be pulled from the test set. Its original shape matches the CNN output for MSE loss calculation make lists of different classes in test training and validation datasets calculate number of samples in each class and dataset If GPU available move the model to GPU. 57 The model performs reasonably well with a 57 accuracy. com soroush361 DeepLearningInBME main Ass1_Arch1. According to the LeNet architecture below create a fully connected model. The code blocks are only for Python codes and comments and currently have a comment like Your code here. In other word we choose the logit distance from the lables as a mean for classification. Display the confusion matrix and classification report. Then create a DataLoader https pytorch. ", "id": "seyedhamidrezaalaie/notebook1cc9bab3f3", "size": "12793", "language": "python", "html_url": "https://www.kaggle.com/code/seyedhamidrezaalaie/notebook1cc9bab3f3", "git_url": "https://www.kaggle.com/code/seyedhamidrezaalaie/notebook1cc9bab3f3", "script": "torch.nn.functional Extra_DenseNet(torch.nn.Module) matplotlib.pyplot __init__ imshow GetHistograms forward confusion_matrix torch.nn sklearn.metrics LeNet(torch.nn.Module) accuracy_score numpy ", "entities": "(('I', '8'), 'be') (('you', 'block'), 'be') (('model', 'reasonably well 57 accuracy'), '57') (('Moreover models', 'older CNNs'), 'be') (('tensr', 'value 0 1 rangeTherefore'), 'check') (('code', 'following block'), 'come') (('therefore classification', 'linearly seperable data'), 'be') (('I', 'models'), 'act') (('asked', 'input size'), 'be') (('empty you', 'question'), 'be') (('criteria', 'best epoch'), 'be') (('Alternatively I', 'lables'), 'change') (('code blocks', 'code'), 'be') (('Therefore min value', 'http'), 'be') (('You', 'job'), 'use') (('none', 'train new set'), 'make') (('same function', 'training also loss'), 'use') (('Obviously you', 'don dataset'), 'need') (('loss functions', 'https pytorch'), 'check') (('I', 'following'), 'use') (('However model', 'faster accuracy'), 'oytperform') (('they', 'images'), 'take') (('works', 'other students'), 'forbid') (('Samples', 'test set'), 'pull') (('number', 'test set'), 'split') (('distance', 'test data'), 'imlemenetd') (('network', 'classification'), 'need') (('they', 'such test_set'), 'name') (('output shape', '1 whilethe CNN'), 'be') (('How you', 'assignment'), 'submit') (('soroush361 deeplearninginbme firsttutorialI', 'also pytorch'), 'com') (('you', 'descriptive questions'), 'be') (('Otherwise weights', 'best last part'), 'be') (('how outputs', 'useful information'), 'roblexnana') (('model', 'test'), 'perform') (('it', 'it'), 'use') (('You', 'whole notebook'), 'save') (('hyper parameters', 'small zero'), 'be') (('I', 'optimal choice'), 'describe') (('GPU', 'GPU'), 'match') (('they', 'implementations'), 'remember') (('we', 'labels'), 'define') (('just answers', 'always more blocks'), 'add') (('Architecture hyper parameter', 'kernels stride zero padding size'), 'include') (('writen here assignment you', 'LeNet https'), 'og') (('such lables', 's same shape'), 'be') (('LeNet', 'grade'), 'grant') (('html dataset', 'classes c Image size d Write class names'), 'identify') (('I', 'resources https following pytorch'), 'sa3724in') (('we', 'classification'), 'choose') (('you', 'properly plagiarism'), 'make') (('It', 'overfit'), 'include') (('densenets', 'image classification'), 'be') (('markdown Currently blocks', 'answer'), 'have') (('Which', 'first question'), 'keep') (('value', 'o'), 'be') (('hat zero y', 'bmatrix'), 'keep') (('1 images', 'tensor type'), 'make') (('inputs multiple times data', 'print confusion matrix'), 'get') (('you', 'hyper parameters'), 'be') (('to 9 hh', '2 category'), 'be') (('model', 'also well 62 accuracy'), 'Note') (('credit 3rd Extra I', 'CNNs'), 'architecture') (('L2 distance', 'CNN classification'), 'be') (('Load train', 'functions'), 'dataset') (('vdots end 8 bmatrix', 'model'), 'show') (('Essentially you', 'learning rate'), 'need') (('worse 43 model', 'LeNEt 57 accuracy'), 'be') (('1 10 training', 'he'), 'plot') (('classification', 'human'), 'be') (('However you', 'architecture'), 'need') (('Howevr I', 'training loop'), 'implement') (('methods', 'similar results'), 'give') (('that', 'first second column'), 'display') (('predicted N', 'classes'), 'be') (('learning curve', 'epochs 200 epochs'), 'show') (('where N', 'classes'), 'make') "}