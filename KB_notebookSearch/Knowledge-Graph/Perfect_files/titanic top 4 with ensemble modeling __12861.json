{"name": "titanic top 4 with ensemble modeling ", "full_name": " h1 Titanic Top 4 with ensemble modeling h3 Yassine Ghouzam PhD h4 13 07 2017 h2 1 Introduction h2 2 Load and check data h3 2 1 Load data h3 2 2 Outlier detection h3 2 3 joining train and test set h3 2 4 check for null and missing values h2 3 Feature analysis h3 3 1 Numerical values h4 SibSP h4 Parch h4 Age h4 Fare h3 3 2 Categorical values h4 Sex h4 Pclass h4 Embarked h2 4 Filling missing Values h3 4 1 Age h2 5 Feature engineering h3 5 1 Name Title h3 5 2 Family size h3 5 3 Cabin h3 5 4 Ticket h2 6 MODELING h3 6 1 Simple modeling h4 6 1 1 Cross validate models h4 6 1 2 Hyperparameter tunning for best models h4 6 1 3 Plot learning curves h4 6 1 4 Feature importance of tree based classifiers h3 6 2 Ensemble modeling h4 6 2 1 Combining models h3 6 3 Prediction h4 6 3 1 Predict and Submit results ", "stargazers_count": 0, "forks_count": 0, "description": "Moreover the more a passenger has parents children the older he is and the more a passenger has siblings spouses the younger he is. 1 Cross validate modelsI compared 10 popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure. Because of the low number of passenger that have a cabin survival probabilities have an important standard deviation and we can t distinguish between survival probability of passengers in the different desks. 2 Categorical values SexIt is clearly obvious that Male have less chance to survive than Female. I preferred to pass the argument soft to the voting parameter to take into account the probability of each vote. Age and Title_1 Master refer to the age of passengers. Load and check data 2. However 1rst class passengers are older than 2nd class passengers who are also older than 3rd class passengers. PclassThe passenger survival is not the same in the 3 classes. This can lead to overweigth very high values in the model even if it is scaled. It means that their predictions are not based on the same features. 4 Feature importance of the tree based classifiers 6. When we superimpose the two densities we cleary see a peak correponsing between 0 and 5 to babies and very young childrens. Skewness is clearly reduced after the log transformation 3. 2 Family sizeWe can imagine that large families will have more difficulties to evacuate looking for theirs sisters brothers parents during the evacuation. 2 Categorical values 4 Filling missing Values 4. IntroductionThis is my first kernel at Kaggle. So Sex might play an important role in the prediction of the survival. 3 Plot learning curves 6. Additionally i decided to created 4 categories of family size. Women and children first It is interesting to note that passengers with rare title have more chance to survive. According to the feature importance of this 4 classifiers the prediction of the survival seems to be more associated with the Age the Sex the family size and the social standing of the passengers more than the location in the boat. No difference between median value of age in survived and not survived subpopulation. Age is not correlated with Sex but is negatively correlated with Pclass Parch and SibSp. I choosed the Titanic competition which is a good way to introduce feature engineering and ensemble modeling. 1 Combining models 6. So even if Age is not correlated with Survived we can see that there is age categories of passengers that of have more or less chance to survive. But the general correlation is negative. In the plot of Age in function of Parch Age is growing with the number of parents children. But in the violin plot of survived passengers we still notice that very young passengers have higher survival rate. SVC and ExtraTrees classifiers seem to better generalize the prediction since the training and cross validation curves are close together. For those who have seen the Titanic movie 1997 I am sure we all remember this sentence during the evacuation Women and children first. 2 Hyperparameter tunning for best modelsI performed a grid search optimization for AdaBoost ExtraTrees RandomForest GradientBoosting and SVC classifiers. 4 Feature importance of tree based classifiersIn order to see the most informative features for the prediction of passengers survival i displayed the feature importance for the 4 tree based classifiers. At this stage we have 22 features. First class passengers have more chance to survive than second class and third class passengers. I supposed that passengers without a cabin have a missing value displayed instead of the cabin number. 1 AgeAs we see Age column contains 256 missing values in the whole dataset. So i decided to replace the Ticket feature column by the ticket prefixe. EmbarkedSince we have two missing values i decided to fill them with the most fequent value of Embarked S. We can say that Pc_1 Pc_2 Pc_3 and Fare refer to the general social standing of passengers. 1 Cross validate models 6. My hypothesis is that first class passengers were prioritised during the evacuation due to their influence. 3 joining train and test setI join train and test datasets to obtain the same number of features during categorical conversion See feature engineering. The prediction seems to be quite similar for the 5 classifiers except when Adaboost is compared to the others classifiers. We notice that age distributions are not the same in the survived and not survived subpopulations. Subpopulations in these features can be correlated with the survival. Last part concerns modeling and predicting the survival on the Titanic using an voting procedure. 4 check for null and missing values 3 Feature analysis 3. 4 Ticket 6 Modeling 6. The computation time is clearly reduced. 1 Name TitleThe Name feature contains information on passenger s title. The family size seems to play an important role survival probability is worst for large families. As we can see Fare distribution is very skewed. So i choosed to create a Fize family size feature which is the sum of SibSp Parch and 1 including the passenger. The correlation map confirms the factorplots observations except for Parch. I used the Tukey method Tukey JW. 1 Numerical valuesOnly Fare feature seems to have a significative correlation with the survival probability. We also see that passengers between 60 80 have less survived. Let s see the Pclass distribution vs EmbarkedIndeed the third class is the most frequent for passenger coming from Southampton S and Queenstown Q whereas Cherbourg passengers are mostly in first class which have the highest survival rate. It is particularly true for cabin B C D E and F. SVC Decision Tree AdaBoost Random Forest Extra Trees Gradient Boosting Multiple layer perceprton neural network KNN Logistic regression Linear Discriminant AnalysisI decided to choose the SVC AdaBoost RandomForest ExtraTrees and the GradientBoosting classifiers for the ensemble modeling. We note that the four classifiers have different top features according to the relative importance. Then i considered outliers as rows that have at least two outlied numerical values. But be carefull this step can take a long time i took me 15 min in total on 4 cpu. In this case it is better to transform it with the log function to reduce this skew. The first letter of the cabin indicates the Desk i choosed to keep this information only since it indicates the probable location of the passenger in the Titanic. Age distribution seems to be the same in Male and Female subpopulations so Sex is not informative to predict Age. This script follows three main parts Feature analysis Feature engineering Modeling 2. At this point i can t explain why first class has an higher survival rate. It seems that passenger coming from Cherbourg C have more chance to survive. 3 CabinThe Cabin feature column contains 292 values and 1007 missing values. Which may be more informative. Title_2 which indicates the Mrs Mlle Mme Miss Ms category is highly correlated with Sex. It seems that very young passengers have more chance to survive. The strategy is to fill Age with the median age of similar rows according to Pclass Parch and SibSp. We detect 10 outliers. 3 Plot learning curvesLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy. Since some passenger with distingused title may be preferred during the evacuation it is interesting to add them to the model. 1 Numerical values 3. Feature analysis 3. 2 Ensemble modeling 6. 3 joining train and test set 2. Sex and Title_2 Mrs Mlle Mme Miss Ms and Title_3 Mr refer to the gender. The 5 classifiers give more or less the same prediction but there is some differences. FareSince we have one missing value i decided to fill it with the median value which will not have an important effect on the prediction. To adress this problem i looked at the most correlated features with Age Sex Parch Pclass and SibSP. My hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown Q Southampton S. Factorplots of family size categories show that Small and Medium families have more chance to survive than single passenger and large families. Fsize LargeF MedF Single refer to the size of the passenger family. But we can see that passengers with a cabin have generally more chance to survive than passengers without X. 1 Predict and Submit resultsIf you found this notebook helpful or you just liked it some upvotes would be very much appreciated That will keep me motivated Load data Outlier detection iterate over features columns 1st quartile 25 3rd quartile 75 Interquartile range IQR outlier step Determine a list of indices of outliers for feature col append the found outlier indices for col to the list of outlier indices select observations containing more than 2 outliers detect outliers from Age SibSp Parch and Fare Show the outliers rows Drop outliers Fill empty and NaNs values with NaN Check for Null values Infos Summarie and statistics Correlation matrix between numerical values SibSp Parch Age and Fare values and Survived Explore SibSp feature vs Survived Explore Parch feature vs Survived Explore Age vs Survived Explore Age distibution Fill Fare missing values with the median value Explore Fare distribution Apply log to Fare to reduce skewness distribution Explore Pclass vs Survived Explore Pclass vs Survived by Sex Fill Embarked nan values of dataset set with S most frequent value Explore Embarked vs Survived Explore Pclass vs Embarked Explore Age vs Sex Parch Pclass and SibSP convert Sex into categorical value 0 for male and 1 for female Filling missing value of Age Index of NaN age rows Get Title from Name Convert to categorical values Title Drop Name variable Create a family size descriptor from SibSp and Parch Create new feature of family size convert to indicator values Title and Embarked Replace the Cabin number by the type of cabin X if not Take prefix Create categorical values for Pclass Drop useless variables Cross validate model with Kfold stratified cross val Modeling step Test differents algorithms Adaboost Best score RFC Parameters tunning Best score Gradient boosting tunning Best score Best score Concatenate all classifier results. Firstly I will display some feature analyses then ill focus on the feature engineering. Feature engineering 5. An outlier is a row that have a feature value outside the IQR an outlier step. 1 Simple modeling 6. GradientBoosting and Adaboost classifiers tend to overfit the training set. 1 Age 5 Feature engineering 5. The 28 89 and 342 passenger have an high Ticket Fare The 7 others have very high values of SibSP. It doesn t mean that the other features are not usefull. Since there is subpopulations that have more chance to survive children for example it is preferable to keep the age feature and to impute the missing values. Titanic Top 4 with ensemble modeling Yassine Ghouzam PhD 13 07 2017 1 Introduction 2 Load and check data 2. I set the n_jobs parameter to 4 since i have 4 cpu. 1 Predict and Submit results 1. 1 Combining modelsI choosed a voting classifier to combine the predictions coming from the 5 classifiers. Indeed there is a peak corresponding to young passengers that have survived. 2 Outlier detection 2. 4 TicketIt could mean that tickets sharing the same prefixes could be booked for cabins placed together. Nevertheless they share some common important features for the classification for example Fare Title_2 Age and Sex. 1977 to detect ouliers which defines an interquartile range comprised between the 1st and 3rd quartile of the distribution values IQR. 2 Hyperparamater tunning for best models 6. Survived missing values correspond to the join testing dataset Survived column doesn t exist in test set and has been replace by NaN values when concatenating the train and test set 3. It could therefore lead to the actual placement of the cabins within the ship. To determine this we need to explore in detail these features SibSPIt seems that passengers having a lot of siblings spouses have less chance to surviveSingle passengers 0 SibSP or with two other persons SibSP 1 or 2 have more chance to surviveThis observation is quite interesting we can consider a new feature describing these categories See feature engineering ParchSmall families have more chance to survive more than single Parch 0 medium Parch 3 4 and large families Parch 5 6. So i decided to use SibSP Parch and Pclass in order to impute the missing ages. 3 Prediction 6. 2 Outlier detectionSince outliers can have a dramatic effect on the prediction espacially for regression problems i choosed to manage them. I decided to detect outliers from the numerical values features Age SibSp Sarch and Fare. Be carefull there is an important standard deviation in the survival of passengers with 3 parents children AgeAge distribution seems to be a tailed distribution maybe a gaussian distribution. Tickets with same prefixes may have a similar class and survival. According to the growing cross validation curves GradientBoosting and Adaboost could perform better with more training examples. Theses differences between the 5 classifier predictions are sufficient to consider an ensembling vote. Filling missing Values 4. There is 17 titles in the dataset most of them are very rare and we can group them in 4 categories. This trend is conserved when we look at both male and female passengers. 4 check for null and missing valuesAge and Cabin features have an important part of missing values. I plot the feature importance for the 4 tree based classifiers Adaboost ExtraTrees RandomForest and GradientBoosting. ", "id": "yassineghouzam/titanic-top-4-with-ensemble-modeling", "size": "12861", "language": "python", "html_url": "https://www.kaggle.com/code/yassineghouzam/titanic-top-4-with-ensemble-modeling", "git_url": "https://www.kaggle.com/code/yassineghouzam/titanic-top-4-with-ensemble-modeling", "script": "Counter sklearn.discriminant_analysis sklearn.svm numpy cross_val_score seaborn detect_outliers ExtraTreesClassifier MLPClassifier SVC sklearn.neighbors sklearn.tree GradientBoostingClassifier sklearn.neural_network sklearn.linear_model learning_curve StratifiedKFold matplotlib.pyplot DecisionTreeClassifier LinearDiscriminantAnalysis sklearn.model_selection pandas RandomForestClassifier LogisticRegression VotingClassifier KNeighborsClassifier AdaBoostClassifier GridSearchCV collections plot_learning_curve sklearn.ensemble ", "entities": "(('I', 'Age SibSp Sarch'), 'decide') (('difference', 'subpopulation'), 'survived') (('4 check', 'missing values'), 'have') (('PclassThe passenger survival', '3 classes'), 'be') (('IntroductionThis', 'first Kaggle'), 'be') (('Age', 'Pclass negatively Parch'), 'correlate') (('age distributions', 'subpopulations'), 'notice') (('I', 'vote'), 'prefer') (('i', 'Age Sex Parch Pclass'), 'look') (('very young passengers', 'more chance'), 'seem') (('GradientBoosting', 'Adaboost training set'), 'tend') (('passengers', 'X.'), 'see') (('passengers', 'more chance'), 'woman') (('Mrs Mlle Mme Miss Ms', 'Title_3 gender'), 'sex') (('which', 'survival highest rate'), 'let') (('clearly Male', 'Female'), 'be') (('Age Master', 'passengers'), 'refer') (('Nevertheless they', 'example'), 'share') (('2 Hyperparameter', 'AdaBoost ExtraTrees RandomForest SVC GradientBoosting classifiers'), 'perform') (('which', 'highly Sex'), 'correlate') (('four classifiers', 'relative importance'), 'note') (('val Modeling step Test differents algorithms', 'tunning Best score'), 'predict') (('It', 'ship'), 'lead') (('Additionally i', 'family size'), 'decide') (('that', 'outlier step'), 'be') (('we', 'babies'), 'see') (('i', '4 tree based classifiers'), 'display') (('we', 'different desks'), 'have') (('Fare 1 Numerical feature', 'survival probability'), 'seem') (('It', 'cabin B C D particularly E'), 'be') (('Skewness', 'log clearly transformation'), 'reduce') (('7 others', 'SibSP'), 'have') (('differences', 'ensembling vote'), 's') (('it', 'missing values'), 'be') (('AgeAge distribution', 'parents 3 children'), 'be') (('we', 'evacuation'), 'be') (('i', '4 cpu'), 'set') (('SVC Decision Tree AdaBoost Random Extra Trees Gradient Boosting layer perceprton neural KNN Logistic regression Linear Discriminant Multiple AnalysisI', 'GradientBoosting ensemble modeling'), 'forest') (('only it', 'Titanic'), 'indicate') (('5 classifiers', 'more same prediction'), 'give') (('column Survived doesn', '3'), 'survive') (('why first class', 'survival higher rate'), 'explain') (('So i', 'missing ages'), 'decide') (('Subpopulations', 'survival'), 'correlate') (('he', 'younger'), 'child') (('i', 'them'), 'have') (('who', 'class also 3rd passengers'), 'be') (('Age column', 'whole dataset'), 'see') (('role survival important probability', 'large families'), 'seem') (('strategy', 'Pclass Parch'), 'be') (('it', 'model'), 'prefer') (('CabinThe Cabin feature 3 column', '292 values'), 'contain') (('class first passengers', 'influence'), 'be') (('passenger', 'more chance'), 'seem') (('it', 'skew'), 'be') (('that', 'Indeed young passengers'), 'be') (('prediction', 'boat'), 'seem') (('3', 'categorical conversion'), 'join') (('which', 'feature good engineering'), 'choose') (('still very young passengers', 'survival higher rate'), 'notice') (('Small families', 'single passenger'), 'show') (('curvesLearning 3 Plot learning curves', 'accuracy'), 'be') (('Pc_1 Pc_2 Pc_3', 'passengers'), 'say') (('2 Hyperparamater', 'best models'), 'tun') (('script', 'three main parts'), 'follow') (('validation curves', 'training'), 'seem') (('i', 'Embarked S.'), 'have') (('Last part', 'voting procedure'), 'concern') (('class First passengers', 'second class third passengers'), 'have') (('which', 'passenger'), 'choose') (('when we', 'male passengers'), 'conserve') (('Titanic Top', 'data'), 'phd') (('we', 'more chance'), 'see') (('Sex', 'Age'), 'seem') (('who', 'Queenstown Q Southampton S.'), 'be') (('Tickets', 'similar class'), 'have') (('TitleThe Name feature', 'title'), '1') (('sizeWe large families', 'evacuation'), 'imagine') (('Firstly I', 'feature engineering'), 'display') (('predictions', 'same features'), 'mean') (('i', '4 cpu'), 'take') (('I', '4 tree based classifiers'), 'plot') (('Cross', 'stratified kfold validation procedure'), 'validate') (('So i', 'ticket prefixe'), 'decide') (('which', 'distribution values'), '1977') (('Parch 3 4 families', 'single Parch 0 medium'), 'feature') (('So Sex', 'survival'), 'play') (('very we', '4 categories'), 'be') (('correlation map', 'Parch'), 'confirm') (('LargeF MedF Fsize Single', 'passenger family'), 'refer') (('passengers', 'cabin instead number'), 'suppose') (('tickets', 'cabins'), 'mean') (('which', 'prediction'), 'have') (('that', 'at least two outlied numerical values'), 'consider') (('GradientBoosting', 'training better more examples'), 'perform') (('even it', 'model'), 'lead') (('when Adaboost', 'others classifiers'), 'seem') (('Combining modelsI', '5 classifiers'), '1') "}