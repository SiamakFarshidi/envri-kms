{"name": "automated feature engineering basics ", "full_name": " h1 Introduction Automated Feature Engineering Basics h2 Feature Engineering h1 Problem h2 Dataset h3 Read in Data and Create Small Datasets h1 Featuretools Basics h1 Entities and Entitysets h1 Relationships h1 Feature Primitives h1 Deep Feature Synthesis h3 DFS with Default Primitives h3 DFS with Selected Aggregation Primitives h2 Notes on Basic Implementation h1 Results h2 Feature Performance Experiments h2 Correlations h3 Correlations with the Target h3 Visualize Distribution of Correlated Variables h4 Collinear Features h2 Feature Importances h2 Remove Low Importance Features h2 Align Train and Test Sets h2 Appendix GBM Model Used Across Feature Sets ", "stargazers_count": 0, "forks_count": 0, "description": "All entities in the entity can be related to each other. Then we establish a relationship between app the parent and previous now the child using SK_ID_CURR. An entity in featuretools must have a unique index where none of the elements are duplicated. Now let s take a look at some of the features we have built and modeling results. Although we did not use the advanced functionality of featuretools we still were able to create useful features that improved the model s performance in cross validation and on the test set. Typically this process is done by hand using pandas operations such as groupby agg or merge and can be very tedious. edu wp content uploads 2017 10 DSAA_DSM_2015. best_iteration_ Record the feature importances feature_importance_values model. There are not datetimes in any of the data but there are relative times given in months or days that we could consider treating as time variables. feature_importances_ k_fold. append valid_auc train_scores. These represent simple calculations many of which we already use in manual feature engineering that can be stacked on top of each other to create complex features. Feature ImportancesThe feature importances returned by a tree based model represent the reduction in impurity https stackoverflow. I took the work done in this notebook and ran the methods on the entire dataset with the results available here https www. In an ideal scenario we would have a set of independent features but that rarely occurs in practice. __previous_application__ previous applications for loans at Home Credit of clients who have loans in the application data. columns Convert to np arrays features np. com guides advanced_custom_primitives. __Transformation__ an operation applied to one or more columns in a single table. These correlations were calculated using the entire training section of the feature matrix. Below we specify all six relationships and then add them to the EntitySet. We did not specify the variable types when creating entities did not use the relative time variables and didn t touch on custom primitives https docs. In theory this allows us to calculate features for any of the entities but in practice we will only calculate features for the app dataframe since that will be used for training testing. 782 13. com automated feature engineering in python 99baf11cc219 aims to help the data scientist with the problem of feature creation by automatically building hundreds or thousands of new features from a dataset. com minute quick start is an open source Python library for automatically creating features out of a set of related tables using a technique called deep feature synthesis http www. com blog secret to data science success by one of the developers of Featuretools. DFS with Selected Aggregation PrimitivesWith featuretools we were able to go from 121 original features to almost 1700 in a few lines of code. Too many irrelevant features can decrease performance by drowning out the important features related to the curse of dimensionality https en. Now we define each entity or table of data. We will stick to the basics so we can get the ideas down and then build upon this foundation in later work when we customize featuretools. This feature is originally recorded as negative so the maximum value would be closest to zero. shape print Testing Data Shape test_features. It s clear that featuretools delivered value on this problem but it still did not leave us without a job. 783 12 hours Three 1803 0. com automated_feature_engineering primitives. We ll join the train and test set together but add a separate column identifying the set. There is one row for every made payment and one row for every missed payment. The most important feature created by featuretools was MAX bureau. DataFrame dataframe of testing features to use for making predictions with the model. Each individual in the parent table can have multiple rows in the _child table_. Featuretools https docs. __bureau_balance__ monthly data about the previous credits in bureau. AMT_PAYMENT which is the average over a client s loans of the minimum value of previous credit application installment payments. Moreover automated feature engineering took a fraction of the time spent manual feature engineering while delivering comparable results. Altogether there are a total of 6 relationships between the tables. mean train_scores Needed for creating dataframe of validation scores fold_names list range n_folds fold_names. html aggregations and transformations Deep feature synthesis https docs. 25 hours Five 1624 0. We would probably want to remove some of these highly correlated variables in order to help the model learn and generalize better. org wiki Curse_of_dimensionality The next call we make will specify a smaller set of features. If we directly link app and cash via SK_ID_CURR previous and cash via SK_ID_PREV and app and previous via SK_ID_CURR then we have created two paths from app to cash. Each row is one month of a credit card balance and a single credit card can have many rows. For example we could use the most important features in order to concentrate on these aspects of a client when evaluating a potential loan. It is ideal tool for problems such as the Home Credit Default Risk competition where there are several related tables that need to be combined into a single dataframe for training and one for testing. split features Training data for the fold train_features train_labels features train_indices labels train_indices Validation data for the fold valid_features valid_labels features valid_indices labels valid_indices Create the model model lgb. feature_importances pd. We establish a relationship between previous the parent and cash the child using SK_ID_PREV. com willkoehrsen introduction to manual feature engineering p2 applied to this competition. There are 7 different data files __application_train application_test__ the main training and testing data with information about each loan application at Home Credit. __POS_CASH_BALANCE__ monthly data about previous point of sale or cash loans clients have had with Home Credit. predict_proba test_features num_iteration best_iteration 1 k_fold. While the absolute value of the importances can be difficult to interpret looking at the relative value of the importances allows us to compare the relevance of features. This can be useful to look at the resulting features before starting an extended computation. html or seed features or interesting values Nonetheless in this notebook we were able to learn the basic foundations which will allow us to more effective use the tool as we learn how it works. DataFrame dataframe of training features to use for training a model. com willkoehrsen feature engineering using feature tools. com static papers DSAA_DSM_2015. When I did feature engineering by hand it took about 12 hours to create a comparable size dataset. Currently only app bureau and previous have unique indices SK_ID_CURR SK_ID_BUREAU and SK_ID_PREV respectively. We need to pass in an index if the data has one or make_index True if not. The model which can be viewed in the appendix is a basic LightGBM algorithm using 5 fold cross validation for training and evaluation. This shows the distribution of a single variable and can be thought of as a smoothed histogram. For intstance if we have a categorical variable that is represented as an integer we might want to let featuretools know the right type. 777 1 hour Four 1156 0. We simply used the default aggregations without thinking about which ones are important for the problem. DataFrame dataframe with training and validation metrics ROC AUC for each fold and overall. valid_metrics pd. reshape 1 test_features col label_encoder. com willkoehrsen introduction to manual feature engineering and part two https www. There are a few concepts that we will cover along the way Entities and EntitySets https docs. However while we get a lot of features in featuretools this function call is not very well informed. append i Catch error if label encoding scheme is not valid else raise ValueError Encoding must be either ohe or le print Training Data Shape features. Using correlations is fine as a first approximation for identifying good features but it is not a rigorous feature selection method. Here we will touch on the concepts of automated feature engineering with featuretools and show how to implement it for the Home Credit Default Risk competition. The training application data comes with the TARGET with indicating 0 the loan was repaid and 1 the loan was not repaid. array test_features Create the kfold object k_fold KFold n_splits n_folds shuffle False random_state 50 Empty array for feature importances feature_importance_values np. To create a feature with a depth of two we could stack primitives by taking the maximum value of a client s average montly payments per previous loan such as MAX previous MEAN installments. zeros len feature_names Empty array for test predictions test_predictions np. org wiki Diamond_graph where there are multiple paths from a parent to a child. DataFrame dataframe with the feature importances from the model. html is a collection of tables and the relationships between them. To perform DFS in featuretools we use the dfs function passing it an entityset the target_entity where we want to make the features the agg_primitives to use the trans_primitives to use and the max_depth of the features. Then featuretools will be able to create features on app derived from both previous and cash by stacking multiple primitives. Notes on Basic ImplementationThese calls represent only a small fraction of the ability of featuretools https docs. __installments_payment__ payment history for previous loans at Home Credit. Later we can convert to a script and run with the entire datasets. Using a computer with 64GB of ram this function call took around 24 hours I don t think I m technically breaking the rules of my university s high powered computing center. Also based on examining some of the features it seems there might be issues with collinearity between features https en. Either ohe for one hot encoding or le for integer label encoding n_folds int default 5 number of folds to use for cross validation Return submission pd. The vital role of the data scientist now comes down to choosing the correct set of primitives and selecting the best features from among all the candidates. Control using only data from the application dataset Test One manual feature engineering using only the application bureau and bureau_balance data Test Two manual feature engineering using all datasets Test Three featuretools default features in the feature_matrix Test Four featuretools specified features in the feature_matrix_spec Test Five featuretools specified features combined with manual feature engineering The number of features is after one hot encoding the validation receiver operating characteristic area under the curve ROC AUC is calculated using 5 fold cross validation the test ROC AUC is from the public leaderboard and the time spent designing is my best estimate of how long it took to make the dataset Test Number of Features Validation ROC AUC Test ROC AUC Time Spent Control 241 0. However that does not mean they are necessarily important. Let s look at the number of features with 0 importance which almost certainly can be removed from the featureset. Removing the low information features and aligning the dataframes has left us with 1689 features Feature selection will certainly play an important role when using featuretools. com c home credit default risk using the featuretools library. First we read in some of the feature matrix using the nrows argument of pandas read_csv function. com willkoehrsen home credit default risk feature tools data in the file called feature_matrix. That only gives us 884 features and takes about 12 hours to run on the complete dataset. Feature importances can be used for dimensionality reduction. shape 0 Lists for recording validation and training scores valid_scores train_scores Iterate through each fold for train_indices valid_indices in k_fold. Extract the ids train_ids features SK_ID_CURR test_ids test_features SK_ID_CURR Extract the labels for training labels features TARGET Remove the ids and target features features. com questions 15810339 how are feature importances in randomforestclassifier determined from including the feature in the model. I have made the entire dataset available here https www. best_score_ train auc valid_scores. Align Train and Test SetsWe also want to make sure the train and test sets have the same exact features. html is an operation applied to a table or a set of tables to create a feature. For each relationship we need to specify the parent variable and the child variable. The observations are in the rows and the features in the columns. 1 n_jobs 1 random_state 50 Train the model model. DataFrame fold fold_names train train_scores valid valid_scores return submission feature_importances metrics Uncomment and run if kernel does not already have featuretools pip install featuretools pandas and numpy for data manipulation featuretools for automated feature engineering matplotlit and seaborn for visualizations Suppress warnings from pandas Read in the datasets and limit to the first 1000 rows sorted by SK_ID_CURR This allows us to actually see the results in a reasonable amount of time Add identifying column Append the dataframes Entity set with id applications Entities with a unique index Entities that do not have a unique index Relationship between app and bureau Relationship between bureau and bureau balance Relationship between current app and previous apps Relationships between previous apps and cash installments and credit Add in the defined relationships Print out the EntitySet List the primitives in a dataframe Default primitives from featuretools DFS with specified primitives DFS with default primitives Specify the aggregation primitives Most negative correlations Most positive correlations Need to reset index for loc to workBU plot repaid loans plot loans that were not repaid Label the plots Iterate through the columns Find correlations above the threshold Read in the feature importances and sort with the most important at the top List of the original features after one hot Iterate through the top 100 features Sort features according to importance Normalize the feature importances to add up to one Make a horizontal bar chart of feature importances Need to reverse the index to plot most important on top Set the yticks and labels Plot labeling Remove features with only one unique value Separate out the train and test sets One hot encoding Align dataframes on the columns. We can calculate the number of top 100 features that were made by featuretools. array test_features col. Feature primitives fall into two categories __Aggregation__ function that groups together child datapoints for each parent and then calculates a statistic such as mean min max or standard deviation. org wiki Multicollinearity made by featuretools. We are only using a sample of the features so this might not be representative of the entire dataset. Featuretools the only library for automated feature engineering at the moment will not replace the data scientist but it will allow her to focus on more valuable parts of the machine learning pipeline such as delivering robust models into production. The bureau dataframe in turn is the parent of bureau_balance because each loan has one row in bureau but multiple monthly records in bureau_balance. 785 0. Trying to interpret this feature is difficult but my best guess is a client s maximum value of average number of atm drawings per month on previous credit card loans. Every loan has its own row and is identified by the SK_ID_CURR. get_dummies test_features Align the dataframes by the columns features test_features features. shape Extract feature names feature_names list features. loan_amount that is a deep feature with a depth of 1. To show the effect of a categorical variable on the distribution of a numeric variable we can color the plot by th value of the categorical variable. __credit_card_balance__ monthly data about previous credit cards clients have had with Home Credit. An EntitySet https docs. 757 8 hours Two 1465 0. Although we want to be careful about placing too much value on the feature importances they can be a useful method for dimensionality reduction and understanding the model. ConclusionsIn this notebook we went through a basic implementation of using automated feature engineering with featuretools for the Home Credit Default Risk dataset. shape 0 Empty array for out of fold validation predictions out_of_fold np. In a _parent table_ each individual has a single row. html Relationships between tables https docs. In the plot below we show the distribution of two of the newly created features colored by the value of the target. RelationshipsRelationships are a fundamental concept not only in featuretools but in any relational database. predict_proba valid_features num_iteration best_iteration 1 Record the best score valid_score model. append overall Dataframe of validation scores metrics pd. Moreover manual feature engineering is limited both by human time constraints and imagination we simply cannot conceive of every possible feature that will be useful. We also see several important features with a depth of two such as MEAN previous_app. n_splits Make predictions test_predictions model. We end up with a lot of features but they are probably not all relevant to the problem. For example if we take the maximum value of a client s previous loans say MAX previous. Collinear FeaturesThese variables all have a 0. 745 0. Let s write a short function to visualize the 15 most important features. DFS stacks feature primitives to form features with a depth equal to the number of primitives. An aggregation works across multiple tables using relationships between tables. The maximum of this value over the previous loans is therefore represented by this feature. encoding str default ohe method for encoding categorical variables. Including them all in the model is unnecessary because it would be encoding redundant information. ProblemThe Home Credit Default Risk competition is a supervised classification machine learning task. Introduction Automated Feature Engineering BasicsIn this notebook we will walk through applying automated feature engineering to the Home Credit Default Risk dataset https www. Slightly advanced note we need to be careful to not create a diamond graph https en. Each previous credit has its own row in bureau and is identified by the SK_ID_BUREAU Each loan in the application data can have multiple previous credits. A parent is a single individual but can have mutliple children. Extracting as much information as possible from the available datasets is crucial to creating an effective solution. Unfortunately this will not run in a Kaggle kernel due to the computational expense of the operation. For an example of using manual feature engineering check out part one https www. We will work with a subset of the data because this is a computationally intensive job that is outside the capabilities of the Kaggle kernels. 25 hours One 421 0. get_dummies features test_features pd. ____Featuretools demonstrably adds value when included in a data scientist s toolbox. Featuretools will automatically infer the types of variables but we can also change them if needed. It s safest to just join them together and treat them as a single dataframe. For a good take on why features are so important here s a blog post https www. 786 0. Correlations with the TargetSeveral of the features created by featuretools are among the most correlated with the TARGET in terms of absolute magnitude. 99 correlation with each other which is nearly perfectly positively linear. The objective is to use historical financial and socioeconomic data to predict whether or not an applicant will be able to repay a loan. An example is calculating the maximum previous loan amount for each client. This ensures we will not read in the entire 2 GB file. CorrelationsNext we can look at correlations within the data. This is important because we are going to want to apply the same exact procedures to each dataset. First we ll make an empty entityset named clients to keep track of all the data. Here we will use the default aggregation and transformation primitives a max depth of 2 and calculate primitives for the app entity. The same analysis could be applied to the default feature set. We still are not using much domain knowledge but this feature set will be more manageable. com guides tuning_dfs. Automated feature engineering like many topics in machine learning is a complex subject built upon a foundation of simpler ideas. Each previous application has one row and is identified by the feature SK_ID_PREV. com willkoehrsen start here a gentle introduction. collect Make the submission dataframe submission pd. com automated_feature_engineering afe. Using an EntitySet allows us to group together multiple tables and manipulate them much quicker than individual tables. Deep Feature SynthesisDeep Feature Synthesis DFS is the process featuretools uses to make new features. com loading_data using_entitysets. Because this process is computationally expensive we can run the function using features_only True to return only a list of the features and not calculate the features themselves. The next step from here is improving the features we actually build and performing feature selection. com willkoehrsen intro to tuning automated feature engineering Appendix GBM Model Used Across Feature Sets pythondef model features test_features encoding ohe n_folds 5 Train and test a light gradient boosting model using cross validation. test_features pd. drop columns SK_ID_CURR One Hot Encoding if encoding ohe features pd. array features test_features np. fit train_features train_labels eval_metric auc eval_set valid_features valid_labels train_features train_labels eval_names valid train categorical_feature cat_indices early_stopping_rounds 100 verbose 200 Record the best iteration best_iteration model. DataFrame SK_ID_CURR test_ids TARGET test_predictions Make the feature importance dataframe feature_importances pd. This will be very useful when we need to define relationships in featuretools. png Read in Data and Create Small DatasetsWe will read in the full dataset sort by the SK_ID_CURR and keep only the first 1000 rows to make the calculations feasible. For the other dataframes we must pass in make_index True and then specify the name of the index. Each row is one month of a previous credit and a single previous credit can have multiple rows one for each month of the credit length. dtype object Map the categorical features to integers features col label_encoder. 784 0. n_splits Record the out of fold predictions out_of_fold valid_indices model. DataFrame feature feature_names importance feature_importance_values Overall validation score valid_auc roc_auc_score labels out_of_fold Add the overall scores to the metrics valid_scores. aspx a service dedicated to provided lines of credit loans to the unbanked population. com kaggle media competitions home credit home_credit. Feature PrimitivesA feature primitive https docs. Parameters features pd. Defining the relationships is relatively straightforward and the diagram provided by the competition is helpful for seeing the relationships. This can be thought of a data structute with its own methods and attributes. Each current loan in the application data can have multiple previous loans. If there are very highly correlated varibables we might want to think about removing some of them. The original paper on automated feature engineering using deep feature synthesis https dai. DataFrame dataframe with SK_ID_CURR and TARGET probabilities predicted by the model. reshape 1 Record the categorical indices cat_indices. They can also be used to help us better understand a problem. __The next steps are to take advantage of the advanced functionality in featuretools combined with domain knowledge to create a more useful set of features. First we establish a control dataset and then we carry out a series of experiments and present the results. Any thoughts would be much appreciated Featuretools Basics Featuretools https docs. We will look explore tuning featuretools in an upcoming notebook https www. The best way to think of a one to many relationship is with the analogy of parent to child. com is an open source Python package for automatically creating new features from multiple tables of structured related data. Features that are highly correlated with one another can diminish interpretability and generalization performance on the test set. Another area to investigate is highly correlated features known as collinear features. 779 1. To generate a subset of the features run the code cell below. We can look for pairs of correlated features and potentially remove any above a threshold. In order to isolate the effect of the features the same model was used to test a number of different feature sets. best_score_ valid auc train_score model. This is a standard supervised classification task __Supervised__ The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features __Classification__ The label is a binary variable 0 will repay loan on time 1 will have difficulty repaying loan DatasetThe data is provided by Home Credit http www. append train_score Clean up memory gc. append valid_score train_scores. __bureau__ data concerning client s previous credits from other financial institutions. The children can then have multiple children of their own. As an example the app dataframe has one row for each client SK_ID_CURR while the bureau dataframe has multiple previous loans SK_ID_PREV for each parent SK_ID_CURR. DFS with Default PrimitivesIf you are interested in running this call on the entire dataset and making the features I wrote a script for that here https www. Visualize Distribution of Correlated VariablesOne way we can look at the resulting features and their relation to the target is with a kernel density estimate plot. Entities can also have time indices where each entry is identified by a unique time. drop columns SK_ID_CURR TARGET test_features test_features. This results in ambiguity so the approach we have to take instead is to link app to cash through previous. com willkoehrsen home credit default risk feature tools. 25 hours It s hard to say which set is exactly the best although I trust the cross validation scores more than the public leaderboard but there are huge discrepancies is the time for development. The importance of creating the proper features cannot be overstated because a machine learning model can only learn from the data we give to it. Featuretools has a default method for doing this available in the selection module. html Entities and EntitysetsAn entity is simply a table or in Pandas a dataframe. ResultsTo determine whether our basic implementation of featuretools was useful we can look at several results Cross validation scores and public leaderboard scores using several different sets of features. Remove Low Importance FeaturesFeature selection is an entire topic by itself but one thing we can do is remove any features that have only a single unique value or are all null. The diagram below provided by Home Credit shows how the tables are related. org wiki Feature_engineering is to create new features alos called explantory variables or predictors to represent as much information from an entire dataset in one table. Therefore the bureau dataframe is the child of the app dataframe. The correlation between this feature and the target is extremely weak and could be only noise. Correlations both between the features and the TARGET and between features themselves Feature importances determined by a gradient boosting machine model Feature Performance ExperimentsTo compare a number of different feature sets for the machine learning task I set up several experiments. We can first one hot encode the data we ll have to do this anyway for our model and then align the dataframes on the columns. enable del model train_features valid_features gc. If you are new to this competition I suggest checking out this post to get started https www. Automated feature engineering https towardsdatascience. align test_features join inner axis 1 No categorical indices to record cat_indices auto Integer label encoding elif encoding le Create a label encoder label_encoder LabelEncoder List for storing categorical indices cat_indices Iterate through each column for i col in enumerate features if features col. Feature EngineeringThe objective of feature engineering https en. html adding a relationship Feature primitives https docs. Two tables are linked via a shared variable. A list of the available features primitives in featuretools can be viewed below. com the multiple comparisons problem e5573e8b9578 if we make a ton of features some are likely to be correlated with the target simply because of random noise. An example would be taking the absolute value of a column or finding the difference between two columns in one table. When we look at correlations with the target we need to be careful about the multiple comparisons problem https towardsdatascience. The most important feature created by featuretools was the maximum number of days before current application that the client applied for a loan at another institution. __Even the default set of features in featuretools was able to achieve similar performance to hand engineered features in less than 10 of the time. At the end of this notebook we ll look at the features themselves as well as the results of modeling with different combinations of hand designed and automatically built features. image https storage. Each row is one month of a previous point of sale or cash loan and a single previous loan can have many rows. DAYS_CREDIT represents the number of days before the current application at Home Credit that the applicant applied for a loan at another credit institution. 766 0. The app and bureau dataframe are linked by the SK_ID_CURR variable while the bureau and bureau_balance dataframes are linked with the SK_ID_BUREAU. The specified featuretools dataset was able to achieve nearly the same performance as the hand engineered features on the test set with 8 of the time invested. LGBMClassifier n_estimators 10000 boosting_type goss objective binary class_weight balanced learning_rate 0. By going through these ideas one at a time we can build up our understanding of how featuretools which will later allow for us to get the most out of it. For the correlations we will focus on the feature_matrix_spec the features we made by specifying the primitives. 05 reg_alpha 0. The SK_ID_CURR 100002 has one row in the parent table and multiple rows in the child. I m not sure if this is allowing data leakage into the train set and if these feature creation operations should be applied separately. Must include the TARGET column. ", "id": "willkoehrsen/automated-feature-engineering-basics", "size": "33314", "language": "python", "html_url": "https://www.kaggle.com/code/willkoehrsen/automated-feature-engineering-basics", "git_url": "https://www.kaggle.com/code/willkoehrsen/automated-feature-engineering-basics", "script": "plot_feature_importances seaborn matplotlib.pyplot kde_target_plot selection featuretools pandas numpy ", "entities": "(('featuretools', 'right type'), 'for') (('It', 'single dataframe'), 's') (('cash single previous loan', 'many rows'), 'be') (('that', 'featuretools'), 'calculate') (('have', 'same exact features'), 'want') (('that', '1'), 'loan_amount') (('we', 'also them'), 'infer') (('it', 'size comparable dataset'), 'take') (('why features', 'good take'), 'be') (('that', 'training testing'), 'allow') (('that', 'complex features'), 'represent') (('look', 'kernel density estimate plot'), 'distribution') (('html', 'them'), 'be') (('This', 'extended computation'), 'be') (('previous application', 'feature'), 'have') (('loan', 'multiple monthly bureau_balance'), 'be') (('Unfortunately this', 'operation'), 'run') (('feature creation operations', 'train set'), 'm') (('train_scores', 'validation scores fold_names list range n_folds fold_names'), 'mean') (('valid_indices', 'model model lgb'), 'feature') (('We', 'SK_ID_PREV'), 'establish') (('com feature how importances', 'model'), 'question') (('we', 'MAX MEAN such previous installments'), 'stack') (('min max', 'such mean'), 'fall') (('how tables', 'Home below Credit'), 'show') (('org wiki Feature_engineering', 'one table'), 'be') (('features', 'features'), 'perform') (('we', 'comparisons problem https multiple towardsdatascience'), 'look') (('Correlations', 'absolute magnitude'), 'be') (('which', '99 other'), 'correlation') (('this', 'entire dataset'), 'use') (('Feature ImportancesThe feature importances', 'impurity https stackoverflow'), 'represent') (('we', 'automatically features'), 'look') (('_ _ bureau _ _ data', 'previous other financial institutions'), 's') (('we', 'parent variable'), 'need') (('ones', 'problem'), 'use') (('we', 'when potential loan'), 'use') (('I', 'https started www'), 'suggest') (('categorical_feature cat_indices', 'valid train'), 'train_features') (('_ next steps', 'features'), '_') (('Here we', 'Home Credit Default Risk competition'), 'touch') (('We', 'notebook https upcoming www'), 'look') (('it', 'redundant information'), 'be') (('We', 'MEAN such previous_app'), 'see') (('app Currently only bureau', 'unique indices'), 'have') (('when we', 'featuretools'), 'stick') (('loan', 'multiple previous credits'), 'have') (('originally negative so maximum value', 'zero'), 'record') (('loan', 'SK_ID_CURR'), 'have') (('example', 'one table'), 'take') (('featuretools specified dataset', 'time'), 'be') (('where none', 'elements'), 'have') (('calculations', 'only first 1000 rows'), 'read') (('bureau Therefore dataframe', 'app dataframe'), 'be') (('same model', 'feature different sets'), 'use') (('it', 'features https'), 'seem') (('we', 'results'), 'let') (('submission', 'dataframe submission pd'), 'make') (('correlations', 'feature matrix'), 'calculate') (('Align', 'test_features features'), 'feature') (('ConclusionsIn notebook we', 'Home Credit Default Risk dataset'), 'go') (('most important feature', 'featuretools'), 'be') (('CorrelationsNext we', 'data'), 'look') (('Then featuretools', 'multiple primitives'), 'be') (('that', 'only single unique value'), 'be') (('i', 'col'), 'join') (('We', 'set'), 'join') (('very highly correlated we', 'them'), 'be') (('where entry', 'unique time'), 'have') (('parent', 'single mutliple children'), 'be') (('Collinear FeaturesThese variables', '0'), 'have') (('target features', 'ids'), 'extract') (('current loan', 'multiple previous loans'), 'have') (('correlation', 'feature'), 'be') (('they', 'probably all problem'), 'end') (('com', 'cross validation'), 'feature') (('we', 'code'), 'featuretool') (('feature', 'feature_importance_values model'), 'importance') (('we', 'dataset'), 'be') (('applicant', 'credit institution'), 'represent') (('Typically process', 'groupby such agg'), 'do') (('RelationshipsRelationships', 'relational database'), 'be') (('that', 'rarely practice'), 'have') (('we', 'previous'), 'be') (('SK_ID_CURR 100002', 'multiple child'), 'have') (('Then we', 'previous SK_ID_CURR'), 'establish') (('we', 'it'), 'overstate') (('Notes', 'featuretools https docs'), 'represent') (('categorical features', 'col label_encoder'), 'feature') (('vital role', 'candidates'), 'come') (('we', 'categorical variable'), 'color') (('applicant', 'loan'), 'be') (('Too many irrelevant features', 'dimensionality https'), 'decrease') (('then we', 'results'), 'establish') (('html Entities entity', 'simply Pandas'), 'be') (('aggregation', 'tables'), 'work') (('we', 'features'), 'determine') (('we', 'primitives'), 'focus') (('it', 'job'), 's') (('Featuretools', 'selection module'), 'have') (('us', 'features'), 'allow') (('arrays', 'np'), 'feature') (('also us', 'better problem'), 'use') (('com', 'dataset'), 'aim') (('we', 'target'), 'show') (('very when we', 'featuretools'), 'be') (('DatasetThe data', 'Home Credit http www'), 'be') (('individual', 'single row'), 'have') (('we', 'GB entire 2 file'), 'ensure') (('label encoding scheme', 'ValueError else Encoding'), 'raise') (('Slightly advanced we', 'diamond graph https'), 'note') (('later us', 'it'), 'by') (('This', 'smoothed histogram'), 'show') (('we', 'columns'), 'encode') (('single previous credit', 'credit length'), 'be') (('where several related that', 'testing'), 'be') (('First we', 'pandas read_csv function'), 'read') (('us', 'much quicker individual tables'), 'allow') (('Later we', 'entire datasets'), 'convert') (('com', 'structured related data'), 'be') (('which', 'credit application installment previous payments'), 'be') (('then we', 'cash'), 'create') (('I', 'computing high powered center'), 'take') (('we', 'features'), 'curse_of_dimensionality') (('First we', 'data'), 'make') (('how it', 'tool'), 'feature') (('function call', 'featuretools'), 'be') (('that', 'simply possible feature'), 'limit') (('list', 'featuretools'), 'view') (('1 loan', 'TARGET'), 'come') (('example', 'client'), 'calculate') (('data', 'make_index one True'), 'need') (('I', 'results'), 'take') (('we', 'feature actually selection'), 'improve') (('n_splits', 'predictions test_predictions model'), 'make') (('same analysis', 'default feature set'), 'apply') (('feature Moreover automated engineering', 'comparable results'), 'take') (('best way', 'child'), 'be') (('observations', 'columns'), 'be') (('This', 'own methods'), 'think') (('Below we', 'EntitySet'), 'specify') (('best guess', 'credit card previous loans'), 'be') (('Extracting', 'effective solution'), 'be') (('which', 'training'), 'be') (('bureau', 'bureau_balance SK_ID_BUREAU'), 'link') (('Test dataset Number', 'Features Validation ROC AUC Test ROC'), 'control') (('it', 'good features'), 'be') (('relatively diagram', 'relationships'), 'be') (('we', 'time variables'), 'be') (('drop columns', 'Hot SK_ID_CURR One ohe pd'), 'feature') (('categorical', 'cat_indices'), 'index') (('who', 'application data'), 'application') (('area', 'collinear highly correlated features'), 'be') (('_ default Even set', 'time'), '_') (('feature set', 'domain still much knowledge'), 'use') (('Feature importances', 'dimensionality reduction'), 'use') (('we', 'index'), 'for') (('only one unique value', 'columns'), 'fold') (('which', 'almost certainly featureset'), 'let') (('when creating', 'didn t custom primitives https docs'), 'specify') (('_ _ Transformation _ _ operation', 'single table'), 'apply') (('computationally intensive that', 'Kaggle kernels'), 'work') (('some', 'simply random noise'), 'com') (('exactly I', 'huge development'), 's') (('html', 'feature'), 'be') (('children', 'own'), 'have') (('Feature SynthesisDeep Feature Synthesis Deep DFS', 'process new features'), 'be') (('class_weight', 'learning_rate'), 'balance') (('_ _ _ _ Featuretools', 'data when toolbox'), 'add') (('MAX', 'previous loans'), 'say') (('Two tables', 'shared variable'), 'link') (('credit single card', 'many rows'), 'be') (('computationally we', 'features'), 'be') (('array test_features', 'n_folds False feature n_splits 50 Empty importances'), 'create') (('they', 'model'), 'be') (('bureau SK_ID_CURR dataframe', 'parent'), 'have') (('that', 'test set'), 'diminish') (('Here we', 'app entity'), 'use') (('validation score valid_auc roc_auc_score Overall labels', 'metrics valid_scores'), 'feature') (('entities', 'other'), 'relate') (('client', 'institution'), 'be') (('we', 'way'), 'be') (('her', 'production'), 'replace') (('DFS stacks', 'primitives'), 'feature') (('feature Automated engineering', 'simpler ideas'), 'be') (('individual', '_ child table _'), 'have') (('_ POS_CASH_BALANCE _ _ monthly data', 'cash loans Home Credit'), '_') (('com minute quick start', 'feature deep www'), 'http') (('s', '15 most important features'), 'let') (('Feature selection', 'when featuretools'), 'leave') (('That', 'complete dataset'), 'give') (('_ _ credit_card_balance _ _ monthly data', 'Home Credit'), 'have') (('Now we', 'data'), 'define') (('We', 'potentially any threshold'), 'look') (('Introduction Automated Feature Engineering notebook we', 'Home Credit Default Risk dataset https www'), 'BasicsIn') (('I', 'https here www'), 'DFS') (('model', 'order'), 'want') (('maximum', 'therefore feature'), 'represent') (('that', 'test set'), 'be') (('I', 'several experiments'), 'correlation') "}