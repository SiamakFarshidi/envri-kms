{"name": "pytanic ", "full_name": " h1 Python walk through for Titanic data analysis h2 This is a work in progress Comments and critical feedback are always welcome h2 Outline h1 1 Load Data and Modules h1 2 Initial Exploration h3 Missing values h3 Cabin numbers h2 Ticket numbers h1 3 Relations between features h1 4 Filling in missing values h1 5 Derived engineered features h3 Child h3 Cabin known h3 Deck h3 Ttype and Bad ticket h3 Age known h3 Family h3 Alone h3 Large Family h3 Shared ticket h3 Title h3 Fare cat h3 Fare eff cat h1 6 Preparing for modelling h1 7 Modelling h2 Splitting the train sample into two sub samples training and testing h2 Test and select the model features h2 Run and describe several different classifiers h2 Examining Optimising one classifier in more detail h2 Model validation h2 Ranking of models and features h2 Stacking Ensemble methods h1 8 Preparing our prediction for submission ", "stargazers_count": 0, "forks_count": 0, "description": "The barplot shows mean survival fractions and the associated 95 confidence limits which are large for the sparse samples. The other two passenger classes are more interesting especially for the male children. But let s investigate this for a moment and check how it would transform the Fare distribution. For those of you who prefer R over Python or want to try out both I m currently building a new R kernel for Titanic https www. This kind of plot is vastly more useful for a set of continuous variables instead of the categorical or integer values we have here. which classifier which meta parameters. For additional insight we compare the feature _importance output of all the classifiers for which it exists The feature importance tells us how much impact an individual feature has on the decisions within the classifier. We see that the distributions become significantly narrower and that the tails and bimodality become much weaker after getting rid of the zero fare values for both groups. For this we look at the combined data set to make sure that we don t miss any titles that might be in train or test only Ok so we have 18 different titles but many of them only apply to a handful of people. However the differences between the methods are relatively small and more likely due to more or less over fitting than anything else. Similar to the standard error of the mean for sampling normal distributions. We learn Different percentages of passenger classes and sexes have embarked from different ports which is reflected in the lower survival rates for S more men fewer 1st class compared to C more women and 1st class. From there you can download it and submit it by going to Leaderboard Submit Predictions in the tab list below the competition header. For a view into Pclass vs Sex let s use a mosaic plot for a 2 dimensional overview. As far as I can see there s still quite a bit of variation here. Violin plots are a modified version of boxplots where the shape is a kernel density estimate of the underlying distribution. plot_confusion_matrix cnf_matrix classes class_names normalize True title Normalized confusion matrix for clf label in zip clf_tree clf_knn clf_svm clf_ext clf_gb clf_xgb clf_pctr clf_log clf_rf clf_bag clf_vote tree knn svm extra gb xgb percep logistic RF Bag Ensemble scores cross_val_score clf X y cv 5 scoring accuracy print Accuracy 0. io 2015 10 28 confusion matrix. Name is the name of the passenger. Go to the top of the page top 7. This is a categorical text string feature. C and F are around 60. html Support Vector Machine Support Vector Machine This classifier fits a set of hyper plane s in the high dimensional space of the training features so that this plane has the largest distance to any training data points. Many thanks Now let s study the new features and see how they relate to the survival chances Child The Pclass 1 plot looks interesting at first but there are only 3 children in this group which makes the apparent pattern just random noise. in the corresponding text field so that you remember the model for this score and don t have to re submit something that you had done already. Large _Family In the same way having a large family appears to be not good for survival. Of course for newbies I also recommend the excellent kernels featured in the Titanic Tutorials https www. 1 and 3 based on large enough numbers suggests that this new feature could still contain some useful information. This is a small number that we could ignore but we are curious aren t we It s Mr Osen and Mr Gustafsson on Ticket 7534. Especially among the 3rd class passengers. For instance Young was designed to replace Age and Title as a combination of the two. Problem Unstable to small variations in the data. The result is written to a submission file according to the competition rules 418 rows only include the columns PassengerId and Survived. Preparing our prediction for submission submit complete 1. org wiki Binomial_distribution. This is a simple yet powerful method that works well for irregular decision boundaries. We learn the following things from studying the individual features Age The medians are identical. Here single trees are combined through the average of the prediction probabilities. A module could be defined further down once it is needed but I prefer to have them all in one place to keep an overview. This is expected to increase the accuracy of the final prediction. Hopefully this will result in a better understanding of stacking. However the corresponding number are not very large. Sharing a ticket number is not uncommon. I recommend that you weigh the arguments and make your own decision. ModellingLet s summarise briefly what we found in our data exploration sex and ticket class are the main factors there seem to be additional impacts from age young men vs young women male children relatives parch 1 3 sibsp 1 2 somewhat explained by sex but not completely maybe the cabin deck but not many are known other apparent effects appear to be strongly connected to the sex class features port of embarkation fare sharing a ticket large family travelling alone known cabin number known age Splitting the train sample into two sub samples training and testing This is best practice for evaluating the performance of our models which should not be tested on the same data they are trained on. The highest ages are well consistent with the overall distribution. Fare is a float feature showing how much each passenger paid for their rather memorable journey. Num positive examples Num negative examples Draw the grid boxes Set xlabels These coordinate might require some tinkering. Embarked Well that does look more interesting than expected. LightGBM The LightGBM https github. The plot tells us that the survival chances were much lower for the cheaper cabins. It does exactly what the name suggests each individual classifier makes a certain prediction and then the majority vote is used for each row. We start with an overview plot of the feature relations Here we show a correlation matrix for each numerical variable with all the other numerical variables. org stable modules naive_bayes. Solution balance the data set by either sampling the same number of samples from each class or by adjusting the sample_weight parameter to normalise the sum of the class weights to the same value. At the start of this section we define a new feature Fare _cat as fare categories in the same way. Embarked shows the port of embarkation as a categorical character value. This goes some way to explain features like better survival for SibSp 1 3. Otherwise there is no significant difference within each class. We learn For females the survival chances appear to be higher between 18 and 40 whereas for men in that age range the odds are flipped. What we want to find are the best predictors for survival. Whether there is actual signal in them that a model can use to improve the learning accuracy needs to be investigated. LightGBM is often signficantly faster than XGBoost and achieves at least a similar accuracy. Fill in secondary metrics accuracy true pos rate etc. Nonetheless this seems useful. Another way of checking the actual numbers are through cross tables Passengers with more than 3 children parents on board had low survival chances. Alone Travelling alone appears bad enough to be significant. Filling in missing valuesAfter studying the relations between the different features let s fill in a few missing values based on what we learned. Derived engineered featuresThe next idea is to define new features based on the existing ones that allow for a split into survived not survived with higher confidence than the existing features. It s like a correlation matrix in a sense. Combining them into parch 4 gives us 9 vs 1 which is much better. Maybe these two actually shared a ticket cabin and we have another transcription data entry error The ticket numbers are very similar and someone could easily write 303 instead of 304. Initial Exploration Look at your data in as many different ways as possible. Cabin _known As suspected it is more likely to know the cabin of a passenger who survived. In addition we see that the Fare was identical for all the passengers in each ticket group. The only passenger on deck T died but that s hardly robust statistics. com kiralt in the comments we also use a Confusion Matrix to evaluate the performance of our classifier. plot kind bar stacked True color nosurv_col surv_col foo combine Age. Final validation with the testing data set TODO Expand this section Ranking of models and features Ranking of models. I ve borrowed that one straight from this very nice kernel because it s a useful summary display of how our models perform At face value some classifiers perform better than others. Fare _cat Let s remind ourselves of the distribution of Fare with respect to Pclass To simplify this broad distribution we decide to classify the fares into 3 fare categories 0 10 10 100 and above 100. one with replacement of the original training set. Despite this oversimplification Naive Bayes classifiers are performing well in many cases. Usually it s most interesting to start with the strong signals in the correlation plot and to examine them more in detail. Check out the other visualisations in your forked copy. I recommend to briefly describe the details of your submission e. This is the default setting. Other correlations are visible in the heatmap above. However since the main point of this challenge is to practice data analysis it is certainly worth to take your time to examine the question in a bit more detail. Ttype and Bad _ticket Let s have a look at the ticket numbers and see whether we can extract some additional deck information from them. where was contributed by GeorgeChou https www. However it could also mean that the listed value is the cumulative fare per cabin and it was simply recorded as the same value for each passenger. For males being in 1st class gives a survival boost otherwise the proportions look roughly similar. This majority process can either give all individual votes the same importance or assign different weights to make some classifiers have more impact than others. Let s try it out Our usual factorplot examination highlights the differences between Pclass as expected but also shows some interesting variations within the Sex feature. Feel free to try them out and let me know whether you find them useful. Here the size of each step multiplied by a factor 0 1. Solution pruning setting maximum depth or PCA beforehand to find the right number of features. Now that is interesting. min_samples_leaf 5 is a useful initial value. Deck Ok so what can we tell from the Deck derived from the Cabin number First of all the overall survival statistics is much better than for the full sample which is what we found above. An individual estimator may have a poor accuracy but if you combine several of them the resulting mean or median average will have a reduced uncertainty. there were more males among the 3rd class passengers. We learn There is a strong impact of Sex and Pclass on this new feature. For this we define everyone under 30 or with a title of Master Miss or Mlle Mademoiselle as Young. Advantages Effectiv in high dimensions and versatile with different kernel options. Set ylabels Fill in initial metrics tp tn etc. 2 Support Vector Classifier parameters Create objects for each classifier Create Numpy arrays of train test and target dataframes to feed into our models Create our OOF train and test predictions. Another convention is to initialise this sequence of models with a single prediction value like the mean of the training survival values. We learn Male children appear to have a survival advantage in 2nd and 3rd class. test of course doesn t have the column that indicates survival. We excluded PassengerID which is merely a row index. Let s find the two passengers and assign the most likely port based on what we found so far These are two women that travelled together in 1st class were 38 and 62 years old and had no family on board. com Microsoft LightGBM is another gradient boosting tool which in 2017 was beginning to eclipse the XGBoost as Kaggle s go to method for efficient boosting. A standard deviation of zero means that there s no difference. Relations between featuresAfter inspecting the available features individually you might have realised that some of them are likely to be connected. Gradient Boosting Gradient boosting This is what we call the step by step improvement of a weak classifier like a tree with only 1 node by successively applying this classifier to the residuals of the previous classifier s results. By default the data is divided up into k equally sized sub samples or folds and the classifier is trained on k 1 of them and evaluated on the remaining one e. Below we decide to assign different somewhat arbitrary weight according to how we think each classifier performs. Therefore we will use 2 Age Groups updating to the Young variable we defined above. This can be done again and again for n_estimator number of times. 05 is usually not significant and therefore solely based on these numbers we cannot say whether the SibSp 5 sample behaves different than the rest. Here we use Taner s function and also include the official sklearn example http scikit learn. com arthurtok introduction to ensembling stacking in python Now we continue to examine these initial indications in more detail. In addition already a grouping without the Parch and SibSp features suggests similar numbers for women in 1st class embarking from C 71 vs S 69 in contrast to the larger overall number of all 1st class passengers leaving from S. The idea is to define a number of possible values for each hyper parameter. com headsortails tidy titarnic along the same philosophy as this one here. Admittedly these are quite a few grouping levels but 30 C vs 20 S are numbers that are still large enough to be useful in this context. This is necessary since not all classifiers can handle string input. The method of Gradient Decent uses this Loss Function to iteratively move into the direction of its greatest decent i. In addition to the tree parameters the most important settings are n_estimators number of trees. Cabin gives the cabin number of each passenger. The overall result is not very surprising Sex and Pclass are the dominant features while everything else is of similar significantly lower importance. We learn parch 4 and sibsp 3 is bad. Not significantly so of course but certainly not worse even though S had a higher percentage of 1st and 2nd class passengers. Family We learn Again we find that having 1 3 family members works best for survival. A plot will explain it better than 1000 words. The parameter cv here defines the number of folds or alternatively something more complex as described in the docs http scikit learn. Preparing our prediction for submission Finally we pick our favourite classifier and predict the expected survival for the passengers in the test data set. Thereby this is an ensemble method which combines the results of individual classifiers to improve the accuracy. Will we ever know Maybe not. Embarking at C resulted in a higher survival rate than embarking at S. This could be useful. What about the other strong feature Sex Now this is somewhat expected since it explains the difference between S and the other ports. by playing with a forked copy rather than reading the whole thing in one go. Let s investigate that a bit more Ok now from here it looks more like S is the interesting port since survival is less probably for that one if you are a 3rd class passenger. Voting can be more powerful when used with weights so that several weaker classifiers can only successfully vote against one two stronger ones if they consistently agree on a specific prediction. Age _known Similar to the known Cabin numbers what about the passengers for which we know the age As we would expect intuitively it appears that we are more likely to know someones age if the survived the disaster. Above we created a new feature called Ttype which defines the type of a ticket through the first digit of the ticket number. This is consistent with the other statistics that show that women were more likely to travel together with larger families. Both factors mean that the impact of Young has to be studied carefully. Random outcomes with 2 possibilities like heads or tails when flipping a coin follow the binomial distribution https en. Let s see about that in the derived features. First we define some plotting function then we plot. Think of it as an average of estimators. Possibly travelling alone Sort of yes. We should include the Child feature in our model testing. almost everyone who embarked at Q went to 3rd class this means that the clear separation in the factorplot for Q isn t very meaningful unfortunately. Now we are connecting individual clues to get a glimpse of the bigger picture. The new feature is called Fare_eff_cat and behaves as follows Go to the top of the page top 6. We use overlapping histograms for ordinal features and barplots for categorical features. But in these days when you were travelling as a group family did everyone really get their own ticket Let s find out how many unique ticket numbers there are Interesting. Consequently the result of the final splits are called leaf notes on a tree it doesn t get smaller than leafs. html bagging meta estimator Decision Tree Decision Tree One of the classifiers that s easiest to visualise. Some properties and connections will be immediately obvious. But more men were travelling alone than women did. K is simply the number of neighbours that are making the decision by majority vote. One visualisation of this process is a tree trunk branching off into successively smaller structures. There seems to be some impact here that isn t captured by the passenger class. Because this is ultimately our goal to apply the classification method we learn from the training data to any data in particular the one that is used to judge this competition. It s hard to say at this stage whether there is any real impact left for the Embarked feature once we correct for these connections. An additional randomness is introduced by selecting random thresholds for each feature and using the best performing threshold. Shared _ticket Sharing a ticket appears to be good for survival. We start with a Logistic Regression to assess the importance of the individual model features. However in my opinion we have better reasons to impute C instead. The estimator above it s a KNN is used multiple times on subsets of the training sample and then it uses the average vote. We compute the new features in the combined data set to make sure that all feature realisations are complete and then split the combine data again into train and test. Load Data and Modules load complete 1. The clever way of computing the Shared _ticket values using group _by and np. Once more this info was digested from the sklearn documentation http scikit learn. This tree can now consider the full number of training samples for splitting a node at another feature instead of having to deal with the decreased sample after the first original node and the resulting impact of random fluctuations. In higher dimensions only mathematics can save you. However the significant difference between e. Common last names might not be unique in the passenger list 2. For now we just continue with a rather intuitive set of important features. It s an approach similar to bootstrapping where we use smaller samples from our data set to check whether the classifier gives similar results for each of them. Also we will start to use factorplots i. Fare _eff _cat Let s investigate the Fare affair in more detail. org wiki Perceptron K Nearest Neighbours Nearest Neighbours a non parametric classifier that uses the training data closest to each test data point to classify it. Based on this plot we define a new feature called Bad _ticket under which we collect all the ticket numbers that start with digits which suggest less than 25 survival e. Boosting works best for weak learners e. How much does it actually matter Well in the big picture these are only 2 passengers and their impact on our model accuracy won t be large. Does the age dependent survival change with sex How are pclass and fare related Are they strongly enough connected so that one of them is superfluous Let s find out. Here we also define a consistent colour scheme for the distinguishing between survived not survived. The names also contain titles and some persons might share the same surname indicating family relations. The matrix gives us an overview as to which features are particularly interesting for our analysis. In our training data set about 60 of the passengers didn t survive. nbsp Best of success and enjoy learning matplotlib inline for seaborn issue dummy tab. SibSp Parch Having 1 3 siblings spouses parents children on board SibSp 1 2 Parch 1 3 suggests proportionally better survival numbers than being alone SibSp Parch 0 or having a large family travelling with you. Rule of thumb for classification max_features sqrt all_features. decision stumps whereas for Bagging Averaging to be successful we want to overfit a littleThe random in random forest comes from the method of training each tree using a random bootstrap sample i. Following that parameter min_weight_fraction_leaf is less biased towards dominating classes. In addition they are fast to compute and only require relatively little data to perform well. Earlier we had a look at the Survived statistics of the individual features in the overview figure. org stable modules cross_validation. Nonetheless we ll try because we are optimistic people at heart. The closer to white a colour is the weaker the correlation. However we see again that a large part of this effect disappears once we control for Sex and Pclass. Disadvantages include Problem A tendency to overfitting. com 2017 01 23 a kaggle master explains gradient boosting Source 2 http scikit learn. Age is the integer age of the passenger. Missing values missing complete 1. One suggestion is to use a large number of highly overfitted trees with small split limits and no depth limit. Let s come back to this point in discussing the derived features. html which shows more information if we need it. html gradient tree boosting In addition This is the only instance where we import a module right when it s needed instead of up top. There a bit more tuning might be appropriate. Plus scrolling only works with arrow keys at least for me on Firefox. One more step is to provide a sampling of rows and features like in the random forest discussed above to increase the diversity in tree splits and thereby a larger amount of information for the method to work with. Example if sex male then go left else go right. This avoids overfitting. It shows scatter plots for the different feature combinations plus a distribution of each feature on the diagonal. Otherwise it s called Pasting. how well the classifier that was trained on a particular sample can be applied to new data. This is a kind of inbuilt cross validation step since the accuracy score of the classifier is estimated on data it wasn t trained on. We should test the predictive power in our modelling. Visualising the tree helps to understand how well it is fitting the data. Source http scikit learn. The step sizes can vary from iteration to iteration. With these optimised parameters let s have a look at the feature importance that this classifier gives us As expected Pclass and Sex have the most impact but our engineered features are doing not bad either. Life just isn t fair. Above we extract the standard deviation of the Fares among the ticket groups. For sparse X convert to sparse csc_matrix to speed up the learningAll of the information above is digested from the sklearn documentation http scikit learn. The 2nd point is somewhat curious since we recall from above that the survival chances for Q were actually slightly better than for S. Lower learning rates make for a slower decent which seems to be empirically more effective. We know that by definition some of our engineered features will have a high collinearity i. Parch in 1 3 and Sibsp in 1 2 is good. Removing less important features will help you to reduce the noise in your prediction and allow your model to generalise to new data which is our priority goal in machine learning. We learn Age decreases progressively as Pclass decreases from 1st to 3rd Most older passengers are 1st class but very few children are. Just change the comment tags to switch between the options. For SibSp we have 15 vs 3 5 vs 0 and 7 vs 0. One of the most popular ways to check a model for robustness is called cross validation. Others will require you to examine the data or parts of it in more specific ways. arange 0 81 1 layout 3 1 sharex True figsize 8 12 for some reason in this plot the colours for m f are flipped max_depth 3 class_weight balanced min_weight_fraction_leaf 0. We learn In the training data a large majority of Cabin numbers are missing together with 177 Age values and 2 Embarked values. Larger values suppress noise but smooth out decision boundaries. Mrs does not contain many teenagers but has a sizeable overlap with Miss especially in the range of 20 30 years old. For this to work we need to adjust for the zero fare entries. Title What can we learn from the titles in the passenger names These could give us a direct independent way to estimate the missing age values so let s look at all the available titles their frequency and mean age. This part of the analysis is called Feature Engineering. left out of the bag. Here s how the standard deviations compare We might even be at a stage now where we can investigate the few outliers more in detail That s really cheap for a 1st class cabin. Of course this assumption should be tested by doing the last name thing too. For instance Master is a boy while Mr is a man. The method used for computing the scores is by default the native scoring method of the classifier but can be changed. The larger the better although improvements become marginal eventually max_features number of random features per subset. That is roughly 20 of the cases that were classified correctly. This is a bit of a generalisation in terms of how Miss and Mrs overlap but it might be a useful starting point. I have also indicated Age 10 which we will use to define children vs teenagers in the engineering part below. In the model parameters this factor is called the learning_rate. The upper right vs lower left triangle that make up this plot contain the same information since the corresponding cells show the correlation coefficients of the same features. This is an iterative process in which you improve your model step by step until you have found the largest feature combination which still has significant impact. The factorplot suggests that bad tickets are worse for male passengers and 3rd class passengers. We could go through the trouble here to identify families by last name. Python walk through for Titanic data analysis This is a work in progress. Note This is not a streamlined analysis but it contains certain redundancies with the purpose of featuring and exploring different visualisation and modelling tools that can be useful in approaching a binary classification problem. Read more in the extensive Kaggle Ensemble Guide https mlwave. It seems that embarking at Q improved your chances for survival if you were a 3rd class passenger. Rather small numbers though. The largest number of cases we have is for B vs C. Comments and critical feedback are always welcome. We will base our Fare prediction on the median of the 3rd class fares Go to the top of the page top 5. We learn There remains a potential trend for males and for 3rd class passengers but the uncertainties are large. Overall there is a certain amount of variance and we re not going to be able to pinpoint a certain age based on the title. We fill in the 1 missing Fare value in the test data frame accordingly. weights uniform assigns equal weight to each neighbour whereas distance gives more weight to neighbours that are closer. Let s try to do better than that. We will start with a grid search algorithm to find the best parameters to run our classifier. groups of pointplots from the seaborn plotting package to visualise the categorical relations We learn Both the factorplot and the mosaicplot indicate that almost all females that died were 3rd class passengers. In addition many modelling procedures break down when missing values are involved and the corresponding rows will either have to be removed completely or the values need to be estimated somehow. It might be worth our while to include this feature in at least the initial stages of modelling to see how it performs. Here left and right defines a split at a so called node the decision itself. There is a notable shortage of teenagers compared to the crowd of younger kids. Here we also use an Out of bag score oob _score True. Let s see whether that s significant Just about formally significant i. But for less than 25 of cabins known this might not be very useful. I m happy to see that my notes are useful for others who are starting out in data analysis and machine learning and I hope that you will be able to get some inspiration from this kernel. Of course the causality might as well go the other way but that s not really the question here. Strong correlations between two other features would suggest that only one of them is necessary for our model and including the other would in fact induce noise and potentially lead to over fitting. std label adjust these methods to my notation training and train test split parameters for reproducibility set folds for out of fold prediction Class to extend the Sklearn classifier this basically unifies the way we call each classifier function for out of fold prediction split data in NFOLDS training vs testing samples select train and test sample train classifier on training sample predict classifier for testing sample predict classifier for original test sample take the median of all NFOLD test sample predictions changed from mean to preserve binary classification Put in our parameters for selected classifiers Random Forest parameters max_features 0. Let s follow up the numbers for Pclass vs Embarked with a pandas crosstab plot We learn a high percentage of those embarked at C were 1st class passengers. further away from the life boats. hist by combine Pclass bins np. This feature is a character string of variable length but similar format. This conflates the impact of Age and Pclass on the survival chances. Also we see that there s quite a range in fares. behave similarly with other new or existing features. For example we fit a tree determine its results prediction survived vs not survived compute the residuals of this prediction vs the real survival numbers all in the training data of course and then fit another tree to these residuals. There might be a correlation with other variables here though. But this could have natural reasons. html is a rapid classification method. Problem Creating biased trees if some classes dominate. However here we get 1 warning per n_estimators from a depreciation warning in the inner workings of the classifier over which we have no control. There is little use in having a classifier that replicates perfectly the training data by following every random noise feature in that data called overfitting but doesn t perform well with new data. A little follow up For SibSp we see in the plot that most of the differences are not very significant overlapping error bars. Afterwards we will try to gradually adapt and simplify the approach to make use of the work we have already done above for all the individual classifiers. A random forest is an averaging classifier for which we train several estimators independently and then average over their individual predictions. com arthurtok introduction to ensembling stacking in python and the references therein. This might be enough to explain all the variance in the Age _known variable. This is an ordinal integer feature. These base results will be used as new features Extra Trees Random Forest AdaBoost Gradient Boost Support Vector Classifier Survived surv_pred. Fare This is case where a linear scaling isn t of much help because there is a smaller number of more extreme numbers. TODO Why do some people have multiple cabin numbers What does that mean Ticket numbers That seems to be a hopeless variable at first because it just looks like random strings. Logistic Regression again this time with only the selected columns Perceptron Perceptron This is a binary classifier that creates a linear decision boundary based on a hyper plane in the parameter space. The minimum maxim values for pclass age sibsp parch show us the range of these features. and making it run in our environment. Hence decision tree. In the next step we will try to incorporate the information from the great Introduction to Ensembling Stacking in Python by Anisotropic https www. org stable modules generated sklearn. Examining Optimising one classifier in more detail For each of these various classifiers we can have a closer look to improve their performance and understand their output. Positive vs negative correlation needs to be understood in terms of whether an increase in one feature leads to an increase positive or decrease negative in the correlated feature. 2 Extra Trees Parameters max_features 0. The confusion matrix plot would allow us to identify significant imbalances in our prediction between the false positives and the false negatives. Here are the age distributions for those We see that Master is capturing the male children teenagers very well whereas Miss applies to girls as well as younger women up to about 40. org stable modules ensemble. A small number will lead to overfitting a large number prevents learning. TODO This part is still quite rudimentary and will be expanded in future versions. We start out by copying the relevant parts of the script verbatim standing on the shoulders of giants and so on. And combine the available features of train and test data sets. We can use a binomial test to estimate the probability that 5 non survivors out of a total 5 passengers with SibSp 5 happened due to chance assuming the overall 38 survival chance for the entire sample. We also create our training and testing feature sets. org stable modules svm. com c titanic tutorials. More importantly there is a reasonable argument to be made for this new Fare_eff feature to represent the actual fare better than the original feature. For this we create a Fare_eff feature above which we derive by dividing Fare by the number of people sharing a ticket Ticket_group which we also newly created. First we make sure that the passengers in each group really had the same Fare values Almost 100 yes. This means that we grow our trees from a sub sample of the training sample using bootstrapping boostrap True and estimate the accuracy based on those entries that were not picked i. We learn There don t seem to be strong differences in Age among the Embarked categories that would point at an imbalance that goes beyond the influence of Pclass and Sex. First a broad overview. com georgechou in the comments. These estimates are smoothed and therefore extend beyond the actual values look closely at the dotted zero level. Alongside the individual features we also compute a median importance. In addition we include the option to use a confusion matrix from this website http notmatthancock. We have already encountered this strategy in our Random Forests or Bagging estimators above where the aim was to get a more accurate estimate from combining multiple runs of a single classifier like a Decision Tree for instance. Normally I would recommend not to ignore warnings but to fix what s causing them. Model validation We want to make sure that our classifiers are not overfitting random data features. We learn Pclass is somewhat correlated with Fare 1st class tickets would be more expensive than 3rd class ones SibSp and Parch are weakly correlated large families would have high values for both solo travellers would have zero for both Pclass already correlates with Survived in a noticeable wayIn addition we plot a Pairplot of the numerical features. An example would be rich woman vs poor man but this particular distinction should be handled well by most classifiers. Their Fares are close enough though to include them in the general treatment. Above we are creating a kind of summary dashboard where we collect relevant visualisations to study the distributions of the individual features. This feature should be tested in the modelling stage. Additional notes Parameters min_samples_split and min_samples_leaf control the number of samples at a leaf note. So is parch sibsp 0 i. Let s study the relation between Fare and Pclass in more detail We learn There is a broad distribution between the 1st class passenger fares rich super rich There s an interesting bimodality in the 2nd class cabins and a long tail in the 3rd class ones. html for plotting confusion matrices. html true negative false positive etc. Maybe a transcription error in the data itself And that s quite expensive for a 3rd class ticket. html forest Extremely Randomised Trees Extremely Randomised Trees is an ensemble classifier similar to random forests. Is this another of these cases It actually is. Also given that so few cabin numbers are know it doesn t seem that there are good records of them. Except for 3rd class the survival for Embarked Q is close to 100 split between male and female. The first split can be followed up by additional ones to narrow down the decision criteria based on the subset defined by each previous split. Missing values Knowing about missing values is important because they indicate how much we don t know about our data. The latter one which you can comment also includes the possibility to plot a normalised confusion matrix. This is the part where the detective puts individual clues together to see whether their sum is more than its parts. Advantages of decision trees are that they can deal with both numerical and categorical data are able to handle multi output problems and are easy to follow and interpret. Therefore it would be more useful to replace the predominantly tree based sample of classifiers with a more diverse set. html Naive Bayes Naive Bayes http scikit learn. Each tree is a series of if then else decisions. Important parameters n_neighbors choosing the right k depends heavily on the data. The idea is to address the issue of missing Age values by combining the Age and Title features into a single feature that should still contain some of the signal regarding survival. The really expensive Fares in Pclass 1 are pretty much all gone. In summary we have 1 floating point feature Fare 1 integer variable Age 3 ordinal integer features Plcass SibSp Parch 2 categorical text features Sex Embarked and 3 text string features Ticket Cabin Name. html Random Forest Random Forest As the name suggests this classifier is using a number of decision trees instead of just a single one. We can try out Working hypothesis if your group mostly family survived then you survived as well unless you were a man and presumably helped your wife daughter lady friend. Turns out that we are more likely to know the age of higher class passengers or women which are the strongest survival predictors we have found so far. Let s follow that up a bit. This is easy to visualise in 2 dimensions as e. For our case there doesn t seem to be an imbalance. We know that some titles can indicate a certain age group. com arthurtok introduction to ensembling stacking in python into our script. However it s noticeable that fewer young adults have survived ages 18 30 ish whereas children younger than 10 ish had a better survival rate. The Loss Function describes how much the prediction is improved when shifting the predicted values by a certain amount. Alternatively you can use a seaborn heatmap for a quick and easy but less pretty plot. SibSp is another ordinal integer feature describing the number of siblings or spouses travelling with each passenger. But I think that it doesn t cover all the signal in the Parch feature. We use the matplotlib subplot tool to line up the individual plots in a grid. Preparing for modelling encode complete 1. For unbalanced problems setting class _weight balanced might be helpful compare decision tree notes. Intuitively this doesn t seem so plausible since you typically record what is paid for a ticket and not for a cabin. What are the types of data and their typical shape and content Together with the PassengerId which is just a running index and the indication whether this passenger survived 1 or not 0 we have the following information for each person Pclass is the Ticket class first 1 second 2 and third 3 class tickets were used. Together these sets of values define a grid which is quite easy to visualise in two dimensions. This process is called K fold cross validation. com kaggle ensembling guide. For larger numbers of Parch we have 4 vs 0 4 vs 1 and 1 vs 0. As we see above a ticket is not always shared by people with a common name. Here we want to look at correlations between the predictor features and how they could affect the target Survived behaviour. We study the correlation of Age with Pclass using a violin plot which is also split between survived right half and not survived left half. Metaphorically speaking this is the part where the detective finds the clues. The weak classifier itself does not necessarily have to be a tree but a tree seems to be the favourite approach to use here. org stable modules grid_search. Here we just focus a bit on the underlying idea. In the plot stronger correlations have brighter colours in either red positive correlation or blue negative correlation. Note that since we are selecting by Age which has many missing values a number of children will be in the Child False group. org stable modules neighbors. In 1st class younger adults had better survival chances than older ones. A confusion matrix contains more information than a simple score because it shows how many data points of each class were correctly incorrectly classified. This adds more depth to the overall content but it also makes the whole notebook rather extensive. The principles of bootstrapping and the out of bag score can be applied to most classifiers and we already used them in the bagging classifier above. As an example we ll be using the Extremely Randomized Trees but any other classifier can be substituted instead. Again the upper right and lower left triangle contain the same information. Another recent kernel definitely worth checking out https www. Beyond that the best decks for survival were B D and E with about 66 chance. We ve already used this cross validation above to compute the scores for the individual classifiers. Or Family and SibSp Parch. We will come back to this in the modelling stage when we will study feature importances and significances soon. More background info here http scikit learn. If you re very new to this subject then I recommend to go through each section on it s own e. This is reflected in the relatively low correlation index of the SVM with everything else. Both strongly positive or negative correlations with the Survived feature are valuable. If you want a step by step overview then have a good look at Anisotropic s Kernel https www. In my opinion the only training feature for which it makes sense to fill in the NAs is Embarked. I prefer the approach to list all the new features that we define together in one place to keep an overview. The important parameters are n_estimators number of boosting stages more is better learning_rate smaller steps need more stages max_depth tune for best performance depends on interaction of features subsample only train on a sub sample of the data set drawn without replacement. for k 4 we use 4 samples leave each of them out once and train on the other 3 then evaluate on the one we ve left out. This plot is inspired by and realised much more aesthetically in the comprehensive Ensemble Stacking Kernel by Anisotropic https www. Perfect correlation would have a correlation index of 1 perfect anti correlation negative correlation would have 1 obviously each feature is perfectly correlated with itself leading to the deep red diagonal. However we have seen before that there might be imbalances in the dominating features Sex and Plcass that create an apparent signal. But again the sharing of tickets is more frequent with females and 1st class passengers. Best to keep that in mind. Load Data and Modules Load Python modules The list of modules grows step by step by adding new functionality that is useful for this project. We can try out I suppose one could take the starting letters which might indicate specific decks on the ship and use them as an additional classifier. Therefore it seems that between more 1st class passengers embarking at C and more men at S there doesn t seem to be much actual influence in the port of embarkation. put back into the bag I suppose. Personally I like histograms for a first look at comparing two or more populations in case of scaled features. Does it matter much Probably not. Seriously you should check it out. This is another string feature. Too many Cabin numbers are missing. Parch is another ordinal integer features that gives the number of parents or children travelling with each passenger. A 60 yr old 3rd class passenger without family on board. This kind of plot is a more detailed visualisation of relationships between variables. Feel free to check it out and let me know your feedback Outline Note the hyper links kind of work in that they take you to the corresponding section but create a separate HTML page every time you click one of them. There are NaN in this column. A natural choice in this case is to transform the values logarithmically. Sex is an indicator whether the passenger was female or male. There are NaN values in this column. For instance Fare _cat and Fare. There are two types of ensemble methods boosting used below and averaging or bagging see above. By flat out predicting that everyone in the testing data set died we would get a 60 accuracy. csv will now appear in the Output tab of this kernel. Solution Use a different classifier. 5 AdaBoost parameters Gradient Boosting parameters max_features 0. This feature is a mix of SibSp and Parch which increases the overall numbers we can work with but might smooth out some more subtle effects. We should test it out in the modelling stage. Also there are no obvious outliers that would indicate problematic input data. This means it s a way to average over a large number of individual classifiers to improve their accuracy by reducing the variance noise. 02 Taner s code sklearn example code from http notmatthancock. OK let s go through the features one by one to see what we find. Following a suggestion by Taner https www. The tentative imbalance between male and female 3rd class probably reflect the observation we made earlier that men were more likely to travel alone. This difference between 18 40 yr olds might be a better feature than Sex and Age by themselves. Therefore a shared ticket might actually be a stronger predictor. We learn Several of these derived parameters are strongly correlated with Sex and Pclass. Cabin numbers This is a tricky feature because there are so many missing values and the strings don t all have the same number or formatting. For instance if the off diagonal elements were 0 and 30. Only 2 values stand out. Also in the test data there is one Fare missing cheeky selection almost 100 Age values are unknown and only 91 Cabin numbers were preserved. Now we want to combine the results of different kinds of classifiers to improve our prediction. Go to the top of the page top 8. There s a difference of about 30 vs 40 and it should be significant Very much so. TODO check cumulative fare question For each class there is strong evidence that the cheaper cabins were worse for survival. org wiki Bayes 27_theorem under the naive assumption that all predictor features are independent from each other and only related to the target variable. Relations between features relations complete 1. All the other rare titles like Don or Lady have average ages that are high enough to count as Not Young. This might be related to the fact that women were more likely to share a cabin and it would therefore indicate that the Fare might be a fare per cabin and not per passenger. Every time we can think of a new feature we come back here to define it and then study it further down. As the kernel continues to grow it is branching out in more detail into the different data analysis steps. Ada Boost AdaBoost A boosting classifier that fits sequences of weak learners that are progressively weighted toward those features that the previous weak learners misclassified. But wouldn t it be nice to combine all these different classifiers to get a more accurate overall prediction This is possible through an approach called Ensemble methods. Bagging Bagging is a general ensemble method. Intuitively classifiers that are highly correlated like ExtraTrees and GradientBoost above are already so similar that stacking doesn t change the result in a significant way. Preparing for modellingBefore we start exploring the different models we are modifying the categorical string column types to integer. And for Age we will choose a different approach below. Finally let s check what s going on between Age and Embarked The curious distribution for the Q survivors somewhat follows the overall trend for 3rd class passengers which make up the vast majority of Q but is notably narrower. This gives us a better impression how robust our results are towards generalisation i. We also recover the age difference between the ticket classes that was already obvious in earlier plots. We learn Bad _ticket might be a lower order effect that could give us some additional accuracy. This is something we will explore in more detail below. Naively one would assume that those cheap cabins were mostly located deeper inside the ship i. eXtreme Gradient Boosting XGBoost eXtreme Gradient Boosting It s not just a good name for a band but XGBoost was also the flavour of the month tool for kaggle competitions in 2016. Most children in 2nd class survived and the majority in 3rd class did too. org stable modules tree. Boys have proportional better survival chances than men whereas girls have similar chances as women have. Pclass There s a clear trend that being a 1st class passenger gives you better chances of survival. Now let s think for a moment Identical fares could mean that the fare for a cabin was shared equally among the passengers in which case our previous treatment would have been justified. Here we see that in the testing data set based on our train test split 12 people who survived were misclassified as dead whereas 21 who died were misclassified as having survived. The last plot doesn t inspire much confidence in a strong correlation between Deck and Bad _ticket but maybe it will be useful otherwise. There s a lot going on in this figure so take your time to look at all the details. The dominating ones are Mr 581 Miss 210 Mrs 170 and Master 53 with the number referring to the combined data. We use the dashed lines in the plot above for an empirical division into 3 classes which separate the cheaper Fare_eff of a Pclass group from the more expensive ones of the next one. It uses the famous Bayes Theorem https en. The devil here is in the details Why is Sex so much weaker for the boosting algorithms And why have features like Alone more impact when boosted Is it because of the lower tree depth What can we learn from these discrepancies with respect to parameter optimisation for the individual classifiers Stacking Ensemble methods Each of the individual classifiers we have used above has its strengths and weaknesses and we should always choose the classifier that s best equipped to handle a certain problem and or has been found to perform with the highest accuracy. Of course it s not the tickets themselves that are bad for survival but the possibility that the ticket numbers might encode certain areas of the ship that would have led to higher or lower survival chances. This is called Stochastic Gradient Decent Source 1 http blog. Except possibly for the Perceptron. Strictly speaking bagging is only the correct term if the sub samples are drawn with replacement i. Test and select the model features Now we are ready to model. But most large families were travelling in 3rd class. All the other titles we group into Not Young. Bagging for a decision tree classifier should be the same as using a Random Forest see below. In addition there is some variation between the 1st class male passengers but it doesn t look overly significant. Nonetheless it is a valuable exploratory tool that has a place in everyone s toolbox. Making inferences based on just a few cases is often unwise. org stable auto_examples model_selection plot_confusion_matrix. Not many of the children there survived but then there were not many children to begin with. Then we evaluate the score of the classifier at each grid point and pick the one parameter combination that gives us the best score. Problem Being just not easy to fit to certain concepts that don t lend themselves to clear yes or no decisions. Instead of reducing the residuals and the corresponding squared errors Gradient Boosting focusses on minimising the Loss Function by training the classifier on the gradient of this function. Initial Exploration explore complete 1. An additional concept is Shrinkage. Here we will see how the distributions of survivors and non survivors compare. The individual significances are not overwhelming but the trend itself might be useful. The support vectors are a subset of training data points used in the decision function. I think that without external information which we are avoiding in this notebook we can t do much better in trying to tie the ticket number to the survival statistics. The barplots show the fraction of people per group who survived. This scheme will soon be used throughout this kernel. Finally we model a fare category Fare_cat as an ordinal integer variable based on the logarithmic fare values Because of the larger number of Miss vs Master mostly women are classified as Young. Stacking of classifiers that have less correlation gives better results. For categorical features we will use barplots plus standard deviation bars to better judge the significance. Go to the top of the page top 3. In the modelling step we will first determine which of the features carry the most signal to be done and then use them to train a number of different classifiers. A similar effect can be seen in a boxplot Go to the top of the page top 4. For once it splits much cleaner between the Pclasses So well in fact that defining new fare categories seems almost redundant because Pclass already captures most of this signal. Admittedly 4 different ones are a bit of an overkill but why not document what we found. Lower numbers decrease variance and increase bias. It is well possible that some of our bad tickets are merely statistical fluctuations from the base survival rate of 38. 1 line that separates 2 classes see the link below. This is called tuning of the hyper parameters http scikit learn. Derived engineered features derived largely complete 1. Ticket is a character string of variable length that gives the ticket number. If sub samples are used then the remaining samples the ones not in the bag we re drawing the data from can be used in out of bag oob estimates oob _score True. This transformation can be easily achieved using the base 10 logarithm The 1 means that our boundaries are slightly shifted in terms of the real Fare. Also this feature should be evaluated in our modelling step to see if it s still significant in the presence of the Sex feature. However the last plot should also indicate that. Further randomness is introduced by making the node split dependent on a random subset of features instead of all of them. Just by themselves the last two are definitely not impressive. Coming soon The next step will use the pre packaged stacking classifier of the mlxtend package. For a final overview before the modelling stage we have another look at the correlation matrix between all old and new features We designed a number of new features and unsurprisingly several of those are correlated with the original features we used to create them. com varimp a mostly tidyverse tour of the titanic makes a convincing case for predicting Embarked S for these two passengers see also the comments. Modelling model medium completeness to be extended 1. First a simple cross validation using the helper function cross _val _score. However this shift avoids computing issues for the zero fare passengers and it makes little difference for our understanding of the fare groups. Nevertheless Miss is more likely to indicate a younger woman. In fact in the plot above the offset had already been applied as well. The easiest method to combine different classifiers is through a Voting Classifier. astype float axis 0. Therefore one should assume that it s more likely to know someone s cabin number if they survived. TODO Say something about the contributions and follow up with some ANOVA like analysis Run and describe several different classifiers Based on the first look we define the input columns we ll be working with. The initial modelling will allow us to decide which features are worth to take to the next step. most negative first derivative. We re looking for something a bit more subtle here. We are aware that some of the survival fractions we see above are based on small number statistics e. ", "id": "headsortails/pytanic", "size": "59288", "language": "python", "html_url": "https://www.kaggle.com/code/headsortails/pytanic", "git_url": "https://www.kaggle.com/code/headsortails/pytanic", "script": "mosaic lightgbm get_oof train_test_split predict show_confusion_matrix SklearnHelper(object) confusion_matrix mlxtend.classifier stats xgboost numpy cross_val_score seaborn ExtraTreesClassifier train show_confusion_matrix2 tree BaggingClassifier GaussianNB sklearn.neighbors sklearn.naive_bayes GradientBoostingClassifier plot_confusion_matrix scipy sklearn sklearn.linear_model KFold matplotlib.pyplot svm sklearn.model_selection pandas matplotlib.gridspec RandomForestClassifier LogisticRegression Perceptron fit VotingClassifier KNeighborsClassifier AdaBoostClassifier StackingClassifier __init__ GridSearchCV sklearn.metrics sklearn.ensemble statsmodels.graphics.mosaicplot ", "entities": "(('factorplot usual examination', 'Sex feature'), 'let') (('Also obvious that', 'input problematic data'), 'be') (('Solution', 'same value'), 'balance') (('18 30 ish children', 'survival younger than 10 better rate'), 's') (('we', 'previous results'), 'be') (('this', 'cabins'), 'be') (('that', 'classification binary problem'), 'note') (('clear separation', 'Q isn t'), 'mean') (('we', 'engineering part'), 'indicate') (('Earlier we', 'overview figure'), 'have') (('it', 'data analysis different steps'), 'continue') (('women', 'similar chances'), 'have') (('we', 'important features'), 'continue') (('it', 'NAs'), 'embark') (('This', 'smaller more extreme numbers'), 'Fare') (('then we', 'plotting function'), 'plot') (('Cabin only 91 numbers', 'one cheeky selection'), 'be') (('that', 'survival'), 'be') (('max_depth 3 class_weight', 'm f'), 'arange') (('org Perceptron Nearest Neighbours Nearest non parametric that', 'it'), 'wiki') (('previous treatment', 'case'), 'let') (('Lower numbers', 'bias'), 'decrease') (('which', 'still significant impact'), 'be') (('It', 'cases'), 'be') (('that', 'existing features'), 'be') (('Here we', 'survived'), 'define') (('Disadvantages', 'overfitting'), 'include') (('you', 'less probably one'), 'let') (('idea', 'hyper parameter'), 'be') (('SibSp alone 0 large family', 'you'), 'Parch') (('Pclass clear being', 'survival'), 's') (('integer ordinal that', 'passenger'), 'be') (('However last plot', 'also that'), 'indicate') (('It', 'sense'), 's') (('order lower that', 'additional accuracy'), 'learn') (('that', 'survival'), 'have') (('classifier', 'different somewhat arbitrary weight'), 'decide') (('some', 'them'), 'realise') (('maxim minimum values', 'features'), 'show') (('that', 'competition'), 'be') (('uncertainties', 'class 3rd passengers'), 'learn') (('Now we', 'model features'), 'test') (('we', 'Randomized Extremely Trees'), 'substitute') (('1st class younger adults', 'older ones'), 'have') (('That', 'class really 1st cabin'), 's') (('t', 'good them'), 'seem') (('Larger values', 'decision boundaries'), 'suppress') (('Finally we', 'test data set'), 'pick') (('Again upper right left triangle', 'same information'), 'contain') (('Now we', 'prediction'), 'want') (('some', 'collinearity high i.'), 'know') (('them', 'them'), 'feel') (('we', 'it'), 'html') (('We', 'giants'), 'start') (('you', 'that'), 'in') (('We', 'modelling'), 'test') (('Hopefully this', 'stacking'), 'result') (('you', 'Kernel https www'), 'have') (('again sharing', 'more females'), 'be') (('plot', 'Anisotropic https www'), 'inspire') (('classifier', 'remaining one e.'), 'divide') (('it', 'terms'), 'be') (('we', 'why what'), 'be') (('large family', 'survival'), 'Family') (('that', 'enough Young'), 'have') (('way', 'survival low chances'), 'be') (('where detective', 'clues'), 'be') (('it', 'passenger'), 'mean') (('Other correlations', 'heatmap'), 'be') (('html Extremely Randomised Extremely Randomised Trees', 'ensemble similar random forests'), 'forest') (('we', '4 vs 0'), 'have') (('We', 'model individual features'), 'start') (('method', 'thereby larger information'), 'be') (('I', 'https Titanic www'), 'for') (('hopeless it', 'just random strings'), 'have') (('that', 'already earlier plots'), 'recover') (('instead categorical we', 'continuous variables'), 'be') (('we', 'Anisotropic https www'), 'try') (('who', 'passenger'), 'suspect') (('we', 'fare zero entries'), 'need') (('Naive Bayes classifiers', 'well many cases'), 'perform') (('we', 'optimistic heart'), 'try') (('Mrs', '20 30 years old'), 'contain') (('you', 'own decision'), 'recommend') (('that', 'Pclass'), 'learn') (('they', 'same data'), 'summarise') (('max_depth tune', 'replacement'), 'be') (('Important choosing', 'heavily data'), 'parameter') (('convincing case', 'also comments'), 'make') (('strings don tricky so many missing all', 'same number'), 'number') (('that', 'html bagging estimator Decision Tree Decision meta One classifiers'), 'Tree') (('best decks', 'B about 66 chance'), 'beyond') (('we', 'classifier'), 'kiralt') (('part', 'analysis'), 'call') (('median average', 'reduced uncertainty'), 'have') (('they', 'cabin more number'), 'assume') (('Here we', 'other numerical variables'), 'start') (('support vectors', 'decision function'), 'be') (('then it', 'average vote'), 's') (('that', 'class quite 3rd ticket'), 'error') (('you', 'confusion normalised matrix'), 'include') (('classifiers', 'string input'), 'be') (('easiest method', 'Voting Classifier'), 'be') (('Young', 'two'), 'design') (('cheap cabins', 'mostly deeper ship'), 'assume') (('which', 'survival less than 25 e.'), 'define') (('less correlation', 'better results'), 'stack') (('Alternatively you', 'quick pretty plot'), 'use') (('where shape', 'kernel density underlying distribution'), 'be') (('very well Miss', 'as well younger about 40'), 'be') (('we', 'which'), 'create') (('s', 'derived features'), 'let') (('who', '12 people'), 'see') (('_ maybe it', 'Deck'), 'inspire') (('that', 'apparent signal'), 'see') (('binary that', 'parameter space'), 'Regression') (('part', 'still quite future versions'), 'TODO') (('that', 'deck T'), 'die') (('family 1 3 members', 'best survival'), 'learn') (('other we', 'Young'), 'title') (('which', 'slower decent'), 'make') (('1st very few children', '3rd Most older passengers'), 'learn') (('Personally I', 'scaled features'), 'like') (('Usually it', 'more detail'), 's') (('how well it', 'data'), 'help') (('which', 'C more women'), 'learn') (('particular distinction', 'well most classifiers'), 'be') (('classifier', 'decision trees'), 'Forest') (('survival chances', 'actually slightly S.'), 'be') (('2017 01 23 kaggle master', 'http Source 2 scikit'), 'com') (('stronger correlations', 'red positive correlation'), 'have') (('s', '2 dimensional overview'), 'let') (('code', 'http notmatthancock'), 'sklearn') (('we', '0'), 'have') (('estimates', 'closely dotted zero level'), 'smooth') (('somewhat it', 'S'), 'about') (('how results', 'i.'), 'give') (('almost Pclass', 'signal'), 'for') (('together sum', 'parts'), 'be') (('1 obviously feature', 'deep red diagonal'), 'have') (('scheme', 'soon kernel'), 'use') (('they', 'consistently specific prediction'), 'be') (('typically what', 'cabin'), 'seem') (('TODO', 'models'), 'set') (('don t', 'themselves'), 'be') (('t', 'well new data'), 'be') (('survival strongest we', 'class higher passengers'), 'turn') (('feature', 'character variable length'), 'be') (('tree', 'then else decisions'), 'be') (('sub only correct samples', 'replacement i.'), 'be') (('that', 'new data'), 'apply') (('where aim', 'instance'), 'encounter') (('we', 'notmatthancock'), 'include') (('we', 'them'), 'let') (('many', 'children'), 'survive') (('strongly positive correlations', 'Survived feature'), 'be') (('Ticket class first 1 second 2 third 3 tickets', 'person'), 'be') (('Here size', 'factor'), 'multiply') (('t', 'case'), 'seem') (('factor', 'parameters'), 'call') (('that', 'ticket number'), 'be') (('we', 'Mlle Young'), 'define') (('We', 'C'), 'let') (('we', 'bootstrap sample random i.'), 'stump') (('learning accuracy', 'actual them'), 'be') (('training data', 'passengers didn t survive'), 'set') (('completely values', 'addition'), 'break') (('otherwise proportions', 'survival boost'), 'give') (('soon next step', 'mlxtend package'), 'use') (('We', 'last name'), 'go') (('we', 'don data'), 'be') (('want', 'best survival'), 'be') (('This', 'e.'), 'be') (('us', 'false positives'), 'allow') (('dominating ones', 'combined data'), 'be') (('that', 'link'), 'see') (('it', 'leafs'), 'call') (('which', 'additional classifier'), 'try') (('Now we', 'more detail'), 'com') (('Extra Trees Random Forest AdaBoost Gradient Boost Support Vector Classifier Survived', 'new features'), 'use') (('we', 'overview'), 'prefer') (('previous weak learners', 'progressively features'), 'classifier') (('the', 'disaster'), 'know') (('additional randomness', 'best performing threshold'), 'introduce') (('closer', 'colour'), 'be') (('we', 'bagging classifier'), 'apply') (('you', 'survival'), 'seem') (('which', 'quite two dimensions'), 'define') (('really expensive Fares', 'Pclass'), 'be') (('Initial Exploration', 'as many different ways'), 'look') (('classifiers', 'others'), 'give') (('only one', 'potentially fitting'), 'suggest') (('only mathematics', 'you'), 'save') (('SibSp', 'passenger'), 'be') (('We', 'new feature'), 'learn') (('first which', 'different classifiers'), 'determine') (('http scikit', 'sklearn also official example'), 'use') (('As I', 'still variation'), 'far') (('color nosurv_col surv_col True foo', 'Age'), 'stack') (('highest ages', 'well overall distribution'), 'be') (('method', 'classifier'), 'be') (('Making', 'just a few cases'), 'be') (('results prediction', 'residuals'), 'fit') (('We', 'modelling stage'), 'test') (('Here we', 'just bit underlying idea'), 'focus') (('tree', 'resulting random fluctuations'), 'consider') (('method', 'greatest decent i.'), 'use') (('large majority', 'Age together 177 values'), 'learn') (('Others', 'more specific ways'), 'require') (('This', 'SibSp'), 'go') (('most important settings', 'n_estimators trees'), 'parameter') (('ensemble which', 'accuracy'), 'be') (('you', 'kernel'), 'm') (('we', 'same way'), 'define') (('we', 'them'), 'have') (('feature', 'modelling stage'), 'test') (('Derived engineered features', 'largely complete 1'), 'derive') (('Solution', 'features'), 'prune') (('XGBoost', '2016'), 'Gradient') (('I', 'overview'), 'define') (('This', 'survival chances'), 'conflate') (('engineered features', 'most impact'), 'let') (('therefore Fare', 'passenger'), 'relate') (('that', 'best score'), 'evaluate') (('text 3 string', 'Ticket Cabin Name'), 'have') (('that', 'entries'), 'mean') (('which', 'next one'), 'use') (('Embarking', 'S.'), 'result') (('I', 'submission e.'), 'recommend') (('well some', '38'), 'be') (('very someone', 'instead 304'), 'share') (('Relations', '1'), 'complete') (('off diagonal elements', 'instance'), 'be') (('Set coordinate', 'tinkering'), 'example') (('Cabin', 'passenger'), 'give') (('cross First simple validation', '_ val _ score'), 'cross') (('Mr', 'instance'), 'be') (('Here single trees', 'prediction probabilities'), 'combine') (('we', 'C.'), 'be') (('natural choice', 'values'), 'be') (('Nevertheless Miss', 'more younger woman'), 'be') (('these', 't'), 'matter') (('It', 'Mr Mr Ticket'), 'be') (('csv', 'kernel'), 'appear') (('K', 'cross validation'), 'call') (('dominant everything', 'else similar significantly lower importance'), 'be') (('corresponding cells', 'same features'), 'contain') (('majority then vote', 'row'), 'do') (('confidence associated 95 which', 'sparse samples'), 'show') (('we', '0 10 10 100 100'), 'let') (('certainly even S', 'class 1st passengers'), 'bad') (('we', 'numerical features'), 'learn') (('apparent pattern', 'just random noise'), 'let') (('features', 'particularly analysis'), 'give') (('so s', 'available titles'), 'title') (('difference', 'themselves'), 'be') (('valuable exploratory that', 'toolbox'), 'be') (('it', 'fare groups'), 'avoid') (('classifier Create Numpy arrays', 'OOF train predictions'), 'create') (('we', 'input columns'), 'say') (('we', 'Young variable'), 'use') (('Rule', 'all_features'), 'sqrt') (('We', 'grid'), 'use') (('most large families', '3rd class'), 'travel') (('t', '1st class male passengers'), 'be') (('right when it', 'instead up top'), 'tree') (('we', 'then it'), 'time') (('how much prediction', 'certain amount'), 'describe') (('we', 'independently then individual predictions'), 'be') (('Therefore it', 'more diverse set'), 'be') (('we', '60 accuracy'), 'predict') (('t', 'data'), 'be') (('time you', 'them'), 'feel') (('kind', 'variables'), 'be') (('we', 'Sex'), 'see') (('last two', 'Just themselves'), 'be') (('we', 'control'), 'get') (('Of course assumption', 'name last thing'), 'test') (('classifiers', 'better others'), 'borrow') (('which', 'half'), 'study') (('significantly tails', 'groups'), 'see') (('titles', 'age certain group'), 'know') (('it', 'about 30 vs 40'), 's') (('s', 'more detail'), 'let') (('we', 'bag oob'), 'use') (('we', 'title'), 'be') (('we', 'already above individual classifiers'), 'try') (('that', 'factorplot'), 'indicate') (('two that', 'board'), 'let') (('Male children', '2nd class'), 'learn') (('where we', 'individual features'), 'create') (('Kaggle', 'efficient boosting'), 'be') (('step sizes', 'iteration'), 'vary') (('earlier men', 'probably observation'), 'reflect') (('feature realisations', 'again train'), 'compute') (('new feature', 'page'), 'call') (('we', 'number statistics above small e.'), 'be') (('individual feature', 'classifier'), 'compare') (('above already so stacking doesn', 'significant way'), 'be') (('features', 'next step'), 'allow') (('as well you', 'presumably wife'), 'try') (('passengers', 'Fare really same values'), 'make') (('One', 'robustness'), 'call') (('It', 'Bayes Theorem famous https'), 'use') (('I', 'Titanic Tutorials https www'), 'recommend') (('we', 'C'), 'have') (('which', 'PassengerID'), 'exclude') (('which', '1'), 'give') (('it', 'Parch feature'), 'think') (('plane', 'training data points'), 'Machine') (('We', 'model testing'), 'include') (('new feature', 'still useful information'), 'suggest') (('We', 'individual classifiers'), 'use') (('when we', 'feature importances'), 'come') (('We', 'something'), 'look') (('s', 'group family'), 'get') (('they', 'only relatively little data'), 'be') (('that', 'survival higher chances'), 's') (('This', 'everything'), 'reflect') (('which', 'Q'), 'let') (('we', 'output'), 'have') (('we', 'integer'), 'start') (('One visualisation', 'tree trunk successively smaller structures'), 'be') (('Also we', 'fares'), 'see') (('Set ylabels', 'tn'), 'fill') (('similar effect', 'page'), 'see') (('Fare', 'ticket group'), 'see') (('we', 'also median importance'), 'compute') (('women', 'more together larger families'), 'be') (('node', 'instead them'), 'introduce') (('larger improvements', 'subset'), 'well') (('This', 'times'), 'do') (('Fares', 'general treatment'), 'be') (('that', 'still enough context'), 'be') (('predict classifier', 'Random Forest max_features'), 'adjust') (('survival', 'close to 100 male'), 'be') (('classifiers', 'data random features'), 'validation') (('We', 'test data frame'), 'fill') (('This', 'progress'), 'walk') (('However differences', 'anything'), 'be') (('gb xgb percep logistic RF cross_val_score y cv scoring extra Bag Ensemble 5 accuracy', 'Accuracy'), 'class') (('that', 'project'), 'module') (('impact', 'Young'), 'mean') (('We', 'page'), 'base') (('classes', 'biased trees'), 'problem') (('This', 'final prediction'), 'expect') (('it', 'variance noise'), 'mean') (('that', 'highest accuracy'), 'be') (('convention', 'training survival values'), 'be') (('survival chances', 'much cheaper cabins'), 'tell') (('We', 'classifier'), 'start') (('we', 'then one'), 'use') (('we', 'what'), 'let') (('we', 'deviation standard better significance'), 'use') (('passenger', 'rather memorable journey'), 'be') (('it', 'bit more detail'), 'be') (('boundaries', 'real Fare'), 'achieve') (('Here we', 'score oob _ score True'), 'use') (('who', 'group'), 'show') (('how it', 'Fare distribution'), 'let') (('Now we', 'bigger picture'), 'connect') (('It', 'diagonal'), 'show') (('Above we', 'ticket groups'), 'extract') (('you', 'competition header'), 'download') (('many', 'people'), 'look') (('medians', 'Age'), 'learn') (('that', 'majority vote'), 'be') (('Several', 'strongly Sex'), 'learn') (('increase', 'positive correlated feature'), 'need') (('This', 'approach'), 'be') (('most', 'differences'), 'follow') (('how it', 'at least initial stages'), 'be') (('we', 'different approach'), 'choose') (('which', 'ticket number'), 'create') (('that', 'roughly cases'), 'be') (('first split', 'previous split'), 'follow') (('how they', 'target'), 'want') (('doesn there t', 'embarkation'), 'seem') (('passenger other two classes', 'more especially male children'), 'be') (('We', 'feature also training sets'), 'create') (('we', 'sample SibSp 5 different rest'), 'say') (('majority', '3rd class'), 'survive') (('Embarked', 'character categorical value'), 'show') (('strong cheaper cabins', 'survival'), 'check') (('One suggestion', 'small split limits'), 'be') (('competition 418 rows', 'only columns'), 'write') (('odds', 'age'), 'learn') (('we', 'connections'), 's') (('We', 'class long 3rd ones'), 'let') (('we', 'common name'), 'share') (('which', 'priority machine learning'), 'help') (('we', 'what'), 'ok') (('small number', 'number large prevents'), 'lead') (('already grouping', 'S.'), 'suggest') (('non 5 survivors', 'entire sample'), 'use') (('recent kernel', 'https definitely www'), 'worth') (('Common last names', 'passenger list'), 'be') (('number', 'Child False group'), 'note') (('it', 'Sex feature'), 'evaluate') (('plot', 'better 1000 words'), 'explain') (('also whole notebook', 'overall content'), 'add') (('scikit', 'hyper parameters'), 'call') (('we', 'more detail'), 'be') (('what', 'them'), 'recommend') (('we', 'more subtle effects'), 'be') (('bad tickets', 'male passengers'), 'suggest') (('that', 'neighbours'), 'equal') (('predictor features', 'target only variable'), 'Bayes') (('data how many points', 'class'), 'contain') (('scikit', 'docs alternatively more complex http'), 'define') (('classifier', 'them'), 's') (('variance', 'variable'), 'be') (('We', 'categorical features'), 'use') (('ticket', 'survival'), 'appear') (('it', 's'), 're') (('they', 'output multi problems'), 'be') (('Bagging', 'Random Forest'), 'be') (('mostly women', 'Young'), 'model') (('how distributions', 'survivors'), 'see') (('sklearn documentation http above scikit', 'information'), 'learn') (('we', 'survival statistics'), 'think') (('persons', 'family relations'), 'contain') (('simple yet powerful that', 'decision well irregular boundaries'), 'be') (('s', 'them'), 'change') "}