{"name": "rsna inceptionv3 keras tf1 14 0 ", "full_name": " h3 InceptionV3 previously ResNet50 Keras baseline model h3 0 Preprocessing brain subudral soft h4 Check with an example if the correction works visually h3 1 Helper functions h3 2 Data generators h3 3a loss function and metric h3 3b Model h3 4 Read csv files h3 5 Train model and predict h3 6 Submit test predictions h3 7 Improvements ", "stargazers_count": 0, "forks_count": 0, "description": "loss function and metric 3b. test_predictions history. Dropout before the output layer Adding some optimal windowing Feel free to comment from keras_applications. com jesucristo rsna introduction eda models and Richard McKinley https www. com dcstang see like a radiologist with systematic windowing Marco https www. Weighted BCE instead of plain BCE training data lowered to 80 from 90. ModelModel is divided into three parts REMOVED The initial layer which will transform map input image of shape _ _ 1 to another image of shape _ _ 3. Not sure how much this will affect the performance of the training but it really speeded it up. valid_predictions here you could also save the predictions with np. PixelRepresentation 0 and int dcm. Submit test predictions 7. stack img ast 3 axis 1 instead reducing learning rate to 5e 4 and add decay increasing batch size to 32 from 16 Increasing training set to 90 of the data 10 for validation slight increase in undersampling fixed some hardcoding for input dims sizes training with weighted BCE again Update 4 Trying out InceptionV3 instead of ResNet50 undersampling without weights adding dense layers with dropout before output clipping HUs between 50 and 450 probably the most relevant value space normalization is now mapping input to 0 to 1 range instead of 1 to 1. Sequence object and thus should be safe for multiprocessing. 2 x x keras. append self. com marcovasquez basic eda data visualization Nanashi https www. ImprovementsSome improvements that could possibly be made Image augmentation which can be put in _read Different learning rate and learning rate schedule Increased input size Train longer Add more dense layers and regularization e. The validation predictions are exponentially weighted averaged over all 5 epochs not in this commit. com appian42 kaggle rsna intracranial hemorrhage windowing Jeremy s kernel https www. valid_images_dir verbose 2 len self. training without learning rate decay. com reppic gradient sigmoid windowing. BitsStored 12 and dcm. The new input image is then passed through InceptionV3 which I named engine. Train model and predict Using train validation and test set Training for 5 epochs with Adam optimizer with a learning rate of 0. Update 1 training for 4 epochs instead of 3. com jhoward cleaning the data for rapid prototyping fastai hopefully it s correct atleast the visualization looked good weighted average of the epochs 1 for the test predictions reducing number of epochs to 5 Update 8 Removing the extra dense layer before output layer keeping everything else the same 0. com reppic gradient sigmoid windowing kernel by Ryan Epp Thus I m trying out the sigmoid brain subdural bone to see if it improves the log loss Number of epochs reduced to 4 increased undersampling and validation predictions removed due to limited time Update 6 did not improve from 5 Going back to raw HUs with a bit of clipping Together with a first initial conv layer with sigmoid activation epochs increased to 6 from 4 and input size increased to 256 256 from 224 224 simple average of epochs 1 for the test predictions Update 7 Trying windowing based on appian42 s repo https github. values np. fit_and_predict returns validation and test predictions for all epochs. Preprocessing brain subudral soft REMOVED Many thanks to Ryan Epp https www. batch size lowered to 16 from 32. Update 2 adding competition metric for training using custom Callback for validation and test sets instead of the run function and global epochs training with plain BCE again merging TestDataGenerator and TrainDataGenerator into one adding undersampling see inside on_epoch_end will now run 6 epochs Update 3 skipping removing windowing value clipping but the transformation to Hounsfield Units is kept removing initial layer doing np. Code is taken from his kernel see his kernel for more information and other peoples work for example David Tang https www. valid_df print validation loss. Helper functions read and transform dcms to 3 channel inputs for e. resnet import ResNet50 Sanity Check Example dicoms ID_2669954a7 ID_5c8b5d701 ID_52c9913b1 ID Label 4045566 ID_5c8b5d701_epidural 0 4045567 ID_5c8b5d701_intraparenchymal 1 4045568 ID_5c8b5d701_intraventricular 0 4045569 ID_5c8b5d701_subarachnoid 1 4045570 ID_5c8b5d701_subdural 1 4045571 ID_5c8b5d701_any 1 example of a bad data point i. Data generatorsInherits from keras. valid_predictions let s do a weighted average for epochs 1. 4f weighted_log_loss_metric self. InceptionV3 previously ResNet50 Keras baseline model This notebook takes you through some important steps in building a deep convnet in Keras for multilabel classification of brain CT scans. valid_predictions axis 0 weights 2 i for i in range len self. 1 x callbacks train set 00 and validation set 10 lets go for the first fold only obtain model obtain test validation predictions history. uses bsb_window from previous cell REMOVED Source for windowing although now partly removed from this kernel https www. 0005 and decay rate of 0. predict_generator DataGenerator self. com jhoward cleaning the data for rapid prototyping fastai cleaning Check with an example if the correction works visually 1. InceptionV3 could be replaced by any of the available architectures in keras_application. save x keras. Finally the output from InceptionV3 goes through average pooling followed by two dense layers including output layer. int_shape x 1 activation relu name dense_hidden_1 x x keras. RescaleIntercept 100 True Another sanity check for training phase we undersample and shuffle keep probability of any 0 and any 1 training phase test phase Commented out to save time self. com appian42 kaggle rsna intracranial hemorrhage instead I also include some cleaning based on Jeremy s kernel https www. doing 5 epochs instead of 6 Update 5 Got some inspiration from this great https www. At first I thought I couldn t use sigmoid windowing for this kernel because of how expensive it is to do but I could resize the image prior to the transformation to save a lot of computation. NEW Based on two great kernels appian42 s repo https github. com omission eda view dicom images with correct windowing. com omission eda view dicom images with correct windowing 2. ", "id": "akensert/rsna-inceptionv3-keras-tf1-14-0", "size": "5660", "language": "python", "html_url": "https://www.kaggle.com/code/akensert/rsna-inceptionv3-keras-tf1-14-0", "git_url": "https://www.kaggle.com/code/akensert/rsna-inceptionv3-keras-tf1-14-0", "script": "math window_with_correction PredictionCheckpoint(keras.callbacks.Callback) floor _build keras InceptionV3 MyDeepModel weighted_log_loss_metric read_trainset tqdm_notebook as tqdm numpy _normalized_weighted_average correct_dcm ResNet50 weighted_loss save on_epoch_end window_testing bsb_window tqdm_notebook backend as K on_train_begin tqdm window_image keras_applications.resnet tensorflow weighted_log_loss __data_generation fit_and_predict matplotlib.pyplot DataGenerator(keras.utils.Sequence) ceil window_without_correction backend keras_applications.inception_v3 sklearn.model_selection pandas datetime read_testset load ShuffleSplit _read __len__ __init__ __getitem__ log ", "entities": "(('correction', 'example'), 'clean') (('I', 'computation'), 'think') (('test 1 predictions', 'everything'), 'clean') (('com dcstang', 'Marco https systematic windowing www'), 'see') (('validation predictions', 'commit'), 'weight') (('input size', 'repo https github'), 'sigmoid') (('InceptionV3', 'keras_application'), 'replace') (('Finally output', 'output layer'), 'go') (('Train model', '0'), 'set') (('we', 'training phase test time 1 self'), 'RescaleIntercept') (('it', 'really it'), 'sure') (('valid_predictions', 'range len self'), 'axis') (('Helper functions', 'e.'), 'read') (('00 validation', 'model obtain test validation predictions only history'), 'go') (('intracranial instead I', 'kernel https www'), 'com') (('I', 'engine'), 'pass') (('Weighted BCE', '90'), 'lower') (('input Increased size', 'longer more dense layers'), 'improvement') (('value space probably most relevant normalization', 'to 1 range'), 'size') (('transformation', 'np'), 'update') (('s', 'epochs'), 'let') (('kernel', 'other example'), 'take') (('output layer', 'keras_applications'), 'dropout') (('initial which', 'shape'), 'divide') (('doing', 'https great www'), 'Got') (('com jesucristo rsna', 'eda models'), 'introduction') (('here you', 'np'), 'save') (('resnet import Sanity Check Example', 'data ID 4045566 0 4045567 1 4045568 0 4045569 1 4045570 ID_5c8b5d701_subdural 4045571 ID_5c8b5d701_any 1 bad point'), 'ResNet50') (('Keras baseline notebook', 'brain CT scans'), 'InceptionV3') "}