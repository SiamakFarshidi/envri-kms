{"name": "vertex ai with mlb player digital engagement ", "full_name": " h1 Getting Started on Vertex AI Notebooks h1 Notebook Setup h3 1 Download this Notebook h3 2 Download Kaggle API Key h3 3 Sign up for Google Cloud Platform h3 4 Create a Project and Enable the Notebook API h3 5 Create a Notebook Instance h3 6 Upload MLB Notebook and API Key h3 7 Authenticate Kaggle API and Download MLB Date h3 9 Run this Tutorial h3 8 Cleaning Up h1 MLB Getting Started h1 Model h1 Evaluate h1 Explain h1 Hyperparameter Tuning with Vizier h2 Setup h2 Define Hyperparameters h2 Create Study h2 Run Study ", "stargazers_count": 0, "forks_count": 0, "description": "Now run the next cell to enable the Vertex AI API on your project and install the Python client in this notebook instance. com notebooks pricing. Run the following cell on AI Notebooks with a Cloud TF image to see model explanations. com c mlb player digital engagement forecasting data for a complete description. So after you re done be sure either to stop the notebook instance from the Notebooks page or else to delete the Cloud project you created. See the StudySpec API documentation https cloud. We ve computed attributions for the validation set the last 30 days of the data but you could try a different set if you like by changing the instances parameter. We will reupload this in an AI Notebooks instance to take advantage of the Explainable AI service. We ll put our model definition and training inside a function we can pass hyperparameters to. Learn more about Notebooks pricing https cloud. Increase the number of Fourier components to model seasonality with in more detail. com vertex ai docs explainable ai overview. You should see a Feature Attributions chart above. Be sure to keep the default TensorFlow Enterprise environment. Or step through cell by cell if you d prefer. To create a new token click on the Create New API Token button. The next cell imports the Vizier service and defines some helper functions we ll use to run the hyperparameter searchNow we ll create the client that will communicate with the Vizier service. Let s finish by looking at the best set of hyperparameters we found Set Matplotlib defaults Helper function to unpack json found in daily data Add Output widgets for each pandas DF as tabs children Rename tab bar titles to df names Display corresponding table output for this tab name Define dataframes to load from training set targets features Other dataframes available for features games rosters teamBoxScores transactions standings awards events playerTwitterFollowers teamTwitterFollowers Read training data Convert training data date field to datetime type Unpack nested dataframes and store in dictionary training_dfs Create date index Aggregate multiple games per day by summing Match target dates to feature dates and create date index Aaron Judge of the NY Yankees Process dataframes Merge for processing Process Filter for chosen player Restore features and targets Create temporal features Create train validation splits Aaron Judge of the NY Yankees number of annual seasonal components attributions relative to the median of the target the names of each feature explanations for the validation set needs to match name of InputLayer in the model ALGORITHM_UNSPECIFIED means Bayesian optimization can also be GRID_SEARCH or RANDOM_SEARCH Fill in your project ID These will be automatically filled in Define the hyperparameter feasible space the name of the quantity we want to minimize choose MINIMIZE or MAXIMIZE Call a helper function to create the study Parse hyperparameters Create and train model Optimize the metric monitored by early_stopping val_loss by default The metric needs to be reported in this format set 1 to get suggestions in parallel good for distributed training keep the name the same to resume a trial. That done we ll call another helper function to run the hyperparameter search. Feature attributions can help you tune your model by indicating which features are important and which are not. Explain Explainable AI on Vertex AI Notebooks lets you compute feature attributions for neural networks. You ll be redirected to a page with a list of your notebook instances. Google Research has a great whitepaper describing the capabilities of Vizier in detail Google Vizier A Service for Black Box Optimization https storage. Features with little importance you could consider dropping from your feature set. You ll need to provide it with your project id. Hyperparameter Tuning with Vizier In the second half of this notebook we ll demonstrate Vertex Vizier Vertex AI s hyperparameter optimization service. In Vizier hyperparameters can be one of four types DOUBLE or INTEGER defined by minimum and maximum values or CATEGORICAL or DISCRETE defined by an enumeration of values. com pub tools public publication data pdf bcb15507f4b52991a0783013df4222240e942381. Create Study Now we ll create a study. Among its capabilities are a Bayesian Optimization algorithm to search efficiently within a hyperparameter space transfer learning to make use of information from previous hyperparameter studies and automated early stopping when tuning models that train incrementally like neural nets with stochastic gradient descent or gradient boosted trees. Getting Started on Vertex AI Notebooks This notebook demonstrates how to do the following on Vertex AI Google s powerful new machine learning platform run the getting started notebook on Vertex AI Notebooks to load the data create a model generate predictions explore explainable AI on Vertex AI to refine your features tune hyperparameters with VizierIt is a complement to the Getting Started with MLB Digital Engagement https www. com ryanholbrook getting started with mlb player digital engagement notebook. Hyperparameters of numeric type may also have their feasible space scaled https cloud. your Gmail account to sign in. Model Now we ll create a neural network with Keras and fit it to our training data. Download Kaggle API Key We ll use the Kaggle API to download the competition data to the notebook instance. com and click on Get Started For Free. Feature attributions describe the contribution each features makes to the final prediction relative to a baseline. We ve picked out a few features from the playerBoxScores dataframe but there are lots more you could try see the data documentation https www. A study conducts trials in order to optimize one or more metrics. Run Study Look at your Studies https console. com vertex ai docs vizier overview for a nice overview. While in preview Vertex Vizier is offered at no charge https cloud. Notebook Setup 1. This is a two step sign up process where you will need to provide your name address and a credit card. It may take a few minutes for the instance you just created to start up. A trial is a selection of hyperparameter values together with the outcome they produce. You ll need a copy of your Kaggle credentials to authenticate your account. com ryanholbrook getting started with mlb player digital engagement tutorial which was designed to be run on Kaggle Notebooks. We ve created one of each below for illustration. com vertex ai docs reference rest v1beta1 StudySpec for more about scaling and other options. com vertex ai pricing vizier. We also see that the model did make use of several of the Fourier features modeling annual seasonality. Sign up for Google Cloud Platform If you don t have a GCP account already go to https cloud. Upload MLB Notebook and API Key From inside JupyterLab click the Upload Files up arrow button in the file browser on the left and upload the files kaggle. MLB Getting Started The first part of this notebook reproduces the data and model setup of the Getting Started https www. In our case the hyperparameters define a neural net architechture and training regimen and will produce a validation loss the metric we hope to minimize. Create a Project and Enable the Notebook API Follow the directions at https cloud. Also see the Vizier guide https cloud. The next cell has a helper function to load data for only a single player by default Aaron Judge of the NY Yankees who had the highest overall engagement during the training period. Create a Notebook Instance Next go to https notebook. ipynb fileNow we can look at explanations using the explainable_ai_sdk library. Run this Tutorial After you ve completed the setup steps above in Vertex AI Notebooks select Run from the menubar and Run Selected Cell and All Below to run through the rest of this notebook automatically. Also be sure to click the Analysis tab for the results presented in a parallel coordinates chart. Scroll down to the section of the page labelled API. Once the notebook instance is running click OPEN JUPYTERLAB just to the right of the instance name. Click the Copy and Edit button to the upper right. The starter account is free and it comes with 300 credit that you can use. Enter an Instance name of your choice and then click the blue CREATE button at the end of the page. Download this Notebook Start by creating your own copy of this notebook. This will download a fresh authentication token onto your machine. You could also look at explanations for other players the players dataframe can tell you the playerId for each player. com vertex ai experiments studies tab and select your study for a summary of the results. This will take you to your account settings at https www. Our feature attribution chart suggest that features like atBats and strikeOuts are important for digital engagement with Aaron Judge but stolenBases much less so. It s common to apply logarithmic scaling when you want to search efficiently across orders of magnitude like for a learning rate between 1e 6 and 1e 1. Authenticate Kaggle API and Download MLB Date Run the next cell to download the competition data. Cleaning Up You ll be billed for any time you keep the notebook instance running. The results of Explainable AI will be easier to understand if we restrict our analysis to a single player. To get attributions relative to another baseline the mean say just change the computation for input_baselines above. Each of the attributions is computed on a given set of instances relative to a baseline in our case the median on the training set. If you don t know your project ID you may be able to retrieve it with this command Define Hyperparameters The search space or feasible space for each hyperparameter is determined by its type. For this step you will need to provide a Google Account i. This tutorial uses Cloud Notebooks a billable component of Google Cloud. com vertex ai docs reference rest v1beta1 StudySpec scaletype. From the site header click on your user profile picture then on My Account from the dropdown menu. Read more about Vertex Explainable AI here Introduction to Vertex Explainable AI for Vertex AI https cloud. Setup First make sure you ve run everything in the notebook prior to this up to Data Pipeline. You should be redirected to a JupyterLab environment. Evaluate We can get an idea of how well our network was able to fit the data by plotting its predictions. json and vertex ai with mlb player digital engagement. Note that the Explainable AI XAI walkthrough is at the bottom of this notebook. Now in the menubar above click File Download Notebook and save a copy of the notebook to your computer. In JupyterLab on Vertex AI Notebooks you can also review a tutorial on XAI in the tutorials explainable_ai sdk_tutorial. com notebooks docs before you begin to setup a notebook project. ", "id": "ryanholbrook/vertex-ai-with-mlb-player-digital-engagement", "size": "9777", "language": "python", "html_url": "https://www.kaggle.com/code/ryanholbrook/vertex-ai-with-mlb-player-digital-engagement", "git_url": "https://www.kaggle.com/code/ryanholbrook/vertex-ai-with-mlb-player-digital-engagement", "script": "pathlib train_test_split StringLookup keras keras.layers.experimental.preprocessing delayed numpy unpack_data seaborn configs Path explainable_ai_sdk.metadata.tf.v2 layers join_datasets create_study (CalendarFourier metric_fn google.cloud.aiplatform_v1beta1 tensorflow Dict statsmodels.tsa.deterministic matplotlib.pyplot explainable_ai_sdk.model typing Parallel make_playerBoxScores sklearn.model_selection pandas VizierServiceClient joblib ipywidgets make_targets tensorflow.keras unpack_json make_training_data run_study get_optimal_trials List params_to_dict SavedModelMetadataBuilder ", "entities": "(('notebook instance', 'instance name'), 'run') (('where you', 'name address'), 'be') (('json', 'mlb player digital engagement'), 'ai') (('you', 'Cloud else project'), 'be') (('lots you', 'data documentation https www'), 'pick') (('we', 'hyperparameter search'), 'do') (('com vertex', 'docs reference rest'), 'ai') (('that', 'Vizier service'), 'import') (('hyperparameters', 'MLB Digital Engagement https www'), 'start') (('you', 'cell'), 'step') (('Each', 'training set'), 'compute') (('you', 'tutorials'), 'review') (('setup ve steps', 'notebook'), 'run') (('You', 'notebook instances'), 'redirect') (('you', 'Data Pipeline'), 'make') (('com vertex', 'nice overview'), 'ai') (('that', 'gradient stochastic descent'), 'be') (('You', 'project'), 'need') (('we', 'single player'), 'be') (('when you', '1e'), 's') (('that', 'Aaron Judge'), 'suggest') (('you', 'instance'), 'take') (('Explainable AI XAI', 'notebook'), 'note') (('ipynb fileNow we', 'explainable_ai_sdk library'), 'look') (('Download Kaggle We', 'notebook instance'), 'api') (('don GCP account', 'https already cloud'), 'sign') (('you', 'that'), 'be') (('notebook instance', 'time'), 'bill') (('Scroll', 'API'), 'label') (('This', 'https www'), 'take') (('we', 'metric'), 'define') (('use', 'annual seasonality'), 'see') (('you', 'feature set'), 'feature') (('You', 'Feature Attributions chart'), 'see') (('Model Now we', 'training data'), 'create') (('tutorial', 'Google Cloud'), 'use') (('site header', 'dropdown menu'), 'click') (('players dataframe', 'player'), 'tell') (('we', 'hyperparameters'), 'put') (('Define search space', 'type'), 'know') (('we', 'Vertex Vizier Vertex hyperparameter optimization service'), 'tune') (('Vertex preview Vizier', 'charge https cloud'), 'offer') (('study', 'one metrics'), 'conduct') (('We', 'illustration'), 'create') (('MLB first part', 'Getting Started https www'), 'start') (('hyperparameters', 'values'), 'be') (('who', 'training period'), 'have') (('how well network', 'predictions'), 'evaluate') (('you', 'Google Account i.'), 'need') (('mean', 'input_baselines'), 'change') (('You', 'JupyterLab environment'), 'redirect') (('com notebooks you', 'notebook project'), 'doc') (('Hyperparameters', 'https also feasible space scaled cloud'), 'have') (('We', 'Explainable AI service'), 'reupload') (('Google Research', 'Black Box Optimization https storage'), 'have') (('com vertex', 'scaling'), 'ai') (('features', 'relative baseline'), 'describe') (('which', 'model'), 'help') (('they', 'together outcome'), 'be') (('You', 'account'), 'need') (('suggestions', 'trial'), 'let') (('This', 'machine'), 'download') (('Run Study', 'Studies https console'), 'look') (('which', 'Kaggle Notebooks'), 'start') (('you', 'neural networks'), 'let') (('you', 'instances parameter'), 'computed') "}