{"name": "house price prediction gboosting adaboost etc ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "The data points obviously follow some sort of curve but our predictor isn t complex enough to capture that information. The graph produces two complexity curves one for training and one for validation. Now lets see the first five rows of the data Check the number of features in the data set Check the data types of each column Check any number of columns with NaN or missing values Check any number of data points with NaN As id and date columns are not important to predict price so we are discarding it for finding correlation Finding Correlation of price woth other variables to see how many variables are strongly correlated with price Printing all the correlated features value with respect to price which is target variable Pairplots to visualize strong correlation Comparing Models on the basis of Model s Accuracy Score and Explained Variance Score of different models Create 10 cross validation sets for training and testing Vary the max_depth parameter from 1 to 10 Calculate the training and testing scores Find the mean and standard deviation for smoothing Plot the validation curve Visual aesthetics. At a maximun depth of 10 model suffers from high variance since training score is 1. date It is the date on which the house was sold out. So that s why 5 should be a good choice. You have more confidence since the test dataset is similar to the training dataset but not the same nor seen by the model. bedrooms It determines number of bedrooms in a house. price to see whether they are positively correlated or negatively correlated to find if they help in prediction process in model building process or not. In fact it adapts too much to the data. What is the datatype of each column. We should keep the balance between the two. sqft_living It is the measurement variable which determines the measurement of house in square foot. STEP 7 PLOTTING OF COMPLEXITY CURVE The following code cell produces a graph for a Gradient Boosting model that has been trained and validated on the training data using different maximum depths. id It is the unique numeric number assigned to each house being sold. 0 but validation score is about 0. sqft_lot It is also the measurement variable which determines square foot of the lot. So let s apply some of the steps that we should generally do while applying machine learning models. Before doing anything we should first know about the dataset what it contains what are its features and what is the structure of data. The description for the 20 features is given below 1. STEP 5 SPLITTING DATA INTO TRAINING AND TESTING SET The training dataset and test dataset must be similar usually have the same predictors or variables. Similar to the learning curves the shaded regions of both the complexity curves denote the uncertainty in those curves and the model is scored on both the training and validation sets using the performance_metric function. Just after depth 5 training score increase upwards and validation score starts to goes down so I it begins to suffer from overfitting. However our model uses a very complex curve to get as close to every data point as possible. If you fit the model on the training dataset then you implicitly minimize error or find correct responses. yr_built It detrmines the date of building of the house. Our model is biased in that it assumes that the data will behave in a certain fashion even though that assumption may not be true. Conclusion So we have seen that accuracy of gradient boosting is around 89. sqft_basement It determines square footage of the basement of the house. STEP 1 IMPORTING LIBRARIES STEP 2 DATA CLEANING AND PREPROCESSING In this step we check whether data contain null or missing values. As max depth increases bias becomes lower and variance becomes higher. STEP 6 APPLYING MACHINE LEARNING MODEL STEP 7 ANALYZING TRAINING TIME EACH MODEL HAS TAKEN From the above figure it is inferred that decision tree has taken negligible amount of time to train where as Randome forest has taken maximum time and it is yet obvious because as we increase the number of tree 400 in this case training time will increase so we should look out for optimal model which has greater accuracy and less training time in comparison to otherSo in this case GBoost is the best choice as its accuracy is highest and it is taking less time to train wrt accuracy. There is however something wrong with the model itself in that it s not complex enough to model our data. grade It determines the overall grade given to the housing unit based on King County grading system on a scale of 1 to 11. A key point is that there s nothing wrong with our training this is the best possible fit that a linear model can achieve. condition It determines the overall condition of a house on a scale of 1 to 5. lat It determines the latitude of the location of the house. long It determines the longitude of the location of the house. If the model predicts good also on the test dataset you have more confidence. bathrooms It determines number of bathrooms in a bedroom of a house. price It is the price of house which we have to predict so this is our target variable and aprat from it are our features. Thus it s possible to prevent overfitting. Two scores are quite close but both the scores are too far from acceptable level so I think it s a high bias problem. Consequently a model with high variance has very low bias because it makes little to no assumption about the data. 28 and also achieved decent variance score of 0. Welcome to my kernel In this dataset we have to predict the sales price of houses in King County Seattle. sqft_living15 Living room area in 2015 implies some renovations 21. In other words a model is overfitting. Therefore it is inferred that Gradient Boosting is the suitable model for this dataset. They differ on the observations and specific values in the variables. STEP 4 EDA or DATA VISUALIZATION This is also a very important step in your prediction process as it help you to get aware you about existing patterns in the data how it is relating to your target variables etc. But this is also one of the most important step as it also involves domain knowledge of the field of the data means you cannot simply remove the feature from your prediction process just because it is negatively correlated because it may contribute in future prediction for this you should take help of some domain knowledge personnel. What are unique values of categorical variables etc. As zipcode is negatively correlated with sales price so we can discard it for sales price prediction. The dataset cantains 20 house features plus the price along with 21613 observations. Again the data points suggest a sort of graceful curve. 87 which is very close to 1. So by splitting dataset into training and testing subset we can efficiently measure our trained model since it never sees testing data before. waterfront This feature determines whether a house has a view to waterfront 0 means no 1 means yes. In other words the model is underfitting. Interpretation of the Curve At a maximum depth of 1 model suffers from high bias. I am just splitting dataset into 20 of test data and remaining 80 will used for training the model. The fitted model provides a good prediction on the training dataset. yr_renovated It detrmines year of renovation of house. As we see from the curve max depth of 5 best generalizes the unseen data. floors It determines total floors means levels of house. Apart from that your valuable suggestions for further improvement and optimization are always welcome from my side do comment Copying data to another dataframe df_train for our convinience so that original dataframe remain intact. It includes homes sold between May 2014 and May 2015. STEP 3 FINDING CORRELATION In this step we check by finding correlation of all the features wrt target variable i. Then you test the model on the test dataset. Further we can also perform model optimization by using GridSearch to find the appropriate parameters to increase the accuracy by fine tuning hyperparameters. sqft_lot15 lotSize area in 2015 implies some renovations Now we know about the overall structure of a dataset. view This feature determines whether a house has been viewed or not 0 means no 1 means yes. zipcode It determines the zipcode of the location of the house. What is the size of the data. sqft_above It determines square footage of house apart from basement. It means the model transfers prediction or learning in real sense. ", "id": "sid321axn/house-price-prediction-gboosting-adaboost-etc", "size": "7942", "language": "python", "html_url": "https://www.kaggle.com/code/sid321axn/house-price-prediction-gboosting-adaboost-etc", "git_url": "https://www.kaggle.com/code/sid321axn/house-price-prediction-gboosting-adaboost-etc", "script": "linear_model cross_validation pearsonr train_test_split LinearRegression sklearn.learning_curve RandomForestRegressor numpy seaborn tree ModelComplexity sklearn.tree GradientBoostingRegressor sklearn.linear_model sklearn explained_variance_score division matplotlib.pyplot sklearn.model_selection pandas DecisionTreeRegressor AdaBoostRegressor r2_score time __future__ ShuffleSplit sklearn.cross_validation scipy.stats sklearn.metrics sklearn.ensemble ", "entities": "(('test be', 'usually same predictors'), 'datum') (('how it', 'target variables'), 'eda') (('It', 'house'), 'zipcode') (('sqft_basement It', 'house'), 'determine') (('we', 'King County Seattle'), 'welcome') (('It', 'real sense'), 'mean') (('test dataset', 'model'), 'have') (('graph', 'validation'), 'produce') (('data Again points', 'graceful curve'), 'suggest') (('original dataframe', 'convinience'), 'comment') (('long It', 'house'), 'determine') (('description', '1'), 'give') (('Thus it', 'overfitting'), 's') (('sqft_living15 Living room area', 'renovations'), 'imply') (('It', 'May'), 'include') (('It', 'house'), 'lat') (('28', '0'), 'achieve') (('even assumption', 'certain fashion'), 'bias') (('Further we', 'tuning fine hyperparameters'), 'perform') (('data', 'values'), 'step') (('then you', 'correct responses'), 'minimize') (('it', 'enough data'), 'be') (('it', 'overfitting'), 'increase') (('It', '1 to 5'), 'condition') (('total floors', 'house'), 'floor') (('data points', 'predictor isn t enough information'), 'follow') (('It', 'house'), 'bathroom') (('Gradient Boosting', 'suitable dataset'), 'be') (('it', 'data'), 'have') (('It', '1 to 11'), 'grade') (('best possible linear model', 'wrong training'), 'be') (('Now we', 'dataset'), 'imply') (('measurement also which', 'lot'), 'sqft_lot') (('positively negatively they', 'model building process'), 'price') (('dataset', '21613 observations'), 'cantain') (('It', 'house'), 'yr_renovated') (('However model', 'data as close point'), 'use') (('that', 'different maximum depths'), 'STEP') (('structure', 'data'), 'before') (('It', 'apart basement'), 'sqft_above') (('remaining', 'model'), 'split') (('It', 'house'), 'bedroom') (('accuracy', '89'), 'conclusion') (('it', 'too far acceptable level'), 'be') (('you', 'domain knowledge personnel'), 'be') (('so this', 'target it'), 'price') (('we', 'sales price prediction'), 'correlate') (('model', 'other words'), 'underfitte') (('We', 'two'), 'keep') (('measurement which', 'square foot'), 'sqft_live') (('model', 'performance_metric function'), 'curve') (('we', 'unseen best data'), 'generalize') (('house', 'waterfront'), 'waterfront') (('training score', 'high variance'), 'suffer') (('They', 'specific variables'), 'differ') (('validation', 'Visual aesthetics'), 'see') (('we', 'target variable i.'), 'correlation') (('it', 'wrt accuracy'), 'MODEL') (('etc', 'unique categorical variables'), 'be') (('fitted model', 'training dataset'), 'provide') (('it', 'data'), 'measure') (('we', 'machine learning generally models'), 'let') (('Then you', 'test dataset'), 'test') (('you', 'more confidence'), 'have') (('house', 'which'), 'date') (('It', 'house'), 'yr_built') "}