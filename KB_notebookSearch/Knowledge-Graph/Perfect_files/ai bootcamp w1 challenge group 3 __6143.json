{"name": "ai bootcamp w1 challenge group 3 ", "full_name": " h1 Week 1 Challenge h2 The task h2 The data h3 Loading the data into Python h2 Logistic regression h2 2 Layer network h2 3 Layer network h2 Excel tips h3 Matrix multiplication h3 You cannot change part of an array h3 Transpose h3 Exponents h3 Softmax h3 Random initialization h2 Grading h2 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "So you need to make the reference to those fixed. You may choose the size of the two hidden layers yourself. GradingThis weeks challenge is not a competition for the most accurate prediction. To make training easier the data has also been normalized already. So scroll to the right to see the full sheet. 3 Layer networkAfter you have implemented a 2 layer network open a new sheet and implement a 3 layer network. The goal of the challenge is to deepen your understanding of how neural networks work. The dataYou can find all data in the datasource connected to this notebook on kaggle. The hidden unit has a size of 5. This is what we return at the end. Note that you have to copy over two sets of weights and biases this time. But first let s solve this weeks challenge. We need to learn these. Excel tips Matrix multiplicationThe excel function for matrix multiplication is called MMULT https support. Week 1 ChallengeCongratulations You made it through the first weeks content The first week is the hardest because you have to learn about many new concepts. TransposeTo transpose a matrix in excel you can use the TRANSPOSE https support. In a famous wine region a disaster has happened. For this you will have to hit CONTROL SHIFT ENTER again. 1 A_1 1 A_1 2 A_1 3 EXP A1 SUM EXP A1 C1 EXP B1 SUM EXP A1 C1 EXP C1 SUM EXP A1 C1 Random initializationYou can create random numbers in Excel using the RAND https support. Instead points are awarded to teams who submit the following implementations Excel Logistic Regression 2 points 2 Layer Neural Net 2 points 3 Layer Neural Net 4 pointsPython 2 Layer Neural Net 2 points 3 Layer Neural Net 4 pointsA total of 14 points can be won. For the python notebook create a public kernel and share the link via slack. This is what we return at the end Package imports Matplotlib is a matlab like plotting library Numpy handles matrix operations SciKitLearn is a useful machine learning utilities library The sklearn dataset module helps generating datasets Display plots inline and change default figure size Helper function to plot a decision boundary. The normalized data and one hot encodings have been copied over to the logistic regression and 2 Layer Wine net sheet. com en us article MMULT function 40593ed7 a3cd 4b6b b9a3 e4ad3c7245eb. The Excel file contains everything you need to solve the challenge in Excel. ExponentsYou can calculate e x using the EXP function. The taskYou got a call from Italy. Instead you are to build a classifier which recognizes the wine maker from 13 attributes of the wine. To multiply two matrices select the output area where you want the output to be enter the formula and hit CONTROL SHIFT ENTER. If you don t fully understand this function don t worry it just generates decision boundary plot Set min and max values and give it some padding Generate a grid of points with distance h between them Predict the function value for the whole gid Plot the contour and training examples Calculate exponent term first Clipping calue Number of samples Loss formula note that np. See this youtube tutorial https www. You deserve to kick your feed up and enjoy a glass of wine. Say you have a sample with 3 values you want to compute softmax for. So we will use these bottles as our training data. The labeling machine has mixed up the labels of three wine cultivars. Note that softmax needs all cells of the example. The original data contains the 13 measurements for all 178 bottles. Because this is a multi class problem the output y has already been converted to one hot matrix. You can train this network the same way as the logistic regression network. Loop over epochs Forward propagation Backpropagation Gradient descent parameter update Assign new parameters to the model Pring loss accuracy every 100 iterations Hyper parameters I picked this value because it showed good results in my experiments Initialize the parameters to random values. That is you enter the formula for one cell and can then expand it for the other cells. com watch v 5bNooxRm960 if you have trouble. So it is all downhill from here. Loop over epochs Forward propagation Backpropagation Gradient descent parameter update Assign new parameters to the model Pring loss accuracy every 100 iterations empty array to store losses Gradient descent. Make sure to use Paste values only when you copy over the weights Try to experiment with the learning rate while you train and get the loss as low as possible. Loading the data into PythonYou can fork this notebook or simply create a new Kernel connected to this Kernels datasource in Kaggle. com en us article TRANSPOSE function ed039415 ed8a 4a81 93e9 4b6dfac76027 function. It uses a softmax activation for the output layer. 2 Layer networkIn the next sheet you will implement a 2 layer neural network. 0 A B C 1 0. You cannot change part of an array. sum sums up the entire matrix and therefore does the job of two sums from the formula Load parameters from model Linear step First activation function Second linear step Second activation function Third linear step Third activation function Load parameters from model Load forward propagation results Get number of samples Backpropagation Calculate loss derivative with respect to output Calculate loss derivative with respect to second layer weights Calculate loss derivative with respect to second layer bias Calculate loss derivative with respect to first layer Calculate loss derivative with respect to first layer weights Calculate loss derivative with respect to first layer bias Store gradients First layer weights First layer bias Second layer weights Second layer bias Third layer weights Third layer bias Package and return model Load parameters Update parameters Store and return parameters Do forward pass get y_hat empty array to store losses Gradient descent. This makes it easier to deal with the data. The first week is also the most math heavy. If you don t fully understand this function don t worry it just generates decision boundary plot Set min and max values and give it some padding Generate a grid of points with distance h between them Predict the function value for the whole gid Plot the contour and training examples Get labels Get inputs Print shapes just to check Calculate exponent term first Clipping calue Number of samples Loss formula note that np. The activation function of the hidden layer is tanh the activation function for the output layer is softmax again. You will learn about normalization next week but the basic goal is to ensure that all features of the data have the same mean and standard deviation. The network has an input size of 13 and an output of 3. To update parameters copy over the New W1 and New b1 over into W1 and b1 on the left of the sheet. You will implement a multi class classifier in Excel and Python. To make the sheet easier to handle Forward pass Backward pass and weight update are ordered horizontally not vertically. To calculate the exponent of multiple values element wise like you have to do for softmax you enter EXP and then the range of cells. In this case either hit CONTROL SHIFT ENTER to apply the formula for the entire area or exit with ESC. Just make sure that the output has a size of 3. So to create random numbers once and then freeze them use RAND for all cells first then copy it and paste the values using paste values only on the same cells. Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280 OD315 of diluted wines Proline The wine makers had 178 bottles left in their cellars. com en us article RAND function 4cbfa695 8869 4788 8d90 021ea9f5be73 function. Your first offer to distinguish the three wine makers by taste and then spend a couple of months drinking wine and labeling bottles as been refused. Note however that the rand function create a new random number every time excel refreshes. Sometimes you might encounter a warning You cannot change part of an array. SubmissionFor the excel challenges submit your finished Excel notebook through slack. Now there are thousands of bottles of which nobody knows who made them. SoftmaxYou will have to enter the formula for softmax element wise. sum sums up the entire matrix and therefore does the job of two sums from the formula Load parameters from model Linear step First activation function Second linear step Second activation function Load parameters from model Load forward propagation results Get number of samples Backpropagation Calculate loss derivative with respect to output Calculate loss derivative with respect to second layer weights Calculate loss derivative with respect to second layer bias Calculate loss derivative with respect to first layer Calculate loss derivative with respect to first layer weights Calculate loss derivative with respect to first layer bias Store gradients First layer weights First layer bias Second layer weights Second layer bias Package and return model Load parameters Update parameters Store and return parameters Do forward pass get y_hat Gradient descent. Not that if you use transpose you always need to use CONTROL SHIFT ENTER otherwise it will not do anything. Get labels Get inputs Print shapes just to check Package imports Matplotlib is a matlab like plotting library Numpy handles matrix operations SciKitLearn is a useful machine learning utilities library The sklearn dataset module helps generating datasets Display plots inline and change default figure size Helper function to plot a decision boundary. You can load the data like this Logistic regressionYour first task it to implement logistic regression in Excel. ", "id": "odraode/ai-bootcamp-w1-challenge-group-3", "size": "6143", "language": "python", "html_url": "https://www.kaggle.com/code/odraode/ai-bootcamp-w1-challenge-group-3", "git_url": "https://www.kaggle.com/code/odraode/ai-bootcamp-w1-challenge-group-3", "script": "OneHotEncoder softmax forward_prop predict initialize_parameters accuracy_score numpy train backward_prop tanh_derivative loss_derivative matplotlib.pyplot pandas train_plot softmax_loss sklearn.metrics plot_decision_boundary update_parameters sklearn.preprocessing ", "entities": "(('python notebook', 'slack'), 'create') (('normalized data', 'one hot logistic regression'), 'copy') (('class multi output', 'already one hot matrix'), 'be') (('excel challenges', 'slack'), 'submit') (('which', 'wine'), 'be') (('utilities sklearn dataset useful machine learning module', 'decision boundary'), 'be') (('You', 'logistic regression same way network'), 'train') (('model Load Update', 'Gradient descent'), 'sum') (('You', 'Excel'), 'load') (('you', 'loss'), 'make') (('who', 'them'), 'be') (('dataYou', 'kaggle'), 'find') (('disaster', 'wine famous region'), 'happen') (('how neural networks', 'understanding'), 'be') (('labeling machine', 'wine three cultivars'), 'mix') (('you', 'Excel'), 'contain') (('You', 'wine'), 'deserve') (('activation function', 'output layer'), 'be') (('rand however function', 'new random number'), 'note') (('So we', 'training data'), 'use') (('CONTROL SHIFT', 'ENTER'), 'have') (('forward pass', 'Gradient descent'), 'sum') (('You', 'two hidden layers'), 'choose') (('wine makers', 'cellars'), 'Alcalinity') (('network', '3'), 'have') (('you', 'other cells'), 'be') (('Logistic Neural Neural Neural Excel Regression 2 2 Net 2 3 Neural Net 4 pointsPython 2 2 3 4 pointsA total', '14 points'), 'award') (('It', 'output layer'), 'use') (('SoftmaxYou', 'softmax element'), 'have') (('original data', '178 bottles'), 'contain') (('Loading', 'Kaggle'), 'fork') (('otherwise it', 'anything'), 'need') (('you', 'trouble'), 'watch') (('GradingThis weeks challenge', 'most accurate prediction'), 'be') (('Layer 2 next sheet you', 'layer neural 2 network'), 'networkin') (('you', 'many new concepts'), 'make') (('Excel tips Matrix multiplicationThe excel function', 'matrix multiplication'), 'call') (('So you', 'those'), 'need') (('layer 2 network', 'layer 3 network'), 'implement') (('you', 'TRANSPOSE https support'), 'transpose') (('you', 'softmax'), 'say') (('distance h', 'samples Loss formula'), 'understand') (('softmax', 'example'), 'note') (('first s', 'weeks challenge'), 'let') (('Pring loss accuracy', 'Gradient descent'), 'update') (('you', 'then cells'), 'enter') (('You', 'Excel'), 'implement') (('CONTROL', 'ESC'), 'SHIFT') (('it', 'data'), 'make') (('You', 'array'), 'encounter') (('output', 'CONTROL SHIFT ENTER'), 'select') (('it', 'values'), 'update') (('EXP A1 SUM EXP A1 C1 EXP B1 SUM EXP A1 C1 EXP C1 SUM EXP A1 Random 1 2 3 initializationYou', 'RAND https support'), '1') (('features', 'same mean deviation'), 'learn') (('Backward pass update', 'Forward'), 'pass') (('Just output', '3'), 'make') (('you', 'weights'), 'note') (('ExponentsYou', 'EXP function'), 'calculate') "}