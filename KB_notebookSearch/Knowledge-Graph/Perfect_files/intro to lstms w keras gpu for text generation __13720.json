{"name": "intro to lstms w keras gpu for text generation ", "full_name": " h1 Applied Introduction to LSTMs with GPU for text generation h2 Part one Data Preparation h3 Read in the data h3 Explore the data h3 Prepare sequence data for input to LSTM h4 Subset the data to form a corpus h4 Format the corpus into arrays of semi overlapping sequences of uniform length and next characters h4 Represent the sequence data as sparse boolean tensors h2 Part two Modeling h3 Defining an LSTM network model h3 Training the model and generating predictions h2 Conclusion h3 Inspiration for next steps ", "stargazers_count": 0, "forks_count": 0, "description": "This next cell step gives us an array sentences made up of maxlen 40 character sequences chunked in steps of 3 characters from our corpus user and next_chars an array of single characters from user at i maxlen for each i. Learn more about deep learning on Kaggle Learn https www. Now that you can use GPUs in Kernels with 6 hours of run time you can train much more computationally intensive models than ever on Kaggle. ConclusionAnd there you have it If you ran this notebook interactively you hopefully caught the model printing out generated text character by character to dramatic effect. Especially if you have experience installing CUDA to use GPUs for deep learning you ll appreciate how wonderful it is to have an environment already setup for you. Since we don t care if our model generates text with correct capitalization we use tolower. Read more about why to use Keras as a deep learning framework here https keras. I m also adding a dense output layer. But first let s take a look at the first few messages from 55a7c9e08a7b72f55c3f991e to get a sense for what they re chatting about I see words and phrases like documentation pair coding BASH Bootstrap CSS etc. static code shared on GitHub is that it s often hard to know how the data you want to work with differs from the code sample. 2 will generate safe guesses whereas values of temperature above 1. When temperature is low we may get lots of the s and and s when temperature is high things get more unpredictable. id 55a7c9e08a7b72f55c3f991e to subset the data and collapse the vector of strings into a single string. Compare the speed up effect of using a CPU versus a GPU on a minimal example. I m also just going to use the first 20 of the data as a sample since we don t need more than that to generate halfway decent text. Anyway so the second is defining a callback function to print out predicted text generated by our trained LSTM at the first and then every subsequent fifth epoch with five different settings of temperature each time see the line for diversity in 0. If you re running this interactively in your own notebook you can click the blue square Stop button next to the console at the bottom of your screen to interrupt the model training. We ve explored the data and reshaped it correctly so we that we can use it as an input to our LSTM model. 0 will start to generate riskier guesses. And of course let s not forget to put our GPU to use This will make training prediction much faster than if we used a CPU. just doing the new signee stuff. hdf5 in the Output tab of the kernel. You have to download it and compare it locally which is a pain. Format the corpus from 1 into arrays of semi overlapping sequences of uniform length and next characters3. predict at the end of the first epoch followed by every fifth epoch with five different temperature setting each time. Since we re training a character level model we relate unique characters e. You can try forking this kernel and experimenting with more or less data if you want. py originally written by Fran\u00e7ois Chollet author of Keras and Kaggler to prepare the data in the correct format for training an LSTM. I hope you ve enjoyed learning how to start from a dataframe containing rows of text to using an LSTM model implemented using Keras in Kernels to generate novel sentences thanks to the power of GPUs. add LSTM batch_size input_shape time_steps features where batch_size is the number of sequences in each sample can be one or more time_steps is the size of observations in each sample and features is the number of possible observable features i. I ve selected a batch_size of 128 which is the number of samples or sequences our model looks at during training before updating. Try out more complicated network architectures like adding dropout layers. org tutorials using_gpu allowing_gpu_memory_growth Read in only the two columns we need We don t want bots user id for CamperBot Show first 100 characters helper function to sample an index from a probability array Function invoked for specified epochs. Read in the dataLooks good Explore the dataIn my plot below you can see the number of posts from the top ten most active chat participants by their user id in freeCodeCamp s Gitter So userid 55a7c9e08a7b72f55c3f991e is the most active user in the channel with over 140 000 messages. Reading in exploring and preparing the data2. is the character h to start the word hello. We re creating a sparse boolean tensors x and y encoding character level features from sentences and next_chars to use as inputs to the model we train. Defining an LSTM network model2. Can you tweak the model or its hyperparameters to generate even better text Try it out for yourself by forking this notebook kernel click Fork Notebook at the top. In a nutshell it defines how conservative or creative the model s guesses are for the next character in a sequence. We ll use RMSprop with a learning rate of 0. Note that we re using our model to predict based on a random sequence or seed from our original subsetted data user start_index random. And overall lower levels of diversity generate text with a lot of repetitions whereas higher levels of diversity correspond to more gobbledegook. Try out the same code with different data fork this notebook go to the Data tab and remove the freeCodeCamp data source then add a different dataset good examples here https www. Training the model and generating predictions Defining an LSTM network modelLet s start by reading in our libraries. Prepare sequence data for input to LSTMRight now we have a dataframe with columns corresponding to user ids and message text where each row corresponds to a single message sent. You can see how the next character following the first sequence hi folks. Using epoch 1 to be consistent with the training epochs printed by Keras define the checkpoint fit model using our gpu. You can experiment with different numbers here if you want. randint 0 len user maxlen 1. Before we feed it any data the cell below defines a couple of helper functions with code modified from this script https github. We start with a sequential model and add an LSTM as an input layer. The shape we end up with will be input_shape maxlen len chars where maxlen is 40 and len chars is the number of features i. I m using Keras which is a popular and easy to use interface to a TensorFlow backend. hdf5 in the Output to predict based on different data in a new kernel what it would be like if the user in this tutorial completed someone else s sentences. Part two ModelingIn part two we do the actual model training and text generation. And the next character following the sequence folks. If you want to review more of the theoretical underpinnings I recommend that you check out this excellent blog post Understanding LSTM Networks http colah. Part one Data PreparationIn part one I ll first read in the data and try to explore it enough to give you a sense of what we re working with. So we ll expect our novel sentences to look roughly like this if we re successful. Prints generated text. 2 for the values of temperature feel free to tweak these too. Now our model is ready. Let s fit our model with these specifications and epochs 15 for the number of epochs to train. There are two parts to this notebook 1. io posts 2015 08 Understanding LSTMs. Lower values of temperature e. There are two sections to this part 1. Represent the sequence data as sparse boolean tensorsThe next cell will take a few seconds to run. In the cell below we define the model. Format the corpus into arrays of semi overlapping sequences of uniform length and next charactersThe rest of the code used here is adapated from this example script https github. If you rerun this code yourself by clicking Fork Notebook you can print out all of the characters used. Now we can compile our model. By the end you ll learn how to format text data as input to a character level LSTM model implemented in Keras and in turn use the model s character level predictions to generate novel sequences of text. 1 to optimize the weights in our model you can experiment with different learning rates here and categorical_crossentropy as our loss function. The text generated by the model s predictions in the first epoch didn t really resemble English at all. Finally we ll use add an activation layer with softmax as our activation function as we re in essence doing multiclass classification to predict the next character in a sequence. Training the LSTM on a single user id s chat logs and generating novel text as outputYou can follow along by simply reading the notebook or you can fork it click Fork notebook and run the cells yourself to learn what each part is doing interactively. Represent the sequence data from 2 as sparse boolean tensors Subset the data to form a corpusIn the next two cells we ll grab only messages from 55a7c9e08a7b72f55c3f991e fromUser. Experiment with different hyper parameters like the amount of training data number of epochs or batch sizes temperature etc. In any case you will still want to grab some lunch or go for a walk while you wait for the model to train and generate predictions if you re running this code interactively. Cross entropy is the same as log loss commonly used as the evaluation metric in binary classification competitions on Kaggle except in our case there are more than two possible outcomes. Applied Introduction to LSTMs with GPU for text generationIn this Python notebook kernel I will use the text from freeCodeCamp s Gitter chat logs https www. Finally we name our callback function generate_text which we ll add to the list of callbacks when we fit our model in the cell after this one. We have another callback ModelCheckpoint which will save the best model at each epoch if it s improved based on our loss value find the saved weights file weights. This give the model one less dimension to learn. The first one sample samples an index from an array of probabilities with some temperature. characters in our case. Think of it as the amount of surpise you d have at seeing an English word start with st versus sg. Below you can see the models layers optimizers and callbacks we ll be using. So how do we get from a dataframe to sequence data in the correct shape I ll break it into three steps 1. Quick pause to ask what is temperature exactly Temperature is a scaling factor applied to the outputs of our dense layer before applying the softmax activation function. just doing the new signee stuf is the character f to finish the word stuff. This way we can fiddle with the temperature knob to see what gets us the best generated text ranging from conservative to creative. unique count of characters from our corpus. You can see how our model improved from the first epoch to the last. I ll use text from one of the channel s most prolific user ids as the training data. And I can only assume the sentence starting With all of the various frameworks. Inspiration for next stepsHere are just a few ideas for how to take what you learned here and expand it 1. Subset the data to form a corpus2. com learn deep learning a series of videos and hands on notebook tutorials in Kernels. The shape we define for our input is identical to our data by this point which is exactly what we need. to numeric indices in the cell below. In this way it should be clear now how next_chars is the data labels or ground truth for our sequences in sentences and our model trained on this labeled data will be able to generate new next characters as predictions given sequence input. com free code camp all posts public main chatroom to train an LSTM network to generate novel messages click on the Data tab of this kernel to view the data preview. Yep sounds like they re on topic as far as freeCodeCamp goes. We ll use their messages to train the LSTM to generate novel 55a7c9e08a7b72f55c3f991e like sentences. I ll use a GPU to train the model in this notebook you can request a GPU for your session by clicking on the Settings tab from a kernel editor. But first what is an LSTM Long Short Term Memory network anyway In this notebook we ll take a hands on approach to implementing this flavor of recurrent neural network especially equipped to handle longer distance dependencies including ones you get with language in Keras a deep learning framework. The two nice things about this tutorial using Kernels is that a I ll try to give you glimpses into the data at every significant step and 2 you can always fork this notebook and boom you ve got a copy of my environment data Docker image and all with no downloads or installs necessary whatsoever. I ve printed out the first 10 strings in the array so you can see we re chunking the corpus into partially overlapping equal length sentences. This is pretty far from the 3D shape the input layer of our LSTM network requires model. is referring to JavaScript. com datasets sortBy hottest group public page 1 pageSize 20 size all filetype all license all tagids 11208. com keras team keras blob master examples lstm_text_generation. One of my frustrations with following non interactive tutorials e. Training the model and generating predictionsFinally we ve made it Our data is ready x for sequences y for next characters we ve chosen a batch_size of 128 and we ve defined a callback function which will print generated text using model. ", "id": "mrisdal/intro-to-lstms-w-keras-gpu-for-text-generation", "size": "13720", "language": "python", "html_url": "https://www.kaggle.com/code/mrisdal/intro-to-lstms-w-keras-gpu-for-text-generation", "git_url": "https://www.kaggle.com/code/mrisdal/intro-to-lstms-w-keras-gpu-for-text-generation", "script": "LSTM keras.optimizers tensorflow sample keras.layers Activation keras.models matplotlib.pyplot Dense ModelCheckpoint on_epoch_end RMSprop Sequential keras.callbacks LambdaCallback pandas numpy ", "entities": "(('which', 'model'), 'make') (('you', 'Kaggle'), 'now') (('exactly Temperature', 'softmax activation function'), 'pause') (('we', '55a7c9e08a7b72f55c3f991e fromUser'), 'represent') (('here you', 'different numbers'), 'experiment') (('We', 'input layer'), 'start') (('t', 'specified epochs'), 'using_gpu') (('next cell', 'a few seconds'), 'take') (('we', 'script https github'), 'feed') (('as far freeCodeCamp', 'topic'), 'sound') (('how guesses', 'sequence'), 'define') (('user', 'else sentences'), 'hdf5') (('one time_steps', 'i.'), 'add') (('you', 'loss here function'), 'experiment') (('We', '0'), 'use') (('I', 'various frameworks'), 'assume') (('what', 'best generated text'), 'way') (('part', 'yourself what'), 'train') (('much faster we', 'CPU'), 'let') (('We', '55a7c9e08a7b72f55c3f991e sentences'), 'use') (('i', 'single string'), 'd') (('character h', 'word'), 'be') (('we', 'sequence'), 'use') (('it', 'weights file saved weights'), 'have') (('you', 'characters'), 'print') (('we', 'unique characters'), 'train') (('we', 'roughly this'), 'expect') (('filetype', 'tagids'), 'dataset') (('I', 'training data'), 'use') (('you', 'code'), 'want') (('you English word', 'sg'), 'think') (('input layer', 'model'), 'be') (('we', 'tolower'), 'use') (('message where row', 'single message'), 'have') (('so second', '0'), 'define') (('text', 'really English'), 'resemble') (('ModelingIn Part two two we', 'model actual training'), 'part') (('commonly evaluation', 'case'), 'be') (('you', 'GPUs'), 'hope') (('we', 'what'), 'part') (('correctly so we', 'LSTM model'), 'explore') (('is', 'sequence input'), 'be') (('This', 'one less dimension'), 'give') (('when we', 'one'), 'name') (('you', 'Docker all downloads'), 'be') (('com keras team keras', 'blob master examples'), 'lstm_text_generation') (('don t', 'halfway decent text'), 'go') (('0', 'riskier guesses'), 'start') (('you', 'kernel editor'), 'use') (('Format', 'example script https here github'), 'adapate') (('you', '1'), 'be') (('I', 'three steps'), 'get') (('you', 'more data'), 'try') (('you', 'top'), 'tweak') (('cell next step', 'i.'), 'give') (('which', 'TensorFlow backend'), 'use') (('len chars', 'features i.'), 'be') (('com', 'Kernels'), 'learn') (('we', 'length partially overlapping equal sentences'), 'print') (('how it', 'already you'), 'appreciate') (('So 55a7c9e08a7b72f55c3f991e', '140 over 000 messages'), 'read') (('you', 'text'), 'learn') (('higher levels', 'more gobbledegook'), 'generate') (('interactively you', 'dramatic effect'), 'have') (('we', 'model'), 'create') (('high things', 's'), 'get') (('Using', 'gpu'), 'define') (('You', 'how next first sequence'), 'see') (('notebook', 'then different dataset good examples'), 'try') (('we', 'exactly what'), 'be') (('I', 'documentation pair coding BASH Bootstrap CSS etc'), 'let') (('Python notebook generationIn I', 'Gitter chat logs https www'), 'Applied') (('you', 'model training'), 'click') (('locally which', 'it'), 'have') (('Understanding LSTM Networks', 'colah'), 'want') (('15 number', 'epochs'), 'let') (('2', '1'), 'generate') (('all', 'data preview'), 'com') (('often how you', 'code sample'), 'be') (('2', 'these'), 'feel') (('we', 'data original subsetted user'), 'note') (('you', 'learning deep framework'), 'be') (('first one sample', 'temperature'), 'sample') (('how model', 'last'), 'see') (('model', 'training'), 'select') (('we', 'models layers optimizers'), 'see') "}