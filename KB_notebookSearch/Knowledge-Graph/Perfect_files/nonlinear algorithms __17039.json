{"name": "nonlinear algorithms ", "full_name": " h4 Hi all h4 I have recently published Beginner Friendly Detailed Explained EDAs For anyone at the beginnings of DS ML journey series h4 After getting positive feedback and requests for Beginner Intermediate Friendly Machine Learning series I started to publish the Machine Learning Basic Series which would help anyone who wants to learn or refresh the basics of ML h4 What we have covered h4 BIAS VARIANCE TRADEOFF h4 LINEAR ALGORITHMS h4 In this post I want to share with you h4 One of the essential concepts of Machine Learning Journey NONLINEAR ALGORITHMS h4 In this series I will focus on the basic concepts with detailed explanations h4 After finishing the beginner friendly explanation series I will publish ML algorithms in action h4 By the way when you like the topic you can show it by supporting h4 Feel free to leave a comment in the notebook h4 All the best h3 Table of Contents h4 An algorithm is a set of instructions designed to perform a specific task This can be a simple process such as multiplying two numbers or a complex operation such as playing a compressed video file h4 As we have mentioned in our first notebook BIAS VARIANCE TRADEOFF basically in the Supervised Learning we aim to build a model on the training data which can make a prediction on the target variable h4 Since we constantly learn from the training data and make a prediction on the target variable we need repetable procedures to perform this task h4 Accomplishing the task of learning data and make the best prediction on the test unseen data is not an easy task Machine learning algorithms try to succeed this task with the best prediction h4 Most of our times goes to improve our predictions and try to improve the performance of our models h4 Since we don t know which algorithm or model works best we have to try different algorithms on the task h4 Before moving on to the details of the algorithms we have to remember that h4 Main aim of the any algorithm is to achieve low bias and low variance h4 Decision trees are widely used models for classification and regression tasks Essentially they learn a hierarchy of if else questions leading to a decision h4 Decision trees are flexible and powerful algorithms for both regression and classification problems h4 To build a tree the algorithm searches over all possible tests and finds the one that is most informative about the target variable h4 Building a tree continues until all leaves are pure leads to models that are very complex and highly overfit to the training data h4 Overfitting can be prevented by stopping the building tree at the early stage or building the tree but removing the nodes which do not contain much inofrmation h4 Decision trees are easy to understand even by the nonexperts h4 Decision trees do not require to scaling standartization or normalization of the features h4 Decision trees are prone to overfitting and high variance h4 Because of that decision trees provide poor generalization h4 As we have mentioned one of the main disadvantages of decision trees is that they have high variance tend to overfit and as an expected result we will have poor accuracy and poor generalization performance h4 To reduce high varaiance of the decision trees ensemble methods Bootstrap Aggregation etc are used h4 Ensemble Building a strong model by using a ensemble of weaker models h4 In the Random forest algorithm to reduce the variance the sub trees are learned so that the resulting predictions from all of the subtrees have less correlation h4 Basically Random forest algorithm build many trees even though each of them overfit the data takes average of the their results to reduce overfitting h4 Random Forests reduces the overfitting and high variance h4 The model works with both regression and the classification problems h4 The Random Forests model does not require normalizing of data h4 The model requires more time to train and needs more computational power h4 The model is not easy to interpret h4 The model does not provide high level performance on the high dimensional data such as text data etc h4 As we have mentioned one of the main disadvantages of decision trees is that they have high variance tend to overfit and as an expected result we will havepoor accuracy and poor generalization performance h4 To reduce high varaiance of the decision trees ensemble methods Bootstrap Aggregation etc are used h4 Gradient Boosting Trees are another ensemble method that combines multiple decision trees to create a more powerful model h4 Let s ask the obvious question what is the difference between Gradient Boosting Trees and Random Forests h4 Gradient Boosting Trees model does not require randomization instead uses pre puning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances h4 Gradient Boosting Trees reduces the overfitting and high variance h4 The model works with both regression and the classification problems h4 Gradient Boosting Trees model are more sensitive to parameter settings than random forests h4 The model requires more time to train and needs more computational power h4 The model is not easy to interpret h4 The model does not provide high level performance on the high dimensional data such as text data etc h4 XGboost is the most widely used algorithm in machine learning whether the problem is a classification or a regression problem It is known for its good performance as compared to other algorithms h4 It provides a parallel tree boosting to solve many data science problems in a fast and accurate way h4 XGBoost s other features can be seen in the following image h4 XGBoost is highly flexible ML algorithm and uses the power of parallel processing It is faster than Gradient Boosting h4 XGBoost supports regularization h4 XGBoost is designed to handle missing data with its in build features h4 XGBoost permits the user to run a cross validation after each iteration h4 The model requires more time to train h4 The model is not easy to interpret h4 Naive It is called Naive because it assumes that occurance of the each input is independent of the occurrence of other inputs h4 Bayes It comes from Bayes s Theorem see below formula h4 Naive Bayes algorithm is a classification algoritmh for both binary and multiclass classification problems h4 Naive Bayes algorithm is generally used in text analysis h4 Naive Bayes learns parameters by looking each input seperately and collects the simple statistics from each feature Which basically means model uses prior knowledge about the problem h4 Naive Bayes is very fast to learn from the training data h4 Naive Bayes is used for Text Data Classification such as sentiment analysis etc Credit Scoring Spam Filtering Medical Data Classification and Real Time Prediction h4 Three kinds of naive Bayes classifiers h4 GaussianNB BernoulliNB and MultinomialNB h4 GaussianNB can be used with any data h4 BernoulliNB and MultinomialNB are mostly used with text data classification h4 P A B Posterior probability h4 P A Prior probability h4 P B A Probability of data d given that the hypothesis A was true h4 P B Probability of the data h4 We try to get posterior probability by using prior probability h4 Naive Bayes models are fast to train and to predict h4 Naive Bayes models training procedures are easy to understand h4 The model works very well with high dimensional sparse data and it is relatively robust to the parameters h4 Naive Bayes models are great baseline models and are often used on very large datasets where training even a linear model might take too long h4 Naive Bayes models assumes that all input variables are unrelated for that reason model can t learn from the relations between the input variables h4 Zero Frequency Let s imagine we have a categorical variable with three categories category A category B category C In the training data we have only two categories of this categorical variable category A category B and one of the category category C of the categorical variable wasn t present in the training data The Naive Bayes model will assign it zero probability But in the test data we have that category of the categorical variable Since Naive Bayes model assign zero probability to category C model won t be able to make any predictions in this regard h4 The KNN algorithm is one of the most simple and still very effective algorithm h4 When we have new data point our model makes predictions by searching through the entire training set for the similar instances K neighbors h4 h4 KNN model is simple Only requires to adjust the number of neighbors k and which the distance metric to be used then ready to go h4 KNN models are fast to train h4 KNN models are easy to understand h4 KNN models don t make any assumption about the data For example linear regression expects linear relation between input and target variables but KNN models do not require any assumption about the data and work with it h4 KNN models needs preprocessing the data scaling and balancing h4 KNN models are sensitive to the outliers h4 KNN models can t deal with the missing values h4 Requires large memory to store the training data h4 Since it requires a lot of memory and its inability to deal with many features KNN models are not often used in the practice h4 Even though SVW is used for both regression and classification problems it is primarily used for classification problems h4 The main goal of the SVM algorithm is to create the best line or decision boundary This best decision boundary is called a hyperplane h4 Support vectors in this sense are the data points that are closer to a hyperplane h4 SVM aims to maximize the distance margin between each of the support vectors h4 SVM works well with the high dimensional data Text data gene data image data etc h4 SVM draws boundary between the data points are not linearly separable h4 SVM works effectively in the higher dimension h4 SVM is not affected by the outliers model s only concern is support vectors h4 SVM is slow when dealing with the larger dataset h4 Even though SVM works well with the seperable classes if classes are overlapped SVM shows poor performance h4 In this study we focus on the NONLINEAR ALGORITHMS h4 After finishing the beginner friendly explanation series I will publish ML algorithms in action h4 By the way when you like the topic you can show it by supporting ", "stargazers_count": 0, "forks_count": 0, "description": "png Image Credits https www. Gradient Boosting TreesTable of Contents As we have mentioned one of the main disadvantages of decision trees is that they have high variance tend to overfit and as an expected result we will havepoor accuracy and poor generalization performance. com max 875 1 05DngXXh_tH1RHF5UaXWjA. Accomplishing the task of learning data and make the best prediction on the test unseen data is not an easy task. What we have covered BIAS VARIANCE TRADEOFF https www. Requires large memory to store the training data. XGBoost Extreme Gradient Boosting Table of Contents XGboost is the most widely used algorithm in machine learning whether the problem is a classification or a regression problem. This best decision boundary is called a hyperplane. The Random Forests model does not require normalizing of data. com master machine learning algorithms. XGBoost s other features can be seen in the following image. Credit Scoring Spam Filtering Medical Data Classification and Real Time Prediction. Basically Random forest algorithm build many trees even though each of them overfit the data takes average of the their results to reduce overfitting. P B Probability of the data We try to get posterior probability by using prior probability. com wp content uploads bayes nagesh 1. BernoulliNB and MultinomialNB are mostly used with text data classification. Let s ask the obvious question what is the difference between Gradient Boosting Trees and Random Forests Gradient Boosting Trees model does not require randomization instead uses pre puning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. But in the test data we have that category of the categorical variable. Since Naive Bayes model assign zero probability to category C model won t be able to make any predictions in this regard. For example linear regression expects linear relation between input and target variables but KNN models do not require any assumption about the data and work with it. XGBoost permits the user to run a cross validation after each iteration. Bayes s Theorem P A B Posterior probability. com Advantages Decision trees are easy to understand even by the nonexperts. Support vectors in this sense are the data points that are closer to a hyperplane. category A category B category C. png Image Credit https media. KNN models are fast to train. com kaanboke ml basics bias variance tradeoff LINEAR ALGORITHMS https www. com Table of Contents Algorithm 0 Decision Tree 1 Random Forests 2 Gradient Boosting Trees 3 XGBoost Extreme Gradient Boosting 4 Naive Bayes 5 KNN k Nearest Neighbors 6 Support Vector Machines SVM 7 Further Reading 8 References 9 AlgorithmTable of Contents An algorithm is a set of instructions designed to perform a specific task. Which basically means model uses prior knowledge about the problem. Naive Bayes models are great baseline models and are often used on very large datasets where training even a linear model might take too long. The model is not easy to interpret The model does not provide high level performance on the high dimensional data such as text data etc. Since we constantly learn from the training data and make a prediction on the target variable we need repetable procedures to perform this task. The model works very well with high dimensional sparse data and it is relatively robust to the parameters. Even though SVM works well with the seperable classes if classes are overlapped SVM shows poor performance. By the way when you like the topic you can show it by supporting Feel free to leave a comment in the notebook. png Image Credit miro. This can be a simple process such as multiplying two numbers or a complex operation such as playing a compressed video file. comAdvantages KNN model is simple. To build a tree the algorithm searches over all possible tests and finds the one that is most informative about the target variable. After finishing the beginner friendly explanation series I will publish ML algorithms in action. Essentially they learn a hierarchy of if else questions leading to a decision. Only requires to adjust the number of neighbors k and which the distance metric to be used then ready to go. jpeg Image Credit miro. comNaive BayesTable of Contents Naive It is called Naive because it assumes that occurance of the each input is independent of the occurrence of other inputs. com max 1400 1 FUZS9K4JPqzfXDcC83BQTw. Machine learning algorithms try to succeed this task with the best prediction. com max 1400 1 1kjLMDQMufaQoS nNJfg1Q. Since it requires a lot of memory and its inability to deal with many features KNN models are not often used in the practice. It is faster than Gradient Boosting. Naive Bayes learns parameters by looking each input seperately and collects the simple statistics from each feature. XGBoost is designed to handle missing data with its in build features. com uploads spriiprad_1589400525_Picturedd1. Naive Bayes is used for Text Data Classification such as sentiment analysis etc. The main goal of the SVM algorithm is to create the best line or decision boundary. After getting positive feedback and requests for Beginner Intermediate Friendly Machine Learning series I started to publish the Machine Learning Basic Series which would help anyone who wants to learn or refresh the basics of ML. com general 253911 1393015 series. Disadvantages The model requires more time to train. Before moving on to the details of the algorithms we have to remember that Main aim of the any algorithm is to achieve low bias and low variance. Building a tree continues until all leaves are pure leads to models that are very complex and highly overfit to the training data. KNN k Nearest Neighbors Table of Contents The KNN algorithm is one of the most simple and still very effective algorithm. The model requires more time to train and needs more computational power. In the training data we have only two categories of this categorical variable category A category B and one of the category category C of the categorical variable wasn t present in the training data. Overfitting can be prevented by stopping the building tree at the early stage or building the tree but removing the nodes which do not contain much inofrmation Image Credit https www. SVM is not affected by the outliers model s only concern is support vectors. comDecision TreeTable of Contents Decision trees are widely used models for classification and regression tasks. By the way when you like the topic you can show it by supporting All the best Further ReadingTable of Contents Machine Learning Beginner Intermediate Friendly BOOKS https www. fr Advantages XGBoost is highly flexible ML algorithm and uses the power of parallel processing. com tutorial machine learning images support vector machine algorithm. com general 255972 References Table of Contents Introduction to Machine Learning with Python https www. com originals 90 00 c0 9000c0e50e1a97d0d12e85dc93affa5f. com Image Credit www. com Introduction Machine Learning Python Scientists dp 1449369413 Master Machine Learning Algorithms https machinelearningmastery. Image Credit relguzman. The model works with both regression and the classification problems. I have recently published Beginner Friendly Detailed Explained EDAs For anyone at the beginnings of DS ML journey https www. png Image Credit https miro. Ensemble Building a strong model by using a ensemble of weaker models. All the best https miro. Support Vector Machines SVM Table of Contents Even though SVW is used for both regression and classification problems it is primarily used for classification problems. P A Prior probability P B A Probability of data d given that the hypothesis A was true. The Naive Bayes model will assign it zero probability. jpeg Image Credit www. com kaanboke ml basics bias variance tradeoff basically in the Supervised Learning we aim to build a model on the training data which can make a prediction on the target variable. com kaanboke ml basics linear algorithms In this post I want to share with you One of the essential concepts of Machine Learning Journey NONLINEAR ALGORITHMS In this series I will focus on the basic concepts with detailed explanations. com max 1400 1 QJZ6W Pck_W7RlIDwUIN9Q. Decision trees are flexible and powerful algorithms for both regression and classification problems. SVM works effectively in the higher dimension. comAdvantages Gradient Boosting Trees reduces the overfitting and high variance. Most of our times goes to improve our predictions and try to improve the performance of our models. com Advantages Random Forests reduces the overfitting and high variance. KNN models are sensitive to the outliers KNN models can t deal with the missing values. When we have new data point our model makes predictions by searching through the entire training set for the similar instances K neighbors. Naive Bayes algorithm is generally used in text analysis. Three kinds of naive Bayes classifiers GaussianNB BernoulliNB and MultinomialNB GaussianNB can be used with any data. com In this study we focus on the NONLINEAR ALGORITHMS. Since we don t know which algorithm or model works best we have to try different algorithms on the task. com Advantages Naive Bayes models are fast to train and to predict Naive Bayes models training procedures are easy to understand. The model is not easy to interpret. com https media. Disadvantages Gradient Boosting Trees model are more sensitive to parameter settings than random forests. Bayes It comes from Bayes s Theorem see below formula Naive Bayes algorithm is a classification algoritmh for both binary and multiclass classification problems. It is known for its good performance as compared to other algorithms. png Image Credit www. It would be good to see overview of the Decison Trees Random Forest Gradient Boosting and XGBoost. In the Random forest algorithm to reduce the variance the sub trees are learned so that the resulting predictions from all of the subtrees have less correlation. com max 591 1 kCqervQNQ5fGDfkFwrMzRQ. Disadvantages SVM is slow when dealing with the larger dataset. SVM works well with the high dimensional data Text data gene data image data etc. Disadvantages Naive Bayes models assumes that all input variables are unrelated for that reason model can t learn from the relations between the input variables. Gradient Boosting Trees are another ensemble method that combines multiple decision trees to create a more powerful model. comAdvantages SVM draws boundary between the data points are not linearly separable. Naive Bayes is very fast to learn from the training data. KNN models are easy to understand. Before finishing let s see the advantages and disadvantages of the some of the algorithms we have mentioned till now. Zero Frequency Let s imagine we have a categorical variable with three categories. Image Credit https www. Disadvantages KNN models needs preprocessing the data scaling and balancing. KNN models don t make any assumption about the data. XGBoost supports regularization. Decision trees do not require to scaling standartization or normalization of the featuresDisadvantages Decision trees are prone to overfitting and high variance Because of that decision trees provide poor generalizationRandom ForestsTable of Contents As we have mentioned one of the main disadvantages of decision trees is that they have high variance tend to overfit and as an expected result we will have poor accuracy and poor generalization performance. com definition algorithm As we have mentioned in our first notebook BIAS VARIANCE TRADEOFF https www. It provides a parallel tree boosting to solve many data science problems in a fast and accurate way. Disadvantages The model requires more time to train and needs more computational power. https miro. SVM aims to maximize the distance margin between each of the support vectors. To reduce high varaiance of the decision trees ensemble methods Bootstrap Aggregation etc. Reference https techterms. ", "id": "kaanboke/nonlinear-algorithms", "size": "17039", "language": "python", "html_url": "https://www.kaggle.com/code/kaanboke/nonlinear-algorithms", "git_url": "https://www.kaggle.com/code/kaanboke/nonlinear-algorithms", "script": "IPython base64 display b64decode ", "entities": "(('SVM', 'effectively higher dimension'), 'work') (('Decision trees', 'flexible regression'), 'be') (('We', 'prior probability'), 'probability') (('com Random Forests', 'overfitting'), 'Advantages') (('it', 'relatively parameters'), 'work') (('we', 'three categories'), 'let') (('distance', 'which'), 'require') (('SVM', 'data points'), 'be') (('best prediction', 'unseen test data'), 'accomplish') (('we', 'categorical variable'), 'have') (('that', 'training very highly data'), 'continue') (('SVM', 'support vectors'), 'aim') (('Naive Bayes', 'training very data'), 'be') (('we', 'NONLINEAR ALGORITHMS'), 'com') (('Naive Bayes', 'sentiment analysis such etc'), 'use') (('that', 'target most variable'), 'build') (('who', 'ML'), 'start') (('SVM', 'well high dimensional data'), 'work') (('I', 'detailed explanations'), 'ml') (('data that', 'hyperplane'), 'be') (('occurance', 'other inputs'), 'BayesTable') (('model', 'text data such etc'), 'be') (('you', 'notebook'), 'show') (('algorithm best we', 'task'), 'have') (('Naive Bayes algorithm', 'classification classification binary problems'), 'Bayes') (('It', 'Decison Trees Random Forest Gradient Boosting'), 'be') (('It', 'other algorithms'), 'know') (('we', 'poor accuracy'), 'require') (('Disadvantages Gradient Boosting Trees model', 'random forests'), 'be') (('which', 'target variable'), 'tradeoff') (('which', 'inofrmation Image Credit https much www'), 'prevent') (('KNN models don t', 'data'), 'make') (('ensemble that', 'more powerful model'), 'be') (('basically model', 'problem'), 'mean') (('SVM', 'outliers only concern'), 'affect') (('Disadvantages KNN models', 'data'), 'need') (('you', 'Contents Machine Learning Beginner Intermediate Friendly BOOKS https www'), 'show') (('Naive Bayes model', 'zero probability'), 'assign') (('BernoulliNB', 'text data mostly classification'), 'use') (('hypothesis A', 'P P B data Prior d'), 'a') (('model', 'similar instances'), 'make') (('It', 'fast way'), 'provide') (('where training', 'even linear model'), 'be') (('com tutorial machine learning images', 'vector machine algorithm'), 'support') (('Three kinds', 'MultinomialNB data'), 'use') (('algorithm', 'specific task'), 'table') (('KNN models', 'missing values'), 'be') (('problem', 'most widely used machine'), 'be') (('com Advantages Decision trees', 'even nonexperts'), 'be') (('I', 'DS ML journey https www'), 'publish') (('classes', 'SVM poor performance'), 'show') (('Essentially they', 'decision'), 'learn') (('we', 'task'), 'need') (('only two categories', 'training data'), 'have') (('resulting predictions', 'less correlation'), 'learn') (('KNN models', 'often practice'), 'require') (('Bayes Naive algorithm', 'text generally analysis'), 'use') (('XGBoost', 'iteration'), 'permit') (('input variables', 'input variables'), 'assume') (('Random Forests model', 'data'), 'require') (('model', 'regression'), 'work') (('XGBoost', 'parallel processing'), 'be') (('Gradient Boosting Trees', 'overfitting'), 'comadvantage') (('model', 'more computational power'), 'require') (('we', 'BIAS VARIANCE TRADEOFF https www'), 'cover') (('I', 'action'), 'publish') (('KNN algorithm', 'most simple still very algorithm'), 'table') (('Naive Bayes', 'feature'), 'learn') (('classification it', 'classification primarily problems'), 'use') (('comDecision TreeTable', 'widely used classification tasks'), 'be') (('KNN models', 'it'), 'expect') (('Main aim', 'low bias'), 'before') (('we', 'algorithms'), 'let') (('This', 'video complex such compressed file'), 'be') (('t', 'regard'), 'be') (('model', 'more time'), 'require') (('other features', 'following image'), 'see') (('com definition we', 'notebook BIAS VARIANCE TRADEOFF https first www'), 'algorithm') (('Most', 'models'), 'go') (('main goal', 'best line'), 'be') (('that', 'instances'), 'let') (('Machine learning algorithms', 'best prediction'), 'try') (('Disadvantages SVM', 'when larger dataset'), 'be') (('we', 'accuracy'), 'be') (('even each', 'overfitting'), 'build') (('XGBoost', 'build features'), 'design') "}