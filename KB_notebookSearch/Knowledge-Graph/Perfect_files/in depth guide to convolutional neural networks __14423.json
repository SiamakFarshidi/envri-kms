{"name": "in depth guide to convolutional neural networks ", "full_name": " h1 Introduction h3 If you like this kernel or wish to fork it please give it an UPVOTE to show your appreciation h1 Import Libraries h2 Loading Training Data EDA h2 Setting up Train Data for Pytorch h2 Setting up Test Data h2 Visualize Images h2 Viewing an Image in More Detail h2 Multi Layer Perceptrons MLP Vs Convolutional Neural Networks CNN h2 Understanding CNNs h3 If you like this kernel or wish to fork it please give it an UPVOTE to show your appreciation h2 Define the Network Architecture h2 Training our CNN h2 Performance Graph h2 Make Predictions on Test Set h3 If you like this kernel or wish to fork it please give it an UPVOTE to show your appreciation ", "stargazers_count": 0, "forks_count": 0, "description": "PyTorch is a powerful deep learning framework which is rising in popularity and it is thoroughly at home in Python which makes it really easy to learn and use. It has no knowledge whatsoever of the fact that it was previously arranged in a grid losing important information. Random horizontal flipping of sample images is applied along with random rotation of 10 degrees for random training examples. One way to think about it is that smaller batches means that the number of parameter updates per epoch is greater. The only difference here is that I m saving the model everytime the validation loss decreases. Another way of thinking about what pooling does is that it generalizes over lower level more complex information. We define the convolutional layers first. There are also alternatives to Pooling like Capsule Networks which is out of the score of this kernel. An even more refined form is passed on to FC layers to make the final prediction. We ll use the Aerial Cactus Dataset from this currently running Kaggle competition https www. I picked this competition because I felt it is the best place for beginners to practice their new found skills with CNNs as MNIST is just way too simple to bring CNNs into play a regular Multi layer perceptron may well do the job. There may be several filters in a Convolutional layer. com topher 2018 May 5b10723a_screen shot 2018 05 31 at 3. png raw true So all it sees is a vector and treats it just as a vector with no special structure. This is because augmentation is only done on the training set to improve the performance by giving our model complex variations that might make it generalize well to new samples on the test set so it doesn t make sense to do data augmentation on the test data as well. I encourage you to go out and explore on your own to find those awesome stuff. Each time we can pull out data of the size of the batch_size defined. Note that these images are very low inresolution yet as humans we have evolved in such a way to make sense of even these kind of pictures. Performance GraphI told you how powerful CNNs are now look at that graph. Note that we don t augment our data on the training set. As you might have guessed already color images are actually three dimensional where Greyscale images for example are 1D. These filters can be modified to get different filters as output. Here comes our final model. The batch size does have an effect on the final test accuracy. Visualize ImagesVisualizing the images and inspecting them to get a better idea of what they are is always helpful before going on the construct our model to make predictions. com topher 2018 May 5b1070e4_screen shot 2018 05 31 at 2. Everything is put together in a forward propagation function later. com abhinand05 mnist introduction to computervision with pytorch on MNIST dataset MLPs were good enough to score more than 90 accuracy. gif When we combine pooling layers with convolutional layers we reduce the dimension of the layers which helps in computation but more than that it picks the pixel values of higher significance. There is also average pooling which you may see out on the wild. This tutorial won t assume much in regards to prior knowledge of PyTorch but it might be helpful to checkout my previous introductory CV tutorial https www. 03 pm screen shot 2018 05 31 at 3. Later we ll use this to loop through the dataset for training. 07 pm screen shot 2018 05 31 at 3. The outputs here don t need to be 0 or 1 what Kaggle expects is the probabilities so we don t need an activation function like sigmoid on the output layer. This helps me explain CNNs later. Details can be mostly interpreted from the comment lines itself. There are several pre processing steps involved which we ll discuss in detail in this section. When we take the maximum element from the window it is known as Max Pooling which is the most common pooling technique. Import Libraries Loading Training Data EDAThe train data contains 17500 images which can be found in a seperate directory also we have a csv file but we can t directly visualize them as we ll see later. png In the example shown 4 different filters produce 4 differently filtered output images. How it is done is a window of fixed size of made to stroll over the image and inside the window a specific value according to a metric is calculated an a new tensor is formed. This is called skewed classes we can use data augmentation sampling and several ways to overcome this. CNNs on the other hand work exactly the other way around capable of working with and elucidating patterns from multi dimensional data that is why they are so powerful. Transforms We can use the transforms feature in PyTorch to apply Data Augmentations which help us improve the accuracy of our model when done right. com media jrzu0JxxZydz0valeu giphy. com abhinand05 mnist introduction to computervision with pytorch. Example https s3. png Convolutional Layer Turns out that in order to understand Convolutional layer first we have to understand a concept called Filters. Finally we ll get the best model params learned. Authored By Abhinand https www. samplerSubsetRandomSampler function to split our data into training and validation sets which is similar to train_test_split fuction of scikit learn. Here in an example gif of Max Pooling. In fact this is very much similar to how our brains interpret images in an instant. Step 1 First up we define a custom class that extends PyTorch s torch. jpg In this notebook we ll train a CNN to classify images based on whether they have a columnar cactus or not. In the next steps we use the torch. The images contain color channels we should keep in mind and they are 32x32 images. We can make an iterator with iter trainloader that can help us grab data. If we do the same for color images we throw away vital information by flattening it which almost never works for real world images. If you like this kernel or wish to fork it please give it an UPVOTE to show your appreciation. Pooling Layer Pooling is used in Convolutional Neural Networks to make the detection of certain features somewhat invariant to scale and orientation changes. com c aerial cactus identification data. We ve achieved state of the art results as promised. If the dataset you are using contains almost or above 90 of the training data belonging to one single class then it will greatly impact your results. When we stack these images we form a complete convolutional layer with a depth of 4 Example https s3. The convolutional layer is produced by applying a series of many different image filters also known as convolutional kernels to an input image. I ll use an example of classifying a car here to explain different layers of a Convolutional Neural Network. Expecially high pass filters. Here we conert it to a PIL image first. Dropout is a regularization technique to avoid overfitting which is also added. Image Credits Stanford CS231n Udacity Define the Network ArchitectureHere comes the important part of defining a CNN which will can done using the torch. read_csv OpenCV Image Library Import PyTorch Counting the number of sample data for each class Data paths Our own custom class for datasets Set Batch Size Percentage of training set to use as validation obtain training indices that will be used for validation Create Samplers prepare data loaders combine dataset and sampler creating test data prepare the test loader unnormalize convert from Tensor image and display obtain one batch of training images convert images to numpy for display plot the images in the batch along with the corresponding labels display 20 images Convolutional Layer sees 32x32x3 image tensor Convolutional Layer sees 16x16x16 image tensor Convolutional Layer sees 8x8x32 image tensor Convolutional Layer sees 4 4 64 image tensor Maxpooling Layer Linear Fully Connected Layer 1 sees 2 2 128 image tensor Linear FC Layer 2 Set Dropout add sequence of convolutional and max pooling layers flatten image input add dropout layer add 1st hidden layer with relu activation function add dropout layer add 2nd hidden layer with relu activation function check if CUDA is available create a complete CNN Move model to GPU if available specify loss function categorical cross entropy loss specify optimizer number of epochs to train the model track change in validation loss keeping track of losses as it happen keep track of training and validation loss train the model move tensors to GPU if CUDA is available clear the gradients of all optimized variables forward pass compute predicted outputs by passing inputs to the model calculate the batch loss backward pass compute gradient of the loss with respect to model parameters perform a single optimization step parameter update update training loss validate the model move tensors to GPU if CUDA is available forward pass compute predicted outputs by passing inputs to the model calculate the batch loss update average validation loss calculate average losses print training validation statistics save model if validation loss has decreased Load Best parameters learned from training into our model to make predictions later Turn off gradients Create Submission file. 36 pm screen shot 2018 05 31 at 2. Here we inspect the data to see the amount of training samples for each class. com abhinand5 CNNs in PyTorch blob master cifar cnn FireShot 20Capture 20084 20 20Intro 20to 20Deep 20Learning 20with 20PyTorch 20 20Udacity 20 20classroom. jpeg I know this may not be the best of definitions you would see for CNNs but the goal here is to make it atleast vaguely understandable. Hope it made sense to you. As we can see about 2 3 of the training data belongs to one class. Multi Layer Perceptrons MLP Vs Convolutional Neural Networks CNN As you might have seen in my previous tutorial https www. FeaturedImage https i. We re going to build a CNN in the next few sections to ultimately achieve just that or even better performance. Each little square you can see are pixels with their normalized values. You might also want to use Batch Normalization but I decided not to because I achieved more than 99 accuracy without it. Make Predictions on Test Set If you like this kernel or wish to fork it please give it an UPVOTE to show your appreciation. These filters produce an output that shows edges of objects and differing textures. We have also applied the transformations there. com media U7PsR7cv9oIcB6eEAd giphy. png Fully Connected Layers Fully connected layers are no different from the ones you already know from MLPs. Example https github. co V06mcY0 Intro to Deep Learning with Py Torch Udacity classroom udacity com. The same steps used above are repeated for the test data as well. We can see aerial images of cactus. Convolutional Layers Pooling Layers Fully Connected LayersWe ll go over each one of there layers in great detail. To detect changes in intensity in an image we ll be using and creating specific image filters that look at groups of pixels and react to alternating patterns of dark light pixels. The test data has 4000 images and it is stored in a seperate directory. We ll be using PyTorch for this tutorial. The outputs are then connected to a FC layer. I think everything in there is pretty straight forward. First you must define a Model class and fill in two functions __init__ and __forward__. com topher 2018 May 5b10729b_screen shot 2018 05 31 at 3. org docs stable torchvision transforms. Creating our Dataset We then use our CreateDataset class to covert the raw data in the way PyTorch expects. Step 3 Batch size is set. Larger value means brighter lower means darker. It is the final piece of the puzzle that makes it special and powerful. com abhinand05 linear algebra data processing CSV file I O e. 77 million colors which is awesome. Unlike MLPs CNNs actually understand the information from pixels that are in close proximity to each other and are more related to each other than those pixels that are far apart and unrelated. Exactly same as the example gif you saw previously. For more information about the dataset visit this page https www. Setting up Train Data for Pytorch We cannot simply use the raw image data to make predictions using PyTorch. Then the percentage of data needed for validation is set to 20 which almost always seem to work for me. Step 2 Now that we ve defined our class it is time to pass the raw data and convert it to PyTorch understandable form. This is how images are represented in our computers. But when combined they give extraordinary results. Understanding CNNsConolutional Neural Networks make use of three different classes of layers which distinctly differ with each other. We define our constructors and add two different methods len and getitem which essentially replaces the parent definitions. It turns out that they re nowhere near good enough to achieve similar results on complex datasets mainly because they expect a flattened vector of the original image like the one below. Example https media. io assets cnn convnet. For this we first contstruct a filter according to a size called window size which is nothing but a matrix like this. So our model so far looks something like this. gif These windows are usually of size 3x3 which helps CNNs identify the patterns in an image. Here is an illustration which you might find helpful. But let me just explain one. Example http cs231n. co ws3htpn 2088474 6a86 3. Our image RGB has 3 channels so depth 3 that s why our first conv layer has 3 incoming channels I ve decided to have 16 filters for the first conv layer so out_channels 16 as it produces 16 different filters the kernel size aka window size is set to 3 with padding 1 which makes up extra spaces on the edges of the image to help the 3x3 kernel slide over all the edge pixels of the image in case of size mismatch. However we still need to convert it to a tensor and normalize it. So this is a perfect beginners competition as someone rightly said in the discussion forums. However I don t believe that s a problem here as we have enough data for CNNs to get great results. MNIST dataset is an exception as it comes already well processed. This RGB color scheme can represent about 16. There are sevaral transformations that can be applied for which you may want to have a look at the documentation here https pytorch. When we combine all three channels we get a color image. com c aerial cactus identification overview. IntroductionIn this notebook we ll go over defining training and testing as well as the pre processing steps needed to feed an image into CNNs to get state of the art results. Note that it doesn t have a csv file as we saw for the train data. Training our CNNNow the training steps are same as that of training an MLP which I explained in a previous kernel. The batch size is usually set between 64 and 256. This is one type of early stopping. Read the docs for nn. But in the end it s just another hyperparameter you can tune. Setting up Test DataThis section of code should make sense to you now. We then convert the images into a PyTorch tensor then we normalize the them. Here in an example gif. Conv2d to know more about the parameters. Viewing an Image in More DetailHere we look at the normalized red green and blue RGB color channels as three separate grayscale intensity images for illustration purposes. There is much more happening behind the scenes which I skipped. We then go on and define the pooling layers where a window size and stride of 2 is set. png The depth of each consecutive convolutional layers may increase resulting in the networks capturing some incredibly complex features patterns from the images. We have the training data passed into the trainloader. Now that you understand how a CNN works everything is pretty much self explanatory. ", "id": "abhinand05/in-depth-guide-to-convolutional-neural-networks", "size": "14423", "language": "python", "html_url": "https://www.kaggle.com/code/abhinand05/in-depth-guide-to-convolutional-neural-networks", "git_url": "https://www.kaggle.com/code/abhinand05/in-depth-guide-to-convolutional-neural-networks", "script": "torch.nn.functional torchvision.transforms DataLoader numpy TensorDataset torch.nn CreateDataset(Dataset) matplotlib.pyplot forward torch.utils.data.sampler CNN(nn.Module) pandas SubsetRandomSampler torch.optim torch.utils.data __len__ Dataset __init__ imshow __getitem__ ", "entities": "(('we', 'training set'), 'note') (('I', 'Convolutional Neural Network'), 'use') (('we', 'color image'), 'combine') (('you', 'appreciation'), 'make') (('window where size', '2'), 'go') (('Everything', 'propagation together forward function'), 'put') (('you', 'wild'), 'be') (('training data', 'trainloader'), 'have') (('Larger value', 'brighter lower means'), 'mean') (('So model', 'this'), 'look') (('we', 'Example https 4 s3'), 'form') (('which', 'world almost real images'), 'throw') (('first we', 'concept'), 'turn') (('MNIST', 'well job'), 'pick') (('Random horizontal flipping', 'training random examples'), 'apply') (('they', 'columnar cactus'), 'jpg') (('We', 'convolutional layers'), 'define') (('4 different filters', 'output 4 differently filtered images'), 'png') (('May 2018 5b1070e4_screen', '2018 05 31 2'), 'com') (('batch size', 'test final accuracy'), 'have') (('I', 'previous kernel'), 'train') (('we', 'section'), 'be') (('you', 'normalized values'), 'be') (('We', 'Kaggle competition https currently running www'), 'use') (('that', 'pixels'), 'understand') (('it', 'lower level more complex information'), 'be') (('that', 'data'), 'make') (('here we', 'great results'), 'believe') (('I', 'which'), 'be') (('it', 'final puzzle'), 'be') (('it', 'special structure'), 'true') (('However we', 'it'), 'need') (('you', 'tutorial https previous www'), 'CNN') (('new tensor', 'metric'), 'be') (('time we', 'batch_size'), 'pull') (('batch size', 'usually between 64'), 'set') (('which', 'scikit'), 'learn') (('filters', 'output'), 'modify') (('We', 'ultimately just that'), 'go') (('which', 'distinctly other'), 'make') (('Convolutional Layers Pooling Layers Connected Fully LayersWe', 'great detail'), 'go') (('they', 'mind'), 'contain') (('Setting', 'you'), 'make') (('here it', 'CNNs'), 'jpeg') (('We', 'PyTorch'), 'set') (('even more refined form', 'final prediction'), 'pass') (('we', 'pictures'), 'note') (('2018 5b10729b_screen', '2018 05 31 3'), 'com') (('then we', 'them'), 'normalize') (('then it', 'greatly results'), 'impact') (('2018 5b10723a_screen', '2018 05 31 3'), 'com') (('you', 'documentation'), 'be') (('you', 'appreciation'), 'give') (('outputs', 'FC then layer'), 'connect') (('detection', 'somewhat invariant scale changes'), 'use') (('Details', 'comment mostly lines'), 'interpret') (('we', 'illustration purposes'), 'look') (('PyTorch', 'way'), 'create') (('how CNNs', 'now graph'), 'tell') (('We', 'art results'), 'achieve') (('depth', 'images'), 'png') (('I', 'awesome stuff'), 'encourage') (('you', 'which'), 'be') (('which', 'Max Pooling'), 'know') (('you', 'Exactly example gif'), 'same') (('which', 'torch'), 'Credits') (('that', 'objects'), 'produce') (('skewed we', 'several this'), 'call') (('same steps', 'test above data'), 'repeat') (('Conv2d', 'parameters'), 'know') (('us', 'model'), 'transform') (('that', 'dark light pixels'), 'detect') (('We', 'tutorial'), 'use') (('com', 'enough more than 90 accuracy'), 'be') (('it', 'seperate directory'), 'have') (('that', 'torch'), 'step') (('1 which', 'size mismatch'), 'have') (('very much how brains', 'instant'), 'be') (('just you', 'end'), 's') (('they', 'predictions'), 'be') (('CNNs', 'image'), 'be') (('which', 'parent essentially definitions'), 'define') (('notebook we', 'art results'), 'introductionin') (('which', 'almost always me'), 'set') (('window which', 'this'), 'contstruct') (('beginners perfect someone', 'discussion rightly forums'), 'be') (('it', 'test data'), 'be') (('Py Torch Udacity', 'V06mcY0 Deep Learning'), 'Intro') (('validation everytime loss', 'decreases'), 'be') (('we', 't directly them'), 'have') (('it', 'important information'), 'have') (('I', 'it'), 'want') (('RGB color scheme', 'about 16'), 'represent') (('why they', 'multi dimensional data'), 'cnn') (('it', 'understandable form'), 'step') (('number', 'epoch'), 'be') (('when they', 'extraordinary results'), 'give') (('Greyscale actually three where images', 'example'), 'be') (('We', 'also transformations'), 'apply') (('mainly they', 'one'), 'turn') (('about 2 3', 'one class'), 'belong') (('we', 'output layer'), 'need') (('it', 'higher significance'), 'reduce') (('which', 'kernel'), 'be') (('Here we', 'class'), 'inspect') (('how images', 'computers'), 'be') (('it', 'Python'), 'be') (('we', 'train data'), 'note') (('Later we', 'training'), 'use') (('predictions', 'Create Submission file'), 'count') (('it', 'CV tutorial https previous introductory www'), 'win') (('convolutional layer', 'input image'), 'produce') (('which', 'regularization overfitting'), 'be') (('First you', '_ _ forward _'), 'define') (('you', 'already MLPs'), 'be') "}