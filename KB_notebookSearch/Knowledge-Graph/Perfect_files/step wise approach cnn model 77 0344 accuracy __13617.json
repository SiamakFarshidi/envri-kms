{"name": "step wise approach cnn model 77 0344 accuracy ", "full_name": " h1 Welcome to my kernel h1 Step 1 importing Essential Libraries h1 Step 2 Making Dictionary of images and labels h1 Step 3 Reading Processing data h1 Step 4 Data Cleaning h1 Step 5 EDA h1 Step 6 Loading and resizing of images h1 Step 7 Train Test Split h1 Step 8 Normalization h1 Step 9 Label Encoding h1 Step 10 Splitting training and validation split h1 Step 11 Model Building h1 CNN h1 Step 12 Setting Optimizer and Annealer h1 Data Augmentation h1 Step 13 Fitting the model h1 Step 14 Model Evaluation h1 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "e the area size pooled each time more the pooling dimension is high more the downsampling is important. To keep the advantage of the fast computation time with a high LR i decreased the LR dynamically every X steps epochs depending if it is necessary when accuracy is not improved. We will move step by step process to classify 7 classes of cancer. In this step I have choosen batch size of 10 and 50 epochs as small as your batch size will be more efficiently your model will train and I have choosen 50 epochs to give the model sufficient epochs to train Step 14 Model Evaluation In this step we will check the testing accuracy and validation accuracy of our model plot confusion matrix and also check the missclassified images count of each typeNow lets which category has much incorrect predictions ConclusionIt seems our model has maximum number of incorrect predictions for Basal cell carcinoma which has code 3 then second most missclassified type is Vascular lesions code 5 then Melanocytic nevi code 0 where as Actinic keratoses code 4 has least misclassified type. Step 11 Model Building CNNI used the Keras Sequential API where you have just to add one layer at a time starting from the input. Plotting the distribution of localization field It seems back lower extremity trunk and upper extremity are heavily compromised regions of skin cancer Now check the distribution of AgeIt seems that there are larger instances of patients having age from 30 to 60Lets see the distribution of males and femalesNow lets visualize agewise distribution of skin cancer typesIt seems that skin cancer types 0 1 3 and 5 which are Melanocytic nevi dermatofibroma Basal cell carcinoma and Vascular lesions are not much prevalant below the age of 20 years Sexwise distribution of skin cancer type Step 6 Loading and resizing of imagesIn this step images will be loaded into the column named image from the image path from the image folder. The CNN can isolate features that are useful everywhere from these transformed images feature maps. zip Step 3 Reading Processing dataIn this step we have read the csv by joining the path of image folder which is the base folder where all the images are placed named base_skin_dir. We also resize the images as the original dimension of images are 450 x 600 x3 which TensorFlow can t handle so that s why we resize it into 100 x 75. In order to avoid overfitting problem we need to expand artificially our HAM 10000 dataset. The second important layer in CNN is the pooling MaxPool2D layer. The rectifier activation function is used to add non linearity to the network. Basal cell carcinoma 5. The Flatten layer is use to convert the final feature maps into a one single 1D vector. If like then kindly dont forget to hit the like used for converting labels to one hot encoding convert to one hot encoding 1. This benchmark dataset can be used for machine learning and for comparisons with human experts. zip into one dictionary This dictionary is useful for displaying more human friendly labels later on Creating New Columns for better readability Now lets see the sample of tile_df to look on newly made columns Checking the image size distribution Perform one hot encoding on the labels Reshape image in 3 dimensions height 75px width 100px canal 3 Set the CNN model my CNN architechture is In Conv2D relu 2 MaxPool2D Dropout 2 Flatten Dense Dropout Out Define the optimizer Compile the model Set a learning rate annealer With data augmentation to prevent overfitting set input mean to 0 over the dataset set each sample mean to 0 divide inputs by std of the dataset divide each input by its std apply ZCA whitening randomly rotate images in the range degrees 0 to 180 Randomly zoom image randomly shift images horizontally fraction of total width randomly shift images vertically fraction of total height randomly flip images randomly flip images Fit the model Function to plot confusion matrix Predict the values from the validation dataset Convert predictions classes to one hot vectors Convert validation observations to one hot vectors compute the confusion matrix plot the confusion matrix. It consists of 10015 dermatoscopicimages which are released as a training set for academic machine learning purposes and are publiclyavailable through the ISIC archive. As we can see image column has been added in its color format code Most interesting part its always better to see sample of imagesBelow we will show images of each cancer type Step 7 Train Test SplitIn this step we have splitted the dataset into training and testing set of 80 20 ratio Step 8 NormalizationI choosed to normalize the x_train x_test by substracting from theor mean values and then dividing by thier standard deviation. zip and HAM10000_images_part2. They applied the consensus label only if both authors independently gave the same unequivocal benign diagnosis. For the data augmentation i choosed to Randomly rotate some training images by 10 degrees Randomly Zoom by 10 some training images Randomly shift images horizontally by 10 of the width Randomly shift images vertically by 10 of the height Once our model is ready we fit the training dataset. Follow up If nevi monitored by digital dermatoscopy did not show any changes during 3 follow up visits or 1. We have to choose the pooling size i. After that we made some new columns which is easily understood for later reference such as we have made column path which contains the image_id cell_type which contains the short name of lesion type and at last we have made the categorical column cell_type_idx in which we have categorize the lesion type in to codes from 0 to 6 Step 4 Data CleaningIn this step we check for Missing values and datatype of each field As it is evident from the above that only age has null values which is 57 so we will fill the null values by their mean. Its better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function. This the HAM10000 Human Against Machine with 10000 training images dataset. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima. In the end i used the features in two fully connected Dense layers which is just artificial an neural networks ANN classifier. It is like a set of learnable filters. As this step resize all the 10015 images dimensions into 100x 75 so be patient it will take some time. Melanocytic nevi 2. In this kernel I have followed following 14 steps for model building and evaluation which are as follows Step 1 Importing Essential Libraries Step 2 Making Dictionary of images and labels Step 3 Reading and Processing Data Step 4 Data Cleaning Step 5 Exploratory data analysis EDA Step 6 Loading Resizing of images Step 7 Train Test Split Step 8 Normalization Step 9 Label Encoding Step 10 Train validation split Step 11 Model Building CNN Step 12 Setting Optimizer Annealing Step 13 Fitting the model Step 14 Model Evaluation Testing and validation accuracy confusion matrix analysis of misclassified instances Step 1 importing Essential Libraries Step 2 Making Dictionary of images and labelsIn this step I have made the image path dictionary by joining the folder path from base directory base_skin_dir and merge the images in jpg format from both the folders HAM10000_images_part1. The LR is the step by which the optimizer walks through the loss landscape. In order to make the optimizer converge faster and closest to the global minimum of the loss function i used an annealing method of the learning rate LR. Step 9 Label EncodingLabels are 7 different classes of skin cancer types from 0 to 6. Benign keratosis like lesions 4. Root Mean Square Propagation RMSProp that also maintains per parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight e. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. Histopathology Histo Histopathologic diagnoses of excised lesions have beenperformed by specialized dermatopathologists. Combining convolutional and pooling layers CNN are able to combine local features and learn more global features of the image. how quickly it is changing. This metric function is similar to the loss function except that the results from the metric evaluation are not used when training the model only for evaluation. We can also further tune our model to easily achieve the accuracy above 80 and I think still this model is efficient in comparison to detection with human eyes having 77. In the last layer Dense 10 activation softmax the net outputs distribution of probability of each class. With the ReduceLROnPlateau function from Keras. Dermatofibroma In this kernel I will try to detect 7 different classes of skin cancer using Convolution Neural Network with keras tensorflow in backend and then analyse the result to see how the model can be useful in practical scenario. We need to encode these lables to one hot vectors Step 10 Splitting training and validation splitI choosed to split the train set in two parts a small fraction 10 became the validation set which the model is evaluated and the rest 90 is used to train the model. 0344 accuracy I hope kagglers like my stepwise approach to classify cancer types. callbacks i choose to reduce the LR by half if the accuracy is not improved after 3 epochs. This technique also improves generalization and reduces the overfitting. This function will iteratively improve parameters filters kernel values weights and bias of neurons. Actinic keratoses 6. It has 7 different classes of skin cancer which are listed below 1. Function to plot model s validation loss and validation accuracy summarize history for accuracy summarize history for loss Merging images from both folders HAM10000_images_part1. It is the error rate between the oberved labels and the predicted ones. It combines all the found local features of the previous convolutional layers. Dropout is a regularization method where a proportion of nodes in the layer are randomly ignored setting their wieghts to zero for each training sample. Welcome to my kernel Skin cancer is the most common human malignancy is primarily diagnosed visually beginning with an initial clinical screening and followed potentially by dermoscopic analysis a biopsy and histopathological examination. Adam is a popular algorithm in the field of deep learning because it achieves good results fast. It looks at the 2 neighboring pixels and picks the maximal value. This means the algorithm does well on online and non stationary problems e. 5 years biologists accepted this as evidence of biologic benignity. natural language and computer vision problems. Data AugmentationIt is the optional step. Step 12 Setting Optimizer and AnnealerOnce our layers are added to the model we need to set up a score function a loss function and an optimisation algorithm. Confocal Reflectance confocal microscopy is an in vivo imaging technique with a resolution at near cellular level and some facial benign with a grey world assumption of all training set images in Lab color space beforeand after manual histogram changes. in order to minimise the loss. We define the loss function to measure how poorly our model performs on images with known labels. This flattening step is needed so that you can make use of fully connected layers after some convolutional maxpool layers. Only nevi but no other benign diagnoses were labeled with this type of ground truth because dermatologists usually do not monitor dermatofibromas seborrheic keratoses or vascular lesions. The idea is to alter the training data with small transformations to reproduce the variations Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Each filter transforms a part of the image defined by the kernel size using the kernel filter. We use a specific form for categorical classifications 2 classes called the categorical_crossentropy. These are used to reduce computational cost and to some extent also reduce overfitting. Consensus For typical benign cases without histopathology or followup biologists provide an expert consensus rating of authors PT and HK. Lesions with this type of groundtruth were usually photographed for educational reasons and did not needfurther follow up or biopsy for confirmation. I choosed Adam optimizer because it combines the advantages of two other extensions of stochastic gradient descent. Filters can be seen as a transformation of the image. Some popular augmentations people use are grayscales horizontal flips vertical flips random crops color jitters translations rotations and much more. The kernel filter matrix is applied on the whole image. The higher LR the bigger are the steps and the quicker is the convergence. Adaptive Gradient Algorithm AdaGrad that maintains a per parameter learning rate that improves performance on problems with sparse gradients e. The most important function is the optimizer. Now lets check the presence of null values again Step 5 EDAIn this we will explore different features of the dataset their distrubtions and actual countsPlot to see distribution of 7 different classes of cell typeIts seems from the above plot that in this dataset cell type Melanecytic nevi has very large number of instances in comparison to other cell typesPlotting of Technical Validation field ground truth which is dx_type to see the distribution of its 4 categories which are listed below 1. Adam realizes the benefits of both AdaGrad and RMSProp. The first is the convolutional Conv2D layer. By applying just a couple of these transformations to our training data we can easily double or triple the number of training examples and create a very robust model. Step 13 Fitting the modelIn this step finally I fit the model into x_train y_train. I choosed to set 32 filters for the two firsts conv2D layers and 64 filters for the two last ones. The metric function accuracy is used is to evaluate the performance our model. relu is the rectifier activation function max 0 x. Vascular lesions 7. We can make your existing dataset even larger. This layer simply acts as a downsampling filter. Automated classification of skin lesions using images is a challenging task owing to the fine grained variability in the appearance of skin lesions. ", "id": "sid321axn/step-wise-approach-cnn-model-77-0344-accuracy", "size": "13617", "language": "python", "html_url": "https://www.kaggle.com/code/sid321axn/step-wise-approach-cnn-model-77-0344-accuracy", "git_url": "https://www.kaggle.com/code/sid321axn/step-wise-approach-cnn-model-77-0344-accuracy", "script": "keras.layers keras.models train_test_split keras confusion_matrix label_binarize keras.utils.np_utils Image numpy keras.preprocessing.image seaborn Adam Dropout ImageDataGenerator BatchNormalization Dense glob backend as K keras.layers.normalization keras.callbacks MaxPool2D plot_confusion_matrix keras.optimizers ReduceLROnPlateau matplotlib.pyplot Sequential backend PIL sklearn.model_selection pandas plot_model_history Conv2D to_categorical # convert to one-hot-encoding to_categorical # used for converting labels to one-hot-encoding sklearn.metrics Flatten sklearn.preprocessing ", "entities": "(('dataset Convert predictions', 'confusion matrix'), 'zip') (('it', 'good results'), 'be') (('Function', 'HAM10000_images_part1'), 'summarize') (('dermatologists', 'dermatofibromas seborrheic usually keratoses'), 'label') (('filter', 'kernel filter'), 'transform') (('when accuracy', 'X steps dynamically epochs'), 'decrease') (('I', '64 two last ones'), 'choose') (('benchmark dataset', 'human experts'), 'use') (('so we', 'mean'), 'make') (('image path', 'HAM10000_images_part1'), 'follow') (('It', 'maximal value'), 'look') (('0344 I', 'cancer types'), 'accuracy') (('Lesions', 'needfurther confirmation'), 'photograph') (('90', 'model'), 'need') (('years 5 biologists', 'biologic benignity'), 'accept') (('how poorly model', 'known labels'), 'define') (('we', 'very robust model'), 'double') (('step images', 'image folder'), 'seem') (('Adam', 'AdaGrad'), 'realize') (('that', 'gradients sparse e.'), 'AdaGrad') (('Combining', 'image'), 'be') (('function metric accuracy', 'performance'), 'use') (('we', 'training dataset'), 'choose') (('which', 'ISIC archive'), 'consist') (('These', 'also overfitting'), 'use') (('that', 'data augmentation techniques'), 'be') (('optimizer', 'probably local minima'), 'be') (('Automated classification', 'skin lesions'), 'be') (('we', 'loss function'), 'add') (('step we', 'then thier standard deviation'), 'add') (('Label Step 9 EncodingLabels', '6'), 'be') (('kernel filter matrix', 'whole image'), 'apply') (('code', '4 least type'), 'choosen') (('function', 'neurons'), 'improve') (('regularization where proportion', 'training sample'), 'be') (('most common human malignancy', 'dermoscopic potentially analysis'), 'be') (('Flatten layer', '1D one single vector'), 'be') (('second important layer', 'CNN'), 'be') (('13 step finally I', 'x_train y_train'), 'step') (('results', 'only evaluation'), 'be') (('algorithm', 'problems well online stationary e.'), 'mean') (('activation rectifier function', 'network'), 'use') (('Filters', 'image'), 'see') (('optimizer', 'loss landscape'), 'be') (('layer', 'downsampling simply filter'), 'act') (('that', 'weight e.'), 'rmsprop') (('i', 'learning rate LR'), 'use') (('grayscales horizontal vertical flips', 'crops color jitters translations rotations'), 'be') (('base where images', 'image folder'), 'step') (('We', 'cancer'), 'move') (('human eyes', '77'), 'tune') (('you', 'maxpool convolutional layers'), 'need') (('accuracy', '3 epochs'), 'callback') (('This', 'distributed way'), 'drop') (('technique', 'overfitting'), 'improve') (('nevi', 'visits'), 'follow') (('which', '1'), 'have') (('training 10000 images', 'Machine'), 'dataset') (('where you', 'input'), 'use') (('75 so it', 'time'), 'be') (('It', 'previous convolutional layers'), 'combine') (('We', 'categorical classifications 2 classes'), 'use') (('which', 'Dense two fully connected layers'), 'use') (('why we', '100 75'), 'resize') (('how model', 'practical scenario'), 'Dermatofibroma') (('it', 'gradient stochastic descent'), 'choose') (('confocal microscopy', 'histogram manual changes'), 'be') (('It', 'error oberved labels'), 'be') (('which', '1'), 'check') (('Consensus', 'authors'), 'provide') (('we', 'artificially HAM'), 'need') (('Histopathology Histo Histopathologic diagnoses', 'specialized dermatopathologists'), 'beenperforme') (('that', 'images feature everywhere transformed maps'), 'isolate') (('only authors', 'independently same unequivocal benign diagnosis'), 'apply') "}