{"name": "how to become a property tycoon in new york ", "full_name": " h1 How to predict House Prices and hopefully become a Property Tycoon in New York h1 1 Load Data and Clean it h1 2 Data Inspection h2 2 1 Dependent Variable Inspection h2 2 2 Independent Variables Inspection h1 3 Modelling h2 3 1 Data Preparation h2 3 2 Split into Training Testing Data h2 3 3 Running the Different Models h1 4 Conclusion and Next Steps ", "stargazers_count": 0, "forks_count": 0, "description": "In a next iteration I will also fine tune these to get an even better result. 2 75 of properties have no COMMERCIAL UNITS 3 At least 50 of all properties have only 1 TOTAL UNIT. In a next iteration I will therefore explore the possibilities to impute the SQUARE FEET data because I deleted a lot here. That s a fair chunk of data. It turns out SPOILER ALERT that this is the best predictor of SALE PRICE in this dataset which means I want to keep as much data as possible instead of just throwing them away. Maybe this isn t all that surprising given that Manhattan is home to a lot of very expensive property. 1 Dependent Variable Inspection SALE PRICE Let s start with the dependent variable as this is the one I want to predict. I will use1 Linear Regression 2 Random Forest Regression 3 Ridge Regression 4 and ElasticNet 1 Linear Regression 2 Random Forest Feature Importance of Random Forest 3 Ridge Regression 4 ElasticNet and GridSearch 4 Conclusion and Next Steps1 An untuned Random Forest Regression managed to get a R2 of 0. However YEAR BUILT isn t quite the variable we are looking for. data that has a mean around 0 and a distribution around that. From the descriptive statistics we could also tell that 75 of the properties in this dataset are cheaper than 950 000 USD. 2 Data InspectionAgain I have already done some data inspection and I will not show those variables that aren t useful for the model. Total Units I am deleting the outliers with very large numbers of TOTAL UNITS and those with 0 units. The same is true for UNITS data. There are 765 duplicates. Those are only a handful of observations though. 1 Load Data and Clean itLet s update the BOROUGH names first according to the instructions found on Kaggle. alpha Perform 10 fold CV ridge_cv_scores Append the mean of ridge_cv_scores to ridge_scores Append the std of ridge_cv_scores to ridge_scores_std Display the plot Instantiate a ridge regressor ridge Fit the model Perform 5 fold cross validation ridge_cv Print the 5 fold cross validation scores Create the hyperparameter grid Instantiate the ElasticNet regressor elastic_net Setup the GridSearchCV object gm_cv Fit it to the training data Predict on the test set and compute metrics. So the original 85k don t count. In the next section I will normalise standardise the data and also take the log in order to get rid of the skewness and to allow for a more normal distribution. Import the modules Data Scaler Regression Metrics Read the data Renaming BOROUGHS Change the settings so that you can see all columns of the dataframe when calling df. 2 Split into Training Testing DataThe step is necessary to ensure that the model is flexible and general enough so that can predict accurately unseen or new data. What we can see by looking at the first few rows is that the column Unnamed 0 is an artifact from the data load and is not needed. Using the log allows me to get rid of the skew in the data and have a more normal distribution. I have done a few iterations of what a good cap on sales prices is and settled for 1 property needs to be more expensive than 100 000 USD2 property needs to be cheaper than 5 000 000 USDEverything is else is a different animal and mixing all together in one model will decrease accuracy. SQUARE FEET I need to get rid of the NULL values and a few outliers. I will now test a few different models to see which one performs best. Recap and Revisit of Entire Data Set After I ve removed some observations due to their prices which I have treated as outliers how many NULL SQUARE FEET observations remain Unfortunately there are still a third of observations remaining that contain no SQAURE FEET data. Update Data I have already done some prior inspection of the data and am now updating the dataset by deleting columns and changing the data type of some of the variables. The numerical variables don t need one hot encoding but will have to be normalised. There seem to be some buildings that were built in the year 0 which can t be correct. I will work on this in a future iteration. Delete the old columns. After that I will look at the independent variables which are the ones I use to predict the price. and this what it looks after Transforming the independent variables Some of the variables contain zeroes which is why I need to add 1 so that I can take the log before normalising it you can see that in the table below. How to predict House Prices and hopefully become a Property Tycoon in New YorkI am going to clean and visualise data and build a model to predict housing prices in New York. Capture the necessary data Plot number of available data per variable Remove observations with missing SALE PRICE Set the size of the plot Plot the data and configure the settings Set the size of the plot Plot the data and configure the settings Set the size of the plot Get the data and format it Plot the data and configure the settings Remove observations that fall outside those caps Set the size of the plot Plot the data and configure the settings Set the size of the plot Plot the data and configure the settings Set the size of the plot Get the data and format it Plot the data and configure the settings Capture the necessary data Plot number of available data per variable Removes all NULL values Keeps properties with fewer than 20 000 Square Feet which is about 2 000 Square Metres Only a handful of properties with 0 total units are remaining and they will now be deleted Remove data where commercial residential doesn t equal total units Correlation Matrix Compute the correlation matrix Generate a mask for the upper triangle Set up the matplotlib figure Generate a custom diverging colormap Draw the heatmap with the mask and correct aspect ratio Choose only the variables I want to use in the model Select the variables to be one hot encoded For each categorical column find the unique number of categories. It s easier that way to see where the NULL values are and how many there are. I ve been toying with the idea of clustering properties on their SALE PRICE classifiying them as something like cheap normal expensive and luxury in order to avoid this issue. The reason why I need to add 1 is because I can t take the log of 0 it is not defined. Transforming the dependent variable SALE PRICE This is what SALE PRICE looks before the transformation. I train the model with the training data and then check how good it performs on the unseen testing data. Everything is done obviously with the aim of becoming a property tycoon one of the most important things to know in order to achieve that is the value of a property. 1 Data Preparationscikit works best with normalized data i. Building Class Category Some of the categories could potentially be merged in a future iteration. There are also a fair number of properties cheaper than 100 000 USD which seems too cheap in my opinion. What we see from the two graphs above is that there are a lot of outliers. BOROUGH This is all in good shape and no surpises here. Let s remove them High Level Data Inspection and Validation Let s show this visually. head EASE_MEANT is empty and can be dropped Unnamed 0 is an artifact from the data load and can be deleted SALE PRICE is object but should be numeric LAND and GROSS SQUARE FEET is object but should be numeric SALE DATE is object but should be datetime Both TAX CLASS attributes should be categorical Delete the duplicates and check that it worked Check the number of rows and columns Get a high level overview of the data types the amount of NULL values etc. and add the new one hot encoded variables Take the log and normalise Add 1 to Units Take the log and standardise Add 1 to Units Take the log and standardise Add 1 to BUILDING AGE Take the log and standardise Split data into training and testing set with 80 of the data going into training X are the variables features that help predict y which tells us whether an employee left or stayed. Those observations will have to be deleted. YEAR BUILT Next one up is YEAR BUILT. The column EASEMENT is completely empty and will be deleted. After removing the missing SALE PRICES we are left with 70k observations down from 85k at the very start. 6 Some properties have a SALE PRICE of 0 which is also wrong or a transfer but not actually a sale. What does the plot look like now OK that s a lot more realistic. 3 Running the Different ModelsFinally the moment we have all been waiting for. Anyways let s check out the same graphs again. And there some missing Sale Prices. This is done for both Create the regressor linreg Fit the regressor to the training data Predict the labels of the test set y_pred Compute 5 fold cross validation scores cv_scores Print the 5 fold cross validation scores Compute 5 fold cross validation scores cv_scores Print the 5 fold cross validation scores Plot the feature importances of the forest Import necessary modules Setup the array of alphas and lists to store scores Create a ridge regressor ridge Compute scores over range of alphas Specify the alpha value to use ridge. But first I get all the relevant variables and one hot encode the categrical variables which is necessary for scikit to work. I am not quite sure what to make of that yet. 20 of all Sale Prices are NULL which is what I wanted to predict. Convert categorical variables into dummy indicator variables i. I ll show the importance of SQAURE FEET in the result section at the end of the workbook. I could still predict a price for those cases but there is no way of verifying the accuracy of the predictions. And we re down from 70k observations to 55k which is 79 remaining. This tells us how many columns we are adding to the dataset. So without further ado let s go for it. Remember that there were a fair amount of NULL SALE PRICE observations and duplicates etc. There is potential to impute those values but we will have to see how well that will work. SQUARE FEET BUILDING AGE and BOROUGH were the most important features determining the SALE PRICE. 2 We only used 30k observations out of a potential of 70k. Now let s get an overview of some descriptive stats of the numerical variables in the data set. There are also around a third of all observations with missing Square Feet data. After updating the data let s check if there are any duplicate values in here. More Data Visualisations 3 Modelling 3. What is more interesting is the BUILDING AGE. Some interesting observations to note 1 There are ZIP CODES with a value of 0 which is probably wrong. In addition I get rid of the observations the sum of COMMERCIAL UNITS and RESIDENTIAL UNITS doesn t match TOTAL UNITS. 3 Instead of throwing outlier SALE PRICE data I will try to use clustering to classify properties w hich would allow me to keep the outlier data maybe. UNITS SQUARE FEET BUILDING AGE 3. I will first look at the dependent variable SALE PRICE which is the one I want to predict. Currently I have removed all the NULL and outlier data of SQUARE FEET and TOTAL UNITS but in a future iteration I will try to impute some data points to keep as much as data as possible. Both contain similar information the second is however a bit more practical. The log of 1 however is. 40 which is certainly not great but maybe not bad given the limited amount of data available. 5 Some buildings were built in the YEAR 0 which again is wrong. 2 Independent Variables Inspection I have already looked at each variable in more detail and only show the ones I am going to keep for the model. 4 There are properties have 0 SQUARE FEET which shouldn t be possible unless they don t exist yet or the data is wrong. ", "id": "akosciansky/how-to-become-a-property-tycoon-in-new-york", "size": "8495", "language": "python", "html_url": "https://www.kaggle.com/code/akosciansky/how-to-become-a-property-tycoon-in-new-york", "git_url": "https://www.kaggle.com/code/akosciansky/how-to-become-a-property-tycoon-in-new-york", "script": "train_test_split LinearRegression Ridge stats RandomForestRegressor numpy cross_val_score seaborn display_plot scipy sklearn sklearn.linear_model KFold matplotlib.pyplot metrics sklearn.model_selection pandas matplotlib.style ElasticNet mean_squared_error sklearn.metrics GridSearchCV Lasso sklearn.ensemble StandardScaler sklearn.preprocessing ", "entities": "(('Next Random Forest GridSearch 4 untuned Regression', '0'), 'will') (('Those', 'only observations'), 'be') (('Data 1 Preparationscikit', 'data best normalized i.'), 'work') (('s', 'data'), 'let') (('100 cheaper than 000 which', 'too opinion'), 'be') (('s', 'this'), 'let') (('employee', 'us'), 'add') (('SQUARE I', 'NULL values'), 'foot') (('Everything', 'property'), 'do') (('Select', 'categories'), 'capture') (('I', 'what'), 'be') (('We', '70k'), '2') (('we', 'very start'), 'leave') (('RESIDENTIAL doesn', 't TOTAL UNITS'), 'rid') (('3 At least 50', 'TOTAL only 1 UNIT'), 'have') (('then how it', 'testing unseen data'), 'train') (('array', 'ridge'), 'do') (('numerical variables', 'don one hot encoding'), 'need') (('Building Class Category Some', 'potentially future iteration'), 'merge') (('that', 'that'), 'datum') (('we', 'dataset'), 'tell') (('elastic_net GridSearchCV object', 'test set'), 'Perform') (('also 75', '950 000 USD'), 'tell') (('I', 'workbook'), 'show') (('that', 'SQAURE FEET data'), 'Recap') (('I', 'issue'), 'toy') (('isn Maybe that surprising Manhattan', 'very expensive property'), 't') (('I', 'price'), 'look') (('we', 'variable'), 'BUILT') (('SALE PRICE', 'transformation'), 'transform') (('I', 'predictions'), 'predict') (('see', 'above outliers'), 'be') (('which', '55k'), 're') (('me', 'more normal distribution'), 'allow') (('Now s', 'data set'), 'let') (('0 which', 'YEAR'), '5') (('I', 'more normal distribution'), 'normalise') (('SQUARE FEET BUILDING AGE', 'SALE most important PRICE'), 'be') (('which', '0'), '6') (('that', 'accurately unseen data'), 'split') (('I', 'lot'), 'explore') (('how that', 'values'), 'be') (('How predict', 'New York'), 'go') (('you', 'table'), 'and') (('else different mixing', 'accuracy'), 'do') (('This', 'good shape'), 'BOROUGH') (('scikit', 'relevant variables'), 'get') (('that', 'model'), '2') (('column', 'data 0 load'), 'be') (('I', 'dependent variable'), 'let') (('I', 'future iteration'), 'work') (('I', 'that'), 'be') (('I', 'even better result'), 'tune') (('Total I', '0 units'), 'unit') (('yet data', 't'), '4') (('second', 'similar information'), 'be') (('me', 'outlier data'), 'try') (('t', 'year'), 'seem') (('I', 'possible instead just them'), 'turn') (('number', 'NULL values'), 'be') (('I', 'SALE first dependent variable PRICE'), 'look') (('s', 'same graphs'), 'let') (('it', '0'), 'be') (('Update Data I', 'variables'), 'do') (('that', 'What'), 'look') (('moment we', 'Different ModelsFinally'), '3') (('one', 'now a few different models'), 'test') (('I', 'model'), 'Inspection') (('I', 'data'), 'remove') (('which', 'data'), '40') (('which', '0'), 'be') (('you', 'when df'), 'import') "}