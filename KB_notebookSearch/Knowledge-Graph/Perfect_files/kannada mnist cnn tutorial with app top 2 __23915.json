{"name": "kannada mnist cnn tutorial with app top 2 ", "full_name": " h1 A Simple CNN Tutorial with App h1 1 Introduction h4 If you will find it useful I would appreciate it if you upvote h4 Then let s begin h1 2 Convolutional Neural Networks h2 Convolution Operation h4 Steps of convulation operation h4 Stride h3 An Example of Convolution Operation h3 Why do we use Filters h2 Padding h3 Why do we use Padding h3 Equation of calculating output dimension changes with using padding process as below h3 Types of Padding in Keras h2 Pooling h3 Hyper Parameters of Pooling Operation h3 Why do we use Pooling h3 Types of Pooling in Keras h2 Batch Normalization h4 What is Batch Norm h4 Why do we use Batch Norm h4 Some notes about Batch Norm h2 Drop Out h1 3 An Image Classification Application with CNN h2 Import Modules h2 Understanding the Data h4 Image of Handwritten Character h2 Data Preprocessing h3 Normalizing Data h3 Cross Validation Training Validation Set Split h3 Reshape Data to Fit Model h2 Building and Training a CNN Model h4 Model Build h3 Model Compile h4 Compile Loss Function Categorical Crossentropy h4 Compile Optimizer h4 On the Fly Data Augmentation h3 Model Fit h2 Evaluation of the Model h3 Accuracy and Loss Curves h3 Test Set Accuracy Score h3 Confusion Matrix h3 F1 Score Calculation h3 Evaluate with Another Dataset h3 Submit for Competition h1 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "f is the horizontal vertical dimension of filter kernel matrix. According to problem stride content may vary change and this will directly effect on output dimensions. Pooling Return Contents 0 Pooling refers to the process of dividing a matrix into pools and select from every pool only one value as representing that pool. Padding Return Contents 0 Padding refers to the process of symmetrically adding values to the input matrix. The information at the edges is convoluted only once while the information in the middle is convoluted 3 times with a 3x3 filter. there is 2 x 2 pooling size and stride 2 sliding on matrix by 2 cell Why do we use Pooling Return Contents 0 1. Changing the distribution of each layer s inputs during training because of the enhancement of parameters of the previous layers calling covariate shifting at the original paper. You have your output 0 1 value. s is the stride constant. Dataset has been already flattened and has 784 pixel values for each pic. We can increase the number of layers with this logic. __As you see below__ while Stochastic Gradient Descent SGD which is a basic gradient descent algorithm cannot escape the saddle point more advanced algorithms escape the saddle point __at different speeds. For example the first layer of the network can detect just __horizontal lines__ while deeper layers of the network __can detect the nose or eye__ and again ongoing deeper layers __can detect even the human face. Numerically high values are indicative of pattern compliance. __Building and Training a CNN Model__ 250 Model Build 251 Model Compile 252 Categorical Cross Entropy 253 Optimizer 255 On the Fly Data Augmentation 257 Model Fit 259 5. F1 Score Calculation Return Contents 0 As mentioned above accuracy 271 gives a general idea about the performance of the model but does not provide any information about the model s trends. 0 quiet 1 update messages. threshold for measuring the new optimum to only focus on significant changes. The reason for using the test set on the final evaluation is the model would have a bias on the validation set because we developed the model according to the validation set performance. __Data Preprocessing__ 230 Normalizing the Data 231 Train and Test Splitting 232 Reshape Data to Appropriate Sizes 233 4. Therefore you just initialize them and with backpropagation or other optimization algorithm they will be optimized like weight parameters. __Drop Out Layer__ 180 3. Result is the upper corner cell s value of your new output matrix. 6x6x3 In this case we will convolute the filter 1 to have three different values from 3 channels and will sum the results to obtain a single number. The confusion matrix shows how confused your classification model is for which classes by detailing the relationship between actual class and predicted class. auto min max. __CONCLUSION__ 290 1. But in another aspect the model s success in predicting dogs is 0. Batch Normalization Return Contents 0 What is Batch Norm Return Contents 0 Batch Normalization is a method used to normalize the input of a hidden layer by adjusting and scaling the activations. Some of them are ADAGRAD ADAM ADAMAX NADAM and RMSPROP. rsz_convolution with multiple filters2. So a kind of overfitting on the validation set is formed. jpg attachment rsz_1smile. An Image Classification Application with CNN Return Contents 0 Import Modules With this code below you can check if the kernel use GPU or not. A feature map can be considered as the new picture we have for the next CNN layer. __If the data wasn t homogeneously__ distributed what would we do 1. __Padding__ 130 Why do we use Padding 131 Equation of Calculating Output Dimension 132 Types of Padding in Keras 133 3. To speed up the computation Reducing the size of the representation for increasing the speed of the computation. __Evaluation of the Model__ 270 Accuracy and Loss Curves 271 Test Set Accuracy Score 272 Confusion Matrix 273 F1 Score Calculation 274 Evaluate with Another Dataset 275 Submit for Competition 275 6. Take the element wise production of the upper left corner sub matrix and filter2. Filter Matrices containing a specific pattern 2. Our brain recognizes some parts of the face separately and combines them to give a final decision. Steps of convulation operation 1. Convolution is a specialized kind of linear operation. Calculation of output matrix dimension for l th layer begin equation n l frac n l 1 f s 1 text output n l x n l end equation Where l demonstrates the layer number. It is about only selecting one value from small groups. png attachment pooling. After training the data Kaggle will evaluate the final performance of our data with test set predictions. Data Preprocessing Normalizing Data Return Contents 0 What is normalizing Normalization means that adjusting values measured on different scales to a notionally common scale. As we don t have the labels we don t know the final performance of the test set until we submit our predictions. If you want to take a more detailed look at Gradient Descent algorithms here https ruder. png As you see above we have 5x5 image matrix and 3x3 filter matrix If we choose the stride S parameter as 1 then our output matrix would be 3x3 matrix If we choose the stride S parameter as 2 then our output matrix would be 2x2 matrix begin equation n l frac 5 3 1 1 3 text output 3 x 3 text n l frac 5 3 2 1 2 text output 2 x 2 end equation If the entrance picture is colored RGB as below the 2nd and 3rd dimensions will also be available. Benan AKCA brain. Because each pixel in every sample of training set has integer values from 0 to 255. Understanding the Data Return Contents 0 We have training and test set CSV files In order to evaluate the generalization skill of the model we will split our training set into training and validation sets. Introduction Return Contents 0 Hello everyone first of all I am pleased to present this work to you. Building and Training a CNN Model Return Contents 0 On Keras Sequential Networks there is three stage for training building compiling and fitting the model. lower bound on the learning rate. In the application part I also gave some theoretical information with visual details according to the necessity. Let s read csv files We have 28 x 28 dimension handwritten pics. io content images 2016 09 saddle_point_evaluation_optimizers. On image processing applications generally we normalize data to 0 1 scale with dividing data to 255. Stride Same as convolution operation for example in the above pic. gif On the Fly Data Augmentation Return Contents 0 On classification tasks on image datasets data augmentation is a common way to increase the generalization of the model. This study consists of two main parts. Model Fit Return Contents 0 Evaluation of the Model Accuracy and Loss Curves Return Contents 0 On classification accuracy metric is calculated as below begin equation classification accuracy frac correct predictions total predictions 100 end equation Test Set Accuracy Score Return Contents 0 Confusion Matrix Return Contents 0 If 90 of the data set is cat image and 10 is dog image your accuracy will be 90 even if you estimate the entire test set as a cat. The important factor in here is filter1 has the same values for all three channel demonstrated as different colors he has. With the ImageDataGenerator on Keras we can handle this objective easily. __Batch Normalization Layer__ 160 5. Drop Out Return Contents 0 Drop out is an effective regularization method. The output matrix dimension getting smaller whenever we use convolution operation. After feature extraction we use a fully connected layer for classification according to extracted features. png __Why do we use Padding __ Return Contents 0 1. __Import Modules__ 210 2. Types of Pooling in Keras Return Contents 0 Max Pooling The operation of selecting the maximum value from each mini group pool as the picture above Average Pooling The operation of selecting the average value from each mini group. Factor by which the learning rate will be reduced. In order to normalize training set data we need to convert x to float type. For I prefer to split Training set 80 Validation set 20 Reshape Data to Fit Model Return Contents 0 In order to feed the CNN model we need to reshape our 54000 x 784 flatten image data to 54000 x 28 x 28 x 1 dimensions. png Why do we use Filters Return Contents 0 To answer this question first we should ask another and more general question What is our main purpose In this kernel our main purpose is as you know recognition handwrite patterns but for simplicity let us think about cats. There are 2 kinds of false predictions __Predicted as 1 but Ground Truth is 0 False Positive __ __Predicted as 0 but Ground Truth is 1 False Negative __ The F1 score creates a success performance metric taking into account both of these incorrect prediction performances as well as the true positive predictions Since our problem has more than 2 classes we calculated the F1 score for each class on a one to all basis. io optimizing gradient descent. co Qcb9y35 Types of Learning in Machine Learning. __Why do we use Drop Out __ The answer is to prevent overfitting. Whenever numpy has 1 value on reshape method it will calculate the dimension which denoted with 1 automatically. Despite the convolution operation Pooling has not any weight because you don t use any filter to operation. It can be used with most types of layers such as dense fully connected layers convolutional layers and recurrent layers such as the long short term memory network layer. When you see a cat face picture how do you understand that is a cat It should have solid patterns on its face right Solid patterns like eye shape nose curve mustaches etc. The loss is calculated using the following formula begin equation Loss frac 1 m sum_ i 1 m sum_ k 1 K Y i_klog hat Y i_k 1 Y i_k log 1 hat Y i_k end equation where k demonstrates class i demonstrates sample number hat Y_c is the predicted value Y_c is the ground truth value m is the sample number in a batch and K is the total number of classes. __Understanding the Data__ 220 3. This slows down training by requiring lower learning rates and careful parameter initialization making it difficult to train models with non linear saturation. com 2019 07 08 keras imagedatagenerator and data augmentation who is the author of PyImageSearch a very instructive web site about computer vision. __By changing__ some of the properties of the image from the code below __you can observe what changes are happening in the dataset__. In this context accuracy may not always give us realistic information about the actual performance of the model. Basic Theoretical Knowledge Causal approaches Visual expression Answers of the questions that may arise in your mind about CNN. Each filter has a job some of them detect the edges Sobel filter as on the example some of them detect just the horizontal lines. This has the effect of making the layer look like and be treated like a layer with a different number of nodes and connectivity to the prior layer. jpg linear algebra data processing CSV file I O e. __INTRODUCTION__ 1 2. The output matrix width w and height h sizes are generally the same in applications but it can be different also. They give us a training set for training the model and a test set without labels for prediction. jpg attachment brain. new_lr lr factor The number of epochs with no improvement after which learning rate will be reduced. Why should you normalize the data With a normalized data weight values reach optimum value faster. io content images 2016 09 contours_evaluation_optimizers. To regularize the network side positive effect Some notes about Batch Norm Return Contents 0 The parameters gamma and beta are learnable parameters. Why do we use log Because cross entropy function penalize bigger difference more and smaller difference less as mentioned below. During training some number of layer outputs are randomly ignored or dropped out. io optimizing gradient descent is a very nice overview article written by Sebastian Ruder https ruder. The algorithm of the batch norm is shown below batch_norm. It is important to know the distribution of data according to the labels they have. __Pooling Layer__ 150 Hyper Parameters of Pooling Operation 151 Why do we use Pooling 152 Types of Pooling in Keras 153 4. Equation of calculating output dimension changes with using padding process as below begin equation n l frac n l 1 2p f s 1 text output n l x n l end equation Types of Padding in Keras Return Contents 0 Valid No padding Same Choosing a p value on above equation to obtain output matrix in same dimension as like input matrix. Model Build On building stage you specify the architecture of the model mainly. Here it can be observed in which regions of the matrix in the previous layer how much data is available for the filter pattern. In the Conv layer the filters are shifted over the image by the number of strides and convolution is performed. We can now detect the patterns in this picture with a new filter. So output images are a matrix in w x h x c dimension. number of epochs to wait before resuming normal operation after learning rate lr has been reduced. com multi class metrics made simple part ii the f1 score ebe8b2c2ca1 is a nice article on how to calculate the F1 score in multiple classes supported by examples. This is the 2nd layer of CNN. Or if we have enough data we can discard some high quantity labels Image of Handwritten Character Return Contents 0 An overview of a picture You can change the num variable to see other numbers. This results in the loss of information on the edges. png Compile Optimizer Return Contents 0 To optimize the weight values of the network we should choose an optimizer algorithm. png attachment padding. Using for determine how many columns will kernel shift to right side in order to calculate output matrixes next cell. Then let s begin. On the example above we use only one filter but in real applications there will be more filters used at the same time as below. png Hyper Parameters of Pooling Operation 1. You can decide the Filter 110 size and Padding 150 type you will use on Convolution 110 operations and add Pooling 150 Batch Normalization Dropout activation function layers with build section. This Kernel is prepared on a Kaggle competition dataset. Totaly we have 60000 pics in training set. Compile Loss Function Categorical Crossentropy Return Contents 0 The Categorical Crossentropy Loss Function computes between network predictions and target values for multiclass classification problems. In short terms we use Batch Norm 1. You can leave one unknown dimension as 1. The matrix obtained as a result of the 3rd process is called a feature map. Practicing with an Application Handwriting recognition with the Kannada handwritten digits dataset which is different than the MNIST handwritten digits dataset. In min mode lr will be reduced when the quantity monitored has stopped decreasing in the max mode it will be reduced when the quantity monitored has stopped increasing in auto mode the direction is automatically inferred from the name of the monitored quantity. read_csv data visualization data visualization First and last values of dataset labels An observation code for our dataset fit parameters from data configure batch size and retrieve one batch of images create a grid of 3x3 images show the plot Quantity to be monitored. __APPLICATION WITH CNN__ 200 1. In classification algorithms it is important to analyze the false predictions of the model. jpg Evaluate with Another Dataset Return Contents 0 Submit for Competition Return Contents 0 Conclusion Return Contents 0 Please do not hesitate to comment and ask questions. Plot training validation loss values. To evaluate the model we need to split our training set into training and validation set. The same explanation is acceptable for CNN filters. jpg attachment rsz_1form C3 BCl. If you will find it useful I would appreciate it if you upvote. png Why do we use Batch Norm Return Contents 0 At artificial neural networks distribution of the inputs changes for each layer during training since the weight parameters of the previous layers change. The task of the filter becomes more complex in the deep layers of the evolutionary network. png attachment conv2. n l is the horizontal vertical dimension of output matrix. To use much higher learning rates and be less careful about initialization 3. png attachment batch_norm. Cross Validation Training Validation Set Split Return Contents 0 In order to measure the generalization ability of the model we train the data with the training set and make the model arrangement according to the error value in the validation set. The method consists of performing normalization on each neuron of a specific layer on each training mini batch. In the first lessons of artificial learning the gradient descent algorithm is taught as the optimization algorithm used in backpropagation. Over time the gradient descent algorithm was developed and the algorithms that achieved faster and more accurate results were obtained. Convolution Operation Return Contents 0 rsz_1rsz_conv_sample. Model Compile Return Contents 0 On Compile Section we specify the loss function optimizer algorithm and metric to use for evaluating the model. Then sum all row and column to obtain a single value3. You can imagine that after the convolution operation each filter creates a different layer on the matrix as below. To increase the learning speed of the model. In addition we determine the final performance of the model with the test set. jpg You can think of left one as the input signal or image and the other called the kernel as a filter on the input image producing an output image so convolution takes two matrix as input and produces a third as output. If there is an anomaly something like above mentioned you can specify the problem with confusion matrix and improve accuracy by various methods like adding more data for a specific class etc. An Example of Convolution Operation Return Contents 0 conv2. It has applications in image and video recognition recommender systems image classification medical image analysis and natural language processing The name Convolutional Neural Network indicates that the network employs a mathematical operation called convolution. To gain robustness on feature extraction Pooling prevents the model from over training by discarding the unnecessary data relative to the selected value. Convolutional Neural Networks Return Contents 0 In Deep Learning Convolutional Neural Network CNN or ConvNet is a class of deep neural networks most commonly applied to analyzing visual imagery. png attachment cross_entropy. Here https towardsdatascience. Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers A convolutional neural network consists of one or several convolutional layers followed by some fully connected layers of neurons like in classical multilayer feedforward neural networks. After that slide the filter over S column on main image and do the same thing. n l 1 is the horizontal vertical dimension of input matrix. These algorithms use techniques such as adaptive learning rate and momentum to achieve the global minimum. __Convolution Layer__ 110 Steps of Convolution Operation 111 Stride 112 An Example of Convolution Operation 113 Why Do We Use Filters 114 2. In effect each update to a layer during training is performed with a different view of the configured layer. Padding mostly used in designing the CNN layers when the dimensions of the input volume need to be preserved in the output volume Same Padding. This data set is __homogeneously__ distributed as you see below. Edge information can be convoluted multiple times thanks to padding. If you found it useful I would appreciate it if you upvote rsz_1smile. jpg attachment rsz_1rsz_conv_sample. __ To sum up filters are used to extract the features of the image. With this observation you can roughly specify the range you should choose. png attachment rsz_convolution with multiple filters2. This kernel was prepared within the scope of handwriting recognition contest using the Kannada MNIST data set and in the competition it managed to be among the __top 2 __. Stride Sliding amount is declared by stride S constant. jpg attachment rsz_images. Normalized neurons of l th layer will have a specific mean and variance for the problem according to trained gamma and beta parameters. com 2019 07 08 keras imagedatagenerator and data augmentation is a comprehensive and inspiring article about data augmentation and ImageDataGenerator written by Adrian Rosebrock https www. A Simple CNN Tutorial with App. w and h is calculated as above equation and the channel dimension of the matrix is the same as filter number used on convolution. __CONVOLUTIONAL NEURAL NETWORKS__ 100 1. Then we could use data augmentation techniques to generate new data for low quantity labels 2. Without further ado I would like to give you brief information about the content of the study. The hidden layers of a CNN typically consist of convolutional layers pooling layers fully connected layers and normalization layers. You can read the original paper written by Sergey Ioffe and Christian Szegedy from here https arxiv. ", "id": "benanakca/kannada-mnist-cnn-tutorial-with-app-top-2", "size": "23915", "language": "python", "html_url": "https://www.kaggle.com/code/benanakca/kannada-mnist-cnn-tutorial-with-app-top-2", "git_url": "https://www.kaggle.com/code/benanakca/kannada-mnist-cnn-tutorial-with-app-top-2", "script": "calc_F1 train_test_split tensorflow.keras.layers to_categorical confusion_matrix keras.utils.np_utils numpy MaxPooling2D seaborn Dropout Adadelta ImageDataGenerator BatchNormalization Dense LeakyReLU RMSprop tensorflow ReduceLROnPlateau tensorflow.keras.utils matplotlib.pyplot tensorflow.keras.preprocessing.image tensorflow.keras.callbacks Sequential sklearn.model_selection pandas EarlyStopping Conv2D l2 tensorflow.keras tensorflow.keras.optimizers tensorflow.keras.regularizers sklearn.metrics Flatten Nadam ", "entities": "(('Convolutional Neural Networks Return Deep Learning Convolutional Neural Network 0 CNN', 'most commonly visual imagery'), 'content') (('Some', 'them'), 'be') (('classification how model', 'actual class'), 'show') (('I', 'you'), 'content') (('filters', 'strides'), 'shift') (('Result', 'corner output upper new matrix'), 'be') (('_ _ again ongoing deeper _', 'even human face'), 'detect') (('They', 'prediction'), 'give') (('It', 'term memory network recurrent such long short layer'), 'use') (('parameters 0 gamma', 'Batch Norm Return Contents'), 'regularize') (('We', 'new filter'), 'detect') (('I', 'necessity'), 'give') (('we', 'model'), 'content') (('more advanced algorithms', '_ different speeds'), '_') (('you', 'roughly range'), 'specify') (('we', 'single number'), '6x6x3') (('K', 'total classes'), 'calculate') (('png _ Why we', 'Padding _ _ Return Contents'), '_') (('rate lr', 'normal operation'), 'reduce') (('normalizing', 'notionally common scale'), '0') (('output So images', 'h c dimension'), 'be') (('algorithm', 'batch_norm'), 'show') (('We', 'logic'), 'increase') (('when dimensions', 'output volume'), 'use') (('h', 'convolution'), 'calculate') (('we', 'predictions'), 'have') (('You', '1'), 'leave') (('so convolution', 'output'), 'think') (('some', 'just horizontal lines'), 'have') (('entropy function', 'bigger difference'), 'use') (('weight parameters', 'previous layers'), 'use') (('f', 'filter kernel horizontal vertical matrix'), 'be') (('Edge information', 'multiple times thanks padding'), 'convolute') (('Dataset', 'pic'), 'flatten') (('gain', 'relative selected value'), 'prevent') (('success', 'dogs'), 'be') (('151 Why we', 'Keras'), 'Layer') (('same explanation', 'CNN filters'), 'be') (('direction', 'monitored quantity'), 'reduce') (('neural convolutional network', 'classical multilayer'), 'be') (('generally we', '255'), 'normalize') (('I', 'study'), 'like') (('we', 'validation set performance'), 'be') (('whenever we', 'convolution operation'), 'get') (('Quantity', '3x3 images'), 'label') (('filter', 'matrix'), 'imagine') (('obtained', '3rd process'), 'call') (('It', 'small groups'), 'select') (('we', 'extracted features'), 'use') (('layer', 'prior layer'), 'have') (('Why you', 'optimum value'), 'normalize') (('we', 'CNN next layer'), 'consider') (('us', 'cats'), 'use') (('1 text end entrance 5 3 2 2 output 2 2 picture', 'RGB 2nd dimensions'), 'png') (('We', '28 28 dimension handwritten pics'), 'let') (('you', 'model'), 'build') (('it', 'generally applications'), 'be') (('you', 'it'), 'find') (('that', 'CNN'), 'approach') (('Pooling Return Contents 0 Pooling', 'pool'), 'refer') (('you', 'Gradient Descent https algorithms here ruder'), 'want') (('Example', 'Convolution Operation Return Contents'), 'conv2') (('you', 'rsz_1smile'), 'find') (('update', 'configured layer'), 'perform') (('kernel', 'GPU'), 'check') (('we', 'training sets'), 'understand') (('n l', 'output horizontal vertical matrix'), 'be') (('we', 'training set'), 'evaluate') (('So kind', 'validation set'), 'form') (('network', 'mathematical operation'), 'have') (('simple part', 'examples'), 'make') (('Then we', 'quantity low labels'), 'use') (('You', 'Christian https arxiv'), 'read') (('faster more accurate results', 'time'), 'develop') (('number', 'layer outputs'), 'ignore') (('It', 'eye shape nose curve right Solid mustaches'), 'see') (('optimization other they', 'weight parameters'), 'initialize') (('anomaly above you', 'class specific etc'), 'be') (('we', 'objective'), 'handle') (('Why we', 'Pooling Return Contents'), 'be') (('Padding 150 you', 'build section'), 'decide') (('Batch Normalization Return Batch Norm Return Batch 0 Contents 0 Normalization', 'activations'), 'content') (('hidden layers', 'layers pooling typically convolutional layers'), 'consist') (('task', 'evolutionary network'), 'become') (('who', 'web computer very instructive vision'), 'com') (('they', 'labels'), 'be') (('it', 'model'), 'be') (('Kaggle', 'test set predictions'), 'evaluate') (('parameter careful it', 'linear non saturation'), 'slow') (('text output l end 1 f 1 Where l', 'layer number'), 'begin') (('learning rate', 'which'), 'factor') (('we', 'more same time'), 'use') (('MNIST handwritten digits', 'Kannada digits handwritten dataset'), 'practice') (('we', 'optimizer algorithm'), 'content') (('130 Why we', '133 3'), 'Padding') (('we', 'validation set'), 'Set') (('this', 'output directly dimensions'), 'vary') (('we', '54000 28 28 1 dimensions'), 'prefer') (('113 Why We', '114 2'), 'Layer') (('he', 'different colors'), 'be') (('which', '1'), 'calculate') (('don t', 'operation'), 'have') (('how much data', 'filter pattern'), 'observe') (('method', 'training mini batch'), 'consist') (('changes', '_ dataset _'), '_') (('keras 2019 07 08 imagedatagenerator', 'Rosebrock https Adrian www'), 'com') (('data augmentation', 'model'), 'gif') (('You', 'other numbers'), '0') (('we', 'test set'), 'determine') (('answer', 'overfitting'), '_') (('Convolution', 'specialized linear operation'), 'be') (('Stride Sliding amount', 'stride S'), 'declare') (('Normalized neurons', 'trained gamma'), 'have') (('n', 'input 1 horizontal vertical matrix'), 'be') (('pixel', '255'), 'have') (('io optimizing', 'overview Sebastian Ruder https very nice ruder'), 'be') (('even you', 'cat'), 'evaluation') (('how many columns', 'output matrixes'), 'kernel') (('we', 'basis'), 'be') (('Kernel', 'Kaggle competition dataset'), 'be') (('algorithms', 'global minimum'), 'use') (('Numerically high values', 'pattern compliance'), 'be') (('training three building', 'model'), 'building') (('only once information', '3 times 3x3 filter'), 'convolute') (('we', 'float type'), 'in') (('it', '_ _'), 'prepare') (('Padding Return Contents 0 Padding', 'input matrix'), 'refer') (('brain', 'final decision'), 'recognize') (('we', '1'), '_') (('Totaly we', 'training set'), 'have') "}