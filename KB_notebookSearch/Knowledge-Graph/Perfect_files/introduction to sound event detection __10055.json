{"name": "introduction to sound event detection ", "full_name": " h2 Update h3 Version note h2 About h2 Model for SED task h3 torchlibrosa h4 LICENSE h3 audioset tagging cnn h4 LICENSE h3 Building blocks h2 Train SED model with only weak supervision h3 Dataset h3 Criterion h3 Callbacks h3 Train h2 Prediction with SED model h2 Postprocess h2 EOF ", "stargazers_count": 0, "forks_count": 0, "description": "att x 10 10 dim 1. In this notebook I ll use Weakly supervised SED model provided by PANNs repository https github. In weakly supervised setting we only have clip level annotation therefore we also need to aggregate that in time axis. com ttahara training birdsong baseline resnest50 fast. The output of CNN feature extractor still contains information about frequency and time it should be 4 dimensional batch size channels frequency time so if we aggregate it only in frequency axis we can preserve time information on that feature map. com c freesound audio tagging 2019 or Freesound General Purpose Audio Tagging Challenge https www. Chunk level prediction can be treated as audio tagging task if we treat each chunk as short audio clip but we can also use SED approach. png In SED task we need to detect sound events from continuous long audio clip and provide prediction of what sound event exists from when to when. I ll use trained model of this which I trained by myself using the data of this competition in my local environment. temperature temperature self. clip_id onset offset event audio_001 0. Dataset Criterion Callbacks TrainSome code are taken from https www. In this competition what we need to provide is 5sec chunk level prediction for site_1 and site_2 data and clip level prediction for site_3 data. Therefore we need to train our SED model in weakly supervised manner. Building blocksWhat is good in PANNs models is that they accept raw audio clip as input. Hense we at first put classifier that outputs class existence probability for each time step just after the feature extractor and then aggregate the output of the classifier result in time axis. 10211 torchlibrosaIn PANNs torchlibrosa a PyTorch based implementation are used to replace some of the librosa s functions. att init_layer self. 28 alarm SED task is different from the tasks in past audio competitions in kaggle. com qiuqiangkong audioset_tagging_cnn blob master pytorch models. time batch_size 1 time_steps freq_bins batch_size 1 time_steps mel_bins Mixup on spectrogram Output shape batch size channels time frequency Aggregate in frequency axis Get framewise output list of list file_path ebird_code loaders model Fixed in V3 Optimizer Scheduler Loss callbacks Overlap deletion this part may not be necessary I deleted this part in other model and found there s no difference on the public LB score. cnn_feature_extractor method will take this as input and output feature map. Train SED model with only weak supervision weak label vs strong label https www. com c birdsong recognition discussion 172356 are expressed about over sharing of top solutions during competition and since I do respect those people who have worked hard to improve their scores I would not make trained weight in common and would not share how I trained this model. com qiuqiangkong torchlibrosa LICENSE ISC LicenseCopyright c 2013 2017 librosa development team. com audioset which is an ImageNet counterpart in audio field. activation activation self. Then we train it normally by using BCE loss with clip level prediction and clip level annotation. 75 speech audio_001 5. SED overview http d33wubrfki0l68. ppm This figure gives us an intuitive explanation what is weak annotation and what is strong annotation in terms of sound event detection. LICENSE The MIT License Copyright c 2010 2017 Google Inc. activation linear return x elif self. Model for SED taskHow can we provide prediction with onset and offset time information To do this models for SED task output segment wise prediction instead of outputting aggregated prediction for a clip which is usually used for Audio Tagging model. THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS ORIMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. sum norm_att cla dim 2 return x norm_att cla def nonlinear_transform self x if self. In PANNsCNN14Att input raw waveform will be converted into log melspectrogram using torchlibrosa s utilities. Conv1d in_channels in_features out_channels out_features kernel_size 1 stride 1 padding 0 bias True self. Now that I ve introduced the basic idea let s look into a SED model with some code. sigmoid x In the forward method it at first calculate self attention map in the first line norm_att torch. Permission to use copy modify and or distribute this software for any purpose with or without fee is hereby granted provided that the above copyright notice and this permission notice appear in all copies. In this way we can get both clip level prediction and segment level prediction if the time resolution is high it can be treated as event level prediction. Thanks ttahara Seems it s learning something. py which is a SED model. Now I ll show how this model works in the inference phase. init_weights def init_weights self init_layer self. I put this functionality in PANNsCNN14Att. bn_att def forward self x x n_samples n_in n_time norm_att torch. For this competition we only have weak annotation clip level annotation. Since several concerns https www. How can we output segment wise prediction The idea is simple. Here I use some functions of torchlibrosa. cla x calculates segment wise classification result. Let s check the output of the feature extractor. Let s check the output. Let s put a chunk into the CNN feature extractor of the model above. segment wise prediction and clip wise prediction is actually calculated in AttBlock of the model. Now let s try to train this model in weakly supervised manner. audioset_tagging_cnnI also use Cnn14_DecisionLevelAtt model from PANNs models https github. Therefore our prediction for SED task should look like this. That feature map has information about which time segment has what sound event. orgPermission is hereby granted free of charge to any person obtaining a copyof this software and associated documentation files the Software to dealin the Software without restriction including without limitation the rightsto use copy modify merge publish distribute sublicense and or sellcopies of the Software and to permit persons to whom the Software isfurnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included inall copies or substantial portions of the Software. Module def __init__ self in_features int out_features int activation linear temperature 1. Although it s downsized through several convolution and pooling layers the size of it s third dimension is 15 and it still contains time information. com qiuqiangkong audioset_tagging_cnn. net profile Anurag_Kumar10 publication 329239818 figure fig5 AS 743089203322880 1554177680169 Weakly Labeled vs Strongly Labeled Strongly labeled data contains time stamps of the. com c freesound audio tagging is Audio Tagging which we ll need to provide clip level prediction and the task in TensorFlow Speech Recognition Challenge https www. Prediction with SED model Postprocess EOF type ignore type ignore type ignore By default use the entire frame Set the default hop if it s not already specified Pad the window out to n_fft size DFT IDFT matrix n_fft 2 1 1 n_fft n_fft 2 1 1 n_fft batch_size channels_num data_length batch_size n_fft 2 1 time_steps batch_size 1 time_steps n_fft 2 1 batch_size n_fft 2 1 time_steps n_fft 2 1 mel_bins Mel spectrogram Logmel spectrogram dim 2 time dim 3 frequency x n_samples n_in n_time Downsampled ratio Spectrogram extractor Logmel feature extractor Spec augmenter t1 time. The model here is pretrained with AudioSet https research. In SED model we provide prediction for each of this. activation sigmoid return torch. __init__ self. Then in the third line attention aggregation is performed to get clip wise prediction. att x 10 10 dim 1 cla self. The task in Freesound Audio Tagging 2019 https www. BatchNorm1d out_features self. THE SOFTWARE IS PROVIDED AS IS AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. Let s check how this is implemented in the PANNs model above. nonlinear_transform self. PANNs paper https arxiv. IN NO EVENT SHALL THEAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHERLIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS INTHE SOFTWARE. Assume we use 2D CNN based model which takes log melspectrogram as input and extract features using CNN feature extractor and do classification with the feature map which is the output of CNN. In the second line cla self. AboutIn this notebook I will introduce Sound Event Detection SED task and model fit for that task and I will show how to train SED model with only weak annotation. This will be used to aggregate the classification result for segment. cla init_bn self. cla x x torch. Changed to use pretrained weight. com c tensorflow speech recognition challenge is Speech Recognition so what we need to predict is which speech command is in that audio clip which is in a sense similar to Audio Tagging task because we only need to provide clip level prediction. Each element of this dimension is segment. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL DIRECT INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE DATA OR PROFITS WHETHER IN AN ACTION OF CONTRACT NEGLIGENCE OR OTHER TORTIOUS ACTION ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. Add link to original paper. net 508a62f305652e6d9af853c65ab33ae9900ff38e 17a88 images tasks challenge2016 task3_overview. Update Version notev3 Training procedure didn t use pretrained weight. ", "id": "hidehisaarai1213/introduction-to-sound-event-detection", "size": "10055", "language": "python", "html_url": "https://www.kaggle.com/code/hidehisaarai1213/introduction-to-sound-event-detection", "git_url": "https://www.kaggle.com/code/hidehisaarai1213/introduction-to-sound-event-detection", "script": "torch.nn.functional State pad_framewise_output pathlib STFT(DFTBase) Spectrogram(nn.Module) preprocess PANNsLoss(nn.Module) IPython.display init_layer contextmanager transform_slice DFTBase(nn.Module) Optional get_logger numpy cnn_feature_extractor progress_bar power_to_db init_weights Path catalyst.dl set_seed idft_matrix SpecAugmentation(nn.Module) f1_score Callback CheckpointCallback init_weight nonlinear_transform torch.nn prediction ConvBlock(nn.Module) mAPCallback(Callback) PANNsCNN14Att(nn.Module) prediction_for_clip SupervisedRunner StratifiedKFold CallbackOrder typing LogmelFilterBank(nn.Module) get_model __getitem__ forward DropStripes(nn.Module) sklearn.model_selection pandas timer on_loader_start contextlib PANNsDataset(data.Dataset) fastprogress torch.optim average_precision_score interpolate torch.utils.data librosa.display __len__ on_batch_end on_loader_end __init__ F1Callback(Callback) Audio dft_matrix soundfile List sklearn.metrics init_bn AttBlock(nn.Module) ", "entities": "(('alarm SED 28 task', 'kaggle'), 'be') (('AUTHOR', 'MERCHANTABILITY'), 'be') (('line attention Then third aggregation', 'clip wise prediction'), 'perform') (('I', 'PANNsCNN14Att'), 'put') (('time segment', 'information'), 'have') (('method', 'output feature input map'), 'take') (('s', 'feature extractor'), 'let') (('we', 'annotation clip level only weak annotation'), 'have') (('that', 'time'), 'existence') (('which', 'CNN'), 'assume') (('we', 'clip level only prediction'), 'be') (('idea', 'segment wise prediction'), 'output') (('they', 'input'), 'be') (('need', 'site_3 data'), 'be') (('com which', 'ImageNet audio field'), 'audioset') (('DAMAGES WHATSOEVER', 'OR SOFTWARE'), 'LIABLE') (('This', 'segment'), 'use') (('Then we', 'clip level prediction'), 'train') (('I', 'only weak annotation'), 'aboutin') (('DFT IDFT batch_size data_length 2 2 2 2 mel_bins Mel spectrogram 2 1 2 1 1 batch_size 1 1 n_fft time_steps n_fft 1 Logmel', 'n_fft size'), 'prediction') (('we', 'TensorFlow Speech Recognition Challenge https www'), 'be') (('we', 'this'), 'provide') (('COPYRIGHT HOLDERS', 'DEALINGS INTHE OTHER SOFTWARE'), 'theauthor') (('which', 'Audio Tagging usually model'), 'model') (('how I', 'model'), 'express') (('PyTorch based implementation', 'functions'), 'torchlibrosa') (('we', 'SED also approach'), 'treat') (('therefore we', 'also time'), 'need') (('audioset_tagging_cnnI', 'PANNs models https github'), 'use') (('segment wise prediction', 'model'), 'calculate') (('we', 'feature map'), 'contain') (('how this', 'PANNs model'), 'let') (('Therefore we', 'weakly supervised manner'), 'need') (('it', 'something'), 'seem') (('s', 'code'), 'let') (('weak what', 'event sound detection'), 'ppm') (('Therefore prediction', 'this'), 'look') (('I', 'local environment'), 'use') (('copyright above notice', 'permission copies'), 'be') (('model', 'AudioSet https here research'), 'pretraine') (('permission notice', 'substantial Software'), 'be') (('element', 'dimension'), 'be') (('how model', 'inference phase'), 'show') (('I', 'LB public score'), 'batch_size') (('s', 'model'), 'let') (('Here I', 'torchlibrosa'), 'use') (('it', 'time still information'), 'be') (('sound event', 'what'), 'png') (('input raw waveform', 'utilities'), 'convert') (('it', 'event level prediction'), 'get') (('Update notev3 Training procedure didn Version t', 'pretrained weight'), 'use') (('Strongly labeled data', 'the'), 'figure') (('Dataset Criterion Callbacks TrainSome code', 'https www'), 'take') (('I', 'PANNs repository https github'), 'use') (('PROVIDED AS', 'PARTICULAR PURPOSE'), 'be') (('Now s', 'weakly supervised manner'), 'let') "}