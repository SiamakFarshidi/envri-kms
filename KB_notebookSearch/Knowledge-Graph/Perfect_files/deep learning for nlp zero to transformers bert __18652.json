{"name": "deep learning for nlp zero to transformers bert ", "full_name": " h1 About this Notebook h1 Contents h1 Configuring TPU s h3 Data Preparation h1 Before We Begin h1 Simple RNN h2 Basic Overview h2 In Depth Understanding h2 Code Implementation h2 Code Explanantion h1 Word Embeddings h1 LSTM s h2 Basic Overview h2 In Depth Understanding h1 Code Implementation h2 Code Explanation h1 GRU s h2 Basic Overview h2 In Depth Explanation h2 Code Implementation h1 Bi Directional RNN s h2 In Depth Explanation h2 Code Implementation h2 Code Explanation h1 Seq2Seq Model Architecture h2 Overview h2 In Depth Understanding h1 Attention Models h2 Code Implementation h1 Transformers Attention is all you need h2 Code Implementation h1 BERT and Its Implementation on this Competition h2 Tokenization h2 Starting Training h1 End Notes h3 I am attaching more resources if you want NLP end to end ", "stargazers_count": 0, "forks_count": 0, "description": "com abhishek approaching almost any nlp problem on kaggleIf you want a more basic dataset to practice with here is another kernel which I wrote https www. Also after discussing all of these ideas I will present a starter solution for this competiton This kernel has been a work of more than 10 days If you find my kernel useful and my efforts appreciable Please Upvote it it motivates me to write more Quality content Configuring TPU sFor this version of Notebook we will be using TPU s as we have to built a BERT ModelWe will drop the other columns and approach this problem as a Binary Classification Problem and also we will have our exercise done on a smaller subsection of the dataset only 12000 data points to make it easier to train the modelsWe will check the maximum number of words that can be present in a comment this will help us in padding laterWriting a function for getting auc score for validation Data Preparation Before We BeginBefore we Begin If you are a complete starter with NLP and never worked with text data I am attaching a few kernels that will serve as a starting point of your journey https www. org learn nlp sequence models lecture KXoay long short term memory lstm https distill. pub 2016 augmented rnns Code Implementation https www. Next we add an 100 LSTM units without any dropout or regularizationAt last we add a single neuron with sigmoid function which takes output from 100 LSTM cells Please note we have 100 LSTM cells not layers to predict the results and then we compile the model using adam optimizer Comments on the modelWe can see our model achieves an accuracy of 1 which is just insane we are clearly overfitting I know but this was the simplest model of all we can tune a lot of hyperparameters like RNN units we can do batch normalization dropouts etc to get better result. html Implementation from Scratch in Pytorch Transformers Attention is all you needSo finally we have reached the end of the learning curve and are about to start learning the technology that changed NLP completely and are the reasons for the state of the art NLP techniques. org learn nlp sequence models lecture BO8PS different types of rnns. org gated recurrent unit networks Code Implementation Bi Directional RNN s In Depth Explanation https www. Then while building the embedding layer we pass Embedding Matrix as weights to the layer instead of training it over Vocabulary and thus we pass trainable False. In Depth Explanation https towardsdatascience. Here is the reason why it s done https stackoverflow. Owing to its popularity Kaggle launched two NLP competitions recently and me being a lover of this Hot topic prepared myself to join in my first Kaggle Competition. I invite you all to come and learn alongside with me and take a step closer towards becoming an NLP expert ContentsIn this Notebook I will start with the very Basics of RNN s and Build all the way to latest deep learning architectures to solve NLP problems. com watch v IHZwWFHWa w list PL_h2yd2CGtBHEKwEH5iqTZH85wLS eUzv index 2 https www. using keras tokenizer here zero pad the sequences A simpleRNN without any pretrained embeddings and one dense layer Multiplying by Strategy to run on TPU s load the GloVe vectors in a dictionary create an embedding matrix for the words we have in the dataset A simple LSTM with glove embeddings and one dense layer GRU with glove embeddings and two dense layers A simple bidirectional LSTM with glove embeddings and one dense layer Visualization of Results obtained from various Deep learning models Loading Dependencies LOADING THE DATA IMP DATA FOR CONFIG Configuration First load the real tokenizer Save the loaded tokenizer locally Reload it with the huggingface tokenizers library. Default distribution strategy in Tensorflow. Then we first add the Embedding layer. word_index simply gives the dictionary of vocab that keras created for us Building the Neural NetworkTo understand the Dimensions of input and output given to RNN in keras her is a beautiful article https medium. Rest of the model is same as before except we have replaced the SimpleRNN By LSTM Units Comments on the ModelWe now see that the model is not overfitting and achieves an auc score of 0. html Code ImplementationSo first I will implement the and then I will explain the code step by step Code Explanantion Tokenization So if you have watched the videos and referred to the links you would know that in an RNN we input a sentence word by word. com how to implement seq2seq lstm model in keras shortcutnlp 6f355f3e5639 A More advanced Seq2seq Model and its explanation https d2l. org lecture natural language processing tensorflow padding 2CyzsAlso sometimes people might use special tokens while tokenizing like EOS end of string and BOS Begining of string. ai chapter_recurrent modern machine translation and dataset. If you are able to understand the intiuition and working of attention block understanding transformers and transformer based architectures like BERT will be a piece of cake. I thought I have to learn someday why not now so I braced myself and sat on the learning curve. io illustrated transformer Code Implementation http nlp. It will cover the Following Simple RNN s Word Embeddings Definition and How to get them LSTM s GRU s BI Directional RNN s Encoder Decoder Models Seq2Seq Models Attention Models Transformers Attention is all you need BERTI will divide every Topic into four subsections Basic Overview In Depth Understanding In this I will attach links of articles and videos to learn about the topic in depth Code Implementation Code ExplanationThis is a comprehensive kernel and if you follow along till the end I promise you would learn all the techniques completelyNote that the aim of this notebook is not to have a High LB score but to present a beginner guide to understand Deep Learning techniques used for NLP. What keras Tokenizer does is it takes all the unique words in the corpus forms a dictionary with words as keys and their number of occurences as values it then sorts the dictionary in descending order of counts. Thus RNN came into existence which solved this issue with the help of a Hidden Layer. In this Notebook I ll be using the GloVe vectors. Try printing first 2 elements of xtrain_seq you will see every word is represented as a digit nowNow you might be wondering What is padding Why its doneHere is the answer https www. com watch v IfsjMg4fLWQ list PLtmWHNX gukKocXQOkQjuVxglSDYWsSh9 index 8 t 0s Introduction to Seq2seq By fast. io a ten minute introduction to sequence to sequence learning in keras. As this community has been very kind to me and helped me in learning all of this I want to take this forward. com shivajbd understanding input and output shape in lstm keras c501ee95c65eThe first line model. com tanulsingh077 twitter sentiment extaction analysis eda and model https www. ai chapter_recurrent neural networks rnn. You can download the GloVe vectors from here http www nlp. About this NotebookNLP is a very hot topic right now and as belived by many experts 2020 is going to be NLP s Year with its ever changing dynamics it is experiencing a boom same as computer vision once did. 82 without much efforts and we know have learnt about RNN s. Now these architectures can be used in two ways 1 We can use the model for prediction on our problems using the pretrained weights without fine tuning or training the model for our sepcific tasks EG http jalammar. Without going into too much details I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it and since I am a fan of GloVe vectors word2vec and fasttext. co transformers main_classes tokenizer. edu 2018 04 03 attention. com xhlulu jigsaw tpu distilbert with huggingface and kerasSteps Involved Data Preparation Tokenization and encoding of data Configuring TPU s Building a Function for Model Training and adding an output layer for classification Train the model and get the resultsEncoder FOr DATA for understanding waht encode batch does read documentation of hugging face tokenizer https huggingface. com attention and its different forms 7fc3674d14dc https distill. com mindorks understanding the recurrent neural network 44d593f112a2 https www. We represent every word as one hot vectors of dimensions Numbers of words in Vocab 1. org learn nlp sequence models lecture HyEui basic models A basic idea of different Seq2Seq Models https blog. ai chapter_recurrent modern encoder decoder. io a visual guide to using bert for the first time Using Pre trained BERT without Tuning2 We can fine tune or train these transformer models for our task by tweaking the already pre trained weights and training on a much smaller dataset EG https www. In traditional neural networks all the inputs and outputs are independent of each other but in cases like when it is required to predict the next word of a sentence the previous words are required and hence there is a need to remember the previous words. GRU s are a variation on the LSTM because both are designed similarly and in some cases produce equally excellent results. It took me 10 days to learn all of this you can learn it at your pace and dont give in at the end of all this you will be a different person and it will all be worth it. org learn nlp sequence models lecture PKMRR vanishing gradients with rnns https www. com watch v 2E65LDnM2cA list PL1F3ABbhcqa3BBWo170U4Ev2wfsF7FN8l https www. I recommend Finishing Part 1 before continuing as the upcoming techniques can be quite overwhelming Seq2Seq Model Architecture OverviewRNN s are of many types and different architectures are used for different purposes. com watch v aircAruvnKk list PL_h2yd2CGtBHEKwEH5iqTZH85wLS eUzv https www. zip or you can search for GloVe in datasets on Kaggle and add the file LSTM s Basic OverviewSimple RNN s were certainly better than classical ML algorithms and gave state of the art results but it failed to capture long term dependencies that is present in sentences. I am attaching more resources if you want NLP end to end 1 Books https d2l. pub 2019 memorization in rnns https towardsdatascience. We have achieve similar accuracy and auc score as before and now we have learned all the types of typical RNN architectures We are now at the end of part 1 of this notebook and things are about to go wild now as we Enter more complex and State of the art models. Here is a nice video explanining different types of model architectures https www. com tanulsingh077 twitter sentiment extaction analysis eda and model After 10 days of extensive learning finishing all the latest NLP approaches I am back here to share my leaning by writing a kernel that starts from the very Basic RNN s to built over all the way to BERT. io This is subtle effort of contributing towards the community if it helped you in any way please show a token of love by upvoting linear algebra data processing CSV file I O e. This architecture is used in a lot of applications like Machine Translation text summarization question answering etc In Depth UnderstandingI will not write the code implementation for this but rather I will provide the resources where code has already been implemented and explained in a much better way than I could have ever explained. Seq2Seq is a many to many RNN architecture where the input is a sequence and the output is also a sequence where input and output sequences can be or cannot be of different lengths. com what are word embeddings The latest approach to getting word Embeddings is using pretained GLoVe or using Fasttext. The point is we got an AUC score of 0. io illustrated bert In Depth Understanding of BERTAfter going through the post Above I guess you must have understood how transformer architecture have been utilized by the current SOTA models. html Implementation of Encoder Decoder Model from scratch https www. com define encoder decoder sequence sequence model neural machine translation keras Basic Encoder Decoder Model and its explanation respectively https towardsdatascience. Join me and make these NLP competitions your first without being overwhelmed by the shear number of techniques used. com understanding gru networks 2ef37df6c9be https www. html here TokenizationFor understanding please refer to hugging face documentation again Starting TrainingIf you want to use any another model just replace the model name in transformers. com illustrated guide to lstms and gru s a step by step explanation 44e9eb85bf21 Code ImplementationWe have already tokenized and paded our text for input to LSTM s Code ExplanationAs a first step we calculate embedding matrix for our vocabulary from the pretrained GLoVe vectors. com blog 2017 12 fundamentals of deep learning introduction to lstm What are LSTM s https www. org learn nlp sequence models lecture 6Oq70 word representation https machinelearningmastery. pub http jalammar. com blog 2019 11 comprehensive guide attention mechanism deep learning Basic Level https pytorch. in 2014 GRU Gated Recurrent Unit aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. Works on CPU and single GPU. ai Attention ModelsThis is the toughest and most tricky part. I have shared all the resources I used to learn all the stuff. read_csv Detect hardware return appropriate distribution strategy TPU detection. GRU s were designed to be simpler and faster than LSTM s and in most cases produce equally good results and thus there is no clear winner. I wrote a kernel on the Tweet Sentiment Extraction competition that has now got a gold medal it can be viewed here https www. It then assigns the first value 1 second value 2 and so on. org learn nlp sequence models lecture agZiL gated recurrent unit gru https www. com sequence 2 sequence model with attention mechanism 9e9ca2a613a https towardsdatascience. We could have used word2vec but the embeddings layer learns during training to enhance the embeddings. 96 which is quite commendable also we close in on the gap between accuracy and auc. If you have understood the Attention models this will be very easy Here is transformers fully explained http jalammar. Sequential tells keras that we will be building our network sequentially. Please read and view the following resources in the order I am providing to ignore getting confused also at the end of this try to write and draw an attention block in your own way https www. com data preparation variable length input sequences sequence prediction https www. If you have followed along from the starting and read all the articles and understood everything these complex models would be fairly easy to understand. No parameters necessary if TPU_NAME environment variable is set this is always the case on Kaggle. ai Jason Brownlee s Books2 Courses https www. Embedding layer is also a layer of neurons which takes in as input the nth dimensional one hot vector of every word and converts it into 300 dimensional vector it gives us word embeddings similar to word2vec. com understanding bidirectional rnn in pytorch 5bd25a5dd66 https d2l. org learn nlp sequence models lecture RDXpX attention model intuition Only watch this video and not the next one https towardsdatascience. com Why do we use an RNN instead of a simple neural network In Depth Understanding https medium. org learn nlp sequence models home welcome Fast. com watch v tIeHLnjs5U8 list PL_h2yd2CGtBHEKwEH5iqTZH85wLS eUzv index 4For Learning how to visualize test data and what to use view https www. We see that in this case we used dropout and prevented overfitting the data GRU s Basic OverviewIntroduced by Cho et al. In Depth UnderstandingWhy LSTM s https www. As I joined the competitions and since I was a complete beginner with Deep Learning Techniques for NLP all my enthusiasm took a beating when I saw everyone Using all kinds of BERT everything just went over my head I thought to quit but there is a special thing about Kaggle it just hooks you. html This presents the code implementation of the architecture presented in the paper by Google BERT and Its Implementation on this CompetitionAs Promised I am back with Resiurces to understand about BERT architecture please follow the contents in the given order http jalammar. com tanulsingh077 what s cookingBelow are some Resources to get started with basic level Neural Networks It will help us to easily understand the upcoming parts https www. This is the part where I spent the most time on and I suggest you do the same. So in 1998 99 LSTM s were introduced to counter to these drawbacks. com watch v hinZO TEk4 t 2933s Tuning BERT For your TASKWe will be using the first example as a base for our implementation of BERT model using Hugging Face and KERAS but contrary to first example we will also Fine Tune our model for our taskAcknowledgements https www. com questions 44579161 why do we do padding in nlp tasksThe code token. So let s suppose word the occured the most in the corpus then it will assigned index 1 and vector representing the would be a one hot vector with value 1 at position 1 and rest zereos. ai chapter_recurrent modern bi rnn. org learn nlp sequence models lecture fyXnn bidirectional rnn https towardsdatascience. org tutorials intermediate seq2seq_translation_tutorial. com Which effect does sequence padding have on the training of a neural network https machinelearningmastery. Why RNN s https www. ai NLP Course3 Blogs and websites Machine Learning Mastery https distill. com jagangupta stop the s toxic comments eda Simple RNN Basic OverviewWhat is a RNN Recurrent Neural Network RNN are a type of Neural Network where the output from previous step are fed as input to the current step. com arthurtok spooky nlp and topic modelling tutorial https www. com watch v Ilg3gGewQ5U list PL_h2yd2CGtBHEKwEH5iqTZH85wLS eUzv index 3 https www. _____ and use accordingly End NotesThis was my effort to share my learnings so that everyone can benifit from it. Transformers were introduced in the paper Attention is all you need by Google. html https machinelearningmastery. Deep learning is really revolutionary Word EmbeddingsWhile building our simple RNN models we talked about using word embeddings So what is word embeddings and how do we get word embeddings Here is the answer https www. html Code Implementation Code ExplanationCode is same as before only we have added bidirectional nature to the LSTM cells we used before and is self explanatory. ", "id": "tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert", "size": "18652", "language": "python", "html_url": "https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert", "git_url": "https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert", "script": "keras.layers Activation keras.models train_test_split SimpleRNN plotly.express tensorflow.keras.layers kaggle_datasets numpy tensorflow.keras.models seaborn Adam Dropout SpatialDropout1D preprocessing BatchNormalization keras.layers.embeddings Dense plotly Bidirectional build_model keras.layers.normalization keras.callbacks tokenizers GlobalMaxPooling1D text tqdm Embedding fast_encode LSTM sklearn KaggleDatasets tensorflow matplotlib.pyplot sequence tensorflow.keras.callbacks ModelCheckpoint pipeline Sequential metrics sklearn.model_selection pandas np_utils EarlyStopping keras.preprocessing Model graph_objs as go graph_objs model_selection plotly.figure_factory Input keras.layers.recurrent keras.utils keras.layers.core Conv1D roc_auc BertWordPieceTokenizer MaxPooling1D tensorflow.keras.optimizers Flatten decomposition GRU ", "entities": "(('I', 'already much better way'), 'use') (('both', 'equally excellent results'), 'be') (('we', 'network'), 'tell') (('that', 'way'), 'tanulsingh077') (('answer https Here www', 'word embeddings'), 'be') (('her', 'keras'), 'give') (('CompetitionAs I', 'jalammar'), 'html') (('LSTM 99 s', 'drawbacks'), 'introduce') (('Embeddings', 'Fasttext'), 'com') (('we', 'LSTM cells'), 'be') (('I', 'stuff'), 'share') (('We', 'dataset EG https much smaller www'), 'io') (('it', 'similar word2vec'), 'be') (('we', 'word'), 'implement') (('embeddings layer', 'embeddings'), 'use') (('What', 'com learning 2017 12 deep introduction'), 'blog') (('TPU_NAME environment necessary variable', 'always Kaggle'), 'be') (('nlp sequence models', 'unit gru https agZiL gated recurrent www'), 'learn') (('where output', 'current step'), 'stop') (('read_csv Detect hardware', 'distribution strategy TPU appropriate detection'), 'return') (('now model', '0'), 'be') (('org', 'nlp sequence models'), 'learn') (('are', 'different different purposes'), 'recommend') (('why it', 'https stackoverflow'), 'be') (('previous words', 'hence previous words'), 'be') (('it', 'counts'), 'be') (('why we', 'nlp'), 'com') (('sometimes people', 'BOS string'), 'padding') (('now we', 'art models'), 'achieve') (('someday why now so I', 'learning curve'), 'think') (('transformer how architecture', 'SOTA current models'), 'illustrate') (('you', 'same'), 'be') (('then it', '1 position'), 'let') (('complex models', 'everything'), 'follow') (('quite also we', 'accuracy'), 'close') (('NLP step closer I', 'NLP problems'), 'invite') (('Loading Dependencies', 'huggingface tokenizers library'), 'use') (('You', 'www nlp'), 'download') (('everyone', 'it'), '_') (('we', 'better result'), 'add') (('tasks sepcific EG', 'jalammar'), 'use') (('com watch', 'eUzv https www'), 'PL_h2yd2CGtBHEKwEH5iqTZH85wLS') (('also where input sequences', 'different lengths'), 'be') (('Why doneHere', 'digit nowNow'), 'try') (('NLP', 'techniques'), 'join') (('org', 'nlp sequence models Seq2Seq Models https HyEui basic basic different blog'), 'learn') (('that', 'journey https www'), 'present') (('we', 'taskAcknowledgements https www'), 'watch') (('we', 'pretrained GLoVe vectors'), 's') (('com watch Ilg3gGewQ5U v list', 'https eUzv index 3 www'), 'PL_h2yd2CGtBHEKwEH5iqTZH85wLS') (('We', 'Vocab'), 'represent') (('PLtmWHNX', 't 0s 8 Seq2seq'), 'gukKocXQOkQjuVxglSDYWsSh9') (('it', 'just you'), 'join') (('which', 'neural standard recurrent network'), 'aim') (('sequence padding', 'network https neural machinelearningmastery'), 'com') (('that', 'sentences'), 'search') (('you', 'transformers'), 'html') (('that', 'art NLP techniques'), 'be') (('nlp sequence models lecture PKMRR', 'rnns https www'), 'learn') (('different it', 'this'), 'take') (('attention model intuition', 'Only video'), 'learn') (('recently me', 'Kaggle first Competition'), 'launch') (('I', 'GloVe vectors'), 'explain') (('It', 'then first value'), 'assign') (('you', 'Google'), 'introduce') (('nlp sequence models', 'term memory lstm https KXoay long short distill'), 'learn') (('you', 'cake'), 'be') (('Here nice video', 'model architectures https www'), 'be') (('we', 'Cho et al'), 'see') (('I', 'GloVe vectors'), 'use') (('I', 'https www'), 'approach') (('org', 'nlp sequence models rnn https bidirectional towardsdatascience'), 'learn') (('computer same vision', 'boom'), 'be') (('which', 'Hidden Layer'), 'come') (('Building', 'hugging face tokenizer https huggingface'), 'tpu') (('I', 'own way'), 'read') (('nlp sequence models', 'rnns'), 'learn') (('we', 'RNN s.'), '82') (('NLP', 'Books https 1 d2l'), 'attach') (('nlp sequence models', 'word representation https 6Oq70 machinelearningmastery'), 'learn') (('aim', 'NLP'), 'cover') (('thus we', 'trainable False'), 'pass') (('GRU s', 'equally good results'), 'design') (('very Here transformers', 'fully http jalammar'), 'understand') (('It', 'parts https easily upcoming www'), 'com') (('it', 'CSV file'), 'be') (('com watch', 'view https www'), 'PL_h2yd2CGtBHEKwEH5iqTZH85wLS') (('it', 'gold now medal'), 'write') (('Why we', 'Depth Understanding https instead simple neural medium'), 'com') (('io', 'transformer Code Implementation nlp'), 'illustrate') (('com define encoder', 'sequence sequence model machine translation keras Basic Encoder Decoder neural Model'), 'decoder') (('I', 'this'), 'be') "}