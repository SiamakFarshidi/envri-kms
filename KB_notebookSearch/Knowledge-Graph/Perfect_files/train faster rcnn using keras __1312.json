{"name": "train faster rcnn using keras ", "full_name": " h1 config h4 Explore data gen train ", "stargazers_count": 0, "forks_count": 0, "description": "com kishor1210 eda and data processing prerequisite We need to create annotation. format mean_overlapping_bboxes epoch_length Generate X x_img and label Y y_rpn_cls y_rpn_regr Train rpn model and get loss value _ loss_rpn_cls loss_rpn_regr Get predicted rpn from rpn model rpn_cls rpn_regr R bboxes shape 300 4 Convert rpn layer to roi bboxes note calc_iou converts from x1 y1 x2 y2 to x y w h format X2 bboxes that iou C. Code Credit https github. shape 18 25 Y. classes_count usask_1 5807 arvalis_1 45716 inrae_1 3701 ethz_1 51489 arvalis_3 16665 rres_1 9635 bg 0 class_mapping usask_1 0 arvalis_1 1 inrae_1 2 ethz_1 3 arvalis_3 4 rres_1 5 bg 6 Save the configuration Shuffle the images with seed Get train data generator which generate X Y image_data cv2. Don t need rpn probs in the later process Training data annotation file Number of RoIs to process at once. arange 0 r_epochs record_df loss_class_regr c plt. com kentaroy47 frcnn from scratch with keras Thank you kentaroy for you nicely documented github repo. Where all the required data preprocessing I have done in Part 1 EDA and Data Processing Kernal https www. shape 18 25 Calculate anchor position and size for each feature map point Top left x coordinate Top left y coordinate width of current anchor height of current anchor Apply regression to x y w and h if there is rpn regression layer Avoid width and height exceeding 1 Convert x y w h to x1 y1 x2 y2 x1 y1 is top left coordinate x2 y2 is bottom right coordinate Avoid bboxes drawn outside the feature map shape 4050 4 shape 4050 Find out the bboxes which is illegal and delete them from bboxes list Apply non_max_suppression Only extract the bboxes. 7 color 1 Add text Draw positive anchors according to the y_rpn_regr cv2. Augmentation flag Augment with horizontal flips in training. arange 0 r_epochs record_df curr_loss r plt. 1 vgg16_weights_tf_dim_ordering_tf_kernels. classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes Y1 one hot code for bboxes from above x_roi X Y2 corresponding labels and corresponding gt bboxes If X2 is None means there are no matching bboxes Find out the positive anchors and negative anchors If number of positive anchors is larger than 4 2 2 randomly choose 2 pos samples Randomly choose num_rois num_pos neg samples Save all the pos and neg samples in sel_samples in the extreme case where num_rois 1 we pick a random pos or neg sample training_data X X2 sel_samples labels Y1 sel_samples Y2 sel_samples X img_data resized image X2 sel_samples num_rois 4 in here bboxes which contains selected neg and pos Y1 sel_samples one hot encode for num_rois bboxes which contains selected neg and pos Y2 sel_samples labels and gt bboxes for num_rois bboxes which contains selected neg and pos plt. com kishor1210 train faster rcnn using kerasPart 3 comming soon. figure figsize 15 5 plt. Print the process or not Name of base network Setting for data augmentation Anchor box scales Note that if im_size is smaller anchor_box_scales should be scaled Original anchor_box_scales in the paper is 128 256 512 Anchor box ratios Size to resize the smallest side of the image Original setting in paper is 600. arange 0 r_epochs record_df loss_class_cls r plt. title total_loss plt. Here I am going to train Faster RCNN with 90 of images datasets. com kishor1210 eda and data processing configParser the data from annotation fileDefine ROI Pooling Convolutional LayerVgg 16 modelRPN layerClassifier layerCalculate IoU Intersection of Union Calculate the rpn for all anchors of all imagesGet new image size and augment the imageGenerate the ground_truth anchorsDefine loss functions for all four outputs Explore data_gen_train data_gen_train is an generator so we get the data by calling next data_gen_train Build the ModelPart 1 https www. putText img gt bbox gt_x1 gt_y1 5 cv2. subplot 1 2 2 plt. title loss plt. 5 ya downscale iy 0. arange 0 r_epochs record_df loss_rpn_cls b plt. com fchollet deep learning models releases tag v0. shape 4 feature_map. width num_anchors Might be 4 18 25 18 if resized image is 400 width and 300 A is the coordinates for 9 anchors for every point in the feature map all 18x25x9 4050 anchors cooridnates anchor_x 128 1 16 8 width of current anchor anchor_y 128 2 16 16 height of current anchor curr_layer 0 8 9 anchors the Kth anchor of all position in the feature map 9th in total shape 18 25 4 shape 4 18 25 Create 18x25 mesh grid For every point in x there are all the y points and vice versa X. arange 0 r_epochs record_df elapsed_time r plt. We need to download pretrained weight Pretrained Weight https github. putText img pos anchor bbox str i 1 center 0 int anc_w 2 center 1 int anc_h 2 5 cv2. csv file to record losses acc and mAP If this is a continued training load the trained model from before Load the records Training setting Just of sharing the karnel running with 2 epoch you try with min 20 epochs print Average number of overlapping bounding boxes from RPN for previous iterations. 5 color 1 define the base network VGG here can be Resnet50 Inception etc define the RPN built on the base layers 9 this is a model that holds both the RPN and the classifier used to load save weights for the models Because the google colab can only run the session several hours one time then you need to connect again we need to save the model and load the model to continue training If this is the begin of the training load the pre traind base network such as vgg 16 Create the record. h5Part 1 https www. com kishor1210 eda and data processingPart 2 https www. txt which you can find in part 1 kernal. classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes one hot code for bboxes from above x_roi X corresponding labels and corresponding gt bboxes 3 in here 3 in here A. Augment with 90 degree rotations in training. 3 and RGB x is the difference between true value and predicted vaue absolute value of x If x_abs C. shape 1 num_rois channels pool_size pool_size num_rois 4 7x7 roi pooling Flatten the convlutional layer and connected to 2 FC and 2 dropout There are two output layer out_class softmax acivation function for classify the class name of the object out_regr linear activation function for bboxes coordinates regression note no regression target for bg class a and b should be x1 y1 x2 y2 128 256 512 1 1 1 2 sqrt 2 2 sqrt 2 1 3x3 9 calculate the output map size based on the network architecture 3 initialise empty output objectives get the GT box coordinates and resize to account for image resizing get the GT box coordinates and resize to account for image resizing rpn ground truth x coordinates of the current anchor box t ignore boxes that go across image boundaries t t t t t y coordinates of the current anchor box ignore boxes that go across image boundaries bbox_type indicates whether an anchor should be a target Initialize with negative this is the best IOU for the x y coord and the current anchor note that this is different from the best IOU for a GT bbox get IOU of the current GT box and the current anchor box calculate the regression targets if they will be needed x y are the center point of ground truth bbox xa ya are the center point of anchor bbox xa downscale ix 0. Set to 300 in here to save training time image channel wise mean to subtract number of ROIs at once stride at the RPN this depends on the network configuration scaling the stdev overlaps for RPN overlaps for classifier ROIs placeholder for the class mapping automatically generated by the parser Print process Make sure the info saved in annotation file matching the format path_filename x1 y1 x2 y2 class_name Note tOne path_filename might has several classes class_name tx1 y1 x2 y2 are the pixel value of the origial image not the ratio value t x1 y1 top left coordinates x2 y2 bottom right coordinates x1 y1 t t t t t t t t t t t t t t t t t t t t t t t t t t t t t x2 y2 if np. Augment with vertical flips in training. subplot 1 2 1 plt. FONT_HERSHEY_DUPLEX 0. arange 0 r_epochs record_df loss_rpn_regr g plt. title elapsed_time plt. Record data used to save the losses classification accuracy and mean average precision Create the config This step will spend some time to load the data e. 5 w h are the width and height of ground truth bbox wa ha are the width and height of anchor bboxe tx x xa wa ty y ya ha tw log w wa th log h ha all GT boxes should be mapped to an anchor box so we keep track of which anchor box was best we set the anchor to positive if the IOU is 0. 7 it does not matter if there was another better box it just indicates overlap we update the regression layer target if this IOU is the best for the current x y and anchor position if the IOU is 0. randint 0 6 0 tall_imgs filename imageset trainval else tall_imgs filename imageset test make sure the bg class is last in the list x 0 is image with shape rows cols channels x 1 is roi with shape num_rois 4 with ordering x y w h Resized roi of the image to pooling size 7x7 Reshape to 1 num_rois pool_size pool_size nb_channels Might be 1 4 7 7 3 permute_dimensions is similar to transpose Block 1 Block 2 Block 3 Block 4 Block 5 x MaxPooling2D 2 2 strides 2 2 name block5_pool x out_roi_pool. ", "id": "kishor1210/train-faster-rcnn-using-keras", "size": "1312", "language": "python", "html_url": "https://www.kaggle.com/code/kishor1210/train-faster-rcnn-using-keras", "git_url": "https://www.kaggle.com/code/kishor1210/train-faster-rcnn-using-keras", "script": "keras.layers pyplot non_max_suppression_fast calc_iou TimeDistributed SGD rpn_loss_cls_fixed_num get_file regularizers categorical_crossentropy layer_utils apply_regr Layer * K.mean(categorical_crossentropy(y_true[0 rpn_layer InputSpec Model X2[ __future__ apply_regr_np classifier_layer class_loss_regr rpn_loss_regr_fixed_num rpn_loss_regr * K.sum(y_true[ keras.models keras augment build initializers MaxPooling2D Adam Dropout nn_base GlobalAveragePooling2D rpn_loss_cls RMSprop rpn_to_roi tensorflow division pandas print_function call average_precision_score = model_classifier.train_on_batch([X matplotlib keras.engine OptionParser intersection compute_output_shape union numpy keras.objectives get_output_length keras.engine.topology Config get_img_output_length keras.optimizers GlobalMaxPooling2D get_source_inputs generic_utils class_loss_regr_fixed_num Conv2D get_config sklearn.metrics Flatten get_anchor_gt keras.utils.data_utils pyplot as plt class_loss_cls iou RoiPoolingConv(Layer) Dense optparse backend as K get_new_img_size backend Input keras.utils get_data __init__ calc_rpn absolute_import ", "entities": "(('we', 'https ModelPart 1 www'), 'com') (('learning com fchollet deep models', 'tag v0'), 'release') (('ratio t x1 y1 top', 'x2 y2 x1 y1 t t t t t t t t t t t t t t t t t t t t t t t t t t t t t x2 left bottom right y2'), 'Set') (('step', 'data e.'), 'use') (('We', 'Weight https pretrained weight Pretrained github'), 'need') (('base pre network', '16 record'), 'be') (('com kentaroy47', 'github nicely documented repo'), 'frcnn') (('Where required I', 'Processing Kernal https Part 1 EDA www'), 'datum') (('that', 'C.'), 'Generate') (('which', 'Only bboxes'), 'shape') (('which', 'selected neg'), 'y1') (('Original setting', 'paper'), 'print') (('1 16 8 width', 'x'), 'be') (('y', 'anchor bbox center xa'), 'channel') (('We', 'annotation'), 'com') (('which', 'X Y image_data cv2'), 'usask_1') (('you', 'previous iterations'), 'file') (('1 4 3 permute_dimensions', '2 2 1 Block 2 3 Block 4 Block 5 MaxPooling2D 2 2 name'), 'imageset') (('IOU', 'anchor'), 'be') (('IOU', 'current x y position'), '7') (('3', 'x'), 'be') (('90', 'datasets'), 'go') (('Training data annotation file Number', 'RoIs'), 'need') (('com kishor1210', 'faster kerasPart'), 'train') "}