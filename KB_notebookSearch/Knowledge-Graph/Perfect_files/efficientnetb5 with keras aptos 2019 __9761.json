{"name": "efficientnetb5 with keras aptos 2019 ", "full_name": " h1 Implementation of EfficientNetB5 for the APTOS 2019 competition with Keras h2 Table Of Contents h2 Dependencies h2 Preparation h2 Metric Quadratic Weighted Kappa h2 EDA Exploratory Data Analysis h2 Preprocessing h2 Modeling EfficientNetB5 h2 Evaluation h2 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "It is probably a good idea to use data augmentation to increase robustness of our model See the modeling section. I highly recommend you to read the EfficientNet paper https arxiv. Through examining these three aspects we can get a good sense of the problem. png from ids Check submission Label distribution Distribution of predictions Check kernels run time. com qubvel efficientnet for sharing an amazing wrapper to get the EfficientNet architecture in one line of code Preparation By examining the data we can readily see that we do not have that much data 700 samples per class. EfficientNet was released this June 2019 by Google AI and is the new state of the art on ImageNet. Just loop through the layers and replace each Batch Normalization layer with a Group Normalization layer. Additional data can be found in this Kaggle dataset https www. com cimages multimages 16 4e7cf150 0179 4d89 86f2 5cbb1f51c266_meansquarederrorformula. Big thanks to Somshubra Majumdar https github. html over rounding thresholds instead of doing normal rounding. We are however not bound by this and can use a smaller size if we want. We will rotate the data on any angle. We specify the input shape and that we want the model without the top the final Dense layer. We can optimize the validation score by doing a Grid Search https scikit learn. This is the traditional transfer learning approach were we can optimize and model for almost any image content. com qubvel efficientnet Standard dependencies Machine Learning Path specifications Specify title of our final model Set seed for reproducability For keeping time. We also stop training if the MSE on the validation set doesn t go down for 4 epochs. org stable modules generated sklearn. com c aptos2019 blindness detection overview evaluation The formula for weighted kappa is https wikimedia. Do they makes sense given the label distribution of the training data That s it Feel free to experiment with this kernel and try a larger implementation of EfficientNet. The original image size from the EfficientNet paper https arxiv. After preprocessing we have managed to enhance the distinctive features in the images. It introduces a systematic way to scale CNN Convolutional Neural Networks in a nearly optimal way. Note that the distribution on the test set can still vary wildly from the training data. EDA Exploratory Data Analysis For EDA on image datasets I think one should at least examine the label distribution the images before preprocessing and the images after preprocessing. We will visualize a random image from every label to get a general sense of the distinctive features that seperate the classes. After that we calculate the Quadratic Weighted Kappa score on the training set and the validation set. For this kernel we will use the B5 version but feel free to play with the larger models. com aroraaman quadratic kappa metric explained in 5 simple steps. com abhishek optimizer for quadratic weighted kappa. Source EfficientNet Paper https arxiv. This is the mean of squared differences between our predictions and labels as showed in the formula below. The original image sizes used for every version of EfficientNet are EfficientNetB0 224 224 3 EfficientNetB1 240 240 3 EfficientNetB2 260 260 3 EfficientNetB3 300 300 3 EfficientNetB4 380 380 3 EfficientNetB5 456 456 3 EfficientNetB6 528 528 3 EfficientNetB7 600 600 3 Metric Quadratic Weighted Kappa The metric that is used for this competition is Quadratic Weighted Kappa QWK Kaggle s Explanation https www. After each epoch we save the model if it is better than the previous one according to the Quadratic Weighted Kappa score on the validation set. If you like this Kaggle kernel feel free to give an upvote and leave a comment I will try to implement your suggestions in this kernel Repository source https github. com abhishek is a great way to do this. com benjaminwarner resized 2015 2019 blindness detection images 35000 additional images. After creating the submission I always check the format and the distribution of the test predictions. The same thresholds should be used when creating the final predictions. com qubvel efficientnet we can load in a model like the Keras API. We train all layers in the network. This will increase performance when we train our EfficientNet model. We can see that EfficientNet achieves state of the art and uses a lot less parameters than most modern CNN architectures. For these images there some to be increasingly more spots and stains on the retina as diabetic retinopathy worsens. An implementation of a QWK loss function for Tensorflow Keras can be found in this Kaggle kernel https www. Modeling EfficientNetB5 Since we want to optimize the Quadratic Weighted Kappa score we can formulate this challenge as a regression problem. com c aptos2019 blindness detection competition. pdf as it signifies a fundamental shift in how the Deep Learning community will approach model scaling Also check out this video on EfficientNet by Henry AI Labs https youtu. As mentioned earlier we use custom thresholds to optimize our score. Preprocessing Here we will use the auto cropping method with Ben s preprocessing as explained in this kernel https www. By optimizing this metric we are also optimizing for Quadratic Weighted Kappa if we round the predictions afterwards. Batch Normalization becomes unstable with small batch sizes 16 and that is why we use Group Normalization https arxiv. reshape inputs to new group shape prepare broadcast shape In this case we must explicitly broadcast all parameters. Also we will flip the data both horizontally and vertically. The metric that we try to optimize is the Mean Squared Error https en. com ratthachat aptos updatedv14 preprocessing ben s cropping. com ratthachat efficientnet keras weights b0b5. py Apply constraints. org and not on medical images there are some limitations to this approach for out challenge. Evaluation To evaluate our performance we predict values from the generator and round them of to the nearest integer to get valid predictions. This kernel provides weights for EfficientNetB0 through B5. In this way we are more flexible in our optimization and we can yield higher scores than solely optimizing for accuracy. Although I do not implement this in the kernel feel free to experiment with adding data. org wiki Mean_squared_error. Lastly we will divide the data by 128 for normalization. Load in EfficientNetB5 Replace all Batch Normalization layers by Group Normalization layers Initialize model For tracking Quadratic Weighted Kappa score Monitor MSE to avoid overfitting and save best model Begin training Visualize mse Load best weights according to MSE Calculate QWK on train set Calculate score Calculate QWK on validation set Calculate score Optimize on validation data and evaluate again Place holder for diagnosis column For preprocessing test images Make final predictions round predictions and save to csv Remove. be 3svIm5UC94I for a clear explanation If you like this Kaggle kernel feel free to give an upvote and leave a comment Image an overview of model architectures and their performance on ImageNet http www. pdf for EfficientNetB5 is 456x456x3. png Since we are not provided with that much data 3662 images we will augment the data to make the model more robust. Thanks to the amazing wrapper by qubvel https github. com tensorflow tpu master models official efficientnet g3doc params. Prepare broadcasting shape. Note that we will use the RAdam optimizer https arxiv. pdf since it often yields better convergence than Vanilla Adam. We will optimize a pre trained EfficientNetB5 with a few added layers. We will take this into account and try to enhance these features in our preprocessing. The OptimizedRounder class by Abhishek Thakur https www. For a more detailed and practical explanation of QWK I highly recommend this Kaggle kernel https www. com CyberZHG keras radam blob master keras_radam optimizers. com tensorflow tpu tree master models official efficientnet. Additionally we are going to same the model which achieves the best QWK score on the validation data through a custom Keras Callback. Thanks to CyberZHG who implemented RAdam for Keras https github. Another option we could use is to directly use Quadratic Weighted Kappa as a loss function. com titu1994 for building an implementation of Group Normalization for Keras. Submission Since the test set is not that large we will not be using a generator for making the final predictions on the test set. GPU limit for this competition is set to 9 hours. Then we load in the weights which are provided in this Kaggle dataset https www. org api rest_v1 media math render svg 2a496e1cef7d812b83bdbb725d291748cf0183f5 In this case we are going to optimize Mean Squared Error MSE See Modeling section since we are using regression and by optimizing MSE we are also optimizing QWK as long as we round predictions afterwards. com christofhenkel weighted kappa loss for keras tensorflow. File sizes and specifications Add extension to id_code Add extension to id_code Specify image size Flatten list of numpy arrays Get predictions and convert to integers We can use sklearns implementation of QWK straight out of the box as long as we specify weights as quadratic Label distribution Example from every label If for some reason we only have two channels If we have a normal RGB images image is too dark so that we crop out everything return original image Example of preprocessed images from every label Labels for training data We use a small batch size so we can handle large images easily Add Image augmentation to our generator Use the dataframe to define train and validation generators Code Source https github. Keras makes it incredibly easy to replace layers. Feel free to experiment with this. Implementation of EfficientNetB5 for the APTOS 2019 competition with KerasIn this kernel we will implement EfficientNet for medical images APTOS 2019 https www. The original class can be found in this Kaggle kernel https www. Since the pre trained model was trained on ImageNet http www. Weights for EfficientNetB6 and B7 can be found in Google AI s repository for EfficientNet https github. This way we can counter overfitting. png Table Of Contents Dependencies 1 Preparation 2 Metric Quadratic Weighted Kappa 3 EDA Exploratory Data Analysis 4 Preprocessing 5 Modeling EfficientNetB5 6 Evaluation 7 Submission 8 Dependencies Special thanks to qubvel https github. We could also try to use additional data from previous competitions to increase performance. ", "id": "carlolepelaars/efficientnetb5-with-keras-aptos-2019", "size": "9761", "language": "python", "html_url": "https://www.kaggle.com/code/carlolepelaars/efficientnetb5-with-keras-aptos-2019", "git_url": "https://www.kaggle.com/code/carlolepelaars/efficientnetb5-with-keras-aptos-2019", "script": "keras.engine coefficients OptimizedRounder(object) cohen_kappa_score get_preds_and_labels keras.layers keras.models RAdam(keras.optimizers.Optimizer) predict keras build compute_output_shape preprocess_image functools initializers Image numpy elu Adam Dropout keras.preprocessing.image GroupNormalization(Layer) ImageDataGenerator efficientnet Dense Callback regularizers on_epoch_end backend as K GlobalAveragePooling2D build_model _kappa_loss keras.callbacks on_train_begin tqdm get_updates scipy keras.optimizers Layer tensorflow ReduceLROnPlateau random matplotlib.pyplot EfficientNetB5 Sequential backend PIL pandas EarlyStopping Conv2D InputSpec call partial get_custom_objects Metrics(Callback) fit constraints crop_image_from_gray keras.utils.generic_utils __init__ keras.activations get_config sklearn.metrics Flatten ", "entities": "(('we', 'regression problem'), 'model') (('com qubvel we', 'Keras API'), 'efficientnet') (('we', 'generators Code Source https train github'), 'add') (('EfficientNet', 'ImageNet'), 'release') (('png', 'time'), 'run') (('which', 'Kaggle dataset https www'), 'load') (('readily we', 'class'), 'efficientnet') (('which', 'Keras custom Callback'), 'go') (('com cimages', '16'), 'multimage') (('doesn', '4 epochs'), 'stop') (('EfficientNet', 'CNN most modern architectures'), 'see') (('it', 'EfficientNet'), 'make') (('I', 'data'), 'feel') (('kappa loss', 'keras'), 'weight') (('that we', 'test set'), 'be') (('we', 'that'), 'be') (('Also we', 'data'), 'flip') (('Lastly we', 'normalization'), 'divide') (('We', 'validation score'), 'optimize') (('earlier we', 'score'), 'use') (('I', 'EfficientNet paper https arxiv'), 'recommend') (('that', 'competition'), 'be') (('GPU limit', '9 hours'), 'set') (('we', 'top'), 'specify') (('It', 'modeling section'), 'be') (('I', 'kernel Repository source https github'), 'feel') (('We', 'few added layers'), 'optimize') (('why we', 'Group Normalization https arxiv'), 'become') (('com ratthachat aptos', 'updatedv14 cropping'), 'preprocesse') (('model', 'data'), 'augment') (('when we', 'EfficientNet model'), 'increase') (('we', 'images'), 'manage') (('original class', 'Kaggle kernel https www'), 'find') (('I', 'test predictions'), 'check') (('Deep Learning how community', 'Henry AI Labs https youtu'), 'pdf') (('we', 'RAdam optimizer https arxiv'), 'note') (('kernel', 'B5'), 'provide') (('as long we', 'predictions'), 'render') (('learning traditional transfer we', 'image almost any content'), 'be') (('distribution', 'training still wildly data'), 'note') (('com benjaminwarner', '35000 additional images'), 'resize') (('implementation', 'Kaggle kernel https www'), 'find') (('This', 'formula'), 'be') (('kernel we', 'https 2019 www'), 'implement') (('We', 'preprocessing'), 'take') (('we', 'solely accuracy'), 'be') (('I', 'Kaggle kernel https highly www'), 'recommend') (('org', 'out challenge'), 'be') (('pre trained model', 'www'), 'http') (('com c aptos2019', 'detection competition'), 'blindness') (('it', 'incredibly layers'), 'make') (('com qubvel efficientnet Standard dependencies', 'time'), 'specification') (('It', 'nearly optimal way'), 'introduce') (('CyberZHG keras', 'blob master keras_radam optimizers'), 'com') (('there some', 'diabetic retinopathy worsens'), 'be') (('We', 'performance'), 'try') (('same thresholds', 'when final predictions'), 'use') (('Weights', 'EfficientNet https github'), 'find') (('we', 'explicitly parameters'), 'prepare') (('you', 'www'), 'be') (('we', 'loss function'), 'be') (('who', 'Keras https github'), 'thank') (('validation', 'training set'), 'calculate') (('we', 'valid predictions'), 'predict') (('that', 'classes'), 'visualize') (('we', 'predictions'), 'optimize') (('final predictions', 'Remove'), 'replace') (('we', 'smaller size'), 'bind') (('Additional data', 'Kaggle dataset https www'), 'find') (('it', 'validation set'), 'save') (('it', 'Vanilla Adam'), 'pdf') (('com aroraaman kappa quadratic metric', '5 simple steps'), 'explain') (('we', 'larger models'), 'use') (('Ben', 'kernel https www'), 'use') (('we', 'problem'), 'get') (('one', 'images'), 'Analysis') (('formula', 'weighted kappa'), 'blindness') "}