{"name": "pytorch rnns and lstms explained acc 0 99 ", "full_name": " h1 1 Introduction h1 2 Before we start h1 3 RNN with 1 Layer h2 3 1 Youtube Videos to Save you Time h2 3 2 RNN with 1 Layer and 1 Neuron h2 3 3 RNN with 1 Layer and Multiple Neurons h2 3 4 Vanilla RNN for MNIST Classification h3 3 4 1 Import the Data h3 3 4 2 RNN Architecture for MNIST Classification h3 Understanding the Model h3 3 4 3 Training h1 4 Multilayer RNNs h2 4 1 Why multilayers h2 4 2 Multilayer RNN for MNIST Classification h3 Understanding the Model h3 Training h1 5 LSTM Long Short Term Memory RNNs h2 5 1 Material to Save you Time h2 5 2 Why RNN might not be the best idea h2 5 3 Vanishing Gradient Problem h2 5 4 How does LSTM work h2 5 5 LSTM for MNIST Classification h3 How the Model Works h3 Training on ALL IMAGES h1 6 Bonuses h2 6 1 Confusion Matrix h2 6 2 Why shouldn t you use Transfer Learning h1 Other How I taught myself Deep Learning Notebooks h1 References ", "stargazers_count": 0, "forks_count": 0, "description": "2 RNN with 1 Layer and 1 Neuron You can always increase the number of neurons in an RNN. Try changing the learning_rate to 0. mm matrix multiplication 3. RNNs model sequential data meaning they have sequential memory. com andradaolteanu how i taught myself deep learning vanilla nns or CNNs https www. Creating the model Checking the output Parameters STATICS RNN inputs Creating the model Checking the output Parameters Customized transform transforms to tensor here you can normalize perform Data Augmentation etc. These DON T change with the input they stay the same through the entire sequence. Download data The Neural Network RNN Layer Fully Connected Layer Initialize hidden state with zeros Creating RNN Log probabilities Reshaped out STATICS how many images to be trained in one iteration image 28 by 28 can be changed to any number neurons 10 different digits Create a train_loader to select a batch from it Select one full batch from the data Reshape Creating the model Understand Model Parameters Create dataloader for training dataset so we can train on multiple batches Shuffle after every epoch Create criterion and optimizer Train the data multiple times Save Train and Test Loss Set model in training mode Get rid of the channel Create log probabilities Clears the gradients from previous iteration Computes loss how far is the prediction from the actual Computes gradients for neurons Updates the weights Save Loss Accuracy after each iteration Print Average Train Loss Accuracy after each epoch Save Test Accuracy Evaluation mode Get rid of the channel Create logit predictions Add Accuracy of this batch Print Final Test Accuracy STATICS Instantiate the model TRAIN Create RNN Create FNN Instantiate hidden_state at timestamp 0 Compute RNN. Illustrated Guide to LSTM s and GRU s A step by step explanationAlso this amazing blog post explains the Vanishing Gradient Problem and LSTMs 5. This technique is often used in deep learning classification problems that use CNN like EffNets ResNets etc. So you can see how well each label is predicted and what labels the model confuses with other labels for example a 7 can be sometimes confused with 1. Pro Tip Use print a lot if you don t understand what is happening helps you visualize Understanding the Model Here is what s happening to the batch below If we unfold the RNN 3. 1 Material to Save you Time I highly recommend going through the references below before continuing. 2 RNN Architecture for MNIST Classification Note Don t bother with the prints they are there for later only to understand what s happening inside the network. Before we start This is my third notebook in the series How I taught myself Deep Learning. 5 LSTM for MNIST Classification Bidirectional LSTMs are an extension of traditional LSTMs that can improve model performance on sequence classification problems. 2 Multilayer RNN for MNIST Classification Understanding the Model Here is what s happening in the batch below If we unfold the Multilayer RNN Example Training. There are 3 layers Input Output and Hidden where the information is stored 3. https towardsdatascience. 1 Youtube Videos to Save you Time I highly recommend watching the following to better understand RNNs. We ll have 2 timesteps 0 and 1. So if the adjustment in the previous layer is small then the adjustment in the current layer will be smaller CONSEQUENCE the first layers of the network don t learn because the adjustemnt are extremely small 5. com andradaolteanu convolutional neural nets cnns explained Why ConvNets Convolutions Explained Computing Activation Maps Kernels Padding Stride AlexNet MNIST Classification using Convolutions 3. a forward step through the network 2. Let s see how the model performs by adding 1 more layer. detach is required to prevent vanishing gradient problem Compute FNN We get rid of the second size STATICS neurons layers Taking a single batch of the images Remove channel from shape Create model instance Making log predictions STATICS Instantiate the model We ll use TANH as our activation function TRAIN Step1 the LSTM model Step2 the FNN we ll have 2 more layers Set initial states Hidden state Cell state Hidden state Cell state LSTM Reshape FNN STATICS width of image number of hidden neurons number of layers possible choices Taking a single batch of the images Remove channel from shape Creating the Model Making log predictions STATICS Instantiate the model We ll use TANH as our activation function TRAIN First we make sure we disable Gradient Computing Model in Evaluation Mode. The Architecture of our class will look like the figure below torch. RNN from PyTorch. 2 Why RNN might not be the best idea Issues in Vanilla RNNs have short term memory caused by the vanishing gradient problem as the RNN process has more steps timestamps it has more and more difficulty retaining information from previous steps 5. com andradaolteanu how i taught myself deep learning 1 pytorch fnn Convolutional Neural Nets CNNs Explained https www. It is not like CNNs when we know we put many filters on the image to extract the essence. This is mainly because recurrent data cannot really be generalized like static data images can be. In addition it usually works with images and text while ML usually works with tabular data. com andradaolteanu how i taught myself deep learning 1 pytorch fnn PyTorch and Tensors Neural Network Basics Perceptrons and a Plain Vanilla Neural Net model MNIST Classification using FNN Activation Functions Forward Pass Backpropagation Loss and Optimizer Functions Batching Iterations and Epochs Computing Classification Accuracy Overfitting Data Augmentation Weight Decay Learning Rate Dropout and Layer Optimization 2. We ll use get_accuracy and train_network functions from my previous notebook https www. com andradaolteanu how i taught myself deep learning vanilla nns but with some changes suited to the RNN s needs. Forget Gate Xt ht 1 desides what information to FORGET the closer to 0 is forget the closer to 1 is remain 2. RNN is a very powerful neural net. Explain code along the way to the best of my ability Note Deep learning coding is VERY different in structure than the usual sklearn for machine learning. com illustrated guide to lstms and gru s a step by step explanation 44e9eb85bf21 The Stacked LSTM is like the Multilayer RNN it has multiple hidden LSTM layers which contain multiple memory cells 5. Output Gate Xt ht 1 ct desides what the next hidden state should be which contains info about previous inputs Note Check THIS blog post for more detailed explanation. io en latest api augmentations. RNN with 1 Layer Recurrent Neural Networks are very different from FNNs https www. As you see the previous examples can t support large inputs and outputs as we would have to input the information at every timestep and output the results. Accuracy improves faster compared to the Vanilla RNN while final TEST Accuracy is slightly bigger. comparing the predictions with the actual values and computing a LOSS function 4. As you ll see it s performance is far grater than a normal FNN or CNN Side Note Images used as input NEED to have 1 channel so need to be B W 3. com watch v 8HyCNIVRbSU Also the blog post https towardsdatascience. Side Note It s AMAZING how important hyperparameters are. This is why RNNs might be weird in the approach for image classification but nevertheless very effective. 3 RNN with 1 Layer and Multiple Neurons Difference vs RNN 1 neuron 1 layer size of output changes because size of n_neurons changes size of the bias changes it s the size of n_neurons and W matrix 3. For somebody that starts in this area with no background whatsoever it can be very confusing especially because I seem to be unable to find code with many explanations and comments. com andradaolteanu how i taught myself deep learning convnet cnns. backpropagation calculates the gradients of the nodes in each layer if the GRADIENT is big the adjustment in weight is big and vice versa PROBLEM during backpropagation each node calculates its gradient with the respect of the effects of the gradients in the layer before it. 4 Vanilla RNN for MNIST Classification From now on we ll use the build in nn. How I taught myself Deep Learning Vanilla NNs https www. 4 How does LSTM work An LSTM is more complex than an simple RNN it is composed by cell states and gates it has the purpose to LEARN what to remember and forget reduntant information it uses SIGMOID functions instead of TANH Composition of the cell in LSTM the cell has 2 outputs the cell state and the hidden state 1. 1 Confusion Matrix A good way to visualize better how the model is performing is through a confusion matrix. io posts 2015 08 Understanding LSTMs Imports To display youtube videos When running on the CuDNN backend two further options must be set Set a fixed value for the hash seed STATICS RNN inputs The Neural Network __init__ the function where we create the architecture Weights are random at first U contains connection weights for the inputs of the current time step for 1 neuron size 4 rows and 1 column W contains connection weights for the outputs of the previous time step for 1 neuron size 1 row and 1 column The bias for 1 neuron size 1 row and 1 column forward function where we apply the architecture to the input Computes two outputs one for each time step two overall. com andradaolteanu convolutional neural nets cnns explained If you have any questions please do not hesitate to ask. Current Cell State ft Ct 1 it Ct 4. 01 and see what happens. It uses previous information to affect later ones2. If you liked this upvote Cheers References Illustrated Guide to Recurrent Neural Networks Understanding the Intuition https www. backpropagation uses the loss to adjust the weights in the network going BACKWARDS 5. Lastly LSTMs were the best performing ones 99 accuracy. The accuracy of the model now is impressive but by altering some of these hyperparameters can change this sweet spot we found instantly. and returns different kinds of outputs the next word letter in the sequence paired with an FNN it can return a classification etc. They train the model forward and backward on the same input so for 1 layer LSTM we get 2 hidden and cell states How the Model Works Below is a schema of how the example code works Training on ALL IMAGES Now we get even HIGHER accuracies than the ones before. Multilayer RNNs 4. This info is stored in the hidden state 5. com watch v LHXXI4 IEns Illustrated Guide to LSTM s and GRU s A step by step explanation https www. Ilustrated Guide to Recurrent Neural Networks Understanding the IntuitionFrom Michael Phi 3. I observed that when using RNNs is just another mathematical method in which the computer learns numbers and can therefore identify patterns. Read more about this here. 3 Vanishing Gradient Problem What is vanishing gradient it is due to the nature of backpropagation during the optimization process the steps are 1. You will understand how LSTMs are different from RNNs how they works and what is Vanishing Gradient Problem. com illustrated guide to lstms and gru s a step by step explanation 44e9eb85bf21 Understanding LSTM Networks https colah. Also try changing the batch_size to 20 instead of 64. There are only 3 matrixes U V W that contain weights as parameters. Try adding weight_decay to the optimizer functions. This notebook is made to bring more clear understanding of concepts and coding so this would also help me add modify and improve it. I am by no means a teacher but in this notebook I will 1. 1 Import the Data Note to further augmentations on the data check albumentations for PyTorch https albumentations. 2 Why shouldn t you use Transfer Learning Transfer learning is a genious way to use the weights of a pretrained model on another set of images. If we make a recap FNNs from my previous notebook had an accuracy of 80 CNNs had and accuracy of almost 90 while RNN reached 97. So if you are frustrated just like I was when I started this stuff I hope the following guidelines will help you. Introduction This notebook is just me being frustrated on deep learning and trying to understand in baby steps what is going on here. However this is not a regular technique used for RNN networks. Input Gate Xt ht 1 creates a candidate with what information to remain 3. Convolutional Neural Nets CNNs Explained https www. Note When using RNN in image classification it is hard to find the logic of why exactly are we doing this. com In recurrent neural networks like LSTMs is it possible to do transfer learning Has there been any research in this area Other How I taught myself Deep Learning Notebooks How I taught myself Deep Learning Vanilla NNs https www. For the moment we ll stick with 1. So please be patient with yourself and if you don t understand something right away continue reading coding and it will all make sense in the end. 1 Why multilayers Why use multiple layers rather than 1 to create higher level abstractions and capture more non linearities between the data Multilayers in RNN we want to create such abstractions and at the same time enforce their correlation with the previous inputs for 2 layers there are 2 hidden states as output for 3 layers there are 3 hidden states and so on Activation functions ReLU vs Tanh use them to try to erase the vanishing gradients problem we ll come back to these in the next chapter LSTM 4. LSTM Long Short Term Memory RNNs 5. The loop passes the input forward sequentialy while retaining information about it4. An RNN takes in different kind of inputs text words letters parts of an image sounds etc. computing the predictions 3. Share articles videos I watched that TRULY helped2. ", "id": "andradaolteanu/pytorch-rnns-and-lstms-explained-acc-0-99", "size": "13622", "language": "python", "html_url": "https://www.kaggle.com/code/andradaolteanu/pytorch-rnns-and-lstms-explained-acc-0-99", "git_url": "https://www.kaggle.com/code/andradaolteanu/pytorch-rnns-and-lstms-explained-acc-0-99", "script": "torch.nn.functional torchvision.transforms IPython.display LSTM_MNIST(nn.Module) numpy seaborn set_seed train_network MultilayerRNN_MNIST(nn.Module) get_accuracy torch.nn get_confusion_matrix matplotlib.pyplot forward VanillaRNN_MNIST(nn.Module) torch.optim RNNVanilla(nn.Module) __init__ YouTubeVideo ", "entities": "(('Deep learning coding', 'machine learning'), 'explain') (('also me', 'it'), 'make') (('how model', '1 more layer'), 'let') (('very especially I', 'many explanations'), 'be') (('letters parts', 'image'), 'take') (('it', 'previous steps'), 'be') (('here you', 'etc'), 'static') (('STATICS', 'TRAIN RNN Create FNN Instantiate Compute timestamp 0 RNN'), 'datum') (('info', 'hidden state'), 'store') (('steps', 'optimization process'), '3') (('better how model', 'confusion matrix'), '1') (('node', 'it'), 'calculate') (('why RNNs', 'image classification'), 'be') (('Convolutional Neural Nets CNNs', 'https www'), 'andradaolteanu') (('we', 'next chapter'), 'want') (('highly following', 'better RNNs'), 'Videos') (('which', 'memory multiple cells'), 's') (('CNN Side Note input', '1 channel'), 'be') (('we', 'results'), 'support') (('com how i', 'convnet deep learning cnns'), 'andradaolteanu') (('we', 'nn'), 'use') (('How I', 'Deep Vanilla https Learning NNs www'), 'teach') (('Transfer Learning Transfer learning', 'images'), 'use') (('U V only 3 matrixes that', 'parameters'), 'be') (('what', 'baby steps'), 'introduction') (('How I', 'Deep Vanilla https Learning NNs www'), 'com') (('Architecture', 'torch'), 'look') (('I', 'highly references'), 'material') (('com', 'step explanation'), 's') (('ConvNets Why Convolutions', 'Convolutions'), 'explain') (('7', 'sometimes 1'), 'see') (('blog amazing post', 'Gradient Vanishing Problem'), 'Guide') (('TANH instead Composition', 'cell state'), '4') (('you', 'questions'), 'explain') (('RNN', '97'), 'have') (('why exactly we', 'this'), 'note') (('Now we', 'ones'), 'train') (('below we', 'RNN'), 'print') (('information', '1 candidate'), 'create') (('We', 'notebook https previous www'), 'use') (('what', 'Gradient Problem'), 'understand') (('we', 'Evaluation Mode'), 'require') (('that', 'sequence classification problems'), 'be') (('ML', 'usually tabular data'), 'work') (('1 You', 'RNN'), 'RNN') (('computer', 'therefore patterns'), 'observe') (('there only what', 'network'), 'bother') (('that', 'etc'), 'use') (('which', 'more detailed explanation'), 'desides') (('you', 'Intuition https www'), 'like') (('we', 'sweet spot'), 'be') (('I', 'notebook'), 'be') (('However this', 'RNN regular networks'), 'be') (('TEST final Accuracy', 'Vanilla faster RNN'), 'improve') (('com how i', 'vanilla deep nns'), 'andradaolteanu') (('RNNs model sequential they', 'sequential memory'), 'datum') (('it', 'end'), 'be') (('it', 'n_neurons'), 's') (('we', 'essence'), 'be') (('Convolutional Neural Nets CNNs', 'https www'), 'explain') (('loop', 'it4'), 'pass') (('where we', 'time step'), 'posts') (('following guidelines', 'you'), 'be') (('It', 'later ones2'), 'use') (('backpropagation', 'BACKWARDS'), 'use') (('com how i', 'needs'), 'andradaolteanu') (('closer to 1', '0'), 'deside') (('it', 'classification'), 'return') (('they', 'same entire sequence'), 'change') (('RNN', 'FNNs https very www'), 'be') (('adjustemnt', 'network don t'), 'be') (('below we', 'Multilayer RNN Example Training'), 'RNN') (('How I', 'Deep Learning'), 'be') (('com how i', 'Tensors Neural Network Basics Plain Vanilla Neural Net model MNIST FNN Activation Functions'), 'andradaolteanu') "}