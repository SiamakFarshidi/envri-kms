{"name": "industrial safety complete solution ", "full_name": " h1 AIML Capstone Project Industrial Safety and Health Analytics Database h2 Table of Contents h2 1 Overview h3 Domain Industrial safety NLP based Chatbot h3 Context h3 Data Description h2 2 Import the necessary libraries h4 Setting Options h2 3 Data Collection h4 Shape of the data h4 Data type of each attribute h2 Data Collection Summary h2 4 Data Cleansing h4 Remove Unnamed 0 and Rename Data Countries Genre Employee or Third Party columns h4 Check Duplicates h4 Drop Duplicates h4 Check Outliers h4 Check Missing Values h2 Data Cleansing Summary h2 5 Data Pre processing h2 6 EDA Data Analysis and Preparation h4 Variable Identification h4 Univariate Analysis h4 Bivariate Analysis and Hypothesis testing h4 Study Summary Statistics h4 Study Correlation h2 EDA Summary h2 7 NLP Analysis h2 8 NLP Pre processing h4 Get the Length of each line and find the maximum length h4 WordCloud h4 NLP text summary statistics h2 NLP Pre processing Summary h2 9 Feature Engineering h4 Variable Creation Word2Vec Embeddings h4 Variable Creation Glove Word Embeddings h4 Variable Creation TFIDF Features h4 Variable Creation Label Encoding h4 Combine Glove and Encoded Features h4 Combine TFIDF and Encoded Features h4 Sampling Techniques Create Training and Test Set h4 Resampling Techniques Oversample minority class h4 SMOTE Generate synthetic samples upsample smaller class h4 Varible Tansformation Normalization and Scaling h4 Use PCA Extract Principal Components that capture about 95 of the variance in the data h2 10 Design train and test machine learning classifiers h4 Here we can use the DummyClassifier to predict all accident levels just to show how misleading accuracy can be h4 Define MultiClass Logloss h4 Train and test model h4 Train and test all models h4 Model with Hyperparameter Tuning h4 1 Modelling Logistic Regression h4 2 Decision Tree Random Forest Classifier h4 3 Modelling Logistic Regression Oversampling h4 4 Modelling Logistic Regression SMOTE h4 All models Original data h4 All models Oversampling data h4 All models SMOTE data h4 Hyperparameter tuning with original features h4 Bootstrap Sampling RandomForestClassifier h4 Bootstrap Sampling AdaBoostClassifier h4 Bootstrap Sampling XGBClassifier h2 11 Design train and test Neural networks classifiers h4 Get ANN Multiclass Classification Metrics h4 Convert Classification to Numeric problem h4 Multiclass classification Target variable One hot encoded h4 Multiclass classification Target variable One hot encoded with SMOTE data h2 12 Design train and test RNN or LSTM classifiers h4 Architecture h2 13 Conclusion h2 14 Recommendations h2 15 Limitations h2 16 Closing Reflections h2 17 References ", "stargazers_count": 0, "forks_count": 0, "description": "com ihmstefanini industrial safety and health analytics database Table of Contents table of contents 2. Body related left right hand finger face foot and glove Employee related employee operator collaborator assistant worker and mechanic Movement related fall hit lift and slip Equipment related equipment pump meter drill truck and tube Accident related accident activity safety injury causing NLP text summary statistics NLP Pre processing Summary 74 of data where accident description 100 is captured in low accident level. 25 on each side indicated by P value border Number of bootstrap samples to create size of a bootstrap sample run bootstrap empty list that will hold the scores for each bootstrap iteration prepare train and test sets Sampling with replacement picking rest of the data not considered in sample fit model fit against independent variables and corresponding target values Take the target column for all rows in test set evaluate model predict based on independent variables in the test data plot scores confidence intervals for 95 confidence tail regions on right and left. Since we are predicting the accident level we need to be 100 sure or at least close to 100 so that we can prevent the lot of accidents in industry. KNeighborsClassifier Best F1_Score 0. 620604 using max_samples 0. AIML Capstone Project Industrial Safety and Health Analytics Database Table of Contents1. Calendar Accidents are recorded from 1st Jan 2016 to 9th July 2017 in every month there are high number of accidents in 2016 and less in 2017. Multiclass classification Target variable One hot encodedIn this section we will create a classification model that uses categorical columns and tf idf features from accident description and one hot encoded target variable. State the H0 and Ha Ho The proportions of industry sector is not differ in different countries Ha The proportions of industry sector is differ in different countries 2. Finally the output dense layer will have five neuorns corresponding to each accident level. Table of Contents table of contents 14. 629247 using depth 4 iterations 300 l2_leaf_reg 4 learning_rate 0. 629247 using bagging_fraction 0. All models SMOTE dataBy comparing the results from all above methods all are over fitting the training data. Data Collection data collection 4. Calculate the p_value using test statistic 5. Gender There are more men working in this industry as compared to women. RandomForestClassifier Best F1_Score 0. Table of Contents table of contents 7. Accident Levels The number of accidents decreases as the Accident Level increases. 89 with accident description alone. one hot encoded Transform independent features Scaling only first 6 feautres Scaling only first 6 feautres generating the covariance matrix and the eigen values for the PCA analysis the relevanat covariance matrix generating the eigen values and the eigen vectors the cumulative variance explained analysis Plotting the variance expalained by the principal components and the cumulative variance explained. Gender Employee type Proportion of third party employees in each gender is equal third party remote employees in each gender is not equal and own employees in each gender is not equal. Sentences can have different lengths and therefore the sequences returned by the Tokenizer class also consist of variable lengths. Limitations We have less number of observations to analyse the cause of accidents correctly and rather we should collect more number of observations to get better results. Logistic Regression Best F1_Score 0. Create a model with Text inputs accident description alone only surprisingly achieved a test accuracy of 73. Lack of access to quality data. io daniellewisdl streamlit cheat sheet app. 028932 with max_features sqrt n_estimators 1000 95 Confidence interval range 0. 1 n_estimators 30 95 Confidence interval range 0. 25 on each side indicated by P value border disable keras warnings get the accuracy precision recall f1 score from model predict probabilities for test set Multiclass predict crisp classes for test set Multiclass Multilabel reduce to 1d array accuracy tp tn p n precision tp tp fp recall tp tp fn f1 2 tp 2 tp fp fn Multiclass Multilabel fix random seed for reproducibility define the model compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves fix random seed for reproducibility define the model Multilabel compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format fix random seed for reproducibility define the model Multilabel compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format Select input and output features Encode labels in column Accident Level. Number of accidents increased during the middle of the week and declined since the middle of th week. Data Pre processing data preprocessing 6. csv file Get the top 5 rows Check datatypes Check Data frame info Column names of Data frame Remove Unnamed 0 column from Data frame Rename Data Countries Genre Employee or Third Party columns in Data frame Get the top 2 rows Check duplicates in a data frame View the duplicate records Delete duplicate rows Get the shape of Industry data Check unique values of all columns except Description column Check the presence of missing values Visualize missing values function to create month variable into seasons Check the proportion of Industry sector in different countries Z test proportions More than 2 samples not implemented yet hence I am passing two elements Summary statistics Check the Correlation Checking 5 random Description and accident_levels from the data Checking 5 random Descriptions and accident_levels from the data where the length of headline is 100 Checking 5 random Descriptions and pot_accident_levels from the data where the length of headline is 100 Text preprocessing and stopwords custom module Get length of each line Get length of each line define training data train model summarize the loaded model summarize vocabulary save model load model this function creates a normalized vector for the whole sentence create sentence GLOVE embeddings vectors using the above function for training and validation set To replace white space everywhere in Employee type To replace white space everywhere in Critical Risk Create Industry DataFrame Label encoding convert integers to dummy variables i. 028086 with max_features log2 min_samples_split 13 n_estimators 90 95 Confidence interval range 0. EDA Data Analysis and Preparation Variable Identification Target variable Accident Level Potential Accident Level Predictors Input varibles Date Country Local Industry Sector Gender Employee type Critical Risk Description Univariate Analysis Country 59 accidents occurred in Country_01 31 accidents occurred in Country_02 10 accidents occurred in Country_03 Local Highest manufacturing plants are located in Local_03 city. 81 with original data. Since the data for these columns is well structured and doesn t contain any sequential or spatial pattern we can use simple densely connected neural networks to make predictions. I will build the real chatbot using Streamlit or some other applications. 023211 with alpha 0. Accident Levels The number of accidents decreases as the Accident Level increases and increases as the Potential Accident Level increases. Creating a Model with Multiple InputsThe first submodel will accept textual input in the form of accident description. org user_guide Customizing_Plots. What can you do to enhance the solution Need to work on limitations. Class imbalance issue is handled using below methods and found out that for this particular dataset with original data we have achieved the better results. Target variable Accident Level distribution is not equal I 309 II 40 III 31 IV 30 V 8. There are a plenty of unofficial ethnic and religious holidays in Brazil. CNC Current Voltage in plants weather information employee s personal data ex. Gender Industry Sector Proportion of Metals Mining and Others sector employees in each gender is not equal Gender Accident Levels Males have a higher accident levels than females. As we know this database comes from one of the biggest industry in Brazil which has four climatological seasos as below. 38 with original data. Modelling Logistic Regression Oversampling 4. The output from the dropout layer of the first submodel and the output from the batch norm layer of the second submodel will be concatenated together and will be used as concatenated input to another dense layer with 10 neurons. References references 1. State the H0 and Ha Ho The proportions of own employees in each gender is equal. There are few alphanumeric characters like 042 TC 06 Nv. Since we have ordinal relationship between each category in target variable I have considered this one as numerical regression problem and try to observe the ANN behaviour. There are only three Employee types in the provided data so there are no outliers in Gender column. LGBMClassifier Best F1_Score 0. We can use simple densely connected neural networks to make predictions. Import the necessary libraries import libraries 3. NLP Analysis nlp analysis 8. So there are no outliers in the Date column. Accident Levels Calendar Accidents are recorded from 1st Jan 2016 to 9th July 2017 in every month there are high number of accidents in 2016 and less in 2017. We have no missing values in this dataset. 613227 using learning_rate 0. Capturing 90 variance of the data DummyClassifier to predict all Accident levels checking unique labels checking accuracy Checking unique values Convert actual to a binary array if it s not already Fit the model on Training set Fit the model on Training set Intercept and Coefficients Predict on Test set Initialise mc_logloss Model Confusion matrix Model Classification report Store the accuracy results for each model in a dataframe for final comparison Save the model return all the metrics along with predictions define classification models early_stopping_rounds 30 early_stopping_rounds 30 Train and Test the model Store the accuracy results for each model in a dataframe for final comparison note the start time Before starting with grid search we need to create a scoring function. Number of accidents are very high in particular days like 4 8 and 16 in every month. Recommendations recommendations 15. 03 95 Confidence interval range 0. Table of Contents table of contents 12. 6831 Total duration 2430. Extracted the day month and year from Date column and created new features such as weekday weekofyear and seasons. We have seven duplicate values in this dataset and dropped those duplicate values. Accident Levels by Employee type Is the distribution of accident levels and potential accident levels differ significantly in different employee types Observations For both accident levels the incidence of Employee is higher at low accident levels but the incidence of Third parties seems to be slightly higher at high accident levels. Convert Classification to Numerical problem achieved a test accuracy of 53. Design train and test RNN or LSTM classifiers Architecture1. Table of Contents table of contents 3. AdaBoostClassifier Best F1_Score 0. Let s check the unique and frequency mode of each variable. There are only three country types so there are no outliers in Country column. There are only three Industry Sector types which are in sequence so there are no outliers in Industry Sector column. There are digits in the description for e. Check Missing Values Data Cleansing Summary 1. There are many low risks at general accident level but many high risks at potential accident level. 6906 Total duration 51. Pre processing such as removing punctuations and lemmatization can be used. Create a model with Multiple Inputs concatenated the layers from text input model and categorical features input model surprisingly achieved a test accuracy of 73. This is accomplished using the make_scorer function of scikit learn. By comparing the results from all ML methods with original data we can select the best method as AdaBoost classifier with f1 score 65. 34 of data where accident description 100 is captured in high medium potential accident level. Bootstrap sampling with XGBClassifier model with an accuracy of 63. Data Pre processing To better understand the data I am extracting the day month and year from Date column and creating new features such as weekday weekofyear. 026945 with C 1. There are only six Potential Accident Level types which are in sequence so there are no outliers in Potential Accident Level column. Table of Contents table of contents 4. 43 own empoyees working in this industry. 38 All models Oversampling dataBy comparing the results from all above methods we can select best method as Ridge classifier with f1 score 62. Decision trees frequently perform well on imbalanced data. 81 with original data TF IDF features from accident description column. Create a model with Text inputs only. Closing Reflections What did we learned from the process How to work on Data Science project to end to end. Overview Domain Industrial safety. Divide our data into testing and training sets Convert both the training and test labels into one hot encoded vectors The first step in word embeddings is to convert the words into thier corresponding numeric indexes. GradientBoostingClassifier Best F1_Score 0. Table of Contents table of contents 15. 034210 with max_samples 0. Identify the test statistic Z test of proportions 4. 25 of data where accident description 100 is captured in medium potential accident level. Hyperparameter tuning with original features1. Country Percentage of accidents occurred in respective countries 59 in Country_01 31 in Country_02 and 10 in Country_03. level 326 Dumper 01 where removing the digits wouldn t help. 1 penalty l1 solver liblinear 95 Confidence interval range 0. Critical Risk Because most part of the Critical Risks are classified as Others it is thought that there are too many risks to classify precisely. Context The database comes from one of the biggest industry in Brazil and in the world. The loss of the model will almost always be lower on the training dataset than the validation dataset. Create a model with Categorical features only achieved a test accuracy of 73. It is an urgent need for industries companies around theglobe to understand why employees still suffer some injuries accidents in plants. Decide to Reject or Accept Null HypothesisHence we fail to reject Null Hypothesis we have enough 95 evidence to prove that the proportion of own employees in each gender is equal. Building a Linear Regression model Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Building a Random Forest Classifier on Training set Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Building a Linear Regression model Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Building a Linear Regression model Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Train and Test all models with Lasso interaction terms Train and Test all models with Lasso interaction terms Train and Test all models with Lasso interaction terms define regressor models define model parameters penalty none l1 l2 elasticnet class_weight none balanced multi_class ovr multinomial class_weight balanced balanced_subsample none class_weight balanced balanced_subsample none Considering all Predictors Consider only top 30 GLOVE features Number of bootstrap samples to create size of a bootstrap sample run bootstrap empty list that will hold the scores for each bootstrap iteration prepare train and test sets Sampling with replacement picking rest of the data not considered in sample fit model fit against independent variables and corresponding target values Take the target column for all rows in test set evaluate model predict based on independent variables in the test data plot scores confidence intervals for 95 confidence tail regions on right and left. Table of Contents table of contents 8. 5 bagging_frequency 8 boosting_type dart early_stopping_rounds 200 feature_fraction 0. 02 95 Confidence interval range 0. 6801 Total duration 176. 026945 with metric euclidean n_neighbors 13 weights uniform 95 Confidence interval range 0. Bootstrap sampling with AdaBoostClassifier model with an accuracy of 51. Ha The proportions of own employees in each gender is not equal. 6831 Total duration 7. What I do differently next time Perhaps I will explore more feature engineering and feature selection techniques. It is showing noisy values of relatively high loss indicating that the model was unable to learn the training dataset at all and model does not have a suitable capacity for the complexity of the dataset. Decide to Reject or Accept Null Hypothesis Hence we reject Null Hypothesis we have enough 95 evidence to prove that the mining sector in country 1 is differ from metals sector Hence we reject Null Hypothesis we have enough 95 evidence to prove that the mining sector in country 1 is differ from others sector b. WordCloud Observations There are many body related employee related movement related equipment related and accident related words. 68 and all other methods are over fitting the training data. Decide the significance level alpha 0. Industry Sector by Countries Is the distribution of industry sector different significantly in differ countries or not Observations Metals and Mining industry sector plants are not available in Country_03. Lowest manufacturing plants are located in Local_09 city. Accident Levels by Gender Is the distribution of accident levels and potential accident levels differ significantly in different genders Observations Proportion of accident levels in each gender is not equal and males have a higher accident levels than females. Less number of features available in dataset. References Good EDA Notebook https www. x in colab Setting Options Table of Contents table of contents 3. Decision Tree Random Forest Classifier While in every machine learning problem it s a good rule of thumb to try a variety of algorithms it can be especially beneficial with imbalanced datasets. We are left with 418 rows and 10 columns after data cleansing. Columns description Data timestamp or time date information Countries which country the accident occurred anonymised Local the city where the manufacturing plant is located anonymised Industry sector which sector the plant belongs to Accident level from I to VI it registers how severe was the accident I means not severe but VI means very severe Potential Accident Level Depending on the Accident Level the database also registers how severe the accident could have been due to other factorsinvolved in the accident Genre if the person is male of female Employee or Third Party if the injured person is an employee or a third party Critical Risk some description of the risk involved in the accident Description Detailed description of how the accident happened. Above one is good fit it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values. define grid search summarize results note the end time calculate the total duration For multiclass problems only newton cg sag saga and lbfgs handle multinomial loss liblinear is limited to one versus rest schemes. Divide our data into testing and training sets Convert both the training and test labels into one hot encoded vectors Variable transformation using StandardScaler Scaling only first 6 feautres Scaling only first 6 feautres fix random seed for reproducibility compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format fix random seed for reproducibility compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves. Design train and test machine learning classifiers Here we can use the DummyClassifier to predict all accident levels just to show how misleading accuracy can be. Accident Levels by Weekday Is the distribution of accident levels and potential accident levels differ significantly in different weekday Observations Both of the two accident level is thought that non severe levels decreased in the first and the last of the week but severe levels did not changed much. 50 cm deep 30 kg where removing the digits wouldn t help. 6831 Total duration 428. Modelling Logistic Regression SMOTE Table of Contents table of contents All models Original dataBy comparing the results from all above methods we can select the best method as AdaBoost classifier with f1 score 65. 75 n_estimators 10 95 Confidence interval range 0. How to handle class imbalance data set. With more detailed information such as machining data ex. 026945 with C 0. 1 n_estimators 30 0. NLP Pre processingFew of the NLP pre processing steps taken before applying model on the data Converting to lower case avoid any capital cases Converting apostrophe to the standard lexicons Removing punctuations Lemmatization Removing stop words Get the Length of each line and find the maximum lengthAs different lines are of different length. Above one is overfit model it can be identified from the learning curve of the training and validation loss only. 01 min_data_in_leaf 90 n_estimators 1000 num_class 5 num_leaves 1550 objective multiclass verbosity 1 0. No missing values in dataset. Country Industry Sector Metals and Mining industry sector plants are not available in Country_03. Table of Contents table of contents 6. html Scikit Learn API https scikit learn. Data Description This The database is basically records of accidents from 12 different plants in 03 different countries which every line in the data is anoccurrence of an accident. How to build different NLP architectures for handling both numerical and text data. Feature Engineering feature engineering 10. Table of Contents table of contents 17. Data Cleansing data cleansing 5. 033099 with learning_rate 0. Able to predict the accident level with a test accuracy of 73. 630245 using learning_rate 0. 026945 with bagging_fraction 0. And it is also thought that it takes so much time to analyze risks and reasons why the accidents occur. Ridge Best F1_Score 0. Table of Contents table of contents 2. There are only two Gender types in the provided data so there are no outliers in Gender column. There are about 425 rows and 11 columns in the dataset. Accident Levels Employee type For both accident levels the incidence of Employee is higher at low accident levels but the incidence of Third parties seems to be slightly higher at high accident levels. 6831 Total duration 6367. Multiclass classification Target variable One hot encoded achieved a test accuracy of 73. Accident Levels by Month Is the distribution of accident levels and potential accident levels differ significantly in different months Observations Both of the two accident level have the tendency that non severe levels decreased throughout the year but severe levels did not changed much and some of these levels increased slightly in the second half of the year. com Brazil Spring September to November Summer December to February Autumn March to May Winter June to AugustWe can create seasonal variable based on month variable. There are quite a lot of Critical risk descriptions and we don t see any outliers but with the help of SME we can decide whether this column has outliers or not. To reduce the occurrences of accidents more stringent safety standards in hand operation will be needed in period when many accidents occur. The number of accidents increases as the Potential Accident Level increases. There are only five Accident Level types which are in sequence so there are no outliers in Accident Level column. Observations 34 of data where accident description 100 is captured in high medium potential accident level. Both of the two accident level is thought that non severe levels decreased in the first and the last of the week but severe levels did not changed much. We noticed that except a date column all other columns are categorical columns. This submodel will consist of an input shape layer an embedding layer and bidirectional LSTM layer of 128 neurons followed by max pool layer drop out and dense layers. Multiclass classification Target variable One hot encoded with SMOTE dataIn this section we will create a classification model that uses categorical columns and tf idf features from accident description and one hot encoded target variable. Recommendations In this project we discovered that the main causes of accidents are mistakes in hand operation and time related factor. 1 n_estimators 70 0. com koheimuramatsu industrial accident causal analysis Holoviews plot tips http holoviews. 23 of data where accident description 100 is captured in low potential accident level. Industry Sector Percentage of manufacturing plants belongs to respective sectors 57 to Mining sector 32 to Metals sector and 11 to Others sector. Conclusion conclusion 14. CatBoostClassifier Best F1_Score 0. ExtraTreesClassifier Best F1_Score 0. 0001 max_depth 10 metric multi_logloss min_child_samples 486 min_child_weight 0. Design train and test RNN or LSTM classifiers nlp models Creating a Model with Text Inputs Only nlp models text input Creating a Model with Categorical features Only nlp models cat features Creating a Model with Multiple Inputs nlp models multiple input 13. Proportion of Others sector employees in each gender is not equal. Accident Levels by Seasons Is the distribution of accident levels and potential accident levels differ significantly in different seasons Observations Both of the two accident level have the tendency that non severe levels decreased throughout the year but severe levels did not changed much and some of these levels increased slightly in the second half of the year. Data Cleansing Remove Unnamed 0 and Rename Data Countries Genre Employee or Third Party columns Check Duplicates There is no need to worry about preserving the data it is already a part of the industry dataset and we can merely remove or drop these rows from your cleaned data Drop Duplicates Check OutliersAs we know there is no concept of outliers detection in categorical variables nominal and ordinal as each value is count as labels. 0 gamma scale kernel rbf 95 Confidence interval range 0. Removed Unnamed 0 column and renamed Data Countries Genre Employee or Third Party columns in the dataset. Number of accidents are high in beginning of the year and it keeps decreasing later. 616087 using alpha 0. The second submodel will accept input in the form of meta information which consists of dense batch norm and drop out layers. Design train and test Neural networks classifiers Get ANN Multiclass Classification Metrics Convert Classification to Numeric problemIn this section we will create a classification model that uses categorical columns and tf idf features from accident description and label encoded target variable. 81 and f1 score of 73. Import the necessary librariesFirstly let s select TensorFlow version 2. Modelling Logistic Regression 2. 0 gamma scale kernel rbf 0. NLP Analysis Observations 74 of data where accident description 100 is captured in low accident level. Data Collection Shape of the data Data type of each attribute From the above output we see that except first column all other columns datatype is object. There are 12 Local cities where manufacturing plant is located and it s types are in sequence so there are no outliers in Local column. Another national holidays are election days. 622235 using max_features sqrt n_estimators 1000 0. 001 fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format Select input and output features Encode labels in column Accident Level. There are no outliers in the dataset. Feature Engineering Variable Creation Word2Vec Embeddings Variable Creation Glove Word Embeddings Variable Creation TFIDF Features Variable Creation Label Encoding Combine Glove and Encoded Features Combine TFIDF and Encoded Features Sampling Techniques Create Training and Test Set Resampling Techniques Oversample minority class SMOTE Generate synthetic samples upsample smaller class Varible Tansformation Normalization and Scaling Use PCA Extract Principal Components that capture about 95 of the variance in the data Table of Contents table of contents 10. 81 and f1 score of 72. Categorical columns Countries Local Industry Sector Accident Level Potential Accident Level Genre Employee or Third Party Critical Risk Description Date column Data Data Collection Summary 1. As expected we see the learning curves for accuracy on the test dataset plateau indicating that the model has no longer overfit the training dataset and it is generalized model. But let s check is that difference is statistically significant 1. We have no outliers in this dataset. We had 7 duplicate instances in the dataset and dropped those duplicates. Define MultiClass Logloss Train and test model Train and test all models Model with Hyperparameter Tuning 1. py Heroku with Python https devcenter. Based on some random headlines seen above it appears that the data is mostly lower cased. EDA Data Analysis and Preparation eda Univariate Analysis univariate analysis Bivariate Analysis and Hypothesis testing bivariate analysis 7. Octoberfest Brazilian Carnival Kinderfest Fenaostra Fenachopp Musikfest Schutzenfest Kegelfest Cavalhadas Oberlandfest Tirolerfest Marejada are among them. How to build different ANN model architectures for handling multi class classification problems. We can create holidays variable based on Brazil holidays list from 2016 and 2017. Table of Contents table of contents 5. Employee type by Gender Is the distribution of employee type differ significantly in different genders Observations Proportion of third party employees in each gender is equal. NLP Pre processing nlp pre processing Word Cloud wordcloud 9. Study Summary Statistics Study Correlation Observations WeekofYear featuer is having very high positive correlation with Month feature. Finally bidirectional LSTM model can be considered to productionalized the model and predict the accident level. 6831 Total duration 12. Employee type 44 Third party empoyees working in this industry. org stable Keras API https keras. They work by learning a hierarchy of if else questions and this can force both classes to be addressed. 13 Third party Remote empoyees working in this industry. 026945 with depth 4 iterations 300 l2_leaf_reg 4 learning_rate 0. 629247 using metric euclidean n_neighbors 13 weights uniform 0. io api Streamlit API https docs. one hot encoded Dummy variables encoding Merge the above dataframe with the original dataframe ind_feat_df Check NaN values Consider only top 30 GLOVE features Consider only top 30 GLOVE features Considering all Predictors Display old accident level counts Concatenate our training data back together Get the majority and minority class Upsample Level1 minority class sample with replacement to match majority class Upsample Level2 minority class sample with replacement to match majority class Upsample Level3 minority class sample with replacement to match majority class Upsample Level4 minority class sample with replacement to match majority class Combine majority class with upsampled minority classes Display new accident level counts Separate input features and target Considering all Predictors Separate input features and target Considering all Predictors Display new accident level counts convert integers to dummy variables i. We observed that there are records of accidents from 1st Jan 2016 to 9th July 2017 in every month. Above one is underfit model it can be identified from the learning curve of the training loss only. 01 min_data_in_leaf 90 n_estimators 1000 num_class 5 num_leaves 1550 objective multiclass verbosity 1 95 Confidence interval range 0. Both of the two accident level have the tendency that non severe levels decreased throughout the year but severe levels did not changed much and some of these levels increased slightly in the second half of the year. Industry Sector 57 manufacturing plants belongs to Mining sector. 32 manufacturing plants belongs to Metals sector. But let s check the proportion of metals mining and others sector in Country_01 and is that difference is statistically significant 1. SMOTE Generate synthetic samples8. Critical Risk Most of the critical risks are classified as Others. We could see it accuracy continually rise during training. There are few alphanumeric characters like AFO 755 captured in description where removing these characters might help. Link to download the dataset https www. 11 manufacturing plants belongs to Others sector. 6794 Total duration 165. Sometimes they also die in such environment. 6890 Total duration 305. Resampling techniques Oversampling minority class b. 6625 Total duration 2. age experience in the industry sector work performance we can clarify the cause of accidents more correctly. Proportion of own employees in each gender is not equal. 6844 Total duration 688. With more number of observations than current number of records 425 so that we can feed more data into ML ANN NLP models to train evaluate the performance of those models and get the better results. com articles getting started with python used to supress display of warnings nlp libraries sampling methods import zscore for scaling the data save models pre processing methods the classification models ensemble models methods and classes for evaluation cross validation methods feature selection methods pre processing methods Deep learning libraries Keras pre processing Reproduce the results suppress display of warnings display all dataframe columns to set the limit to 3 decimals display all dataframe rows Read IHMStefanini_industrial_safety_and_health_database_with_accidents_description. Few of the NLP pre processing steps taken before applying model on the data Converting to lower case avoid any capital cases Converting apostrophe to the standard lexicons Removing punctuations Lemmatization Removing stop words After pre processing steps Minimum line length 64 Maximum line length 672 Minimum number of words 10 Maximum number of words 98 Table of Contents table of contents 9. Creating a Model with Text Inputs OnlyIn this section we will create a classification model that uses accident description column alone. Distribution of industry sector differ significantly in each country. Note Surprisingly we observe that same f1 score 73. 030167 with learning_rate 0. Design train and test machine learning classifiers ml models All models Original data ml models original data All models Oversampling data ml models oversampling data All models SMOTE data ml models smote data Hyperparameter tuning with original features ml models hyperparam tuning Bootstrap Sampling ml models bootstrap sampling 11. Design train and test Neural networks classifiers ann models Convert Classification to Numeric problem ann models clas to num problem Multiclass classification Target variable One hot encoded ann models multi class 12. Proportion of third party remote employees in each gender is not equal. Explored below options in Neural Networks. 5182700157166 Table of Contents table of contents Bootstrap Sampling RandomForestClassifier Bootstrap Sampling AdaBoostClassifier Bootstrap Sampling XGBClassifier Table of Contents table of contents 11. SVC Best F1_Score 0. 3370 CX 212 captured in description where removing these characters might help. 75 n_estimators 10 0. There are quite a lot of Critical risk descriptions but with the help of SME we can decide whether this column has outliers or not and also SME can help us in understanding the data better. Overview overview 2. Table of Contents table of contents Bivariate Analysis and Hypothesis testing a. 1 n_estimators 70 95 Confidence interval range 0. I realized that the detail information of accidents like Description is so useful to analyze the cause. 628276 using max_features log2 min_samples_split 13 n_estimators 90 0. Closing Reflections closing reflections 17. BaggingClassifier Best F1_Score 0. Limitations limitations 16. 57 which is a bad result. Table of Contents table of contents 13. Create a model with Multiple inputs. 8 learning_rate 0. EDA Summary Local Highest manufacturing plants are located in Local_03 city and lowest in Local_09 city. Note Considering official holidays only. Industry Sector by Gender Is the distribution of industry sector differ significantly in different genders Observations Proportion of Metals sector employees in each gender is not equal. Proportion of Mining sector employees in each gender is not equal. Bootstrap sampling with RandomForestClassifier model with an accuracy of 66. We need to load the built in GloVe word embeddings Build a LSTM Neural Network LSTM_Layer_1 LSTM 128 embedding_layer dense_layer_1 Dense 5 activation softmax LSTM_Layer_1 model Model inputs deep_inputs outputs dense_layer_1 Use earlystopping callback tf. 1 penalty l1 solver liblinear 0. EarlyStopping monitor loss patience 5 min_delta 0. Where does our model fall short in the real world Once we deploy the finalised model in Production we might get less f1 score as compared to productionalized model results. 28 with original data. We need to pad the our sequences using the max length. Employee type 44 Third party empoyees 43 own empoyees and 13 Third party Remote empoyees working in this industry. Create a model with Categorical inputs only. Creating a Model with Categorical features OnlyIn this section we will create a classification model that uses categorical columns alone. ", "id": "vinayakshanawad/industrial-safety-complete-solution", "size": "36328", "language": "python", "html_url": "https://www.kaggle.com/code/vinayakshanawad/industrial-safety-complete-solution", "git_url": "https://www.kaggle.com/code/vinayakshanawad/industrial-safety-complete-solution", "script": "_render lightgbm keras.layers sklearn.discriminant_analysis maxnorm SMOTE get_classification_metrics SGD recall_score mutual_info_classif BaggingClassifier keras.callbacks randint as sp_randint sklearn.neighbors GaussianNB sklearn.linear_model matplotlib.pyplot ModelCheckpoint precision_score metrics train_test_model l1 Model missingno sklearn.pipeline tensorflow.keras scipy.stats DummyClassifier Pipeline f_classif keras.regularizers Lasso tensorflow.keras.preprocessing.text sklearn.feature_selection Activation keras.models gensim.models keras.constraints zscore text_preprocess_py sklearn.svm XGBClassifier cross_val_score accuracy_score tensorflow.keras.models catboost Dropout min_max_norm imblearn.over_sampling tensorflow.keras.preprocessing.sequence Bidirectional pad_sequences hyperparameterstune_model sklearn.naive_bayes sklearn.dummy Metrics(tf.keras.callbacks.Callback) Embedding proportions_ztest train_test_allmodels on_train_begin holoviews sklearn tensorflow StratifiedKFold pandas SelectKBest LogisticRegression * #(custom module) l2 nltk.corpus AdaBoostClassifier GridSearchCV keras.layers.merge sklearn.ensemble model_from_json optimizers reset_random_seeds RidgeClassifier keras.wrappers.scikit_learn os confusion_matrix plot_model sklearn.utils CatBoostClassifier nltk.tokenize numpy ExtraTreesClassifier Word2Vec BatchNormalization randint statsmodels.stats.proportion on_epoch_end LGBMClassifier sklearn.tree Tokenizer keras.optimizers ReduceLROnPlateau resample DecisionTreeClassifier np_utils RandomForestClassifier multiclass_logloss RepeatedStratifiedKFold LabelEncoder sent2vec sklearn.metrics Flatten StandardScaler RFECV classification_report train_test_split RFE tensorflow.keras.layers xgboost seaborn Concatenate TfidfVectorizer f1_score SVC sklearn.feature_extraction.text Dense tqdm GradientBoostingClassifier roc_auc_score LSTM PCA KFold KerasClassifier month2seasons Sequential LinearDiscriminantAnalysis sklearn.model_selection PowerTransformer EarlyStopping load_model stopwords opts TruncatedSVD Input unit_norm RandomizedSearchCV KNeighborsClassifier makedirs keras.utils StackingClassifier __init__ word_tokenize sklearn.decomposition GlobalMaxPool1D l1_l2 sklearn.preprocessing ", "entities": "(('we', 'predictions'), 'be') (('s', 'variable'), 'let') (('accident where description', 'accident 100 low level'), 'observation') (('data', 'it'), 'appear') (('severe levels', 'week'), 'be') (('Convert Classification', '53'), 'achieve') (('h5 format Select input', 'column Accident Level'), 'get') (('629247', 'depth 4 iterations'), 'use') (('Target variable Accident Level distribution', 'I'), 'be') (('maximum lengthAs different lines', 'different length'), 'Pre') (('accident where description', 'accident 100 medium potential level'), '25') (('Country Industry industry sector Sector Metals plants', 'Country_03'), 'be') (('underfit it', 'training loss'), 'be') (('we', 'industry'), 'predict') (('I', 'Streamlit'), 'build') (('We', 'predictions'), 'use') (('Distribution', 'significantly country'), 'differ') (('it', 'especially imbalanced datasets'), 'Classifier') (('therefore sequences', 'also variable lengths'), 'have') (('model', 'dataset'), 'show') (('why accidents', 'risks'), 'think') (('males', 'females'), 'be') (('manufacturing 32 plants', 'Metals sector'), 'belong') (('that', 'accident description column'), 'create') (('Accident number', 'Potential Accident Level increases'), 'level') (('just how misleading accuracy', 'accident levels'), 'classifier') (('43 own empoyees', 'industry'), 'work') (('sentence GLOVE', 'variables dummy i.'), 'get') (('number', 'curves'), 'divide') (('that', 'accident tf idf description'), 'variable') (('difference', 'Country_01'), 'let') (('EarlyStopping', 'loss patience'), 'monitor') (('Multiple InputsThe first submodel', 'accident description'), 'accept') (('we', 'f1 score'), 'model') (('overfit it', 'training loss'), 'be') (('nominal value', 'labels'), 'Remove') (('31 accidents', 'Local_03 city'), 'varible') (('Create', '73'), 'concatenate') (('proportions', 'different countries'), 'state') (('This', 'scikit'), 'accomplish') (('we', 'AdaBoost f1 score'), 'by') (('Proportion', 'gender'), 'be') (('NLP processing nlp', 'Word Cloud'), 'Pre') (('accident where description', 'accident 100 low level'), 'leave') (('Industry Sector Percentage', 'Others sector'), 'belong') (('equal employees', 'gender'), 'be') (('that', 'accident tf idf description'), 'encode') (('columns other datatype', 'first column'), 'Shape') (('euclidean metric n_neighbors 13 weights', 'Confidence interval 95 range'), 'uniform') (('accuracy', 'continually training'), 'see') (('train', 'right'), '25') (('Accident Levels Calendar Accidents', '2017'), 'record') (('Summer December', 'month variable'), 'com') (('Observations Proportion', 'gender'), 'be') (('Industry manufacturing Sector 57 plants', 'Mining sector'), 'belong') (('accident where description', 'accident 100 high medium potential level'), '34') (('Number', 'month'), 'be') (('we', 'model productionalized results'), 'fall') (('Octoberfest Brazilian Carnival Kinderfest Fenaostra Fenachopp Musikfest Schutzenfest Kegelfest Cavalhadas Oberlandfest Tirolerfest Marejada', 'them'), 'be') (('We', 'max length'), 'need') (('Gender Accident Levels Males', 'females'), 'be') (('Industry Sector only three which', 'Industry Sector so column'), 'be') (('dataBy', 'training data'), 'be') (('where removing', 'characters'), 'be') (('We', 'duplicates'), 'have') (('Decision trees', 'frequently well imbalanced data'), 'perform') (('Accident Level only six Potential which', 'Potential Accident Level so column'), 'be') (('sector employees', 'gender'), 'be') (('We', '2017 month'), 'observe') (('manufacturing 11 plants', 'Others sector'), 'belong') (('we', 'accidents'), 'experience') (('max_features', '1000 Confidence interval n_estimators 95 range'), 'sqrt') (('loss', 'validation dataset'), 'be') (('when many accidents', 'period'), 'need') (('Sometimes they', 'also such environment'), 'die') (('proportions', 'gender'), 'be') (('We', 'dataset'), 'have') (('column', 'outliers'), 'be') (('Varible Tansformation Scaling Use Extract Principal that', 'contents'), 'Creation') (('I', 'ANN behaviour'), 'consider') (('this', 'classes'), 'work') (('mining sector', 'sector b.'), 'decide') (('line', 'accident'), 'description') (('data TF original IDF', 'accident description column'), 'feature') (('severe levels', 'week'), 'think') (('industry sector Observations Metals plants', 'Country_03'), 'be') (('differently next time Perhaps I', 'feature more feature engineering techniques'), 'do') (('Create', '73'), 'achieve') (('Finally output dense layer', 'accident level'), 'have') (('where removing', 'characters'), '3370') (('accident level new counts', 'variables dummy i.'), 'consider') (('other methods', 'training data'), 'be') (('accident where description', 'accident 100 high medium potential level'), 'observation') (('it', 'Others'), 'risk') (('Ha proportions', 'gender'), 'state') (('Calendar Accidents', '2017'), 'record') (('Surprisingly we', 'f1 same score'), 'note') (('output', '10 neurons'), 'concatenate') (('main causes', 'hand operation'), 'recommendation') (('Pre processing', 'such punctuations'), 'use') (('Multiclass classification Target variable', '73'), 'achieve') (('models', 'ml models Bootstrap Sampling ml models sampling'), 'model') (('other columns', 'date column'), 'notice') (('io daniellewisdl', 'cheat sheet app'), 'streamlit') (('We', '2016'), 'create') (('activation softmax LSTM_Layer_1 model Model LSTM 128 embedding_layer Dense 5 deep_inputs', 'callback'), 'need') (('results suppress display', 'rows Read dataframe IHMStefanini_industrial_safety_and_health_database_with_accidents_description'), 'article') (('correctly rather we', 'better results'), 'have') (('We', 'duplicate values'), 'have') (('librariesFirstly s', 'TensorFlow version'), 'let') (('types', 'Local column'), 'be') (('why employees', 'plants'), 'be') (('detail information', 'so cause'), 'realize') (('submodel', 'layers'), 'consist') (('it', 'year'), 'be') (('manufacturing Lowest plants', 'city'), 'locate') (('Study Summary Statistics Study Correlation Observations WeekofYear featuer', 'Month feature'), 'have') (('Most', 'Others'), 'classify') (('Country Percentage', 'Country_03'), 'occur') (('cumulative variance', 'principal components'), 'feature') (('total duration', 'rest schemes'), 'note') (('first step', 'thier corresponding numeric indexes'), 'be') (('that', 'categorical columns'), 'create') (('proportion', 'gender'), 'fail') (('that', 'loss two final values'), 'be') (('Accident Level only five which', 'Accident Level so column'), 'be') (('incidence', 'accident slightly high levels'), 'be') (('that', 'target variable'), 'get') (('Lemmatization Removing', 'contents'), 'pre') (('train', 'right'), 'build') (('I', 'weekday such weekofyear'), 'Pre') (('much some', 'year'), 'be') (('which', 'layers'), 'accept') (('we', 'better results'), 'handle') (('425 we', 'better results'), 'feed') (('we', 'end'), 'reflection') (('koheimuramatsu accident causal analysis Holoviews plot com industrial tips', 'holoviews'), 'http') (('number', 'Potential Accident Level increases'), 'increase') (('Detailed how accident', 'accident'), 'description') (('Number', 'th week'), 'increase') (('h5 format Select input', 'column Accident Level'), 'fit') (('incidence', 'accident slightly high levels'), 'type') (('LSTM Finally bidirectional model', 'accident level'), 'consider') (('EDA Summary Local manufacturing Highest plants', 'city'), 'locate') (('We', 'data 10 cleansing'), 'leave') (('Accident number', 'Accident Level increases'), 'level') (('it', 'training longer dataset'), 'expect') (('also SME', 'data'), 'be') (('we', 'AdaBoost f1 score'), 'table') (('much some', 'year'), 'have') (('Context database', 'world'), 'come') (('which', 'four climatological seasos'), 'come') (('accident where description', 'accident 100 low potential level'), '23') (('we', 'scoring function'), 'capture') (('you', 'limitations'), 'do') "}