{"name": "heart desease ml classificationexercises ", "full_name": " h2 If you find this kernel helpful Please UPVOTES h2 Problem Definition h2 Data contains h2 READ DATA AND EXPLORING DATA h2 SOME VISUALIZATION h3 Count of disease and not desease h3 Distribution of disease and not disease with scatter h3 Distrbution of age with distplot h3 Distribution of age with boxplot h3 Dividing into age groups h3 Dividing into age groups with barplot h3 Dividing into age groups with pieplot h3 Distrubution of Age and Target with violinplot h3 sex and ca hue target with barplot h3 Sex and Oldpeak hue restecg h3 Count of target with hue sex h3 Number of people who have heart disease according to age h2 Correlation matrix heatmap h3 Interpretation of heatmap h3 correlation only with target and other variables h2 Target and Thalech h3 Interpretation h4 CONCLUSION OF VISUALIZATION h2 LETS NORMALIZE THE VARIABLES h3 Normalization h1 LETS TRY CLASSIFICATIONS METHODS h2 1 LOGISTIC REGRESSION h3 A Train test splitting h3 B Modeling of Logistic R Method h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h2 2 NAIVE BAYES METHOD h3 A Train test splitting h3 B Modeling of Naive B Method h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h2 3 KNN METHOD h3 A Train test splitting h3 B Modeling of KNN Medhod h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 Look at accuracy score h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h3 Conclusion KNN h2 4 SVM SUPPORT VECTOR MACHINES h3 A Train test splitting h3 B Modeling of SVM Medhod h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h3 E TUNING THE PREDICTION WE can tune our prediction h4 Tuning1 change C and gamma h4 Tuning2 changing kernel linear c 100 h3 Conclusion h2 5 RANDOM FOREST METHOD h3 A Train test splitting h3 B Modeling of SVM Medhod h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h3 Lets look at importance 6 variables h4 Conclusion h2 6 DECISION TREE METHOD h3 A Train test splitting h3 B Modeling of Decision Tree h3 C Lets control the succes score prediction accuracy score confusion m on test data h3 D Model tuning h3 Lets look at 6 importance variables h2 LETS SEE ALL SCORE OF CLASSIFICATIONS METHODS h2 FINISH ", "stargazers_count": 0, "forks_count": 0, "description": "There are many kinds of metric in KNN. correlation only with target and other variables Target and Thalech Interpretation We can see that those people suffering from heart disease target 1 have relatively higher heart rate thalach as compared to people who are not suffering from heart disease target 0. nb_tuned_bestscore 89 and cmnb_best are our best best score and our best confusion matrix 3 KNN METHOD In pattern recognition the k Nearest Neighbors algorithm or k NN for short is a non parametric method used for classification and regression. SVM is used fo both regression and classification problems but generally for classification. So we need to try C parameter to find best value. Logistic regression measures the relationship between the categorical dependent variable feature and one or more independent variables features by estimating probabilities using a logistic function which is the cumulative logistic distribution. SVM i\u00e7erisinde C parametresi vard\u0131r ve C parametresinin default de\u011feri 1 dir. We ll train it find the patterns on the training set. We can call this small data set. CONCLUSION OF VISUALIZATIONFindings of Bivariate Analysis are as follows There is no variable which has strong positive correlation with target variable. si 1 olma olasiligi oranlari KNeighborsClassifier algorithm auto leaf_size 30 metric minkowski metric_params None n_jobs None n_neighbors 3 p 2 weights uniform Hepsi icin yapilabilir ilk 10 datatest deki tahminlerimiz 1. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. There is a C parameter inside the SVM algoritma and the default value of C parameter is 1. E TUNING THE PREDICTION WE can tune our prediction Look at c kernel gamma Tuning1 change C and gamma Tuning2 changing kernel linear c 100Tuning3 changing kernel rbf c 100 Conclusion svm_score1 84 is the best score and c_svm is the best confusion matrix 5 RANDOM FOREST METHODRandom Forests is one of the most popular model. C nin k\u00fc\u00e7\u00fck olmas\u0131 yanl\u0131\u015f s\u0131n\u0131fland\u0131rmaya neden olur. E TUNING THE PREDICTION WE can tune our prediction n_estimators importance variables Lets look at importance 6 variables Conclusion rf2_score 84 is the best score and c_rf2 is the best confusion matrix 6 DECISION TREE METHODThis model uses a Decision Tree as a predictive model which maps features tree branches to conclusions about the target value tree leaves. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. A Train test splitting B Modeling of Logistic R. target and thalach variable are also mildly positively correlated correlation coefficient 0. The people suffering from heart disease target 1 have relatively higher heart rate thalach as compared to people who are not suffering from heart disease target 0. If we use n_neighbors 21 we can obtain best score. target and exang variable are mildly negatively correlated correlation coefficient 0. predict x_test cok uzun suruyor we changed the kernel We can use linear poly rbf. 0 n_estimators 100 n_jobs None oob_score False random_state None verbose 0 warm_start False Hepsi icin yapilabilir yukarda import edildi ilk 10 datatest deki tahminlerimiz 1. If C is big it causes ovetfitting. Given a set of training samples each marked as belonging to one or the other of two categories an SVM training algorithm builds a model that assigns new test samples to one category or the other making it a non probabilistic binary linear classifier. confusion matrixle tahmin etme sayilarini bulduk 1 icin 31 i dogru tahmin 0 icin 35 i dogru tahmin En ustte import edildi. We can see that the thalach variable is slightly negatively skewed. on test_data HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap. And we ll test it use the patterns on the test set. A Train test splitting B Modeling of SVM Medhod C Lets control the succes score prediction accuracy_score confusion m. 0001 degree 9 kernel poly svm_tune1. si 1 olma olasiligi oranlari Hepsi icin yapilabilir tree_cv_model. If we change metric and use tuned n neigbors acurracy_score is best. si 1 olma olasiligi oranlari ERROR ON TRAIN DATA We use Grid for tuning we obta cross validation yontemi kullaniliyor. A Train test splitting B Modeling of Decision Tree C Lets control the succes score prediction accuracy_score confusion m. 0 class_weight None criterion gini max_depth None max_features auto max_leaf_nodes None max_samples None min_impurity_decrease 0. Finally we can say our test point belongs to the class. CLASSICICATION REPORT we can also see classification report. We re going to try machine learning models 1 Logistic Regression 2 K Nearest Neighbours Classifier 3 Support Vector machine 4 Decision Tree Classifier 5 Random Forest Classifier 1 LOGISTIC REGRESSION Logistic Regression is a useful model to run early in the workflow. K 1 SECERSEK OVERFITTING OLABILIR K BUYUK SECERSEK UNDERFITTING OLABILIR A Train test splitting B Modeling of KNN Medhod C Lets control the succes score prediction accuracy_score confusion m. LETS NORMALIZE THE VARIABLES Normalization LETS TRY CLASSIFICATIONS METHODS Now we ve got our data split into training and test sets it s time to build a machine learning model. Decision tree builds classification or regression models in the form of a tree structure. While choosing k number we have to be carefull because small k value causes overfitting while big k value causes underfitting. A sample is classified by a majority vote of its neighbors with the sample being assigned to the class most common among its k nearest neighbors k is a positive integer typically small. target and ca variable are weakly negatively correlated correlation coefficient 0. This is one of ensamble method which uses multiple classes to predict the target and very powerfull technique. predict x_test accuracy_score y_test y_pred uzun suruyor RandomForestClassifier bootstrap True ccp_alpha 0. on test_data D Model tuning Lets look at 6 importance variables LETS SEE ALL SCORE OF CLASSIFICATIONS METHODS FINISH This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. Problem Definition Given clinical parameters about a patient can we predict whether or not they have heart disease Data contains age age in years sex 1 male 0 female cp chest pain type gogus agrisi tipi trestbps resting blood pressure in mm Hg on admission to the hospital kan basinci chol serum cholestoral in mg dl mg dl cinsinden serum kolesterol\u00fc fbs fasting blood sugar 120 mg dl 1 true 0 false restecg resting electrocardiographic results dinlenme elektrokardiyografik sonu\u00e7lar\u0131 thalach maximum heart rate achieved ula\u015f\u0131lan maksimum kalp at\u0131\u015f h\u0131z\u0131 exang exercise induced angina 1 yes 0 no egzersize ba\u011fl\u0131 anjina 1 evet 0 hay\u0131r oldpeak ST depression induced by exercise relative to rest dinlenmeye g\u00f6re egzersizin neden oldu\u011fu ST depresyonu slope the slope of the peak exercise ST segment en y\u00fcksek egzersiz ST segmentinin e\u011fimi ca number of major vessels 0 3 colored by flourosopy thal 3 normal 6 fixed defect 7 reversable defect target have disease or not 1 yes 0 no hastal\u0131\u011f\u0131 var m\u0131 yok mu 1 evet 0 hay\u0131r READ DATA AND EXPLORING DATA SOME VISUALIZATION Count of disease and not desease Distribution of disease and not disease with scatter Distrbution of age with distplot Distribution of age with boxplot Dividing into age groups Dividing into age groups with barplot There are a few young ages Dividing into age groups with pieplot Distrubution of Age and Target with violinplot sex and ca hue target with barplot Sex and Oldpeak hue restecg Count of target with hue sex Number of people who have heart disease according to age Correlation matrix heatmap Interpretation of heatmapFrom the above correlation heat map we can conclude that target and cp variable are mildly positively correlated correlation coefficient 0. E TUNING THE PREDICTION WE can tune our prediction If we tune our data for nb it increase a little. read_csv For data visualization Plotly for interactive graphics Disabling warnings Input data files are available in the read only. A Train test splitting B Modeling of Naive B. target and slope variable are weakly positively correlated correlation coefficient 0. Dogru tahmin etme yuzdesi bulunuyor We found the numbers of guessing with confusion matrix 31 for 1 correct guess 0 for 35 correct guess The top was imported. The cp and thalach variables are mildly positively correlated with target variable. LOOK AT ALL PREDICTION VALUE ON TEST DATA E TUNING THE PREDICTION WE can tune our prediction We can see If we change our condition for probobilty our prediction and confusion matrix and accuracy_score change 2 NAIVE BAYES METHOD In machine learning Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes theorem with strong naive independence assumptions between the features. Random forests or random decision forests are an ensemble learning method for classification regression and other tasks that operate by constructing a multitude of decision trees n_estimators 100 300 at training time and outputting the class that is the mode of the classes classification or mean prediction regression of the individual trees This methods basically use multiple number of decision trees and take the avarage of the results of these decision trees. on test_data Look at accuracy_score HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap. If C is small it causes the misclassification. Method C Lets control the succes score prediction accuracy_score confusion m. Bu y\u00fczden en iyi de\u011feri bulmak i\u00e7in C parametresini denememiz gerekiyor. fit x_train y_train y_pred svc_tuned. Conclusion KNN knn_tuned_bestscore 85 and cmknn_best are our best best score and our best confusion matrix 4 SVM SUPPORT VECTOR MACHINES Support Vector Machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Decision tree classification can be used for both binary and multi classes Coding is the same for all supervised classes and we jus need to change the last part of the code. Hepsi icin yapilabilir ilk 10 datatest deki tahminlerimiz 1. 0001 kernel linear svc_tuned. E TUNING THE PREDICTION WE can tune our prediction we can tune n_neigbors metric. And we use this avarage to determine the class of the test point. we changed the kernel We can use linear poly rbf. svc_tuned SVC C 100 gamma 0. 0 class_weight None dual False fit_intercept True intercept_scaling 1 l1_ratio None max_iter 100 multi_class auto n_jobs None penalty l2 random_state None solver liblinear tol 0. And we put the most important one to the top of the related tree. Number of people who have heart disease according to age Let s make our correlation matrix a little prettier with jitter with boxplot We can see what there is in lr icinde hangi secenekler vargormek icin LogisticRegression C 1. fit x_train y_train y_pred svm. C b\u00fcy\u00fckse overfitting e neden olur. There is no variable which has strong negative correlation with target variable. 0001 verbose 0 warm_start False sabit katsayi degisken katsayilari The y predicted by the y in the test are compared test deki y ile tahmin edilen yler karsilastiriliyor. si 1 olma olasiligi oranlari we can look at which option is there in GaussionNB nb confusion matrixle tahmin etme sayilarini bulduk 1 icin 32 i dogru tahmin 0 icin 30 i dogru tahmin En ustte import edildi. Coding is the same for all supervised classes and we jus need to change the last part of the code. If you find this kernel helpful Please UPVOTES. 0 decision_function_shape ovr degree 9 gamma scale kernel poly max_iter 1 probability False random_state None shrinking True tol 0. According to \u0131nformation entropy we can determine which feature is the most important. Tree models where the target variable can take a finite set of values are called classification trees in these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels. There is no correlation between target and fbs. we tune the knn than our score increase. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session chose randon sample from row DISTRUBUTION OF AGE WITH DISTPLOT DISTRUBUTION OF AGE WITH BOXPLOT we can see in pie. Naive Bayes classifiers are highly scalable requiring a number of parameters linear in the number of variables features in a learning problem. LOOK AT ALL PREDICTION VALUE ON TEST DATA. on test_data CLASSICICATION REPORT we can also see classification report. nesnesi tanimlandi Hepsi icin yapilabilir svm SVC C 5 break_ties False cache_size 200 class_weight None coef0 0. 001 verbose False Hepsi icin yapilabilir EN UYGUN C VE GAMMA DEGERI BULMA svm_tune1 SVC C 100 gamma 0. It means that we chose k number of points of classes which are nearest to the out test point. 0 min_impurity_split None min_samples_leaf 1 min_samples_split 2 min_weight_fraction_leaf 0. SVM hem regresyon hem de s\u0131n\u0131fland\u0131rma problemleri i\u00e7in kullan\u0131l\u0131r ancak genellikle s\u0131n\u0131fland\u0131rma i\u00e7in kullan\u0131l\u0131r. Decision trees where the target variable can take continuous values typically real numbers are called regression trees. target and thal variable are also waekly negatively correlated correlation coefficient 0. Lojistik regresyon k\u00fcm\u00fclatif lojistik da\u011f\u0131l\u0131m olan bir lojistik fonksiyon kullanarak olas\u0131l\u0131klar\u0131 tahmin ederek kategorik ba\u011f\u0131ml\u0131 de\u011fi\u015fken \u00f6zellik ile bir veya daha fazla ba\u011f\u0131ms\u0131z de\u011fi\u015fken \u00f6zellik aras\u0131ndaki ili\u015fkiyi \u00f6l\u00e7er. In this method we need to choose k value. target and oldpeak variable are also mildly negatively correlated correlation coefficient 0. We count the number of classes in the small dataset and determine the highest number of class. ", "id": "ozericyer/heart-desease-ml-classificationexercises", "size": "12098", "language": "python", "html_url": "https://www.kaggle.com/code/ozericyer/heart-desease-ml-classificationexercises", "git_url": "https://www.kaggle.com/code/ozericyer/heart-desease-ml-classificationexercises", "script": "classification_report train_test_split init_notebook_mode confusion_matrix accuracy_score cross_val_score numpy sklearn.svm seaborn confusion_matrix   #Hepsi icin yapilabilir SVC KNeighborsRegressor GaussianNB sklearn.neighbors sklearn.naive_bayes RandomForestClassifier  #n_estimotors=11 is best sklearn.tree roc_auc_score sklearn sklearn.linear_model matplotlib.pyplot BaggingRegressor DecisionTreeClassifier plotly.graph_objs sklearn.model_selection pandas DecisionTreeRegressor plotly.offline roc_curve iplot r2_score LogisticRegression RandomForestClassifier scale model_selection ShuffleSplit mean_squared_error matplotlib KNeighborsClassifier GridSearchCV sklearn.metrics sklearn.ensemble sklearn.preprocessing ", "entities": "(('UYGUN C VE GAMMA DEGERI', 'SVC svm_tune1 C 100 gamma'), 'verbose') (('default value', 'C parameter'), 'be') (('CLASSICICATION we', 'classification also report'), 'see') (('it', 'one category'), 'mark') (('we', 'n neigbors tuned acurracy_score'), 'be') (('SVM i\u00e7erisinde parametresi vard\u0131r ve C parametresinin C default', '1 dir'), 'de\u011feri') (('maps', 'target value tree leaves'), 'tuning') (('Decision tree', 'tree structure'), 'build') (('test point', 'class'), 'say') (('We', 'poly linear rbf'), 'change') (('target', 'correlation weakly negatively coefficient'), 'correlate') (('we', 'code'), 'be') (('i', '0 35'), 'icin') (('Logistic Nearest Support LOGISTIC REGRESSION Logistic 1 Regression 2 Neighbours Classifier 3 Vector machine 4 Decision Tree 5 Random Forest Classifier 1 Regression', 'useful early workflow'), 'go') (('We', 'class'), 'count') (('SVM', 'classification generally classification'), 'use') (('We', 'Heatmap'), 'on') (('We', 'poly linear rbf'), 'predict') (('we', 'metric'), 'tuning') (('kernel', 'UPVOTES'), 'find') (('warm_start False 0 Hepsi', 'yapilabilir yukarda import edildi ilk'), 'n_estimators') (('It', 'kaggle python Docker image https github'), 'look') (('SECERSEK OLABILIR K BUYUK K 1 UNDERFITTING', 'KNN Medhod C Lets'), 'overfitte') (('nb_tuned_bestscore 89', 'k short non parametric classification'), 'be') (('Naive Bayes classifiers', 'features'), 'at') (('we', 'test point'), 'use') (('which', 'target'), 'be') (('methods', 'decision trees'), 'be') (('we', 'pie'), 'list') (('we', 'classification also report'), 'see') (('oranlari KNeighborsClassifier algorithm auto minkowski None p weights si 1 olma olasiligi leaf_size 30 metric n_neighbors 3 2 Hepsi', 'yapilabilir ilk'), 'metric_param') (('which', 'target variable'), 'be') (('who', 'heart disease target'), 'correlation') (('Input data files', 'read'), 'be') (('which', 'out test point'), 'mean') (('cp variable', 'correlation mildly positively coefficient'), 'Definition') (('Hepsi', 'yapilabilir ilk'), 'icin') (('we', 'best score'), 'obtain') (('that', 'class labels'), 'call') (('k big value', 'underfitting'), 'choose') (('target variable', 'correlation weakly positively coefficient'), 'correlate') (('SVM hem regresyon hem de s\u0131n\u0131fland\u0131rma problemleri i\u00e7in kullan\u0131l\u0131r ancak', 's\u0131n\u0131fland\u0131rma i\u00e7in kullan\u0131l\u0131r'), 'genellikle') (('we', 'k value'), 'need') (('target', 'exang correlation mildly negatively coefficient'), 'correlate') (('it', 'test set'), 'test') (('that', 'classification analysis'), 'be') (('k', 'most common k nearest neighbors'), 'classify') (('gamma scale kernel False 0 decision_function_shape ovr degree 9 poly max_iter 1 None', 'True'), 'probability') (('Method C Lets', 'succes score prediction accuracy_score confusion m.'), 'control') (('icinde hangi secenekler vargormek', 'LogisticRegression C'), 'number') (('So we', 'best value'), 'need') (('it', 'training set'), 'train') (('it', 'misclassification'), 'cause') (('cp', 'thalach target mildly positively variable'), 'correlate') (('target', 'correlation oldpeak also mildly negatively coefficient'), 'correlate') (('we', 'code'), 'use') (('feature', '\u0131nformation entropy'), 'determine') (('bir veya daha \u00f6zellik ile fazla', 'aras\u0131ndaki ili\u015fkiyi de\u011fi\u015fken \u00f6zellik \u00f6l\u00e7er'), 'de\u011fi\u015fken') (('top', '0 35 correct guess'), 'tahmin') (('it', 'nb'), 'tuning') (('who', 'heart disease target'), 'have') (('it', 'machine learning model'), 'NORMALIZE') (('typically real numbers', 'continuous values'), 'call') (('i', '0 30'), 'si') (('target', 'thalach correlation also mildly positively coefficient'), 'correlate') (('olma oranlari si 1 olasiligi Hepsi', 'yapilabilir'), 'icin') (('we', 'validation yontemi kullaniliyor'), 'datum') (('we', 'related tree'), 'put') (('ile tahmin', 'yler karsilastiriliyor'), 'verbose') (('thal target variable', 'correlation also waekly negatively coefficient'), 'correlate') (('confusion RANDOM FOREST METHODRandom 84 best best 5 Forests', 'most popular model'), 'tuning') (('which', 'logistic function'), 'measure') (('Train test', 'succes score prediction accuracy_score confusion m.'), 'control') (('Naive Bayes classifiers', 'learning problem'), 'be') (('decision associated tree', 'same time'), 'break') "}