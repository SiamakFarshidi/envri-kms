{"name": "exploring embeddings with gensim ", "full_name": " h1 Looking up embeddings h2 Vector similarity h2 Cosine Distance h1 Exploring embeddings with Gensim h1 Semantic vector math h2 Analogy solving h1 Your turn h3 P S ", "stargazers_count": 0, "forks_count": 0, "description": "Earlier we trained a model to predict the ratings users would give to movies using a network with embeddings learned for each movie and user. org wiki Cosine_similarity. Fortunately there s already a library for exactly this sort of work Gensim. This is our conventional as the crow flies notion of distance between two points. com kernels fork 1598893 to get some hands on practice working with exploring embeddings with gensim. We never directed the model about how to use any particular embedding dimension. Is our model wrong or were we Another difference we failed to account for between Cars 2 and Brave is that the former is a sequel and the latter is not. our embeddings manage to nail a wide variety of cinematic niches Semantic vector mathThe most_similar https radimrehurek. Embeddings can be thought of as a smart distance metric. Your turn Head over to the Exercises notebook https www. com gensim models keyedvectors. Let s look at the closest neighbours for a few more movies from a variety of genres Artsy erotic dramas raunchy sophomoric comedies old school musicals superhero movies. most_similar a b then instead of finding the vector closest to a it will find the closest vector to a b. We left it alone to learn whatever representation it found useful. Mathematically we can also extend it to 32 dimensions though good luck visualizing it. While the library is most frequently used in the text domain we can use it to explore embeddings of any sort. at right angles their cosine similarity is 0. A shower is a milder form of a deluge. Cosine distance is just defined as 1 minus the cosine similarity and therefore ranges from 0 to 2. Cosine DistanceIf you check out the docs for the scipy. WordEmbeddingsKeyedVectors. Embeddings are powerful But how do they actually work Previously I claimed that embeddings capture the meaning of the objects they represent and discover useful latent structure. One interpretation would be that Brave is like Cars 2 except that the latter is aimed primarily at boys and the former might be more appealing to girls given its female protagonist. The embedding weights are part of the model s internals so we ll have to do a bit of digging around to access them. Okay I m being facetious. It s kind of astounding that this works but people have found that these can often be effectively solved by simple vector math on word embeddings. Sequelness is an important property to our model which suggests that some of the variance in our data is accounted for the fact that some people tend to like sequels more than others. most_similar method optionally takes a second argument negative. In the exercise coming up you ll get to do a little hands on investigation that digs into this question more deeply. Let s look at an example movie vector What movie is this the embedding of Let s load up our dataframe of movie metadata. Limit to movies with at least this many ratings in the dataset Split long titles over multiple lines. Comparing dimension by dimension these look vaguely similar. com 82826473884269 I d greatly appreciate it. Which movies are most similar to Toy Story Which movies fall right between Psycho and Scream in the embedding space We could write a bunch of code to work out questions like this but it d be pretty tedious. For example how would you fill in the following equation Scream Psycho ________ Scream and Psycho are similar in that they re violent scary movies somewhere on the border between Horror and Thriller. Can we solve movie analogies with our embeddings Let s try. 7 10 of our results are also sequels. How does this compare to a pair of movies that we would think of as very different As expected much further apart. What s a milder form of stare A good answer here would be glance or look. Let s put that to the test Looking up embeddingsLet s load a model we trained earlier so we can investigate the embedding weights that it learned. So I d say Scream is what you d get if you combined Psycho with a comedy. Analogy solvingThe SAT test which is used to get into American colleges and universities poses analogy questions like shower deluge _____ stare Read shower is to deluge as ___ is to stare To solve this we find the relationship between deluge and shower and apply it to stare. Aside You may notice that Gensim s docs and many of its class and method names refer to word embeddings. But we can actually ask Gensim to fill in the blank for us via vector math after some rearranging ________ Scream PsychoIf you are familiar with these movies you ll see that the missing ingredient that takes us from Psycho to Scream is comedy and also late 90 s teen movie ness. What about Brave Cars 2 Pocahontas _____ The answer is not clear. In terms of vector math we can frame this as. If they re orthogonal i. Okay so which movies are most similar to Toy Story Wow these are pretty great It makes perfect sense that Toy Story 2 is the most similar movie to Toy Story. You can also leave public feedback in the comments below or on the Learn Forum https www. If they point in opposite directions it s 1. We ll grab the layer responsible for embedding movies and use the get_weights method to get its learned weights. This course is still in beta so I d love to get your feedback. If you have a moment to fill out a super short survey about this lesson https form. Easy to grok in 1 2 or 3 dimensions. Let s calculate a couple cosine distances between movie vectors Aside Why is cosine distance commonly used when working with embeddings The short answer as with so many deep learning techniques is empirically it works well. After that the results are a bit more perplexing. Our weight matrix has 26 744 rows for that many movies. When judging the similarity of embeddings it s more common to use cosine similarity https en. The biggest difference is that Scream has elements of comedy. So how do we check whether these representations are sane and coherent Vector similarityA simple way to test this is to look at how close or distant pairs of movies are in the embedding space. And most of the rest are animated kids movies with a similar computer animated style. If we wanted to assign a single number to their similarity we could calculate the euclidean distance between these two vectors. This tells us something interesting about our learned embeddings and ultimately about the problem of predicting movie preferences. WordEmbeddingsKeyedVectors with our model s movie embeddings and the titles of the corresponding movies. spatial module https docs. So it s learned something about 3 d animated kids flick but maybe that was just a fluke. Of course it s Toy Story I should have recognized that vector anywhere. If two vectors point in the same direction their cosine similarity is 1. html you ll see there are actually a lot of different measures of distance that people use for different tasks. Exploring embeddings with GensimI ll instantiate an instance of WordEmbeddingsKeyedVectors https radimrehurek. In brief the cosine similarity of two vectors ranges from 1 to 1 and is a function of the angle between the vectors. It s hard to make anything of these vectors at this point. If our embedding matrix is any good it should map similar movies like Toy Story and Shrek to similar vectors. Why would you want to do that It turns out that doing addition and subtraction of embedding vectors often gives surprisingly meaningful results. So maybe the answer should be like Pocahontas a mid 90 s conventional animation kids movie but more of a boy movie. Each row is 32 numbers the size of our movie embeddings. Cars 2 Brave X _____ Pocahontas X Rearranging we get ____ Pocahontas Cars 2 Brave We can solve this by passing in two movies Pocahontas and Cars 2 for the positive argument to most_similar with Brave as the negative argument This weakly fits our prediction the 4 closest movies are indeed kids animated movies from the 90s. Hercules The Lion King Let s ask our embeddings what they think. ", "id": "colinmorris/exploring-embeddings-with-gensim", "size": "8504", "language": "python", "html_url": "https://www.kaggle.com/code/colinmorris/exploring-embeddings-with-gensim", "git_url": "https://www.kaggle.com/code/colinmorris/exploring-embeddings-with-gensim", "script": "tensorflow WordEmbeddingsKeyedVectors matplotlib gensim.models.keyedvectors pyplot pyplot as plt keras distance plot_most_similar pandas scipy.spatial numpy ", "entities": "(('we', 'around them'), 'be') (('latter', 'Cars'), 'be') (('it', 'that'), 'let') (('they', 'Horror'), 'fill') (('doing', 'often surprisingly meaningful results'), 'want') (('row', 'movie 32 embeddings'), 'be') (('we', 'it'), 'pose') (('So maybe answer', 'animation kids boy mid 90 s conventional movie'), 'be') (('weight matrix', 'many movies'), 'have') (('docs', 'word embeddings'), 'notice') (('that', 'question'), 'get') (('empirically it', 'short answer'), 'let') (('Exploring', 'https radimrehurek'), 'instantiate') (('results', 'that'), 'be') (('we', 'two vectors'), 'calculate') (('Embeddings', 'distance smart metric'), 'think') (('cosine similarity', 'right angles'), 'be') (('We', 'learned weights'), 'grab') (('course', 'feedback'), 'be') (('Toy Story', 'Toy 2 most similar Story'), 'be') (('it', 'opposite directions'), 's') (('these', 'word embeddings'), 's') (('you', 'lesson https form'), 'have') (('cosine similarity', 'vectors'), 'range') (('we', 'that'), 'compare') (('s', 'movie metadata'), 'let') (('Mathematically we', 'it'), 'extend') (('Cosine DistanceIf you', 'scipy'), 'check') (('about 3 d animated maybe that', 'something'), 'learn') (('we', 'this'), 'frame') (('it', 'similar vectors'), 'map') (('it', 'b.'), 'find') (('s', 'milder stare good answer'), 'be') (('answer', 'Pocahontas _ _ _ _ Brave Cars 2 _'), 'about') (('people', 'more others'), 'be') (('You', 'Learn Forum https www'), 'leave') (('people', 'different tasks'), 'see') (('they', 'useful latent structure'), 'be') (('Scream', 'comedy'), 'be') (('crow', 'two points'), 'be') (('We', 'embedding how particular dimension'), 'direct') (('4 closest movies', '90s'), 'car') (('that', 'Scream'), 'ask') (('most_similar method', 'optionally second argument'), 'take') (('Toy I', 'vector'), 's') (('it', 'cosine similarity more https'), 's') (('you', 'comedy'), 'say') (('former', 'female protagonist'), 'be') (('how close pairs', 'embedding space'), 'check') (('we', 'sort'), 'use') (('embeddings', 'niches Semantic cinematic vector'), 'manage') (('it', 'representation'), 'leave') (('these', 'dimension'), 'look') (('they', 'what'), 'let') (('s', 'embeddings'), 'solve') (('This', 'movie preferences'), 'tell') (('ratings users', 'movie'), 'train') (('It', 'point'), 's') (('d', 'greatly it'), 'com') (('cosine similarity', 'same direction'), 'be') (('Cosine distance', '2'), 'define') (('Artsy erotic dramas', 'school musicals superhero old movies'), 'let') (('We', 'it'), 'be') (('most', 'similar computer animated style'), 'animate') "}