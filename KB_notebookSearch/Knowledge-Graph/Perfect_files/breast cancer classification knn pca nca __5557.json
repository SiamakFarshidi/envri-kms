{"name": "breast cancer classification knn pca nca ", "full_name": " h1 Breast Cancer Wisconsin Diagnostic n Introduction nFeatures are computed from a digitized image of a fine needle aspirate FNA of a breast mass They describe characteristics of the cell nuclei present in the image nn the 3 dimensional space is that described in K P Bennett and O L Mangasarian Robust Linear Programming Discrimination of Two Linearly Inseparable Sets Optimization Methods and Software 1 1992 23 34 n nThis database is also available through the UW CS ftp server nftp ftp cs wisc edu ncd math prog cpo dataset machine learn WDBC n nAlso can be found on UCI Machine Learning Repository https archive ics uci edu ml datasets Breast Cancer Wisconsin 28Diagnostic 29 n n n nContent n n1 Prepare Problems 1 n Load Libraries 2 n Load Dataset 3 n1 Descriptive Analysis 4 n1 EDA 5 n1 Missing Values 6 n1 Data Visualization 7 n Count Plot 8 n Pie Chart 9 n Distribution Plot 10 n n1 Outlier Detection 11 n Let s The Outliers via Bubble Chart 12 n1 Drop Outliers 13 n1 Create Train and Test Dataset 14 n1 Standardization 15 n1 KNN Model 16 n KNN Tuning 17 n Make Prediction After Tuning 18 n n1 Principal Component Analysis PCA 19 n Visualize Of New Dataframe 20 n Classification After PCA 21 n1 Neighborhood Components Analysis NCA 22 n Visualize Of New Dataframe 23 n Classification After NCA 24 n1 Compare Accuracies 25 n n nPrepare Problems n n Predict whether the cancer is benign or malignant n n Load Libraries n n Load Dataset nAttribute Information n n 1 ID number n 2 Diagnosis M malignant B benign n3 32 n nTen real valued features are computed for each cell nucleus n n radius mean of distances from center to points on the perimeter n texture standard deviation of gray scale values n perimeter n area n smoothness local variation in radius lengths n compactness perimeter 2 area 1 0 n concavity severity of concave portions of the contour n concave points number of concave portions of the contour n symmetry n fractal dimension coastline approximation 1 n nThe mean standard error and worst or largest mean of the three nlargest values of these features were computed for each image nresulting in 30 features For instance field 3 is Mean Radius field n13 is Radius SE field 23 is Worst Radius n nAll feature values are recoded with four significant digits n nMissing attribute values none n nClass distribution 357 benign 212 malignant n n Descriptive Analysis n n EDA n n Missing Values n n Data Visualization n n Count Plot n n Pie Chart n n Distribution Plot n You can make more lots via dist plot n n Outlier Detection n Outlier detection with Local Outlier Factor LOF nThe Local Outlier Factor LOF algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors It considers as outliers the samples that have a substantially lower density than their neighbors This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit learn Note that when LOF is used for outlier detection it has no predict decision function and score samples methods See User Guide for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection n nThe number of neighbors considered parameter n neighbors is typically set 1 greater than the minimum number of samples a cluster has to contain so that other samples can be local outliers relative to this cluster and 2 smaller than the maximum number of close by samples that can potentially be local outliers In practice such informations are generally not available and taking n neighbors 20 appears to work well in general nTo get https scikit learn org stable auto examples neighbors plot lof outlier detection html n https scikit learn org stable images sphx glr plot lof outlier detection 001 png n property fit predict n is inlierarray shape n samples n Returns 1 for anomalies outliers and 1 for inliers n n Let s The Outliers via Bubble Chart n n Drop Outliers n n Create Train and Test Dataset n n Standardization n n KNN Model n Sentisitive for outliers n It is problem on big data n Curse of Dimensionality n Feature Scaling n It is problem on imbalance data n Depends on K model will check K nearst neighbour n n KNN Tuning n n Make Prediction After Tuning n n Principal Component Analysis PCA n n Visualize Of New Dataframe n n Classification After PCA n Prepare X and Y n KNN Model via PCA Features n Let s Which Points are in the correct area n n Neighborhood Components Analysis NCA n n Visualize Of New Dataframe n n Classification After NCA n KNN Model via NCA Features n Let s Which Points are in the correct area n n Compare Accuracies h1 n Introduction nFeatures are computed from a digitized image of a fine needle aspirate FNA of a breast mass They describe characteristics of the cell nuclei present in the image nn the 3 dimensional space is that described in K P Bennett and O L Mangasarian Robust Linear Programming Discrimination of Two Linearly Inseparable Sets Optimization Methods and Software 1 1992 23 34 n nThis database is also available through the UW CS ftp server nftp ftp cs wisc edu ncd math prog cpo dataset machine learn WDBC n nAlso can be found on UCI Machine Learning Repository https archive ics uci edu ml datasets Breast Cancer Wisconsin 28Diagnostic 29 n n n nContent n n1 Prepare Problems 1 n Load Libraries 2 n Load Dataset 3 n1 Descriptive Analysis 4 n1 EDA 5 n1 Missing Values 6 n1 Data Visualization 7 n Count Plot 8 n Pie Chart 9 n Distribution Plot 10 n n1 Outlier Detection 11 n Let s The Outliers via Bubble Chart 12 n1 Drop Outliers 13 n1 Create Train and Test Dataset 14 n1 Standardization 15 n1 KNN Model 16 n KNN Tuning 17 n Make Prediction After Tuning 18 n n1 Principal Component Analysis PCA 19 n Visualize Of New Dataframe 20 n Classification After PCA 21 n1 Neighborhood Components Analysis NCA 22 n Visualize Of New Dataframe 23 n Classification After NCA 24 n1 Compare Accuracies 25 n n nPrepare Problems n n Predict whether the cancer is benign or malignant n n Load Libraries n n Load Dataset nAttribute Information n n 1 ID number n 2 Diagnosis M malignant B benign n3 32 n nTen real valued features are computed for each cell nucleus n n radius mean of distances from center to points on the perimeter n texture standard deviation of gray scale values n perimeter n area n smoothness local variation in radius lengths n compactness perimeter 2 area 1 0 n concavity severity of concave portions of the contour n concave points number of concave portions of the contour n symmetry n fractal dimension coastline approximation 1 n nThe mean standard error and worst or largest mean of the three nlargest values of these features were computed for each image nresulting in 30 features For instance field 3 is Mean Radius field n13 is Radius SE field 23 is Worst Radius n nAll feature values are recoded with four significant digits n nMissing attribute values none n nClass distribution 357 benign 212 malignant n n Descriptive Analysis n n EDA n n Missing Values n n Data Visualization n n Count Plot n n Pie Chart n n Distribution Plot n You can make more lots via dist plot n n Outlier Detection n Outlier detection with Local Outlier Factor LOF nThe Local Outlier Factor LOF algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors It considers as outliers the samples that have a substantially lower density than their neighbors This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit learn Note that when LOF is used for outlier detection it has no predict decision function and score samples methods See User Guide for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection n nThe number of neighbors considered parameter n neighbors is typically set 1 greater than the minimum number of samples a cluster has to contain so that other samples can be local outliers relative to this cluster and 2 smaller than the maximum number of close by samples that can potentially be local outliers In practice such informations are generally not available and taking n neighbors 20 appears to work well in general nTo get https scikit learn org stable auto examples neighbors plot lof outlier detection html n https scikit learn org stable images sphx glr plot lof outlier detection 001 png n property fit predict n is inlierarray shape n samples n Returns 1 for anomalies outliers and 1 for inliers n n Let s The Outliers via Bubble Chart n n Drop Outliers n n Create Train and Test Dataset n n Standardization n n KNN Model n Sentisitive for outliers n It is problem on big data n Curse of Dimensionality n Feature Scaling n It is problem on imbalance data n Depends on K model will check K nearst neighbour n n KNN Tuning n n Make Prediction After Tuning n n Principal Component Analysis PCA n n Visualize Of New Dataframe n n Classification After PCA n Prepare X and Y n KNN Model via PCA Features n Let s Which Points are in the correct area n n Neighborhood Components Analysis NCA n n Visualize Of New Dataframe n n Classification After NCA n KNN Model via NCA Features n Let s Which Points are in the correct area n n Compare Accuracies h2 Prepare Problems n n Predict whether the cancer is benign or malignant n n Load Libraries n n Load Dataset nAttribute Information n n 1 ID number n 2 Diagnosis M malignant B benign n3 32 n nTen real valued features are computed for each cell nucleus n n radius mean of distances from center to points on the perimeter n texture standard deviation of gray scale values n perimeter n area n smoothness local variation in radius lengths n compactness perimeter 2 area 1 0 n concavity severity of concave portions of the contour n concave points number of concave portions of the contour n symmetry n fractal dimension coastline approximation 1 n nThe mean standard error and worst or largest mean of the three nlargest values of these features were computed for each image nresulting in 30 features For instance field 3 is Mean Radius field n13 is Radius SE field 23 is Worst Radius n nAll feature values are recoded with four significant digits n nMissing attribute values none n nClass distribution 357 benign 212 malignant n n Descriptive Analysis n n EDA n n Missing Values n n Data Visualization n n Count Plot n n Pie Chart n n Distribution Plot n You can make more lots via dist plot n n Outlier Detection n Outlier detection with Local Outlier Factor LOF nThe Local Outlier Factor LOF algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors It considers as outliers the samples that have a substantially lower density than their neighbors This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit learn Note that when LOF is used for outlier detection it has no predict decision function and score samples methods See User Guide for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection n nThe number of neighbors considered parameter n neighbors is typically set 1 greater than the minimum number of samples a cluster has to contain so that other samples can be local outliers relative to this cluster and 2 smaller than the maximum number of close by samples that can potentially be local outliers In practice such informations are generally not available and taking n neighbors 20 appears to work well in general nTo get https scikit learn org stable auto examples neighbors plot lof outlier detection html n https scikit learn org stable images sphx glr plot lof outlier detection 001 png n property fit predict n is inlierarray shape n samples n Returns 1 for anomalies outliers and 1 for inliers n n Let s The Outliers via Bubble Chart n n Drop Outliers n n Create Train and Test Dataset n n Standardization n n KNN Model n Sentisitive for outliers n It is problem on big data n Curse of Dimensionality n Feature Scaling n It is problem on imbalance data n Depends on K model will check K nearst neighbour n n KNN Tuning n n Make Prediction After Tuning n n Principal Component Analysis PCA n n Visualize Of New Dataframe n n Classification After PCA n Prepare X and Y n KNN Model via PCA Features n Let s Which Points are in the correct area n n Neighborhood Components Analysis NCA n n Visualize Of New Dataframe n n Classification After NCA n KNN Model via NCA Features n Let s Which Points are in the correct area n n Compare Accuracies h2 n n Predict whether the cancer is benign or malignant n n Load Libraries n n Load Dataset nAttribute Information n n 1 ID number n 2 Diagnosis M malignant B benign n3 32 n nTen real valued features are computed for each cell nucleus n n radius mean of distances from center to points on the perimeter n texture standard deviation of gray scale values n perimeter n area n smoothness local variation in radius lengths n compactness perimeter 2 area 1 0 n concavity severity of concave portions of the contour n concave points number of concave portions of the contour n symmetry n fractal dimension coastline approximation 1 n nThe mean standard error and worst or largest mean of the three nlargest values of these features were computed for each image nresulting in 30 features For instance field 3 is Mean Radius field n13 is Radius SE field 23 is Worst Radius n nAll feature values are recoded with four significant digits n nMissing attribute values none n nClass distribution 357 benign 212 malignant n n Descriptive Analysis n n EDA n n Missing Values n n Data Visualization n n Count Plot n n Pie Chart n n Distribution Plot n You can make more lots via dist plot n n Outlier Detection n Outlier detection with Local Outlier Factor LOF nThe Local Outlier Factor LOF algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors It considers as outliers the samples that have a substantially lower density than their neighbors This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit learn Note that when LOF is used for outlier detection it has no predict decision function and score samples methods See User Guide for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection n nThe number of neighbors considered parameter n neighbors is typically set 1 greater than the minimum number of samples a cluster has to contain so that other samples can be local outliers relative to this cluster and 2 smaller than the maximum number of close by samples that can potentially be local outliers In practice such informations are generally not available and taking n neighbors 20 appears to work well in general nTo get https scikit learn org stable auto examples neighbors plot lof outlier detection html n https scikit learn org stable images sphx glr plot lof outlier detection 001 png n property fit predict n is inlierarray shape n samples n Returns 1 for anomalies outliers and 1 for inliers n n Let s The Outliers via Bubble Chart n n Drop Outliers n n Create Train and Test Dataset n n Standardization n n KNN Model n Sentisitive for outliers n It is problem on big data n Curse of Dimensionality n Feature Scaling n It is problem on imbalance data n Depends on K model will check K nearst neighbour n n KNN Tuning n n Make Prediction After Tuning n n Principal Component Analysis PCA n n Visualize Of New Dataframe n n Classification After PCA n Prepare X and Y n KNN Model via PCA Features n Let s Which Points are in the correct area n n Neighborhood Components Analysis NCA n n Visualize Of New Dataframe n n Classification After NCA n KNN Model via NCA Features n Let s Which Points are in the correct area n n Compare Accuracies ", "stargazers_count": 0, "forks_count": 0, "description": "0 concavity severity of concave portions of the contour concave points number of concave portions of the contour symmetry fractal dimension coastline approximation 1 The mean standard error and worst or largest mean of the threelargest values of these features were computed for each image resulting in 30 features. Prepare Problems 1 Load Libraries 2 Load Dataset 3 1. Outlier Detection 11 Let s The Outliers via Bubble Chart 12 1. Missing Values 6 1. Principal Component Analysis PCA 19 Visualize Of New Dataframe 20 Classification After PCA 21 1. Neighborhood Components Analysis NCA 22 Visualize Of New Dataframe 23 Classification After NCA 24 1. n the 3 dimensional space is that described in K. In practice such informations are generally not available and taking n_neighbors 20 appears to work well in general. Missing attribute values noneClass distribution 357 benign 212 malignant Descriptive Analysis EDA Missing Values Data Visualization Count Plot Pie Chart Distribution Plot You can make more lots via dist_plot Outlier Detection Outlier detection with Local Outlier Factor LOF The Local Outlier Factor LOF algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. png property fit_predict is_inlierarray shape n_samples Returns 1 for anomalies outliers and 1 for inliers. edu ml datasets Breast Cancer Wisconsin 28Diagnostic 29Content 1. For instance field 3 is Mean Radius field13 is Radius SE field 23 is Worst Radius. html https scikit learn. org stable _images sphx_glr_plot_lof_outlier_detection_001. They describe characteristics of the cell nuclei present in the image. Change object to integer So make threshold we decide about max and min of outlier_score Radius for our outliers Tuning Decision Tree Model Drop Unnecessary columns Change object to integer PCA needs scaled data Build PCA \u00e7evre \u00e7izgileri visualize step size in the mesh Put the result into a color plot Plot also the training points visualize step size in the mesh Put the result into a color plot Plot also the training points Compare Model s Acc. Data Visualization 7 Count Plot 8 Pie Chart 9 Distribution Plot 10 1. educd math prog cpo dataset machine learn WDBC Also can be found on UCI Machine Learning Repository https archive. Let s The Outliers via Bubble Chart Drop Outliers Create Train and Test Dataset Standardization KNN Model Sentisitive for outliers It is problem on big data Curse of Dimensionality Feature Scaling It is problem on imbalance data Depends on K model will check K nearst neighbour KNN Tuning Make Prediction After Tuning Principal Component Analysis PCA Visualize Of New Dataframe Classification After PCA Prepare X and Y KNN Model via PCA Features Let s Which Points are in the correct area Neighborhood Components Analysis NCA Visualize Of New Dataframe Classification After NCA KNN Model via NCA Features Let s Which Points are in the correct area Compare Accuracies Load Libraries Any results you write to the current directory are saved as output. See User Guide for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection. Drop Outliers 13 1. org stable auto_examples neighbors plot_lof_outlier_detection. Compare Accuracies 25 Prepare Problems Predict whether the cancer is benign or malignant Load Libraries Load DatasetAttribute Information 1 ID number 2 Diagnosis M malignant B benign 3 32 Ten real valued features are computed for each cell nucleus radius mean of distances from center to points on the perimeter texture standard deviation of gray scale values perimeter area smoothness local variation in radius lengths compactness perimeter 2 area 1. KNN Model 16 KNN Tuning 17 Make Prediction After Tuning 18 1. All feature values are recoded with four significant digits. Standardization 15 1. Breast Cancer Wisconsin Diagnostic IntroductionFeatures are computed from a digitized image of a fine needle aspirate FNA of a breast mass. To get https scikit learn. This database is also available through the UW CS ftp server ftp ftp. This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit learn. Drop Unnecessary columns data shape column names descriptions class distribution correlation \u00fcst \u00fcste gelecek \u015fekilde. It considers as outliers the samples that have a substantially lower density than their neighbors. Descriptive Analysis 4 1. Mangasarian Robust Linear Programming Discrimination of Two Linearly Inseparable Sets Optimization Methods and Software 1 1992 23 34. The number of neighbors considered parameter n_neighbors is typically set 1 greater than the minimum number of samples a cluster has to contain so that other samples can be local outliers relative to this cluster and 2 smaller than the maximum number of close by samples that can potentially be local outliers. Create Train and Test Dataset 14 1. Note that when LOF is used for outlier detection it has no predict decision_function and score_samples methods. ", "id": "abdulmeral/breast-cancer-classification-knn-pca-nca", "size": "5557", "language": "python", "html_url": "https://www.kaggle.com/code/abdulmeral/breast-cancer-classification-knn-pca-nca", "git_url": "https://www.kaggle.com/code/abdulmeral/breast-cancer-classification-knn-pca-nca", "script": "classification_report train_test_split confusion_matrix dist_plot accuracy_score numpy cross_val_score LocalOutlierFactor seaborn NeighborhoodComponentsAnalysis ListedColormap sklearn.neighbors sklearn PCA KFold matplotlib.pyplot metrics plotly.offline plotly.graph_objs pandas matplotlib.colors sklearn.model_selection plotly.figure_factory KNeighborsClassifier GridSearchCV sklearn.decomposition sklearn.metrics StandardScaler sklearn.preprocessing ", "entities": "(('feature values', 'four significant digits'), 'recode') (('Breast Cancer Wisconsin Diagnostic IntroductionFeatures', 'breast mass'), 'compute') (('png property n_samples', 'inliers'), 'fit_predict') (('it', 'decision_function methods'), 'note') (('training also points', 'color plot'), 'object') (('database', 'UW CS ftp server ftp also ftp'), 'be') (('1 mean standard error', '30 features'), 'severity') (('edu ml', 'Breast Cancer Wisconsin'), 'dataset') (('detection unsupervised anomaly which', 'neighbors'), 'miss') (('Drop Unnecessary columns data shape column names', 'class distribution correlation \u00fcst \u00fcste gelecek \u015fekilde'), 'description') (('that', 'samples'), 'consider') (('you', 'output'), 'let') (('KNN 16 Tuning', '18 1'), 'Model') (('that', 'neighbors'), 'consider') (('They', 'present image'), 'describe') (('Load Libraries malignant benign DatasetAttribute Information 1 ID number 2 Diagnosis M B benign 3 32 Ten real valued features', 'radius lengths'), 'accuracy') (('Mean Radius 3 field13', 'instance field'), 'be') (('WDBC', 'UCI Machine Learning Repository https Also archive'), 'learn') (('that', 'K.'), 'be') (('which', 'scikit'), 'show') (('generally taking', '20'), 'be') (('Outlier Detection', 'Bubble 11 Chart'), 'let') "}