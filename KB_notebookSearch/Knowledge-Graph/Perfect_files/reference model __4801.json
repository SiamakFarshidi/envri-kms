{"name": "reference model ", "full_name": " h1 Level 5 Kaggle Reference Model h3 Outline h4 B Creating input and targets h4 C Training a network to segment objects h4 D Inference and postprocessing h4 E Visualizing the results not included in this kernel h4 F Evaluation h3 Train Validation split h2 B Creating input and targets h2 C Training a network to segment objects h4 You can interpret the above visualizations as follows h3 Model limitations ", "stargazers_count": 0, "forks_count": 0, "description": "Homogeneous transformation matrix from sensor coordinate frame to ego car frame. The boxes are imprecise the input has a very low resolution one pixel is 40x40cm in the real world and we arbitrarily threshold the predictions and fit boxes around these boxes. That means that we go from a list of coordinates of points to a X by Y by Z space. 75 meters tall and is at the same height of the ego vehicle which is surely a wrong assumption. Note Each of these boxes describes the ground corners of a 3D box. Let s try it with some example values Note X and Y are flipped So that the values in the voxels range from 0 1 we set a maximum intensity. Training a network to segment objects1. Optional Creating a GIF of a scene. bev stands for birds eye view take one channel only Transpose the input volume CXY to XYC order which is what matplotlib requires. Defining datasets dataloaders2. Loading the ground truth8. Next we take our predicted boxes transform them back into world space and make them 3D. Running this on all of the data in parallel C. Optional for multi GPU training and inference Only select the first n images We weigh the loss for the 0 class lower to account for some of the big class imbalance. We compress the height dimension into only 3 channels. N 4 2 N 4 2 N 4 2 2 N 4 Add Z dimension 2 N 4 3 N 4 We don t know at where the boxes are in the scene on the z axis up down let s assume all of them are at the same height as the ego vehicle. We produce top down images and targets2. Next steps Compute mAP on the validation set using the evaluation script provided in the SDK Run inference on the test set. Creating input and targetsLet s load the first sample in the train set. Our code will generate data visualization and model checkpoints they will be persisted to disk in this folder Disable multiprocesing for numpy opencv. Note This kernel is a work in progressI didn t want to hold off on releasing this kernel as I think it will help with getting started in this competition as it is. We re allocating 20GB of memory otherwise. Inference and postprocessing4. We want it in the car s reference frame so we transform each point A sanity check the points should be centered around 0 in car space. Let s load the ground truth for the validation set. The above image looks pretty well separated some boxes seem to be wrongly merged together and may be problematic. Note that red green yellow. Making a submission. As we evaluate with IoUs between 0. 3 N 4 N 4 3 We don t know the height of our boxes let s assume every object is the same height. We train a U Net https arxiv. To get the center of the box in 3D we ll have to add half the height to it. The height of the lidar points are separated into three bins which visualized like this these are the RGB channels of the image. com In this Kernel we provide a near end to end example solution for the Lyft Level 5 Kaggle competition. We already multiprocess ourselves this would mean every subprocess produces even more threads which would lead to a lot of context switching slowing things down a lot. This could use some refactoring. Predicting our validation set. Training a network to segment objectsWe train a U net fully convolutional neural network we create a network that is less deep and with only half the amount of filters compared to the original U net paper implementation. N 3 H W N H W with class indices 0 1 N 2 H W We quantize to uint8 here to conserve memory. In other words Black True Negative Green False Negative Yellow True Positive Red False Positive 2. The input image or semantic input map not in this kernel blended together with targets predictions4. Width and height is arbitrary we don t know what way the vehicles are pointing from our prediction segmentation It doesn t matter for evaluation so no need to worry about that here. Training the model D. backprojecting our predicted boxes into world space E. N 1 H W N H W with class indices 0 1 N 2 H W Visualize the first prediction Get probabilities for non background Arbitrary threshold in our system to create a binary image to fit boxes around. Alternatively we could consider doing it by scenes date or completely randomly. com jvanvugt pytorch unet it is MIT licensed. As input for our network we voxelize the LIDAR points. Loading the dataset2. At this point we have pred_box3ds and gt_box3ds they are the predictions and the targets on the validation set. Thresholding the probability map. The top images have two color channels red for predictions green for targets. This implementation was copied from https github. Sanity check let s count the amount of connected components in an image Let s take the center pixel value as the confidence value Let s remove candidates with very low probability Visualize the boxes in the first sample Visualize their probabilities Load annotations and filter predictions and annotations. Model limitations The model performs very poorly on uncommon classes. Performing a morphological closing operation to filter out tiny objects presuming they are false positives 7. The predictions thresholded at 0. Defining the network architecture U net 3. We can use that to test the functions we ll define next that transform the data to the format we want to input into the model we are training. Visualizing the results not included in this kernel x. Splitting all data into a train and validation set by car B. com mailto gzuidhof lyft. Creating an index and splitting into train and validation scenes1. Creating input and targets1. We can then threshold this probability map and fit boxes around each of the detections. You can expect to train the model in a couple of hours on a modern GPU with inference times under 30ms per image. We only use LIDAR data and we only use one lidar sweep. We perform an opening morphological operation to filter tiny detections Note that this may be problematic for classes that are inherently small e. 8 the actual size we need to adjust for that Determine the rotation of the box XYZW WXYZ order of elements. Some hyperparameters we ll need to define for the system We scale down each box so they are more separated when projected into our coarse voxel space. We do this to keep training and inference time low. Homogeneous transformation matrix from car frame to world frame. Train Validation splitLet s split the data by car to get a validation set. Note We may be able to train for longer and expect better results the reason this number is low is to keep the runtime short. For each scene we fit boxes to the segmentations. Bring box to car space We only care about the bottom corners Drop z coord Don t worry about it being mirrored. We assume every object is 1. 04597 fully convolutional neural network to predict whether a car or other object is present for every pixel in a birds eye view of the world centered on the car. 75 we can expect that to hurt the score. Note We scaled our targets to be 0. The model is barely converged we could train for longer. Above is an example of what the input for our network will look like. You can interpret the above visualizations as follows There are four different visualizations stacked on top of eachother 1. Level 5 Kaggle Reference ModelAuthor Guido Zuidhof gzuidhof lyft. The lidar pointcloud is defined in the sensor s reference frame. Creating top down visualizations of the ground truth and predictions using the nuScenes SDK. For each box and each class we write it s probability in the center pixel. Creating a dataframe with one scene per row. It s a top down projection of the world around the car the car faces to the right in the image. ", "id": "gzuidhof/reference-model", "size": "4801", "language": "python", "html_url": "https://www.kaggle.com/code/gzuidhof/reference-model", "git_url": "https://www.kaggle.com/code/gzuidhof/reference-model", "script": "torch.nn.functional recall_precision UNetUpBlock(nn.Module) Box visualize_predictions scipy.spatial.transform lyft_dataset_sdk.eval.detection.mAP_evaluation functools numpy lyft_dataset_sdk.lyftdataset Image draw_boxes center_crop normalize_voxel_intensities create_transformation_matrix_to_voxel_space car_to_voxel_coords LyftDataset Quaternion load_groundtruth_boxes tqdm_notebook get_unet_model torch.nn move_boxes_to_car_space Pool tqdm lyft_dataset_sdk.utils.geometry_utils transform_points prepare_training_data_for_scene LidarPointCloud visualize_lidar_of_sample matplotlib.pyplot lyft_dataset_sdk.utils.data_classes forward PIL pandas transform_matrix Box3D datetime partial multiprocessing UNet(nn.Module) __len__ Rotation as R Rotation scale_boxes __init__ view_points create_voxel_pointcloud __getitem__ BEVImageDataset(torch.utils.data.Dataset) UNetConvBlock(nn.Module) ", "entities": "(('other pixel', 'car'), 'network') (('we', 'LIDAR points'), 'voxelize') (('top images', 'targets'), 'have') (('these', 'RGB image'), 'separate') (('we', 'fit boxes'), 'be') (('1 we', 'maximum intensity'), 'let') (('they', 'validation set'), 'have') (('It', 'so that'), 'be') (('we', 'it'), 'have') (('we', 'model'), 'use') (('probabilities', 'annotations'), 'let') (('we', 'lidar only one sweep'), 'use') (('We', 'memory'), 'allocate') (('object', 'boxes'), '3') (('which', 'lot'), 'multiprocess') (('matplotlib', 'what'), 'take') (('first prediction', 'boxes'), '1') (('You', 'eachother'), 'interpret') (('that', 'classes'), 'perform') (('we', 'Kaggle Lyft Level 5 competition'), 'com') (('it', 'Drop z coord Don worry'), 'bring') (('semantic map', 'together targets'), 'blend') (('implementation', 'https github'), 'copy') (('lidar pointcloud', 'reference frame'), 'define') (('input', 'network'), 'be') (('Train Validation s', 'validation set'), 'splitLet') (('them', 'world back space'), 'take') (('all', 'ego vehicle'), 'n') (('car', 'image'), 's') (('which', 'ego vehicle'), 'tall') (('we', 'center pixel'), 's') (('they', 'voxel more when coarse space'), 'need') (('We', 'detections'), 'threshold') (('we', 'segmentations'), 'fit') (('model they', 'Disable numpy opencv'), 'generate') (('we', 'Z space'), 'mean') (('rotation', 'elements'), 'size') (('We', 'only 3 channels'), 'compress') (('Each', '3D box'), 'note') (('We', 'this'), 'do') (('points', 'car around 0 space'), 'transform') (('it', 'competition'), 'note') (('Model model', 'very poorly uncommon classes'), 'limitation') (('Alternatively we', 'scenes date'), 'consider') (('H 0 1 2 We', 'here memory'), '3') (('You', 'image'), 'expect') (('We', 'class big imbalance'), 'select') (('they', 'tiny objects'), 'be') (('s', 'validation set'), 'let') (('that', 'paper original U net implementation'), 'create') (('number', 'runtime'), 'be') "}