{"name": "eda pca and random forest algorithm ", "full_name": " h2 Table of Content h3 Feature Explanation h1 h3 EDA h3 PCA h3 t SNE h3 Normalization h3 Decision tree h3 Logistic Regression h3 Random Forest h3 kNN h3 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "The dataset gives information about tumor features that are computed from a digitized image of a fine needle aspirate FNA of a breast mass. The image was captured as a 512 x 480 resolution 8 bit pixel Black and White file. However even with such a small dataset the t SNE algorithm takes significantly more time to complete than PCA. So the SD of gray scale values means how intense levels are spread for particular individual cells. Furthermore it gives low prediction accuracy for a dataset as compared to other machine learning algorithms. 3567251461988304kNN Accuracy Score 0. rise very sharply in the beginning i. 9672 Precision Score is 0. As all features are numerical we do not need to change the default metric which is minkowski. Let s compare how kNN performs if we select 3 and 5 closest neighbors. As with all the shape features a higher value corresponds to a less regular contour and thus to a higher probability of malignancy. Most of the issues involved in the preparation of the sample lie in the medical realm. Moreover it handles both continuous and discrete variables equally well. Kaitlin Kirasich and Trace Smith described in their review 9 the main differences between Random Forest and Logistic Regression in Binary Classification for Heterogeneous Datasets. Exploratory Data Analysis and Data Visualization EDA 3. It is a widely used technique because it is very efficient does not require too many computational resources highly interpretable and easy to regularize. There are many successful use cases where the random forest algorithm was used in highly unbalanced datasets. 05 Logistic regression Accuracy Score 0. Let s load our dataset as a dataframe and explore what kind of features are given along with their datatypes. Feature ExplanationFrom the database description features like radius perimeter area are perfect comprehensible to me. To examine multicollinearity I will look at pairwise scatter plots of pairs of first 10 and last 10 variables in the sake of simplicity and visualization looking for near perfect relationships. With bagging the base algorithms are trained on different random subsets of the original feature set. distance takes a similar vote except gives a heavier weight to those neighbors that are closer. 3567251461988304 Recall Score how much of malignant tumours were predicted correctly 1. But in this case all independent variables are numerical and target class has ratio approximately 0. Here that means retaining 6 principal components therefore we reduce the dimensionality from 30 features to 6. The difficulty of model selection by evaluating the overall classification performance between random forest and logistic regression for datasets comprised of various underlying structures increasing the variance in the explanatory and noise variables increasing the number of noise variables increasing the number of explanatory variables increasing the number of observations. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. Plot the decision boundary. Logistic regression deals well with scaled numerical features and when the data is linearly separable. Decision trees in general do not usually require scaling. Then numerical features will be scaled with StandartScaler function in Python such that the distribution has a mean value of 0 and a standard deviation of 1. However if variables are not normalized the accuracy score drops sharply from 95 to 35. As variables have been already scaled I simply visualize the magnitude of coefficients in the logit model Random ForestRandom forest is the construction of uncorrelated trees using CART bagging and the random subspace method. Each pixel of an image is represented by the 8 bit integer or a byte from 0 to 255 providing the amount of light where 0 is clear black and 255 is clear white. mean of the three largest values PCAToo many variables can cause such problems as too complex visualizations efficiency decrease by including variables that have no effect or difficult data interpretation. The visualization has the following meaning the higher the number of concave points in the cell nucleus and the greater the radius is the higher the probability that the cell is cancerous. An interesting observation I found when increasing the variance in the explanatory and noise variables logistic regression consistently performed with higher overall accuracy as compared to random forest. Next smoothness is quantified by measuring the difference between length of radial line and the mean length of two radial lines surrounding it If the number is small then the contour is smooth in that region smoothness frac sum limits_ i left r_i frac r_ i 1 r_ i 1 2 right perimeter The concavity is captured by drawing chords between two boundary points which lie outside the nuclear. In this case greater coefficients are assigned for concavity_se and concave_points_se attributes with non normalized dataAfter that GridSearch implements a fit and a score method. In practice we would choose the number of principal components such that we can explain 90 of the initial data dispersion via the explained_variance_ratio. And later I will compare classification performance for the initial dataset and for pca components. Other studies either have used direct scanning of Feulgen stained material or have analyzed digitized images. At the same time concavity_se and concave_points_se have small ranges and concave_points_se feature varies approximately from 0 to 0. But in comparison with a single decision tree Random Forest s output is more difficult to interpret. In first nine features the median of malignant tumor is easily contrasted with the benign. Although PCA reduces attribute space from a larger number of variables to a smaller number of components Breast Cancer Wisconsin Diagnostic Data Set has only 31 features which is not very large number. The implementation of logistic regression in Python can be accessed from class LogisticRegression in scikit learn library. 9181286549707602 Recall Score how much of malignant tumours were predicted correctly 0. But the best hyperparameters are usually impossible to determine ahead of time. The image for digital analysis was generated by a JVC TK 1070U color video camera mounted above an Olympus microscope and the image was projected into the camera with a 63 x objective and a 2. After computing 10 features for each nucleus the mean standart error and extreme value was computed as it mentioned above. EDALet s look at the general statistics. 74 CV precision score 93. This impairs the interpretability of the model. With t SNE the picture looks better since PCA has a linear constraint while t SNE uses a non linear approach in the background. Let s look at the standart deviation of features To interpret these results the area radius perimeter of cancerous cells are widely distributed and dramatically vary from cell to cell in the cytology slide as well as the number of concave points vary broadly for malignant nuclei. For algorithms like linear regressions and kNN numerical features have to be scaled in order to avoid over fitting and make more accurate predictions. More formally the method follows the compactness hypothesis if the distance between the examples is measured well enough then similar examples are much more likely to belong to the same class. Whichever class has the greatest number of votes becomes the class for the new data point. 05 while radius_mean lies within 6. Hello Kagglers This work is part of my Capstone project in Data Analytics Predictive Analytics and Big Data course at Ryerson University Toronto. We could avoid this ugly slicing by using a two dim dataset we create an instance of Neighbours Classifier and fit the data. Decision treeDecision tree is one of the simplest algorithms which can be used for classification and regression where the relationship between features and outcome is nonlinear or where features interact with each other. The most important is recall score as we are interested in how many of malignant tumours were predicted correctly. Normalization Normalization 5. One of the simplest options to understand the influence of given parameters in a linear classification model is to consider the magnitude of its coefficient times the standard deviation of the corresponding parameters in the data. 0 Precision Score how much of tumours which were predicted as malignant were actually malignant 0. 8615384615384616Random forest is considered as an advanced machine learning technique especially if the dataset is imbalanced or has categorical features. Principal component analysis PCA is a mathematical procedure that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables called principal components. For each observation there are 10 features which describe tumor size density texture symmetry and other characteristics of the cell nuclei present in the image. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. 8310 and Accuracy Score 0. One of the main disadvantages of using Decision tree is a prone to overfitting. I fitted the logit model both on non normalized and normalized data to compare the results and examine the performance of both approaches a Logistic regression fits raw data and makes predictions based on non scaled features. As can be observed the best number of neighbours for the training data is 5 where recall score is above 0. But what do texture smoothness compactness concavity symmetry and fractal dimensions mean exactly Texture is a standard deviation of gray scale values. We need to avoid overfitting by pruning setting a minimum number of samples in each leaf or defining a maximum depth for the tree. To assign the class when neighbors do not have the same class KNeighborsClassifier method in Python has weights parameter uniform takes a simple majority vote from the neighbors. A log transformation a popular method is often used to transform skewed data to approximately normal and thus to augment the reliability of the linear regression analyses. For example if the neighbor is 5 units away then weight its vote 1 5. http Let s have a look how the target class is distributed http As it can be seen there is no missing values except for the last column. Best Parameters Best parameters horizontal line model quality with default C value Create Data frame of Regression coefficients Merge Regression coefficients with feature names Set up the matplotlib figure Let s draw top 10 important features Create Data frame of Regression coefficients Merge Regression coefficients with feature names Set up the matplotlib figure Let s draw top 10 important features Stratified split for the validation process initialize the set of parameters for exhaustive search and fit to find out the optimal parameters RandomForest classifier with the default parameters Define k NN classifier and train on a scaled dataset step size in the mesh Create color maps we only take the first two features radius_mean and concave points_mean. The area on the aspirate slides to be analyzed was visually selected for minimal nuclear overlap. 97 for texture_mean and texture_worst pair it equals to 0. ConclusionRandom forest shows better performance based on recall scores which means more malignant tumours were predicted correctly although logistic regression has a higher precision score. Data cleaning Data cleaning 4. The aspirated material was expressed onto a silane coated glass slide which was placed under a similar slide. Data cleaningAs the dataset is not large I am not going to remove any outliers in order to keep as much data as possible. The categorical target feature indicates the type of the tumor. Feature explanation Feature 2. For the concavity_mean the mean value of these lengths is calculated. To interpret results of Random Forest classifier randomly selected trees could be visualized As can be seen concavity_mean is the root node and has the highest information gain which is why it is split first. That is why it would be better to remove say perimeter and area as well as all features from worst samples since worst or largest instances are also considered in the initial sample which means and standart errors were computed for therefore it leads to high correlation 0. One of the greatest advantages of using Random forest in a classification problem is that it is robust to outliers because of the random sampling method and it handles both continuous and discrete variables equally well. The underlying intuition is that you look like your neighbors. Each histogram is similar to lognormal distribution a continuous distribution in which the logarithm of a variable has a normal distribution. And to be scaled numerical features must follow normal distribution. Put the result into a color plot Plot also the training points. Scores are computed for the holdout part which takes 30 of data using 10 fold cross validation and compared with actual values. Wittekind and Schulte found that mean nuclear area mean maximum nuclear diameter and mean nuclear perimeter differed significantly between benign and malignant breast cell obtained by FNA. I am going to stick with PCA since it provides similar results to t SNE and takes less time to compute the components. As the neighbor gets further away the weight gets smaller. That is not surprising as these features were modeled in such way that higher values are typically associated with malignancy. This like decision trees is one of the most comprehensible approaches to classification. The darker the image is the lower is the mean of intensity level of a pixel i. read_csv Ignore filter warnings By default Pandas displays 20 columns and 60 rows I will increase it to 150 and 100 And convert it to categorical feature Remove the last empty column Set up the matplotlib figure Draw the heatmap with the mask and correct aspect ratio Cross table break down by diagnosis Invoke the TSNE method Log transformation Scaler should be trained on train set only to prevent information about future from leaking. When fitting LogisticRegression on a dataset all the possible values of regularization parameter are evaluated using 10 fold stratified cross validation and the best value and array of scores are retained. As it can be seen that the first 6 components correspond to approximately 91 of the cumulative sum over all the variance. Two target classes where dark is benign and light is malignant are almost linearly separable t SNEt Distributed stochastic neighbor embedding t SNE minimizes the divergence between two distributions a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low dimensional points in the embedding. Using Scikit Learn s GridSearchCV method I define a grid of hyperparameter ranges and randomly sample from the grid performing Stratified K Fold cross validation with each combination of values. For example the correlation between radius_worst and radius_mean is 0. With the help of GridSearchCV function in Python which exhaustively searches model optimal parameters by cross validated grid search over a parameter grid best parameters such as the depth of the tree split criteria the minimum number of samples for a leaf node can be identified The best split criteria here is the entropy the depth of the tree equals to 2 as all instances are fully exhausted within two splits. 89 CV recall score 92. This implementation can fit binary logistic regression with default L2 or L1 regularization. While the bias is very difficult to quantify it is possible that if the physician suspects the sample to be malignant then the selected cells will reflect that suspicion. One of the greatest advantages of using Random forest in a classification problem is that it is robust to outliers because of the random sampling method. Other researchers have applied computer based image analysis to various aspects of breast cytology interpretation. The data has 30 dimensions but I reduce it creating 2 principal components to see whether variables can be separated into clusters. In practice an increase in the tree number almost always improves the composition and therefore rarely overfits. I will apply log function to make features normallyOverall almost all features have bell shaped distribution despite concave points features which could be affected by malignant instances where the number of contour concavities increases dramatically. All independent features are numerical and the target feature is converted to categorical. In order to measure symmetry the major axis or longest chord through the center is found. Decision trees are a good choice for the base classifier in bagging since they are quite sophisticated and can achieve zero classification error on any sample. It resulted in almost zero coefficients for three features radius_mean texture_mean and texture_se. 8309859154929577Random Forest CV accuracy score 94. But to avoid multicollinearity I will remove some of the features to prevent overfitting. Moreover PCA makes independent variables less interpretable. From the EDA we now know that radius perimeter and area are highly correlated which makes sense. Using GridSearchCV it can be computed that uniform metric performs better than distance in this case and that the best number of neighbors equals to five. 9649By default LogisticRegression use L2 penalty and I search for a best regularization parameter C the inverse of regularization strength. 9672131147540983Precision Score how much of tumours which were predicted as malignant were actually malignant 0. 80 which is not surprising too. The second nodes are texture_mean and concavity_se the less contrasted the picture is the more probability that the cell nucleus is benign. For continuous data kNN uses a distance metric like Euclidean or Minkowski distance. However the trees are very sensitive to the noise in input data the whole model could change if the training set is slightly modified e. For that we will assign a color to each point in the mesh x_min x_max x y_min y_max. Features with a larger range of values can dominate the distance metric relative to features that have a smaller range so feature scaling is important. Decision tree Accuracy Score 0. Conclusion Conclusion The Diagnostic Wisconsin Breast Cancer Database is a publicly available data set from the UCI machine learning repository. for very small values near zero peaks out early then decreases sharply and leave the long tail. 93 There are a number of model evaluation techniques for the classification problem I decided to choose three performance metrics accuracy recall and precision scores. I will use the log transformation in Logistic regression and kNN algorithms before scaling the data. Fractal dimensions is approximated using the coastline approximation the perimeter of the nucleaus is measured by a using increasingly larger rulers and as the ruler size increases the perimeter decreases. NormalizationBefore scaling numerical features let s check whether they follow normal distribution Almost all distributions are skewed to the right i. The bias could be reduced by selecting a number of different areas for digitization or possibly eliminated altogether by automating the selection process. kNNThe nearest neighbors method is another quite popular classification method that is also sometimes used in classification problems. 91 and so on and so forth. Plotting log transformation of the perimeter against log of the ruler size and measuring the downward slope gives us the fractal dimension. Another great drawback of using decision trees is that we need to avoid overfitting by pruning setting a minimum number of samples in each leaf or defining a maximum depth for the tree. For example area_mean is higher for cancerous mass on average. Hyperparameter tuning relies more on experimental results than theory and thus the best method to determine the optimal settings is to try many different combinations to evaluate the performance of each model. 9180327868852459 Precision Score how much of tumours which were predicted as malignant were actually malignant 0. Generally speaking for the benign mass the median is lower for all features which makes sense because features were modeled such that higher values are typically associated with malignancy. It means that small changes in concave_points_se could affect the result and change the target class from 0 to 1 or the other way around while small changes in radius_mean could not make such big impact and hardly has any effect on the response variable. Every violinplot includes markers indicating the median and the interquartile middle 50 range. including mean std median percentiles and range. A typical image contains approximately from 10 to 40 nuclei. These features have larger ranges in comparison with other attributes and logistic regression assigns very small coefficients to them to reduce their impact on a result. 9181 Logistic Regressionwith normalized dataLogistic Regression is one of the most used Machine Learning algorithms for binary classification. Please see the full Table of Content below Table of Content 1. remove a feature add some objects. In addition the trees are very sensitive to the noise in input data the whole model could change if the training set is slightly modified e. The random subspace method reduces the correlation between the trees and thus prevents overfitting. These features are modeled such that higher values are typically associated with malignancy. It might be usiful to convert the target class denoting malignant as 0 and benign as 1 and examine correlation among variables There are strong positive linear relationships between malignancy and radius of nuclear number of concave points perimeter and area. 9181286549707602Recall Score how much of malignant tumours were predicted correctly 0. Decision tree Decision tree 6. Moreover random forest is insensitive to the scaling of features as well as any other monotonic transformations due to the random subspace selection. 9016 and precision is 0. And finally the worst i. We then measure the length difference between lines perpendicular to the major axis and the nuclear boundary in both directions. To compare how kNN performs for 3 and 5 closest neighbors the colour plot could be drawn where purple background represents areas predicted as malignant and pink represents areas predicted as benign Recall score could be also compared between train and test sets plotting for each number of nearest neighbors. The higher SD the more contrasting the image is. Random Forest Random Forest 8. Next let s see how means are distributed among target class I standartized variables as their ranges are quite different and not representable on a small graph. Certain selection bias is introduced in the process when the physician decides what part of the sample should be extracted. Logistic Regression Logistic Regression a with normalized data with normalized data b with non normalized data with non normalized data 7. The mean standard error and worst mean mean of the three largest values of these features were computed for each image resulting in 30 features. ", "id": "sulianova/eda-pca-and-random-forest-algorithm", "size": "23408", "language": "python", "html_url": "https://www.kaggle.com/code/sulianova/eda-pca-and-random-forest-algorithm", "git_url": "https://www.kaggle.com/code/sulianova/eda-pca-and-random-forest-algorithm", "script": "train_test_split pyplot pyplot as plt IPython.display plot_violinplot confusion_matrix accuracy_score cross_val_score numpy Image seaborn ListedColormap recall_score sklearn.manifold sklearn.neighbors sklearn.tree sklearn sklearn.linear_model StratifiedKFold DecisionTreeClassifier precision_score matplotlib.colors sklearn.model_selection pandas RandomForestClassifier LogisticRegression export_graphviz sklearn.pipeline matplotlib KNeighborsClassifier Pipeline TSNE GridSearchCV sklearn.metrics sklearn.ensemble StandardScaler decomposition sklearn.preprocessing ", "entities": "(('correctly logistic regression', 'precision higher score'), 'show') (('higher SD', 'more image'), 'be') (('we', 'data'), 'avoid') (('database description', 'me'), 'be') (('target categorical feature', 'tumor'), 'indicate') (('magnitude', 'data'), 'be') (('which', 'malignant'), '0') (('mean standard error', '30 features'), 'compute') (('I', 'overfitting'), 'remove') (('well enough then similar examples', 'much more same class'), 'follow') (('Furthermore it', 'other machine learning algorithms'), 'give') (('subspace random method', 'thus overfitting'), 'reduce') (('first 6 components', 'variance'), 'see') (('Logistic regression', 'non scaled features'), 'fit') (('10 which', 'present image'), 'be') (('We', 'tree'), 'need') (('Recall benign score', 'nearest neighbors'), 'compare') (('which', 'default metric'), 'need') (('image', '512 480 resolution 8 bit pixel Black file'), 'capture') (('Hello work', 'Big Data Ryerson University Toronto'), 'Kagglers') (('Decision trees', 'general usually scaling'), 'require') (('therefore it', 'high correlation'), 'be') (('when data', 'well scaled numerical features'), 'deal') (('we', 'points_mean'), 'parameter') (('that', 'embedding'), 'be') (('Moreover it', 'continuous variables'), 'handle') (('that', 'neighbors'), 'take') (('which', 'malignant'), 'score') (('that', 'effect'), 'mean') (('the higher cell', 'cell nucleus'), 'have') (('weights parameter uniform', 'neighbors'), 'take') (('Other studies', 'digitized images'), 'use') (('training set', 'input data'), 'be') (('perimeter', 'decreases'), 'approximate') (('implementation', 'library'), 'access') (('such distribution', 'standard 1'), 'scale') (('scaled numerical features', 'normal distribution'), 'follow') (('which', 'malignant'), '9180327868852459') (('higher value', 'malignancy'), 'feature') (('best number', 'five'), 'compute') (('it', 'components'), 'go') (('I', 'near perfect relationships'), 'look') (('which', 'only 31 features'), 'have') (('which', 'nuclear'), 'quantify') (('how intense levels', 'particular individual cells'), 'mean') (('therefore we', '6'), 'mean') (('Kaitlin Kirasich', 'Heterogeneous Datasets'), 'describe') (('nuclear perimeter', 'FNA'), 'find') (('symmetry', 'longest center'), 'find') (('increase', 'almost always composition'), 'improve') (('Almost distributions', 'right i.'), 'let') (('classification quite popular that', 'classification also sometimes problems'), 'be') (('GridSearch', 'fit'), 'assign') (('image', 'x 63 objective'), 'generate') (('We', 'nuclear directions'), 'measure') (('it', 'continuous variables'), 'be') (('interesting I', 'random forest'), 'observation') (('Hyperparameter tuning', 'model'), 'be') (('concave_points_se', '0'), 'have') (('around small changes', 'response variable'), 'mean') (('kind', 'datatypes'), 'let') (('clear 255', 'light'), 'represent') (('they', 'sample'), 'be') (('8309859154929577Random Forest CV accuracy', '94'), 'score') (('lower', 'pixel i.'), 'be') (('t SNE', 'background'), 'look') (('use forest many successful where random algorithm', 'highly unbalanced datasets'), 'be') (('why it', 'root information highest gain'), 'visualize') (('correlation', 'radius_worst'), 'be') (('that', 'breast mass'), 'give') (('variables', 'clusters'), 'have') (('best hyperparameters', 'usually ahead time'), 'be') (('how many', 'malignant tumours'), 'be') (('bias', 'selection possibly altogether process'), 'reduce') (('algorithms', 'feature original set'), 'train') (('logarithm', 'normal distribution'), 'be') (('as well number', 'broadly malignant nuclei'), 'let') (('then selected cells', 'suspicion'), 'be') (('widely used it', 'very too many computational resources'), 'be') (('SNE t algorithm', 'PCA'), 'take') (('Random output', 'decision single tree'), 'be') (('mean value', 'lengths'), 'calculate') (('it', '0'), '97') (('we', 'closest neighbors'), 'let') (('I', 'values'), 'define') (('it', 'missing last column'), 'let') (('Moreover random forest', 'subspace as well other monotonic random selection'), 'be') (('It', 'python docker image https kaggle github'), 'come') (('extreme it', 'nucleus'), 'compute') (('it', 'sampling random method'), 'be') (('features', 'result'), 'have') (('we', 'mesh'), 'assign') (('highly which', 'sense'), 'know') (('ranges', 'quite small graph'), 'let') (('This', 'classification'), 'be') (('target feature', 'categorical'), 'be') (('area_mean', 'cancerous mass'), 'be') (('higher values', 'typically malignancy'), 'be') (('violinplot', 'median'), 'include') (('implementation', 'default L2'), 'fit') (('Other researchers', 'breast cytology interpretation'), 'apply') (('I', 'regularization strength'), 'use') (('part', 'sample'), 'introduce') (('Conclusion Diagnostic Wisconsin Breast Cancer Database', 'UCI machine learning publicly available repository'), 'Conclusion') (('This', 'model'), 'impair') (('independent variables', 'case'), 'be') (('first nine median', 'easily benign'), 'feature') (('algorithms', 'more accurate predictions'), 'have') (('which', 'similar slide'), 'express') (('recall where score', '0'), 'be') (('greatest number', 'data new point'), 'have') (('how much', 'malignant tumours'), 'predict') (('typical image', 'approximately 10 to 40 nuclei'), 'contain') (('later I', 'pca components'), 'compare') (('evaluated', 'best scores'), 'retain') (('I', 'performance metrics three accuracy recall'), '93') (('mathematical that', 'uncorrelated variables'), 'be') (('I', 'kNN data'), 'use') (('slides', 'visually minimal nuclear overlap'), 'select') (('It', 'features radius_mean three texture_mean'), 'result') (('where features', 'other'), 'be') (('where number', 'contour concavities'), 'apply') (('popular method', 'regression linear analyses'), 'use') (('especially dataset', 'categorical features'), 'consider') (('fractal exactly Texture', 'scale standard gray values'), 'mean') (('I', 'as much data'), 'cleaningas') (('One', 'overfitting'), 'be') (('we', 'tree'), 'be') (('such we', 'explained_variance_ratio'), 'choose') (('Logistic Regressionwith', 'binary classification'), 'normalize') (('It', 'concave points perimeter'), 'be') (('feature so scaling', 'smaller range'), 'dominate') (('such higher values', 'typically malignancy'), 'be') (('accuracy score', '35'), 'drop') (('instances', 'fully two splits'), 'be') (('Log transformation Scaler', 'future'), 'display') (('Plotting', 'fractal dimension'), 'give') (('neighbor', '5 away then vote'), 'weight') (('higher values', 'typically malignancy'), 'model') (('you', 'neighbors'), 'be') (('Random ForestRandom forest', 'CART bagging'), 'visualize') (('which', 'actual values'), 'compute') "}