{"name": "adams ss20 final ", "full_name": " h1 ADAMS Assignment SS20 Prediction of Claps h1 Introduction h3 Description of Imports h1 Data exploration h2 Data import h2 Duplicate removal h2 Analysis of TotalClapCount h2 Language analysis h1 Data Cleaning h2 Publication Details in test set h1 Natural language processing and predictions h2 Tf Idf Vectorizer with Support Vector Regression h2 Test set predictions h2 BERT Multi classification h3 Predictions on validation set h3 Predictions on provided test set h1 Regression Analysis on non text features h2 Ridge Regression h3 Ridge Test set predictions h2 Light Gradient Boosting Machine h3 LGB Test set predictions h1 Ensembling h1 Conclusion h1 Bibliography ", "stargazers_count": 0, "forks_count": 0, "description": "The Kernels used are linear and radial basis function with the latter being a common choice for the hyperplane decision boundary. A learning rate of 2e 5 was chosen after experimenting with other learning rates in the range of 3e 6 and 2e 3. Predictions are made based on the GridSearch determined values. This is adequately depicted in the graphic above taken from Javatpoint. It is an example of social journalism giving professional journalists and inexperienced private parties the chance to publish on the same platform 2. A small improvement on the mean_absolute_error was achieved for some samples however results were not stable enough to consider standard scaling a definite improvement. com pushkarmandot https medium com pushkarmandot what is lightgbm how to implement it how to fine tune the parameters 60347819b7fc 24 Ke G. It has not been tuned and was chosen mostly for its efficiency and ease of use. png Data CleaningThe data cleaning performed here is focused on overall quality improvement of the dataset. Results again revealed an overwhelming majority of English texts with a number of Spanish and Portuguese texts. Furthermore he posits NLP to be a multiple step process with the three main parts being 1. Predicting which articles might go viral based on language level content poses to be extraordinarily difficult with many big companies aiming to achieve just that. The code cell below highlights this with the actual article behind this having over 50000 Claps and the predictions being in the range of billiards. Using tf idf to determine word relevance in document queries. To help with the reversal a function that reassigns the original bin names was written too. strip function removes whitespaces and linebreaks to only have the sentence left. LightGBM as a Gradient Boosting implementation iterates so far over the training until validation scores as defined by the metric specified do not improve anymore. While the compressed file uploaded to Kaggle was less than 300MB in size uncompressed it turns into a multiple gigabyte big comma separated value file. The representation and meaning involves the processing and determination of sentence level meaning. Medium itself is a digital publishing platform allowing people to share their self created articles and other written works with people all around the world. Unicode Error in the train set image. https towardsdatascience. com uc export download id 1vCWy7MXl_dQmIXI52CNKZa9QFvz4tM 2 The Self attention mechanism allows the Transformer to take contextual clues into account when assigning encodings for the word it relating it to a robot at the beginning of the sentence 16. This pretraining is done on a large amount of text in a semi supervised way with the help of enormous amounts of computational power leaving the user with a pre trained model that only requires some finetuning for the specific task at hand. Ridge RegressionRidge regression is a regression with a built in penalty term which is equal to the square of the coefficient 20. Look up the embedding of each input word in the embedding matrix which is made up of the vocabulary times the embedding size which depends on the exact version of BERT used 2. To make restarting easier the adams 2020 folder contains train and test files where the text has already been cleaned using the cleaning functions implemented at the beginning of this notebook. In Proceedings of the first instructional conference on machine learning Vol. Its median however is at 5. float32 label Y_train_regr lgb_x_valid_regr lgb. To correct for negative numbers and the single extreme outlier value these are normalized to 0 and 50000 respectively. It highlights clearly how different algorithms put weights on different aspects of variables used for prediction and how pre trained embeddings can be used as a multi classifier in a regression context. ADAMS Assignment SS20 Prediction of ClapsCredit Medium. best_iteration Just to be run if the code above has not been run in full. Bibliography Bibliography IntroductionThis notebook here is concerned with predicting claps for a selection of Medium. NAs found in this column are replaced with a 0 as no response information for these cells can be established. To perform an adequate regression analysis some preparatory steps are necessary. long as the Crossentropyloss used in later execution expects. Unfortunately this command can only be run after succesful runs unsuccessful runs or errors demand a notebook restart. Ridge Test set predictionsThe Ridge predictions for the test set show the same outliers as present in the predictions for the validation set. The test file also contains duplicates however they cannot be dropped since predictions need to be made for all of them. The duplicate_remover function then takes the tokenized sentences and for each text checks whether the same sentence already exists in its cache. All text gets transformed to lower case. The maximum number of rounds of 200 is set in accordance with personal experience and the fact that throughout all runs the validation score always stopped improving before reaching round 200. The GPU is heavily used for training as it happens in parallel on the thousands of cores available on the GPU. This indicates that text cleaning is necessary which will be performed in the next step. The Tf Idf Vectorizer takes the cleaned and lemmatized texts and calculates the values for each of them then transforms the Train and Test set into vectorized form. The margins include the errors neglected in the support vector machine while the support vector around the hyperplane try to separate the data into two separate spaces. The number of labels given over to the model is twenty for the number of categories to detect. Tf Idf values are especially fitting to the task at hand of numerical prediction as the values can be directly fed to a Support Vector Machine. com articles with a lot of claps are themselves being recommended to users through the use of recommender algorithms at the hand of Medium. This can be understood better by looking at a fractioned totalClapCount in a value range between 1 and 200 as plooted in the second plot below. With a mean wordCount of 901 and a median word count of 703 even the maximum sequence length of 512 for BERT does not suffice to capture all words for each article every time. Regression Analysis on non text features Regression Analysis on non text features 1. The test set does not contain as much information as the train set and omisses many of the columns present in the train set. It might be that using only two input variables is not enough. A short plot of the log1p transformed totalClapCount showing the distribution afterwards. With no totalClapCount available the MAE and MSE cannot be calculated. The text gets tokenized each sentence gets broken up into separate words or more specifically strings get divided into substrings 10. For the most parts the words contained in the title are present in the text column too and will be picked up by BERT automatically. Data exploration Data exploration 3. Most common in the train dataset is a missing publisher while the most common publication domain in the test set is Netflix. This has multiple advantages It drastically reduces time complexity and reduces memory usage. Polyglot self describes as a natural language pipeline that supports massive multilingual applications 6 and is licensed as free software under a GPLv3 license. Tf Idf is widely being used in recommender algorithm for its accuracy and useability being of high relative efficiency in comparison to other word vectorization methods. Further data exploration in this direction would necessitate either the prediction of variables or the scraping of further information from the Internet. An important step in the BERT process is the creation of a TensorDataset. First the sentence_extract function uses the sentence tokenizer function in the nltk package to extract sentences. A mean absolute error of 108 seems quite good in comparison to the 175 achieved through the use of the Ridge regression. Plotted the totalClapCount shows that most articles receive 0 claps. scale_loss is specifically called to deal with the loss as part of Automated Mixed Precision execution on the GPU 19. The Length variable contained in the test set might be worth a closer look. It also seems to pick up quite well on the articles containing many more claps than the average. Personal experience has shown a Tf Idf based SVR to be close to a fully trained BERT in a binary and multi class text classification setting while keeping computational cost at a fraction of the deep neural net. In this case the metric used is the mean absolute error. To guarantee that the parameters used for the predictions are optimal a GridSearch Crossvalidation is implemented testing different combinations of parameters to find the optimum ones. As Chowdhary in Fundamentals of Artificial Intelligence describes At the core of any NLP task there is the important issue of natural language understanding 7. BERT was pretrained on an immensely big dataset removing the need to train a machine learning model for NLP from nothing and starting off with a model already full of language knowledge. Running the Tf Idf with a sample size significantly bigger than 5000 such as 30000 led to crashes in the Kaggle environment due to RAM constraints. In the case of this analysis performed here LGBM can serve as a good improvement upon the Ridge regression with much more refinement options and a good framework for big datasets 24. The test set contains 514 links to Medium articles for each of which a prediction of claps is to be made. What is LightGBM How to implement it How to fine tune the parameters Medium. Both feats are achieved through the use of bins 23 Another advantage is its ability to handle categorical data by one hot encoding it without the need for further preprocessing. This tag_map is then handed to the Lemmatizer function which then lemmatizes each word in the corpus and appends it to a list of lemmatized words. com 2014 03 29 the new rules of social journalism a proposal 3 Alammar J. Ensembling All predictions performed above need to be combined in some way to form a final prediction. This might be a result of the articles in the test set containing more responses on average. The non text features are analysed using a Ridge Regression and LGB Machine. The responses are an obvious feature as they directly measure public interest in the article at hand just differently. The Encoding in BERT is different from the one used by many other algorithms as it makes use of Byte Pair Encoding. Add positional encoding Indication of the order of words in the sequence for the transformer blocks3. get lang english Installing Nvidia Apex Converting the lines to BERT format Thanks to https www. In the end the outputs of all models are combined in an ensemble model. squeeze and numpy values created which are saved in a valid_preds array. join dirname filename Any results you write to the current directory are saved as output. Language detection on the test set revealed that only three texts were not in English. This is further limited by the fact that the full sequence length does not fit into the VRAM provided by the Nvidia K80 or P100 GPUs on Kaggle. LGB Test set predictionsPredicting on the test set with the LGBM the average number of claps predicted seems quite high with more than 1720 claps predicted. The creation of bins is of much importance in this case of a multiclassification approximation for a regression task. From there it is visible that the model still has difficulties picking up on articles with more than just a few claps. Natural language processing and predictions Natural language processing and predictions 1. To start off the text in both train and test set will be cleaned in a way that makes it more understandable legible and better suited to further processing steps. Visualization of a Support Vector Machine Credit Javatpoint SVR https static. com and in form quite similar to Likes on platforms such as Facebook and Instagram. The optimizer_grouped_parameters define the optimizer parameters and set the decay thereby defining how much the learning rate gets adjusted per iteration. Afterwards the column type is changed to an integer. The dataframe containing the true clap count and the predictions. Publication Details in test setThe publication details in the test set contain more information than is useful for a single column so an extraction of the date seems useful. md machine learning challenge winning solutions 23 Mandot P. This format is necessary for the execution in the pyTorch environment. With most of the columns being the object type the describe function highlights the numeric columns by giving their mean median and more information. Predictions have to be fed to a sigmoid function to determine positive comparable value from which the maximum is drawn by a softmax which itself returns the number in the array this maximum occurs at. This is partially caused by the way a regression works and partially by the content used for predicting this phenomenon. The number of maximum leaves was set at 255 after which the tree will grow laterally. The focus here lies on the text itself as it contains much more information than the title only. A Tf Idf vectorization might therefore replicate the recommendation at the base of a higher amount of claps for an article. Test set predictionsFollowing are the prediction calculations for the test set provided as part of this task. Byte pair encoding breaks down words into segments and uses the subwords in the embedding. Padding of sequences if they are shorter than max_seq_length 512 4. Personal experience has shown GPT 2 to be a very useable zero shot classifier its successor GPT 3 is likely to improve upon this immensely however its size prohibits any realistic handling on single GPU machines. Launched in 2012 more than 60 million users read articles on Medium in one 2017 1. The size of these models is often a result of them being pre trained on huge datasets to give them an innate understanding of text. In the same way many articles with a low number of responses still receive thousands of Claps. MODEL CREATION To create and define the model used for training many parameters and steps have to be defined starting with the learning rate. Retrieved August 30 2020 from https github. Ridge Regression Ridge Regression 2. In the end all predictions were combined into final predictions that hopefully capture some of the information given to predict the claps. The totalClapCount is once again log1p transformed to make the target data look normally distributed while taking into account the potential of 0 claps for an article which log alone would not do. The apex install takes up a few minutes and cannot be skipped as Nvidia Apex is a CUDA requirement and not yet part of the Kaggle preinstalled packages. The tensorflow BERT model gets converted to a pyTorch compatible BERT model. It could be assumed that longer articles attract more claps. In the beginning a Dataloader object is created based on the train set with batch size 32 as above and no shuffling performed. Representation and meaning3. Term frequency In most cases the number of times a term appears in a given text. Afterwards the gradients get added up to calculate the model variable updates 18. Overall the English language texts still make up over 93 of the texts. However runtimes can be very long even with significant computational backing. io whats a slug f7e74b6c23e0 6 Welcome to polyglot s documentation Polyglot 16. To do so a first look at duplicates in the row uniqueSlug reveals a lot of duplicates. io illustrated bert 18 Haleva R. Retrieved August 30 2020 from https www. Thus we can use the Slug as a legible substitute for the URL which is used as a column identifier in the dropping of duplicates below. html 20 Lasso vs ridge vs elastic net ml. The clapcat column is defined as the target column intended for training later. Improvements will likely come from longer training on more GPUs providing more VRAM to load more data at once. One way to do so is by using the polyglot package which guesses the languages for each text inserted into it by comparing it to its own vocabulary. Both algorithmic approaches aim to solve a numeric prediction task which means that human language level text must be transformed into numeric statistical representations to extract the necessary information. SVMs can be used in conjunction with polynomial radial and other Kernel functions in order to perform non linear classification. Claps are being predicted in the million and billion Claps range which is impossible as the most clapped article as of August 2020 has no more than 281000 claps 21. Self attention allows the Transformer to determine encodings for each word with the help of other words in the input sequence and connecting them on a logical level thus using other relevant words in a sequence to understand the one currently being processed. The csv containing the BERT predictionsPotential improvements might come from the use of ROBERTA or ALBERT. Other authors do not occur more often than 4 times. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. The CrossEntropyLoss takes untransformed values as inputs. This is in line with the assumptions from above. The batch size was chosen as 32 with batch sizes mostly limited by available VRAM. The uniqueSlug attribute is especially fitting for a duplicate removal as A slug is a human readable unique identifier used to identify a resource instead of a less human readable identifier like an id 5. The LGBM already achieves much better results than the Ridge regression. With the predictions being vastly different it makes sense to consider an ensemble of all predictions. BERT Multiclassification BERT Multiclassification 5. The describe function above suggests that the recommends column might be correlated with the number of claps. This problem would not occur could direct hardware access be provided but the layer of abstraction provided through the Kaggle virtual environment prevent directs hardware access to the GPU and therefore the empyting of the Cache. For analysis the Length equivalent in the train set wordCount is plotted against the totalClapCount below. NVIDIA CUDA software and GPU parallel computing architecture. Combined with a 50 value of 5 this shows how unequal the distribution of claps is throughout the dataset with a high number of articles having less than 5 claps while some contain up to 291000 claps accounting for the significant standard deviation. html 13 Albert A lite bert for self supervised learning of language representations. com tutorial machine learning images support vector machine algorithm. The loss in the Multiclassification case is a CrossentropyLoss as it combines a negative log likelihood function with a logarithmic softmax function to determine the optimal item out of the 20 potential classes for each tensor item. Lightgbm A highly efficient gradient boosting decision tree. Ensembling is only possible when all prediction approaches have been run. The boxplot in particular shows how a number of articles receive an abnormally high amount of claps. The first one tries to extract information from each word in a text by using its frequency of occurrence thus focusing on the first step of the process while the pre trained embedding based NLP approach tries to extract sentence and context dependent meaning from a text thus incorporating the second and third steps. At this point it becomes important to take a closer look again at the test set. Retrieved August 30 2020 from https nvidia. In the case of NLP subword pairs are merged together instead of being replaced by another byte. The average accuracy is calculated as described above. com uc export download id 1NHFBLsBD4bysTyANgAXhk79zoFrkA2Qy Actual training The actual training happens iteratively over many loops with the number of epochs as defined above being the number of times the whole training data will be run through in the course of training. Its prowess can be directly compared to that of the Tf Idf SVR which although not pre trained and much smaller can often still perform competitively in language level tasks. This makes handling the data more difficult as computational power and storage needs to be capable of handling all data at once. Bibliography 1 Bort J. As a first and rough measure their value could just be set to 100000 with further corrections being made through the combination of the Ridge predictions with the predictions from the other approaches in this notebook. Retransformation of the log1p transformed totalClapCount. A tutorial on support vector regression. Can be weighted or adjusted for special usecases2. The primary advantage of BERT and its iterations is the pre training done on them. org lasso vs ridge vs elastic net ml 21 Top medium stories. Like other tree based algorithms it makes use of a leaf wise tree growth however it uses histogram based algorithms instead of pre sort based algorithms such as the one used by the XGBoosting framework. Kaggle offers certain advantages here by offering 16GB of RAM and access to high end GPUs Graphical Processing Units by nVidia which can significantly speed up computation 4. The y_pred_list object and its calculations exist as a visualization of the predictive process going on in the loop. A first preliminary analysis shows the train dataset to contain over 270000 lines of information each concerning a Medium article. It also outperforms a GPT 2 classification model. float32 num_iteration lgb_opt. A combination of both text and non text data in one model would have been possible but this notebook provides the ability to compare them all in the same predictive task. This limits us to an analysis of the categorical and numerical features that can be replicated in both the train and the test set. Train test split based on totalClapCount with 20 of the train_small sample reserved for the test set. Natural language processing. Overall Ridge regression on the self created test set did not provide satisfactory results. ConclusionThe analyses performed above showed multiple approaches to the same problem The prediction of claps for a Medium article. This has been aided by big advances in computing power helping with the possibility of ever greater models being created. In their paper A tutorial on support vector regression Alex Smola and Bernhard Sch\u00f6lkopf describe the goal of a Support Vector regression to find a function f x that has at most \u03b5 deviation from the actually obtained targets for all the training data and at the same time is as flat as possible. Can be omitted for a final runthrough more than doubling the runtime. By default it detects Nouns only. The dataset contains NAs in multiple columns however not all of them. zero_grad Louis Stuff Louis Stuff lgb_x_train_regr lgb. One must thus ask themselves what a model should be more focused on capturing then A realistic number of claps for most of the articles or finding just those articles that might go viral. It is not task specific as each prediction approach used in this notebook requires its own preprocessing and data cleaning appropriate to the underlying algorithms. Stopwords are automatically removed as part of the lemmatization on the basis of the NLTK stopwords 11. Finally the text_cleaner function removes The same text cleaning is applied to the test set. The BERT model chosen was BertForSequenceClassification as this Classification head is the one best suited for Multiclass classification tasks. Regression Analysis on non text featuresThe non text features present are deserving of further investigation too. Afterwards predictions are made on the provided test set. The following code cell produces a CSV file which contains the indices and the predictions based on the svr_out dataframe. The Lemmatizer only focuses on alphabetic english words in accordance with the language analysis above. In the end the Lemmatized words are added back into the text_final column which is then used for further analysis. Accuracy function giving back the accuracy for training in a multiclass setting. Converting tokens to IDsCreate a smaller sample as the whole train_df is too large for efficient handling. Bert Pre training of deep bidirectional transformers for language understanding. Another advantage aside from the unmatched NLP prowess is its lack of requirement for extensive preprocessing. correlation between responses and totalclapcount visible def sentence_extract text lang en return sent_tokenize text LANGUAGES. This removes outliers lowering the Mean absolute error but does not help in detecting articles with a very high number of claps anymore. It can be mathematically posited as follows CrossEntropyLoss https drive. Nonetheless the BERT model in particular proves just how far pre trained embeddings have come in their understanding of human language with the use case presented here being just one of many in which they can be taken and used. Again the number of claps are not normal distributed with the log1p transformation helping but still hightlighting this fact. The field of research in computer assisted natural language processing has been experiencing immense changes over the last years with big steps forward still being made on a regular basis. Light Gradient Boosting Machine Light Gradient Boosting Machine 6. Length as used here is the strings in the text field divided into substrings by whitespace and then counted. The following graphic by Adrien Sieg shows how BERT fits into the general scheme of Natural Language Processing algorithms. The final ensemble output for the test set predictions. Context dependent environmentThe first part relates to the understanding of words themselves extracting its structure and meaning as well as a breakdown of its informational content. 2019 The Illustrated BERT ELMo and co. Tokenize package Nltk 3. Retrieved August 30 2020 from https topmediumstories. long type instead of float Bug fix thanks to chinhuic optimizer. The following implementation of a Tf Idf based SVR makes use of a train test split first and calculates predictions for a test set first to check the model s validity. Its mean is approximately 163 with a standard deviation of 1813. This takes multiple hours. In Advances in neural information processing systems pp. This helps in achieving greater validity through the use of multiple differently drawn samples for the predictive models. Further improvements on the mean absolute error could not be made however the robustness behind the model as a result of the parameter tuning should have increased. Average loss and accuracy are set to 0 for each epoch so that following each run through the dataset the loss can be compared hoping for an improvement. How NLP Cracked Transfer Learning. Duplicate removalIn datasets of this size especially ones containing data scraped from websites the question of potential duplicates must be posed early as the removal of duplicates can significantly alter the size of the dataset and accelerate further analysis. The version of BERT used here is case insensitive as the text content is scraped HTML and likely not formatted correctly in the beginning. Following is an examination of the data given after which preprocessing steps such as feature creation and text cleaning are performed. org stable modules generated sklearn. The mean_absolute_error and mean_squared_error are calculated below with the mean_absolute error being acceptable although not exceptionally high. The following code cell extracts the publication platform from the PublicationDetails. nbsp The BERT pre trained model from the GoogleAI has been described as mind blowing by machine learning developers involved with developing state of the art algorithms aiming for new highscores in comparison metrics such as GLUE and SQuAD 13. Steps involved are nearly identical with the production of a dataframe containing index and predicted Claps at the end. Retrieved August 30 2020 from http ai. This can be plotted in a scatter plot as seen below. Conclusion Conclusion 8. The train test split is based on the tokenized array and the target column with 20 of the data held back for test purposes. The last step is the context dependent understanding seeing how words and sentences carry information and meaning in a given domain thus understanding their meaning at a higher level. This process of self attention is visualized succinctly by Jay Alammar in this graphic 15 attention https drive. Ridge regression can therefore be considered an extremely rudimentary baseline not worthy of further optimization in this use case. Ensembling Ensembling 7. This is done here through the use of the dateparser package. Description of ImportsThe following imports are necessary for many computations in this notebook and are therefore at the beginning and not called at later points in the notebook. Seeing just how overly high some of the values in the Ridge predictions are it makes sense to assign them less weight in the combination of all predictions. BERT handles most of the preprocessing itself taking its pre trained knowledge and applying it to the new texts it shall handle. The ensemble created from all predictions here tries to find the middle ground for the problem established above its overall MAE and MSE might not be award winning but it might likely still pick up on much clapped articles. Language analysisWith a dataset this varied in content it is of question which languages are present in the texts. Different calculation methods were tried with the ones commented out discarded for lack of improvement on the test set. Predictions on provided test setThe predictions made here are for the provided test set. For some reason the Ridge model predicts one single absurdly high value for one of the articles. What is gradient accumulation in deep learning Medium. Analysis of TotalClapCount As described above the totalClapCount variable has its mean at around 163. The following code cell converts the text into token an ID array with the NAs replaced with a dummy value to avoid errors in tokenization. com louispk adams ss20 final As the notebook has to be set to public in order for the link to work it will only be public after the final submission deadline starting from the 1st of September 2020 guaranteeing no other person can access it before. This suggests a highly imbalanced distribution which is plotted below. This makes these languages difficult to use in NLP tasks as existing vocabulary cannot be easily expanded for these languages. The following Natural language processing analyses try to extract information from text on different levels as can be compared to the process described above. As Tf Idf is case sensitive this is necessary. com 2019 12 albert lite bert for self supervised. This furthers the time consuming aspect of BERT. Tf Idf Vectorizer with Support Vector Regression Tf Idf Vectorizer with Support Vector Regression 2. The epsilon value specifies the epsilon tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value 12. The Nvidia apex repository needs to be present in order for it to be installed for this session runtime. Lemmatization must be performed on the test set too. The Author column in the test dataset contains the authors names only in the first 39 lines after which the author is gradually replaced by the date in this column. The indices are dropped then to make further merges easier. Data exploration Data importWhen importing the data into the Kaggle Notebook it becomes immediately clear that the train dataset is quite large. To circumvent this the author name was extracted from the PublicationDetails column. A combination of both would be perfect but is as of now still elusive. Byte pair encoding Credit Jay Alammar Bytepairencoding https drive. The optimizer performs an optimization step every two accumulation steps and can take a step backwards if necessary. This involves the order of words their part in a given sentence and the sentence itself. This is not guaranteed. Predictions on the validation data set look okay however the model does not seem to pick up on outlier cases. The text preparation steps in BERT are as follows 1. Thus claps are a whole number integer representative of how many people like a certain Medium post. The Support Vector Regression is performed below with different versions and Kernels being commented out but left there for interesting comparisons. Tf Idf Vectorizer with Support Vector RegressionThe Tf Idf Vectorizer is the short form of a term frequency inverse document frequency Vectorizer. In the use case here it presents a basic first baseline for the clap predictions made by using only non text variables. Personal experience has shown that BERT and GPT 2 do better with classes recategorized on a scale from 1 to x with x being the maximum number of classes. 2018 The Illustrated BERT ELMo and co. com httpwwwfszyc bert in keras taming create tensor dataset Y_train tensor type changed to long for Bert Multiclass as Crossentropyloss expects tensor. Language detection on the train set using the cleaned text. To do so the predictions are detached from the CPU further dimensions are removed through. Predictions on validation setIn this first prediction part with the now trained BERT model predictions are made on the test set created from the train set above. y_pred is a tensor vector containing predictions for each batch as determined by the model receiving the x_batch tensor values with an attention_mask binary tensor indicating which tensors to focus on. This is then used in the prediction loop where the model predicts classes for each batch object. The imports here enable preprocessing prediction and error calculation with a random seed ensuring replicability. The text column is replaced by the clean_text column. Numeric features especially can carry strong correlations with the target variable. The task at hand here is more suited to a regression analysis however BERT s last layer cannot be easily modified for a regression function. This ensemble can be created in many different ways. Following is an overview of the totalClapCount column. The distances between each bin increase in order to have a workable number of samples for each bin. The Support Vector regression can also be performed with standardized values as performed below. A first rough combination of the predictions in one dataframe reveals how different the predictions value achieved by each one are. Both tensor types in the train set have to be of type torch. This code cell allows for the VRAM CUDA Cache to be flushed opening it up for additional calculation. This notebook can be run chronologically in order with the available resources given by Kaggle. A high number of responses is very likely to be correlated with a high number of claps as people tend to leave a comment response only under articles they are already interested in. This saves valuable time. The describe and dtypes functions show the extent of information contained in the train dataset. Designed and distributed by Microsoft it is has won many competitions aimed at predicting numeric variables 22. Retrieved August 30 2020 from https polyglot. It remains to be seen whether the BERT based predictions can improve upon the Tf Idf SVR predictions. com bedigunjit simple guide to text classification nlp using svm and naive bayes with python 421db3a72d34 12 Sklearn. Preparatory analysis is not limited to the train dataset but also extends to the test dataset. com what is gradient accumulation in deep learning ec034122cfa 19 Apex. However many like codeBlockCount imageCount socialRecommendsCount and others are not present in the test set and cannot be realistically replicated with the information provided in the test set. In case it still does not suffice additional sequences will be used for a text. Link to the Notebok on Kaggle https www. This way the clean_text column does not get modified by the following preprocessing steps allowing for repeated execution. The BERT model seems to consistently predict the lowest values while the non text based ones predict very high values. While BERT does not necessarily need cleaned text it makes sense to use the cleaned text from above for calculation as it will likely at least help in speeding up the process. To give the predicted values into a visually more appealing look they are put into a dataframe and retransformed from class bins into whole numer Claps. com 22 Microsoft lightgbm. Lemmatization involves the reduction of a word to its base form while taking into account contextual meaning thus reducing errors prone to happen when using a Stemming function only The tag_map is necessary to provide the Wordnetlemmatizer with tags indicating whether a word is a noun verb adjective or adverb. com max 968 1 F6SrJR7_s95r6oCF3ugMZw. Visual inspection of a few heads and URLs suggests some authors to post a multitude of articles. Version 2 Bug fix thanks to chinhuic This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. com followed by hackernoon. float32 label Y_valid_regr lgb_pred np. The most common publisher in the train set is towardsdatascience. The criterion needs to be defined before execution and initialized as an instance of CrossEntropyLoss. Importing the Bert tokenizer. Seeing that they make up less than 0 4 of the test dataset these languages can be overlooked in NLP based analysis. Byte pair encoding represents a well working and efficient compromise between word level and character level encodings 17. Retrieved August 30 2020 from https scikit learn. This is then compared to the correct class as given in y_test which contains the actual values. The Dataloader object is iterated through in batches with a progressbar itarator visible for the duration of training in order to visualize progress. Claps are a form of appreciation users can give an article on Medium. The optimum parameters are printed out after execution. Inverse Document Frequency Logarithmically scaled inverse fraction of texts with the term in themTf Idf calculates numerical values for each word in a given text based on the number of times it appears in a text and the relative occurence of the word in all texts given to the Tf Idf algorithm 8. In gradient accumulation a configured number of steps are run with update of the model variables. Defining the device as a CUDA capable pytorch device. The number of accumulation steps was left at two as this mostly relates to the ability of parallel GPU based computing through Gradient accumulation. For the parameters used in the first SVR approach with the train and test split as above the mean absolute error can be seen below. If it doesn t it adds the sentence to the text that is now cleaned from duplicates. However it presented itself as quite difficult to predict the claps for articles which have received thousands in some cases hundreds of thousands of likes. The implementation used here is based on the pyTorch library an open source machine learning library capable of almost all state of the art NLP algorithms 17. Changes in parameters especially in the BERT implementation can break the notebook and require a restart. io illustrated gpt2 16 Pytorch. The NLTK package provides a Lemmatizer function which is used here to Lemmatize each word. The BERT model used here consists of twelve encoder layers 768 hidden units and 12 attention heads. However the max and min number of predictions seem to quite in line with a realistic number of claps expectable. These values are tensor values which need to be converted into legible values. The plot suggests a positive correlation between the two however it is weak. With few articles having over 10000 claps all having received more are put into one bin. Natural language processing prediction approaches are made using two separate approaches with the first one being a Tf Idf Support Vector Regression SVR followed by a BERT based multiclass classification approach. Considering these articles to be exceptional in their claps their content usually is not particularly different from other articles that might receive less than 5 of the claps. Natural language processing and predictions The most important part of this predictive analysis is the Natural language processing part. The following code cell executes multiple steps necessary for Tf Idf word vectorization. walk kaggle input for filename in filenames print os. Polyglot gives a prediction for each language it knows and uses the highest prediction as the most likely language. Running BERT requires the use of a GPU accelerator and the VRAM to be completely free at the beginning. First a Dataloader object has to be created from the validation dataset. The most common values present in the dataset are very small which is why more bins were created in the lower range. com uc export download id 1fx2ZEoLMCvRhORAWiXEJ3z1MHZpjXwiL One specialty of BERT is its use of a CLS and SEP token. png A Support Vector Machine SVM is a supervised machine learning approach originally developed for classification tasks that can be successfully used for regression tasks. The original train dataset is sampled to create a smaller more computable sample. The BERT model used here is one of these pre trained models. This suggests that many articles in the dataset are on scientfic and computer science related topics with high informational content. This way comparisons between the real value and predictive values can be made. The numerical features that can be directly replicated are the Length and Responses. A dataframe is created for the prediction with a truth column containing the actual number of claps and the pred column containing the predicted number of claps. Running the same language detection on the train file led to a problem The input file contains Unicode characters the Polyglot function cannot handle as can be seen below. However bumps are visible at 50 and 100 claps. In Fundamentals of Artificial Intelligence pp. INSIDE MEDIUM S MELTDOWN How an idealistic Silicon Valley founder raised 134 million to change journalism then crashed into reality. A sample of the whole dataset is used here to ensure that the other predictions can also be run without having to restart the notebook thereby loosing the output of the SVR. read_csv Input data files are available in the. The multiclassification has a built in error as the bins do not always exactly match the totalClapCount allowing for approximation with the loss as part of the algorithmic approach. Of particular value here is the totalClapCount column as this is the target variable which will be predicted in the test set. Data cleaning Data cleaning 4. Light Gradient Boosting MachineLight GBM short for Light Gradient Boosting Machine is a tree based gradient boosting implementation. After duplicate removal the train file contains 72337 lines. This allows words to be split into more frequent subwords and reduces the vocabulary size immensely. The learning rate is the step size made at each iteration and needs to be set at a low enough rate to detect minima without being too low so that it gets stuck or overfits. To do so multiple text preprocessing functions are used 1. com articles based on their textual content and other information given. They will be taken care of in further calculations. Accordingly it consists of two parts which both get combined for the final value 1. to device command which directly sends the underlying vector calculations to be performed on the device in this case defined above as the Cuda device. While the Header Text Length Responses and Author are present in almost the same way in the train file the PublicationDetails column contains the publicationdomain the publishing date and the author again. Statistics and computing 14 3 199 222. These values are therefore obviously false and need to be corrected. An appropriate number of of training steps per epoch is dependent on the size of the train set batch size and accumulation steps and should be of a size that the whole training set can be iterated through during training. Again features used here can only be created from information available in the test set. Twenty bins have been chosen in order to gain more separation in categories as the number of claps increases. Introduction Introduction 2. Depending on the type of task predicting articles with a very high number of claps might be more preferable as they could for example be more actively recommended by the Medium. Two of these were in a language not using the Latin Alphabet namely Chinese and Japanese. The train_dataset contains a lot more useful variables which could help in obtaining better prediction results. Calculating the number of times a single author has published something did not lead to significant insight as it is Netflix Technology and TE FOOD who make up the most often occurences with 175 and 34 times respectively. In the Medium articles given the content ranges widely in topics however it is likely that certain keywords attract special attention and thereby Claps as Medium. png Table of Contents 1. Output files from each predictive apporach are contained in the adams2020 folder for easier replicability. This problem might be helped in part by the removal of stopwords and the performed cleaning of the text above which reduces the text length quite a bit. BERT s vocabulary consists of approximately 30000 subwords allowing it to represent a vast majority of words with a fixed vocabulary saving much valuable RAM by using a fixed size. A higher batch size would have allowed for better approximation but sizes starting at 32 can be considered for adequate results. CrossEntropyLoss as implemented in pyTorch combines a LogSoftmax with a NLLLoss. In other words we do not care about errors as long as they are less than \u03b5 but will not accept any deviation larger than this 9. io illustrated bert 4 Kirk D. com microsoft LightGBM blob master examples README. Improvements on BERT such as the recently released ALBERT still dominate Natural Language Processing benchmarks proving the supreme ability of BERT in handling understanding and solving human level text tasks 14. Support vector regression allows for relatively fast estimation while having adequate accuracy. The LGBM seems promising for the Ensembling with the results of the text based predictions. Without actual knowledge it can be guessed that Length is a numerical representation of the number of characters present in the Text. valid_preds contains the predictions for the validation dataset. Overview of Embeddings and models Credit Adrien Sieg embedding https drive. The first imports are for the data exploration and analysis part with adequate visiualization packages such as seaborn being imported too. These two tokens are not present in other pre trained models such as GPT 2. png attachment image. Good results on the test set were reached with a Linear Kernel and a C value of 50 which ensures higher weighting of the outliers and enables greater deviation from the mean of the regression. Some imports here are already NLTK and BERT related although more imports for both are done later as they require package installs that take up time therefore only being done at the point of execution. The Responses column is modified in such a way that only the numeric response values are left with no textual string content in the cells left. com uc export download id 1bOHPwkRoOLO7wk8sDKbmE 2bEcMeqxhT Functionally BERT makes use of the Transformer architecture which itself makes use of the concept of self attention. Test train split along the totalClapLog is performed for the regression trainset with the number of claps dropped from the train and validation set afterwards. Seeing that one variable might be correlated with the target variable totalClapCount it only makes sense to take a deeper look at the number of claps itself. It extracts the date from any text and save it as a list element. This code cell allows for the outputs of each predictive approach as saved in the subfolder and computed during earlier runs to be imported in order to faster create the ensemble model again. To get a further feel for the number of claps the log1p transformed Claps are visualized here. The GPU is specifically targeted in the loop through the. Retrieved August 15 2020 from http jalammar. While it is possible to use BERT in a regression context by using the sentence embeddings of all sentences generated by BERT as inputs for a logistic regression model in the case of our already performed logistic regression it is more intuitive to convert the dataset at hand into a multiclass classification task suitable dataset and predicting classes which can then be used in an ensemble with the predictions generated above. BERT Multi classification This implementation of a Bidirectional Encoder Representations from Transformers short BERT is inspired by publicly available work from Kaggle users yuval reina and Abhishek Thakur who both developed implementations for BERT in the KAGGLE environment providing valuable help with the use of GPU acceleration. Simple guide to text classification Nlp using svm and naive bayes with python. This mechanism at the base of BERT helps it be successful at various tasks from question answering to text generation to text classification. com inside the meltdown of evan williams startup medium 2017 2 2 The new rules of social journalism A proposal. The SEP token marks the end of a sequence while the CLS classification token tries to capture a tokenized sequence s meaning. Here too the Length is calculated by splitting the strings into substrings anc counting them. The following code cell produces two separate plots showing the number of articles released by each publication domain. predict X_valid_ridge. For example running this by clicking run or pressing Shift Enter will list the files in the input directory for dirname _ filenames in os. Machine learning has reached a point where the understanding of complex and context dependent texts is possible with high accuracy and allows for the capture of meaning and sentiments in language level tasks 3. ", "id": "louispk/adams-ss20-final", "size": "52149", "language": "python", "html_url": "https://www.kaggle.com/code/louispk/adams-ss20-final", "git_url": "https://www.kaggle.com/code/louispk/adams-ss20-final", "script": "lightgbm Ridge stats SVR date preprocessing tqdm_notebook norm BertAdam scipy sklearn.linear_model polyglot.detect.base matplotlib.pyplot ImageColorGenerator PorterStemmer metrics PIL search_dates plot probplot __future__ sklearn.pipeline scipy.stats WordCloud sent_tokenize wordnet as wn duplicate_remover collections torch.nn.functional sklearn.svm CountVectorizer accuracy_score pyLDAvis nltk.stem torch.nn sklearn convert_lines wordnet division nltk svm pandas print_function model_selection nltk.corpus text_cleaner pos_tag GridSearchCV InteractiveShell string sentence_extract BertForSequenceClassification nltk.tokenize numpy Image make_pipeline Detector defaultdict WordNetLemmatizer IPython.core.interactiveshell BertConfig LabelEncoder sklearn.metrics StandardScaler train_test_split apex multi_acc BertTokenizer polyglot.detect seaborn TfidfVectorizer amp STOPWORDS SVC sklearn.feature_extraction.text mean_absolute_error dateparser.search tqdm roc_auc_score pytorch_pretrained_bert sklearn.model_selection matplotlib.gridspec datetime stopwords LinearSVR naive_bayes mean_squared_error digits convert_tf_checkpoint_to_pytorch word_tokenize logger = {v wordcloud absolute_import sklearn.preprocessing ", "entities": "(('Length', 'whitespace'), 'be') (('model', 'just a few claps'), 'be') (('ever greater models', 'possibility'), 'aid') (('It', 'average'), 'seem') (('author name', 'PublicationDetails column'), 'extract') (('y_pred_list object', 'loop'), 'exist') (('clean_text column', 'repeated execution'), 'way') (('loss', 'improvement'), 'set') (('Regression Analysis', 'text non features'), 'feature') (('this', 'Gradient based accumulation'), 'leave') (('where model', 'batch object'), 'use') (('actual article', 'billiards'), 'highlight') (('Results', 'Spanish texts'), 'reveal') (('clapcat column', 'training'), 'define') (('number', 'categories'), 'be') (('which', 'BERT'), 'look') (('obvious they', 'hand'), 'be') (('Kernels', 'there interesting comparisons'), 'perform') (('which', 'article'), 'transform') (('Tf Idf Vectorizer', 'inverse document frequency short term frequency Vectorizer'), 'be') (('which', 'then further analysis'), 'add') (('column Afterwards type', 'integer'), 'change') (('predictions', 'final prediction'), 'ensemble') (('it', 'at least process'), 'clean') (('1fx2ZEoLMCvRhORAWiXEJ3z1MHZpjXwiL One specialty', 'CLS token'), 'com') (('that', 'GPLv3 license'), 'describe') (('INSIDE MEDIUM Silicon Valley How idealistic founder', 'then reality'), 'S') (('Dataloader object', 'progress'), 'iterate') (('criterion', 'CrossEntropyLoss'), 'need') (('Test', 'task'), 'be') (('This', 'claps'), 'remove') (('long instead float Bug', 'chinhuic optimizer'), 'type') (('tree', 'which'), 'set') (('above recommends column', 'claps'), 'suggest') (('that', 'regression successfully tasks'), 'be') (('It', 'list element'), 'extract') (('train file', '72337 lines'), 'contain') (('predictions how value', 'one'), 'reveal') (('it', 'test again set'), 'become') (('setThe predictions', 'test here provided set'), 'prediction') (('LGBM', 'text based predictions'), 'seem') (('so predictions', 'CPU further dimensions'), 'detach') (('extraordinarily many big companies', 'just that'), 'pose') (('BERT', 'language already full knowledge'), 'pretraine') (('it', 'fixed size'), 'consist') (('Retransformation', 'log1p'), 'transform') (('This', 'BERT'), 'further') (('which', 'Cuda above device'), 'to') (('transformed Claps', 'claps'), 'get') (('Two', 'namely Chinese'), 'be') (('which', 'next step'), 'indicate') (('using', 'input only two variables'), 'be') (('model variable', '18'), 'add') (('it', 'processing more legible further steps'), 'clean') (('response only numeric values', 'cells'), 'modify') (('Netflix TE who', '175 times respectively'), 'lead') (('enough standard', 'definite improvement'), 'achieve') (('This', 'above Javatpoint'), 'depict') (('prevent', 'therefore Cache'), 'occur') (('text here case content', 'likely correctly beginning'), 'be') (('test where text', 'notebook'), 'easy') (('penalty', 'actual value'), 'specify') (('prediction specific approach', 'underlying algorithms'), 'be') (('implementation', 'art'), 'base') (('This', 'part given sentence'), 'involve') (('However runtimes', 'very even significant computational backing'), 'be') (('tensor which', 'legible values'), 'be') (('more specifically strings', 'substrings'), 'tokenize') (('LightGBM', 'parameters'), 'be') (('This', 'predictive models'), 'help') (('gradient accumulation', 'deep learning Medium'), 'be') (('describe', 'train dataset'), 'show') (('dataset', 'them'), 'contain') (('y_pred', 'tensors'), 'be') (('train_df', 'too efficient handling'), 'be') (('support vector', 'two separate spaces'), 'include') (('Twenty bins', 'claps increases'), 'choose') (('code following cell', 'publication domain'), 'produce') (('SVMs', 'linear non classification'), 'use') (('training whole set', 'training'), 'be') (('optimum parameters', 'execution'), 'print') (('more they', 'more actively Medium'), 'be') (('which', 'self attention'), 'com') (('correlation', 'text LANGUAGES'), 'sent_tokenize') (('it', 'session'), 'need') (('most clapped article', '2020 no more than 281000 claps'), 'predict') (('Steps', 'end'), 'be') (('which', 'dataframe'), 'produce') (('way comparisons', 'real value'), 'make') (('less than 0 4', 'NLP based analysis'), 'overlook') (('Improvements', 'more data'), 'come') (('Tf Idf Vectorizer', 'vectorized form'), 'take') (('train original dataset', 'smaller more computable sample'), 'sample') (('that', 'now duplicates'), 'add') (('Afterwards predictions', 'test provided set'), 'make') (('It', 'same platform'), 'be') (('train test split', 'test back purposes'), 'base') (('notebook', 'Kaggle'), 'run') (('CUDA yet part', 'packages'), 'take') (('author', 'column'), 'contain') (('32 as above shuffling', 'batch size'), 'create') (('Crossentropyloss', 'tensor'), 'change') (('languages', 'texts'), 'analysisWith') (('average number', 'quite more than 1720 claps'), 'set') (('however it', 'two'), 'suggest') (('code following cell', 'tokenization'), 'convert') (('longer articles', 'more claps'), 'assume') (('This', 'between 1 second plot'), 'understand') (('Dataloader First object', 'validation dataset'), 'have') (('size', 'text'), 'be') (('bins', 'algorithmic approach'), 'have') (('maximum', 'array'), 'have') (('numpy which', 'valid_preds array'), 'squeeze') (('same sentence', 'already cache'), 'take') (('extraction', 'date'), 'Details') (('Personal experience', 'deep neural net'), 'show') (('Kernels', 'hyperplane decision common boundary'), 'use') (('most common publisher', 'train set'), 'be') (('Further improvements', 'parameter tuning'), 'increase') (('however it', 'XGBoosting framework'), 'make') (('This', 'dateparser package'), 'do') (('Gradient Boosting MachineLight Light GBM', 'Light Gradient Boosting Machine'), 'short') (('only sentence', 'whitespaces'), 'remove') (('early removal', 'further analysis'), 'one') (('pair encoding', 'word level'), 'byte') (('wordCount', 'totalClapCount'), 'plot') (('regression', 'phenomenon'), 'cause') (('MODEL create', 'learning rate'), 'CREATION') (('single extreme outlier these', '0'), 'normalized') (('each', 'Medium article'), 'show') (('mean_absolute_error', 'mean_absolute below error'), 'calculate') (('Furthermore he', 'step multiple three main parts'), 'posit') (('users', 'Medium'), 'be') (('that', 'claps'), 'combine') (('GridSearch Crossvalidation', 'optimum ones'), 'guarantee') (('vastly it', 'predictions'), 'make') (('GPU', 'the'), 'target') (('Support Vector regression', 'also standardized values'), 'perform') (('object describe function', 'mean median'), 'with') (('certain keywords', 'thereby Medium'), 'range') (('Polyglot function', 'Unicode characters'), 'run') (('it', 'new texts'), 'handle') (('train', 'train present set'), 'contain') (('Bibliography Bibliography IntroductionThis notebook', 'Medium'), 'concern') (('totalClapCount', 'MAE'), 'calculate') (('it', 'tensor item'), 'be') (('This', 'assumptions'), 'be') (('publication missing most common domain', 'test set'), 'be') (('which', 'highly imbalanced distribution'), 'suggest') (('CrossEntropyLoss', 'inputs'), 'take') (('it', 'sentence'), 'com') (('they', 'numer whole Claps'), 'put') (('It', 'use'), 'tune') (('long Crossentropyloss', 'later execution'), 'expect') (('they', 'which'), 'prove') (('Description', 'notebook'), 'be') (('mean wordCount', 'article'), 'suffice') (('This', 'scatter plot'), 'plot') (('how trained embeddings', 'regression context'), 'highlight') (('CLS classification token', 'tokenized meaning'), 'mark') (('that', 'execution'), 'be') (('notebook', 'same predictive task'), 'be') (('only succesful runs unsuccessful runs', 'notebook restart'), 'run') (('it', 'predictions'), 'make') (('what', '6 Welcome documentation'), 'io') (('primary advantage', 'pre them'), 'be') (('immensely however size', 'GPU single machines'), 'show') (('code following cell', 'PublicationDetails'), 'extract') (('60 more than million users', 'one 2017 1'), 'read') (('pair encoding', 'embedding'), 'byte') (('metric', 'case'), 'be') (('they', 'max_seq_length'), 'padding') (('combination', 'now still elusive'), 'be') (('prediction first part', 'train'), 'setIn') (('target which', 'test set'), 'be') (('it', 'title'), 'lie') (('they', 'only articles'), 'be') (('language processing following Natural analyses', 'process'), 'try') (('too it', 'minima'), 'be') (('BERT tensorflow model', 'BERT pyTorch compatible model'), 'convert') (('validation scores', 'metric specified'), 'LightGBM') (('examination', 'feature such creation'), 'perform') (('short plot', 'totalClapCount distribution'), 'transform') (('advantage', 'extensive preprocessing'), 'be') (('configured number', 'model variables'), 'run') (('Tf Idf vectorization', 'article'), 'replicate') (('language Natural most important part', 'predictive analysis'), 'processing') (('which', 'predictions'), 'be') (('Stopwords', 'NLTK stopwords'), 'remove') (('VRAM CUDA Cache', 'additional calculation'), 'allow') (('scale_loss', 'GPU'), 'call') (('outputs', 'faster ensemble model'), 'allow') (('many articles', 'Claps'), 'receive') (('which', 'duplicates'), 'use') (('sequence full length', 'P100 Kaggle'), 'limit') (('it', 'available GPU'), 'use') (('valid_preds', 'validation dataset'), 'contain') (('more computational power', 'data'), 'make') (('how BERT', 'Natural Language Processing algorithms'), 'show') (('It', 'python docker image https kaggle github'), 'version') (('read_csv Input data files', 'the'), 'be') (('batch size', 'mostly available VRAM'), 'choose') (('so first look', 'duplicates'), 'reveal') (('predictionsThe Ridge predictions', 'validation set'), 'set') (('Lemmatizer', 'language analysis'), 'focus') (('most articles', '0 claps'), 'show') (('Output files', 'easier replicability'), 'contain') (('however model', 'outlier cases'), 'look') (('performed cleaning', 'text length'), 'help') (('two tokens', 'such GPT'), 'be') (('values', 'Support Vector directly Machine'), 'be') (('ConclusionThe analyses', 'Medium article'), 'perform') (('publishing digital people', 'all world'), 'be') (('dependent how words', 'higher level'), 'be') (('where understanding', 'language level tasks'), 'reach') (('png Data CleaningThe data cleaning', 'dataset'), 'focus') (('Overview', 'models Credit Adrien https drive'), 'embed') (('text column', 'clean_text column'), 'replace') (('which', 'prediction better results'), 'contain') (('what', 'ec034122cfa 19 Apex'), 'com') (('imports', 'replicability'), 'enable') (('Length variable', 'test set'), 'be') (('further corrections', 'notebook'), 'set') (('Predictions', 'GridSearch determined values'), 'make') (('process', 'attention https graphic 15 drive'), 'visualize') (('text same cleaning', 'test set'), 'remove') (('Changes', 'restart'), 'break') (('words', 'vocabulary size'), 'allow') (('non text based ones', 'very high values'), 'seem') (('learning rate', '3e'), 'choose') (('language processing prediction Natural approaches', 'Tf Idf Support Vector Regression multiclass classification BERT based approach'), 'make') (('Running', 'RAM constraints'), 'lead') (('which', 'own vocabulary'), 'be') (('However bumps', '50 claps'), 'be') (('data exploration part', 'such seaborn'), 'be') (('Following', 'totalClapCount column'), 'be') (('here LGBM', 'good big datasets'), 'serve') (('it', 'Tf Idf algorithm'), 'scale') (('language Overall English texts', 'texts'), 'make') (('as long they', 'larger 9'), 'care') (('LGBM', 'Ridge regression'), 'achieve') (('it', 'text'), 'use') (('which', 'final value'), 'consist') (('which', 'coefficient'), 'be') (('it', 'numeric variables'), 'design') (('BERT', 'maximum classes'), 'show') (('important step', 'TensorDataset'), 'be') (('lang english Installing Nvidia Apex', 'https Thanks www'), 'get') (('languages', 'easily languages'), 'make') (('response information', 'cells'), 'replace') (('received', 'more one bin'), 'put') (('Chowdhary', 'language important natural understanding'), 'be') (('prediction', 'claps'), 'contain') (('text preparation steps', '1'), 'be') (('Lemmatization', 'test'), 'perform') (('Abhishek who', 'GPU acceleration'), 'classification') (('NLP trained based approach', 'thus second steps'), 'try') (('it', 'likely still much clapped articles'), 'try') (('that', 'hand'), 'do') (('sentence_extract First function', 'sentences'), 'use') (('bin original names', 'reversal'), 'write') (('that', 'claps'), 'be') (('field', 'forward still regular basis'), 'assist') (('dataframe', 'claps'), 'create') (('BERT based predictions', 'Tf Idf SVR predictions'), 'remain') (('only three texts', 'English'), 'reveal') (('Improvements', 'level text human tasks'), 'dominate') (('representation', 'sentence level meaning'), 'involve') (('language level human text', 'necessary information'), 'aim') (('They', 'further calculations'), 'take') (('which', 'regression'), 'reach') (('slug', 'instead less human readable i 5'), 'be') (('text non features', 'Ridge Regression'), 'analyse') (('optimizer', 'step'), 'perform') (('why more bins', 'lower range'), 'be') (('Overall Ridge regression', 'satisfactory results'), 'provide') (('how fine tune', 'parameters'), 'pushkarmandot') (('which', 'here Lemmatize'), 'provide') (('ensemble', 'many different ways'), 'create') (('particular how number', 'claps'), 'show') (('mean', '1813'), 'be') (('many articles', 'high informational content'), 'suggest') (('creation', 'regression task'), 'be') (('Classification head', 'Multiclass classification best tasks'), 'be') (('PublicationDetails column', 'publishing date'), 'contain') (('text features featuresThe non present', 'further investigation'), 'Analysis') (('totalClapCount it', 'claps'), 'see') (('Test train', 'train'), 'perform') (('It', 'CrossEntropyLoss https mathematically drive'), 'posit') (('preparatory steps', 'regression adequate analysis'), 'be') (('you', 'output'), 'join') (('learning thereby how much rate', 'iteration'), 'define') (('Support vector regression', 'adequate accuracy'), 'allow') (('words', 'too BERT'), 'be') (('outputs', 'ensemble model'), 'combine') (('mean absolute error', 'Ridge regression'), 'seem') (('trained model', 'such GLUE'), 'nbsp') (('as mean absolute error', 'train'), 'see') (('it', 'most likely language'), 'give') (('train immediately dataset', 'Kaggle Notebook'), 'import') (('Length', 'present Text'), 'guess') (('which', 'language level much often still competitively tasks'), 'compare') (('Transformer', 'one'), 'allow') (('however last layer', 'regression easily function'), 'be') (('here it', 'text only non variables'), 'present') (('CrossEntropyLoss', 'NLLLoss'), 'combine') (('BERT model', 'encoder here twelve layers'), 'consist') (('which', 'likes'), 'present') (('codeBlockCount imageCount However many socialRecommendsCount', 'test set'), 'be') (('com tutorial machine learning images', 'vector machine algorithm'), 'support') (('it', 'text classification'), 'help') (('However max number', 'claps'), 'seem') (('validation throughout score', 'always round 200'), 'set') (('Again number', 'still fact'), 'be') (('BERT model', 'here pre trained models'), 'be') (('format', 'pyTorch environment'), 'be') (('here other predictions', 'SVR'), 'use') (('Retrieved August', 'https 30 2020 scikit'), 'learn') (('calculation Different methods', 'test set'), 'try') (('Numeric', 'target variable'), 'feature') (('This', 'more responses'), 'be') (('It', 'memory usage'), 'have') (('Albert 13 lite bert', 'language representations'), 'html') (('code following cell', 'Tf Idf word necessary vectorization'), 'execute') (('it', 'value multiple gigabyte big comma separated file'), 'turn') (('Ridge regression', 'use case'), 'consider') (('predictions', 'them'), 'contain') (('themselves', 'as well informational content'), 'relate') (('It', 'classification also GPT 2 model'), 'outperform') (('following implementation', 'first validity'), 'make') (('data Further exploration', 'Internet'), 'necessitate') (('that', 'train'), 'limit') (('tensor types', 'type torch'), 'have') (('sizes', 'adequate results'), 'allow') (('that', 'just articles'), 'ask') (('20', 'test set'), 'split') (('Here too Length', 'them'), 'calculate') (('authors', 'articles'), 'suggest') (('at most deviation', 'same time'), 'in') (('themselves', 'Medium'), 'com') (('number', 'given text'), 'frequency') (('word', 'tags'), 'involve') (('which', 'actual values'), 'compare') (('it', 'Byte Pair Encoding'), 'be') (('Thus claps', 'certain Medium post'), 'be') (('which', 'lemmatized words'), 'hand') (('which', 'significantly computation'), 'offer') (('Ridge model', 'articles'), 'predict') (('some', 'significant standard deviation'), 'show') (('Tf Idf', 'word vectorization other methods'), 'use') (('23 advantage', 'further preprocessing'), 'achieve') (('Again features', 'test available set'), 'create') (('csv', 'ROBERTA'), 'come') (('2020 other person', 'it'), 'adams') (('training whole data', 'training'), 'com') (('Running BERT', 'completely beginning'), 'require') (('Preparatory analysis', 'test also dataset'), 'be') "}