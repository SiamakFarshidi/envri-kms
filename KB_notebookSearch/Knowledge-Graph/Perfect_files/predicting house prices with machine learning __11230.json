{"name": "predicting house prices with machine learning ", "full_name": " h1 Predicting House Prices with Machine Learning h2 Introduction h2 0 Understanding the Client and their Problem h2 1 Loading Data and Packages h2 2 Analyzing the Test Variable Sale Price h2 3 Multivariable Analysis h2 4 Impute Missing Data and Clean Data h3 Imputing Missing Values h2 5 Feature Transformation Engineering h3 Fixing skewed features h2 6 Modeling and Predictions h3 Stacked models h3 XGBoost h3 LightGBM h3 Ensemble Prediction h3 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "TotRmsAbvGrd Total rooms above grade does not include bathrooms 9. Since this feature has mostly SBrkr we can set that for the missing value. We will just substitute in the most common string SaleType Fill in again with most frequent which is WD MSSubClass Na most likely means No building class. Fixing skewed features. Thank you hipsters So we can t really establish any particular order of response to be better or worse than the other. Here is a short description of each. We are going to take advantage of all of the feature variables available to use and use it to analyze and predict house prices. TotalBsmtSF Total square feet of basement area6. Stacked models XGBoost LightGBM Ensemble PredictionNote To get our weights for each model we ll take the inverse of each regressor and average it out of 100 Submission set png here when working on notebook Load train and Test set Check the numbers of samples and features Save the Id column Now drop the Id column since it s unnecessary for the prediction process. Going to have to fix this later We want our data to be as normal as possible. Important Note This data is from Ames Iowa. MSSubClass Identifies the type of dwelling involved in the sale. Categorical data is just like it sounds. Here we fix all of the skewed data to be more normal so that our models will be more accurate when making predictions. Here data_description. Let s do it It s a nice overview but oh man is that a lot of data to look at. Like the features that make up a person an educated party would want to know all aspects that give a house its value. 01 point Pearson R Score increase but looks much better Everything looks fine here. Note removal of data is totally discretionary and may or may not help in modeling. Moreover from a substantive perspective we need to ensure that the missing data process is not biased and hiding an inconvenient truth. BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath and BsmtHalfBath Replacing missing data with 0. FullBath Full bathrooms above grade8. 4 car garages result in less Sale Price That doesn t make much sense. Well the most correlated feature to Sale Price is. However today that is not the case. What doesn t make sense is the two datapoints in the bottom right of the plot. The location is extremely correlated with Sale Price. Numerical data is data in number form. For example a 2 000 square foot place is 2 times bigger than a 1 000 square foot place. We can then safely remove it. August 2017 IntroductionThis notebook is going to be focused on solving the problem of predicting house prices for house buyers and house sellers. We can replace missing values with None 5. Modeling and Predictions 0. Here we average ENet GBoost KRR and lasso. Since the house with NoSewa is in the training set this feature won t help in predictive modelling. GarageCars Size of garage in car capacity4. Let s combine both training and test data into one dataset to impute missing values and do some cleaning. The client will invest on making rooms at a small cost to get a large return. What does that mean Is a 90 type 3 times better than a 30 type This feature was interpreted as numerical when it is actually categorical. Impute Missing Data and Clean DataImportant questions when thinking about missing data How prevalent is the missing data Is missing data random or does it have a pattern The answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. The types listed here are codes not values. GarageArea Size of garage in square feet5. Feature Transformation Engineering6. MasVnrArea and MasVnrType NA most likely means no masonry veneer for these houses. Checking performance of base models by evaluating the cross validation RMSLE error. KitchenQual Only one NA value and same as Electrical we set TA which is the most frequent for the missing value in KitchenQual. 1stFlrSF First Floor square feet7. BsmtQual BsmtCond BsmtExposure BsmtFinType1 and BsmtFinType2 For all these categorical basement related features NaN means that there isn t a basement. For the other 9 they are as listed. Multivariable AnalysisLet s check out all the variables There are two types of features in housing data categorical and numerical. Functional data description says NA means typical. Now they want to know if the house price matches the house value. Since our lasso model performed the best we ll use it as a meta model. This can prevent us from proceeding with the analysis. Thank you data_description. This makes sense because we are trying to predict it 2. For example buying a house at a good location but small square footage. What about categories that do Let s take a look at Kitchen Quality. Is it because of inflation or stock market crashes Let s leave the years alone. It seems like houses with more than 11 rooms come with a 100k off coupon. Let s remove those outliers. Thus we need to feature transformation with this and many other features. Load Data and Packages2. For example take a feature of Downtown. Thus it could be implied that downtown establishments cost less to live in. We ll add in XGBoost and LightGBM later. Imputing Missing Values PoolQC data description says NA means No Pool MiscFeature data description says NA means no misc feature Alley data description says NA means no alley access Fence data description says NA means no fence FireplaceQu data description says NA means no fireplace LotFrontage Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood we can fill in missing values by the median LotFrontage of the neighborhood. Who could have thought These features are in a linear relationship with each other. Feature Transformation EngineeringLet s take a look at some features that may be misinterpreted to represent something it s not. GrLivArea Above grade ground living area square feet3. However looking at the skewness score we can see that the sale prices deviate from the normal distribution. With this study they can understand which features ex. We can fill 0 for the area and None for the type. MSZoning The general zoning classification RL is by far the most common value. Analyzing the Test Variable Sale Price 3. This client wants to take advantage of the features that influence a house price the most. OverallQual Rates the overall material and finish of the house 1 Very Poor 10 Very Excellent 2. They typically want to buy a house at a low price and invest on the features that will give the highest return. GarageType GarageFinish GarageQual and GarageCond Replacing missing data with None. Sale Price Of course. Utilities For this categorical feature all records are AllPub except for one NoSeWa and 2 NA. Let s zoom into the top 10 features most related to Sale Price. GarageYrBlt GarageArea and GarageCars Replacing missing data with 0. With 81 features how could we possibly tell which feature is most related to house prices Good thing we have a correlation matrix. Modeling and PredictionsFor our models we are going to use lasso elastic net kernel ridge gradient boosting XGBoost and LightGBM regression. Exterior1st and Exterior2nd Both Exterior 1 2 have only one missing value. Check data size after dropping the Id variable Getting Description Plot Histogram Get the fitted parameters used by the function Checking Categorical Data Checking Numerical Data Correlation Matrix Heatmap Top 10 Heatmap number of variables for heatmap Overall Quality vs Sale Price Living Area vs Sale Price Removing outliers manually Two points in the bottom right Living Area vs Sale Price Garage Area vs Sale Price Removing outliers manually More than 4 cars less than 300k Garage Area vs Sale Price Garage Area vs Sale Price Removing outliers manually More than 1000 sqft less than 300k Garage Area vs Sale Price Basement Area vs Sale Price First Floor Area vs Sale Price Total Rooms vs Sale Price Total Rooms vs Sale Price Combining Datasets Find Missing Ratio of Dataset Percent missing data by feature Check if there are any missing values left MSSubClass The building class Changing OverallCond into a categorical variable Year and month sold are transformed into categorical features. Number of bathrooms location etc. A house value is simply more than location and square footage. 20 1 STORY 1946 NEWER ALL STYLES 30 1 STORY 1945 OLDER 40 1 STORY W FINISHED ATTIC ALL AGES 45 1 1 2 STORY UNFINISHED ALL AGES 50 1 1 2 STORY FINISHED ALL AGES 60 2 STORY 1946 NEWER 70 2 STORY 1945 OLDER 75 2 1 2 STORY ALL AGES 80 SPLIT OR MULTI LEVEL 85 SPLIT FOYER 90 DUPLEX ALL STYLES AND AGES 120 1 STORY PUD Planned Unit Development 1946 NEWER 150 1 1 2 STORY PUD ALL AGES 160 2 STORY PUD 1946 NEWER 180 PUD MULTILEVEL INCL SPLIT LEV FOYER 190 2 FAMILY CONVERSION ALL STYLES AND AGESSo the average is a 57 type. If all matches they can ensure that they are getting a fair price. So we can fill in missing values with RL. influence the final price of the house. The test set has 1459 rows and 80 features. It looks like an outlier but I ll let it slide. Understanding the Client and their ProblemA benefit to this study is that we can have two clients at the same time Think of being a divorce lawyer for both interested parties However in this case we can have both clients with no conflict of interest Client Housebuyer This client wants to find their next dream home with a reasonable price tag. Predicting House Prices with Machine Learning Eric Kim B. They have their locations of interest ready. The response is either Near Far Yes and No. From looking at the head of both sets we can see that the only difference in features is Sale Price. Use at your own preference. YearBuilt Original construction dateLet s take a look at how each relates to Sale Price and do some pre cleaning on each feature if necessary. Here we stack the models to average their scores. Back then living in downtown usually meant that you couldn t afford to live in uptown. We are going to break everything into logical steps that allow us to ensure the cleanest most realistic data for our model to make accurate predictions from. 02 point increase in the Pearson R Score. I had to take a double take at a point since I consider myself a house browsing enthusiast With an average house price of 180921 it seems like I should relocated to Iowa Looks like a normal distribution Not quite Looking at the kurtosis score we can see that there is a very nice peak. txt comes to the rescue again Kitchen Quality Ex Excellent Gd Good TA Typical Average Fa Fair Po PoorIs a score of Gd better than TA but worse than Ex I think so let s encode these labels to give meaning to their specific orders. Although it seems like house prices decrease with age we can t be entirely sure. Analyzing the Test Variable Sale Price Let s check out the most interesting feature in this study Sale Price. That looks much better. In our previous example we could tell that our categories don t follow a particular order. Let s engineer one feature to combine square footage this may be useful later on. Impute Missing Data and Clean Data5. We need to take care of this What we will do is remove these outliers manually. Client Houseseller Think of the average house flipper. What People pay more for better quality Nothing new here. Again with the bottom two data points. Multivariable Analysis4. Electrical It has one NA value. It makes sense that people would pay for the more living area. Process columns and apply LabelEncoder to categorical features Check shape Adding Total Square Feet feature We use the numpy fuction log1p which applies log 1 x to all elements of the column Check the new distribution Get the fitted parameters used by the function Check the skew of all numerical features Cross validation with k folds we define clones of the original models to fit the data in Train cloned base models Now we do the predictions for cloned models and average them We again fit the data on clones of the original models Train cloned base models then create out of fold predictions that are needed to train the cloned meta model Now train the cloned meta model using the out of fold predictions Example. It isn t necessarily linear but it follows some kind of pattern. Loading Data and PackagesSo the training set has 1460 rows and 81 features. ", "id": "erick5/predicting-house-prices-with-machine-learning", "size": "11230", "language": "python", "html_url": "https://www.kaggle.com/code/erick5/predicting-house-prices-with-machine-learning", "git_url": "https://www.kaggle.com/code/erick5/predicting-house-prices-with-machine-learning", "script": "boxcox1p RobustScaler Counter lightgbm pearsonr Lasso train_test_split LinearRegression rmsle_cv predict ElasticNetCV sklearn.kernel_ridge rmsle Ridge sklearn.ensemble stats xgboost RandomForestRegressor numpy cross_val_score seaborn make_pipeline skew Normalizer BayesianRidge norm LassoLarsCV AveragingModels(BaseEstimator scipy sklearn.linear_model learning_curve clone KFold scipy.stats.stats matplotlib.pyplot TransformerMixin) ExtraTreesRegressor sklearn.model_selection pandas sklearn.base AdaBoostRegressor scipy.special BaseEstimator StackingAveragedModels(BaseEstimator LassoLarsIC sklearn.pipeline ElasticNet fit mean_squared_error scipy.stats RegressorMixin LassoCV TransformerMixin __init__ LabelEncoder GridSearchCV sklearn.metrics GradientBoostingRegressor collections StandardScaler KernelRidge sklearn.preprocessing ", "entities": "(('we', 'missing value'), 'set') (('Note removal', 'totally modeling'), 'be') (('make', 'plot'), 'be') (('houses', 'coupon'), 'seem') (('2 foot 000 square place', '1 foot 2 times 000 square place'), 'be') (('Important data', 'Ames Iowa'), 'Note') (('we', 'XGBoost'), 'go') (('SPLIT 190 FAMILY MULTILEVEL INCL LEV 2 average', '70 1 2 1945 75 2 2 STORY'), 'story') (('which', 'ex'), 'understand') (('necessarily it', 'pattern'), 'linear') (('house value', 'simply location'), 'be') (('Exterior', '1 2 only one missing value'), 'Exterior1st') (('People', 'Nothing'), 'pay') (('it', 'something'), 'take') (('s', 'cleaning'), 'let') (('usually you', 'uptown'), 'mean') (('client', 'price reasonable tag'), 'understand') (('GarageType GarageFinish GarageQual', 'None'), 'replace') (('it', 'prediction process'), 'model') (('that', 'data'), 'let') (('that', 'house price'), 'want') (('MasVnrArea', 'houses'), 'mean') (('only difference', 'features'), 'from') (('This', 'analysis'), 'prevent') (('OverallQual', 'house'), 'rate') (('we', 'outliers'), 'be') (('this', 'square footage'), 'let') (('test set', '1459 rows'), 'have') (('client', 'large return'), 'invest') (('Here we', 'scores'), 'stack') (('numerical when it', '90 3 times better 30 type'), 'be') (('s', 'Sale most related Price'), 'let') (('GarageYrBlt GarageArea', '0'), 'replace') (('stock market s', 'years'), 'be') (('we', 'meta model'), 'perform') (('We', 'type'), 'fill') (('most correlated feature', 'Sale Price'), 'be') (('doesn', 'much sense'), 'result') (('that', 'highest return'), 'want') (('we', 'correlation matrix'), 'feature') (('house', 'predictive modelling'), 'win') (('response', 'Far'), 'be') (('data', 'this'), 'go') (('Total rooms', 'bathrooms'), 'totrmsabvgrd') (('features', 'other'), 'think') (('we', 'kurtosis quite score'), 'have') (('people', 'more living area'), 'make') (('We', 'house prices'), 'go') (('us', 'accurate predictions'), 'go') (('so s', 'specific orders'), 'come') (('that', 'predictions fold Example'), 'column') (('locations', 'interest'), 'have') (('missing data', 'sample size'), 'question') (('downtown establishments', 'less'), 'imply') (('So we', 'RL'), 'fill') (('categories', 'don particular order'), 'tell') (('pre', 'feature'), 'dateLet') (('they', 'fair price'), 'ensure') (('location', 'Sale extremely Price'), 'correlate') (('we', '2'), 'make') (('which', 'WD MSSubClass building most likely class'), 'substitute') (('sale prices', 'normal distribution'), 'see') (('house price', 'house value'), 'want') (('it', 'outlier'), 'look') (('s', 'study Sale Price'), 'let') (('IntroductionThis 2017 notebook', 'house buyers'), 'August') (('Here we', 'GBoost KRR'), 'average') (('we', 'neighborhood'), 'say') (('We', 'None'), 'replace') (('hipsters So we', 'other'), 'thank') (('Thus we', 'this'), 'need') (('records', 'one NoSeWa'), 'be') (('more models', 'more when predictions'), 'fix') (('Multivariable AnalysisLet', 'housing data'), 'check') (('zoning classification general RL', 'most common value'), 'mszone') (('s', 'Kitchen Quality'), 'about') (('missing values', 'categorical features'), 'check') (('we', 'age'), 'be') (('data missing process', 'inconvenient truth'), 'need') (('training set', '1460 rows'), 'Loading') (('which', 'KitchenQual'), 'value') (('that', 'value'), 'like') "}