{"name": "wheat detection eda augmentation fasterrcnn ", "full_name": " h1 Wheat Spike Detection h1 Wheat Spike Detection h1 h1 Basic Data Exploration h1 Basic Data Exploration h1 h2 How are the images distributed by source h2 How about em wheat heads h1 Looking into the box h1 Image Augmentation h1 Image Augmentation h1 h1 Train Model h1 Train Model h1 ", "stargazers_count": 0, "forks_count": 0, "description": "Next we split the data into a train_set 2 708 images and a validation set 665 images. For wheat breeding purposes assessing the production of wheat spikes as the grain bearing organ is a useful proxy measure of grain production. We have already seen that the albumentations library is a good resource. We first extract these regions and for each region CNN is used to extract specific features. I can keep playing with this all day But it is time to move on Train Model We will train a FasterRCNN model for this analysis. This is could be one of those wheat heads that is still forming. jpg Machine learning has a lot of applications in various industries. The images with count between 30 and 60 look good. com albumentations team albumentations and is a really good resource for beginners. In this section we will look at the bounding boxes. Deep neural networks need a lot of data to be effective. We have some exteremly large bounding boxes with area greater than 100 000. And Some of them are on the ground and a few are not clearly visbible. The first step is to write a function to prepare the dataset. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. Wheat Spike Detection https cdn. There is a fantastic library called Albumentations that helps in creating augmenations quickly and effectively within a few lines of code. The area becomes important for IOU detection. It is the process of creating more images from the existing training data by applying transformations. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. And has also enabled in detecting diseases in the crops at a very initial stage. It will be helpful if we take care of it now since we have to analyze this later. Finally these maps are classified and the bounding boxes are predicted. Now we setup the model. com 2015 01 wheat starch image 825x338. These include but not limited to flips adding blur increase sharpness and more. Faster R CNN fixes the problem of selective search by replacing it with Region Proposal Network RPN. That is where image augmentation comes into play. For the images with less than 5 wheat heads we see these are very small heads. R CNN extracts a bunch of regions from the given image using selective search and then checks if any of these boxes contains an object. iscrowd instacnces with iscrowd True will be ignored i don t know what this means and have set it to False In addition we can add masks if available and also specify transformations. The maximum area is approximately 500 000. Unfortunately R CNN becomes rather slow due to these multiple steps involved in the process. We will know better once we look at the images. Finally these features are then used to detect objects. read_csv Input data files are available in the read only. Also an other thing to note is there is a clear demarcation in the brightness of the pictures. As a first step we will create the individual bbox columns and also create the bounding box area column. image id unique identifier 4. area area of the bounding box 5. The first is when we want to start from a pre trained model and just finetune the last layer. labels for each bounding box background is always 0. The IQR is between 28 and 59 wheat heads per image with a median value of 43. Fast R CNN on the other hand passes the entire image to ConvNet which generates regions of interest instead of passing the extracted regions from the image. As stated earlier these will have an impact on the IOU and may have to be dealt with. In this case we will use the pre trained model and finetune the last layer since our dataset is not that large. Nevertheless let s a take a look at some of the images with both types of bounding boxes. Since this is my first Kaggle competition and I have not really done object detection analysis I am going to use the Albumentation library to play around a little bit to understand the different features available. The function below will do all of the above. There are some really large boxes that don t make a lot sense Image Augmentation We have only about 3 300 images in the training data. The data prep step in PyTorch requires three main parameters an image with size h and w. Fast R CNN is however not fast enough when applied on a large dataset as it also uses selective search for extracting the regions. Also instead of using three different models as we saw in R CNN it uses a single model which extracts features from the regions classifies them into different classes and returns the bounding boxes. The other is when we want to replace the backbone of the model with a different one for faster predictions for example. These steps are pretty self explanatory. The recent development in technology has also enabled ML to step into the realm of agriculture. The minimum size required is a 800 x 800 image. I am very new to DL and object detection. Wow those look cluttered but they are a good source of information. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session import required libraries plotly libraries initialize source paths read data table expand the bbox column into seprate columns add xmax ymax and area columns for bounding box count distinct images by source Change the bar mode create list of images per the regions identified define a function to display the images define functions for augmentation and display i already have the area in my dataframe there is only one class so all will be 1 suppose all instances are not crowd define transformation functions load an instance segmentation model pre trained pre trained on COCO get number of input features for the classifier replace the pre trained head with a new one split the dataset in train and test set train on the GPU or on the CPU if a GPU is not available define the number of classes one for wheat and one for background get the model using our helper function move model to the right device construct an optimizer for epoch in range num_epochs train for one epoch printing every 10 iterations update the learning rate evaluate on the test dataset evaluate model valid_data_loader device device. Below steps sumarize the steps Take an input image and pass it to the ConvNet which returns feature maps for the image Apply Region Proposal Network RPN on these feature maps and get object proposals Apply ROI pooling layer to bring down all the proposals to the same size Finally pass these proposals to a fully connected layer in order to classify any predict the bounding boxes for the imageThe Faster R CNN model will be implemented with PyTorch. Specifically the area distribution and how many of these narrow boxes we have in the dataset. Some of them are dark and some of them are light. Next we define the dataloaders. The smallest are of the bounding box is 2. We first extract feature maps from the input image using ConvNet and then pass those maps through a RPN which returns object proposals. Image analysis has significantly enhanced the potential for achieving high throughput analysis of crop fields. How are the images distributed by source How about em wheat heads The ethz_1 source has a significant number of wheat heads per image approximately 69 per image. Now let s look at the overall distribution for the wheat heads by image. The spikes stand out This is really cool. All these steps are done simultaneously thus making it execute faster as compared to R CNN. The images seem sharper but we need to look at more of these and also analyze those parameters. We will have to use some augmentation techniques for these. Some of these are very helpful to increase the accuracy of the models. I am going to try my best to explain the proces in the most layman terms. We also have heads with the shoe background. So we will start only with a flip for the train. Note that the validation set should not be augmented There are two common situations where one might want to modify one of the available models in torchvision modelzoo. The github link is here https github. It will help us to know how many images are there what are the sources for the data image sizes bounding box information etc. There are some images with a counts greater than 100 and as less as 1. Looking into the box. The library was created by Kaggle Grandmasters and has helped win Kaggle competitions. Maybe these are very small and will be highly concentrated. We will look at images in these 3 ranges. a target dictionary that needs the following mandatory fields 1. adding weather aspects to the data. The bounding box area distribution is extremely skewed. Basic Data Exploration First we will look at the data source to get an intial understanding of what is there. Thus being able to detect and characterize spikes from images of wheat fields is an essential component in a wheat breeding pipeline for the selection of high yielding varieties. coordinates of the bounding boxes 2. ", "id": "ajaysamp/wheat-detection-eda-augmentation-fasterrcnn", "size": "7875", "language": "python", "html_url": "https://www.kaggle.com/code/ajaysamp/wheat-detection-eda-augmentation-fasterrcnn", "git_url": "https://www.kaggle.com/code/ajaysamp/wheat-detection-eda-augmentation-fasterrcnn", "script": "albumentations.pytorch.transforms albumentations DataLoader init_notebook_mode torchvision.models.detection.faster_rcnn visualize_bbox plotly.express get_model_instance_segmentation FastRCNNPredictor AnchorGenerator SequentialSampler numpy Image ToTensorV2 aug_plots get_aug torchvision.models.detection.rpn plotly.io matplotlib.patches get_train_transform collate_fn train_one_epoch matplotlib.pyplot get_valid_transform PIL plotly.graph_objs plotly.offline pandas torch.utils.data.sampler iplot get_bbox torchvision.models.detection torch.utils.data __len__ Dataset WheatDataset(Dataset) FasterRCNN __init__ plot_image engine __getitem__ visualize ", "entities": "(('recent development', 'agriculture'), 'enable') (('grain', 'grain useful proxy production'), 'be') (('some', 'them'), 'be') (('simultaneously thus it', 'R faster CNN'), 'do') (('Thus being', 'high yielding varieties'), 'be') (('Finally features', 'then objects'), 'use') (('which', 'image'), 'CNN') (('I', 'layman most terms'), 'go') (('then any', 'object'), 'extract') (('read_csv Input data files', 'read'), 'be') (('when we', 'just last layer'), 'be') (('Nevertheless s', 'boxes'), 'let') (('we', 'also parameters'), 'seem') (('We', 'area'), 'have') (('it', 'regions'), 'be') (('It', 'kaggle python Docker image https github'), 'come') (('So we', 'train'), 'start') (('few', 'ground'), 'be') (('we', 'bounding boxes'), 'look') (('IQR', '43'), 'be') (('we', 'if also transformations'), 'ignore') (('images', 'between 30'), 'look') (('earlier these', 'IOU'), 'have') (('library', 'Kaggle competitions'), 'create') (('that', 'wheat heads'), 'be') (('R Unfortunately CNN', 'process'), 'become') (('We', 'analysis'), 'keep') (('Now s', 'image'), 'let') (('We', 'shoe background'), 'have') (('we', 'dataset'), 'distribution') (('two common where one', 'torchvision modelzoo'), 'note') (('dataset', 'last layer'), 'use') (('data prep step', 'size h'), 'require') (('R Faster CNN', 'Region Proposal Network RPN'), 'fix') (('now we', 'this'), 'be') (('fantastic that', 'code'), 'be') (('jpg Machine learning', 'various industries'), 'have') (('which', 'object proposals'), 'extract') (('ethz_1 source', 'approximately image'), 'be') (('10 iterations', 'test dataset evaluate device model device'), 'list') (('Image analysis', 'crop fields'), 'enhance') (('what', 'intial understanding'), 'look') (('CNN', 'specific features'), 'extract') (('better we', 'images'), 'know') (('We', 'training data'), 'be') (('I', 'detection'), 'be') (('which', 'bounding boxes'), 'CNN') (('when we', 'example'), 'be') (('area', 'IOU detection'), 'become') (('there sources', 'data image box information'), 'help') (('It', 'transformations'), 'be') (('different features', 'little bit'), 'be') (('We', 'these'), 'have') (('target that', 'following mandatory fields'), 'dictionary') (('they', 'good information'), 'look') (('Next we', '665 images'), 'split') (('These', 'blur increase sharpness'), 'include') (('We', '3 ranges'), 'look') (('Also other thing', 'pictures'), 'be') (('we', 'bounding box area also column'), 'create') (('labels', 'bounding box background'), 'be') (('are', 'bounding box'), 'be') (('which', 'PyTorch'), 'take') (('function', 'above'), 'do') (('image where augmentation', 'play'), 'be') (('lot', 'data'), 'need') (('these', 'wheat less than 5 heads'), 'see') (('Some', 'models'), 'be') (('first step', 'dataset'), 'be') "}