{"name": "ii shopee model training with pytorch x rapids ", "full_name": " h1 Model Training Submission h1 1 Introduction h3 Libraries W B h1 2 Load the Data h1 3 Competition Metric h1 4 PyTorch Dataset h3 The Bert Tokenizer data from Abhishek Thakur h1 5 Grouping using Image Embeddings h2 I Retrieving the embeddings h2 II Creating the predictions h3 Bonus 3D Plotting on Image Embeddings Clusters h1 6 Grouping using Text Embeddings h2 I Retrieving the embeddings h2 II Creating the predictions h3 Bonus 3D Plotting on Text Embeddings Clusters h1 7 Final predictions h2 Submission h1 Specs on how I trained h3 on my local machine ", "stargazers_count": 0, "forks_count": 0, "description": "What we are doing is appending to EACH batch of images 16 1000 the ids extracted from BERT 16 16 and the masks 16 16 16 1032 II. Inspiration HUGE thanks to Chris Deotte for creating a trendsetter notebook with a baseline so we can all get started and to zzy990106 for his PyTorch version on Chris s work. The Embeddings are actually the abstract representation of the images input an image of 3 256 256 3 channels of size 256x256 output an array of 1000 items which is the abstract representation of the input structure see image below I. csv data For commiting we ll read in train. 17 Libraries CPU Libaries GPU Pytorch Deep Learning Environment check Secrets Color scheme Device Base paths When running on the CuDNN backend two further options must be set Set a fixed value for the hash seed Set COMPUTE_CV value Switch to False if test. html The output is as follows input_ids indices corresponding to each token in the sentence attention_mask indicates to the model which tokens should be attended to and which should not documentation on attention_mask here https huggingface. html attention mask Now we can create the dataset and the dataloader. 67 with a submission score in Leaderboard of 0. csv so we can plot a CV score as well When this notebook is commited the data variable will have 34 000 rows. Note The cell below takes 6 mins to run. concat principalDf y axis 1 All images that have the same phash are identical so we ll add these too Concatenate all predictions Return combined unique preds Plot F1 Score on product Make a custom plot to save into W B Prepare data Create Table. Hence we ll create clusters of a maximum size of 50. com cdeotte part 2 rapids tfidfvectorizer cv 0 700 So without doing anything we have a CV score of 0. Creating the predictions Bonus 3D Plotting on Text Embeddings Clusters We ll use PCA to downsize the data from 1000 features to only 3. explained_variance_ratio_. ai andrada shopee kaggle workspace user andrada. Hence I have saved the image_embeddings numpy array here https www. Computes the score by following the formula Create artificial prediction column Get F1 score for each row Instantiate one of the tokenizer classes of the library from BERT Image Augmentation Read in image and text data Transform image transpose channels color height width Tokenize the text using BERT Return dataset info Compute dataloader for test data Extract Efficientnet and put model on GPU model_resnet resnet50 pretrained False. The goal is to group similar products together although we have a target variable named label_group in the train dataset there can be multiple other types of groups in the test dataset completely unseen during training. co transformers preprocessing. However when we ll commit it the data will access the 70 000 hidden rows in the test. cuda as well Add information from ids and mask as well Concatenate all embeddings Save it to a binary file in NumPy. pth Extract embeddings of the image the EffnetB0 representation We aren t training only extracting the representation Don t forget to append the image to. Return the necessary information to feed into the model afterwards The Bert Tokenizer data from Abhishek Thakur https www. DataFrame data principalComponents columns pc_1 pc_2 pc_3 finalDf cudf. In this part we ll create a TfIdf Vectorizer to extract these embeddings. com abhishek bert base uncased code datasetId 431504 sortBy voteCount Pretrained tokenizer that splits sentences into tokens source from transformers library click here for more info https huggingface. Bonus 3D Plotting on Image Embeddings Clusters We ll use PCA to downsize the data from 1000 features to only 3. loc 2000 axis 0 data_gpu cudf. Retrieving the embeddings Note Because we do not have Internet access for this notebook we need to import the EffNet model from a dataset. Libraries W B You can find my W B Dashboard on this competition here https wandb. com andradaolteanu shopee preprocessed data. This means that the amount of observations pushed through the pipeline will double. fit_transform X y data label_group pca PCA_gpu n_components 3 principalComponents pca. Again the methodology is highly inspired from PART 2 RAPIDS TfidfVectorizer CV 0. To submit it you ll have to set the Internet Off and to comment the lines of code that save information into the W B Project. You can find more on PyToch EfficientNet here https github. Competition MetricLet s now understand the competition metric. Note This notebooks uses internet to connect to the W B Dashboard. Introduction Goal Building a model that can identify which images contain the same product s. Grouping using Text EmbeddingsAs we also have the title of the image available it would be a shame not to use this data for predicting as well. com lukemelas EfficientNet PyTorch. fit_transform X principalDf cudf. This notebook has the purpose of going deeper with the explanations regarding the code and process and an attempt of improving the baseline score as we go along. concat data data data. cuda model_resnet. Hence we can t use the label_group as our target y feature. Grouping using Image EmbeddingsNow we can safely extract the embeddings from our images using EffNet. To avoid any memory errors you would want to also experiment by pushing 70 000 rows as well to make sure your code isn t crushing somewhere along the way. load_state_dict torch. This is how the distribution shows in the W B dashboard Submission Note Don t forget to disable the Internet access before submitting. sum Plot Extract the Tf Idf Matrix TODO Extract more features add preprocessing from notebook I Save image_embeddings to W B TODO to be developed https www. input pretrained pytorch models resnet50 19c8e357. concat data_gpu data_gpu data_gpu. To consider This competition is a little different as it doesn t use Supervised ML Techniques but Unsupervised ML Techniques. input shopee preprocessed data image_embeddings. csv has more than 3 values CPU data Read in data Set a filepath column Map on for each product all posting_id that are labeled as the same GPU data CPU data No Target Here GPU data data pd. Remember if COMPUTE_CV True dataset_data variable will contain train. For submission we ll read in test. csv data COMPUTE_CV False dataset_data variable will contain test. Model Training Submission 1. Creating the predictionsThe competition says that group sizes are capped at 50 so there is no benefit to predict more than 50 matches. npy Save image_embeddings to W B Clean memory Create the model instance Train the model Creating the splits to prevent memory errors Making the prediction Clean environment Add predictions to dataframe Create dataframe Separating out the features Standardizing the features Separating out the target PCA pca. com kozodoi efficientnet pytorch. Retrieving the embeddings A TfIdf Process looks like the example below II. Read in the image and title 3. Receive the metadata2. values X StandardScaler_gpu. I usually like to have this down as it is a very important part of the prediction process. Perform image augmentation and tokenization4. Final predictionsNow that we have predictions linked to both image and title embeddings we can combine them and create the final predictions that we ll also submit to the leaderboard. DataFrame text_embeddings X text_embeddings_df. com c shopee product matching discussion 230486 Creating the splits to prevent memory errors Making the prediction Cosine similarity distance Clean environment Add predictions to dataframe text_embeddings_df cudf. loc 2000 axis 0 Let s look at it Save data to W B Artifacts Find the common values in target and prediction arrays. save image_embeddings all_image_embeddings Read in image_embeddings all_image_embeddings np. Load the DataLet s read the data by always taking into account the state of the notebook whether is in submission or commiting process. Nikita Kozodoi has kindly already created this for us here https www. PyTorch DatasetWe ll create a Dataset class called ShopeeDataset that will 1. co transformers glossary. Specs on how I trained on my local machine Z8 G4 Workstation 2 CPUs 96GB Memory NVIDIA Quadro RTX 8000 RAPIDS version 0. ", "id": "andradaolteanu/ii-shopee-model-training-with-pytorch-x-rapids", "size": "6723", "language": "python", "html_url": "https://www.kaggle.com/code/andradaolteanu/ii-shopee-model-training-with-pytorch-x-rapids", "git_url": "https://www.kaggle.com/code/andradaolteanu/ii-shopee-model-training-with-pytorch-x-rapids", "script": "torch.nn.functional cuml.experimental.preprocessing DataLoader albumentations torchvision.models F1_score resnet34 cuml.neighbors StandardScaler as StandardScaler_gpu numpy seaborn TfidfVectorizer set_seed resnet50 transformers VerticalFlip torch.nn PCA as PCA_gpu cuml.decomposition EfficientNet NearestNeighbors HorizontalFlip PCA matplotlib.pyplot mplot3d kaggle_secrets Compose find_matches_cupy Normalize pandas Resize ShopeeDataset(Dataset) AutoTokenizer efficientnet_pytorch mpl_toolkits combine_predictions torch.utils.data __len__ UserSecretsClient Dataset get_f1 __init__ sklearn.decomposition cuml.feature_extraction.text __getitem__ StandardScaler sklearn.preprocessing ", "entities": "(('which', 'attention_mask'), 'be') (('Tokenize', 'False'), 'compute') (('that', 'Dataset class'), 'create') (('Again methodology', 'RAPIDS TfidfVectorizer highly PART 2 CV'), 'inspire') (('Hence we', 'y feature'), 'use') (('group sizes', 'so more than 50 matches'), 'say') (('images', 'same product'), 'build') (('Set COMPUTE_CV value', 'False'), 'check') (('TfIdf Process', 'II'), 'retrieve') (('that', 'GPU data CPU same data'), 'have') (('csv data COMPUTE_CV False dataset_data variable', 'test'), 'contain') (('amount', 'pipeline'), 'mean') (('Return', 'W B Prepare data Create Table'), 'concat') (('similarity distance Clean environment', 'text_embeddings_df cudf'), 'discussion') (('csv we', 'train'), 'datum') (('Clean environment', 'target'), 'save') (('little it', 'ML Supervised Techniques'), 'consider') (('together we', 'completely training'), 'be') (('uncased code datasetId 431504 sortBy voteCount Pretrained that', 'info https here more huggingface'), 'click') (('it', 'prediction very important process'), 'like') (('Hence I', 'numpy https array here www'), 'save') (('16 1000 ids', 'BERT'), 'append') (('0 s', 'target'), 'loc') (('input', 'data shopee image_embeddings'), 'preprocesse') (('data', 'test'), 'access') (('COMPUTE_CV True dataset_data variable', 'train'), 'remember') (('notebooks', 'W B Dashboard'), 'note') (('available it', 'data'), 'have') (('Libraries W You', 'https here wandb'), 'b') (('com andradaolteanu', 'shopee data'), 'preprocesse') (('isn t', 'somewhere way'), 'avoid') (('we', 'baseline score'), 'have') (('how distribution', 'Internet access'), 'be') (('I', 'https developed www'), 'extract') (('we', 'dataset'), 'retrieve') (('embeddings', 'NumPy'), 'add') (('that', 'W B Project'), 'have') (('we', 'also leaderboard'), 'predictionsNow') (('We', 'only 3'), 'create') (('which', 'I.'), 'be') (('we', 'work'), 'HUGE') (('we', 'embeddings'), 'create') (('DataFrame data principalComponents', 'pc_1'), 'column') (('we', '0'), 'com') (('We', 'only 3'), '3d') (('You', 'PyToch https EfficientNet here github'), 'find') (('Nikita Kozodoi', 'https us here www'), 'create') (('we', 'EffNet'), 'extract') (('Don t', 'image'), 'embedding') (('html attention Now we', 'dataset'), 'mask') (('data variable', '34 000 rows'), 'csv') (('Hence we', '50'), 'create') "}