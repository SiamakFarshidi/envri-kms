{"name": "readability url scrape ", "full_name": " h3 Scrape URLs for CommonLit Readability Prize competition h1 kids frontiersin org h1 en wikibooks org h1 simple wikipedia org h1 en wikipedia org ", "stargazers_count": 0, "forks_count": 0, "description": "com teeyee314 readability external data eda with additional preparation for use with competition training. There contains over 600 URLs to scrape. org 571 Total 95 Missing kids. You re welcome Count Url 196 simple. org select rows that have urls grab the domain name list all reference urls by frequency in descending order remove some artifacts not present in competition data remove tables remove spans remove un ordered lists remove ordered lists remove sup tags remove artifact from using requests library on wikipedia remove another artifact. Scrape URLs for CommonLit Readability Prize competition 1. You should perform your own exploratory data analysis to discover any remaining artifacts that occured during scraping. I only scrape 570 URLs from 3 4 separate domains. I created a notebook https www. There may be some undetected artifacts in the text so use with caution. Wikipedia was the most annoying to scrape cleanly. ", "id": "teeyee314/readability-url-scrape", "size": "840", "language": "python", "html_url": "https://www.kaggle.com/code/teeyee314/readability-url-scrape", "git_url": "https://www.kaggle.com/code/teeyee314/readability-url-scrape", "script": "filter_newline seaborn numpy remove_ufeff remove_copyright clean_brackets clean_frontiersin remove_xa0 matplotlib.pyplot show_html_wiki BeautifulSoup clean_newline clean_http remove_http_url pandas show_html bs4 ", "entities": "(('I', '3 4 separate domains'), 'scrape') (('that', 'scraping'), 'perform') (('lists', 'artifact'), 'select') "}