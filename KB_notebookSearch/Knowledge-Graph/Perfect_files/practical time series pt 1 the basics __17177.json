{"name": "practical time series pt 1 the basics ", "full_name": " h1 Groundwork h1 Patterns h2 Examples h3 Passengers dataset h3 Changes in level of savings in the US h3 Annual averages of the daily sunspot areas h1 Dependence h1 Stationarity h2 A little bit of theory h3 Differencing h3 Tests for stationarity ", "stargazers_count": 0, "forks_count": 0, "description": "It is observed when there is a distinct repeated pattern observed between regular intervals due to seasonal factors annual monthly or weekly. Phenomena measured over time are everywhere so a natural question is what can we do with time series Some of the more popular applications reasons to bother are interpretation we want to be able to make sense of diverse phenomena and capture the nature of the underlying dynamics modelling understanding inherent aspects of the time series data so that we can create meaningful and accurate forecasts. For a slightly more interesting example of non stationary behavior we can examine the passengers dataset The stationarity of a series can be checked by examining the distribution of the series we split the series into 2 contiguous parts and compute the summary statistics like the mean variance and the autocorrelation. It accepts a 2D array with 2 columns as the main argument. It determines how strongly a time series is defined by a trend. Alternative Hypothesis H1 Alternative Hypothesis of the test is that the time series is stationary. a import matplotlib as mpl general settings adjust the parameters for displayed figures decomposition decomposition Non stationary example series. pdf two people who set out to bring statistical functionality in Python into the 21st century. It is a product of impressive work by Seabold and Perktold http conference. The values are in the first column and the predictor X is in the second column. The Null hypothesis is that the series in the second column does not Granger cause the series in the first. DependenceThe only way to succesful prediction is if past values of a series carry some information about the future behavior in other words if the present values are dependent on the past. As we can see the hypothesis of non stationarity is non rejected for the trend component but not for seasonal. Tests for stationarityWhile inspecting plots before after transformation can be useful to assess presence of trends or seasonalities as we did above with the passengers dataset in practice we need a more formal approach like testing a hypothesis introduction to statistical tests is beyond the scope of this notebook so if you feel like you need a refresher please consult other sources. com konradb practical time series pt 4 prophet This notebook summarizes some elementary methods for time series analysis sometimes you don t have the time hardware data to go for a Transformer and vintage methods can be your friend. Depending on the nature of the trends and seasonality a time series can be modeled as an additive or multiplicative time series. However visual inspection is not what one would could a rigorous criterion so let s define things in a formal manner. once done hammer the other all the way to reduce to an already solved caseI warned you it was bad but there is an analogy to how time series theory handles the problem of nonstationarity. I_t the irregular component residuals consists of the fluctuations in the time series that are observed after removing trend and seasonal cyclical variations. Null Hypothesis H0 Null hypothesis of the test is that the time series can be represented by a unit root that is not stationary. A fast and therefore very popular manner of examining this dependence are the autocorrelation and partial autocorrelation functions which are defined below Mean function of time series begin equation mu_t E X_t end equation Autocovariance function of a time series begin equation gamma s t Cov X_s X_t E X_s X_t E X_s E X_t end equation which leads to the following definitions of ACF PACF Autocorrelation begin equation rho u t u Cor X_ u X_ t u frac Cov X_t X_ t u Var X_t Var X_ t u end equation Partial autocorrelation begin equation phi u Cor X_t X_ t u X_ t 1 ldots X_ t u 1 end equation An intuitive way to think about it is that ACF at lag k measures a linear dependence between X_t and X_ t k while PACF captures the dependence between those values correcting for all the intermediate effects. It represents a long term progression of the series secular variation S_t the seasonal component with fixed and known period. This implies that parameters such as mean and variance also do not change over time. It is important to understand the difference between iterating diffencing operator n times and differencing once at lag n which is best demonstrated in an example. org proceedings scipy2010 pdfs seabold. A little bit of theoryGiven a stochastic process X_t and cdf F_X a process is strictly stationary iff begin equation F_X X_ t_1 ldots X_ t_n F_X X_ t_1 tau ldots X_ t_n tau end equation The definition is very powerful if we know a cdf of a distribution we can infer everything however it is not possible to verify in practice. Changes in level of savings in the USLet s check how does seasonal decomposition work with some other popular datasets Annual averages of the daily sunspot areasAs we can see changing the decomposition type to multiplicative alleviates the problem with residual behavior but only to a certain degree the pattern is still not consistent with increase in the amplitude in the middle of the sample. com konradb practical time series part 1 the basics this notebook Part 2 smoothing methods https www. We can verify that intuition by examining ACF and PACF TODO lag plots moar explanations CausalityMOAR Granger causality test is used to determine if one time series will be useful to forecast another. Next we can try differentiating to get rid of the trendAs expected differentiation removes the trend oscillations happen around a fixed level but variations amplitude is magnified. Let s start with a basic additive decomposition Trend and seasonality are behaving more or less in line with expectations but the behavior of the residuals is clearly not consistent over time average level of oscillations in the middle of the sample is very different than on either end. If the stats are quite different then the series is not likely to be stationary. StationarityThe reason why bother with the concept in stationarity can be is best summarized with a bad joke I heard many years ago as a math student say a mathematician is given a piece of wood with two nails one hammered all the way in the other half there and told to get remove them. start with the nail that s completetely in that s the more interesting case2. We continue moving in the right direction what happens if we combine the two transformations So after applying logarithm to stabilize the variance and differentiation to remove the trend we have transformed our series to one that can be plausibly treated as stationary. The core idea is that it s much easier to model dynamic behavior over time if the statistical properties do not change oscillations happen around the same level the amplitude does not change too much etc in other words the probability distribution of X_t is the same as the distribution of X_ t h such models are well understood. Let s go back to our hammer and nail approach and try some transformations to make the series stationary. Below we plot a few examples of stationary and non stationary series starting with the simplest non trivial stationary series Gaussian white noise. If the P Values are less than a significance level 0. It is based on the idea that if X causes Y then the forecast of Y based on previous values of Y AND the previous values of X should outperform the forecast of Y based on previous values of Y alone. If you are likely to use statistics in your work and you are a Pythonista familiarizing yourself with this library is a very good idea https www. If we are dealing with a process that does not adhere to those characteristics we can either try and capture them directly or transform it in such a manner that it can be considered stationary. Applying a logarithm does not remove the trend but it does seem to stabilize the amplitude periodic variations have comparable magnitude now. Instead it is generally used on exogenous not Y lag variables only. Compute the summary statistics The values are clearly very different across the two data subsets which strongly suggests non stationarity. When a log transformation has been used this is equivalent to using a multiplicative decomposition because begin equation X_t T_t S_t I_t end equation is equivalent to begin equation log X_t log T_t log S_t log I_t end equation A popular implementation for calculating the fundamental decomposition can be used via the statsmodels package Examples Passengers datasetEvery field of knowledge has the dataset that is used for teaching purposes machine learning has Iris and CIFAR differential equations Canadian lynx data and statistics has the airline passengers dataset between 1949 and 1960 first compiled by Box and Jenkins you will be hearing those two names again in the next module in 1976. The second argument maxlag says till how many lags of Y should be included in the test. So Granger causality test should not be used to test if a lag of Y causes Y. A trend stationary process is not strictly stationary but can easily be transformed into a stationary process by removing the underlying trend which is solely a function of time the same holds true for a stationary process with an added cyclical component. com konradb practical time series pt 3 arma and friends Part 4 Prophet https www. More formally time series data is a sequence of data points or observations recorded at different time intervals those intervals are frequently but not always regular hourly daily weekly monthly quarterly etc begin equation X_t quad t 1 ldots T end equation A strict formulation would be that a time series discrete realization of a continuous stochastic process generating the data and the underlying reason why we can infer from the former about the latter is the Kolmogorov extension theorem. high quantiles of a distribution is only possible with simulation because there is not enough historical dataSome useful references include Brockwell and Davies Time Series Theory and Methods Shumway and Stoffer Time Series Analysis and Its Applications Durbin and Koopman Time Series Analysis by State Space Methods Just about anything written by Rob Hyndman https robjhyndman. This is a first part of a series of notebooks about practical time series methods Part 1 the basics https www. Interpretation of p value above alpha Accepts the Null Hypothesis H0 the data has a unit root and is non stationary. com konradb practical time series part 2 vintage methods Part 3 ARMA https www. Algorithms are likely to yield better predictions if we apply them to stationary processes because we do not need to worry about e. Since stationarity is an assumption underlying many statistical procedures used in time series analysis non stationary data are often transformed to become stationary. We may have different combinations of trends and seasonality. We will use this dataset to demonstrate in practice what kind of information can be obtained using seasonal decomposition. C_t optional cyclical component is a repetitive pattern which does not occur at fixed intervals usually observed in an economic context like business cycles. Obvious examples include daily power consumption patterns or annual sales of seasonal goods. below alpha Rejects the Null Hypothesis H0 the data is stationary. The reason for that is that ADF test check for a very specific form of non stationarity namely variation in the presence of a linear trend existence of a single unit root while the seasonal component is clearly not stationary see graph above it is a qualitatively different kind of behavior. There is more content where that came from based on a course I used to teach so further installments will involve ARIMA state space methods and eventually sequence models because DeepLearnEverything Groundwork section one Patterns section two Dependence section three Stationarity section four We start by importing the necessary libraries most of them are familiar to anybody working with the data science with the exception of statsmodels. To define things more formally a stationary time series is one whose unconditional joint probability distribution does not change when shifted in time. pdf PatternsThe first we can do to identify patterns in a time series is separate it into components with easily understandable characteristics begin equation X_t T_t S_t C_t I_t quad end equation where T_t the trend shows a general direction of the time series data over a long period of time. For that reason a less strict variant has been introduced weak a. htmlIn this module we are merely scratching the surface of statsmodel functionality with seasonal decomposition as our primary tool. forecasting prediction we want to know something about the future filtering smoothing we want to get a better understanding of the process based on partially fully observed sample simulation in certain applications calculating e. This demonstrates that while seasonal decomposition is a fast tool it has severe limitations when dealing with more sophisticated data generating processes. Each observation in the series can be expressed as either a sum or a product of the components. concept drift between our training and test sets. com Ross Ihaka Time Series Analysis https www. How does that translate into ADF results The null is still not rejected but p value has dropped which indicates the transformations are the right way to go. How does he solve it 1. second order stationarity a process X_t is weakly stationary if it satisfies the following conditions mu_t mu_ t tau gamma s t gamma s t Var X_t infty Implication constant mean covariance only depends on distance in time between variables autocorrelation begin equation rho u frac gamma u gamma 0 end equation So how can we turn a non stationary series into a stationary one Popular transformations include but are not limited to differencing the series taking the log of the series power transforms Differencing Lag operator of order d begin equation nabla_d X_t X_t X_ t d end equation Differencing can help stabilize the mean of a time series by removing changes in the level of a time series and so eliminating trend and seasonality. Differencing at lag 1 is best thought of as discreet counterpart to differentiation first derivative of a linear function is flat first derivative of a quadratic function is linear etc so if we want to get rid of a trend behaving like polynomial of degree n we need to apply the differencing operator n times. It is worth pointing out that an alternative to using a multiplicative decomposition is to first transform the data until the variation in the series appears to be stable over time then use an additive decomposition. The most popular tests dealing with stationarity are Augmented Dickey Fuller ADF Kwiatkowski Phillips Schmidt Shin KPSS Philips Perron PP ADF test is a unit root test. plot skip the start of the series adfuller does not handle missing values which appear for values within the first full period. The proper mathematical treatment of this theory is way beyond the scope of this notebook so a mathematically inclinded reader is advised to look up those terms and then follow the references. While there are many possible reasons one quick explanation is the additive vs multiplicative relationship between the series components which is something we can examine quickly Not much of a qualitative change in trend and seasonality components but the residuals looks much more stable around a constant level such phenomenon does not of course imply stationarity by itself but at least a clear signal in the opposite direction is not there anymore. 05 then we reject the null hypothesis and conclude that the said lag of X is indeed useful. ACF PACFWe can decompose the series to check the components one by one which parts are responsible for the non stationary behavior An important caveat it is useful to remember that statistical tests do not accept a hypothesis we can only fail to reject it. GroundworkTL DR Time series is any sequence you record over time and applications are everywhere. It is implemented in the statsmodel package. ", "id": "konradb/practical-time-series-pt-1-the-basics", "size": "17177", "language": "python", "html_url": "https://www.kaggle.com/code/konradb/practical-time-series-pt-1-the-basics", "git_url": "https://www.kaggle.com/code/konradb/practical-time-series-pt-1-the-basics", "script": "pandas.plotting IPython.display plot_pacf numpy Image seaborn adfuller statsmodels.tsa.ar_model random CFG matplotlib.pyplot gauss plot_acf statsmodels.api AR pandas seasonal_decompose statsmodels.tsa.seasonal statsmodels.tsa.stattools statsmodels.graphics.tsaplots matplotlib autocorrelation_plot statsmodels.formula.api ", "entities": "(('It', 'S_t seasonal fixed period'), 'represent') (('most', 'statsmodels'), 'be') (('parameters', 'also time'), 'imply') (('time series how theory', 'nonstationarity'), 'warn') (('however it', 'practice'), 'bit') (('high quantiles', 'Rob Hyndman https robjhyndman'), 'be') (('who', '21st century'), 'pdf') (('lag', 'Y.'), 'use') (('which', 'best example'), 'be') (('you', 'other sources'), 'test') (('which', 'added cyclical component'), 'be') (('future filtering we', 'e.'), 'prediction') (('X _ t PACF', 'intermediate effects'), 'be') (('how many lags', 'test'), 'say') (('fast it', 'data generating when more sophisticated processes'), 'demonstrate') (('mathematically inclinded reader', 'then references'), 'be') (('It', 'statsmodel package'), 'implement') (('predictor X', 'second column'), 'be') (('we', 'meaningful forecasts'), 'be') (('pattern', 'sample'), 'change') (('which', 'non strongly stationarity'), 'compute') (('clearly graph', 'qualitatively different behavior'), 'be') (('Obvious examples', 'annual seasonal goods'), 'include') (('that', 'one'), 'continue') (('module we', 'primary tool'), 'htmlIn') (('less strict variant', 'reason'), 'introduce') (('It', 'seasonal factors'), 'observe') (('time how strongly series', 'trend'), 'determine') (('P Values', 'significance level'), 'be') (('we', 'mean variance'), 'examine') (('Below we', 'simplest non trivial stationary series'), 'plot') (('you', '1976'), 'be') (('this', '1 basics'), 'part') (('so s', 'formal manner'), 'be') (('probability unconditional joint distribution', 'when time'), 'be') (('kind', 'seasonal decomposition'), 'use') (('said lag', 'X'), '05') (('that', 'unit root'), 'be') (('applications', 'time'), 'be') (('we', 'differencing operator'), 'be') (('hypothesis', 'trend non component'), 'reject') (('series', 'first'), 'be') (('present values', 'past'), 'be') (('it', 'manner'), 'deal') (('observation', 'components'), 'express') (('such models', 'X _ t h'), 'be') (('we', 'only it'), 'decompose') (('at least clear signal', 'opposite direction'), 'be') (('then forecast', 'Y'), 'base') (('end quad where trend', 'time'), 'pdf') (('you', 'library'), 'be') (('assumption', 'time series analysis non stationary data'), 'transform') (('that', 'trend'), 'consist') (('Instead it', 'lag generally exogenous Y variables'), 'use') (('series', 'transformations'), 'let') (('data', 'Null Hypothesis H0'), 'reject') (('time series', 'test'), 'be') (('import mpl general settings', 'figures decomposition example displayed decomposition Non stationary series'), 'matplotlib') (('amplitude periodic variations', 'comparable magnitude'), 'remove') (('variation', 'additive then decomposition'), 'be') (('This', 'notebooks'), 'be') (('variations amplitude', 'fixed level'), 'try') (('time series', 'additive'), 'depend') (('It', 'http Seabold conference'), 'be') (('It', 'main argument'), 'accept') (('one', 'there them'), 'summarize') (('average level', 'very end'), 'let') (('We', 'trends'), 'have') (('_ t end equation nabla_d d Differencing', 'so trend'), 'be') (('underlying why we', 'latter'), 'be') (('data', 'unit root'), 'accept') (('sometimes you', 'hardware Transformer methods'), 'series') (('transformations', 'ADF results'), 'translate') (('which', 'first full period'), 'skip') (('we', 'e.'), 'be') (('repetitive which', 'business cycles'), 'be') (('that', 'completetely that'), 'start') (('Augmented Dickey Fuller ADF Kwiatkowski Phillips Schmidt Shin KPSS Perron ADF Philips test', 'stationarity'), 'be') (('time one series', 'another'), 'verify') "}