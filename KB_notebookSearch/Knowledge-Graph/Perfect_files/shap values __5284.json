{"name": "shap values ", "full_name": " h1 Introduction h1 How They Work h1 Code to Calculate SHAP Values h1 Your Turn h2 SHAP values are awesome Have fun applying them alongside the other tools you ve learned to solve a full data science scenario ", "stargazers_count": 0, "forks_count": 0, "description": "For context we ll look at the raw predictions before looking at the SHAP values. com dansbecker partial plots lessons. How They WorkSHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we d make if that feature took some baseline value. We will look at SHAP values for a single row of the dataset we arbitrarily chose row 5. Have fun applying them alongside the other tools you ve learned to solve a full data science scenario https www. We won t go into that detail here since it isn t critical for using the technique. KernelExplainer works with all models though it is slower than other Explainers and it offers an approximation rather than exact Shap values. TreeExplainer my_model. But what if you want to break down how the model works for an individual prediction SHAP Values an acronym from SHapley Additive exPlanations break down a prediction to show the impact of each feature. com learn machine learning explainability discussion to chat with other learners. DeepExplainer works with Deep Learning models. The shap_values object above is a list with two arrays. Now we ll move onto the code to get SHAP values for that single prediction. Feature values causing increased predictions are in pink and their visual size shows the magnitude of the feature s effect. Though the ball possession value has a meaningful effect decreasing the prediction. An example is helpful and we ll continue the soccer football example from the permutation importance https www. png How do you interpret this We predicted 0. Code to Calculate SHAP ValuesWe calculate SHAP values using the wonderful Shap https github. The team is 70 likely to have a player win the award. But the results tell the same story. Have questions or comments Visit the course discussion forum https www. We could ask How much was a prediction driven by the fact that the team scored 3 goals But it s easier to give a concrete numeric answer if we restate this as How much was a prediction driven by the fact that the team scored 3 goals instead of some baseline number of goals. If you subtract the length of the blue bars from the length of the pink bars it equals the distance from the base value to the output. Could use multiple rows if desired package used to calculate Shap values Create object that can calculate shap values Calculate Shap values use Kernel SHAP to explain test set predictions. It s cumbersome to review raw arrays but the shap package has a nice way to visualize the results. com dansbecker permutation importance and partial dependence plots https www. Your TurnSHAP values are awesome. We typically think about predictions in terms of the prediction of a positive outcome so we ll pull out SHAP values for positive outcomes pulling out shap_values 1. Of course each team has many features. If you look carefully at the code where we created the SHAP values you ll notice we reference Trees in shap. This blog post https towardsdatascience. The biggest impact comes from Goal Scored being 2. 7 whereas the base_value is 0. The first array is the SHAP values for a negative outcome don t win the award and the second array is the list of SHAP values for the positive outcome wins the award. In the next lesson you ll see how these can be aggregated into powerful model level insights. Convert from string Yes No to binary use 1 row of data here. com kernels fork 1637226. IntroductionYou ve seen and used techniques to extract general insights from a machine learning model. This allows us to decompose a prediction in a graph like this Imgur https i. Where could you use this A model says a bank shouldn t loan someone money and the bank is legally required to explain the basis for each loan rejection A healthcare provider wants to identify what factors are driving each patient s risk of some disease so they can directly address those risk factors with targeted health interventionsYou ll use SHAP Values to explain individual predictions in this lesson. For this example we ll reuse the model you ve already seen with the Soccer data. com one feature attribution method to supposedly rule them all shapley values f3e04534983d has a longer theoretical explanation. Feature values decreasing the prediction are in blue. So if we answer this question for number of goals we could repeat the process for all other features. There is some complexity to the technique to ensure that the baseline plus the sum of individual effects adds up to the prediction which isn t as straightforward as it sounds. The results aren t identical because KernelExplainer gives an approximate result. Here is an example using KernelExplainer to get similar results. Specifically you decompose a prediction with the following equation sum SHAP values for all features pred_for_team pred_for_baseline_values That is the SHAP values of all features sum up to explain why my prediction was different from the baseline. In these tutorials we predicted whether a team would have a player win the Man of the Match award. png If you want a larger view of this graph here is a link https i. SHAP values do this in a way that guarantees a nice property. But the SHAP package has explainers for every type of model. com slundberg shap library. ", "id": "dansbecker/shap-values", "size": "5284", "language": "python", "html_url": "https://www.kaggle.com/code/dansbecker/shap-values", "git_url": "https://www.kaggle.com/code/dansbecker/shap-values", "script": "train_test_split sklearn.model_selection pandas sklearn.ensemble RandomForestClassifier numpy ", "entities": "(('health targeted interventionsYou', 'lesson'), 'say') (('IntroductionYou', 'machine learning model'), 'see') (('list', 'award'), 'be') (('Calculate Shap values', 'test set predictions'), 'use') (('you', 'Soccer already data'), 'reuse') (('feature', 'baseline value'), 'interpret') (('it', 'approximation'), 'work') (('as straightforward it', 'which'), 'be') (('why prediction', 'baseline'), 'decompose') (('visual size', 'effect'), 'be') (('Now we', 'single prediction'), 'move') (('questions', 'course discussion forum https www'), 'have') (('t KernelExplainer', 'approximate result'), 'aren') (('here it', 'isn t technique'), 'win') (('likely player', 'award'), 'be') (('we', 'arbitrarily row'), 'look') (('ball possession value', 'prediction'), 'have') (('shap_values object', 'above two arrays'), 'be') (('shap package', 'results'), 's') (('SHAP package', 'model'), 'have') (('we', 'other features'), 'repeat') (('biggest impact', 'Goal'), 'come') (('it', 'output'), 'subtract') (('team', 'instead baseline goals'), 'ask') (('how these', 'model level powerful insights'), 'see') (('you', 'graph'), 'png') (('We', '0'), 'png') (('that', 'nice property'), 'do') (('we', 'permutation importance https www'), 'be') (('we', 'shap_values'), 'think') (('player', 'Match award'), 'predict') (('we', 'shap'), 'look') (('we', 'SHAP values'), 'look') (('us', 'Imgur https i.'), 'allow') (('how model', 'feature'), 'want') (('you', 'data science scenario https full www'), 'have') (('Feature values', 'blue'), 'be') (('Here example', 'similar results'), 'be') (('DeepExplainer', 'Deep Learning models'), 'work') "}