{"name": "v auto encoder vs frauds on imbalance data wins ", "full_name": " h2 As we can see fraud cases are negligible and we have to build model to detect anamolies h2 so i will be building auto encoder first in keras then for production level code i will be building it in h2 tensorflow and finally in tensorflow serving API for it s deployment h2 will post github link it it h2 Feature distribution h3 data preprocessing it requires normalization why normalization h3 whenever we are seeing multiple features which are different ranges of distributions h3 then we should prefer normalization if same range every feature but still lot of within range distribution h3 we should give it standardization h3 Now big question why auto encoder for classification h3 how it is possible that neural network which is used for recontruction of input values can be used as a classifier for fraud transcations h3 so answer to this curiousity is very simple h3 as we know autoencoder is useful for reconstruciton of values h3 but if I train it on non fraudulent transaction then it will be able to contruct non fraudlent only h3 so if I pass fraudulent transaction with non fraudulent one then mse will be high for fraud transaction one h3 why because it s weight are made on the basis of non fraudulent transaction h3 then at last I will decide a perfect threshold for classify fraudulent vs non fraudulent transaction h1 Initial Preprocessing h1 Spliting strategy h3 20 percent in test set of 0 class then 10 in validation set of class 0 h3 as we know in training label 1 class will not go but to decide the threshold of mse so that we can h3 classify anomaly perfectly so 50 of the 1 class percent we will be in test set rest 50 in validate set h4 splitting dataset as per strategy I have dicussed h4 we will train it on non fraudulent transcation and test on both the classes h3 removing of fractional subset from main train set done h3 now starting up with making of test and validation set h2 Merging of test and validation sets h3 just re checking size of train test and validate set h3 still need some small part of training set in testing of autoencoder network for reconstruction of values h1 Autoencoder here we go h1 What type of activations h3 Generally MSE as loss function but why Mae we can use too or not h3 after 22 epochs we achieved plateaue and accuracy of 99 23 in reconstruction on test data h3 reconstruction error on x test set h3 as you can std deviation is not so much varying while reconstruction of training data h3 as far as we are good to go for testing of out main test set and validation set h3 after that we can develope our same model in Tensorflow using TF records for mainline production h1 Evaluation of mse on both classes on test set h3 now checking how much reconstruction error present in class 0 and class 1 h3 very small mse will be present we are concerned for deciding the threshold in test set h3 as we can see that from both graphs MSE for fraudulent cases is x10 times Non fraudulent cases h4 selection of threshold as you can see max is 0 02 but if you observed 3rd quartile range it is really very small in comparison to max one which indicates that in selection of threshold we should not take max into the account because mean value of mse in frauds cases is 0 012 with std of 0 013 h4 even minimum value of mse in normal transaction is range of minima of fraud cases but it s mse approx 30 is higher from normal transaction h4 so these cases which i saw are very much corner cases means and at extreme points to be get classified correctly h4 let s present our evaluation metrics over the threshold we have decided h3 now question comes up why precision is so low while recall is high h3 see we are actually testing imbalance test but in real world frauds cases will be h3 like this so what should we do h3 focus on precision why see we can t catch every fraud but what we can catch as fraud case should be fraud to save company s money and customer as well h3 so conclusion that out of 246 fraud cases 209 we were able to classify correctly h3 and normal transaction 352 cases model declaring them as fraud out of 14k cases h3 so here threshold is making final model is like that 85 percent cases model can detect but it not precise so much means model yes or no has no value only what model can do is put those cases in suspect but can t bring final conclusions h2 Stats can might change after running the kernel but it will approximate to those which were stated earlier h2 now thershold is changed to focus on precision as primary h3 our roc curve is telling that our model is doing really great in classifying both the classes h3 but one should never forget sample size of test cases are imbalanced so always precision and recall h3 before deploying the model in real world h3 now same for final evaluation set that is our validation set h3 our model is performing really well even on validaiton dataset on fraud cases better than test set h3 So guys here I expalined my strategy why I have chosen auto encoder as for classifying fraud cases but as you can see that training of this model requires lot of computation time atleast 45 minson kaggle s gpu h3 we have move towards some faster solution not to save our training time but to save company resources as well h3 sometimes even in kaggle competition time contsraint issues can be solved if training can be done much more faster ways h3 but is just not about winining kaggle competition it is about real time working model deployment should be done in Tensorflow h3 so why keras as framework and it s model file should not be sent in model deployment in production h3 Tensorflow offers advance methods for managing and devlopement Neural Nets h4 1 highly efficient data pipeline using tf records h4 2 allows you to use tf data pipeline api for feeding the model more faster h4 3 allow you to control cpu gpu parrallel scheduling so that data preprocessing and training can be done much more faster but in keras everything is explicityly done by TF which takes time for processing data and training it cpu gpu schedulling is not able to take place in keras h4 4 model file is very small compared to keras so it faster process deployable and it high scalability if deployed on aws EKS cluster performance on images response or any input data will increase to significant scale h4 so hardware best utilization we have to make it feel tired h4 utlimately apart from data scientist we need the best engineering skills to deploy our solution h4 with faster process if we can t then what is the use building such stacked and blend models h1 So let s start with Tensorflow what is first we need to do h4 data exploration is done already h2 Basic understanding h3 tensorflow session dataflow graphs placeholders variables training h3 so that you can come to know how in tf neural nets are structured first why the need of advance methods arises up h1 Basic example NN as classfication using TF placeholders h2 Advanced methods h3 1 step conversion of data to tf records binary format data h3 2 step use tf data input pipeline to connect tf records and send to base model h3 3 step advance methods to schedule cpu and gpu processing Parrallelize Data Transformation h3 4 step tf flags namescopes are really important for documenting your model ", "stargazers_count": 0, "forks_count": 0, "description": "com images q tbn 3AANd9GcQ5aL46qJsIl3AjFoOLyNbn_vdLe2a2tPns9 PikUI8EhpaLTJx usqp CAU so answer to this curiousity is very simple as we know autoencoder is useful for reconstruciton of values but if I train it on non fraudulent transaction then it will be able to contruct non fraudlent only so if I pass fraudulent transaction with non fraudulent one then mse will be high for fraud transaction one why because it s weight are made on the basis of non fraudulent transaction then at last I will decide a perfect threshold for classify fraudulent vs non fraudulent transaction Initial Preprocessing Spliting strategy 20 percent in test set of 0 class then 10 in validation set of class 0 as we know in training label 1 class will not go but to decide the threshold of mse so that we can classify anomaly perfectly so 50 of the 1 class percent we will be in test set rest 50 in validate set of course class 0 will be there too with class 1 in both sets validation and test splitting dataset as per strategy I have dicussed we will train it on non fraudulent transcation and test on both the classes removing of fractional subset from main train set done now starting up with making of test and validation set Merging of test and validation sets just re checking size of train test and validate set still need some small part of training set in testing of autoencoder network for reconstruction of values Autoencoder here we go What type of activations Linear Autoencoders with a single hidden layer with k hidden neurons and linear activations create equivalent representations to PCA with k principal components. ReLU Rectified linear units are widely used in deep learning models. com images D SocketOverload 01. Generally MSE as loss function but why Mae we can use too or not after 22 epochs we achieved plateaue and accuracy of 99. 23 in reconstruction on test data reconstruction error on x_test set as you can std deviation is not so much varying while reconstruction of training data as far as we are good to go for testing of out main test set and validation set after that we can develope our same model in Tensorflow using TF records for mainline production Evaluation of mse on both classes on test set now checking how much reconstruction error present in class 0 and class 1 very small mse will be present we are concerned for deciding the threshold in test set as we can see that from both graphs MSE for fraudulent cases is x10 times Non fraudulent cases selection of threshold as you can see max is 0. 02 but if you observed 3rd quartile range it is really very small in comparison to max one which indicates that in selection of threshold we should not take max into the account because mean value of mse in frauds cases is 0. array Get the next mini batch of training samples Get the next mini batch of training samples from the dataset Reshape the list of arrays into a nxn np. SELU Scaled exponential linear units activation function is a formidable alternative to ReLU as it preserves the advantages of linearly passing the positive inputs while it enables the flow of negative too. Binary It is often used as introduction to ANN and not in real world applications. As a result it produces stronger gradients than sigmoid and should be preferred. All operations within this context will become operation nodes in this graph Create two constants of value 5 Multiply the constants with each other Create a session to execute the dataflow graph Perform the calculation defined in the dataflow graph and get the result Create an empty graph that will be filled with operation nodes later Register this graph as default graph. As we can see fraud cases are negligible and we have to build model to detect anamolies so i will be building auto encoder first in keras then for production level code i will be building it in tensorflow and finally in tensorflow serving API for it s deployment will post github link it it Feature distribution data preprocessing it requires normalization why normalization whenever we are seeing multiple features which are different ranges of distributions then we should prefer normalization if same range every feature but still lot of within range distribution we should give it standardization Now big question why auto encoder for classification how it is possible that neural network which is used for recontruction of input values can be used as a classifier for fraud transcations https encrypted tbn0. we have move towards some faster solution not to save our training time but to save company resources as well sometimes even in kaggle competition time contsraint issues can be solved if training can be done much more faster ways but is just not about winining kaggle competition it is about real time working model deployment should be done in Tensorflow https www. com images q tbn 3AANd9GcTpdOSZuDXbWZdIaoSJRT2LvruTlATWgrEkgtKZhLaQpY6Sj60i usqp CAU So let s start with Tensorflow what is first we need to do data exploration is done already Basic understanding tensorflow session dataflow graphs placeholders variables training so that you can come to know how in tf neural nets are structured first why the need of advance methods arises up Basic example NN as classfication using TF placeholders Advanced methods 1 step conversion of data to tf records binary format data 2 step use tf. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. data input pipeline to connect tf records and send to base model 3 step advance methods to schedule cpu and gpu processing Parrallelize Data Transformation 4 step tf flags namescopes are really important for documenting your model This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. Using ggplot2 style visuals Def plot distribution Import the TensorFlow library Create an empty graph that will be filled with operation nodes later Register this graph as default graph. Tanh Hyperbolic tangent is similar to sigmoid with the difference that is symmetric to the origin and its slope is steeper. We must provide the values for the placeholders with feed_dict dictionary list that will contain our training set How many digits should this binary number have Mini batch size Create a binary number of type string Convert binary string number to type float Create the corresponding binary label class Put the feature label pair into the list shuffle the data convert the list to np. Sigmoid The most commonly used activation function for autoencoders. array Reshape the labels Create the training graph Define the placeholders for the features and the labels Create the weight matrices and the bias vectors First hidden layer Second hidden layer Outputlayer without an activation function input for the loss function Compute the probability scores after the training Define the loss function Perform a gradient descent step How many mini batches in total Iterate over the entire training set for 10 times Iterate over the number of mini batches Get the next mini batches of samples for the training set Perform the gradient descent step on that mini batch and compute the loss value Compute the probability scores for the last mini batch Iterate over the features and labels from the last mini batch as well as the predicitons made by the network and compare them to check the performance Get the class with the highest probability score Get the actual probability score. jpg so why keras as framework and it s model file should not be sent in model deployment in production Tensorflow offers advance methods for managing and devlopement Neural Nets 1 highly efficient data pipeline using tf records 2 allows you to use tf data pipeline api for feeding the model more faster 3 allow you to control cpu gpu parrallel scheduling so that data preprocessing and training can be done much more faster but in keras everything is explicityly done by TF which takes time for processing data and training it cpu gpu schedulling is not able to take place in keras 4 model file is very small compared to keras so it faster process deployable and it high scalability if deployed on aws EKS cluster performance on images response or any input data will increase to significant scale so hardware best utilization we have to make it feel tired utlimately apart from data scientist we need the best engineering skills to deploy our solution with faster process if we can t then what is the use building such stacked and blend models https encrypted tbn0. However they are not suitable for AEs because they distort the decoding process by outputting 0 for negative inputs and consequently do not lead to faithful representations of the input features. read_csv Input data files are available in the. 013 even minimum value of mse in normal transaction is range of minima of fraud cases but it s mse approx 30 is higher from normal transaction so these cases which i saw are very much corner cases means and at extreme points to be get classified correctly let s present our evaluation metrics over the threshold we have decided now question comes up why precision is so low while recall is high see we are actually testing imbalance test but in real world frauds cases will be like this so what should we do focus on precision why see we can t catch every fraud but what we can catch as fraud case should be fraud to save company s money and customer as well so conclusion that out of 246 fraud cases 209 we were able to classify correctly and normal transaction 352 cases model declaring them as fraud out of 14k cases so here threshold is making final model is like that 85 percent cases model can detect but it not precise so much means model yes or no has no value only what model can do is put those cases in suspect but can t bring final conclusions Stats can might change after running the kernel but it will approximate to those which were stated earlier now thershold is changed to focus on precision as primary our roc curve is telling that our model is doing really great in classifying both the classes but one should never forget sample size of test cases are imbalanced so always precision and recall before deploying the model in real world now same for final evaluation set that is our validation set our model is performing really well even on validaiton dataset on fraud cases better than test set So guys here I expalined my strategy why I have chosen auto encoder as for classifying fraud cases but as you can see that training of this model requires lot of computation time atleast 45 minson kaggle s gpu. All operations within this context will become operation nodes in this graph Define the placeholders that will feed python arrays into the dataflow graph Create a session to execute the dataflow graph Perform the calculation defined in the dataflow graph and get the result. For example running this by clicking run or pressing Shift Enter will list all files under the input directory Any results you write to the current directory are saved as output. ", "id": "rohandx1996/v-auto-encoder-vs-frauds-on-imbalance-data-wins", "size": "8821", "language": "python", "html_url": "https://www.kaggle.com/code/rohandx1996/v-auto-encoder-vs-frauds-on-imbalance-data-wins", "git_url": "https://www.kaggle.com/code/rohandx1996/v-auto-encoder-vs-frauds-on-imbalance-data-wins", "script": "keras.layers keras.models train_test_split keras contextmanager confusion_matrix get_next_batch precision_recall_curve numpy seaborn Adam preprocessing randint Dense regularizers tensorflow.compat.v1 keras.callbacks randint as sp_randint roc_auc_score keras.optimizers sklearn auc ReduceLROnPlateau random plotly.tools matplotlib.pyplot plot_distribution ModelCheckpoint uniform plotly.offline plotly.graph_objs pandas timer sklearn.model_selection shuffle roc_curve TensorBoard contextlib load_model EarlyStopping Model plotly.figure_factory Input scipy.stats (confusion_matrix sklearn.metrics uniform as sp_uniform ", "entities": "(('blend models https', 'tbn0'), 'offer') (('next mini batches', 'probability actual score'), 'Reshape') (('which', 'tbn0'), 'be') (('It', 'python docker image https kaggle github'), 'be') (('ReLU Rectified linear units', 'learning widely deep models'), 'use') (('mini next batch', 'nxn np'), 'get') (('slope', 'origin'), 'be') (('type', 'k principal components'), 'tbn') (('mean value', 'frauds cases'), '02') (('time working model real deployment', 'Tensorflow https www'), 'move') (('that', 'default graph'), 'use') (('read_csv Input data files', 'the'), 'be') (('we', '99'), 'MSE') (('max', 'x10 cases Non fraudulent threshold'), '23') (('Binary It', 'world real applications'), 'use') (('training', 'minson atleast 45 gpu'), 'be') (('they', 'input features'), 'be') (('it', 'sigmoid'), 'produce') (('you', 'output'), 'list') (('step Advanced methods 1 conversion', 'binary format data'), 'image') (('data', 'np'), 'provide') (('that', 'result'), 'become') (('it', 'negative'), 'be') (('that', 'default graph'), 'become') "}