{"name": "complete titanic tutorial with ml nn ensembling ", "full_name": " h1 Complete Titanic tutorial with ML NN Ensembling h3 Welcome on this Titanic tutorial It is aimed for beginners but whatever your level you could read it and if you find a way to improve it I encourage you to fork this notebook and contribute by adding a better solution h3 In this notebook we are going to predict wether a passenger of the famous boat will survive or not By doing this we will go through several topics and fundamental techniques of machine learning Here is a list of these techniques and some additional resources that you can consult to find out more h2 Table of Contents h2 Imports and useful functions h2 1 Data exploration h3 There is 77 of missing data in the cabin column it s usually way too much for this column to be exploitable but as we have a small amount of data we will still try to use it in feature engineering h3 For the age we will either interpolate missing values or we will fill it with the mean for the corresponding category in term of class age sex of passenger h3 There is only two missing values for the embarked column let s try to replace it Below is the distribution of Embarked according to Fare and sex and the two observations with missing Embarked value Let s look at there two observations and choose the best matching embarked value according to their fare value and sex h3 Both passengers are female who paid 80 dollars as fare for their tickets Moreover they have the same ticket and cabin so they probably had to board at the same place According to the distribution above the more probable embarked value for them is Cherbourg C We ll replace these two missing values later during features engineering part h3 Finally let s plot some histograms to visualise the distributions of our variables h3 With this first exploration we can see that h2 2 Features engineering h3 My advice is to group all the transformations to be done on the dataset in a single function This way you can apply the same changes to the training dataset and the test dataset easily Moreover if you want to add a modification you ll have to do it only in the function h2 3 Try several models h3 Introduction to metrics h3 To evaluate our models on the test set for this classification problem we are going to use several metrics which will be displayed into a confusion matrix to easily see the false positive and the false negative predicted by the model i e respectivey type I II errors From those two types of error some metrics can be computed the F1 score the Recall the accuracy You can find on the image below a quick summary of what is a confusion matrix how to read it and what are those metrics h3 3 1 Logistic regression h3 Logistic regression is the hello world of machine learning algorithms It is very simple to understand how it works here is a good article which cover theory of this algorithm h3 3 2 Decision tree h3 Decision tree is a quite intuitive model easy to vizualize and interpret Here we are even going to display our tree to improve our understanding on how the algorithm manage to classify our samples h3 To find out more about decision trees DT h3 3 3 SVM h3 SVMs aim at solving classification problems by finding good decision boundaries between two sets of points belonging to two different categories To understand how it works you can refer to this webpage h3 3 4 Random forest h3 Random forest is a robust practical algorithm based on decision trees It outperforms almost always the two previous algorithm we saw If you want to find out more about this model here is a good start h3 3 5 Artificial neural network h3 Neural networks are more complex and more powerful algorithm than standars machine learning it belongs to deep learning models To build a neural network we are going to use Keras Keras is a high level API for tensorflow which is a tensor manipulation framework made by google Keras allows you to build neural networks by assembling blocks which are the layers of our neural network For more details here is a great keras tutorial h3 To understand better the above implemented architecture and dive deeply that s the case to say in this exciting field I highly recommend you to read the great book Deep Learning with Python link at the beginning of the kernel written by Fran\u00e7ois Chollet the creator of the keras framework h2 4 Finding the best model using k folds cross validation h3 The precision we calculated above for those 4 different models does not mean anything In fact if we execute each cell again we could have sightly different accuracy because we trained again our models We need to verify which model has the best accuracy over several training steps We can do it using cross validation method which consists of dividing out training set in k parts folds and evaluating k times using successively each part as the test set and the 9 other parts as the training set Therefore we can compute a mean error over the 10 trainings of our model h3 Let s check which one of our previously implemented model is the best one with this method We will not only compute the mean but also the variance because a good model needs to have the lowest possible variance in addition to have a low bias h3 All models seems to have a good accuracy and nearly the same variance it seems that there is no best model Indeed there is no one model which seems truly better than the other In fact if we make submissions with all of these models we will obtain approximately the same score Moreover the variance is a little bit too high for saying that these models are reliable 0 05 variance means that the same model can score 0 75 and 0 8 which is not very convenient h3 To obtain a better score we will in the next part build our own classifier which will combine predictions from a random forest an svm classifier and a keras neural networks The diversity from these 3 very different models will increase the quality of our predictions and reduce the variance h2 5 Ensembling creating a homemade classifier h3 Ensembling is the science of combining classifiers to improve the accuracy of a models Moreover it diminushes the variance of our model making it more reliable You can start learning about ensembling here h3 We are going to make our own classifier To do that we ll create a class with two methods fit predict just as the other classifiers we used from sckitlearn and keras In the fit method we just train our 3 classifiers on training data In the predict method we make a prediction with each of the 3 classifier and combine it if two or more classifiers classified the passenger as a survivor our homemade EsembleClassifier classify it as survivor Else it ll predict that the passenger did not survived h3 Our new model seems to be quite performing You can try to train and validate it several time on train test split you ll see that the variance is not high so our model is also quite constant in its performances h3 Let s try this new model on the test dataset now h2 6 Apply our homemade model on test dataset and submit on kaggle h2 7 Results h3 We are not done yet What about the results I ve tried to make 30 submissions with this classifier here are the results h3 Although I know that we can do much better the 0 79904 still places us in the top 16 On the other hand we see that our homemade classifier has reduced the variance so it is reliable and constant in its performance something that is not tested by the leaderboard but still important for a data sceintist Moreover our solution remains simple and accessible even for beginners h3 Thank you for your reading feel free to fork this kernel and improve it and enjoy datascience D ", "stargazers_count": 0, "forks_count": 0, "description": "On the other hand we see that our homemade classifier has reduced the variance so it is reliable and constant in its performance something that is not tested by the leaderboard but still important for a data sceintist. Below is the distribution of Embarked according to Fare and sex and the two observations with missing Embarked value. Esembling Homemade classifier ensembling 6. In the predict method we make a prediction with each of the 3 classifier and combine it if two or more classifiers classified the passenger as a survivor our homemade EsembleClassifier classify it as survivor. It is very simple to understand how it works here https towardsdatascience. Finally let s plot some histograms to visualise the distributions of our variables With this first exploration we can see that Only aproximately 35 of passengers survived. Our new model seems to be quite performing You can try to train and validate it several time on train_test_split you ll see that the variance is not high so our model is also quite constant in its performances. 8 which is not very convenient. com chiragsehra42 decision trees explained easily 28f23241248 title dt. Choosing the best model choose 5. 05 variance means that the same model can score 0. com two is better than one ensembling models 611ee4fa9bd8 We are going to make our own classifier. If you want to find out more about this model here https medium. However you can look at my EDA kernel https www. Here is a list of these techniques and some additional resources that you can consult to find out more EDA Data exploration https medium. Keras is a high level API for tensorflow which is a tensor manipulation framework made by google. 3 SVM SVMs aim at solving classification problems by finding good decision boundaries between two sets of points belonging to two different categories. com 2014 11 svm understanding math part 1. png Let s check which one of our previously implemented model is the best one with this method. com metrics evaluate machine learning algorithms python Evaluating a model over several trainings k fold cross validation https towardsdatascience. Results We are not done yet What about the results I ve tried to make 30 submissions with this classifier here are the results Score Nb of occurrences 0. To understand how it works you can refer to this webpage https www. For more details here https elitedatascience. com understanding logistic regression 9b02c2aec102 is a good article which cover theory of this algorithm. 4 Random forest Random forest is a robust practical algorithm based on decision trees. You can start learning about ensembling here https towardsdatascience. 79904 still places us in the top 16. Scaling means that each columns as a 0 mean and a 1 variance Split dataset for model testing Create and train model on train data sample Predict for test data sample Compute error between predicted data and true response and display it in confusion matrix Initializing our ANN Adding the input layer and the first hidden layer of our ANN with dropout Add other layers it is not necessary to pass the shape because there is a layer before Adding the output layer Compiling the ANN Training the ANN Predicting the Test set results convert probabilities to binary output Compute error between predicted data and true response and display it in confusion matrix Create and train model on train data sample Predict for test data sample. 77511 2 0. Data exploration There is 77 of missing data in the cabin column it s usually way too much for this column to be exploitable but as we have a small amount of data we will still try to use it in feature engineering. Features engineering My advice is to group all the transformations to be done on the dataset in a single function. Moreover they have the same ticket and cabin so they probably had to board at the same place According to the distribution above the more probable embarked value for them is Cherbourg C. Moreover if you want to add a modification you ll have to do it only in the function 3. To build a neural network we are going to use Keras. jpg cb 1440698161 3. com vi yuMNWt6S0ZA maxresdefault. net wp content uploads 2017 01 bias variance tradeoff. machinelearningtutorial. Keras allows you to build neural networks by assembling blocks which are the layers of our neural network. 78468 10 0. Apply our homemade model on test dataset and submit on kaggle 7. Submission submission 7. svg All models seems to have a good accuracy and nearly the same variance it seems that there is no best model. Here we are even going to display our tree to improve our understanding on how the algorithm manage to classify our samples To find out more about decision trees DT https medium. As we saw before the two missing values for embarked columns can be replaced by C Cherbourg We replace missing ages by the mean age of passengers who belong to the same group of class sex family We replace the only missing fare value for test dataset and the missing values of the cabin column Create a Title column from name column Filling Age missing values with mean age of passengers who have the same title Transform categorical variables to numeric variables Create a Family Size Is Alone Child and Mother columns Modification of cabin column to keep only the letter contained corresponding to the deck of the boat Create a ticket survivor column which is set to 1 if an other passenger with the same ticket survived and 0 else Note this implementation is ugly and unefficient if sombody found a way to do it easily with pandas it must be a way please comment the kernel with your solution These two columns are not useful anymore Let s divide the train dataset in two datasets to evaluate perfomance of the machine learning models we ll use We scale our data it is essential for a smooth working of the models. Moreover the variance is a little bit too high for saying that these models are reliable 0. The diversity from these 3 very different models will increase the quality of our predictions and reduce the variance 5. 2 Decision tree Decision tree is a quite intuitive model easy to vizualize and interpret. com keras tutorial deep learning in python is a great keras tutorial. com williamkoehrsen random forest simple explanation 377895a60d2d is a good start. By doing this we will go through several topics and fundamental techniques of machine learning. More than the half of passengers are in the lowest class pclass 3 Most of the fare tickets are below 50 Majority of passengers are alone sibsp and parch Note this EDA is not complete at all since it is not the purpose of this kernel to make a deep exploration of data. It outperforms almost always the two previous algorithm we saw. com nhlr21 titanic colorful eda for this competition if this interests you. com train test split and cross validation in python 80b61beca4b6 Neural network with keras https elitedatascience. Feature egineering fe 3. Try several models trymodels 4. respectivey type I II errors. In fact if we make submissions with all of these models we will obtain approximately the same score. Else it ll predict that the passenger did not survived. com qconrio machinelearningforeveryone 150826200704 lva1 app6892 95 qcon rio machine learning for everyone 51 638. From those two types of error some metrics can be computed the F1 score the Recall the accuracy. In the fit method we just train our 3 classifiers on training data. To obtain a better score we will in the next part build our own classifier which will combine predictions from a random forest an svm classifier and a keras neural networks. com keras tutorial deep learning in python Deep Learning with python https www. 79425 2 0. Thank you for your reading feel free to fork this kernel and improve it and enjoy datascience D Check out this beautiful distribution of kaggle scoring for our homemade classifier Ignore warnings Some useful functions we ll use in this notebook Path of datasets Create dataframe for training dataset and print five first rows as preview Compute some basical statistics on the dataset Let s plot some histograms to have a previzualisation of some of the data. 77990 3 0. com books deep learning with python Ensembling https mlwave. For the age we will either interpolate missing values or we will fill it with the mean for the corresponding category in term of class age sex of passenger. net profile Kiret_Dhindsa publication 323969239 figure fig10 AS 607404244873216 1521827865007 The K fold cross validation scheme 133 Each of the K partitions is used as a test. Finding the best model using k folds cross validation The precision we calculated above for those 4 different models does not mean anything. Complete Titanic tutorial with ML NN Ensembling Welcome on this Titanic tutorial It is aimed for beginners but whatever your level you could read it and if you find a way to improve it I encourage you to fork this notebook and contribute by adding a better solution https i. Indeed there is no one model which seems truly better than the other. Let s try this new model on the test dataset now 6. Ensembling creating a homemade classifier Ensembling is the science of combining classifiers to improve the accuracy of a models. Results results Imports and useful functions 1. We ll replace these two missing values later during features engineering part. 79904 2 Although I know that we can do much better the 0. Therefore we can compute a mean error over the 10 trainings of our model https www. com kaggle ensembling guide Table of Contents 1. Data exploration data_exploration 2. com machine learning comprehensive guide feature engineering Evaluating a model over one training metrics https machinelearningmastery. jpg In this notebook we are going to predict wether a passenger of the famous boat will survive or not. To understand better the above implemented architecture and dive deeply that s the case to say in this exciting field I highly recommend you to read the great book Deep Learning with Python link at the beginning of the kernel written by Fran\u00e7ois Chollet the creator of the keras framework. Moreover our solution remains simple and accessible even for beginners. com python pandemonium introduction to exploratory data analysis in python 8b6bcb55c190 Features engineering https adataanalyst. 1 Logistic regression Logistic regression is the hello world of machine learning algorithms. 5 Artificial neural network Neural networks are more complex and more powerful algorithm than standars machine learning it belongs to deep learning models. 78947 11 0. There is only two missing values for the embarked column let s try to replace it. You can find on the image below a quick summary of what is a confusion matrix how to read it and what are those metrics https image. In fact if we execute each cell again we could have sightly different accuracy because we trained again our models We need to verify which model has the best accuracy over several training steps We can do it using cross validation method which consists of dividing out training set in k parts folds and evaluating k times using successively each part as the test set and the 9 other parts as the training set. Let s look at there two observations and choose the best matching embarked value according to their fare value and sex Both passengers are female who paid 80 dollars as fare for their tickets. This way you can apply the same changes to the training dataset and the test dataset easily. We will not only compute the mean but also the variance because a good model needs to have the lowest possible variance in addition to have a low bias https www. To do that we ll create a class with two methods fit predict just as the other classifiers we used from sckitlearn and keras. Moreover it diminushes the variance of our model making it more reliable. Try several models Introduction to metrics To evaluate our models on the test set for this classification problem we are going to use several metrics which will be displayed into a confusion matrix to easily see the false positive and the false negative predicted by the model i. ", "id": "nhlr21/complete-titanic-tutorial-with-ml-nn-ensembling", "size": "11098", "language": "python", "html_url": "https://www.kaggle.com/code/nhlr21/complete-titanic-tutorial-with-ml-nn-ensembling", "git_url": "https://www.kaggle.com/code/nhlr21/complete-titanic-tutorial-with-ml-nn-ensembling", "script": "optimizers keras.layers keras.models train_test_split keras.wrappers.scikit_learn predict IPython.display keras EsemblingClassifier visualize_tree sklearn.svm cross_val_score numpy seaborn Dropout Dense SVC preprocess_data sklearn.tree sklearn sklearn.linear_model matplotlib.pyplot KerasClassifier DecisionTreeClassifier sklearn.exceptions Sequential metrics sklearn.model_selection pandas display_confusion_matrix RandomForestClassifier LogisticRegression export_graphviz build_ann fit scipy.stats NotFittedError display __init__ draw_missing_data_table sklearn.ensemble StandardScaler sklearn.preprocessing ", "entities": "(('Only aproximately 35', 'passengers'), 'let') (('s', 'data'), 'thank') (('nearly same it', 'good accuracy'), 'seem') (('wp net content', 'bias variance 2017 01 tradeoff'), 'upload') (('it', 'models'), 'see') (('way you', 'training dataset'), 'apply') (('which', 'neural network'), 'allow') (('also good model', 'bias https low www'), 'compute') (('we', 'sckitlearn'), 'predict') (('forest Random 4 Random forest', 'decision robust practical trees'), 'be') (('metrics https image', 'confusion how it'), 'find') (('metrics', 'Recall'), 'compute') (('passenger', 'famous boat'), 'jpg') (('very how it', 'https here towardsdatascience'), 'be') (('ANN', 'test data sample'), 'mean') (('com keras tutorial', 'deep python'), 'be') (('good which', 'algorithm'), 'be') (('SVM 3 SVMs', 'two different categories'), 'aim') (('You', 'https here towardsdatascience'), 'start') (('we', 'training data'), 'train') (('it', 'learning deep models'), 'be') (('this', 'you'), 'com') (('homemade EsembleClassifier', 'survivor'), 'make') (('Indeed one which', 'truly other'), 'be') (('Results', 'Imports functions'), 'result') (('Therefore we', 'model https www'), 'compute') (('s', 'test dataset'), 'let') (('com chiragsehra42 decision trees', 'title easily 28f23241248 dt'), 'explain') (('com machine', 'training metrics https over one machinelearningmastery'), 'feature') (('we', 'feature engineering'), 'exploration') (('we', 'Keras'), 'go') (('we', 'approximately same score'), 'obtain') (('it', 'model'), 'diminushe') (('they', 'them'), 'have') (('you', 'model'), 'want') (('However you', 'EDA kernel https www'), 'look') (('Moreover solution', 'even beginners'), 'remain') (('which', 'model false i.'), 'go') (('here results', 'Score occurrences'), 'result') (('We', 'own classifier'), 'be') (('we', 'anything'), 'find') (('We', 'features engineering later part'), 'replace') (('model', 'also quite performances'), 'seem') (('one', 'best method'), 'let') (('com train test', 'Neural keras https 80b61beca4b6 elitedatascience'), 'split') (('you', 'EDA Data exploration https more medium'), 'be') (('I', 'keras framework'), 's') (('k', 'cross validation https towardsdatascience'), 'evaluate') (('I', 'solution https better i.'), 'aim') (('who', 'tickets'), 'let') (('same model', '0'), 'mean') (('you', 'webpage https www'), 'refer') (('at all it', 'data'), 'be') (('we', 'almost always two previous algorithm'), 'outperform') (('Logistic regression Logistic 1 regression', 'hello machine learning algorithms'), 'be') (('which', 'training 9 other set'), 'have') (('diversity', 'variance'), 'increase') (('that', 'data still sceintist'), 'see') (('we', 'machine fundamental learning'), 'go') (('you', 'only function'), 'moreover') (('which', 'svm classifier'), 'build') (('com books', 'python Ensembling https deep mlwave'), 'learning') (('we', 'passenger'), 'interpolate') (('Below distribution', 'two Embarked value'), 'be') (('how algorithm', 'decision trees DT https medium'), 'go') (('133 Each', 'test'), 'profile') (('s', 'it'), 'be') (('which', 'tensor manipulation google'), 'be') (('advice', 'single function'), 'feature') (('Ensembling', 'models'), 'be') "}