{"name": "using featuretools for missed appointments ", "full_name": " h1 Using Featuretools to Predict Missed Appointments h2 Structuring the Data h2 Generating Features with Deep Feature Synthesis h2 Machine Learning h1 Some Plots ", "stargazers_count": 0, "forks_count": 0, "description": "Changing the variable type will change which functions are automatically applied to generate features. We ll go through the content step by step in the next section. As an example computers tend to read age as numeric. In order to make predictions at the time the appointment is scheduled we set the cutoff_time for each row to be the scheduled_time. EntitySets are a collection of tables with information about relationships between tables and semantic typing for every column. By explicitly marking those we can avoid using data from the future while creating features. list_primitives max_depth 3 says to stack up to 3 primitives deep. Data Wrangling After loading the data with pandas we have to fix typos in some column names but we change others as well to suit personal preference. Using Featuretools to Predict Missed AppointmentsIn this notebook we use Featuretools https github. PERCENT_TRUE appointments. Feel free to fork this kernel and modify the parameters. Feature engineering requires that we use what we understand about the data to build numeric rows feature vectors which we can use as input into machine learning algorithms. It would be equally easy to make a feature matrix for the locations entity agg_primitives and trans_primitives are lists of which primitives will be used while constructing features. Each row represents a scheduled appointment and our goal is to predict if the patient actually shows up for that appointment. Supposing that we don t want to use our label to predict itself we either need to specify that it is valid for use after the other columns or drop it out of the dataframe entirely. Notice that we specified an index a time index a secondary time index and the variable_types from the last cell as keyword arguments. We ll walk through the steps now. In particular because of our secondary time index it won t use the label to create features. Similar to the other kernels we do not do a good job predicting no shows. 5 with similar scores for F1 and K first. As an example the feature locations. Here s a short overview of the keywords used target_entity is the entity for which we re building features. The variable_types dictionary is a place to store information about the semantic type of each column. PERCENT_TRUE no_show will calculate percentage of patients of at this location that haven t shown up in the past. That means that DFS while building features will only use the data that is known as the appointment is made. com somrikbanerjee predicting show up no show and make some other interesting features automatically. Even though ages are numbers it s can be useful to think of them as Categorical or Ordinal. The time index and secondary time index notate when certain columns are valid for use. Here we ll show the number of visits by neighborhood and the likelihood to show up by neighborhood and age as created by DFS. Generating Features with Deep Feature SynthesisWith our data structued in an EntitySet we can immediately build features across our entity and relationships with Deep Feature Synthesis DFS https docs. This is where the time indices get used. To do that we only need to specify where we start appointments the name of the new entity e. It takes roughly 20 minutes of work to structure any data and make your first feature matrix using Featuretools. Machine LearningWe can put the created feature matrix directly into sklearn. The full list can be found by running ft. This feature matrix has features like MONTH and WEEKDAY of the scheduled time and also more complicated features like how often do patients not show up to this location locations. Next we make an entity appointments We have turned the dataframe into an entity by calling the function entity_from_dataframe. patients and what the index should be e. This dataset is a single table of appointments with more than sixty thousand unique patients. Having those additional Entities and Relationships tells the algorithm about reasonable groupings which allows for some neat aggregations. Show the size of the data in a print statement This is all of the code from the notebook No need to run read this cell if you re running everything else List the semantic type for each column Use those variable types to make an EntitySet and Entity from that table Add a patients entity with patient specific variables Make locations ages and genders Take the index and the appointment time to use as a cutoff time Rename cutoff time columns to avoid confusion Make feature matrix from entityset cutoff time pair List the semantic type for each column Make an entity named appointments which stores dataset metadata with the dataframe Make a patients entity with patient specific variables Make locations ages and genders Show the patients entity Take the index and the appointment time to use as a cutoff time Rename columns to avoid confusion Generate features using the constructed entityset. We take unique values from patient age neighborhood and gender and make a new Entity for each whose rows are the unique values. We quickly reconstruct the features that were made by hand in the most popular kernel https www. approximate 3h rounds cutoff times into blocks that are 3 hours long for faster computation cutoff_time is a dataframe that says when to calculate each row verbose True makes the progress barFor more information see the documentation https docs. From that table we use Featuretools to automatically generate the features below. com automated_feature_engineering afe. We re going to show how to pass in information about semantic types of columns load in a dataframe to an EntitySet and tell the EntitySet about reasonable new Entities to make from that dataframe. com loading_data using_entitysets. com Featuretools featuretools to automatically generate features relating to when patients don t show up for doctor appointments. Structuring the DataWe are given a single table of data. The primary benefit of Featuretools is that it does not require you to make those features by hand. We have applied and stacked simple functions called primitives such as MONTH WEEKDAY and PERCENT_TRUE to build features across all the Entities in our EntitySet. By doing so you can get very different feature matrices. Note For convenience the next cell has all of the code necessary to create the feature matrix. While many types can be detected automatically some are necessarily tricky. With one unshuffled train test split our roc_auc_score is roughly. Finally we build new entities from our existing one using normalize_entity. Some PlotsAn interesting workflow with this dataset is to plot generated features to learn about the data. That knowledge is stored in a Featuretools EntitySet https docs. Instead the requirement is that you pass in what you know about the data. ", "id": "sjrothsc/using-featuretools-for-missed-appointments", "size": "6124", "language": "python", "html_url": "https://www.kaggle.com/code/sjrothsc/using-featuretools-for-missed-appointments", "git_url": "https://www.kaggle.com/code/sjrothsc/using-featuretools-for-missed-appointments", "script": "bokeh.layouts train_test_split gridplot plot_f1 show accuracy_score numpy bokeh.models.sources plot_roc_auc recall_score f1_score plot_kfirst roc_auc_score figure bokeh.io bokeh.plotting output_notebook featuretools ColumnDataSource precision_score sklearn.model_selection pandas roc_curve featuretools.variable_types RandomForestClassifier bokeh.models sklearn.metrics sklearn.ensemble HoverTool ", "entities": "(('you', 'data'), 'be') (('we', 'shows'), 'do') (('that', 'kernel https most popular www'), 'reconstruct') (('we', 'as well personal preference'), 'wrangle') (('also more complicated how often patients', 'location locations'), 'feature') (('appointment', 'only data'), 'mean') (('knowledge', 'Featuretools EntitySet https docs'), 'store') (('PlotsAn interesting workflow', 'data'), 'be') (('we', 'machine learning algorithms'), 'require') (('it', 'features'), 'in') (('we', 'Deep Feature Synthesis DFS https docs'), 'structue') (('entity', 'constructed entityset'), 'show') (('so you', 'feature very different matrices'), 'get') (('EntitySets', 'semantic column'), 'be') (('it', 'dataframe'), 'need') (('we', 'row'), 'in') (('dataset', 'sixty more than thousand unique patients'), 'be') (('patient', 'actually appointment'), 'represent') (('which', 'neat aggregations'), 'tell') (('functions', 'automatically features'), 'change') (('next cell', 'feature necessary matrix'), 'note') (('variable_types dictionary', 'column'), 'be') (('only where we', 'entity new e.'), 'need') (('we', 'features'), 'by') (('we', 'automatically features'), 'use') (('we', 'keyword arguments'), 'notice') (('Structuring', 'data'), 'give') (('more information', 'documentation https docs'), 'approximate') (('index secondary when certain columns', 'use'), 'notate') (('rows', 'each'), 'take') (('We', 'entity_from_dataframe'), 'make') (('it', 'Categorical'), 'be') (('patients don when t', 'doctor appointments'), 'featuretool') (('it', 'hand'), 'be') (('haven t', 'past'), 'calculate') (('example computers', 'numeric'), 'tend') (('We', 'next section'), 'go') (('full list', 'ft'), 'find') (('Here we', 'DFS'), 'show') (('feature first matrix', 'Featuretools'), 'take') (('We', 'dataframe'), 'go') (('Finally we', 'normalize_entity'), 'build') (('train one unshuffled test', 'roc_auc_score'), 'be') (('primitives', 'features'), 'be') (('we', 'building features'), 's') (('We', 'EntitySet'), 'apply') (('Machine LearningWe', 'directly sklearn'), 'put') (('we', 'Featuretools https github'), 'use') "}