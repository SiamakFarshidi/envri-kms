{"name": "nlp workshop ml india deep graph learning ", "full_name": " h2 Deep Graph Embeddings h2 SDNE Structural Deep Network Embeddings h3 SDNE algorithm principle h4 Similarity definition h4 2nd order similarity optimization goal h2 LINE Large Scale Information Network Embedding h3 first order proximity h3 second order proximity h2 Graph Neural Networks h2 Vanilla GNN VGCN h2 Laplacian GNN LAPGCN h2 Spline GNN SGCN h2 ChebNets GNN ChebGCN h2 Conclusion of Spectral GCN h2 GCN using Torch Geometric h2 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "For the i th vertex we have x_i s_i xi si Every s_i si Both contain the neighbor structure information of vertex i so this reconstruction process can make vertices with similar structures have similar embedding representation vectors. Here the spectral filter weights are initialized using keras tf. Generally a softmax activation is applied for classifying the outputs according to the labels. The rest of the code segment remains the same. number_of_nodes A preprocess_adj A X np. w_ u V pu wu 1. shape 1 dropout_ratecreate_spline2_reg 2. second order proximityIs only 1st order similarity enough Obviously not enough. The first order proximity suggests that nodes can be related based on adjacency and the second order suggests that nodes are characterised based on the neighbourhood of the nodes. So far we learned to know how vanilla graph nets work. The rest of the part is similar to the previous case. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. number_of_nodes idx np. io pytorch Deep Learning en week13 13 2 ChebNets GNN ChebGCN This is one of the most important parts of spectral GCN where Chebyshev polynomials are used instead of the laplacian. LINE Large Scale Information Network Embedding first order proximityThe first order similarity is used to describe the local similarity between pairs of vertices in the graph and the formal description is if u u v vThere are straight edges between them then the edges are right w_ uv wuv It is the similarity between two vertices. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. Standard ConvNets produce anisotropic filters because Euclidean grids have direction while Spectral GCNs compute isotropic filters since graphs have no notion of direction up down left right. The goal of LINE is to minimize the difference between the input and embedding distributions. Both the papers associated with the embeddings are present here SDNE https paperswithcode. This matrix is then processed and additional layers such as Embedding Layer LSTM can be added to perform node classification. Nevertheless the simplicity and effectiveness of LINE are just a couple reasons why it was the most cited paper on WWW of 2015. compile optimizer adam loss categorical_crossentropy weighted_metrics categorical_crossentropy acc print type model_input 0 type model_input 1 model. is formally defined asOrder p_u w_ u 1. SDNE Structural Deep Network Embeddings SDNE algorithm principle Similarity definitionThe definition of similarity in SDNE is the same as LINE. number_of_nodes degree_vals np. This kind of neighborhood aggregation is called Graph Convolutional Networks GCN look here for a good introduction. coo_matrix degree_vals idx idx shape n n L D A L nx. io pytorch Deep Learning en week13 13 2 The steps to produce this include creating the adjacency matrix representation along with the node features from the inputs. Simply put the first order similarity measures the similarity between two adjacent vertex pairs. The features of the node itself are therefore multiplied by the same weights as all of its neighbors. Blog https atcold. from_scipy_sparse_matrix L L nx. train 1 0 1 def train self epochs 10 initial_epoch 0 verbose 1 return tf. Overall optimization goalThe loss function of joint optimization is Lmix L2nd \u03b1L1st \u03bdLreg L_ reg Lreg Is the regularization term alpha \u03b1To control the parameters of the first order lossThe second order proximity is preserved by passing the adjacency matrix of te graph through an unsupervised autoencoder which has a built in reconstruction loss function it must minimize. KL Divergence is an important similarity metric in information theory and entropy. com method line These are very high order embeddings and used for capturing representations on exclusively large networks. Vanilla GNN VGCN The most important part of spectral convolution arises from Vanilla GNNs. Before we come to the implementation I want to introduce a slight modification that has shown to regularly outperform normal graph nets. html Torch Geometric Examples https pytorch geometric. ChebNets are GCNs that can be used for any arbitrary graph domain but the limitation is that they are isotropic. shape 1 feature_dim 32 y_train. This is basically the idea of a graph net we aggregate information of neighbors and neighbors of neighbors etc. Notice two important thing here first the weights for all the neighboring nodes is the same they don t have individual weights. com overview of deep learning on graph embeddings 4305c10ad4a4 Graph Neural NetworksThe major difference between graph data and normal data we encounter in other machine learning tasks is that we can derive knowledge from two sources Just like in other machine learning applications every node has a set of features. read_csv Input data files are available in the read only. nodes n graph. We can extend ChebNets to multiple graphs using a 2D spectral filter. One problem here is that due to the sparseness of the graph the number of non zero elements in the adjacency matrix S is far fewer than zero elements so for the neural network as long as all output 0 can achieve a good effect this is not ours want. Then information about friends of friends and so on. By looking at friends of a person it is often possible to get some insight into this person. com drive 14OvFnAXggxB8vM4e8vSURUp1TaKnovzX usp sharing scrollTo imGrKO5YH11 Torch Geometric https pytorch geometric. com paper chebnet efficient and stable constructions of Conclusion of Spectral GCNSpectral GCN is the core part of GCN where we analysed laplacian transforms in the light of convolutions. degree node for node in list graph. com rusty1s pytorch_geometric Colab https colab. The original repository is here https github. A descriptive view of the same is provided here We define that the nodes that don t have predecessors are in layer 0. For implementing the Chebnets an additional function has been added which recursively fills the Laplacian with the cheb polynomials upto k degree. The embeddings of these nodes are just their features. The classification uses the spectral graph convolution aspect where the formula is as follows A Vanilla GCN GNN utilizes the graph laplacian not normalized laplacian along with a spectral filter and recursively augments the weights of the next layer based on the previous layer. Here for simplicity k has been chosen as 2. The model inputs are in the form of node features adjacency matrix representation and the outputs are one hot encoded node labels. wu V Represents vertices u u1st order similarity with all other vertices then u uversus v vThe second order similarity can be passed p_u pu with p_v pv The similarity is expressed. We can first of all look just at one person itself. Information is also encoded in the structure of the graph. As shown in the figure above there is a straight edge between 6 and 7 and the edge weight is larger it is considered that the two are similar and the first order similarity is higher but there is no between 5 and 6 If the edges are directly connected the first order similarity between the two is 0. The two distributions are the adjacency matrix and the dot product of node embedding. shape 1 print X feature_dim A. The rest of the part involves multiplying the Laplacian tuple node_features adjacency matrix with the spectral filter kernel and applying an activation over the result. The labels have to be one hot encoded to maintain the dimensions of the inputs. Deep Graph Embeddings This is a notebook which walks through 2 of the most popular deep learning based graph embeddings SDNE and LINE. train 1 0 1 def train self epochs 10 initial_epoch 0 verbose 1 pip install tensorflow ReLU AXW X_emb LSTM 3235 return_sequences True X_emb print Xemb X_emb H Reshape X_emb. shape 1 X_emb print type Adj H X_in y_train list set list labels print len y_train A nx. This work helped inspire interest in Graph Learning as a niche in Machine Learning and eventually Deep Learning in specific. Where L is the Laplacian matrix corresponding to the graph L D S L D S D is the degree matrix of the vertices in the graph and S is the adjacency matrix. If u uversus v vIf there are no identical neighbor vertices the second order similarity is 0. html ConclusionYou have seen how to apply GNNs to real world problems and in particular how they can effectively be used for boosting a model s performance. fit model_input y_train X. To calculate the embeddings of layer k we weight the average embeddings of layer k 1 and put it into an activation function. Spline GNN SGCN Spline GCN involve computing smooth spectral filters to get localized spatial filters. io graph convolutional networks blog https atcold. The nodes are not ordered in any way. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session self. laplacian_matrix L inputs A L A nx. Papers with Code https paperswithcode. The normalization factor is not the same for all nodes but depends on their individual number of neighbors. This also makes sense intuitively when characterizing a node the neighbors do play an important role but the node itself is also important. shape 1 X_emb print type Adj H X_in y_train list set list labels print len y_train inputs A L A nx. The second order similarity measures the similarity of the neighbor sets of two vertices. So how does all of this come together as a neural network Let s continue with our example of a social circle. For example when we look at a social network every node can be a person with a certain age gender interests political views etc. asarray X_n model_input A A print X type X print A type A model GCN A. Since the algorithm has to define new functions for each increasing order of proximity LINE doesn t perform very well if the application needs an understanding of node community structure. This represents the eigenvectors which can be added independently instead of taking the entire laplacian at one time. One method given in the article is to use a weighted loss function which has a higher penalty coefficient for non zero elements. The graph below shows a small friend group where an edge between two nodes means that these two people are friends with each other. KLDivergence y_pred y_true self. adjacency_matrix graph nodelist range graph. 1st order similarity optimization goalCapture Reconstruction loss of the predicted and true values2nd order similarity optimization goalUse the trace of the laplacian matrix and normalize the resultsThe loss function can make the embedding vectors corresponding to two adjacent vertices in the graph close in the hidden space. This may be useful for example in recommender systems where we have movie graphs and user graphs. shape 1 A create_spline A print X X print X type X print A type A print feature_dim feature_dim print type model_input 0 type model_input 1 Add aggregation Step 1 Add self loops Step 2 Multiply with weights Step 3 Calculate the normalization Step 4 Propagate the embeddings to the next layer Normalize node features. 2nd order similarity optimization goal L2nd i 1n x i xi 22 Here we use the adjacency matrix of the graph for input. Laplacian GNN LAPGCN This is another variation of GCNs where instead of a laplacian we take the normalized laplacian matrix as the adjacency matrix and the feature matrix as inputs. As shown in the figure above although there are no straight edges between 5 and 6 they have many similar neighbor vertices 1 2 3 4. The following image depicts how a particular node learns from the neighbours when passed through h number of deep learning layers. There are two major differences image 6 We have just one set of weights in each layer so no different weights W and B anymore. The algorithm is used in probabilistic generative models like Variational Autoencoders which embed inputs of an autoencoder into a latent space which becomes the distribution. shape 1 X_n for i in range feature_dim X_n. Let s look at a simple example to make things clearer. Implementation Details For creating Line embeddings we use the default SDNE script and convert the reconstruction loss autoencoder loss for SDNE to a KL divergence based loss. number_of_nodes A preprocess_adj A print X X print X type X print A type A print feature_dim feature_dim print type model_input 0 type model_input 1 ReLU AXW X_emb LSTM 3235 return_sequences True X_emb print Xemb X_emb H Reshape X_emb. Then we can compile information about the friends of a person. This is achieved using KL divergence LINE defines two joint probability distributions for each pair of nodes then minimizes the KL divergence of the distributions. The following are the resources which are helpful blog https tkipf. shape 1 print X X print X type X print A type A print feature_dim feature_dim print type model_input 0 type model_input 1 ReLU AXW X_emb LSTM 3235 return_sequences True X_emb print Xemb X_emb H Reshape X_emb. com method sdne LINE https paperswithcode. Second when we calculate the embedding for a node v we also want to include the features of this node so we add self loops to every node. Some points Since no Laplacian eigen decomposition is used all operations are in the spatial not spectral domain Another drawback of LapGCNs is that convolutional layers involve sparse linear operations which GPU s are not fully optimized for. shape 1 dropout_rate 0. io en latest notes introduction. io pytorch Deep Learning en week13 13 2 GCN using Torch GeometricTorch Geometric is a library which has different modules for creating GCNs and variations of Graph networks and can be found in the links provided below. If there are no straight edges the first order similarity is 0. These architectures are different from the previous notebooks owing to the fact that these use first and second order proximity to determine the node representations. These are scalable representations which are based on regressed adjacency properties and laplacian maps. Resource blog https towardsdatascience. The connection between smoothness in frequency domain and localization in space is based on Parseval s Identity also Heisenberg uncertainty principle smaller derivative of spectral filter smoother function smaller variance of spatial filter localization In this case we wrap the vanilla GCN with an additional spline functionality by decomposing the laplacian to its diagonals 1 spline. number_of_nodes D scipy. This can actually indicate that 5 and 6 are similar and 2 Order similarity is used to describe this relationship. io en latest notes colabs. ", "id": "abhilash1910/nlp-workshop-ml-india-deep-graph-learning", "size": "176008", "language": "python", "html_url": "https://www.kaggle.com/code/abhilash1910/nlp-workshop-ml-india-deep-graph-learning", "git_url": "https://www.kaggle.com/code/abhilash1910/nlp-workshop-ml-india-deep-graph-learning", "script": "torch.nn.functional get_embeddings LINE() degree cal networkx Planetoid create_model init_notebook_mode Reshape normalize_adj chebyshev_polynomial model Net(torch.nn.Module) tensorflow.keras.layers build download_plotlyjs numpy tensorflow.keras.models torch_geometric.datasets Dropout train sklearn.manifold GCN Identity glorot_uniform plot_dataset tensorflow.keras.initializers Zeros chebyshev_distance Loss() Embedding LSTM Layer tensorflow MessagePassing matplotlib.pyplot encode_onehot message preprocess_adj reconstruction_loss GraphConvolution(tf.keras.layers.Layer) forward plotly.offline plotly.graph_objs pandas create_spline torch_geometric.utils iplot kldivergence_loss GCNConv(MessagePassing) plotter loss_laplace Model l2 call Input add_self_loops test recursion node_level_embedding __init__ SDNE() TSNE get_config LabelEncoder tensorflow.keras.regularizers torch_geometric.nn sklearn.preprocessing ", "entities": "(('is', 'LSTM node classification'), 'add') (('labels', 'inputs'), 'have') (('which', 'links'), 'pytorch') (('scalable which', 'adjacency regressed properties'), 'be') (('Graph Convolutional Networks GCN', 'here good introduction'), 'call') (('which', 'zero elements'), 'be') (('It', 'two vertices'), 'order') (('which', 'one time'), 'represent') (('first order', 'node representations'), 'be') (('number_of_nodes preprocess_adj A', 'feature_dim print 1 AXW True X_emb feature_dim type model_input 0 model_input 3235 print'), 'a') (('We', '2D spectral filter'), 'extend') (('where we', 'convolutions'), 'be') (('papers', 'embeddings'), 'be') (('we', 'KL divergence based loss'), 'use') (('normalization', 'next layer'), 'shape') (('t', 'session outside current self'), 'list') (('outputs', 'adjacency matrix representation'), 'be') (('most important part', 'Vanilla GNNs'), 'VGCN') (('embedding vectors', 'close hidden space'), 'optimization') (('13 week13 2 steps', 'inputs'), 'pytorch') (('rest', 'previous case'), 'be') (('read_csv Input data files', 'read'), 'be') (('very well application', 'node community structure'), 'perform') (('com method These', 'exclusively large networks'), 'line') (('Vanilla GCN GNN', 'previous layer'), 'use') (('following image', 'learning deep layers'), 'depict') (('probability two joint distributions', 'distributions'), 'achieve') (('GPU', 'which'), 'be') (('particular how they', 'effectively performance'), 'see') (('which', 'graph embeddings most popular deep learning SDNE'), 'embedding') (('rest', 'result'), 'involve') (('Information', 'graph'), 'encode') (('list labels', 'print len'), 'type') (('things', 'simple example'), 'let') (('vertices', 'embedding representation similar vectors'), 'have') (('that', 'graph regularly normal nets'), 'come') (('Order 2 similarity', 'relationship'), 'indicate') (('train', 'train self 1 1 def 10 0 1 return'), 'verbose') (('SDNE Structural Deep Network Embeddings SDNE algorithm principle Similarity definitionThe definition', 'LINE'), 'be') (('L D S L D S D', 'graph'), 'be') (('where we', 'movie graphs'), 'be') (('goal', 'input distributions'), 'be') (('It', 'kaggle python Docker image https github'), 'come') (('it', 'person'), 'by') (('X print X X', 'feature_dim print 1 AXW True X_emb feature_dim type model_input 0 model_input 3235 print'), 'shape') (('Chebyshev where polynomials', 'instead laplacian'), 'pytorch') (('Then we', 'person'), 'compile') (('order directly first similarity', 'two'), 'be') (('order second similarity', 'two vertices'), 'measure') (('they', 'don individual weights'), 'notice') (('asarray model_input A print X type X', 'type model'), 'X_n') (('embeddings', 'nodes'), 'be') (('we', 'neighbors'), 'be') (('this', '0 good effect'), 'be') (('Here we', 'input'), 'goal') (('s', 'social circle'), 'come') (('which', 'latent space'), 'use') (('neighbor order identical second similarity', 'u vIf'), 'be') (('node', 'important role'), 'make') (('list labels', 'print len y_train L A nx'), 'type') (('node', 'political views'), 'be') (('we', 'diagonals 1 spline'), 'base') (('work', 'Machine Learning'), 'help') (('node', 'features'), 'embedding') (('rest', 'code segment'), 'remain') (('graphs', 'direction'), 'produce') (('we', 'activation function'), 'calculate') (('which', 'k degree'), 'add') (('differences two major 6 We', 'layer'), 'be') (('just a couple why it', '2015'), 'be') (('nodes', 'nodes'), 'suggest') (('they', '1 2 3 4'), 'have') (('don t', 'layer'), 'provide') (('Spline GNN SGCN Spline GCN', 'localized spatial filters'), 'involve') (('softmax Generally activation', 'labels'), 'apply') (('they', 'graph arbitrary domain'), 'be') (('filter Here spectral weights', 'keras'), 'initialize') (('two distributions', 'adjacency dot node embedding'), 'be') (('we', 'feature inputs'), 'LAPGCN') (('k', '2'), 'choose') (('so we', 'node'), 'want') (('normalization factor', 'neighbors'), 'be') (('KL Divergence', 'similarity information important theory'), 'be') (('two people', 'other'), 'show') (('We', 'just one person'), 'look') (('it', 'reconstruction loss function'), 'be') (('features', 'neighbors'), 'multiply') (('similarity', 'p_v'), 'vertice') "}