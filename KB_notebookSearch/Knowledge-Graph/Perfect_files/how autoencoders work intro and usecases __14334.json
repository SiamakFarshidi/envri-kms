{"name": "how autoencoders work intro and usecases ", "full_name": " h2 How Autoencoders work Understanding the math and implementation h3 Contents h2 1 Introduction h2 1 1 What are Autoencoders h2 1 2 How Autoencoders work h2 Different Rules for Different data h2 2 Implementation h2 2 1 UseCase 1 Image Reconstruction h3 2 Dataset Prepration h3 3 Create Autoencoder architecture h2 2 2 UseCase 2 Image Denoising h3 Noisy Images h2 2 3 UseCase 3 Sequence to Sequence Prediction using AutoEncoders h4 Autoencoder Architecture h3 Excellent References ", "stargazers_count": 0, "forks_count": 0, "description": "2 How Autoencoders work Lets understand the mathematics behind autoencoders. In this type of data the key problem will be to obtain the projection of data in single dimention without loosing information. Here is the summary of our autoencoder architecture. Lets understand what type of network needs to be created for this problem. The main idea behind autoencoders is to learn a low level repersenation of a high level dimentional data. Encoding Architecture The encoding architure is composed of 3 Convolutional Layers and 3 Max Pooling Layers stacked one by one. Role of upsampling layer is to upsample the dimentions of a input vector to a higher resolution dimention. com applied deep learning part 3 autoencoders 1c083af4d7983. 3 UseCase 3 Sequence to Sequence Prediction using AutoEncodersNext use case is sequence to sequence prediction. Different Rules for Different dataSame rules cannot be applied to all types of data. Umsampling layers make use of this property to project the reconstructions from a low dimentional feature space. u1B u2B encoding where W is the weight matrix of hidden layer. By using Keras we can access both output states of the LSTM layer as well as the current states of the LSTM layers. In simple terms the learning process can be defined as a rule equation which converts B in the form of A and L. jpg In this autoencoder network we will add convolutional layers because convolutional networks works really well with the image inputs. Relu is used as the activation function in the convolution layers and padding is kept as same. Different types of noises can be added to the images. A typical autoencoder architecture comprises of three main components Encoding Architecture The encoder architecture comprises of series of layers with decreasing number of nodes and ultimately reduces to a latent view repersentation. com shivamb a very comprehensive tutorial nn cnn. For example consider the following data manifold view. Create Autoencoder architectureIn this section lets create an autoencoder architecture. Or in other terms what is the equation among B A and L. com wp content uploads 2017 11 denoising autoencoder 600x299. Since we know that the decoding process is the mirror image of the encoding process. So how does neural networks solves this problem The intution is In the manifold space deep neural networks has the property to bend the space in order to obtain a linear data fold view. Most of the code of this section is taken from the following reference shared by Jason Brownie in his blog post. Dataset Prepration Load the dataset separate predictors and target normalize the inputs. For more information related to CNN refer to my previous kernel https www. 2 UseCase 2 Image DenoisingAutoencoders are pretty useful lets look at another application of autoencoders Image denoising. No matter how much shifts and rotation are applied original data cannot be recovered. Many a times input images contain noise in the data autoencoders can be used to get rid of those images. Lets now create an autoencoder architecutre for learning and producing sequences made up of LSTM layers. com blog 2018 06 unsupervised deep learning computer vision 2. The encoding part comprises of three layers with 2000 1200 and 500 nodes. Encoding architecture is connected to latent view space comprising of 10 nodes which is then connected to decoding architecture with 500 1200 and 2000 nodes. Reference https machinelearningmastery. Implementation and UseCases 2. We can define the number of LSTM memory units in the LSTM layer Each unit or cell within the layer has an internal memory cell state often abbreviated as c and outputs a hidden state often abbreviated as h. https towardsdatascience. If we can define following Reference Point on the line A Angle L with a horizontal axis then any other point say B on line A can be repersented in terms of Distance d from A and angle L. But instead of 3 maxpooling layers we will be adding 3 upsampling layers. For example Salt and Pepper Noise Gaussian Noise Periodic Noise Speckle Noise Lets introduce salt and pepper noise to our data which is also known as impulse noise. Lets look at the model summariesNow lets train the autoencoder model using Adam optimizer and Categorical Cross Entropy loss functionLets write a function to predict the sequence based on input sequence Generate some predictions Excellent References1. Autoencoder architectures applies this property in their hidden layers which allows them to learn low level representations in the latent view space. com develop encoder decoder model sequence sequence prediction keras Thanks for viewing the kernel please upvote if you liked it. u1B u2B Inverse W. 2 How Autoencoders Work 2. For example consider the following data in 2dimentional space. When this type of data is projected in latent space a lot of information is lost and it is almost impossible to deform and project it to the original shape. To apply convolutions on image data we will reshape our inputs in the form of 28 28 matrix. 3 UseCase 3 Sequence to Sequence Prediction 1. Autoencoders are widly used with the image data and some of their use cases are Dimentionality Reduction Image Compression Image Denoising Image Generation Feature Extraction 1. An autoencoder is a regression task where the network is asked to predict its input in other words model the identity function. These low level features are then deformed back to project the actual data. png Step1 Repersent the points in Latent View Space If the coordinates of point A and B in the data representation space are Point A x1A x2A Point B x1B x2B then their coordinates in the latent view space will be x1A x2A 0 0 x1B x2B u1B u2B Point A 0 0 Point B u1B u2B Where u1B and u2B can be represented in the form of distance between the point and the reference point u1B x1B x1A u2B x2B x2A Step2 Represent the points with distance d and angle L Now u1B and u2B can represented as a combination of distance d and angle L. d L d 0 after rotation This is the output of the encoding process and repersents our data in low dimensions. com develop encoder decoder model sequence sequence prediction keras Autoencoder Architecture The architecuture of this use case will contain an encoder to encode the source sequence and second to decode the encoded source sequence into the target sequence called the decoder. It presents itself as sparsely occurring white and black pixels. Before adding noiseAfter adding noiseLets now create the model architecture for the autoencoder. Lets understand this process from a autoencoder perspective. png A highly fine tuned autoencoder model should be able to reconstruct the same input which was passed in the first layer. I am using imaug package which can be used to augment the images with different variations. Lets try to understand the encoding process with an example. Lets first of all generate a sequence dataset containing random sequences of fixed lengths. Lets obtain the predictions of the modelIn this implementation I have not traiened this network for longer epoochs but for better predictions you can train the network for larger number of epoochs say somewhere in the range of 500 1000. The answer is straigtforward there is no fixed equation but a best possible equation is obtained by the unsupervised learning process. This usecase can be applied in machine translation. First lets understand the internal working of LSTMs which will be used in this architecture. But it is possible to reduce the dimensions of this space into lower dimensions ie. In the previous example we input an image which was a basicaly a 2 dimentional data in this example we will input a sequence data as the input which will be 1 dimentional. Again the activation function will be same relu and padding in convolution layers will be same as well. We will create a function to generate random sequences. These networks has a tight bottleneck of a few neurons in the middle forcing them to create effective representations that compress the input into a low dimensional code that can be used by the decoder to reproduce the original input. How Autoencoders work Understanding the math and implementation Contents 1. There are two components An encoder architecture which takes a sequence as input and returns the current state of LSTM as the output A decoder architecture which takes the sequence and encoder LSTM states as input and returns the decoded output sequence We are saving and accessing hidden and memory states of LSTM so that we can use them while generating predictions on unseen data. For example in the previous example we projected a linear data manifold in one dimention and eliminated the angle L. Data Manifold is the space inside the data repersentation space in which the true data resides. The final layer comprises of exact number of nodes as the input layer. Noisy Images We can intentionally introduce the noise in an image. sample rate the frequency of the signal the points on the x axis for plotting Lets add sample noise Salt and Pepper input layer encoding architecture decoding architecture compile the model convert the elements to categorical using keras api remove unnecessary dimention. X1 repersents the input sequence containing random numbers X2 repersents the padded sequence which is used as the seed to reproduce the other elements of the sequence y repersents the target sequence or the actual sequence Next lets create the architecture of our model in Keras. 1 What are Autoencoders Autoencoders are a special type of neural network architectures in which the output is same as the input. Consider a data repersentation space N dimentional space which is used to repersent the data and consider the data points repersented by two variables x1 and x2. 1 UseCase 1 Image Reconstruction1. An important point is that Rules Learning function encoding decoding equation will be different for different types of data. Thanks to ColinMorris for suggesting the correction in salt and pepper noise. In this kernel I will walk you through the working of autoencoders and their implementation. Decoding Architecture Similarly in decoding architecture the convolution layers will be used having same dimentions in reverse manner as the encoding architecture. Increase the number of epochs to a higher number for better results. io building autoencoders in keras. If we recall the fundamental equation of a neural network with weights and bias of every layer then d 0 W. edu people karpathy convnetjs demo autoencoder. png Lets implement an autoencoder using keras that first learns the features from an image and then tries to project the same image as the output. Unlike other recurrent neural networks the network s internal gates allow the model to be trained successfully using backpropagation through time or BPTT and avoid the vanishing gradients problem. Load the required libraries 2. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. Similarly the decoding architecture converts back this representation to original form u1B u2B and then x1 x2. This layer applies a max filter to non overlapping subregions of the initial representation. d 0 decoding The reduced form of data x1 x2 is d 0 in the latent view space which is obtained from the encoding architecture. Here is the model summaryTrain the model with early stopping callback. The following image describes this property https i. Generate the predictions on validation data. First lets prepare the train_x and val_x data contianing the image pixels. This noise introduces sharp and sudden disturbances in the image signal. But the key question here is with what logic or rule point B can be represented in terms of A and angle L. One such variation can be introduction of noise. Latent View Repersentation Latent view repersents the lowest level space in which the inputs are reduced and information is preserved. But what if the data manifold cannot be projected properly. 1 What are Autoencoders 1. https machinelearningmastery. And if we rotate this by angle L towards the horizontal axis L will become 0. Decoding Architecture The decoding architecture is the mirro image of the encoding architecture but in which number of nodes in every layer increases and ultimately outputs the similar almost input. Unlike CNNs in image example in this use case we will use LSTMs. The Long Short Term Memory or LSTM is a recurrent neural network that is comprised of internal gates. Lets plot the original and predicted image Inputs Actual Images Predicted Autoencoder Output So we can see that an autoencoder trained with 20 epoochs is able to reconstruct the input images very well. 2 UseCase 2 Noise Removal 2. 1 UseCase 1 Image Reconstruction 2. Lets look at other use case of autoencoders Image denoising or removal of noise from the image. Next we will train the model with early stopping callback. The max pooling operation is non invertible however an approximate inverse can be obtained by recording the locations of the maxima within each pooling region. Role of max pooling layer is to downsample the image dimentions. To repersent this data we are currently using 2 dimensions X and Y. Example of sequence data are time series data and text data. Consider the autoencoder with no hidden layers the inputs x1 and x2 are encoded to lower repersentation d which is then further projected into x1 and x2. Lets see it in action. ", "id": "shivamb/how-autoencoders-work-intro-and-usecases", "size": "14334", "language": "python", "html_url": "https://www.kaggle.com/code/shivamb/how-autoencoders-work-intro-and-usecases", "git_url": "https://www.kaggle.com/code/shivamb/how-autoencoders-work-intro-and-usecases", "script": "keras.layers keras.models train_test_split init_notebook_mode to_categorical reverse_onehot numpy array_equal Image augmenters predict_sequence randint Dense argmax keras.callbacks MaxPool2D imgaug define_models LSTM random matplotlib.pyplot PIL plotly.graph_objs plotly.offline sklearn.model_selection pandas EarlyStopping iplot Conv2D Model UpSampling2D Input dataset_preparation keras.utils ", "entities": "(('which', 'then further x1'), 'encode') (('noise', 'image signal'), 'introduce') (('summaryTrain', 'stopping early callback'), 'be') (('decoding process', 'mirror encoding process'), 'know') (('number', 'ultimately similar almost input'), 'decode') (('equation', 'B A'), 'be') (('architecture decoding model', 'unnecessary dimention'), 'rate') (('architecuture', 'target sequence'), 'develop') (('it', 'dimensions lower ie'), 'be') (('This', 'low dimensions'), 'd') (('Most', 'blog post'), 'take') (('that', 'original input'), 'have') (('which', 'also impulse noise'), 'introduce') (('input Many times images', 'images'), 'contain') (('following image', 'property https i.'), 'describe') (('convolutional networks', 'image really well inputs'), 'jpg') (('First lets', 'image val_x pixels'), 'prepare') (('Dataset Prepration', 'inputs'), 'Load') (('work', 'autoencoders'), 'understand') (('Different types', 'images'), 'add') (('Lets', 'fixed lengths'), 'generate') (('which', 'first layer'), 'be') (('How Autoencoders', 'math'), 'work') (('Here summary', 'autoencoder architecture'), 'be') (('we', 'LSTM as well current layers'), 'access') (('Encoding encoder architecture', 'view ultimately latent repersentation'), 'comprise') (('which', 'different variations'), 'use') (('Autoencoders', 'input data'), 'train') (('section architectureIn lets', 'autoencoder architecture'), 'create') (('It', 'sparsely occurring white pixels'), 'present') (('Noisy We', 'image'), 'Images') (('decoding', 'data'), 'be') (('you', '500 1000'), 'obtain') (('output', 'input'), '1') (('One such variation', 'noise'), 'be') (('we', 'angle'), 'project') (('Next actual lets', 'Keras'), 'repersent') (('true data', 'which'), 'be') (('data points', 'variables two x1'), 'consider') (('Example', 'sequence data'), 'be') (('which', '500 1200 nodes'), 'connect') (('them', 'view latent space'), 'apply') (('final layer', 'input layer'), 'comprise') (('usecase', 'machine translation'), 'apply') (('convolution layers', 'encoding architecture'), 'use') (('Next we', 'stopping early callback'), 'train') (('which', 'encoding architecture'), 'be') (('we', 'dimensions currently 2 X'), 'use') (('deep neural networks', 'view'), 'solve') (('non however approximate inverse', 'pooling region'), 'be') (('Encoding encoding architure', 'one'), 'architecture') (('encoding part', '2000'), 'comprise') (('u2B u1B where W', 'weight hidden layer'), 'encode') (('it', 'original shape'), 'lose') (('we', 'upsampling 3 layers'), 'add') (('L', 'horizontal axis'), 'become') (('Role', 'image dimentions'), 'be') (('Role', 'resolution higher dimention'), 'be') (('Umsampling layers', 'feature low dimentional space'), 'make') (('which', 'architecture'), 'understand') (('activation function', 'convolution layers'), 'use') (('which', 'input'), 'input') (('we', 'unseen data'), 'be') (('3 UseCase 3 Sequence', 'prediction'), 'be') (('you', 'it'), 'develop') (('fixed best possible equation', 'learning unsupervised process'), 'be') (('Lets', 'image'), 'look') (('activation Again function', 'convolution same layers'), 'be') (('We', 'random sequences'), 'create') (('type', 'problem'), 'understand') (('recurrent neural that', 'internal gates'), 'be') (('main idea', 'high level dimentional data'), 'be') (('We', 'often h.'), 'define') (('Autoencoders', 'Image Generation'), 'use') (('level low features', 'then back actual data'), 'deform') (('which', 'A'), 'define') (('pretty useful lets', 'autoencoders Image denoising'), '2') (('rule here B', 'A'), 'be') (('com blog', 'computer 2018 06 deep learning vision'), 'unsupervise') (('model', 'gradients vanishing problem'), 'allow') (('Lets', 'autoencoder perspective'), 'understand') (('edu people', 'demo autoencoder'), 'convnetjs') (('I', 'autoencoders'), 'walk') (('Lets', 'LSTM layers'), 'create') (('layer', 'initial representation'), 'apply') (('summariesNow lets', 'predictions'), 'look') (('we', 'LSTMs'), 'use') (('Lets', 'example'), 'try') (('input', 'identity function'), 'be') (('information', 'which'), 'repersent') (('that', 'output'), 'implement') (('Different Rules', 'data'), 'apply') (('autoencoder', 'input images'), 'plot') (('A', 'L.'), 'say') (('Now u1B', 'distance d'), 'repersent') (('key problem', 'information'), 'be') (('we', 'layer'), 'recall') (('we', '28 28 matrix'), 'reshape') "}