{"name": "beating everything with depthwise convolution ", "full_name": " h3 How many samples for each class are there in the dataset h3 Preparing validation data h2 Augmentation h3 Training data generator h2 Model ", "stargazers_count": 0, "forks_count": 0, "description": "Feel free to use it for further fine tuning of the network. The aim of this kernel is to develop a robust deep learning model from scratch on this limited amount of data. Instead of randomly initialized weights for these layers it would be much better if you fine tune them. We will initialize the weights of first two convolutions with imagenet weights I have commented out the training step as of now as it will train the network again while rendering the notebook and I would have to wait for an hour or so which I don t want to. It introduces lesser number of parameters as compared to normal convolution and as different filters are applied to each channel it captures more information. but for your reference I will attach the screenshot of the training steps here. Let s look at how a normal case is different from that of a pneumonia case. The rage for competing on Kaggle should never end. 0001 decay 1e 6 Get a train data generator Define the number of training steps history model. Happy Kaggling This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. We will insert the data into this list in img_path label format Go through all the normal cases. Initialize the first few layers from a network that is pretrained on imagenet. png When a particular problem includes an imbalanced dataset then accuracy isn t a good metric to look for. either there will be too many normal cases or there will be too many cases with the disease. Each of the above directory contains two sub directories NORMAL These are the samples that describe the normal no pneumonia case. For example Depthwise SeparableConv is a good replacement for Conv layer. We will load those weights and will run the inference on the test set using those weights only. But augmentation can be much more helpful where the dataset is imbalanced. The classifier might label every example as negative and still achieve 95 accuracy. 4 Load the model weights Preparing test data Evaluation on test dataset Get predictions Original labels Get the confusion matrix Calculate Precision and Recall. AugmentationData augmentation is a powerful technique which helps in almost every case for improving the robustness of a model. But if applied very carefully it can benefit the world in enormous ways. You can generate different samples of undersampled class in order to try to balance the overall distribution. Choose layers that introduce a lesser number of parameters. We all know that deep learning models are data hungry but if you know how things work you can build good models even with a limited amount of data. But if you notice the precision is only 80. Reproducibility is a great concern when doing deep learning. Xception a powerful network is built on top of such layers only. This is all that I have done in the next code block. If you look carefully then there are some cases where you won t be able to differentiate between a normal case and a pneumonia case with the naked eye. We will normalize the pixel values and resizing all the images to 224x224 Normal cases Pneumonia cases Convert the list into numpy arrays Augmentation sequence horizontal flips roatation random brightness Get total number of samples in the data Define two numpy arrays for containing batch data and labels Get a numpy array of all the indices of the input data Initialize a counter Get the next batch one hot encoding read the image and resize check if it s grayscale cv2 reads in BGR mode by default normalize the image pixels generating more samples of the undersampled class Open the VGG16 weight file Select the layers for which you want to set weight. In such problems a good recall value is expected. This is exactly what the code block given below is doing. Choose a simple architecture. PNEUMONIA This directory contains those samples that are the pneumonia cases. The dataset consists of only very few samples and that too unbalanced. The label for these cases will be 1 Get a pandas dataframe from the data we have in our list Shuffle the data How the dataframe looks like Get the counts for each class Plot the results Get few samples for both the classes Concat the data in a single list and del the above two list Plot the data Get the path to the sub directories Get the list of all the images List that are going to contain validation images data and the corresponding labels Some images are in grayscale while majority of them contains 3 channels. I will explain this in detail but before that I would love to share one of the best practices when it comes to building deep learning models from scratch on limited data. We will set a numer of things in order to make sure that the results are almost reproducible if not fully. It comes with a very clean api and you can do hell of augmentations with it. There was a good discussion on KaggleNoobs slack regarding this. That s it folks I hope you enjoyed this kernel. For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. You will notice Oneof and it does exactly that. We will be doing partial transfer learning and rest of the model will be trained from scratch. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. Do it for the depth of your network too. You should transfer learn but wisely. I have uploaded the weights of the best model I achieved so far. There is one case in the above plot at least for me which is too much confusing. read_csv Input data files are available in the. So if the image is grayscale we will convert into a image with 3 channels. How many samples for each class are there in the dataset As you can see the data is highly imbalanced. For example if your dataset contains 95 negatives and 5 positives having a model with 95 accuracy doesn t make sense at all. Hello everyone Hope everything is fine and you are enjoying things on Kaggle as usual. Pneumonia is a very common disease. Preparing validation dataWe will be defining a generator for the training dataset later in the notebook but as the validation data is small so I can read the images and can load the data without the need of a generator. I like imgaug https imgaug. This dataset consists pneumonia samples belonging to the first two classes. Precision and Recall are really good metrics for such kind of problems. This is because first few layers capture general details like color blobs patches edges etc. It can be either 1 Bacterial pneumonia 2 Viral Pneumonia 3 Mycoplasma pneumonia and 4 Fungal pneumonia. Precision and Recall follows a trade off and you need to find a point where your recall as well as your precision is more than good but both can t increase simultaneously. Set the seed for hash based operations in python Set the numpy seed Disable multi threading in tensorflow ops Set the random seed in tensorflow at graph level Define a tensorflow session with above session configs Set the session in keras Make the augmentation sequence deterministic Define path to the data directory Path to train directory Fancy pathlib. We will do some analysis on that look at some of the samples check the number of samples for each class etc. opt RMSprop lr 0. Model This is the best part. The dataset is divided into three sets 1 train set 2 validation set and 3 test set. training https i. If you look at other kernels on this dataset everyone is busy doing transfer learning and fine tuning. We have almost with thrice pneumonia cases here as compared to the normal cases. The label for these cases will be 0 Go through all the pneumonia cases. fit_generator train_data_gen epochs nb_epochs steps_per_epoch nb_train_steps validation_data valid_data valid_labels callbacks es chkpt class_weight 0 1. You can do more than this if you want but I think at this point this is more than enough I need. It s worth exploring In the next code block I will define a augmentation sequence. At each iteration it will take one augmentation technique out of the three and will apply that on the samples Training data generator Here I will define a very simple data generator. Once you know a good depth start training your network with a lower learning rate along with decay. Hence we need to look for alternative metrics. And as a Machine learning engineer it s our responsibility to help people as much as we can in all possible ways. This is one thing to notice. It s more than just a classification problem. If we can build a robust classifier it would be a great assist to the doctor too. You can read about Xception and Depthwise Separable Convolutions in this https arxiv. Let s grab the dataset We will first go through the training dataset. The stake is very high. Train with a higher learning rate and experiment with the number of neurons in the dense layers. path Path to validation directory Path to test directory Get the path to the normal and pneumonia sub directories Get the list of all the images An empty list. The data will always be imbalanced. Add dense layers with reasonable amount of neurons. Machine Learning and Deep Learning have a huge scope in healthcare but applying them in healthcare isn t that simple. We will get the confusion matrix from our predictions and see what is the recall and precision of our model. We will look at somes samples from our training data itself. This situation is very normal when it comes to medical data. Nice So our model has a 98 recall. As the network becomes deeper batch norm start to play an important role. Use batch norm with convolutions. ", "id": "aakashnain/beating-everything-with-depthwise-convolution", "size": "7132", "language": "python", "html_url": "https://www.kaggle.com/code/aakashnain/beating-everything-with-depthwise-convolution", "git_url": "https://www.kaggle.com/code/aakashnain/beating-everything-with-depthwise-convolution", "script": "mlxtend.plotting skimage.transform matplotlib.image join keras.layers pathlib keras.models SGD train_test_split os keras to_categorical listdir confusion_matrix keras.preprocessing.image numpy Image MaxPooling2D seaborn isdir img_to_array Dropout Concatenate Path ImageDataGenerator Adam BatchNormalization load_img Dense Callback remove preprocess_input backend as K build_model imgaug.augmenters keras.layers.normalization imread imgaug keras.callbacks os.path plot_confusion_matrix RMSprop keras.optimizers tensorflow getcwd skimage.io keras.applications.vgg16 matplotlib.pyplot ModelCheckpoint GlobalMaxPooling2D Sequential backend PIL abspath resize pandas sklearn.model_selection data_gen EarlyStopping Conv2D Model SeparableConv2D Input VGG16 makedirs isfile keras.utils expanduser keras.layers.merge sklearn.metrics Flatten exists StandardScaler sklearn.preprocessing ", "entities": "(('you', 'data'), 'know') (('first few layers', 'etc'), 'be') (('I', 'best model'), 'upload') (('You', 'overall distribution'), 'generate') (('I', 'code next block'), 'be') (('it', 'exactly that'), 'notice') (('code block', 'exactly what'), 'be') (('aim', 'data'), 'be') (('which', 'at least me'), 'be') (('as well more both', 'point'), 'follow') (('Here I', 'data very simple generator'), 'take') (('Choose that', 'parameters'), 'layer') (('batch deeper norm', 'important role'), 'start') (('powerful which', 'model'), 'be') (('recall good value', 'such problems'), 'expect') (('We', 'training data'), 'look') (('don t', 'which'), 'initialize') (('you', 'Kaggle'), 'Hope') (('classifier', 'negative still 95 accuracy'), 'label') (('it', 'more information'), 'introduce') (('you', 'weight'), 'normalize') (('When particular problem', 'accuracy isn then good metric'), 'png') (('Machine Learning', 'healthcare isn t'), 'have') (('that', 'imagenet'), 'initialize') (('how normal case', 'pneumonia case'), 'let') (('results', 'order'), 'set') (('We', 'weights'), 'load') (('We', 'scratch'), 'do') (('path Path', 'empty list'), 'get') (('I', 'training steps'), 'attach') (('Original labels', 'confusion matrix Calculate Precision'), 'Load') (('rage', 'Kaggle'), 'end') (('read_csv Input data files', 'the'), 'be') (('what', 'model'), 'get') (('I', 'enough'), 'do') (('We', 'training first dataset'), 'let') (('dataset', 'first two classes'), 'consist') (('that', 'pneumonia normal case'), 'contain') (('good depth', 'decay'), 'start') (('It', 'classification just problem'), 's') (('1 train', '2 validation'), 'divide') (('dataset', 'only very few samples'), 'consist') (('when it', 'limited data'), 'explain') (('you', 'kernel'), 's') (('Precision', 'problems'), 'be') (('0', 'pneumonia cases'), 'be') (('You', 'Separable https arxiv'), 'read') (('powerful network', 'such layers'), 'build') (('you', 'output'), 'list') (('I', 'generator'), 'define') (('you', 'it'), 'come') (('augmentation sequence', 'directory Fancy pathlib'), 'set') (('We', 'class etc'), 'do') (('everyone', 'transfer learning'), 'be') (('We', 'normal cases'), 'insert') (('Depthwise SeparableConv', 'Conv good layer'), 'be') (('t', 'sense'), 'make') (('majority', '3 channels'), 'be') (('carefully then where you', 'pneumonia naked eye'), 'be') (('very when it', 'medical data'), 'be') (('I', 'augmentation sequence'), 's') (('very carefully it', 'enormous ways'), 'benefit') (('Reproducibility', 'great when deep learning'), 'be') (('We', 'here normal cases'), 'have') (('It', 'python docker image https kaggle github'), 'happy') (('that', 'samples'), 'PNEUMONIA') (('we', '3 channels'), 'convert') (('it', 'great doctor'), 'be') (('as much we', 'possible ways'), 's') (('data', 'there dataset'), 'be') (('much you', 'fine them'), 'be') (('Hence we', 'alternative metrics'), 'need') "}