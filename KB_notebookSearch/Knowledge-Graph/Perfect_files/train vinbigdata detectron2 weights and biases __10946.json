{"name": "train vinbigdata detectron2 weights and biases ", "full_name": " h1 Detectron 2 Training and weights and biases integration h1 Author Sourabh Chauhan h2 Detectron 2 Training with weights and biases monitoring h1 Table of Contents h1 Dataset preparation h1 Installation h1 Training method implementations h2 Data preparation h1 Customizing detectron2 trainer h2 Mapper for augmentation h2 Evaluator h2 Loss evaluation hook h2 LR scheduling h1 Loading Data h1 Data Visualization h1 Training h1 Visualize loss curve competition metric AP40 ", "stargazers_count": 0, "forks_count": 0, "description": "Here implemented VinbigdataEvaluator is constructed we can also use COCOEvaluator here. It is nice to start with however I want to customize the training behavior more to improve the model s performance. Data preparation detectron2 provides high level API for training custom dataset. AugInput image transforms self. 40 and replaced to show this value instead of AP with IoU 0. Its content is saved to metric. I insert LossEvalHook before evalutor to work well. Data VisualizationIt s also very easy to visualize prepared training dataset with detectron2. width width of the image. roi_batch_size_per_image 128 faster and good enough for this toy dataset default 512 eval_period 20 aug_kwargs HorizontalFlip p 0. com c vinbigdata chest xray abnormalities detection discussion 207955. Mapper for augmentation Mapper class is used inside pytorch DataLoader. Python3 s json writer which always produces strings cannot serialize a bytestream unless you decode it. 95 1e 5 step dtype torch. We can analyze plot them to check how the training proceeded. You just need to wrap above training scripts by main method and use launch method provided by detectron2. The output will be in the following directory kaggle working results v23 model_final. XYXY_ABS is used here meaning that absolute value of xmin ymin xmax ymax annotation is used in the bbox. Loss evaluation hookWe implemented Evaluator and now we can calculate competition metric however validation loss is not calculated inside Evaluator. Inference from the notebook should be combined with 2 Class filter to obtain a incerement of public leaderboard metric of about 0. NOTE this config means the number of classes but a few popular unofficial tutorials incorrect uses num_classes 1 here. inspired from Detectron https github. com facebookresearch detectron2 blob master INSTALL. This dataset_dicts contains the metadata for actual data fed into the neural network. log validation loss local dict_mylaptop_val_loss x step_local dict_mylaptop_val_loss y 1. height height of the image. Predicted box_proposals are in XYXY_ABS mode. dimension of precision TxRxKxAxM IoU dimension of recall TxKxAxM stats 2 _summarize 1 iouThr. read_csv imgdir test_meta. This can be done by adding Hook which calculates the loss to the trainer. Therefore we subtract 0. annotation This is the ground truth annotation data for object detection which contains following bbox bounding box pixel location with shape n_boxes 4 bbox_mode BoxMode. Note that current implementation is not efficient in the sense that Evaluator s evaluation and LossEvalHook s loss calculation run separately even if both need a model forward calculation for same validation data. Thankfully utf 8 works out which is also what the pycocotools _mask. 05 alpha_affine 120 0. html build_lr_scheduler supports only 2 types of LR scheduling WarmupMultiStepLR default WarmupCosineLR. 3 optical distrotion not applied here ElasticTransform alpha 120 sigma 120 0. This is the inverse of data loading logic in datasets coco. put_scalars validation_loss mean_loss is called to put this validation loss to the storage which will be saved to metrics. We remove the bbox field to let mask AP use mask area. http file home sourabh kaggle xrays VBD chestxray hyper_param_06 03. Load 1 image to get image size. build_hooks This method defines how to construct hooks. Here I modified COCOEvaluator implementation to calculate AP with IoU 0. pop sem_seg_file_name None return dataset_dict h w Copyright c Facebook Inc. 5 ShiftScaleRotate scale_limit 0. md we need to know CUDA and pytorch version to install correct detectron2. plotly models setup GPU check Installing Detectron2 For debug. Here AlbumentationMapper is passed to construct DataLoader to insert customized augmentation process. py L255 noqa Record max overlap value for each gt box Return vector of overlap values all small medium large 96 128 128 256 256 512 512 inf sort predictions in descending order TODO maybe remove this and make it explicit in the documentation guard against no boxes find which proposal box maximally covers each gt box and get the iou amount of coverage for each gt box find which gt box is best covered i. put_scalar validation_loss mean_loss return losses How loss is calculated on train_loop self. LR_SCHEDULER_NAME as you can see from the docs. array obj category_id for obj in dataset_dict annotations dtype np. Famouns dataset s evaluator is already implemented in detectron2. 3 OpticalDistortion distort_limit 2 shift_limit 0. TrainingIt s actually very easy to use multiple gpus for training. build_evaluator This class method defines how to construct Evaluator. Visualize loss curve competition metric AP40As I explained the calculated metrics are saved in metrics. It provides Visualizer class we can use it to draw an image with bounding box as following. This is because model s evaluation is done in model. build_train_loader build_test_loader These class methods deine how to construct DataLoader for training data validation data respectively. put_scalars timetest 11 from detectron2. 5 RandomBrightnessContrast p 0. To calculate validation loss we need to call model with the training mode. You can refer the Detectron2 Beginner s Tutorial https colab. Now the methods are ready. float32 compute recall for each iou threshold ar 2 np. Jump directly to weights and biases integration cell training Make sure to add wandb_api key to your notebook using Add ons. You can change which one to use by setting cfg. com facebookresearch Detectron blob a6a835f5b8208c45d0dce217ce9bbda915f44df7 detectron datasets json_dataset_evaluator. 5 to be consistent with the annotation format. png Detectron 2 Training with weights and biases monitoring UPDATE 2021 2 21 Training 512 512 on R101 backbone 3 000 iterations comparison with the laptop output v16 UPDATE 2021 2 20 Training 1024 1024 on R101 backbone 10 000 iterations training anchor box change things UPDATE 2021 2 20 Training 1024 1024 on R101 backbone 18 000 iterations UPDATE 2021 2 16 Training 1024 1024 on R101 backbone 30 000 iterations did not run. I will demonstrate these augmentations later so you can skip reading the code and please just jump to next. json files can be directly used for inference. It is loaded beforehand of the training on memory so it should contain all the metadata image filepath etc to construct training dataset but should not contain heavy data. best most iou find the proposal box that covers the best covered gt box record the iou coverage of this gt box mark the proposal box and the gt box as used append recorded iou coverage level thresholds torch. Customizing detectron2 trainer This section is advanced I recommend to jump to Training scripts section for the first time of reading. DefaultTrainer is used in the example which provides the starting point to train your model with custom dataset. Here I will just use the dataset VinBigData Chest X ray Resized PNG 256x256 https www. is_train dataset_dict. Loading DataThis Flags class is to manage experiments. pop sem_seg_file_name None return dataset_dict h w it will be modified by code below aug_input T. print row print row class_name class_name row class_name It is No finding Use this No finding class with the bbox covering all image area. We can make own Trainer class MyTrainer here for this purpose and override methods to provide customized behavior. com facebookresearch detectron2 blob master configs COCO Detection faster_rcnn_R_50_FPN_3x. py L222 L252 noqa precision has dims iou recall cls area range max dets area range index 0 all area ranges max dets index 1 typically 100 per image tabulate it use RLE to encode the masks because they are too large and takes memory since this evaluator stores outputs of the entire dataset counts is an array encoded by mask_util as a byte stream. trapz recalls thresholds When evaluating mask AP if the results contain bbox cocoapi will use the box area as the area of the instance instead of the mask area. 03 same problem with elastic transform RandomResizedCrop height 100 width 100 p 0. Please refer official example train_net. the standard metrics Compute per category AP from https github. com xhlulu vinbigdata chest xray resized png 256x256 to skip the preprocessing and focus on modeling part. com corochann vinbigdata detectron2 train This notebook includes a Weights and biases integration of the Detectron2 models to follow hyperparameters of the datasets. EvaluatorTo evaluate validation dataset to calculate competition metric we need Evaluator. For example many kinds of AP Average Precision is calculted in COCOEvaluator. augmentations aug_input image aug_input. Ref detectron2 docs Dataloader https detectron2. read_csv datadir sample_submission. To define custom dataset we need to create list of dict dataset_dicts where each dict contains following file_name file name of the image. image category_id np. Detectron 2 Training and weights and biases integration Author Sourabh ChauhanGoals Integrate the Detectron2 models for computer vision with wandb api for hyperparameter optimization in VBD project. Copy so the caller can do whatever with results unmap the category ids for COCO cocoapi does not handle empty results very well Saving generated box proposals to file. It is responsible for converting dataset_dicts into actual data fed into the neural network and we can insert augmentation process in this Mapper class. Trainer has attribute storage and calculated metrics are summarized. com facebookresearch detectron2 blob master MODEL_ZOO. This annotator does not find anything skip. com apofeniaco training on detectron2 with a validation set and plot loss on it to avoid overfitting 6449418fbf4e Now all the preparation has done MyTrainer overwraps build_evaluator method of DefaultTrainer provided by detectron2 to support validation dataset evaluation. HACKING overwrite iouThrs to calc ious 0. overflew 9 hour limit of GPU time on kaggle sessions UPDATE 2021 2 16 Training 1024 1024 on R101 backbone UPDATE 2021 2 12 Training on the full size image instead of 256 256 Table of Contents Dataset preparation dataset Installation installation Training method implementations train_method Customizing detectron2 trainer custom_trainer Advanced topic skip it first time Mapper for augmentation mapper Evaluator evaluator Loss evaluation hook loss_hook Loading Data load_data Data Visualization data_vis Training training Visualize loss curve competition metric AP40 vis_loss Visualization of augmentation by Mapper vis_aug Next step next_step Dataset preparationPreprocessing x ray image format dicom into normal png image format is already done by xhlulu in the below discussion Multiple preprocessed datasets 256 512 1024px PNG and JPG modified and original ratio https www. com drive 16jcaJoc6bCFAQ96jDe2HwtXj7BMD_ m5 scrollTo QHnVupBBn9eR Colab Notebook or version 7 of this kernel https www. pass aug_kwargs to cfg Let training initialize from model zoo pick a good LR Small value Frequent save need a lot of storage. Below LossEvalHook calculates validation loss in _do_loss_eval method and self. json jsonl format during training. 15 rotate_limit 10 p 0. 3 RandomContrast p 0. evaluation import COCOEvaluator PascalVOCDetectionEvaluator return PascalVOCDetectionEvaluator dataset_name not working return COCOEvaluator dataset_name bbox False output_dir output_folder flags General Data config all_train or valid20 original or wbf Training config images per batch this corresponds to total batch size WarmupMultiStepLR default or WarmupCosineLR Overwrite by param_dict flags_dict debug True outdir results debug imgdir_name vinbigdata chest xray resized png 256x256 split_mode valid20 iter 100 debug small value should be set. category_id class label id for each bounding box with shape n_boxes get_vinbigdata_dicts is for train dataset preparation and get_vinbigdata_dicts_test is for test dataset preparation. We can finetune these pre trained architectures. Default build_lr_schduler method docs https detectron2. We can follow installation instruction https github. pop annotations None dataset_dict. 5 RandomGamma p 0. io en latest _modules detectron2 solver build. int64 Remove unnecessary field. maxDets 2 Infering it from predictions should be better Test set json files do not contain annotations evaluation must be performed using the COCO evaluation server. eval mode and it outputs bounding box prediction but does not output loss. 95 but we need AP with IoU 0. In practice loading all the taining image arrays are too heavy to be loaded on memory so these are loaded inside DataLoader on demand This is done by mapper class in detectron2 as I will expain later. Training method implementationsBasically we don t need to implement neural network part detectron2 already implements famous architectures and provides its pre trained weights. 1 args parse Read data kaggle input vinbigdata 1024 image dataset vinbigdata Read in the data CSV files alias sample_submission pd. record image_id index objs record annotations objs utils configs T. I will tune these parameters through the competition to improve model s performance. Please upvote the dataset as well Installationdetectron2 is not pre installed in this kaggle docker so let s install it. In COCO annotations keypoints coordinates are pixel indices. io en latest tutorials data_loading. image_id id of the image index is used here. com corochann vinbigdata detectron2 train scriptVersionId 51628272 for the simple usage of detectron2 how to train custom dataset. These models are summarized in MODEL_ZOO. Ref Training on Detectron2 with a Validation set and plot loss on it to avoid overfitting https medium. main scripts starts from here. LR schedulingTo further customize learning rate scheduling you may override build_lr_scheduler class method to construct any pytorch LRScheduler. 4 Use the COCO default keypoint OKS sigmas unless overrides are specified COCOAPI requires every detection and every gt to have keypoints so we just take the first entry from both Copying inference_on_dataset from evaluator. Resize 800 800 it will be modified by code below if not self. html I implemented MyMapper which uses augmentations implemented in detectron2 and AlbumentationsMapper which uses albumentations library augmentations. com facebookresearch detectron2 blob master tools train_net. COCOEvaluator only calculates AP with IoU from 0. yaml for this kernel. csv To get number of data. In this competition we need object detection model I will choose R50 FPN https github. Notes This notebook uses corochan s Detectron2 implementation for VBD dataset http https www. This leads to a different definition of small medium large. However our predictions are floating point coordinates. ", "id": "sourabhchauhan/train-vinbigdata-detectron2-weights-and-biases", "size": "10946", "language": "python", "html_url": "https://www.kaggle.com/code/sourabhchauhan/train-vinbigdata-detectron2-weights-and-biases", "git_url": "https://www.kaggle.com/code/sourabhchauhan/train-vinbigdata-detectron2-weights-and-biases", "script": "tools lightgbm albumentations dataclasses pathlib Boxes plotly.express DatasetEvaluator detectron2.config Optional evaluate preprocessing Path _do_loss_eval MyTrainer(DefaultTrainer) field plotly.io scipy _get_loss matplotlib.pyplot instances_to_coco_json detectron2.data COCOeval load_yaml _summarizeKps Visualizer PathManager CfgNode as CN DefaultTrainer get_vinbigdata_dicts_test pycocotools.cocoeval detectron2.data.datasets.coco pycocotools.mask collections _summarizeDets create_small_table get_vinbigdata_dicts detectron2.structures process _eval_predictions build_train_loader detectron2.evaluation catboost Flags MyMapper _summarize detectron2.evaluation.fast_eval_api setup_logger sklearn _derive_coco_results HookBase plotly.graph_objs build_evaluator pandas detectron2.evaluation.evaluator vin_summarize detectron2.engine display subplots IPython.core.display detectron2.utils.visualizer pycocotools.coco detectron2.utils.file_io build_detection_train_loader detectron2.engine.hooks save_yaml numpy get_cfg detectron2 dataclass plotly PascalVOCDetectionEvaluator detectron2.utils.logger Any detectron2.data.transforms detection_utils as utils build_test_loader COCOeval_opt typing log_every_n_seconds build_hooks plotly.offline VinbigdataEvaluator(DatasetEvaluator) LossEvalHook(HookBase) tqdm.notebook build_detection_test_loader DefaultPredictor plotly.figure_factory after_step pairwise_iou HTML strtobool _tasks_from_predictions MetadataCatalog __call__ AlbumentationsMapper COCOEvaluator convert_to_coco_json DatasetCatalog launch xgboost update Union OrderedDict seaborn distutils.util _evaluate_predictions_on_coco tqdm detectron2.utils.comm COCO CfgNode _evaluate_box_proposals Dict detectron2.config.config KFold kaggle_secrets model_zoo tabulate sklearn.model_selection reset BoxMode _eval_box_proposals UserSecretsClient __init__ detection_utils ", "entities": "(('models', 'MODEL_ZOO'), 'summarize') (('gt box', 'gt box find'), 'overlap') (('com Detectron blob detectron', 'json_dataset_evaluator'), 'facebookresearch') (('which', 'custom dataset'), 'use') (('which', 'metrics'), 'call') (('html build_lr_scheduler', 'scheduling WarmupMultiStepLR default LR WarmupCosineLR'), 'support') (('models plotly GPU', 'Installing debug'), 'check') (('we', 'also COCOEvaluator'), 'implement') (('dataset_dicts', 'neural network'), 'contain') (('LR learning schedulingTo further customize you', 'pytorch'), 'rate') (('t', 'pre trained weights'), 'method') (('class method', 'how Evaluator'), 'define') (('which', 'trainer'), 'do') (('args parse data kaggle input image dataset 1 1024 Read', 'sample_submission pd'), 'Read') (('30 000 iterations', 'R101 2021 2 16 Training 1024 1024 backbone'), 'train') (('com corochann vinbigdata detectron2 notebook', 'datasets'), 'train') (('so s', 'it'), 'upvote') (('This', 'datasets coco'), 'be') (('Here I', 'just dataset'), 'use') (('competition we', 'Evaluator'), 'need') (('Data preparation detectron2', 'custom dataset'), 'provide') (('it', 'loss'), 'output') (('detectron2 solver', '_ io latest modules'), 'build') (('put_scalar validation_loss mean_loss return How loss', 'train_loop self'), 'loss') (('800 800 it', 'code'), 'Resize') (('tutorials a few popular unofficial incorrect', 'num_classes'), 'mean') (('I', 'performance'), 'tune') (('which', 'bbox_mode n_boxes 4 BoxMode'), 'annotation') (('evaluation', 'model'), 'be') (('I', 'reading'), 'customize') (('method', 'how hooks'), 'define') (('Now preparation', 'validation dataset evaluation'), 'train') (('that', 'gt iou coverage level thresholds used append recorded torch'), 'find') (('You', 'Tutorial https Detectron2 colab'), 'refer') (('1024px 256 512 PNG', 'below discussion'), 'UPDATE') (('evaluator stores outputs', 'byte stream'), 'dim') (('I', 'evalutor'), 'insert') (('We', 'override customized behavior'), 'make') (('many kinds', 'COCOEvaluator'), 'calculte') (('get_vinbigdata_dicts_test', 'dataset test preparation'), 'd') (('You', 'detectron2'), 'need') (('Frequent save', 'storage'), 'pass') (('print row print row class_name class_name row It', 'image area'), 'class_name') (('where dict', 'image'), 'define') (('md we', 'pytorch correct detectron2'), 'need') (('Loading', 'experiments'), 'be') (('We', 'trained architectures'), 'finetune') (('This', 'small medium large'), 'lead') (('Famouns evaluator', 'already detectron2'), 'implement') (('annotations evaluation', 'COCO evaluation server'), 'maxdet') (('I', 'detectron2'), 'be') (('later you', 'just next'), 'demonstrate') (('calculated metrics', 'attribute storage'), 'have') (('Here AlbumentationMapper', 'augmentation customized process'), 'pass') (('Predicted box_proposals', 'XYXY_ABS mode'), 'be') (('LossEvalHook', '_ do_loss_eval method'), 'calculate') (('TxKxAxM 2 _', '1 iouThr'), 'stat') (('we', 'box'), 'provide') (('which', 'albumentations library augmentations'), 'implement') (('Mapper class', 'pytorch DataLoader'), 'use') (('com xhlulu vinbigdata chest xray', 'part'), 'resize') (('however I', 'more performance'), 'want') (('you', 'it'), 'serialize') (('it', 'heavy data'), 'load') (('Detectron 2 Training', 'VBD project'), 'Integrate') (('how training', 'them'), 'analyze') (('we', 'evaluator'), 'Use') (('d', 'image index'), 'image_id') (('ymin xmax ymax annotation', 'bbox'), 'use') (('We', 'installation instruction https github'), 'follow') (('Data VisualizationIt', 'detectron2'), 's') (('category ids', 'box very well generated proposals'), 'copy') (('validation however loss', 'Evaluator'), 'evaluation') (('Here I', 'IoU'), 'modify') (('record index objs record annotations', 'utils configs'), 'image_id') (('COCOEvaluator', '0'), 'calculate') (('Inference', 'about 0'), 'combine') (('calculated metrics', 'metrics'), 'metric') (('we', 'training mode'), 'need') (('loss separately even both', 'validation same data'), 'note') (('dataset_dict it', 'aug_input T.'), 'sem_seg_file_name') (('3 OpticalDistortion', '2 shift_limit'), 'distort_limit') (('notebook', 'VBD dataset http https www'), 'Notes') (('class build_train_loader build_test_loader methods', 'data validation data'), 'deine') (('json files', 'directly inference'), 'use') (('output', 'directory kaggle working following results'), 'be') (('you', 'docs'), 'LR_SCHEDULER_NAME') (('bbox cocoapi', 'instance'), 'recalls') (('mask AP', 'mask area'), 'remove') (('we', 'Mapper class'), 'be') (('split_mode valid20 iter 100 debug small value', 'png'), 'return') (('I', 'R50 FPN https github'), 'need') (('However predictions', 'point coordinates'), 'float') (('TrainingIt', 'training'), 's') (('You', 'cfg'), 'change') "}