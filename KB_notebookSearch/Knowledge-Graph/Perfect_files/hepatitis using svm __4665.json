{"name": "hepatitis using svm ", "full_name": " h2 Dataset Reading and Pre Processing steps h4 4 Check the datatype of each variable h4 5 Drop columns which are not significant h4 6 Identify the Categorical Columns and store them in a variable cat cols and numerical into num cols h4 7 Checking the null values h4 8 Split the data into X and y h4 9 Split the data into X train X test y train y test with test size 0 20 using sklearn h4 10 Check null values in train and test check value counts in y train and y test h4 11 Impute the Categorical Columns with mode and Numerical columns with mean h4 Convert all the categorical columns to Integer Format before dummification 2 0 as 2 etc h4 12 Dummify the Categorical columns h4 13 Scale the numeric attributes age bili alk sgot albu protime h2 MODEL BUILDING SVM h4 Non Linear SVM RBF h3 SVM with Grid Search for Paramater Tuning ", "stargazers_count": 0, "forks_count": 0, "description": "Impute the Categorical Columns with mode and Numerical columns with mean Convert all the categorical columns to Integer Format before dummification 2. gender male 1 female 2 no 2 yes 1 4. xj If there is a function which could calculate the dot product and the result is the same as when we transform the data into higher dimension it would be fantastic. However if we transform the two dimensional data to a higher dimension say three dimension or even ten dimension we would be able to find a hyperplane to separate the data. Read the HEPATITIS dataset and check the data shapes 2. steroid no yes 5. Drop columns which are not significant 6. histology no yes16. Dummify the Categorical columns 13. SVM with Grid Search for Paramater Tuning Code to ignore warnings target 1 Die 2 Live null values in train null values in test Impute on train df_cat_train df_cat_train. The problem is if we have a large dataset containing say millions of examples the transformation will take a long time to run. fatique no yes 7. When gamma is high the curve of the decision boundary is high which creates islands of decision boundaries around data points. When C is small the classifier is okay with misclassified data points high bias low variance. Check for value counts in target variable 4. 20 using sklearn 10. spleen no yes 12. Identify the Categorical Columns and store them in a variable cat_cols and numerical into num_cols 7. Kernel Trick Image you have a two dimensional non linearly separable dataset you would like to classify it using SVM. liverFirm no yes 11. target DIE 1 LIVE 2 2. sgot 13 100 200 300 400 500 19. Check basic summary statistics of the data 3. It looks like not possible because the data is not linearly separable. protime 10 20 30 40 50 60 70 80 90 NA s are represented with Dataset Reading and Pre Processing stepsimport required libraries 1. liverBig no yes 10. iloc 0 Impute on test df_cat_test df_cat_test. Split the data into X_train X_test y_train y_test with test_size 0. iloc 0 Impute on train df_num_train df_num_train. Check null values in train and test check value_counts in y_train and y_test 11. age 10 20 30 40 50 60 70 803. When gamma is low the curve of the decision boundary is very low and thus the decision region is very broad. To solve this problem we actually only care about the result of the dot product xi. Attribute information 1. Gamma Gamma is a parameter of the RBF kernel and can be thought of as the spread of the kernel and therefore the decision region. mean Combine numeric and categorical in train Combine numeric and categorical in test Train Test Train Test scale on train scale on test Create a SVC classifier using a linear kernel Train the classifier Get the best parameters. Split the data into X and y 9. alk 33 80 120 160 200 250 18. This function is called a kernel function. Checking the null values 8. spiders no yes13. Scale the numeric attributes age bili alk sgot albu protime MODEL BUILDING SVM Non Linear SVM RBF Radial Basis Function is a commonly used kernel in SVC where x i x2212 x j 2 is the squared Euclidean distance between two data points xi and xjIt is only important to know that an SVC classifier using an RBF kernel has two parameters gamma and C. When C is large the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points low bias high variance. C C is a parameter of the SVC learner and is the penalty for misclassifying a data point. ascites no yes 14. anorexia no yes 9. varices no yes15. antivirals no yes 6. Check the datatype of each variable 5. malaise no yes 8. mean Impute on test df_num_test df_num_test. In essence what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions. ", "id": "mragpavank/hepatitis-using-svm", "size": "4665", "language": "python", "html_url": "https://www.kaggle.com/code/mragpavank/hepatitis-using-svm", "git_url": "https://www.kaggle.com/code/mragpavank/hepatitis-using-svm", "script": "numpy sklearn.impute train_test_split f1_score SVC SimpleImputer confusion_matrix sklearn.model_selection pandas sklearn.metrics GridSearchCV accuracy_score StandardScaler sklearn.svm sklearn.preprocessing ", "entities": "(('transformation', 'long time'), 'be') (('SVC only classifier', 'parameters two gamma'), 'sgot') (('even ten we', 'data'), 'say') (('you', 'SVM'), 'image') (('HEPATITIS', 'data shapes'), 'read') (('C C', 'data point'), 'be') (('decision very thus region', 'decision boundary'), 'be') (('Check', 'target'), 'variable') (('we', 'dot product xi'), 'care') (('does', 'higher dimensions'), 'be') (('classifier', 'data misclassified points high bias low variance'), 'be') (('protime', 'Pre Processing required libraries'), 'represent') (('classifier', 'best parameters'), 'mean') (('which', 'data points'), 'be') (('it', 'higher dimension'), 'xj') (('Gamma Gamma', 'kernel'), 'be') (('therefore bends', 'data backwards misclassified points low high variance'), 'penalize') (('SVM', 'train'), 'target') "}