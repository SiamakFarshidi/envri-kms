{"name": "generalization to imagenet sketch ", "full_name": " h1 Can pretrained ImageNet models generalize to sketches h1 The Images h1 The Models h1 Results h1 Top 1 Results by Model h1 Top 1 Differences h1 Top 5 Differences h1 Text Summary of Top 1 and Top 5 Differences h1 Best and Worst Predictions h1 The Best Predictions h1 The Worst Predictions ", "stargazers_count": 0, "forks_count": 0, "description": "From working with adversarial attacks and defense in the past Jigsaw is one of those classes that s easy to confuse NN. Right away we see that the Instragram pretrained ResNeXt models are in class of their own and suprisingly doing not too shabbily Top 1 Differences Looking a bit closer we plot the differences between the ImageNet Sketch Top 1 validation scores and the original ImageNet 1k validation scores. In terms of quality and detail there is a wide range highly detailed sketches with lots of texture to cartoonish or child like drawings. ipynb dataset and comparing models for runtime performance https colab. Top 1 results by model2. Import packages setup our logger and check if we have CUDA linear algebra data processing CSV file I O e. The ModelsThe pretrained models that will be run cover a wide range of performance on ImageNet 1k from mobile optimized MobileNet V3 through the workhorse range in the the ResNet50 SE ResNext50 to some larger higher performing models like PNASNet5 Large and EfficientNet B5 and finally ending up with the full set of released Instagram pretrained ResNeXt101 models. Can pretrained ImageNet models generalize to sketches I ve been exploring generalization capabilities of models recently after poking around with the ImageNetV2 https colab. com rwightman pytorch image models The ImagesLooking at a random sampling from the dataset we can get a taste of what the images look like. This is definitely very different from ImageNet samples. Compare differences for Top 1 accuracy between each model on ImageNet 1K and ImageNet Sketch3. Several models especially Facebook s weakly supervised Instagram tag pretrained ResNeXt models https pytorch. Also visible are other model to model variations in how well they do on Sketch vs original validation. We ll collect per example losses and top 5 predictions and then display the results. Top 5 DifferencesThe same as above for the Top 5 validation scores. ResultsWe re going to walk through the results as follows 1. You can basically apply any image on top of a jigsaw puzzle and this is indeed the case in examples for this class. You can make a similar case for the shower curtains a tighter crop of those illustrations would have helped. Let s see how the models perform without training them on this dataset. Same as above but for Top 5 differences Top 1 Results by ModelThe figure and text output below display the Top 1 accuracy of the models on the ImageNetSketch dataset. Again the IG ResNeXts stand apart. The Worst PredictionsLooking at these samples I actually feel the model could be doing even better. The InceptionResnetV2 is one of the less impacted non IG models while the EfficientNet B2 and MobileNet V3 are hit the hardest. read_csv a basic validation routine and runner that configures each model and loader metrics for collecting per sample prediction loss details only bother collecting top5 we re also collecting per example loss cleanup checkpoint cache to avoid running out of disk space add some non metric values for charting comparisons create key to identify model in charts load the dataset Run all the models through validation Setup the common charting elements download ImageNet 1k resuls run on my model collection for top 1 top 5 comparisons some helpers for the dumbbell plots draw the ImageNet Sketch dots we re sorted on this draw the original ImageNet 1k validation dots draw the ImageNet Sketch top 5 dots we re sorted on this draw the original ImageNet 1k validation dots create mappings of label id to text and synset re run validation on just one model this time collecting per example losses and predictions a function to display images in a grid and ground truth vs predictions for specified indices. The ImageNet Sketch dataset here fits the bill the same classes as ImageNet 1k but a very different distribution in terms of the images themselves. I am not expecting reasonable performance. com github rwightman pytorch image models blob master notebooks GeneralizationToImageNetV2. org hub facebookresearch_WSL Images_resnext stood out as having a comparatively lower drop in accuracy when applied to different datasets without retraining them. I wanted to explore this further on other datasets that had OOTB compatibility with ImageNet pretrained models. First thing that stood out for me as that they are lacking in color not completely but almost completely void of color. There are clearly issues with the dataset in terms of mislabled examples and confusing images that have multiple valid answers. Text Summary of Top 1 and Top 5 DifferencesIf you prefer text an absolute and relative diff ranking of all models by their top 1 and top 5 Best and Worst PredictionsWe re going to re run inference on one of our better models a ResNext101 32x16 pretrained on Instagram tags. We ll be using models and helpers from my PyTorch Image Models TIMM at https github. The Best PredictionsWhat does the model predict best Nothing too suprising here aside from upside down washing machine perhaps all of these samples have distinct shapes for their class or good levels of detail. com github rwightman pytorch image models blob master notebooks EffResNetComparison. ", "id": "rwightman/generalization-to-imagenet-sketch", "size": "4231", "language": "python", "html_url": "https://www.kaggle.com/code/rwightman/generalization-to-imagenet-sketch", "git_url": "https://www.kaggle.com/code/rwightman/generalization-to-imagenet-sketch", "script": "show_img draw_line_horiz Image OrderedDict numpy draw_line_vert timm.utils transforms torchvision make_grid matplotlib.pyplot validate PIL show_summary pandas runner label_line_vert matplotlib.lines label_line_horiz collections torchvision.utils ", "entities": "(('This', 'ImageNet definitely very samples'), 'be') (('ImageNet 1k validation original dots', 'specified indices'), 'routine') (('ImageNet Sketch dataset', 'images'), 'fit') (('that', 'released Instagram'), 'cover') (('We', 'https github'), 'use') (('tighter crop', 'illustrations'), 'make') (('1 Top 5 you', 'Instagram 32x16 tags'), 'prefer') (('ResultsWe', '1'), 'go') (('org hub facebookresearch_WSL Images_resnext', 'them'), 'stand') (('that', 'ImageNet pretrained models'), 'want') (('I', 'reasonable performance'), 'expect') (('images', 'what'), 'model') (('CUDA', 'CSV file'), 'setup') (('this', 'class'), 'apply') (('EfficientNet B2', 'MobileNet hardest'), 'be') (('perhaps all', 'detail'), 'predict') (('confusing that', 'multiple valid answers'), 'be') (('We', 'top 5 then results'), 'collect') (('that', 'NN'), 'be') (('I', 'ImageNetV2 https recently around colab'), 'pretraine') (('bit closer we', 'ImageNet validation Sketch Top 1 scores'), 'see') (('Several models', 'Instagram models https especially s weakly tag ResNeXt pytorch'), 'supervise') (('Same', 'ImageNetSketch dataset'), 'display') (('they', 'completely almost completely color'), 'void') (('how well they', 'original validation'), 'be') (('how models', 'dataset'), 'let') (('I', 'actually model'), 'do') "}