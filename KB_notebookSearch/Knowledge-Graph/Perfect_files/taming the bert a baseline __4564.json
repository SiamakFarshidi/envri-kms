{"name": "taming the bert a baseline ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "See also the BERT paper https arxiv. The gap development file contains the same data as the test_stage_1 file that we re trying to make predictions on. We use the method defined above to parse the contextual embeddings for each of the 3 GAP data files. The current version uses a much smaller MLP for the supervised classification problem with more regularization. The best LB score I got with this method is 0. Next we feed BERT the data from these three files. net forum id SJzSgnRcKX What do you learn from context Probing for sentence structure in contextualized word representations by Tenney et al. 05Ceshine Lee independently published a kernel with a very neat PyTorch implementation of the same idea. Fine tune the BERT model instead of using the pre trained weights. So instead of removing rows make them 0 Will train on data from the gap test and gap validation files in total 2454 rows Will predict probabilities for data from the gap development file initializing the predictions testing predictions Training and cross validation split training and validation data Define the model re initializing for each fold train the model make predictions on validation and test data oof valid_index pred_valid. The other two files gap test and gap validation are used for training the model. Downloading the pre trained BERT Base Uncased model. The idea for the architecture 1 2 above comes from the paper https openreview. 6 in the hidden layer up from 0. com ceshine pytorch bert baseline public score 0 54. tsv data contains the same 2000 rows as test_stage_1. jsonl I m lazy so I m only saving the output of the last layer. Feel free to change layers 1 to save the output of other layers. Activation relu X tX layers. Here are some helper functions to keep track of the offsets of the target words. 5 L2 regularization in the output layer of 0. Keep in mind that we will use X_test and X_validation for training and then make predictions on X_development. The kernel needs an Internet connection to do this so make sure it s enabled. py runs forward propagation through BERT and writes the output in the file output. Convert them to lower case since we re using the uncased version of BERT For each word find the offset not counting spaces. Of course gap development also contains the true labels but I m not using these when making predictions. They are very few so I m just dropping the rows. As such the MLP hyperparameters they use may not be the best for our current task. Read the three GAP files pass them through BERT and write the contextual embeddings in json files. Finally let s download all the data from the GAP repo. Now that we have the embeddings we pass them to a multi layer perceptron i. You can check it out here https www. Next in order to feed our data to the model we ll use some scripts from the bert repo on GitHub. Use a pre trained version for the BERT transformer model to obtain contextual word embeddings for the 3 target words in each passage A B and Pronoun. So we ll make predictions on it. For coreference resolution they use the OntoNotes and Definite Pronoun Resolution datasets but not GAP. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. The other two files gap validation. tsv with 2000 rows will be used for training. This is necessary for comparison with the output of BERT Figure out the length of A B not counting spaces or special characters Initialize embeddings with zeros Initialize counts Get the BERT embeddings for the current line in the data file Iterate over the BERT tokens for the current line we skip over the first 2 tokens which don t correspond to words See if the character count until the current token matches the offset of any of the 3 target words print token print token print token Update the character count Taking the average between tokens in the span of A or B so divide the current value by the count t Work out the label of the current piece of text Put everything together in emb n_test 100 L2 regularization First dense layer Second dense layer tX layers. Dense dense_layer_sizes 0 name dense1 X tX layers. The variable names here may be a bit counter intuitive. For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. The data I m using comes from the 3 GAP files available here https github. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. I m using the GitHub repo for the BERT project https github. Tune some of the hyperparameters of the MLP model I haven t played with them at all. For each line in the data file get the words A B Pronoun. reshape 1 Print CV scores as well as score on the test data Write the prediction to file for submission. com google research datasets gap coreference. vanilla neural network which learns to classify the triples of embeddings emb_A emb_B emb_P as A B or NEITHER. com google research bert to obtain the pre trained model. We define a model with two hidden layers and one output layer in Keras. There are many things to try that could improve this and I may attempt some of these in the future 1. tsv with 454 rows and gap test. For each line we want to obtain contextual embeddings for the 3 target words A B Pronoun. The following method takes the data from a file passes it through BERT to obtain contextual embeddings for the target words then returns these embeddings in the emb DataFrame. Dropout dropout_rate seed 9 X Output layer Create model Sorting the DataFrame because reading from the json file messed with the order Concatenate features One hot encoding for labels Read development embeddigns from json file this is the output of Bert There may be a few NaN values where the offset of a target word is greater than the max_seq_length of BERT. Updates V7 In the previous version I was a little worried by the large variance of the model. We want predictions for all development rows. Look at all occurences of the target words A and B in the text instead of just the one specified by the offset. In this kernel I m trying to obtain a baseline for the following model 1. Feed this into a multi layer perceptron MLP which learns to solve the coreference resolution problem as a supervised classification task. This achieves the same mean CV score with lower variance. read_csv Input data files are available in the. Use a mix of the BERT layers instead of just the output of the last layer. 04805 by Devlin et al. Below we will use it 3 times once for each of the files gap test gap development gap validation. The hyperparameters I use below are quite different from theirs. Specifically the current MLP has only one hidden layer of size 37 down from two hidden layers of sizes 59 31 dropout rate of 0. Unfortunately I wasn t able to silence TensorFlow so it s giving a lot of information and warnings when I run this cell. downloading weights and cofiguration file for the model From the current file take the text only and write it in a file which will be passed to BERT The script extract_features. BatchNormalization name bn1 X tX layers. I only use the true labels to evaluate the predictions made by my model. ", "id": "mateiionita/taming-the-bert-a-baseline", "size": "4564", "language": "python", "html_url": "https://www.kaggle.com/code/mateiionita/taming-the-bert-a-baseline", "git_url": "https://www.kaggle.com/code/mateiionita/taming-the-bert-a-baseline", "script": "optimizers train_test_split optimizers as ko keras callbacks run_bert initializers count_length_no_special numpy cross_val_score regularizers layers parse_json models compute_offset_no_spaces count_chars_no_special callbacks as kc tensorflow KFold build_mlp_model backend sklearn.model_selection pandas constraints sklearn.metrics log_loss ", "entities": "(('tsv', 'training'), 'use') (('NaN a few where offset', 'BERT'), 'Create') (('which', 'script extract_features'), 'take') (('following method', 'emb DataFrame'), 'take') (('I', 'future'), 'be') (('I', 'them'), 'tune') (('we', 'target 3 words'), 'want') (('Next we', 'three files'), 'feed') (('which', 'A B'), 'emb_A') (('We', 'output one Keras'), 'define') (('We', 'development rows'), 'want') (('I', 'when predictions'), 'contain') (('predictions', 'that'), 'contain') (('so I', 'last layer'), 'jsonl') (('Below we', 'files gap test gap development gap validation'), 'use') (('Lee', 'same idea'), '05Ceshine') (('files gap other two test', 'gap model'), 'use') (('tsv data', 'test_stage_1'), 'contain') (('they', 'Definite Pronoun OntoNotes datasets'), 'use') (('com ceshine pytorch bert', 'public score'), 'baseline') (('they', 'current task'), 'be') (('we', 'multi layer'), 'now') (('com google research', 'gap coreference'), 'dataset') (('idea', 'paper https 1 2 openreview'), 'come') (('read_csv Input data files', 'the'), 'be') (('I', 'model'), 'v7') (('you', 'Tenney et al'), 'forum') (('Specifically current MLP', 'dropout 59 31 0'), 'have') (('current version', 'more regularization'), 'use') (('I', 'method'), 'be') (('initializing', 'validation'), 'make') (('we', 'GitHub'), 'next') (('I', 'GAP 3 files'), 'come') (('GAP three files', 'json files'), 'read') (('helper Here functions', 'target words'), 'be') (('This', 'lower variance'), 'achieve') (('Finally s', 'GAP repo'), 'let') (('when I', 'cell'), 'wasn') (('It', 'python docker image https kaggle github'), 'come') (('very so I', 'just rows'), 'be') (('you', 'output'), 'list') (('py', 'file output'), 'run') (('Print CV 1 scores', 'submission'), 'reshape') (('We', 'GAP data 3 files'), 'use') (('I', 'BERT project https github'), 'm') (('I', 'below quite theirs'), 'be') (('I', 'model'), 'use') (('I', 'following model'), 'm') (('we', 'X_development'), 'keep') (('current token', 'Second dense layers'), 'be') (('offset', 'spaces'), 'find') (('so it', 'this'), 'need') (('perceptron which', 'classification supervised task'), 'feed') "}