{"name": "for beginners go even deeper with char gram cnn ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "In the early stages you would want to preserve as much information as possible so you will want to have a 32 x 32 x 3 matrix back. Train on 143613 samples validate on 15958 samplesEpoch 1 6143613 143613 2580s 18ms step loss 0. In this notebook we are going to tackle the same toxic classification problem just like my previous notebooks but this time round we are going deeper with the use of Character level features and Convolutional Neural Network CNNs. We use an embedding size of 240. gif With the resulting matrix at hand you do a max pooling that basically down samples or in another words decrease the number of dimensions without losing the essence. Traditionally CNN is used to solve computer vision problems but there s an increased trend of using CNN not just in Kaggle competitions but also in papers written by researchers too. You could imagine that this approach introduce another problem an explosion of dimensions. But in global max pooling we perform pooling operation across several dimensions 2nd and 3rd dimension into a single dimension. Next we pass it to the Bidriectional LSTM that we are famliar with since the previous notebook. 9816Epoch 3 6143613 143613 2471s 17ms step loss 0. Each character is represented in a row 8 characters and each embedding dimension is represented in a column 5 dimensions in this starting matrix. 9829Epoch 6 6143613 143613 2961s 21ms step loss 0. Hyper parameter tunings3. By defining the dimension of the filter you can control the window of infomation you want to summarize. In our context some characters in each filter would be selected through this max pooling process based on their values. In our previous notebook we have began using Kera s helpful Tokenizer class to help us do the gritty text processing work. Sometimes that wouldn t affect the model s capability to make good judgement but most of the time it s unable to correctly classify because the misspelt words are not in the model s dictionary. Afterwhich we apply a max pooling again but this time round it s a global max pooling. Since there are sentences with varying length of characters we have to get them on a constant size. The training loss decreases steadily along with validation loss until at the 5th or 6th epoch where traces of overfitting starts to emerge. Experiment with different architecture layersThank you for your time in reading and if you like what I wrote support me by upvoting my notebook. Let s put them to a length of 500 characters for each sentence Just in case you are wondering the reason why I used 500 is because most of the number of characters in a sentence falls within 0 to 500 Finally we can start buliding our model. So it outputs a num of sentences X 120 2D matrix. As it s sliding it grabs the maximum value and put it into a smaller sized matrix. 9806Epoch 2 6143613 143613 2426s 17ms step loss 0. With the output of embedding layer we feed it into a convolution layer. Despite it s simplicity it s works quite well and it s a pretty niffy way to reduce the data size. There s something I deliberately missed out earlier filters. To translate back in the picture each of the feature maps could contain 1 high level representation of the embeddings for each character. But if you really want to have the resulting matrix to be reduced you can set the padding parameter to valid. Hence if we could go deeper by splitting the sentence into a list of characters instead of words the chances that the same characters that are present in both training and prediction set are much higher. Note that my explanation hides some technical details to facilitate understanding. What s the difference between this and the previous max pooling attempt In the previous max pooling attempt we merely down sampled a single 2nd dimension which contains the number of characters. com sbongo do pretrained embeddings give you the extra edge For Beginners Tackling Toxic Using Keras https www. Indirectly we are only concerned about the existence of 8 and not the location of it. Then we pass it to the max pooling layer that applies the max pool operation on a window of every 4 characters. There are different ways to down sample the data such as min pooling average pooling and in max pooling you simply take the maximum value of the matrix. We have talked about embedding layer in my previous notebooks so feel free to check them out. The convolution layer works by sliding a window across the input data and as it slides the window filter applies some matrix operations with the underlying data that falls in the window. From a matrix of num of sentences X 500 X 100 it becomes num of sentences X 125 X 100 which is still a 3d matrix. png Firstly as seen in the above picture we feed the data image in this case into the convolution layer. That also means that we are projecting characters on a 240 dimension vector space. As always we start off with the importing of relevant libraries and dataset Split into training and test set Store the comments as seperate variables for further processing. jpg Why do we consider the idea of using char gram features You might noticed that there are a lot of sparse misspellings due to the nature of the dataset. First we set up our input layer. You could experiment with the dropout rate and size of the dense connected layer to see it could decrease overfitting. com sbongo for beginners tackling toxic using keras A brief glance at Convolutional Neural Network CNNs CNN is basically a feed forward neural network that consists of several layers such as the convolution pooling and some densely connected layers that we are familiar with. 9829Epoch 5 6143613 143613 3023s 21ms step loss 0. Okay Let s see how we could implement CNN in our competition. Tweak CNN parameters such as number of strides different padding settings window size. With the toxic competition coming to an end in a month I wish everyone godspeed uncomment below to train in your local machine. png For simplicity sake let s imagine we have a 32 x 32 x 3 input matrix and a 5 x 5 x 3 filter if you apply the filter on the matrix with 1 stride you will end up with a 28 x 28 x 3 matrix. Finally we move on to train the model with 6 epochs and the results seems pretty decent. What does this padding means https i. Imagine that you have a list 1 4 0 8 5. Now that we have a 2D matrix it s convenient to plug it into the densely connected layer followed by a relu activation function. When we train our model using the word vectors from our training set we might be missing out some genuine words and mispellings that are not present in the training set but yet present in our prediction set. Therefore I believe it deserve a writeup and without much ado let s see how we can apply CNN to our competition at hand. You would begin the convolution process by using filters of different dimensions to slide across your initial matrix to get a lower dimension feature map. As mentioned in the Keras documentation we have to include the shape for the very first layer and Keras will automatically derive the shape for the rest of the layers. As usual we would then feed into a normal densely connected layer that outputs to a softmax function which gives the probabilities of each class. For instance the stride size which determine how often the filter will be applied narrow VS wide CNN etc. gif The sliding window that I mention earlier are actually filters that are designed to capture different distinctive features in the input data. Next we would apply a max pooling to get the maximum value in each feature map. 9831I hope this notebook serves as a good start for beginners who are interested in tackling NLP problems using the CNN angle. There are some ideas which you could use to push the performance further such as 1. Here s the meat of our notebook. If we add padding some zeros around the original input matrix we will be sure that the result output matrix dimension will be the same. Notice that we have set padding to same. 9823Epoch 4 6143613 143613 2991s 21ms step loss 0. One of the ways to tackle this problem is to use CNN as it s designed to solve high dimensional dataset like images. So for a list of 10 sentences that consists of 50 characters each using a 30 dimensional embedding will allow us to feed in a 10x50x30 matrix into our convolution layer. We use a window size of 4 remember it s 5 in the earlier picture above and 100 filters it s 6 in the earlier picture above to extract the features in our data. Due to Kaggle kernel time limit I have pasted the training output of these 6 epochs. When you do max pooling on this list you will only retain the value 8. There s a whole load of things that you could tweak with CNN. We ll pass it through a dropout layer and a densely connected layer that eventually passes to a sigmoid function. This function allows Tokenizer to create an index of the tokenized unique characters. We are going to use it again to help us split the text into characters by setting the char_level parameter to true. I have skipped some elaboration of some concepts like embeddings which I have went through in my previous notebooks so take a look at these if you are interested in learning more Do Pretrained Embeddings Give You The Extra Edge https www. png Consider this simplified image of max pooling operation above. jpg Looking at the above picture let s just focus for now on 1 sentence instead of a list. That also means we slides a window across the 240 dimensions of embeddings for each of the 500 characters and it will result in a num of sentences X 500 X 100 matrix. a 1 b 2 etcThen we get back a list of sentences with the sequence of indexes which represent each character. Again with the down sized after pooled matrix you could feed it to a densely connected layer which eventually leads to prediction. It will output a num of sentences X 500 X 240 matrix. And that is why we get an output of num of sentences X 125 X 100 matrix. How does this apply to NLP in our case Now forget about real pixels about a minute and imagine using each tokenized character as a form of pixel in our input matrix. And when you eventually collect all the result of the matrix operations you will have a condensed output in another matrix we call it a feature map. In the above example we slide a 2 X 2 filter window across our dataset in strides of 2. Just like word vectors we could also have character vectors that gives a lower dimension representation. ", "id": "sbongo/for-beginners-go-even-deeper-with-char-gram-cnn", "size": "11410", "language": "python", "html_url": "https://www.kaggle.com/code/sbongo/for-beginners-go-even-deeper-with-char-gram-cnn", "git_url": "https://www.kaggle.com/code/sbongo/for-beginners-go-even-deeper-with-char-gram-cnn", "script": "optimizers codecs keras.layers Activation keras.preprocessing.sequence keras.models train_test_split os keras numpy initializers Dropout Dense regularizers Bidirectional pad_sequences layers keras.callbacks Tokenizer Embedding csv LSTM matplotlib.pyplot ModelCheckpoint re sklearn.model_selection EarlyStopping sys load_model keras.preprocessing.text Model Input constraints Conv1D MaxPooling1D GlobalMaxPool1D GRU ", "entities": "(('how we', 'competition'), 'okay') (('why we', 'X 125 X 100 matrix'), 'be') (('approach', 'dimensions'), 'imagine') (('that', '4 characters'), 'pass') (('that', 'prediction'), 'miss') (('you', '28 28 3 matrix'), 'let') (('X 125 100 which', 'sentences'), 'from') (('possible you', '32 32 3 matrix'), 'want') (('correctly misspelt words', 'model'), 's') (('you', 'infomation'), 'control') (('143613 samples', '15958'), 'train') (('we', 'single dimension'), 'perform') (('us', 'convolution layer'), 'allow') (('we', 'convolution layer'), 'feed') (('how often filter', 'instance'), 'size') (('Finally we', 'model'), 'let') (('it', 'relu activation function'), 's') (('we', 'constant size'), 'be') (('pretrained embeddings', 'Toxic Using Keras https www'), 'give') (('you', 'only value'), 'retain') (('I', '6 epochs'), 'paste') (('we', 'previous notebook'), 'pass') (('this', 'max pooling operation'), 'consider') (('Do Pretrained Embeddings', 'Extra Edge https www'), 'skip') (('how we', 'hand'), 'believe') (('you', 'valid'), 'set') (('everyone', 'uncomment below local machine'), 'wish') (('we', 'text processing gritty work'), 'begin') (('result output matrix dimension', 'input original matrix'), 'be') (('it', 'images'), 'be') (('How this', 'input matrix'), 'apply') (('You', 'dataset'), 'consider') (('that', 'dimension lower representation'), 'have') (('I', 'deliberately earlier filters'), 's') (('where traces', '5th epoch'), 'decrease') (('it', 'sentences'), 'mean') (('So it', 'sentences'), 'output') (('who', 'CNN angle'), 'hope') (('that', 'training set'), 'go') (('again us', 'true'), 'go') (('that', 'window'), 'work') (('Indirectly we', 'it'), 'be') (('we', '2'), 'slide') (('It', 'sentences'), 'output') (('also we', 'dimension vector 240 space'), 'mean') (('Next we', 'feature map'), 'apply') (('You', 'dimension feature lower map'), 'begin') (('you', 'matrix'), 'be') (('again time it', 'max'), 'apply') (('time round we', 'Character level features'), 'go') (('it', 'overfitting'), 'experiment') (('I', 'notebook'), 'experiment') (('results', '6 epochs'), 'move') (('each', 'character'), 'contain') (('quite well it', 'data pretty niffy size'), 'simplicity') (('it', 'smaller sized matrix'), 'grab') (('always we', 'further processing'), 'start') (('densely connected that', 'eventually sigmoid function'), 'pass') (('which', 'class'), 'feed') (('which', 'eventually prediction'), 'feed') (('We', 'so them'), 'talk') (('Traditionally CNN', 'researchers'), 'use') (('we', 'it'), 'have') (('earlier actually that', 'input data'), 'gif') (('explanation', 'understanding'), 'note') (('we', 'layers'), 'have') (('that', 'essence'), 'gif') (('100 filters it', 'data'), 'use') (('Tokenizer', 'tokenized unique characters'), 'allow') (('we', 'densely connected that'), 'be') (('embedding dimension', 'starting matrix'), 'represent') (('s', 'just 1 sentence'), 'let') (('you', 'further such 1'), 'be') (('which', 'character'), 'a') (('which', 'characters'), 's') (('you', 'CNN'), 's') (('characters', 'values'), 'select') "}