{"name": "build gan assignment 5 ", "full_name": " h1 Controllable Generation h3 Goals h3 Learning Objectives h2 Getting started h4 CelebA h4 Packages and Visualization h4 Generator and Noise h4 Classifier h2 Specifying Parameters h2 Train a Classifier Optional h2 Loading the Pretrained Models h2 Training h4 Update Noise h4 Generation h2 Entanglement and Regularization ", "stargazers_count": 0, "forks_count": 0, "description": "The classifier has the same archicture as the earlier critic remember that the discriminator critic is simply a classifier used to classify real and fake. This mean will be your target_score. Controllable Generation GoalsIn this notebook you re going to implement a GAN controllability method using gradients from a classifier. Finally add this penalty to the target score. Feel free to skip this code block and if you do want to train your own classifier it is recommended that you initially go through the assignment with the provided classifier Loading the Pretrained ModelsYou will then load the pretrained generator and classifier using the following code. To fix this you can try to isolate the target feature more by holding the classes outside of the target class constant. However in case you would like to train your own classifier the code for that has been provided as well. This is because some features are entangled. The target score is the mean of the target class in the current noise. com uc id 1xn6LXNdQHia4Av31qx_TDP3r5QkC_cTe Packages and Visualization Generator and Noise Classifier Specifying ParametersBefore you begin training you need to specify a few parameters z_dim the dimension of the noise vector batch_size the number of images per forward backward pass device the device type Train a Classifier Optional You re welcome to train your own classifier with this code but you are provided with a pretrained one later in the code. This makes sense For example it may not be able to generate a face that s smiling but whose mouth is NOT slightly open. alt text https drive. Here you ll have to implement the score function the higher the better. Optional hints for get_score1. You have also been provided with the generator noise and classifier code from earlier assignments. Given the noise with its gradient already calculated through the classifier you want to return the new noise vector. This may also expose a limitation of the generator. In the code given to you here you can generate smiling faces. 2 Calculate the norm magnitude of changes per example. You do this by performing stochastic gradient ascent. Their formulas are essentially the same however instead of subtracting the weighted value stochastic gradient ascent adds it it can be calculated by new old old weight where is the gradient of old. Getting started You will start off by importing useful libraries and packages and defining a visualization function. Feel free to change the target index and control some of the other features in the list You will notice that some features are easier to detect and control than others. Optional hint for calculate_updated_noise1. If you d like to do this you ll have to download it and run it ideally using a GPU train_classifier filename Downlaod the prtrained models from Google Drive UNQ_C1 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION calculate_updated_noise UNIT TEST Check that the basic function works Check that it works for generated images First generate a bunch of images with the generator Number of gradient steps to take Number of gradient steps to skip in the visualization Feel free to change this value to any string from feature_names UNQ_C2 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_score Steps 1 Calculate the change between the original and current classifications as a tensor by indexing into the other_indices you re trying to preserve like in x features. This will be your other_class_penalty. TrainingNow you can start implementing a method for controlling your GAN Update NoiseFor training you need to write the code to update the noise to produce more of your desired feature. Resolve some of the challenges that entangled features pose to controllability. Remember the equation for gradient ascent new old old weight. The higher the score the better 2. Calculating the magnitude of the change requires you to take the norm of the difference between the classifications not the difference of the norms. In the following block of code you will run the gradient ascent with this new score function. norm https pytorch. Alternatively even if the generator can produce an image with the intended features it might require many intermediate changes to get there and may get stuck in a local minimum. If you wanted to control another feature you would need to get data that is labeled with that feature and train a classifier on that feature. Entanglement and RegularizationYou may also notice that sometimes more features than just the target feature change. This L2 regularization would apply a penalty for this difference using the L2 norm and this would just be an additional term on the loss function. org docs stable generated torch. This process may change features which the classifier was not trained to recognize since there is no way to penalize them with this method. If you wanted to reduce the amount of the feature you would perform gradient descent. Make sure to negate the value since it s a penalty 4 Take the mean of the current classifications for the target feature over all the examples. Gradient ascent is gradient descent over the negative of the value being optimized. You use stochastic gradient ascent to find the local maxima as opposed to stochastic gradient descent which finds the local minima. Set for our testing purposes please do not change Build the neural network You can run this code to train your own classifier but there is a provided pretrained one. You perform stochastic gradient ascent to try and maximize the amount of the feature you want. By training a classifier to recognize a relevant feature you can use it to change the generator s inputs z vectors to make it generate images with more or less of that feature. For every non target class take the difference between the current noise and the old noise. CelebA is a dataset of annotated celebrity images. It may fail more often at producing the target feature when compared to the original approach. The list you have here are the features labeled in CelebA which you used to train your classifier. You want to calculate the loss per image so you ll need to pass a dim argument to torch. Calculate the norm magnitude of changes per example and multiply by penalty weight Take the mean of the current classifications for the target feature UNIT TEST Feel free to change this value to any string from feature_names from earlier. Observe how controllability can change a generator s output. However in this assignment you are interested in maximize your feature using gradient ascent since many features in the dataset are not present much more often than they re present and you are trying to add a feature to the images not remove. Since they are colored not black and white the images have three channels for red green and blue RGB. You will calculate the magnitude of the change take the mean and negate it. If you d like to use this just run train_classifier filename to train and save a classifier on the label indices to that filename. 3 Multiply the mean of the example norms by the penalty weight. Learning Objectives1. If you trained your own classifier you can load that one here instead. One way you can implement this is by penalizing the differences from the original class with L2 regularization. The score is calculated by adding the target score and a penalty note that the penalty is meant to lower the score so it should have a negative value. This suggests that the model may not be able to generate an image that has the target feature without changing the other features. Target all the classes so that s how many the classifier will learn classifier_val_losses Dataloader returns the batches Calculate the gradients Update the weights Keep track of the average classifier loss Uncomment the last line to train your own classfier this line will not work in Coursera. Whether it s possible to train models to avoid changing unsupervised features is an open question. You will be started you off with a pre trained generator and classifier so that you can focus on the controllability aspects. You might notice a few things after running it 1. GenerationNow you can use the classifier along with stochastic gradient ascent to make noise that generates more of a certain feature. CelebAFor this notebook instead of the MNIST dataset you will be using CelebA http mmlab. The greater this value is the more features outside the target have changed. ", "id": "amoghjrules/build-gan-assignment-5", "size": "7041", "language": "python", "html_url": "https://www.kaggle.com/code/amoghjrules/build-gan-assignment-5", "git_url": "https://www.kaggle.com/code/amoghjrules/build-gan-assignment-5", "script": "DataLoader torchvision.datasets show_tensor_images calculate_updated_noise seaborn make_gen_block nn tqdm transforms make_classifier_block torchvision get_score make_grid matplotlib.pyplot Generator(nn.Module) tqdm.auto forward Classifier(nn.Module) torch.utils.data get_noise __init__ torch CelebA train_classifier torchvision.utils ", "entities": "(('line', 'Coursera'), 'target') (('Gradient ascent', 'value'), 'be') (('It', 'when original approach'), 'fail') (('that', 'other features'), 'suggest') (('it', 'examples'), 'make') (('it', 'negative value'), 'calculate') (('this', 'loss just additional function'), 'apply') (('You', 'it'), 'calculate') (('s', 'unsupervised features'), 'be') (('how controllability', 'generator s output'), 'observe') (('classifier', 'method'), 'change') (('features', 'others'), 'notice') (('you', 'score new function'), 'run') (('Controllable Generation you', 'classifier'), 'GoalsIn') (('discriminator critic', 'simply real'), 'remember') (('it', 'where old'), 'be') (('you', 'controllability aspects'), 'start') (('you', 'noise new vector'), 'want') (('You', 'visualization function'), 'start') (('more features', 'target'), 'be') (('you', 'gradient descent'), 'want') (('You', 'own classifier'), 'change') (('target score', 'current noise'), 'be') (('that', 'controllability'), 'resolve') (('you', 'feature'), 'perform') (('you', 'one'), 'load') (('it', 'feature'), 'input') (('Calculating', 'norms'), 'require') (('CelebA', 'celebrity annotated images'), 'be') (('you', 'x features'), 'have') (('You', 'classifier earlier assignments'), 'provide') (('it', 'there local minimum'), 'require') (('you', 'torch'), 'want') (('you', 'L2 regularization'), 'be') (('that', 'certain feature'), 'use') (('you', 'following code'), 'load') (('mouth', 'face'), 'make') (('you', 'desired feature'), 'start') (('black images', 'red green'), 'have') (('here you', 'smiling faces'), 'generate') (('TEST', 'feature_names'), 'calculate') (('You', 'gradient stochastic ascent'), 'do') (('that', 'feature'), 'want') (('you', 'filename'), 'run') (('you', 'later code'), 'com') (('You', 'it'), 'notice') (('Entanglement', 'target feature also sometimes more just change'), 'notice') (('This', 'generator'), 'expose') (('code', 'that'), 'like') (('Here you', 'score function'), 'have') (('you', 'images'), 'be') (('you', 'mmlab'), 'dataset') (('you', 'classifier'), 'be') (('you', 'target class'), 'try') (('which', 'local minima'), 'use') "}