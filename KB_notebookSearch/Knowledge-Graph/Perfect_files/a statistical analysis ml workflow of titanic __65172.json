{"name": "a statistical analysis ml workflow of titanic ", "full_name": " h1 Need to paraphrase this section h2 Feature Selection h3 Feature selection is an important part of machine learning models There are many reasons why we use feature selection h3 Before Scaling h3 After Scaling h1 Evaluating a classification model h1 AUC ROC Curve h4 Let s look at the feature importance from decision tree grid h3 Why Random Forest Pros and Cons h2 Introducing Ensemble Learning h4 Source GA h4 Resampling from original dataset to bootstrapped datasets h4 Source https uc r github io h4 Source https prachimjoshi files wordpress com h3 Why use Bagging Pros and cons h4 Random Forest VS Bagging Classifier h3 Resources h3 Resources h3 Resources h1 Resources h1 Credits h4 If you like to discuss any other projects or just have a chat about data science topics I ll be more than happy to connect with you on h1 If you have come this far Congratulations h1 If this notebook helped you in any way or you liked it please upvote and or leave a comment ", "stargazers_count": 0, "forks_count": 0, "description": "26 Negative Correlation Features Fare and Pclass 0. Instead bagging reduces overfitting since a lot of the sample training data are repeated and used to create base estimators. In order to calculate the total erorr we add up all the misclassified weights. Let s again look at a the sample of the train dataset below. The t statistics is the measure of a degree to which our groups differ standardized by the variance of our measurements. the test data new data then go through this averaged classifier combined classifier and predict the output. The models use weights that are assigned to each data point raw indicating their importance. With a lot of equally likely training data bagging is not very susceptible to overfitting with noisy data therefore reduces variance. It is an ultimate balance between type 1 error and type 2 error. Resources Confusion Matrix https www. Hypothesis testing for Titanic Formulating a well developed researched question Regarding this dataset we can formulate the null hypothesis and alternative hypothesis by asking the following questions. On the other hand Overfitting is when the model performs too well on the training data but does poorly in the validation set or test sets. Here is an image to make it clear on how bagging works Source https prachimjoshi. However the most critical part is the error rates. com dansbecker underfitting and overfitting explains this topic well. A Glimpse of the Datasets. TN TN FP 149 149 28 0. and label them with the respective list entries Rotate the tick labels and set their alignment. This breaking down process is done by asking questions about the features of the datasets. Since the stump barely captures essential specs about the dataset the model is highly biased in the beginning. we have fit the model using X_train and y_train and predicted the outcome of X_test in the variable y_pred. Here is an image to show how bootstrapped dataset works. Now the denominator of this fraction bar x mu is basically the strength of the signal. I am hoping to write about that in a different kernel. It is good to have an understanding of what going on in the background. Random Forest ClassifierI admire working with decision trees because of the potential and basics they provide towards building a more complex model like Random Forest RF. Do a statistical analysis of how some group of people was survived more than others. There are multiple ways to do feature scaling. bar x _F is the mean of female group samples. ROC Curve AUC Curve. Most of the passengers were with in the Fare range of 100. Our y_test has a total of 294 data points part of the original train set that we splitted in order to evaluate our model. This negative correlation with a magnitude of 0. There were very few females boarded on Queenstown however most of them survived. Once we train our algorithm using 2 3 of the train data we start to test our algorithms using the remaining data. On average the combined estimator is usually better than any of the single base estimator because its variance is reduced. Khan Academy has a set of videos that I think are intuative and helped me understand conceptually. One crucial details about Random Forest is that while using a forest of decision trees RF model takes random subsets of the original dataset bootstrapped and random subsets of the variables features columns. This situation is also known as having less bias but more variation and perform poorly as well. Both of them have the concept of signal noise. We withhold part of the data where we know the output result of each datapoints and we use this data to test the trained models. They are Embarked Age Survived Sex. This https support. is for us to have an overview and play around with the dataset. Let s say once the weight is distributed we run the model and find 2 incorrect predicitons. In order words it is basically the measure of signal over noise. Once we have a subgroup with only the unique type of labels we end the tree in that node. Can be used for classification and regression equally well. There is a couple of different type of variables They are. We never accept the null hypothesis just because we are doing the statistical test with sample data points. We randomly select 50 people to be in the male group and 50 people to be in the female group. The motivation is to combine several weak models to produce a powerful ensemble. This theory can also be supported by mentioning another Pclass correlation with our dependent variable Survived. This equation slightly differs depending on one sample test or two sample test 6. However we will use scipy. We learn how much say a stump has in the final classification by calculating how well it classified the samples aka calculate the total error of the weight. n_M and n_F are the sample number of observations in each group. Each number here represents certain details about our model. AdaBoost combines a lot of weak learners they are also called stump a tree with only one node and two leaves to make classifications. Resources Statquest Principles of Machine Learning AdaBoost Video Pros and cons of boosting Pros Achieves higher performance than bagging when hyper parameters tuned properly. We will have a detailed statistical analysis of Titanic data set along with Machine learning model implementation. Which is almost impossible and if we were to go that route there is no point of doing statistics in the first place. The x_label represents Sex feature while the y_label represents the of passenger survived. Missclassification is equivalent to 1 minus Accuracy. Sometimes we want to create extra features from with in the features that we have sometimes we want to remove features that are alike. 74 female passenger survived while only 19 male passenger survived. Part 1 Importing Necessary Libraries and datasets 1a. Only the categories can also be ordered or ranked. com watch v E4KCfcVwzyw explains the p value well. People who acts according to model results have a better understanding of the model. Loading librariesPython is a fantastic language with a vibrant community that produces many amazing libraries. comPlease check out this https www. With all three points above in mind how confident are we that the measured difference is real or statistically significant we can perform a t test to evaluate that. Compute the P value P value is the probability that a test statistic at least as significant as the one observed would be obtained assuming that the null hypothesis is correct. Going back to our dataset like we are saying these means above are part of the whole story. Often times a dataset contain features highly varying in magnitude and unit. When we have a model that overfits meaning less biased and more of variance we introduce some bias in exchange of having much less variance. The age distribution chart on top provides us with some more info such as what was the age range of those three unlucky females as the red color give away the unsurvived once. This may hint on the posibility that children and infants were the priority. Finally we put together the combined classifier which is AdaBoost X sign left sum_ t 1 T alpha_t h_t X right Here AdaBoost X is the classification predictions for y using predictor matrix X T is the set of weak learners alpha_t is the contribution weight for weak learner t h_t X is the prediction of weak learner t and y is binary with values 1 and 1 P. The pseducode for calculating the new sample weight is as follows. Something I have noticed that some readers might find questionable is that the lower class passengers have survived more than second class passengers. AdaBoost Classifier AdaBoost is another ensemble model and is quite different than Bagging. com docs docs BiasVariance biasvariance. Enhanced generalisation by reducing overfitting. OverviewDatasets in the real world are often messy However this dataset is almost clean. png Ideally we want to pick a sweet spot where the model performs well in training set validation set and test set. It seems about right since females and children were the priority. Therefore It is impossible for us at this point to know the population means of survival for male and females. XGBClassifier 7k. I will discuss more on that in a different kernel. Bagging ClassifierIf some of you are like me you may find Random Forest to be similar to Bagging Classifier. Plot non normalized confusion matrix Plot normalized confusion matrix run model 10x with 60 30 split intentionally leaving out 10 estimator knn param_grid param n_jobs 1 to instruct scikit learn to use all available processors. However they all seem to book under the same letter followed by different numbers. com masumrumi bagging with titanic dataset kernel if you want to find out more about bagging classifier. I will discuss more on that. set_text Middle legs 2. Let s write a simple function so that we can give cabin names based on the means. We will follow each of these steps above to do your hypothesis testing below. Here We can take the average of the Fare column to fill in the NaN value. We know our sample is selected from a broader population trainning set. where we calculate the difference between hypothesized mean and sample mean. Therefore I have decided to break this kernel down and explain each algorithm in a different kernel and add the links here. Facetgrid is a great way to visualize multiple variables and their relationships at once. Easily handles mixed data types. In this case the amount of say is 0. If we were to think about this interms of column and raw we could see that. We can reset the filters by clicking anywhere in the whilte space. These models are built to deal with the bias variance tradeoff. We then compare the outcomes to determine the performance of the algorithms. set_text Upper legs 1. 8418079096045198 Precision How often is it correct when the model predicts yes. Third We need to learn how to modify the weights so that the next stump will take the errors that the current stump made into account. We need to ask the right question that can be answered using statistical analysis. The first stump starts with uniformly distributed weight which means in the beginning every datapoint have an equal amount of weights. We are to predict that using machine learning models. We will use machine learning model Random Forest Regressor to impute missing value instead of Null value. Hypothesis Testing OutlineA hypothesis test compares the mean of a control group and experimental group and tries to find out whether the two sample means are different from each other and if they are different how significant that difference is. As we assumed it looks like an outlier with a fare of 512. name_length Creating a new feature name_length that will take the count of letters of each name title Getting the title of each name as a new feature. Now you can see that the predicted not survived and predicted survived sort of overlap with actual survived and actual not survived. 7565217391304347we have our confusion matrix. Bagging Averaging Methods In averaging methods the driving principle is to build several estimators independently and then to average their predictions. Statistical Test for Correlation Statistical tests are the scientific way to prove the validation of theories. Now these means can help us determine the unknown cabins if we compare each unknown cabin rows with the given mean s above. There is a definite positive correlation between Fare and Survived rated. Summary As we suspected female passengers have survived at a much better rate than male passengers. It seems like there are some passengers that had booked multiple cabin rooms in their name. Use more training data. Let s make an effort to fill these missing values starting with Embarked feature. Now let s see how the features are related to each other by creating some visualizations. S is the standard deviation. aspx article is pretty good as well. Print classification report for y_test Gradient Boosting Classifier from xgboost import XGBClassifier XGBClassifier XGBClassifier XGBClassifier. We use the total error to determine the amount of say a stump has in the final classification using the following formula alpha_t frac 1 2 ln left frac 1 epsilon_t epsilon_t right text where epsilon_t 1 Where epsilon_t is the misclassification rate for the current classifier epsilon_t frac text misclassifications _t text observations _t Here. Grid Search on Logistic Regression What is grid search What are the pros and cons Gridsearch is a simple concept but effective technique in Machine Learning. The code below basically splits the train data into 4 parts X_train X_test y_train y_test. We will use the sampling distribution approach to do the test. Extra Trees Classifier 7l. If the Total Error is 1 or 0 then this equation will freak out. We may also say passengers with cabin record were more likely to be taken into consideration when loading into the boat. It looks like the features have unequal amount of data entries for every column and they have many different types of variables. When the stump does an average job similar to a coin flip the ratio of getting correct and incorrect 50 50 then the total error is 0. This is where bias variance tradeoff comes in. the second column is of the statistics that the model predicted as survievd. com masumrumi decision tree with titanic dataset kernel. If you are a bit confused thats okay. Many machine learning algorithms uses euclidian distances to calculate the distance between two points it is quite a problem. We have separated dependent and independent features We have separated train and test data. Part 5 Feature Engineering Feature Engineering is exactly what its sounds like. In any case when we look at the data we seem to have an intuitive understanding of where data is leading us. Expends models predictability. However the most significant correlation with our dependent variable is the Sex variable which is the info on whether the passenger was male or female. We know we could have totally ended up with a different random sample of males and females. These are the top 10 features determined by Decision Tree helped classifing the fates of many passenger on Titanic on that night. When we use cross validation it is important to remember not to use X_train X_test y_train and y_test rather we will use X and y. only 38 passenger survived during that tragedy. You can probably agree with me more on this in the next section of visualizations where we look at the distribution of ticket fare and survived column. The feature with the lowest Gini index becomes the first stump in the AdaBoost stump chain the lower the Gini index is the better unmixed the label is therefore better split. However machines do not understand the value of categorical values for example in this dataset we have gender male or female algorithms do not accept categorical variables as input. It is important to remember that we will create new features in such ways that will not cause multicollinearity when there is a relationship among independent variables to occur. The Total Error for a stump is the sum of the weights associated with the incorrectly classified samples. Ideally we want to configure a model that performs well not only in the training data but also in the test data. When fare is approximately more than 280 dollars there is no gray shade which means either everyone passed that fare point survived or maybe there is an outlier that clouds our judgment. set_title PClass legs leg_1. Here we get 1 10 1 10 2 10 or 1 5. For now I will get rid off the ticket feature. This KDE plot is pretty self explanatory with all the labels and colors. This theory aligns with one other correlation which is the correlation between Fare and Pclass 0. When the error rate is high let s say close to 1 then the amount of say will be negative which means if the stump outputs a value as survived the included weight will turn that value into not survived. Let s write a functin to print the total percentage of the missing values. We are going to use our model to predict passenger survival status. PassengerIdIt seems like PassengerId column only works as an id in this dataset without any significant effect on the dataset. Specify the significance level Specifying a significance level is an important step of the hypothesis test. The population mean is a statistical term statistician uses to indicate the actual average of the entire group. Overview Survived vs non survied Generate a mask for the upper triangle taken from seaborn example gallery separating male and female dataframe. Cabin Embarked Port of Embarkation C Cherbourg Q Queenstown S Southampton Dichotomous Nominal variable with only two categories Sex Female Male Ordinal variables that have two or more categories just like nominal variables. This can happen for the following reasons. So now we will use a confusion matrix to compare between y_test and y_pred. Nothing more and nothing less. or the mean difference between male and female passenger in the survival rate is zero. There are many reasons why we use feature selection. You are probably wondering why two datasets Also Why have I named it train and test To explain that I am going to give you an overall picture of the supervised machine learning process. Therefore we approach this problem using sample sets. get_legend leg_1. This base model fitting is an iterative process where each stump is chained one after the other It cannot run in parallel. However don t let that fool you to thinking that similar to a decision tree bagging also overfits the model. com blog statistics and quality data analysis what is a t test and why is it like telling a kid to clean up that mess in the kitchen as reference to describe the t statistics here. These optimal parameters are also known as Hyperparameters. If you have any idea suggestions about this notebook please let me know. Therefore we say Null Hypothesis H_0 There is no difference in the survival rate between the male and female passengers. Features engineering is the simple word for doing all those. This variety makes the RF model more effective and accurate. Train Set Test SetThis is a sample of train and test dataset. Combined Feature Relations In this section we are going to discover more than two feature relations in a single graph. So I am going to introduce a few necessary libraries for now and as we go on we will keep unboxing new libraries when it seems appropriate. The way we use machine learning algorithm in a dataset is that we train our machine learning model by specifying independent variables and dependent variable. As Data Scientists we need to remember no to creating models with too many variables since it might overwhelm production engineers. 63 first class passenger survived titanic tragedy while 48 second class and only 24 third class passenger survived. The test set should be used to see how well our model performs on unseen data. For now we are going to work with a less complicated and quite popular machine learning dataset. Evaluating a classification modelThere are multiple ways to evaluate a classification model. Dealing with Missing values Missing values in train dataset. It shows that among all the females 230 survived and 70 did not survive. The correlation between Pclass and Survived is 0. Samples with higher weight have a higher influence on the total error of the next model and gets more priority. We sure can delete this point. jpg As you see in the chart above. AUC ROC Curve Using Cross validation Pros Helps reduce variance. calculated_fareSome people have travelled in groups like family or friends. Each stump is made by talking the previous stump s mistakes into account. In addition to that we want to identify whether the test is a one tailed test or two tailed test. Summary The first class passengers had the upper hand during the tragedy. Creating dummy variablesYou might be wondering what is a dummy variable Dummy variable is an important prepocessing machine learning step. Loop over data dimensions and create text annotations. predict X_test XGBClassifier_accy round accuracy_score y_pred y_test 3 print XGBClassifier_accy XGB_Classifier XGBClassifier models pd. However from this facet grid we can also understand which age range groups survived more than others or were not so luckyThis is another compelling facet grid illustrating four features relationship at once. In terms of raws the first raw indexed as Not survived means that the value in that raw are actual statistics of not survived once. Can use robust loss functions that make the model resistant to outliers. We want to see how the left vertical bar changes when we filter out unique values of certain features. Determine the test statistic test statistic can be used to assess the truth of the null hypothesis. MinMaxScaler Scales the data using the max and min values so that it fits between 0 and 1. io After running a learning algorithm on each one of the bootstrapped datasets all models are combined by taking their average. Let s do the confusion matrix. The null hypothesis H_0 and Alternating hypothesis H_1 The null hypothesis H_0 is something that is assumed to be true. Let s stop for a second here and think through how we can take advantage of some of the other features here. the first column is of data points that the machine predicted as not survived. If you have come this far Congratulations If this notebook helped you in any way or you liked it please upvote and or leave a comment Import necessary modules config InlineBackend. If the model performs well we dump our test data in the algorithms to predict and submit the competition. After all it is a matrix and we have some terminologies to call these statistics more specifically. Predict Use machine learning classification models to predict the chances of passengers survival. This is our total error. stats to find the t statistics. n is the sample size. We want our models to be at the bottom of that U shape where the error rate is the least. Pclass and Survived It looks like. is_alone ticketI have yet to figureout how to best manage ticket feature. Gaussian Process Classifier 7m. https cdncontribute. Underfitting is when the model fails to capture important aspects of the data and therefore introduces more bias and performs poorly. Which means each data point will have 1 10 weight. Visualization and Feature Relations Before we dive into finding relations between independent variables and our dependent variable survivor let us create some assumptions about how the relations may turn out among features. com masumrumi logistic regression with titanic dataset. The Hyperparameters are model parameters that are set before fitting the model and determine the behavior of the model. True Positive TP 87 True Negative TN 149 False Positive FP 28 False Negative FN 30From these four terminologies we can compute many other rates that are used to evaluate a binary classifier. Here Age and Calculated_fare is much higher in magnitude compared to others machine learning features. Squaring the correlation feature not only gives on positive correlations but also amplifies the relationships. This kernel is for all aspiring data scientists to learn from and to review their knowledge. Gatter more data and gather better quality data. The test set does not provide passengers survival status. If you are reading this on github I recommend you read this on kaggle. The more samples we take and the more sample means will be added and the closer the normal distribution will reach towards population mean. However in order to calculate between two sample population mean or in our case we will use the follow equation. DataFrame Model Support Vector Machines KNN Logistic Regression Random Forest Naive Bayes Decision Tree Gradient Boosting Classifier Voting Classifier XGB Classifier ExtraTrees Classifier Bagging Classifier Score svc_accy knn_accy logreg_accy random_accy gaussian_accy dectree_accy gradient_accy voting_accy XGBClassifier_accy extraTree_accy bagging_accy. While working with a dataset having meaningful value for example male or female instead of 0 s and 1 s is more intuitive for us. Specify a Significance level and Confidence Interval The significance level alpha is the probability of rejecting a null hypothesis when it is true. I am going to use this post http blog. Let s see how the Fare is distributed among all Pclass and Embarked feature valuesHere in both training set and test set the average fare closest to 80 are in the C Embarked values where pclass is 1. Loading Datasets After loading the necessary modules we need to import the datasets. Misclassification Rate Misclassification Rate is the measure of how often the model is wrong Misclassification Rate and Accuracy are opposite of each other. After each iteration weights gets re calculated in order to take the errors misclassifications from the last stump into consideration. As we compare we use confusion matrix to determine different aspects of model performance. This can create problems as many machine learning models will get confused thinking Age and Calculated_fare have higher weight than other features. bar x is the sample mean. Gaussian Naive Bayes using the best found hyper paremeters to get the score. or the mean difference in the survival rate between male and female is not zero. Now let s go through the features and describe a little. Many of the business problems usually come with a tremendous amount of messy data. An Alternative hypothesis H_A is a claim and the opposite of the null hypothesis. Simple models are easier to interpret. RobustScaler Scales the data similary to Standard Scaler but makes use of the median and scales using the interquertile range so as to aviod issues with large outliers. Lets analyze and see what we have here. Missing values in test set. To Udemy Course Deployment of Machine Learning. Now let s look at the value counts of the cabin features and see how it looks. rumi Part 2 Overview and Cleaning the Data 2a. Bagging Classifier Bagging Classifier Bootstrap Aggregating is the ensemble method that involves manipulating the training set by resampling and running algorithms on it. Inversely we want to decrease the sample weight of the correctly classified samples hinting the next stump to put less emphasize on those. H0 male mean is greater or equal to female meanH1 male mean is less than female mean. Print them out Creating a new colomn with a separating our independent and dependent variable Feature Scaling import LogisticRegression model in python. While among male passengers 110 survived and 480 did not survive. 7435897435897436 False Positive Rate How often the model predicts yes survived when it s actually no not survived FP FP TN 28 28 149 0. Since we do not know the standard deviation sigma and n is small we will use the t distribution. We see that in both train and test dataset have missing values. Before ScalingAfter ScalingYou can see how the features have transformed above. Confusion Matrix tells us how many our model predicted correctly and incorrectly in terms of binary multiple outcome classes by comparing actual and predicted cases. Our model may be overfitting or underfitting. Correlation Matrix and Heatmap Correlations Sex is the most important correlated feature with Survived dependent variable feature followed by Pclass. Using RandomizedSearchCVRandomized search is a close cousin of grid search. family_size Creating a new feature called family_size. TP TN total 87 149 294. The smaller the P value the stronger the evidence against the null hypothesis. The first stump will uniformly distribute an weight amoung all the datapoints. This https stattrek. Some stumps get more say in the final classifications than others. However for the sake of learning and practicing we will try something else. Let s do itLet s take a look at the histogram of the age column. These penalties require specific alpha the strength of the regularization technique to set beforehand. png Train info I have gathered a small summary from the statistical overview above. 6 Sex and Survived 0. Any feedback about further improvements would be genuinely appreciated. We can take the average of the values where Pclass is 3 Sex is male and Embarked is S Age Feature We know that the feature Age is the one with most missing values let s see it in terms of percentage. We want to increase the sample weight of the misclassified samples hinting the next stump to put more emphasize on those. Misclassification Rate is also known as Error Rate. This can further help our cause. Cannot be parallelized like bagging bad scalability when huge amounts of data. For example if we say our significance level is 5 then our confidence interval would be 1 0. with this in mind we can update modify our null and alternative hypothesis. It is going against the status quo. This count plot shows the actual distribution of male and female passengers that survived and did not survive. There are two types of ensemple learnings. Alternative Hypothesis H_A There is a difference in the survival rate between the male and female passengers. Simplify the model by changing the hyperparameters. In a supervised machine learning process we are giving machine computer models specific inputs or data text number image audio to learn from aka we are training the machine to learn certain aspects based on the data and the output. However this process makes this kernel too lengthy to sit and read at one go. A hypothesis test usually consists of multiple parts 1. It is essential to understand those model parameters are different from models outcomes for example coefficients or model evaluation metrics such as accuracy score or mean squared error are model outcomes and different than hyperparameters. So let s fill in the missing values as C Cabin Feature Approximately 77 of Cabin feature is missing in the training data and 78 missing on the test data. Here S is standard deviation which tells us how much variation is there in the data. age_groupWe can create a new feature by grouping the Age columnNeed to paraphrase this sectionFeature SelectionFeature selection is an important part of machine learning models. For some machine learning models it is not a problem. A small amount of error is added to prevent this from happening. set_text Lower adding saved target variable with train. Let s apply cabin_estimator function in each unknown cabins cabin with null values. This can also be explained by saying that first class passenger had a better chance of surviving than the second or the third and so on. 33 So Let s analyze these correlations a bit. Let s get a better perspective of the dataset through this visualization. We can also think about it epsilon_t frac text misclassifications _t text observations _t Since the weight is uniformly distributed all add up to 1 among all data points the total error will always be between 0 perfect stump and 1 horrible stump. We do not want to use any part of the test data in any way to modify our algorithms Which are the reasons why we clean our test data and train data separately. In order to feed data in a machine learning model we ageAs I promised before we are going to use Random forest regressor in this section to predict the missing age values. 19 True Positive Rate Recall Sensitivity How often the model predicts yes survived when it s actually yes survived TP TP FN 87 87 30 0. For example To find the age population mean of Bulgaria we will have to account for every single person s age and take their age. D_t i Current Sample weight. For example if we click on upper and Female tab we would see that green color dominates the bar with a ratio of 91 3 survived and non survived female passengers a 97 survival rate for females. We were given part of the data to train our machine learning models and the other part of the data was held back for testing. Embarked feature It looks like there are only two null values 0. For now we have decided to make our significance level alpha 0. Resources AdaBoost Tutorial Chris McCormick Explaining AdaBoost by Robert Schapire One of the original author of AdaBoost 7i. Null Hypothesis H0 male mean is greater or equal to female mean. We may have alphanumerical or and text features. mu is the hypothesized mean. Why use Bagging Pros and cons Bagging works best with strong and complex models for example fully developed decision trees. However let s dig a little deeper. This train data set has 891 raw and 9 columns. When we say unseen data we mean that the algorithm or machine learning models have no relation to the test data. I will explain more as we keep reading. com max 400 1 hFJ LI7IXcWpxSLtaC0dfg. Pclass A proxy for socio economic status SES 1 Upper 2 Middle 3 Lower Numeric Discrete Passenger ID Unique identifing for each passenger SibSp Parch Survived Our outcome or dependent variable 0 1 Continous Age Fare Text Variable Ticket Ticket number for passenger. Easier to implement by software developers model production. The spike in the plot under 100 dollar represents that a lot of passengers who bought the ticket within that range did not survive. Fare and Survived This plot shows something impressive. Reduced risk of data errors during model use Data redundancy Part 6 Pre Modeling Tasks 6a. Introducing regularization models. It seems like there is a significance with the letters rather than the numbers. Gender and Survived This bar plot above shows the distribution of female and male survived. That sweet spot is also known as Optimum Model Complexity OMC. Passenger who traveled in big groups with parents children had less survival rate than other passengers. 22 in the Embarked feature we can replace these with the mode value S. As we ask questions we are breaking down the dataset into more subsets. Accuracy Accuracy is the measure of how often the model is correct. I am super excited to share my first kernel with the Kaggle community. When we set out to experiment we form the null hypothesis by saying that there is no difference between the means of the control group and the experimental group. setting the number of runs r and or loops n leg_1 ax1. Is there a substantial difference in the survival rate between the male and female passengers The Null Hypothesis and The Alternative Hypothesis We can formulate our hypothesis by asking questions differently. I will incorporate new concepts of data science as I comprehend them with each update. One particular tactic for this task is regularization models Ridge Lasso Elastic Net. About This Dataset The data has split into two groups training set train. Resampling from original dataset to bootstrapped datasetsSource https uc r. Source Diogo Medium Let s dive into each one of the nitty gritty stuff about AdaBoost First we determine the best feature to split the dataset using Gini index basics from decision tree. Once we get the outcomes we compare it with y_test By comparing the outcome of the model with y_test we can determine whether our algorithms are performing well or not. We have two choices we can either get rid of the whole feature or we can brainstorm a little and find an appropriate way to put them in use. com hypothesis test hypothesis testing. Pretty much every male that boarded on Queenstown Q did not survive. Feature Scaling Feature scaling is an important concept of machine learning models. This is where Central limit theory comes from. Name Name of the passenger. keeping in mind that. We have found some moderately strong relationships between different features. Our dependent variable or target variable is something that we are trying to find and our independent variable is the features we use to find the dependent variable. I think this video https www. The group can be any gathering of multiple numbers such as animal human plants money stocks. S 2 is the sample variance. Create green Bars Create orange Bars Custom x axis Show graphic Kernel Density Plot Kernel Density Plot Kernel Density Plot Placing 0 for female and 1 for male in the Sex column. TP TP FP 87 87 28 0. Using this method the RF model creates 100 s 1000 s the amount can be menually determined of a wide variety of decision trees. If you like to discuss any other projects or just have a chat about data science topics I ll be more than happy to connect with you on LinkedIn Github masumrumi. At this point I think we are quite confident that these outliers should be deleted. Tableau Visualization of the Data I have incorporated a tableau visualization below of the training data. is done without making any changes including Null values to any features of the dataset. Voting Classifier Part 8 Submit test predictions Resources Statistics Types of Standard Deviation What Is a t test And Why Is It Like Telling a Kid to Clean Up that Mess in the Kitchen What Are T Values and P Values in Statistics What is p value How we decide on our confidence level. Calculating the t statistics t frac bar x mu frac S sqrt n Here. Machine Learning is simply Machine and Learning. X_train and y_train first used to train the algorithm. Therefore we need to do feature scaling to get a better result. figure_format retina This is preferable for retina display. While passenger who traveled in small groups with sibilings spouses had better changes of survivint than other passengers. We will go more in depth of this topic later on. Often times Categorical variables are an important features which can be the difference between a good model and a great model. We can use multiple filters to see if there are any correlations among them. However the downside is that this leads to an increase in bias. Assumptions Gender More female survived than male Pclass Higher socio economic status passenger survived more than others. Now that we know what we want in terms of under fitting and over fitting let s talk about how to combat them. The following equation help us to do this calculation. Let s do a quick review Bagging classifier uses a process called bootstrapped dataset to create multiple datasets from one original dataset and runs algorithm on each one of them. let s take 50 random sample of male and female from our train data. Age and Survived There is nothing out of the ordinary about this plot except the very left part of the distribution. We may have categorical features. Let s see what are those two null values We may be able to solve these two missing values by looking at other independent variables of the two raws. So We still haven t done any effective work to replace the null values. This kernel https www. I will elaborate on this in a future update. FP FN Total 28 30 294 0. More than 60 of the passengers died boarded on Southampton. This relationship reveals that the passenger who paid more money for their ticket were more likely to survive. Situation like this calls for a statistical approach. Writing pythonic code Six steps to more professional data science code Creating a Good Analytics Report Code Smell Python style guides The Best of the Best Practices BOBP Guide for Python PEP 20 The Zen of Python The Hitchiker s Guide to Python Python Best Practice Patterns Pythonic Sensibilities Why Scikit Learn Introduction to Scikit Learn Six reasons why I recommend scikit learn Why you should learn Scikit learn A Deep Dive Into Sklearn Pipelines Sklearn pipelines tutorial Managing Machine Learning workflows with Sklearn pipelines A simple example of pipeline in Machine Learning using SKlearn Credits To Brandon Foltz for his youtube channel and for being an amazing teacher. So why do we still have to split our training data If you are curious about that I have the answer. In an alternative theory the observations show a real effect combined with a component of chance variation. com masumrumi a stats analysis and ml workflow of house pricing edit run 9585160. However it is essential to understand what our end goal is. Let s do some statistics to see how statistically significant this correlation is. I will try my best to illustrate most of the feature relations. D_ t 1 i D_t i e alpha_t y_i h_t x_i Here D_ t 1 i New Sample Weight. Secondly we need to determine how much say a stump will have in the final classification and how we can calculate that. Fare Passenger with higher fare survived more that other passengers. It seems like Fare column kept a record of the total fare rather than the fare of individual passenger therefore calculated fare will be much handy in this situation. Let s take a look at sample datasets. When we perform a t test we are usually trying to find out an evidence of significant difference between population mean with hypothesized mean 1 sample t test or in our case difference between two population means 2 sample t test. In our test dataset we do not have a dependent variable feature. This can be quite correlated with Pclass. We will discuss more in depth about those in another lesson. This relationship can be explained by saying that first class passenger 1 paid more for fare then second class passenger 2 similarly second class passenger paid more than the third class passenger 3. More risk of overfitting compared to bagging. estimator knn param_grid param n_jobs 1 to instruct scikit learn to use all available processors. NOTE If you want to learn more about Advanced Regression models please check out this kernel. So check for them and please leave a comment if you have any suggestions to make this kernel better Going back to the topics of this kernel I will do more in depth visualizations to explain the data and the machine learning classifiers will be used to predict passenger survival status. For this competition when we train the machine learning algorithms we use part of the training set usually two thirds of the train data. Let s point out the core concepts. Therefore we can group these cabins according to the letter of the cabin name. Part 7 Modeling the Data In the previous versions of this kernel I thought about explaining each model before applying it. We then run each test data point through all of these 100 s to 1000 s of decision trees or the RF model and take a vote on the output. Compare P value with alpha It looks like the p value is very small compared to our significance level alpha of 0. This part of the kernel is a working progress. Let s combine train and test data first and for now will assign all the null values as N All the cabin names start with an English alphabet following by multiple digits. True Negative TN values that model predicted as no not survived and is actually no not survived False Positive or Type I error values that model predicted as yes survived but actually no not survived False Negative or Type II error values that model predicted as no not survived but actually yes survived For this dataset whenever the model is predicting something as yes it means the model is predicting that the passenger survived and for cases when the model predicting no it means the passenger did not survive. 54 which points towards some undeniable insights. We extract those data from many sources. For example in terms of this dataset our model is a binary one and we are trying to classify whether the passenger survived or not survived. This bar plot shows that 74 female passenger survived while only 19 male passenger survived. Categorical Nominal variables that have two or more categories but which do not have an intrinsic order. RF is an ensemble method combination of many decision trees which is where the forest part comes in. Let s see how we can rewrite this. GridSearch finds the optimal value of alpha among a range of values provided by us and then we go on and use that optimal value to fit the model and get sweet results. Feature ImportanceWhy Random Forest Pros and Cons Introducing Ensemble LearningIn statistics and machine learning ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. com watch v 5NcMFlrnYp8 list PLIeGtxpvyG LrjxQ60pxZaimkaKKs0zGF video is also quite helpful understanding these topics. Describe the result and compare the p value with the significance value alpha If p alpha we say that we fail to reject the null hypothesis. The final prediction is typically constructed by a weighted vote where weights for each base model depends on their training errors or misclassification rates. The idea of using sample set is that if we take multiple samples of the same population and take the mean of them and put them in a distribution eventually the distribution start to look more like a normal distribution. Shorter training times. 15819209039548024 True Negative Rate Specificity How often the model predicts no not survived when it s actually no not survived True Negative Rate is equivalent to 1 minus False Positive Rate. Most passengers seem to be boarded on Southampton S. In a null hypothesis the observations are the result of pure chance. Let us describe the previous sentence a bit more for clarification. Gradient Boosting Classifier Resources Gradient Descent StatQuest Gradient Boost Regression Main Ideas StatQuest Gradient Boost Regression Calculation StatQuest Gradient Boost Classification Main Ideas StatQuest Gradient Boost Classification Calculation StatQuest Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting GBM in Python 7j. This is because many of them travelled with family. So any suggestion would be truly appreciated. Let s see what they are True Positive TP values that the model predicted as yes survived and is actually yes survived. K Nearest Neighbor classifier KNN Manually find the best possible k value for KNN Grid search on KNN classifier Using best estimator from grid search using KNN. So our confidence interval or non rejection region would be 1 alpha 1 0. bar x _M is the mean of our male group sample measurements. Determine the test statistics This will be a two tailed test since the difference between male and female passenger in the survival rate could be higher or lower than 0. 8027We can also calculate accuracy score using scikit learn. If you would like to check out some of my other tableau charts please click here. As I go on in this journey and learn new topics I will incorporate them with each new updates. The p value is known to be unintuitive and even many professors are known to explain it wrong. fare_groupFare group was calculated based on calculated_fare. To Kaggle community for inspiring me over and over again with all the resources I need. The idea is to unmix the labels by asking fewer questions necessary. Lets find out a bit more about the train and test dataset. Even though this sentence is grammatically wrong it is logically right. StandardScaler Scales the data so that it has mean 0 and variance of 1. Using the best parameters from the grid search. Splitting the training data There are multiple ways of splitting data. When the stump does a reasonably good job and the total error is minimal then the amount of say Alpha is relatively large and the alpha value is positive. Depending on the standard deviation we either use t statistics or z statistics. For example We may say passengers with cabin record had a higher socio economic status then others. The higher the value of alpha the more penalty is being added. In fact for those of you don t know what overfitting and underfitting is Let s find out. Formulate a well developed research problem or question The hypothesis test usually starts with a concrete and well developed researched problem. com en us minitab 18 help and how to statistics basic statistics supporting topics basics null and alternative hypotheses article explains it pretty well. However there is a fundamental difference between these two which is Random Forests ability to pick subsets of features in each node. To GA where I started my data science journey. As the model gets complex bias decreases variance increases. Computing T statistics and P value Let s take a random sample and see the difference. This considerable amount is our Significant level. This facet grid unveils a couple of interesting insights. Therefore It would be unwise to replace the missing values with median mean or mode. Age Younger passenger survived more than other passengers. The only difference is that we replace our hypothesis mean with another sample mean and the two sample sizes repalce one sample size. To illustrate what we have talked about so far let s look at the following visualization. Now how can we determine that machine is actually learning what we are try to teach That is where the test set comes to play. If you like to review logistic regression please click here https www. It is the status quo. alpha_t Amount of Say alpha value this is the coefficient that gets updated in each iteration and y_i h_t x_i place holder for 1 if stump correctly classified 1 if misclassified. How about we give it a little more character. But how do we make sure that our model is performing well. Confusion MatrixConfusion matrix a table that describes the performance of a classification model. In other words we are comfortable confident with rejecting the null hypothesis a significant amount of times even though it is true. Let s look at the feature importance from decision tree grid. Gaussian Naive Bayes Support Vector Machines SVM Decision Tree ClassifierDecision tree works by breaking down the dataset into small subsets. If you want to learn more about regression models try this kernel https www. We will keep the age column unchanged for now and work on that in the feature engineering section. com This kernel will always be a work in progress. More than 60 of the passengers lived boarded on Cherbourg C. Fare Feature If you have paid attention so far you know that there is only one missing value in the fare column. If you would like to get a detailed understanding of Decision tree classifier please take a look at this https www. The age variable seems to be promising for determining survival rate. 55 Pclass and Survived 0. alpha_t Amount of Say epsilon_t Total errorWe can draw a graph to determine the amount of say using the value of total error 0 to 1 Source Chris McCormick The blue line tells us the amount of say for Total Error Error rate between 0 and 1. Is there a significant difference in the mean sex between the passenger who survived and passenger who did not survive. I am showing a sample of both before and after so that you can see how scaling changes the dataset. How to combat over fitting Simplify the model by using less parameters. and the Survived indexed raw are values that actually survived. We can use the average of the fare column We can use pythons groupby function to get the mean fare of each cabin letter. However we will keep it for now. From the chart in section 3a we have a intuation that female passengers had better prority than males during the tragedy. t frac bar x _M bar x _F sqrt s 2 frac 1 n_M frac 1 n_F This equation may seem too complex however the idea behind these two are similar. Onc thing we can do is try to set up the Null and Alternative Hypothesis in such way that when we do our t test we can choose to do one tailed test. Source GAResource Ensemble methods bagging boosting and stacking 7g. the numerator of this fraction S sqrt n calculates the amount of variation or noise of the data set. In addition to that Significance level is one minus our Confidence interval. Compute the T Statistics Z Statistics Computing the t statistics follows a simple equation. New Sample Weight Sample Weight e alpha_t Here the alpha_t AmountOfSay can be positive or negative depending whether the sample was correctly classified or misclassified by the current stump. org wp content uploads fittings. Both passengers paid a fare of 80 are of Pclass 1 and female Sex. Part 4 Statistical Overview title https cdn images 1. Separating dependent and independent variables Before we apply any machine learning models It is important to separate dependent and independent variables. To specify them we need to separate them from each other and the code below does just that. However for many other ones its quite a problem. However as the chain of stumps continues and at the end of the process AdaBoost becomes a strong tree and reduces both bias and variance. printing confision matrix Compute confusion matrix Only use the labels that appear in the data We want to show all ticks. Let s see what they are. Our observation sample is statistically significant. Positive Correlation Features Fare and Survived 0. Let s apply some of these methods and see how we are doing with our predictions. com watch v 8Oog7TXHvFY Under fitting Over fitting So we have our first model and its score. We will take a different approach since 20 data in the Age column is missing in both train and test dataset. Do an exploratory data analysis EDA of titanic with visualizations and storytelling. If the mean difference is higher then the signal is stronger. Alternative Hypothesis H1 male mean is less than female mean. I have used and modified some of the code from this course to help making the learning process intuitive. In addition to that this https www. For example lets say we start a stump with 10 datasets. Please check back again for future updates. It is true since there were a lot more third class passengers than first and second. The color illustrates passengers survival status green represents survived gray represents not survived The column represents Sex left being male right stands for female The row represents Embarked from top to bottom S C Q Now that I have steered out the apparent let s see if we can get some insights that are not so obvious as we look at the data. The grid above clearly demonstrates the three outliers with Fare of over 500. Cons Difficult and time consuming to properly tune hyper parameters. So according to the explanation above the t value or t statistics is basically measures the strength of the signal the difference to the amount of noise the variation in the data and that is how we calculate the t value in one sample t test. For now we will use Standard Scaler to feature scale our dataset. fit X y y_pred XGBClassifier. Also the following chart gives us a mental picture of where we want our models to be. It doesn t always provide the best result but its fast. Boosting Methods The other family of ensemble methods are boosting methods where base estimators are built sequentially and one tries to reduce the bias of the combined estimator. Therefore our null hypothesis is ruled out and our alternative hypothesis is valid which is There is a significant difference in the survival rate between the male and female passengers. The word GridSearch stands for the fact that we are searching for optimal parameter parameters over a grid. this can be a good exercise for beginners to try to write simple functions like this. then X_test is used in that trained algorithms to predict outcomes. Now we have to understand that those two means are not the population mean bar mu. Let s determine the value of all these terminologies above. 63 first class passengers survived while only 24 lower class passenger survived. For example when we choose to use linear regression we may decide to add a penalty to the loss function such as Ridge or Lasso. According to the samples our male samples bar x _m and female samples bar x _f mean measured difference is 0. We may have missing values in our features. I am not a big fan of importing everything at once for the newcomers. 55 statistically this is called the point estimate of the male population mean and female population mean. Once that is done we will separate our train and test to continue towards machine learning modeling. However when we do statistical tests we get a scientific or mathematical perspective of how significant these results are. According to this https support. Here our dependent variable or target variable is Survived. com en us minitab 18 help and how to statistics basic statistics supporting topics basics null and alternative hypotheses article one tailed tests are more powerful than two tailed test. Kernel Goals There are three primary goals of this kernel. csv The training set includes our target variable dependent variable passenger survival status also known as the ground truth from the Titanic tragedy along with other independent features like gender class fare and Pclass. ", "id": "masumrumi/a-statistical-analysis-ml-workflow-of-titanic", "size": "65172", "language": "python", "html_url": "https://www.kaggle.com/code/masumrumi/a-statistical-analysis-ml-workflow-of-titanic", "git_url": "https://www.kaggle.com/code/masumrumi/a-statistical-analysis-ml-workflow-of-titanic", "script": "sklearn.utils.multiclass classification_report fare_group completing_age train_test_split pyplot pyplot as plt percent_value_counts confusion_matrix family_group name_length_group precision_recall_curve RandomForestRegressor numpy accuracy_score cross_val_score seaborn sklearn.svm xgboost XGBClassifier recall_score ExtraTreesClassifier SVC name_converted BaggingClassifier mean_absolute_error GaussianNB sklearn.neighbors sklearn.naive_bayes missing_percentage plot_confusion_matrix sklearn.tree unique_labels GradientBoostingClassifier sklearn.linear_model age_group_fun auc cabin_estimator StratifiedKFold GaussianProcessClassifier StratifiedShuffleSplit matplotlib.pyplot DecisionTreeClassifier precision_score sklearn.model_selection pandas roc_curve RandomForestClassifier LogisticRegression balanced_accuracy_score matplotlib KNeighborsClassifier RandomizedSearchCV VotingClassifier AdaBoostClassifier GridSearchCV sklearn.gaussian_process sklearn.metrics sklearn.ensemble StandardScaler sklearn.preprocessing ", "entities": "(('1 Where epsilon_t', '_ t'), 'use') (('we', 'age'), 'have') (('We', 'Null instead value'), 'use') (('that', 'many amazing libraries'), 'be') (('optimal parameters', 'also Hyperparameters'), 'know') (('facet grid', 'couple interesting insights'), 'unveil') (('breaking process', 'datasets'), 'do') (('you', 'kernel'), 'NOTE') (('More than 60', 'Cherbourg C.'), 'live') (('Chris blue line', '0'), 'epsilon_t') (('com', 'it'), 'explain') (('feedback', 'further improvements'), 'appreciate') (('why we', 'test data'), 'want') (('rather we', 'X'), 'be') (('it', '0'), 'Scales') (('We', 'cabin letter'), 'use') (('even many professors', 'it'), 'know') (('we', 'follow equation'), 'in') (('how well it', 'weight'), 'learn') (('I', 'ticket feature'), 'rid') (('we', 'one tailed test'), 'be') (('s', 'age column'), 'let') (('statistical statistician', 'entire group'), 'mean') (('stump', 'correctly 1'), 'Amount') (('model', 'cases'), 'tell') (('I', 'training data'), 'visualization') (('theory', 'dependent variable Survived'), 'support') (('Now denominator', 'basically signal'), 'be') (('hypothesis Alternative H_A', 'null hypothesis'), 'be') (('models', 'mental picture'), 'give') (('passenger', 'no'), 'value') (('We', 'back testing'), 'give') (('AdaBoost First we', 'decision tree'), 'let') (('s', 'train data'), 'let') (('which', 'node'), 'be') (('how we', 'predictions'), 'let') (('com', 'one tailed more two tailed test'), 'be') (('I', 'different kernel'), 'discuss') (('us', 'dataset'), 'be') (('many why we', 'feature selection'), 'be') (('we', 'use'), 'have') (('you', 'kaggle'), 'read') (('480', 'male passengers'), 'survive') (('model parameters', 'model mean squared hyperparameters'), 'be') (('it', 'production engineers'), 'need') (('only 38 passenger', 'tragedy'), 'survive') (('We', 'Machine model learning implementation'), 'have') (('red color', 'away unsurvived'), 'provide') (('I', 'over over resources'), 'to') (('Sex Female Male Ordinal that', 'just nominal variables'), 'Embarked') (('we', 'input'), 'understand') (('alpha relatively value', 'reasonably good job'), 'do') (('who', 'ticket'), 'reveal') (('Fare Passenger', 'more other passengers'), 'survive') (('learning process', 'course'), 'use') (('plot', 'something'), 'Fare') (('which', 'intrinsic order'), 'variable') (('We', 'algorithms'), 'compare') (('P s', 'difference'), 'let') (('sample', 'hypothesized mean'), 'calculate') (('Correlation Matrix', 'Pclass'), 'be') (('this', 'bias'), 'be') (('sample two sizes', 'sample one size'), 'be') (('I', 'answer'), 'have') (('we', 'mode value'), '22') (('Train Set Test SetThis', 'train'), 'be') (('It', 'ultimate type 1 error'), 'be') (('small amount', 'this'), 'add') (('model', 'survievd'), 'be') (('Misclassification Rate', 'Error also Rate'), 'know') (('observations', 'pure chance'), 'be') (('we', 'train data'), 'use') (('we', 'more subsets'), 'break') (('Statistical Test', 'theories'), 'be') (('class Summary first passengers', 'tragedy'), 'have') (('data aspiring scientists', 'knowledge'), 'be') (('I', 'feature relations'), 'try') (('we', 't distribution'), 'use') (('so far s', 'following visualization'), 'let') (('they', 'Random Forest RF'), 'admire') (('statistically we', 'that'), 'with') (('situation', 'also less bias'), 'know') (('So now we', 'y_test'), 'use') (('first stump', 'datapoints'), 'distribute') (('Therefore we', 'cabin name'), 'group') (('Therefore we', 'sample sets'), 'approach') (('calculated_fareSome people', 'family'), 'travel') (('that', 'also model'), 'let') (('grid', 'over 500'), 'demonstrate') (('we', 'machine learning modeling'), 'separate') (('RF model', 'columns'), 'be') (('we', 'much less variance'), 'introduce') (('that', 'statistical analysis'), 'need') (('how we', 'that'), 'need') (('they', 'two classifications'), 'combine') (('Most passengers', 'Southampton S.'), 'seem') (('Null Alternative We', 'questions'), 'be') (('Evaluating', 'classification modelThere multiple model'), 'be') (('model that', 'model'), 'be') (('also passengers', 'when boat'), 'say') (('class lower passengers', 'class more second passengers'), 'be') (('Facetgrid', 'great multiple variables'), 'be') (('1000 amount', 'decision trees'), 'create') (('variance', 'base single estimator'), 'be') (('measured difference', 'samples'), 'accord') (('me', 'videos'), 'have') (('8027We', 'scikit learn'), 'calculate') (('that', 'features'), 'want') (('We', 'topic'), 'go') (('how group', 'more others'), 'do') (('driving principle', 'independently then predictions'), 'method') (('Features engineering', 'simple those'), 'be') (('One particular tactic', 'task'), 'be') (('The smaller P', 'null hypothesis'), 'value') (('penalties', 'regularization technique'), 'require') (('is_alone ticketI', 'ticket yet how best feature'), 'have') (('when it', 'FP FP actually TN'), '7435897435897436') (('t', 'always best result'), 'doesn') (('too however idea', 'two'), 'sqrt') (('which', 'undeniable insights'), '54') (('More than 60', 'Southampton'), 'die') (('two tailed difference', '0'), 'be') (('s', 'visualization'), 'let') (('datapoint', 'weights'), 'start') (('s', 'missing values'), 'let') (('you', 'https www'), 'take') (('Summary we', 'male passengers'), 'survive') (('It', 'letters'), 'seem') (('AdaBoost Classifier AdaBoost', 'ensemble quite Bagging'), 'be') (('we', 'statistics'), 'be') (('s', 'decision tree grid'), 'let') (('Now s', 'little'), 'let') (('Samples', 'more priority'), 'have') (('We', 'alphanumerical features'), 'have') (('still t', 'null values'), 'haven') (('Both', 'signal noise'), 'have') (('so luckyThis', 'facet features compelling four relationship'), 'understand') (('is', 'dummy variablesYou'), 'wonder') (('Random Forest', 'Bagging Classifier'), 'ClassifierIf') (('that', 'test also data'), 'want') (('com watch E4KCfcVwzyw', 'p value'), 'v') (('us', 'bit more clarification'), 'let') (('sweet spot', 'Optimum Model Complexity also OMC'), 'know') (('respective list', 'alignment'), 'label') (('therefore calculated', 'much situation'), 'seem') (('kernel', 'always progress'), 'com') (('how results', 'scientific perspective'), 'get') (('how we', 'sample t one test'), 'measure') (('AdaBoost', 'strong bias'), 'continue') (('Total Error', 'incorrectly classified samples'), 'be') (('even it', 'times'), 'be') (('I', 'Kaggle community'), 'be') (('we', 'dataset'), 'use') (('me', 'notebook'), 'let') (('S sqrt n', 'data set'), 'numerator') (('k best possible value', 'KNN'), 'find') (('here how we', 'other features'), 'let') (('we', 'something'), 'try') (('Gridsearch', 'Machine simple effective Learning'), 'Search') (('how well model', 'unseen data'), 'use') (('green color', 'females'), 'see') (('so we', 'data'), 'illustrate') (('current stump', 'account'), 'need') (('you', 'https here www'), 'click') (('groups', 'measurements'), 'be') (('we', 'dependent variable'), 'be') (('Misclassification wrong Rate', 'other'), 'be') (('when model', 'therefore more bias'), 'be') (('we', 'datasets'), 'load') (('Feature Scaling Feature scaling', 'machine learning important models'), 'be') (('We', 'feature engineering section'), 'keep') (('we', 'such Ridge'), 'decide') (('AUC ROC Curve Using Cross Pros Helps', 'variance'), 'validation') (('female passengers', 'tragedy'), 'have') (('They', 'variables'), 'be') (('model', 'variance increases'), 'decrease') (('sample', 'correctly current stump'), 'alpha_t') (('which', 'Fare'), 'align') (('It', 'dependent variables'), 'separate') (('we', 'null hypothesis'), 'update') (('Sklearn Pipelines Sklearn pipelines tutorial Managing Machine Learning', 'youtube channel'), 'write') (('we', 'alternative following questions'), 'testing') (('we', 'males'), 'know') (('that', 'hypothesis hypothesis null H_1'), 'H_0') (('Gender', 'female survived'), 'show') (('How about we', 'little more character'), 'give') (('bar how left vertical when we', 'certain features'), 'want') (('number', 'model'), 'represent') (('hypothesis test', 'usually concrete well researched problem'), 'formulate') (('many', 'family'), 'be') (('who', 'model'), 'have') (('we', '10 datasets'), 'say') (('fare_groupFare group', 'calculated_fare'), 'calculate') (('Lets', 'bit more train'), 'find') (('class also first passenger', 'second'), 'explain') (('sequentially one', 'combined estimator'), 'boost') (('it', '512'), 'look') (('We', 'passenger survival status'), 'go') (('where I', 'data science journey'), 'to') (('bar _ F', 'group female samples'), 'be') (('training set', 'gender class fare'), 'csv') (('algorithms', 'y_test'), 'determine') (('more penalty', 'alpha'), 'add') (('We', 'them'), 'use') (('Feature Engineering Feature Part 5 Engineering', 'exactly what'), 'be') (('70', 'females'), 'show') (('20 data', 'train'), 'take') (('we', 'null hypothesis'), 'describe') (('correlation', 'Pclass'), 'be') (('we', 'sample t 2 test'), 'mean') (('group', 'animal plants money such human stocks'), 'be') (('PassengerId column', 'dataset'), 'seem') (('we', 'model performance'), 'use') (('we', 'above whole story'), 'be') (('s', 'you'), 'let') (('We', 'train'), 'separate') (('we', 't statistics'), 'use') (('Missclassification', 'minus 1 Accuracy'), 'be') (('mean difference', 'male'), 'be') (('We', 'hypothesis above testing'), 'follow') (('that', 'new feature'), 'create') (('who', 'passenger'), 'be') (('Squaring', 'also relationships'), 'give') (('test', 'that'), 'want') (('I', 'LinkedIn Github masumrumi'), 'be') (('we', 'variable y_pred'), 'fit') (('population', 'mean'), 'add') (('that', 'binary classifier'), '30from') (('s', 'null values'), 'let') (('I', 'different kernel'), 'hope') (('cabin names', 'multiple digits'), 'let') (('mean difference', 'survival rate'), 'be') (('Overview', 'male dataframe'), 'survied') (('s', 'terminologies'), 'let') (('PLIeGtxpvyG LrjxQ60pxZaimkaKKs0zGF video', 'also quite topics'), 'watch') (('following equation', 'calculation'), 'help') (('when model', 'poorly validation'), 'be') (('two null We', 'two raws'), 'let') (('we', 'independent variables'), 'be') (('maybe that', 'judgment'), 'be') (('important which', 'good model'), 'be') (('eventually distribution', 'more normal distribution'), 'be') (('motivation', 'powerful ensemble'), 'be') (('I', 'update'), 'incorporate') (('True Negative actually Rate', 'Positive 1 False Rate'), '15819209039548024') (('almost we', 'first place'), 'be') (('equation', 'sample slightly one test'), 'differ') (('stumps', 'others'), 'get') (('I', 'at once newcomers'), 'be') (('where weights', 'training errors'), 'construct') (('you', 'kernel https www'), 'try') (('they', 'variables'), 'look') (('I', 'machine learning supervised process'), 'wonder') (('who', 'other passengers'), 'have') (('How we', 'confidence level'), 'type') (('how bagging', 'Source https prachimjoshi'), 'be') (('passenger', 'dataset'), 'be') (('com dansbecker underfitting', 'topic'), 'explain') (('good beginners', 'this'), 'be') (('code', 'below just that'), 'need') (('we', 'grid'), 'word') (('lot', 'base estimators'), 'reduce') (('So s', 'correlations'), '33') (('p value', '0'), 'value') (('passengers', 'Pclass'), 'pay') (('This', 'quite Pclass'), 'correlate') (('part', 'kernel'), 'be') (('that', 'when independent variables'), 'be') (('Inversely we', 'those'), 'want') (('forest where part', 'method decision ensemble many trees'), 'be') (('where model', 'well training'), 'want') (('However they', 'different numbers'), 'seem') (('Here We', 'NaN value'), 'take') (('t why it', 't statistics'), 'statistic') (('Using', 'grid close search'), 'be') (('7565217391304347we', 'confusion matrix'), 'have') (('model', 'outliers'), 'use') (('just we', 'sample data points'), 'accept') (('passenger', 'dependent variable'), 'be') (('Most', '100'), 'be') (('Here Age', 'others machine learning features'), 'be') (('We', 'output'), 'run') (('We', 'whilte anywhere space'), 'reset') (('error where rate', 'shape'), 'want') (('total error', 'perfect stump'), 'think') (('So we', 'first model'), 'v') (('Determine', 'null hypothesis'), 'use') (('test data new data', 'output'), 'go') (('s', 'train dataset'), 'let') (('Therefore we', 'better result'), 'need') (('female population', 'point population male mean'), 'call') (('iteration weights', 'consideration'), 'calculate') (('we', 'that'), 'see') (('when it', 'new libraries'), 'go') (('s', 'Embarked feature'), 'let') (('48 second class', 'titanic tragedy'), 'survive') (('test where set', 'actually what'), 'determine') (('before you', 'how dataset'), 'show') (('sample', 'population broader trainning'), 'know') (('observations', 'chance variation'), 'show') (('then we', 'sweet results'), 'find') (('that', 'Queenstown Q'), 'survive') (('you', 'bagging classifier'), 'bag') (('pseducode', 'sample new weight'), 'be') (('_ bar M', 'group sample male measurements'), 'be') (('We', 'features'), 'miss') (('machine', 'data points'), 'be') (('I', 'future update'), 'elaborate') (('models', 'bias variance tradeoff'), 'build') (('ensemble methods', 'constituent learning algorithms'), 'introduce') (('that', 'name'), 'seem') (('Age', 'other features'), 'create') (('it', '1'), 'scale') (('we', 'misclassified weights'), 'in') (('then X_test', 'outcomes'), 'use') (('when model', 'yes'), 'be') (('We', 'lesson'), 'discuss') (('We', 'test values'), 'see') (('who', 'range'), 'represent') (('then total error', '50 50'), 'do') (('train data set', '891 raw columns'), 'have') (('Specify', 'hypothesis important test'), 'be') (('SibSp Parch', 'Continous Age Fare Text Variable Ticket Ticket 0 1 passenger'), 'proxy') (('y', 'values'), 'put') (('dataset contain', 'magnitude'), 'feature') (('We', 'machine learning models'), 'be') (('78', 'test data'), 'let') (('standard which', 'how much there data'), 'be') (('null hypothesis', 'at least as significant one'), 'be') (('how features', 'visualizations'), 'let') (('where data', 'us'), 'in') (('s', 'how them'), 'let') (('we', 'remaining data'), 'train') (('some', 'click'), 'like') (('Age Younger passenger', 'more other passengers'), 'survive') (('estimator knn param_grid instruct 1 scikit', 'available processors'), 'param') (('Here image', 'dataset how bootstrapped works'), 'be') (('how difference', 'other'), 'compare') (('us', 'male'), 'be') (('Many', 'messy data'), 'come') (('often However dataset', 'real world'), 'be') (('learning algorithm models', 'test data'), 'mean') (('that', 'male passengers'), 'show') (('code', '4 parts'), 'split') (('I', 'new updates'), 'incorporate') (('age variable', 'survival rate'), 'seem') (('data point', '1 10 weight'), 'mean') (('60 30 split', 'available processors'), 'normalize') (('we', 'node'), 'end') (('children', 'posibility'), 'hint') (('H0 male mean', 'female mean'), 'be') (('that', 'classification model'), 'matrix') (('limit Central theory', 'where'), 'be') (('Pclass Higher socio status male economic passenger', 'more others'), 'Gender') (('class class then second passenger 2 similarly second passenger', 'class third passenger'), 'explain') (('hypothesis test', 'usually multiple parts'), 'consist') (('model', 'highly beginning'), 'capture') (('what', 'background'), 'be') (('StatQuest Gradient', 'Python 7j'), 'boost') (('data text number image we', 'data'), 'give') (('passengers', 'then others'), 'say') (('hyper when parameters', 'bagging'), 'achieve') (('the', 'passenger'), 'represent') (('which', 'male passengers'), 'rule') (('data', 'machine learning passenger survival status'), 'check') (('We', 'those'), 'want') (('Naive Bayes Support Vector Machines SVM Decision Tree ClassifierDecision Gaussian tree', 'small subsets'), 'work') (('Therefore we', 'male passengers'), 'say') (('age_groupWe', 'machine learning important models'), 'create') (('model', 'TP True Positive that'), 'let') (('idea', 'fewer questions'), 'be') (('where we', 'column'), 'agree') (('it', 'machine learning models'), 'be') (('We', 'different features'), 'find') (('confidence then interval', 'example'), 'be') (('This', 'following reasons'), 'happen') (('it', 'noise'), 'be') (('We', 'ticks'), 'use') (('quite outliers', 'point'), 'think') (('we', 'given mean'), 'help') (('ensemble that', 'it'), 'be') (('s', 'percentage'), 'take') (('it', 'two points'), 'use') (('population', 'bar mu'), 'have') (('test set', 'passengers survival status'), 'provide') (('we', 'dependent variable feature'), 'have') (('we', 'means'), 'let') (('we', 'age missing values'), 'in') (('where pclass', 'C Embarked values'), 'let') (('Bagging classifier', 'them'), 'let') (('how statistically correlation', 'statistics'), 'let') (('we', '2 incorrect predicitons'), 'let') (('better label', 'AdaBoost stump first chain'), 'become') (('we', 'single graph'), 'Relations') (('how relations', 'features'), 'visualization') (('so far you', 'only one missing fare column'), 'feature') (('Null Hypothesis H0 male mean', 'female mean'), 'be') (('we', 'model'), 'have') (('it', 'modules Import necessary InlineBackend'), 'come') (('we', 'control group'), 'form') (('Bagging Why Pros', 'decision fully developed trees'), 'use') (('stump', 'account'), 'make') (('These', 'night'), 'be') (('data', 'groups training set two train'), 'about') (('Therefore I', 'links'), 'decide') (('however most', 'them'), 'be') (('kernel', 'too one go'), 'make') (('We', 'test'), 'use') (('KDE plot', 'pretty self labels'), 'be') (('we', 'trained models'), 'withhold') (('50 people', 'female group'), 'select') (('It', 'parallel'), 'be') (('models', 'average'), 'dataset') (('when it', 'null hypothesis'), 'specify') (('Therefore It', 'median mean'), 'be') (('included weight', 'value'), 'be') (('RobustScaler', 'large outliers'), 'scale') (('figure_format This', 'retina display'), 'retina') (('how it', 'cabin features'), 'let') (('we', 'competition'), 'dump') (('X_train', 'first algorithm'), 'use') (('png Train I', 'statistical overview'), 'info') (('Age', 'distribution'), 'be') (('value', 'raw'), 'mean') (('Alternative Hypothesis H1 male mean', 'female mean'), 'be') (('the', 'sort of actual survived'), 'see') (('we', 'machine learning less complicated quite dataset'), 'go') (('Compute', 'simple equation'), 'follow') (('I', 'it'), 'part') (('that', 'importance'), 'use') "}