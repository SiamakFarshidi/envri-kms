{"name": "titanic step wise detailed explanation ", "full_name": " h1 Table of content h1 1 Introduction Loading libraries and dataset h2 1 3 Analysis goal h2 1 4 A very first look into the data h1 2 Exploratory Data Analysis EDA Cleaning and Engineering features h2 2 1 Correcting and completing features h3 Detecting and correcting outliers h3 Completing features h2 2 2 Descriptive analysis univariate h2 2 3 Feature Engineering Bi variate statistical analysis h3 Pclass h3 Name length h3 Gender Sex h3 Age h3 Family SibSp and Parch h3 Fare h3 Cabin h3 Embarked h3 Titles h3 Extracting deck from cabin h2 2 4 Visualising updated dataset h2 2 5 Descriptive statistics h1 3 Correlation analysis Multi variate analysis h2 3 1 Correlation analysis with histograms and pivot tables h2 3 2 Dropping features h2 3 3 Pearson Correlation Heatmap h2 3 4 Pairplots h1 4 Predictive modelling cross validation hyperparameters and ensembling h2 4 1 Logistic Regression h2 4 2 Support Vector Machines supervised h2 4 3 k Nearest Neighbors algorithm k NN h2 4 4 Naive Bayes classifier h2 4 5 Perceptron h2 4 6 Linear SVC h2 4 7 Stochastic Gradient Descent sgd h2 4 8 Decision tree h2 4 9 Random Forests h2 4 10 Model summary h2 4 11 Model cross validation with K Fold h3 Cross validation scores h2 4 12 Hyperparameter tuning learning curves for selected classifiers h2 4 13 Selecting and combining the best classifiers h2 4 14 Ensembling h2 4 15 Summary of most important features h1 5 Producing the submission file for Kaggle ", "stargazers_count": 0, "forks_count": 0, "description": "It is OK to select algorithms with various results as there is strenght in diversity. I also welcome your comments questions and feedback. We will therefore ensemble the remaining four predictors. Pearson Correlation heatmap. Observations to fine tune our models First let s compare their best score after fine tuning their parameters 1. Dr Major Col around 40 5. for the case of a linear kernel. But not too close The two major sources of error are bias and variance as we reduce these two then we could build more accurate models Bias The less biased a method the greater its ability to fit data well. We will keep this feature. info function below shows how complete or incomplete the datasets are. Top right in the graph above first class and age categories 1 and 2 The not so lucky are mostly in men Pclass 3 and age category 1 younger folks Observations The colors represent blue 0 is for women green 1 for men Clearly women had more chance of surviving with or without cabin Interesting is that accompanied women without a cabin had less survival chance than women alone without cabin. Linear SVCThis is another implementation of Support Vector Classification similar to 4. And therefore the risk that the model will not adapt accurately to new test data. The approach to to complete missing data is to impute using mean median or mean randomized standard deviation. Create a new feature Title containing the titles of passenger names Mapping titles Explore Age vs Survived for dataset in full_data dataset Boys 0 dataset. loc dataset Age 0 dataset Sex 1 Boys 1 dataset Boys. The categories are sized to group passengers with similar Survival rates. Then we divide the values mean is already subtracted of each feature by its standard deviation. I found that dropping the outliers actually lower the prediction. Model cross validation with K Fold 4. Define function to extract titles from passenger names If the title exists extract and return it. Therefore we re going to extract these and create a new feature that contains a persons deck. Random forests or random decision forests are an ensemble learning method for classification regression and other tasks that operate by constructing a multitude of decision trees n_estimators 100 at training time and outputting the class that is the mode of the classes classification or mean prediction regression of the individual trees 4. It is a type of linear classifier i. Observations The Detect_Outliers function found 10 outliers. The missing values will be converted to zero. value_counts Takes a scalar and returns a string with the css property color red if below 0. IsAlone does not result in a significant difference of survival rate. Women and men alone on first class second pivot red showing survival rate below 0. This is influenced by the following two factors 1 Women versus men and the compounding effect of Name_length and 2 Passengers paying a high price Fare have a higher chance of survival there are also in first class have a title. 5 time IQR below Q1 or 1. In this case it is better to transform it with the log function to reduce the skewness and redistribute the data. Observations from the Pearson analysis Correlation coefficients with magnitude between 0. The reverse also holds the greater the bias the lower the variance. 14 EnsemblingThis is the final step pulling it together with an amazing Voting function from sklearn. Cross validation process https image. This is a useful frist step of our anblaysis in order to determine the empirical relationship between all features. FacetGrid train_df col Embarked Feature selection X_train all features for training purpose but excluding Survived Y_train survival result of X Train and test are our 3 main datasets for the next sections test data for Kaggle submission Preparing data for Submission 1 print i score Preparing data for Submission 2 Cross validate model with Kfold stratified cross validation Modeling step Test differents algorithms Adaboost ExtraTrees Gradient boosting tunning SVC classifier Best score Random Forest Best score Concatenate all classifier results Preparing data for Submission 3 Submit File. We start to find where most survivors are older women 48 to 64 year old and younger passengers. Decision submit 3 as best predictor 1. The interquartile range IQR is a measure of statistical dispersion being equal to the difference between the 75th and 25th percentiles or between upper and lower quartiles. What is statistically interesting is that only young boys Age Category 0 have high survival rates unlike other age groups for men. Naive Bayes classifierThis is a family of simple probabilistic classifiers based on applying Bayes theorem with strong naive independence assumptions between the features. Model summaryI found that the picture illustrates the various model better than words. 4 Visualising updated dataset 2. k Nearest Neighbors algorithm k NN 4. This resulted in a significant improvement of the prediction accuracy on the test data score. So I decided to keep them. The plots show training points in solid colors and testing points semi transparent. k Nearest Neighbors algorithm k NN This is a non parametric method used for classification and regression. The algorithm allows for online learning in that it processes elements in the training set one at a time. The clustering of red dots indicates the combination of two features results in higher survival rates or the opposite clustering of blue dots lower survival For example Smaller family sizes in first and second class Middle age with Pclass in third category only blue dotThis can be used to validate that we extracted the right features or help us define new ones. Similarly having a cabin increases the chance of survival. In addition the slight difference between men and women go in different direction i. Analysis goal The Survived variable is the outcome or dependent variable. So many new terms new functions new approaches but the subject really interested me so I dived into it studied one line of code at a time and captured the references and explanations in this notebook. The general method of calculation is to determine the distribution mean and standard deviation for each feature. 3 Feature Engineering Bi variate statistical analysisOne of the first tasks in Data Analytics is to convert the variables into numerical ordinal values. The next step after dropping less relevant features is to scale them a very good recommendation from Konstantin s kernel It helps to boost the score. Predictive modelling cross validation hyperparameters and ensembling5. Gender and age seem to have a stronger influece of the survival rate. PassengerID 28 89 and 342 passenger have an high Ticket Fare The seven others have very high values of SibSP. I added many variations and additional features to improve the code as much as I could as well as additional visualization. Cross validation is a way to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available. This indicates that it predicts differently than the others when it comes to the test data. Naive Bayes classifiers are highly scalable requiring a number of parameters linear in the number of features in a learning problem. Observations As indicated before Adaboost has the lowest correlations when compared to other predictors. But it shows how each classifier algorithm partitions the same data in different ways. Family size of 3 or 4 from first pivot2. Predictive modelling cross validation hyperparameters and ensembling 4. It involves the analysis of one or two features and their relative impact of Survived. png Observations The above models classifiers were applied to a split training and x_test datasets. We will this by using StratifiedKFold to train and test the models on sample data from the overall dataset. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. This is the case here with Gradient Boost high score but cross validation is very distant. We can see the red and green curves from ExtraTrees RandomForest and SVC are pretty close. Introduction Loading libraries and dataset2. We will first compare in the next section the classifiers results between themselves and applied to the same test data. We will do this in section 2. At the same time there have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores. The algorithm itself is a fork from Anisotropic s Introduction to Ensembling Stacking in Python a great notebook in itself. PerceptronThis is an algorithm for supervised learning of binary classifiers like the other classifiers before it decides whether an input represented by a vector of numbers belongs to some specific class or not. This happens when the classifiers use many input features to include noise in each feature on the complete dataset and ends up memorizing the noise instead of finding the signal. They do not have to be the same size or equally distributed. Descriptive analysis univariate 2. This overfit model will then make predictions based on that noise. I will combine those into the first category. Many overfit the training data up to 90 but do not generate more accurate predictions with the test data. It estimates probabilities using a cumulative logistic distribution The first value shows the accuracy of this model The table after this shows the importance of each feature according this classifier. value_counts Mapping Gender plot distributions of age of passengers who survived or did not survive Qcut is a quantile based discretization function to autimatically create categories not used here dataset Age pd. Summary of most important featuresNice graphics but the obsevation is unclear in my opinion On one side we hope as analyst that the models come out with similar patterns. Decision treeThis predictive model maps features tree branches to conclusions about the target value tree leaves. reset_index drop True Qcut is a quantile based discretization function to autimatically create categories dataset Name_length pd. Exploratory Data Analysis EDA Cleaning and Engineering featuresWe will start with a standard approach of any kernel correct complete engineer the right features for analysis. This is the datadframe that will be returned. This happened as an iterative process by reviwing the outcome of the feature importance graph in the next section. Scaling features is helpful for many ML algorithms like KNN for example it really boosts their score. In other words SGD tries to find minima or maxima by iteration. Lastly the right ensembling was best achieved with a votingclassifier with soft voting parameterOne last word please use this kernel as a first project to practice your ML Python skills. In data processing it is also known as data normalization. loc dataset Fare 2. 2 dataset Fare 3. 2 with the fillna function dataset Fare dataset Fare. IsAlone alone is not a good predictor of survival. Model cross validation with K FoldThe fitting process applied above optimizes the model parameters to make the model fit the training data as well as possible. What are the data types for each feature Survived int Pclass int Name string Sex string Age float SibSp int Parch int Ticket string Fare float Cabin string Embarked string 1. Don Rev Capt Jonkheer no data4. There are null values or missing data in the age cabin and embarked field. This results in some classifiers Decision_tree and Random_Forest over fitting the model to the training data. It performs unusually well on its training data but will not necessarilyimprove the prediction quality with new data from the test dataset. Mr below 20 Extracting deck from cabinA cabin number looks like C123 and the letter refers to the deck a big thanks to Nikas Donge. 7 more survivors CabinIt appears that Has_Cabin has a strong impact on the Survival rate. Observations The pairplot graph all trivariate analysis into one figure. Then ensemble them together with an automatic function callled voting. This can be due to its strong relationship with other features such as Is_Alone or Parch Parent Children. A high bias method builds simplistic models that generally don t fit well training data. 15 _images plot_classifier_comparison_0011. Afterwords we will convert the feature into a numeric variable. Variance with a lower bias comes typically a higher the variance. Missing values can be bad because some algorithms don t know how to handle null values and will fail. Naive Bayes classifier 4. We will analyse it further in the next sections and drop it if we cannot demonstrate a proven relationship to Survived. Correcting and completing features Detecting and correcting outliersReviewing the data there does not appear to be any aberrant or non acceptable data inputs. Descriptive statistics Initial observations from the descriptive statistics Only 38 survived a real tragedy Passengers in more expensive classes 1 and 2 had much higher chance of surviving than classes 3 or 4. Women 0 higher chance than men 1 Younger people slightly more chance than older Being alone decreased your chance to survive. And lone men should join a family to improve their survival rates. Correlation analysis with histograms and pivot tables Observations for Age graph 0 or blue represent women 1 or orange represent men. If k 1 then the object is simply assigned to the class of that single nearest neighbor. image http scikit learn. Observations Log Fare categories are 0 to 2. extra trees on various sub samples of the dataset and use averaging to improve the predictive accuracy and control over fitting. a stronger ability to apply the model to new data. A test dataset has been created to test our algorithm. The three rows represent the three different data set on the right. drop Outliers_to_drop axis 0. 13 Selecting and combining the best classifiersSo how do we achieve the best trade off beween bias and variance 1. Gender Sex AgeThe best categories for age are 0 Less than 14 1 14 to 30 2 30 to 40 3 40 to 50 4 50 to 60 5 60 and more Family SibSp and ParchThis section creates a new feature called FamilySize consisting of SibSp and Parch. An easy direction to follow. Feature standardization makes the values of each feature in the data have zero mean when subtracting the mean in the numerator and unit variance. 3 types of deck 1 with 15 passengers 2 to 6 and 7 to 8 most passengers 2. RandomForest operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes classification or mean prediction regression of the individual trees. O will drop this feature. Some key take away from my personal experiments and what if analysis over the last couple of weeks The engineering of the right features is absolutely key. While others like decision trees can handle null values. But this is not true for men. As we say in business diversity brings better results this seems to be true with algorithms as well 5. Stochastic Gradient Descent sgd This is a stochastic approximation of the gradient descent optimization and iterative method for minimizing an objective function that is written as a sum of differentiable functions. The last line applied the ensemble predictor to the test data for submission. I tried many many different algorightms. This can lead to overweigthing the model with very high values. All descripotion adapted from Wikipedia. An ensemble is a supervised learning algorithm that it can be trained and then used to make predictions. I also used multiple functions from Yassine Ghouzam. EmbarkedIrrespective of the class passengers embarked in 0 S and 2 Q have lower chance of survival. Adaboost is used in conjunction with many other types of learning algorithms to improve performance. The second one their average survival rates. Exploratory analysis engineering and cleaning features Bi variate analysis3. There are potential outliers that we will identify steps from Yassine Ghouzam It creates firset a function called detect_outliers implementing the Tukey method For each column of the dataframe this function calculates the 25th percentile Q1 and 75th percentile Q3 values. Correlation analysis Multi variate analysisThis section summarizes bivariate analysis asthe simplest forms of quantitative statistical analysis. Summary of most important features 4. 9 generates a public score of 0. All other variables are potential predictor or independent variables. Feature scaling is a method used to standardize the range of independent variables or features of data. Submission 2 The prediction with random_forest in Section 4. What helped best is to group together passengers with the same survival rates. Pearson is bivariate correlation measuring the linear correlation between two features. A sample is classified by a majority vote of its neighbors with the sample being assigned to the class most common among its k nearest neighbors k is a positive integer typically small. This points to a lower variance i. Mme Ms Lady Sir Mlle Countess 100. In simple words it allows to test how well the model performs on new data. In our case cross validation will also be applied to compare the performances of different predictive modeling procedures. Pearson Correlation HeatmapThe Seaborn plotting package allows us to plot heatmaps showing the Pearson product moment correlation coefficient PPMCC correlation between features. PclassEmbarked does not seem to have a clear impact on the survival rate. Bottom line it would have been better for women without cabin to pretend that they were alone. We will create a new feature called young boys Observations here are the survivors 1. Producing the submission file for KaggleFinally having trained and fit all our first level and second level models we can now output the predictions into the proper format for submission to the Titanic competition. Introduction Loading libraries and datasetI created this Python notebook as the learning notes of my first Machine Learning project. TitlesThere are 4 types of titles 0. This cross validation object is a variation of KFold which returns stratified folds. The SVM training algorithm builds a model that assigns new test samples to one category or the other making it a non probabilistic binary linear classifier. FacetGrid train_df col Pclass hue Survived graph distribution of qualitative data Pclass grid sns. All outlier data get then pulled into the outlier_indices dataframe. 12 Hyperparameter tuning learning curves for selected classifiers 4. Logistic Regression 4. 12 Hyperparameter tuning learning curves for selected classifiers 1. generates a public score of 0. Support Vector Machines supervised Given a set of training samples each sample is marked as belonging to one or the other of two categories. AdaBoost is sensitive to noisy data and outliers. Finally the detect_outliers function will select only the outliers happening multiple times. His notebook was itself based on Faron s Stacking Starter as well as Sina s Best Working Classfier. Any data points outside 1. 6 Fare 2 Feature that tells whether a passenger had a cabin on the Titanic O if no cabin number 1 otherwise Remove all NULLS in the Embarked column Mapping Embarked Takes a scalar and returns a string with the css property color red if below 0. 7 Stochastic Gradient Descent 4. GradientBoost produces a prediction model in the form of an ensemble of weak prediction models typically decision trees. The folds are made by preserving the percentage of samples for each class. ageMany feature engineering steps were taken from Anisotropic s excellent kernel. Also the higher the fare the higher the chance. You can try to remove them and rerun the prediction to observe the result with the following function Completing featuresThe. PairplotsFinally let us generate some pairplots to observe the distribution of data from one feature to the other. A very first look into the dataThis is only a quick of the relationships between features before we start a more detailed analysis. 7 indicate variables which can be considered moderately correlated. The proposed categories are less than 23 mostly men 24 to 28 29 to 40 41 and more mostly women. com kagglesharingmarkpeng20151216finalpresented 151216161621 95 general tips for participating kaggle competitions 13 638. Logistic RegressionLogistic regression measures the relationship between the categorical dependent feature in our case Survived and the other independent features. In this case Name Categorical Sex b Numeric or quantitative data Discrete could be ordinal like Pclass or not like Survived. The target features take a finite set of values are called classification trees in these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels. I shameless ley sotle and learnt from many Kagglers through my learning process please do the same with the code in this kernel. Submission 3 Kaggle Version 85 The prediction with gsrandom_forest in Section 4. SVC 83It appears that GBC and SVMC are doing the best job on the Train data. Dropping featuresBottom line of the bi variate and tri variate analysis as well as the feature importance analysis from running the classifiers multiple times I decided to drop less relevant features. In the next section we will cross validate the models using sample data against each others. Credit Yvon Dalat 1. The output of the other learning algorithms weak learners is combined into a weighted sum that represents the final output of the boosted classifier. The Seaborn pairplot class will help us visualize the distribution of a feature in relationship to each others. The reality os that the algorithms work with many dimensions 11 in our case. This should be taken with a grain of salt as the intuition conveyed by these two dimensional examples does not necessarily carry over to real datasets. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. qcut dataset Name_length 6 labels False train Name_length. Support Vector Machines supervised 4. Correlation analysis Tri variate analysis4. 5 time IQR above Q3 is considered an outlier. For instance the visualization helps understand how RandomForest uses multiple Decision Trees the linear SVC or Nearest Neighbors grouping sample by their relative distance to each others. We will drop unncessary features just before Section 3. This is good because we want to keep the model as close to the training data as possible. I used the above graphs to optimize the parameters for Adaboost ExtraTrees RandomForest GradientBoost and SVC. The goal is to predict this dependent variable only using the available independent variables. 7 less survivors More than 2. Random decision forests correct for decision trees habit of overfitting to their training set. 13 Selecting and combining the best classifiers 4. Men alone have less chance than accompanied. The outlier_list_col column captures the indices of these outliers. What worked better is to use the cross validation on selected algotirhms. The lower right shows the classification accuracy on the test set. It provides train test indices to split data in train test sets. 7 dataset Fare 3. It builds the model in a stage wise fashion like other boosting methods do and it generalizes them by allowing optimization of an arbitrary differentiable loss function. SVMC or Support Vector Machines. Next we subtract the mean from each feature. Submitting results Note Click on Copy and Edit button to see the full Detailed Explanation. Mrs Miss around 70 survival2. Observation This classfier confirms the importance of Name_length FamilySize did not show a strong Pearson correlation with Survived but comes here to the top. Decision trees where the target variable can take continuous values typically real numbers are called regression trees. ExtraTrees implements a meta estimator that fits a number of randomized decision trees a. vGiven a set of training examples each marked as belonging to one or the other of two categories an SVM training algorithm builds a model that assigns new examples to one category or the other making it a non probabilistic binary linear classifier. Which model to choose These are the results of my many submissions Submission 1 The prediction with KNeighborsClassifier KNN in Section 4. Importing Library Load libraries for analysis and visualization collection of functions for data processing and analysis modeled after R dataframes with SQL like features foundational package for scientific computing Regular expression operations Collection of functions for scientific and publication ready visualization Open source library for composing editing and sharing interactive data visualization Machine learning libraries Implementation of gradient boosted decision trees designed for speed and performance that is dominative competitive machine learning Visualization library based on matplotlib provides interface for drawing attractive statistical graphics Collection of machine learning algorithms Load in the train and test datasets from the CSV files Store our passenger ID for easy access Display the first 5 rows of the dataset a first look at our data 5 first row 5 sample rows and basic statistics Outlier detection iterate over features columns 1st quartile 25 3rd quartile 75 Interquartile range IQR outlier step Determine a list of indices of outliers for feature col append the found outlier indices for col to the list of outlier indices select observations containing more than 2 outliers detect outliers from Age SibSp Parch and Fare Show the outliers rows Drop outliers train train. The goal there is to create the right categories between survived and not survived. Fare Observations The Fare distribution is very skewed to the left. Name_lengthThe first graph shows the amount of people by Name_length. 14 after stratification and model cross validation generates a public score of 0. It is a binary nominal datatype of 1 for survived and 0 for did not survive. qcut dataset Age 6 labels False Using categories as defined above Create new feature FamilySize as a combination of SibSp and Parch Create new feature IsAlone from FamilySize Create new feature Boys from FamilySize Interactive chart using cufflinks Remove all NULLS in the Fare column and create a new feature Categorical Fare Explore Fare distribution Apply log to Fare to reduce skewness distribution dataset. Stratified K Folds is a cross validation iterator. jpg cb 1452565877 Cross validation scores 4. In addition I found out that AdaBoost does not do a good job with this dataset as the training score and cross validation score are quite far apart. Random ForestsThis is one of the most popular classfier. The problem with less important features is that they create more noice and actually take over the importance of real features like Sex and Pclass. We can see from the red cells that many features are moderately correlated specifically IsAlone Pclass Name_length Fare Sex. There are multiple types of data a Qualitative data discrete Nominal no natural order between categories. ", "id": "amark720/titanic-step-wise-detailed-explanation", "size": "25417", "language": "python", "html_url": "https://www.kaggle.com/code/amark720/titanic-step-wise-detailed-explanation", "git_url": "https://www.kaggle.com/code/amark720/titanic-step-wise-detailed-explanation", "script": "Counter classification_report train_test_split pyplot color_negative_red confusion_matrix xgboost precision_recall_curve sklearn.svm numpy cross_val_score accuracy_score seaborn SGDClassifier detect_outliers cufflinks SVC LinearSVC GaussianNB sklearn.neighbors sklearn.naive_bayes sklearn.tree sklearn.linear_model learning_curve StratifiedKFold plotly.tools KFold matplotlib.pyplot plot_learning_curve DecisionTreeClassifier (RandomForestClassifier plotly.offline plotly.graph_objs pandas sklearn.model_selection LogisticRegression Perceptron sklearn.cross_validation matplotlib KNeighborsClassifier AdaBoostClassifier get_title GridSearchCV collections sklearn.metrics sklearn.ensemble StandardScaler sklearn.preprocessing ", "entities": "(('algorithm', 'great itself'), 'be') (('when explicit validation', 'hypothetical validation'), 'be') (('engineering', 'right features'), 'take') (('I', 'first category'), 'combine') (('This', 'next section'), 'happen') (('We', 'therefore remaining four predictors'), 'ensemble') (('Adaboost', 'performance'), 'use') (('Drop outliers', 'train train'), 'feature') (('Observations Log Fare categories', '2'), 'be') (('relationship', 'case'), 'measure') (('validation', 'Gradient here Boost high score'), 'be') (('SGD', 'iteration'), 'try') (('decision Random forests', 'training set'), 'correct') (('Men', 'alone less chance'), 'have') (('we', 'Titanic competition'), 'output') (('GradientBoost', 'typically trees'), 'produce') (('don generally t', 'training well data'), 'build') (('predictions', 'feature vector'), 'algorithm') (('importance', 'here top'), 'Observation') (('IsAlone', 'alone good survival'), 'be') (('Categorical Fare Explore Fare distribution Apply', 'distribution skewness dataset'), 'dataset') (('that', 'class labels'), 'take') (('Qualitative data', 'natural categories'), 'be') (('who', 'Age here pd'), 'be') (('sample', 'two categories'), 'supervise') (('Random ForestsThis', 'most popular classfier'), 'be') (('This', 'non parametric classification'), 'algorithm') (('We', 'overall dataset'), 'will') (('Afterwords we', 'numeric variable'), 'convert') (('C123', 'Nikas big thanks Donge'), 'look') (('red curves', 'ExtraTrees RandomForest'), 'see') (('Similarly cabin', 'survival'), 'have') (('You', 'following function'), 'try') (('when it', 'test data'), 'indicate') (('The', 'data'), 'close') (('input', 'specific class'), 'be') (('We', 'just Section'), 'drop') (('outlier data', 'outlier_indices then dataframe'), 'get') (('algorithm partitions', 'same different ways'), 'show') (('It', 'relative Survived'), 'involve') (('it', 'data'), 'be') (('Naive Bayes classifiers', 'learning problem'), 'be') (('classifiers', 'test same data'), 'compare') (('overfit model', 'noise'), 'make') (('Many', 'test data'), 'overfit') (('This', 'very high values'), 'lead') (('learning supervised it', 'then predictions'), 'be') (('Feature scaling', 'data'), 'be') (('that', 'individual trees'), 'be') (('model', 'training data'), 'cross') (('it', 'loss arbitrary differentiable function'), 'build') (('score Best Concatenate', 'Submit Submission 3 File'), 'FacetGrid') (('that', 'differentiable functions'), 'sgd') (('that', 'individual trees'), 'operate') (('Introduction Loading libraries', 'Machine Learning first project'), 'create') (('we', 'Survived'), 'analyse') (('boys young here survivors', 'new feature'), 'create') (('price high Fare', 'title'), 'influence') (('Fare', 'Fare'), 'dataset') (('Fare Fare distribution', 'very left'), 'observation') (('qcut', 'False Name_length'), 'dataset') (('ley sotle', 'kernel'), 'shameless') (('approach', 'standard deviation'), 'be') (('graph', 'one figure'), 'observation') (('12 Hyperparameter', 'selected classifiers'), 'tuning') (('they', 'Sex'), 'be') (('as much I', 'additional code'), 'add') (('It', 'diversity'), 'be') (('Women', 'alone chance'), 'decrease') (('Observations Detect_Outliers function', '10 outliers'), 'find') (('This', 'such Is_Alone'), 'be') (('validation', 'modeling different predictive procedures'), 'cross') (('three rows', 'right'), 'represent') (('this', 'classifier'), 'estimate') (('therefore model', 'test accurately new data'), 'risk') (('detect_outliers Finally function', 'only outliers'), 'select') (('how RandomForest', 'others'), 'help') (('PairplotsFinally us', 'other'), 'let') (('so I', 'notebook'), 'approach') (('Variance', 'typically a higher variance'), 'come') (('5 time IQR', 'Q3'), 'consider') (('it', 'data also normalization'), 'know') (('Linear SVCThis', 'similar 4'), 'be') (('discretization quantile based autimatically categories', 'Name_length pd'), 'drop') (('k', 'most common k nearest neighbors'), 'classify') (('title', 'it'), 'function') (('Family 0 Less than 14 1 14 to 30 2 30 to 40 3 40 to 50 4 50 to SibSp section', 'SibSp'), 'Sex') (('Has_Cabin', 'Survival rate'), 'appear') (('categories', 'Survival similar rates'), 'size') (('It', 'survived'), 'be') (('test dataset', 'algorithm'), 'create') (('validation score', 'training score'), 'find') (('outlier_list_col column', 'outliers'), 'capture') (('when classifiers', 'instead signal'), 'happen') (('lower right', 'test set'), 'show') (('AdaBoost', 'noisy data'), 'be') (('model predictive maps', 'target value tree leaves'), 'feature') (('This', 'features'), 'be') (('Next we', 'feature'), 'subtract') (('EmbarkedIrrespective', 'survival'), 'embark') (('It', 'train test sets'), 'provide') (('statistically only young boys', 'men'), 'be') (('last line', 'submission'), 'apply') (('Name_lengthThe first graph', 'Name_length'), 'show') (('variate', 'numerical ordinal values'), '3') (('14 EnsemblingThis', 'sklearn'), 'be') (('Gender', 'survival rate'), 'seem') (('notebook', 'Stacking Starter'), 'be') (('training points', 'solid colors'), 'show') (('they', 'cabin'), 'line') (('interquartile range IQR', 'upper quartiles'), 'be') (('data', 'Detecting'), 'appear') (('I', 'Yassine Ghouzam'), 'use') (('Predictive modelling', 'validation hyperparameters'), 'cross') (('model', 'Section'), 'be') (('last word', 'ML Python skills'), 'achieve') (('us', 'new ones'), 'indicate') (('seven others', 'SibSP'), 'have') (('proposed categories', '40 41 women'), 'be') (('which', 'variables'), 'indicate') (('it', 'really score'), 'be') (('that', 'persons deck'), 'go') (('data types', 'Survived Pclass int Name Sex string int Age'), 'be') (('intuition', 'necessarily real datasets'), 'take') (('picture', 'better words'), 'find') (('I', 'Adaboost ExtraTrees RandomForest GradientBoost'), 'use') (('subsequent weak learners', 'previous classifiers'), 'be') (('that', 'decision trees randomized a.'), 'implement') (('it', 'time'), 'allow') (('values', 'standard deviation'), 'divide') (('Name Categorical Sex quantitative Discrete', 'Survived'), 'b') (('that', 'boosted classifier'), 'combine') (('PclassEmbarked', 'survival rate'), 'seem') (('It', 'test dataset'), 'perform') (('1 then object', 'single nearest neighbor'), 'assign') (('Only 38', 'classes'), 'survive') (('we', 'more detailed analysis'), 'be') (('general method', 'standard feature'), 'be') (('0 Sex', '1 1 Boys'), 'dataset') (('Mapping Embarked', '0'), 'feature') (('helped', 'survival same rates'), 'be') (('us', 'features'), 'Correlation') (('zero', 'numerator variance'), 'make') (('slight difference', 'different direction'), 'go') (('model stratification validation', '0'), '14') (('This', 'test data score'), 'result') (('multiple times I', 'less relevant features'), 'drop') (('that', 'alone cabin'), 'right') (('us', 'others'), 'help') (('worked', 'selected algotirhms'), 'be') (('which', 'stratified folds'), 'be') (('Adaboost', 'when other predictors'), 'observation') (('algorithms', 'don how null values'), 'be') (('folds', 'class'), 'make') (('dropping', 'actually prediction'), 'find') (('we', 'others'), 'validate') (('algorithms', '11 case'), 'os') (('IsAlone', 'survival rate'), 'result') (('feature engineering steps', 'excellent kernel'), 'take') (('lone men', 'survival rates'), 'join') (('It', 'score'), 'be') (('Boys', 'full_data'), 'create') (('summarizes', 'quantitative statistical analysis'), 'analysis') (('missing values', 'zero'), 'convert') (('graph distribution', 'sns'), 'FacetGrid') (('First s', 'parameters'), 'observation') (('reverse', 'the lower variance'), 'hold') (('more one', 'better scores'), 'be') (('this', 'algorithms'), 'bring') (('it', 'one category'), 'build') (('we', 'training close data'), 'be') (('function', 'Q3 values'), 'be') (('Predictive modelling', '4'), 'cross') (('png models above classifiers', 'split training'), 'observation') (('others', 'null values'), 'handle') (('goal', 'only available independent variables'), 'be') (('I', 'comments also questions'), 'welcome') (('how we', 'beween bias'), 'achieve') (('Pearson', 'two features'), 'be') (('many features', 'IsAlone Pclass Name_length Fare moderately specifically Sex'), 'see') (('models', 'similar patterns'), 'be') (('how well model', 'new data'), 'allow') (('GBC', 'Train data'), 'appear') (('Naive classifierThis', 'features'), 'Bayes') (('typically real numbers', 'continuous values'), 'call') (('value_counts', '0'), 'take') (('EDA Cleaning', 'analysis'), 'start') "}