{"name": "titanic ml or blatt ", "full_name": " h1 Part 1 Introduction h2 1 1 Background h2 1 2 Goal h2 1 3 Strategy h2 1 4 What to expect h3 Feature Engineering sneak peek h1 Table of Contents h1 Part 2 Question Problem Definition h1 Part 3 About This Dataset h2 3 1 Pre Processing Facts h2 3 2 Features breakdown h3 3 2 1 Text h3 3 2 2 Numerical h4 Discrete Ordinal h4 Continous h3 3 2 3 Categorical h4 Nominal h4 Binary h3 3 2 4 Time Series Data h3 3 2 5 Useless h3 3 2 6 Summary h2 3 3 Insights and Observations h3 3 3 1 Observations in a nutshell for all features h3 3 3 2 What is the distribution of numerical feature values across the samples h4 Outliers h3 3 3 3 What is the distribution of categorical features h3 3 3 4 All Observations h1 Install Libraries h1 Part 4 Acquire training and testing data h2 Dabl Pandas profiling for quick insights h2 Import Libraries h1 Part 5 Data Wrangling h2 Feature Completing h3 Train Dataset missing values X 891 Passengers h3 Test Dataset missing values X 418 Passengers h3 Steps h3 Completing techniques WIP h2 Feature Correcting h3 Detect outliers h3 Reduce skewness h3 Visualize distribution of missing values using missingno h2 5 1 Completing Train Embarked h2 5 2 Complete Test Fare h2 5 3 Complete Cabin Cabin Prefix h3 Cabin Outlier Correcting h3 Describe datasets h2 5 3 1 Cabin Prefix Train h3 5 3 2 Complete Cabin Prefix Test h2 5 4 Complete Age Train Test DataFrames h1 Part 6 Feature Engineering h2 6 1 Family Size Ordinal Numeric Feature h2 6 2 Calculated Fare based on Family Size h2 6 3 Ticket Frequency Ordinal Numeric Feature h2 6 4 Family Title h2 6 5 Survival Rate Survival Rate NA h4 Considering that passengers from the same family and or the same ticket are found in train and test datasets we can calculate the family survival rate and ticket survival rate h3 Title Correcting and Grouping Rare h4 Title Correcting typos h4 Title Grouping Rare h2 6 6 Is Married Feature h2 6 7 Features Correcting Power Transforms h3 Correct outliers and skewness h3 Deal with Fare Skewness by log h3 Shapiro Wilks test h4 Power Transforms h3 Box Cox Transform h3 Yeo Johnson Transform h3 Reduce skewness in Test Data h1 Part 7 Feature Transformation h2 Recap h2 7 1 Feature Binning h3 1 Continious Features Binning h2 Feature Binning Bayesian Blocks h1 Cleaning things up pre feature selection h2 7 2 Label Encoding h2 7 3 OneHotEncoding h2 Part 8 Feature Selection h2 8 1 Multicollinearity Variance Infalction Factor VIF h3 Summary of Multicollinary Changes based on VIF decrease and feature removal h2 8 2 Predictive Power Score WIP h1 Part 9 Modelling h2 9 1 Define X and y features h2 9 2 Split Training data in two datasets anti overfitting h3 Dabl h2 9 3 Hyperparameters Optimization Grid Search h3 9 3 1 Models without Hyperparameters h3 9 3 2 Tune Model with Hyper Parameters h2 Hyperparameters Tuning Impact h2 9 4 Voting Classifier h1 Part 10 Submission h1 Change Log h1 Part 11 Credits and Resources h1 Interesting Resources ", "stargazers_count": 0, "forks_count": 0, "description": "Alone SibSp Parch 0Let s check who has multiple independent variables like the above mentioned ones. Complete Age Train Test DataFrames 20 missing values use np. All Observations Sex SexSurvival The chance of survival for women is high as compared to men. copy deep True combined_df_copy test_df. items print passengers_without_cabin_column passengers_without_cabin_value mean_of_numerical_variable std_of_numerical_variable get_mean_and_std_of_numerical_variable df final_df_cabin_prefix_features_description cabin_prefix cabin_prefix df_column passengers_without_cabin_column mean_plus_std mean_minus_std get_mean_plus_minus_std mean mean_of_numerical_variable std std_of_numerical_variable is_correlated_feature mean_plus_std passengers_without_cabin_value mean_minus_std 0 else 0 retrieve just the numeric input values perform a box cox transform of the dataset unknown why feature_range 1 2 instead of the default feature_range 0 1 convert the array back to a dataframe histograms of the variables retrieve just the numeric input values perform a yeo johnson transform of the dataset define the pipeline convert the array back to a dataframe histograms of the variables retrieve just the numeric input values perform a yeo johnson transform of the dataset define the pipeline convert the array back to a dataframe histograms of the variables scotts binned histogram blocks binned histogram bins bayesian_blocks train_df Calculated_Fare https www. find out which passengers share the same group puchase by ticket. cut test_df Calculated_Fare bins np. 2 One Hot Encoding the Categorical Features number of categories for a categorical feature rename columns assign changes to input dataframe and delete the original non one hot encoded feature remove cols 0 to avoid multicollinarity df. Embarked takes three possible values. fit_transform values print integer_encoded dummies pd. 05 indicates significant skewness. 1 algorithm SAMME SAMME. columns store_covariance True False QuadraticDiscriminantAnalysis store_covariance True False XGBClassifier booster gbtree gblinear dart Ensemble Methods Gaussian Processes Navies Bayes Nearest Neighbor Discriminant Analysis xgboost http xgboost. Small Family 3 people4. 1309 371 passengers. CategoricalEmbarked and Sex will be encoded using OneHotEncoding Nominal Embarked character. C passengers_representing_those_with_missing_embarked1 temp_df temp_df. 7 of the subsets let s know the worst that can happen save MLA predictions see section 6 for usage print and sort table https pandas. This rational is derived from here https www. Parch 0 790 values it s too frequent. com helgejo an interactive data science tutorial 5. com ash316 eda to prediction dietanic Observations in a Nutshell for all features. Survival_Rate Survival_Rate_NA Considering that passengers from the same family and or the same ticket are found in train and test datasets we can calculate the family survival rate and ticket survival rate. html remember to install it first pip install dabl data analysis and wrangling visualization machine learning https dzone. Submission https www. concat PassengerId result axis 1 submit base_results model_selection. To avoid over filtering I will correlate 3 independent variables Sex Pclass Age Fare Cabin_prefix. Age Children less than 5 10 years do have a high chance of survival In particular infants Age 4 had high survival rate. describe by Cabin_Prefix and by feature to get the mean and std of the numerical feature for a specific Cabin_Prefix sliced_description_df_by_cabin_prefix df. com questions 17995024 how to assign a name to the a size column https seaborn. Few elderly passengers 1 within age range 65 80. Hence a cabin_estimator function will mislead us and will most likely impute false values to complete Cabin feature. groupby Cabin_Prefix Fare. 1 kernel linear poly rbf sigmoid precomputed degree range 1 5 shrinking True False probability True False gamma scale auto decision_function_shape ovo ovr break_ties True False LinearSVC penalty l1 l2 loss hinge squared_hinge dual True False DecisionTreeClassifier criterion gini entropy splitter best random max_depth 2 4 None min_samples_split 2 5 min_samples_leaf 1 2 max_features auto sqrt log2 ExtraTreeClassifier criterion gini entropy splitter best random max_depth 2 4 None min_samples_split 2 5 min_samples_leaf 1 2 max_features auto sqrt log2 LinearDiscriminantAnalysis solver svd lsqr eigen shrinkage None auto 0. com jeffd23 scikit learn ml from start to finishhttps www. Fare Higher fare paying passengers had better survival chances Money Matters. These passengers better represent the passenger with missing Fare because they share same characteristics which are depicted by the correlated independant variables. fit_transform X_train y_train from category_encoders import LeaveOneOutEncoder enc LeaveOneOutEncoder cols Name_of_col Another_name training_set enc. SibSp is the count of siblings and spouse and Parch is the count of parents and children. Test dataset range 0 10 Parch Integer Meaning the number of parents or children travelling with each passenger. min train_df. Age Parch SibSp Moreover the more a passenger has parents children the older he is and the more a passenger has siblings spouses the younger he is. Calculated_Fare based on Family_Size Resource https www. Observations in a nutshell https www. pdf except astropy https docs. Completing Age https www. More Parch Older Age More SibSp Younger Age Embarked The chances for survival for Port C is highest around 0. Pre Processing Facts How Many Survived https www. com masumrumi a statistical analysis ml workflow of titanic Part 7 Modeling the Datahttps www. Pclass The upper class passengers Pclass 1 were more likely to have survived compared to second third classes Pclass 2 OR Pclass 3. 2 Outlier detection is an excellent code snippet for that based on Tukey method 1977 using IQR. com arthurtok introduction to ensembling stacking in python Interesting Resourceshttps www. Hence not considered. Pclass Pclass Survival Pclass1 highest chances of survival most survivals. Voting Classifier https www. Is_Married Feature 6. Define X and y features x independent features explanatory predictor etc. One way to solve this problem is to use a MixMaxScaler transform first to scale the data to positive values then apply the transform. To be more precise The chances of survival is good for somebody who has 1 3 parents on the ship. Children age high chance of survival. columns use ppscore Predictive Power Score PPS TO DO https www. Pre Processing Facts 3. Deal with Fare Skewness by logMany machine learning algorithms perform better when the distribution of variables is Gaussian distribution aka normal distribution. 05 max_features auto sqrt log2 RandomForestClassifier n_estimators 50 70 100 120 criterion gini entropy max_depth 2 3 4 6 8 10 None min_samples_split 2 5 10. Features Correcting Power Transforms Shapiro Wilks test https www. Test dataset range 0 6 Continous Fare Float Meaning how much each passenger paid for their journey. The PPS is an asymmetric data type agnostic score that can detect linear or non linear relationships between two columns. astype int result pd. com orblat titanic ml or blatt 8. com questions 25140998 pandas compute mean or std standard deviation over entire dataframe train_df. toarray encoded_data pd. keys return df train_df_copy concat_duplicate_columns train_df_copy test_df_copy concat_duplicate_columns test_df_copy combined_df_copy pd. Hence we ll remove Is_Married featureAlthough Sex Feature still has high VIF score I d preserve it. com parulpandey useful python libraries for data science Single Predictive Power Score Part 9 Modelling ChooseModel https scikit learn. Feature Binning https www. Calculated_Fare based on Family_Size 6. Cabin Prefix Train 5. contains B na False train_df. One of the common ways to check for multicollinearity is the Variance Inflation Factor VIF VIF 1 Very Less Multicollinearity VIF 5 Moderate Multicollinearity VIF 5 Extreme Multicollinearity This is what we have to avoid From the output we see that there s a multicollinearity problem and we can conclude the following 1. PclassParch The number of children increases with Pclass. 4 of the total training set survived the crash. add_subplot 1 2 1 corr_heatmap train_df. 1 Automagichttps scikit learn. com questions 40111161 pandas sort column by maximum values sort correlation dataframe by maximum access the last column the one with the lowest maximum vif and choose the feature to drop to achieve this vif scores pip install ppscore f plt. Maximum number of deaths were in the age group of 30 40. com vikasmalhotra08 feature binning using bayesian blocks Use of Bayesian Block in Santander Customer Transaction Prediction Challenge scott s rule Calculated_Age_bins int round train_df Age. com blog 2020 03 one hot encoding vs label encoding using scikit learn Feature Selectionhttps www. T cabin passenger has the closest resemblance to A Cabin_Prefix passengers so he is grouped with A Cabin. fit_transform X_train y_train from sklearn. org examples scatterplot_matrix. inf labels False bins_temp_with_inf remove Text Features remove Family Ticket Survival Rate features that were usued to create Survival_Rate and Survival_Rate_NA remove the features which were used to build other features we have Calculated_Fare instead https stackoverflow. Voting Classifier Part 10 SubmissionUnfortunately I discovered that I created a complex model overfitting the dataset. Introduction https www. 05 max_features auto sqrt log2 NuSVC nu 0. cut train_df Calculated_Fare bins Calculated_Fare_bins retbins True labels False bins_temp train_df Calculated_Fare bins_temp_with_inf pd. com orblat titanic ml or blatt Part 5 Data Wrangling 5. Source https www. com orblat titanic ml or blatt Part 3 About This Dataset 3. cut train_df Calculated_Fare bins Calculated_Fare_bins retbins True labels False train_calculated_fare_bins_array scotts binned histogram blocks binned histogram bins bayesian_blocks train_df Age https www. Embarked C highest survival chances Embarked S lowest survival chances EmbarkedPclass Embarked C Pclass 1 better survival chances even though the majority of Pclass1 passengers got up at S. drop columns nominal_categorical_features concatenated_df pd. html https stackoverflow. 05 max_depth 2 3 4 6 8 10 min_samples_split 2 5 10. 5 4 5 independent variables are correlated. com ldfreeman3 a data science framework to achieve 99 accuracy 3. randint with mean std range for each DataFrame to impute missing Age values. The dummy variables which are created using one hot encoding have infinite VIF. condition_alone train_df. Family_Survival_Rate is calculated from families in training set since there is no Survived feature in test set. Learn from others implement others code to form a whole that is greater that the sum of its parts. DataFrame data test_survived columns Survived 1st row as the column names submit pd. fit_transform df nominal_categorical_features. com masumrumi a statistical analysis ml workflow of titanic calculated_fare Some people have travelled in groups like family or friends. columns dupli column dupli column 1 if column in dupli. Insights and Observations Most observations were taken from here https www. This feature implies that family survival rate is not applicable to those passengers because there is no way to retrieve their survival rate. Modelling https www. preprocessing import OneHotEncoder low_dimensional_nominal_categorical_columns Pclass Embarked high_dimensional_nominal_categorical_columns Cabin_Prefix Title nominal_categorical_columns low_dimensional_nominal_categorical_columns high_dimensional_nominal_categorical_columns onehot_encoder OneHotEncoder sparse False for df in train_df_copy for column in nominal_categorical_columns reshaped_column df column. com orblat titanic ml or blatt Part 7 Feature Transformation 7. From dataset of two passengers the dataset grew to 128 passengers with correlated independent variables. Family_Size SibSp Parch 1 2. Family_Name Title https www. To understand what s the best or more percise way to deal with outliers skewness I got inspiration from Jochen Wilhelm here https www. fit_transform df feature. com questions 24458645 label encoding across multiple columns in scikit learn https www. Hence we should use Yeo Johnson Transform Yeo Johnson TransformUnlike Box Cox it supports zero values and negative values. According to the titanic tale women and children were evacuated first. com masumrumi a statistical analysis ml workflow of titanic Embarked feature The passengers with missing Embarked values have the following independent variables 1. drop i str k 1 1 df df. Multicollinearity Variance Infalction Factor VIF https www. com orblat titanic ml or blatt 6. It s important to note more predictor variables do not make a better model but the right variables. Text Name The names also contain titles. Tune Model with Hyper Parameters Source https www. outliers_influence import variance_inflation_factor https stackoverflow. They weren t counted as family but they used the same ticket. 22 Clean Data Title Correcting typos Title Grouping Rare 6. This trend is conserved when we look at both male and female passengers. At this stage I decided to re evaluate whether mean SD SEM is the right way to complete the Cabin feature. Hyperparameters Optimization Grid Search Hyperparameters Tuning Impact https www. For women the chance of survival from Pclass1 is almost 1 and is high too for those from Pclass2. Series of one passenger with no cabin df2 final_df_cabin_prefix_features_description output dicionary key for each cabin prebix with value of list of True False in range for each feature based on SD MODE 5. LabelEncoder basically labels the classes from 0 to n. Although I got a high accuracy score when it comes to training the model 0. apply changes on all dataframes Function to calculate VIF same as from statsmodels. concat objs train_df_copy test_df_copy axis 0. 9 the real prediction score turned to be 0. com gunesevitan titanic advanced feature engineering tutorial 2. org pdf physics 0605197. This process is necessary for models to learn from those features. Most Passengers are between age group 15 to 35 and died a lot. isna dataset df Cabin i 0 SyntaxError can t assign to function call method 3 https www. com 10 essential numerical summaries in statistics for data science theory python and r f3ee5e0eca32 normal distribution mean standard deviation SD no normal distribution median and range. com ash316 eda to prediction dietanic How many Survived Out of 891 passengers in training set only around 350 survived i. It s important our algorithm has not seen the subset we will use to test so it doesn t cheat by memorizing the https miro. com questions 54052471 mapping values in place for example with gender from string to int in pandas d why use global and not mutable default arguments https docs. com ldfreeman3 a data science framework to achieve 99 accuracy 5. completing 80 values. However when I tried to do so I discovered a big standard deviation of Fare for each Cabin_Prefix same for calculating median instead of mean to overcome skewness. 4 What to expect Data Wrangling Feature completing Feature correcting detect outliers handle skewness and kurtosis using power transforms and Shapiro Wilks test Feature Engineering Feature Transformation Feature binning Scott s rule Bayesian Blocks Label encoding OneHotEncoding Feautre Selection Handle multicollinearity using Variance Infalction Factor VIF Modeling Hyperparameters Optimization via Grid Search VotingClassifier Feature Engineering sneak peek Table of Contents1. Maximum passengers boarded from S. 77 Test Dataset missing values X 418 Passengers Cabin Age Fare 1 Fare missing. com max 1007 1 pIptNvUJHFiJ_lizQsxyOw. Complete missing values in Train Embarked. com gunesevitan titanic advanced feature engineering tutorialhttps www. com orblat titanic ml or blatt 9. Survived integer Target Found in train dataset only. What is the distribution of numerical feature values across the samples This helps us determine among other early insights how representative is the training dataset of the actual problem domain. copy deep True for df in test_df_copy train_df_copy combined_df_copy for feature in nominal_categorical_features encoded_feat OneHotEncoder. Big Family 4 7 people5. Data Wrangling https www. loc axis 0 slice cabin_prefix cabin_prefix sliced_description_df_by_cabin_prefix_and_feature sliced_description_df_by_cabin_prefix. com parulpandey useful python libraries for data science use chi squared statistic and the mutual information statistic https machinelearningmastery. 9 1 n_components range len train_df. 1 BackgroundAspired by a brilliant home assignment given by a data science startup and not merely any background I decided to get a hands on understanding of Machine Learning. com orblat titanic ml or blatt Shapiro Wilks test Box Cox Transform https www. keys print nCabin Prefix cabin_prefix for passengers_without_cabin_column passengers_without_cabin_value in one_passenger_without_cabin categorical_columns. com gunesevitan titanic advanced feature engineering tutorial 1. Pclass Age Sex Survival Survival chances for Passenegers aged 20 50 from Pclass1 is high and is even better for Women. 128 passengers compared to two passengers at the beginning. com cross validation 430d9a5fee22 Dabl Quick various model fitting using Dabl 9. from_frame final_df_cabin_prefix_features_description for row_index row_series in passengers_without_cabin. com yassineghouzam titanic top 4 with ensemble modeling 5. add_subplot 1 2 2 matrix_df pps. 7 1 kernel linear poly rbf sigmoid degree range 1 5 shrinking True False probability True False gamma scale auto decision_function_shape ovo ovr break_ties True False DecisionTreeClassifier criterion gini entropy splitter best random min_samples_split 2 5 10. Standard Deviation SD is a measure of data variability around mean of a sample of population. Pclass Sex Survival Sex Female Pclass 1 Survival 1 Sex Female Pclass 2 high chances of survival. com omarelgabry a journey through titanic Completing Age Titanic Advanced Feature Engineering Tutorial https www. https stackoverflow. inf retbins True labels False test_df Calculated_Fare pd. html split dataset in cross validation with this splitter class http scikit learn. reset_index drop True set train_df_copy. com helgejo an interactive data science tutorial 2. SibSp Integer Meaning the number of siblings or spouses travelling with each passenger. org generated seaborn. Features Breakdown https www. nunique print n from sklearn. sort_values https stackoverflow. preprocessing import OneHotEncoder nominal_categorical_features Cabin_Prefix Pclass Embarked Sex train_df_copy train_df. com omarelgabry a journey through titanic Part 6 Feature Engineering1. reshape len integer_encoded 1 onehot_encoded onehot_encoder. Acquire training and testing data https www. Since all available information 1 http users. fit train_df train_df_x_calc train_df Target result best_model_for_submission. A list of family names that are occuring in both training and test set non_unique_families is created. SexPclassSurvival Sex Female Pclass 1 2 Survival 1 SexAge Age distribution seems to be the same in Male and Female subpopulations so Sex is not informative to predict Age. This is important so we don t overfit our model. Multicollinearity Variance Infalction Factor VIF 8. Complete Cabin Cabin Prefix 5. com ash316 eda to prediction dietanic and here https www. com questions 42718870 defining a default argument as a global variable https stackoverflow. max train_df Calculated_Fare. Aside from ML being a buzzword I discovered a whole world thanks to Kaggle. I d love to fix this. A Journey through Titanic https www. com orblat titanic ml or blatt Part 11 Credits and Resources Part 2 Question Problem Definition Find out who survived the titanic incident. edu gmeeden papers hist. Then the dataset of similar passengers will better represent the passengers with missing Embarked values. 05 max_features auto sqrt log2 ExtraTreeClassifier criterion gini entropy splitter best random min_samples_split 2 5 10. Age 38 temp_df. heatmap matrix_df cmap BrBG annot True from sklearn. cut train_df Age bins Calculated_Age_bins retbins True labels False train_df Age bins_temp_with_inf pd. SexAgeSurvival For males the survival chances decreases with an increase in age. items mode_of_categorical_variable get_mode_of_categorical_variable df final_df_cabin_prefix_features_description cabin_prefix cabin_prefix df_column passengers_without_cabin_column is_correlated_feature mode_of_categorical_variable passengers_without_cabin_value temp_boolean_list. Completing Train Embarked 5. Hence I ll convert this binary feature to a nominal feature adding child as an option. Credits and Resources https www. com max 351 1 D8B5_HlEfwSQURgQkymoBA. com startupsci titanic data science solutionshttps www. some titles can indicate a certain age group. com masumrumi a statistical analysis ml workflow of titanic Embarked feature Resource2 https www. Family TitleAll text and code derived from this excellent Resource https www. Feature Correcting Power Transforms https www. It is a binary nominal datatype of 1 for survived and 0 for did not survive. inf retbins True labels False test_df Family_Size pd. com data types from a machine learning perspective with examples 111ac679e8bc Most data can be categorized into 4 basic types from a Machine Learning perspective numerical data categorical data time series data and text. PclassAge 1st class passengers are older than 2nd class passengers who are also older than 3rd class passengers. Insights and Observations 3. com max 501 1 lheLiN7y4sSD2JKvow clw. Feature BinningHandling Continious Features Grouping Binning Banding https miro. com orblat titanic ml or blatt Part 8 Feature Selection 8. png Source https towardsdatascience. Binary Sex string Meaning an indicator whether the passenger was female or male. Complete Cabin Prefix Test 5. All other variables are potential predictor or independent variables. Sex The chance of survival for women is high as compared to men. inf labels False bins_temp_with_inf using Bayesian_blocks train_df Calculated_Fare train_calculated_fare_bins_array pd. R random_state None 42 BaggingClassifier n_estimators range 10 20 bootstrap True False bootstrap_features True False oob_score True False warm_start True False random_state None 42 ExtraTreesClassifier n_estimators 50 70 100 120 criterion gini entropy max_depth 2 4 None min_samples_split 2 5 min_samples_leaf 1 2 max_features auto sqrt log2 GradientBoostingClassifier loss deviance exponential n_estimators 50 70 100 120 criterion friedman_mse mse mae max_depth 2 4 None min_samples_split 2 5 min_samples_leaf 1 2 max_features auto sqrt log2 RandomForestClassifier n_estimators 50 70 100 120 criterion gini entropy max_depth 2 4 None min_samples_split 2 5 min_samples_leaf 1 2 max_features auto sqrt log2 GaussianProcessClassifier multi_class one_vs_rest one_vs_one warm_start True False LogisticRegressionCV fit_intercept True False cv 3 5 7 10 dual True False penalty l1 l2 elasticnet solver newton cg lbfgs liblinear sag saga multi_class auto ovr multinomial PassiveAggressiveClassifier fit_intercept True False warm_start True False RidgeClassifierCV SGDClassifier penalty l2 l1 elasticnet average True False Perceptron penalty l2 l1 elasticnet warm_start True False BernoulliNB GaussianNB KNeighborsClassifier weights uniform distance algorithm auto ball_tree kd_tree brute leaf_size range 20 40 5 SVC kernel linear poly rbf sigmoid precomputed degree range 1 5 gamma scale auto shrinking True False probability True False NuSVC nu list float_range 0 1. Parch SibSp Having 1 2 siblings spouse on board or 1 3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with you. Since I used Power Transforms Yeo Johnsons on age and Calculated_Fare I assume that there s a big difference between Scott s rule bin size and Bayesian blocks bin size. com parulpandey useful python libraries for data science Github https github. shape 0 encoded_df. Pclass3 lowest chances of survival most passaengers. com orblat titanic ml or blatt 5. EmbarkedSex Port Q looks looks to be unlukiest for Men as almost all were from Pclass 3 which is known for low chances of survival for as compared to other Pclasses. Feature Selection https www. keys print nCabin Prefix cabin_prefix for passengers_without_cabin_column passengers_without_cabin_value in one_passenger_without_cabin numerical_columns. 1 Label Encoding Non Numerical FeaturesEmbarked Sex Deck Title and Family_Size_Grouped are object type and Age and Fare features are category type. 0 correlation to another model run model 10x with 60 30 split intentionally leaving out 10 Ensemble Methods Gaussian Processes Navies Bayes Nearest Neighbor xgboost http xgboost. com 2013 09 08 basic feature engineering with the titanic data Family_Size is created by adding SibSp Parch and 1. S port used by most passengers top S Cabin values have several dupicates across samples. Feel free to suggest how I can fix this. The Survived variable is our outcome or dependent variable https www. com arthurtok introduction to ensembling stacking in pythonhttps www. com questions 17679089 pandas dataframe groupby two columns and get counts 17679517 17679517 https stackoverflow. We can explore this by adding a StandardScaler as a first step in the pipeline. completing 20 values vs. Feature Transformation https www. 05 max_features auto sqrt log2 parameters_dict AdaBoostClassifier n_estimators range 50 100 10 learning_rate list float_range 0 1. Embarked The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S. Then we could identify how much the hyperparameters helped to increase the model score. 3 Title Is Married 6. 21 The 4 C s of Data Cleaning Correcting Completing Creating and Converting Feature Completing Resource https www. To achieve Gaussian distribution we ll use and compare two power transforms Yeo Johnson Transform vs. Predictive Power Score WIP 9. How is this feature different than Family_Size Many passengers travelled along with groups. com orblat titanic ml or blatt Part 6 Creating Feature Engineering 6. concat data_other_cols encoded_data axis 1 return concatenated_df train_df_copy onehotencode_dataframe train_df_copy nominal_categorical_features train_df_copy. com orblat titanic ml or blatt Title Correcting and Grouping Rare 6. items del dupli key if val 1 for i in range val dupli key str i val else dupli key 1 df. Numerical Definition Numerical data is any data where data points are exact numbers. html removed models w o attribute predict_proba required for vote classifier and models with a 1. Define X and y features 9. We can use a Pipeline object to apply both transforms in sequenceBox Cox Transform managed to deal with Age but not with Fare due to zero values. std Calculated_Age_bins _ bins_temp pd. The oldest Passengers were saved 80 years. Ticket_Frequency Ticket groupby transform count 3. cut test_df Age bins np. gov pmc articles PMC2959222 2 http www. Features Correcting Power Transforms Fare and Age features has outliers are skewed. Ticket_Frequency https www. format feature n for n in range 1 n 1 encoded_df pd. Family_Survival_Rate Ticket_Survival_Rate https www. extract_surname function is used for extracting surnames of passengers from the Name feature. Multicollinearity occurs where there is a dependency between the independent features. org pandas docs stable generated pandas. Survival_Rate Survival_Rate_NA 6. com a 56780852 Ensemble Methods Gaussian Processes Navies Bayes Nearest Neighbor Discriminant Analysis xgboost http xgboost. 2 GoalExplore Learn and apply best practices in various machine learning fields. Label Encoding https www. Most passengers 75 did not travel with parents or children. Note that Grid Search is a slow method. Fare 80 passengers_representing_those_with_missing_embarked1 passengers_representing_those_with_missing_embarked2 temp_df condition_female condition_pclass1 condition_age_38_to_62 condition_fare condition_pclass1 condition_female condition_age_38_to_62 condition_fare condition_age_38_to_62 condition_female condition_pclass1 condition_fare condition_fare condition_female condition_pclass1 condition_age_38_to_62 fillna inplace doesn t work https stackoverflow. To tackle this I decided to plot the Fare range for each Cabin_prefix but Fare is too Skewed and no insights could be concluded. com masumrumi a statistical analysis ml workflow of titanic Cabin Feature one way to complete Cabin feature is by calculating the mean Fare of each Cabin_Prefix. However for those who still want to detect and remove outliers here https www. to_file train_data_pandas_profiling. Cabin string object Meaning an alphanumeric representation of the cabin number of each passenger. Around 38 samples survived representative of the actual survival rate at 32. Complete missing values in Train Test Age. Time Series Datanone. 95 of the Passengers at Q were from Pclass3. Nearly 30 of the passengers had siblings and or spouse aboard. Part 3 About This Dataset 3. ca epidemiology hanley IntMedResidents SD SE. com power transforms with scikit learn will make the probability distribution of a variable more Gaussian. Observations in a nutshell for all features 4. An extra binary feature Family_Survival_Rate_NA is created for families that are unique to the test set. Decision find out which passengers share the same cabin. Correct outliers and skewnessIn several notebooks I saw some that detect and remove outliers whereas others reduce skewness. org stable modules generated sklearn. resource https machinelearningmastery. Pclass 1 temp_df. html ExtraTreesClassifier n_estimators 50 70 100 120 criterion gini entropy max_depth 2 4 6 8 10 None min_samples_split 2 5 10. Cabin i 0 for i in all_data. 4 Cabin Titanic_Deck https vignette. Already mapped from categories to numerical values. Features breakdown Resource https towardsdatascience. Spoiler C it is 5. com analytics vidhya data scientists stop randomly binning histograms 1069d7380c3a I could find is mostly theoretical and not implemented python wise I d appreciate if anyone can enlighten me on this https miro. Predictive Power Score WIP https www. index encoded_features. Label Encoding 7. Sometimes a lift in performance can be achieved by first standardizing the raw dataset prior to performing a Yeo Johnson transform. com masumrumi a statistical analysis ml workflow of titanic Embarked feature Completing missing features and creating calculated_fare A Journey through Titanic https www. 3 5 independent variables are correlated. Feature Binning Bayesian Blocks https www. Install Libraries Part 4 Acquire training and testing data Dabl Pandas_profiling for quick insights Import Libraries Part 5 Data Wrangling Great resource for the 4 Cs https www. Still the chances for survival is low here that is because many passengers from Pclass3 around 81 didn t survive. 12 Tune Model with Hyper Parameters Hyperparameters Tuning Impact 9. Hyperparameters Optimization Grid Search We ll first fit each basic algorithm then iterate through various parameters for each algorithm and compare the results. Correct typos in Mlle Miss Ms Miss Mme Mrs 2. 1 Continious Features BinningFeature Binning Conversion of a continuous variable to categorical Types of Binning Unsupervised Binning Equal width binning Equal frequency binning adaptive binning fixed binning Supervised Binning Entropy based binningLet s use bayesian_blocks to determine bin size and then bin age Calculated_Fare and Family_Size. This data has meaning as a measurement Numerical data can be characterized into continuous or discrete data. Is_Married https www. com orblat titanic ml or blatt Box Cox Transform Yeo Johnson Transform https www. the survival for families with 5 8 members is 0 all the large families in Pclass3 3 died. net titanic images f f9 Titanic_side_plan. Those columns are added in order to find the total size of families. Shapiro Wilks testThe null hypothesis for Shapiro Wilks test is that the data is a sample from a normal distribution so a p value less than 0. Observations in a nutshell for all featuresMainly taken from here https www. Embarked S majority of the rich people boarded and majority of Pclass3 passengers boarded. This is often described as removing a skew in the distribution. Yeo Johnson Transform handled skewness better hence we ll use it. Pclass 1 train_df. Ticket_Survival_Rate and Ticket_Survival_Rate_NA features are also created with the same method. cut train_df Calculated_Fare bins np. com questions 13610074 is there a rule of thumb for how to divide a dataset into training and validatio 13623707 13623707Feature ranking with recursive feature elimination and cross validated selection of the best number of features. profile_report profile. cut train_df Family_Size bins np. com power transforms with scikit learn resource2 https www. com headsortails pytanic https www. Feature Engineering https www. Complete Test Fare Instead of imputing the missing value from the fare median for all passengers in train test datasets I decided to narrow down the list via correlation of independant variables of the sole passenger who has NaN Fare. Please comment this notebook and submit your suggestions Change Log21 02 21 Kernel is open to feedback. fit_transform X_train y_train from category_encoders import WOEEncoder enc WOEEncoder cols Name_of_col Another_name training_set enc. Ticket_Frequency Ordinal Numeric Feature 6. columns sorted set re. com questions 10212445 python map list item to function with arguments condition for each independant variable originally 80 I used 60 100 range for variance. com headsortails pytanic Train Dataset missing values X 891 Passengers Cabin Age Embarked 2 Embarked values are missing. png revision latest cb 20180322183733 Cabin Outlier CorrectingAccording to Titanic Advanced Feature Engineering Tutorial https www. Being alone also proves to be fatal and the chances for survival decreases when somebody has 4 parents on the ship. apply function_to_apply return list_of_dataframes train_df test_df combined_df apply_to_train_test_combined_dataframes mask_boolean_indexing_of_sex https stackoverflow. cut Family_SizeAs far as I understand this means that the bins are 1. Split Training data in two datasets anti overfitting 9. com orblat titanic ml or blatt Part 4 Acquire training and testing data 5. mean 100 print base_results test_score. Outliers Fares varied significantly with few passengers 1 paying as high as 512USDs. Family_Size https www. 3292 USD Age Float Test dataset range 0. cut test_df Family_Size bins np. copy deep True a onehot_encoder. com What is seed in random number generation pip install dabl Machine Learning Algorithm MLA Selection and Initialization Ensemble Methods Gaussian Processes Navies Bayes Nearest Neighbor Discriminant Analysis xgboost http xgboost. DataFrame transformed_data index df. concat test_df_copy encoded_df len nominal_categorical_features axis 1 import re def concat_duplicate_columns df dupli for column in df. Pclass Integer Meaning represents social economic status SES. net post Outlier_and_skewness_effect_on_Normality_and_homogeneity_of_variance_testing The bottom line Removing outliers is only sensible if these values are bad values that is when they are extremely implausible Since the outliers of Age and Fare values are plausible and since we want to provide certain models with features represented by normal distribution I decided to opt for skewness reduction. Complete Test Fare 5. append encoded_df test_df_copy pd. describe include all for mode Create two dataframes for passengers with cabin each dataframe for numerical categorical set of features https stackoverflow. com max 1050 1 MAr4rWj6zw0Rdo01ecZu1A. Predictive Power Score WIP Predictive Power Score PPS. Meaning Port of Embarkation Options S Q C Southampton Queenstown Cherbourg. Alternatively passengers shared a group purchase. max train_df Age. com vikasmalhotra08 feature binning using bayesian blocks Use of Bayesian Block in Santander Customer Transaction Prediction Challenge color black _ bins_temp pd. For those who have seen the Titanic movie 1997 I am sure we all remember this sentence during the evacuation Women and children first. 3 Strategy No exploratory data analysis. Ticket_Survival_Rate and Family_Survival_Rate are averaged and become Survival_Rate and Ticket_Survival_Rate_NA and Family_Survival_Rate_NA are also averaged and become Survival_Rate_NA. nunique cols _. com a 56780852 prettify using pyplot https matplotlib. fit_transform reshaped_column label_encoder LabelEncoder integer_encoded label_encoder. Insights and Observations https www. Test dataset range 0 512. 23 Convert Formats 9. org writing gotchas mutable default arguments https stackoverflow. Passenger who travel together may have purchased tickets with the same prefix. com kaggle docker python linear algebra data processing CSV file I O e. Ticket string object Meaning an alphanumeric representation of the ticket number of each passenger. The survival rate is calculated for families with more than 1 members in that list and stored in Family_Survival_Rate feature. Split Training data in two datasets anti overfitting https www. com orblat titanic ml or blatt Feature Binning Bayesian Blocks 7. 5 Feature Transformation 2. 2 Setup helper FunctionsClassifiersEnsamblingVotingConfusion matrixcross_validation avoid overfitting underfitting CV use cross validation for small datasets. concat PassengerId result axis 1 submit 1st row as the column names. Needs to be predicted in test dataset. read_csv list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session ignore warnings to be used for submission import pandas_profiling profile train_df. The mode operation will be used to impute the missing embarked values based on a larger dataset. Power TransformsA power transform 1 https en. The Dummy Variable Trap leads to the problem known as multicollinearity. inf retbins True labels False test_df Age pd. Feature Correcting Detect outliers Fare Reduce skewness Box Cox Yeo Johnson Power Transforms Visualize distribution of missing values using missingnoFound in an amazing notebook https www. shape 0 1 3 3. 2 Frequency Encoding Resource 2 https www. iterrows pass def get_estimated_letter_bundle pass def get_mode_of_categorical_variable df final_df_cabin_prefix_features_description cabin_prefix cabin_prefix df_column passengers_without_cabin_column slice a pd. cut train_df Family_Size bins Family_Size_bins retbins True labels False train_df Family_Size bins_temp_with_inf pd. std Calculated_Fare_bins _ bins_temp pd. describe def get_mean_and_std_of_numerical_variable df final_df_cabin_prefix_features_description cabin_prefix cabin_prefix df_column passengers_without_cabin_column slice a pd. com vikasmalhotra08 feature binning using bayesian blocks Use of Bayesian Block in Santander Customer Transaction Prediction Challenge using Scott s rule Calculated_Fare_bins int round train_df Calculated_Fare. OneHotEncoding https www. merge filtered_df_of_passengers_with_cabin_mean_std_numeric description_of_passengers_with_cabin_categorical_features on Cabin_Prefix how inner. com feature selection with categorical data define y variable aka target outcome define x variables for original features aka feature selection data1_x Sex Age SibSp Parch Family_Size Calculated_Fare Ticket_Frequency Survival_Rate Survival_Rate_NA Pclass Embarked Cabin_Prefix Title pretty name values for charts data1_x_calc list coded for algorithm calculation random_state seed or control random number generator https www. OneHotEncoding Part 8 Feature Selection 8. SummaryIn summary we have 11 12 test train features 4 numerical features Fare Age SibSp Parch 3 text features Name Cabin Ticket 4 catrgorical features Embarked Pclass Sex Survived and 1 useless feature PassengerID 2 Numerical Continous features as float datatype Fare Age 2 Numerical Discrete Ordinal features as integer datatype SibSp Parch 3 Text features as string object dataype Name Cabin Ticket 1 Categorical Nominal features as character datatype Embarked 1 Categorical ordinal features as integer datatype Pclass. Hyperparameters Optimization Grid Search https www. com startupsci titanic data science solutions some observations were rephrased. Voting Classifier 10. com orblat titanic ml or blatt Yeo Johnson Transform 7. Sex female boolean indexing error def apply_to_train_test_combined_dataframes function_to_apply list_of_dataframes train_df test_df combined_df for df in list_of_dataframes df df. 05 max_features auto sqrt log2 bootstrap True False oob_score True False warm_start True False GradientBoostingClassifier loss deviance exponential n_estimators range 10 150 30 criterion friedman_mse mse mae min_samples_split 2 5 10. 15 died a lot. Now let us drop one of the dummy variables to solve the multicollinearity issue Summary of Multicollinary Changes based on VIF decrease and feature removalWe can see that Title_4 and Is_Married create multicullinarity. Calculated_Fare https www. isnull missing_embarked_value update combined_df with missing embarked value checks that changes were saved correctly condition for each independant variable male reasonable range for age 60. com yassineghouzam titanic top 4 with ensemble modeling Fare Reduce Fare and Age skewness using Shapiro s law and power transforms Multicollinearity Variance Infalction Factor VIF https www. Family feature is created with the extracted surname. com questions 45003806 python pandas use slice with describe versions greater than 0 20 train_df train_df Cabin_Prefix. Part 1 Introduction 1. Oldest passengers Age 80 survived. Adding 1 at the end is the current passenger. This notebook contains a mixture of lessons I ve learned from various notebooks as well as on my own. It seems like Fare column kept a record of the total fare rather than the fare of individual passenger therefore calculated fare will be much handy in this situation. Completing Cabin Cabin Prefix https www. Continuous data can assume any value within a range whereas discrete data has distinct values. html Scott s rule https medium. Define X and y features https www. columns axis 1 change reindex_axis to reindex for kaggle compatibility df. index data_other_cols df. Ticket feature has high ratio 22 of duplicate values unique 681. toarray n df feature. com questions 10665889 how to take column slices of dataframe in pandas 44736467 filter frequency of features based on mean std for numerical features mode freq for categorical features passengers_with_cabin final_df_cabin_prefix_features_description pd. html This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. 4 Target Encoding Creating survival_rate and Survival_Rate_NA Titanic Top 4 with ensemble modeling https www. Cabin https stackoverflow. 4 Cabin there is one person on the boat deck in T cabin and he is a 1st class passenger. com ResidentMario missingno 5. 1 86 Age values are missing. Completing Fare https www. join count_df_of_passengers_with_cabin_per_prefix final_df_cabin_prefix_features_description pd. html why choose one model when you can pick them all with voting classifier http scikit learn. Complete Age Train Test DataFrames 6. fit_transform col concatenated_df df transformed_data onehotencoder. DataFrame data result columns Survived 1st row as the column names submit pd. 2 Meet and Greet Data. This feature is also necessary because there is no way to calculate those families survival rate. std 100 3 best_model_for_submission vote_hard test_survived best_model_for_submission. html Hard Vote or majority rules Soft Vote or weighted probabilities best_model_for_submission ensemble. html Passenger in the T deck is changed to A https stackoverflow. 05 max_depth 2 3 4 6 8 10 None min_samples_split 2 5 10. y dependent target outcome response etc Source https www. com yassineghouzam titanic top 4 with ensemble modeling 2. 3 Ticket classes 1 First 2 Second 3 Third. Apparantly Standard Deviation SD and Standard Error of Mean SE SEM are used interchangeably by mistake in various medical journals 1 https www. com ldfreeman3 a data science framework to achieve 99 accuracy Huge inspiration for wrangling data completing missing features and modeling A Statistical Analysis ML workflow of Titanic https www. com questions 17114904 python pandas replacing strings in dataframe with numbers https stackoverflow. Complete Cabin Cabin PrefixAccording to A Statistical Analysis ML workflow of Titanic https www. Hoping that I haven t fallen to this pit this is the summary The Standard Error of Mean SEM is a measure of precision for an estimated population mean. 1 Catrgorical Binary feature as string object datatype Sex To be converted to 3 options nominal feature male female child 1 running index with no meaning PassengerID 1 Catrgorical Binary feature as integer datatype Survived 3. 1 177 Age values are missing. Money Wins PclassAge Survival Infant Toddlers age 4 passengers in Pclass 2 and Pclass 3 mostly survived. Family Title Title Correcting Grouping https www. Family_Size Ordinal Numeric Feature Resource1 https www. png Resource https towardsdatascience. append is_correlated_feature print Mode mode_of_categorical_variable print Passenger s value passengers_without_cabin_value print is_correlated_feature return temp_boolean_list def get_boolean_list_for_numerical_correlation_of_passenger_value_mean_and_std one_passenger_without_cabin row_series cabin_prefix_dictionary boolean_dictionary_per_cabin_prefix final_df_cabin_prefix_features_description final_df_cabin_prefix_features_description print get_boolean_list_for_numerical_correlation_of_passenger_value_mean_and_std n print 40 temp_boolean_list numerical_columns Age SibSp Parch Fare for cabin_prefix in cabin_prefix_dictionary. jpeg Discrete Ordinal Definition Ordinal feature means its values may be arranged in some order that makes logical sense. keys for i in set re. There are other methods to explore such as Bayesian Optimization Particle based methods Convex optimizers and Simulated annealing 9. Parch SibSp Parch SibSp Survival SibSp 1 2 U Parch 1 3 alone U large family. com max 700 1 n QnKwkk_hxNihfAYxfAzg. preprocessing import OneHotEncoder LabelEncoder onehot_encoder OneHotEncoder sparse False temp_train_df train_df_copy. Completing Train Embarked Resource1 https www. inf labels False bins_temp_with_inf scotts binned histogram blocks binned histogram https www. Models without Hyperparameters 9. Complete missing values in Train Test Cabin. com orblat titanic ml or blatt Part 10 Submission 11. com orblat titanic ml or blatt Part 1 Introduction 2. For each missing Cabin the value will be imputed by a cabin_estimator function based on a Fare range of each Cabin_Prefix. drop columns feature cols 0 inplace True use this to avoid multicollinarity without checking VIF as I am about to do. Reduce skewness in Test Data Part 7 Feature Transformation 1 https www. com articles pandas find rows where columnfield is null importing the library don t forget to install pip install missingno train_df train_df. fit_transform temp_train_df Pclass. Total samples are 891 or 40 of the actual number of passengers on board the Titanic 2 224. Thanks Part 11 Credits and Resources A Data Science Framework To Achieve 99 Accuracy https www. org en stable visualization histogram. loc axis 1 df_column mean_of_numerical_variable sliced_description_df_by_cabin_prefix_and_feature mean 0 std_of_numerical_variable sliced_description_df_by_cabin_prefix_and_feature std 0 return mean_of_numerical_variable std_of_numerical_variable def get_mean_plus_minus_std mean mean_of_numerical_variable std std_of_numerical_variable return round mean std round mean std round for Parch and SibSp def get_boolean_list_for_categorical_correlation_of_passenger_value_and_mode one_passenger_without_cabin row_series cabin_prefix_dictionary boolean_dictionary_per_cabin_prefix final_df_cabin_prefix_features_description final_df_cabin_prefix_features_description print get_boolean_list_for_categorical_correlation_of_passenger_value_and_mode n print 40 temp_boolean_list categorical_columns Sex Embarked Pclass for cabin_prefix in cabin_prefix_dictionary. GradientBoostingClassifier criterion mae loss deviance max_depth 4 max_features sqrt min_samples_leaf 2 min_samples_split 5 n_estimators 100 best_model_for_submission. 25 Split Training and Testing Data use sklearn function to split the training data in two datasets 75 25 split. com orblat titanic ml or blatt 3. com masumrumi a statistical analysis ml workflow of titanic comments all_data. com helgejo an interactive data science tutorialhttps www. 49 train_df Calculated_Fare. Parch and SibSp have slightly more than 5 VIF. describe by Cabin_Prefix and by feature to get the mode of the categorical feature for a specific Cabin_Prefix sliced_description_df_by_cabin_prefix df. Group rare title names. cut train_df Age bins np. png Feature Binning Bayesian Blocks pd. Family_Size Ordinal Numeric Feature 6. copy deep True test_df_copy test_df. reshape 1 1 n temp_train_df Pclass. What is the distribution of categorical features Names are unique across the dataset count unique 891 Sex variable as two possible values with 65 male top male freq 577 count 891. com orblat titanic ml or blatt Hyperparameters Tuning Impact 9. com blog 2020 03 one hot encoding vs label encoding using scikit learn Dummy Variable Trap is a scenario in which variables are highly correlated to each other. Question Problem Definition https www. Alternatively several passengers shared a cabin. cross_validate if this is a non bias random sample then 3 standard deviations std from the mean should statistically capture 99. keys else 1 for key val in dict dupli. We ll find the best one to remove and then check VIF again2. In this notebook I will include techniques for Feature Engineering ML twicking Hyperparamaters Ensambling etc. This is a categorical nominal feature that was already mapped to numerical values. They are converted to numerical type with LabelEncoder. Features breakdown 3. com yassineghouzam titanic top 4 with ensemble modeling Fare Fare distribution is very skewedThis can lead to overweigth very high values in the model even if it is scaled. Is_Married FeatureCalculate Is_Married boolean value based on Title of Mrs. groupby Cabin_Prefix. descriptive statistics resource https towardsdatascience. fit train_df train_df_x_calc train_df Target print base_results train_score. com feature engineering deep dive into encoding and binning techniques 5618d55a6b38 Continious features need to be binned and then encoded in order to be used by ML algorithms. com python10pm pandas 100 tricks https www. astype str df i str k 1. It can be used as an alternative to the correlation matrix. Passengers at Q were all from Pclass3. 4 Ticket Resource 3 https www. 55 while it is lowest for S. Those groups consist of friends nannies maids and etc. The score ranges from 0 no predictive power to 1 perfect predictive power. Describe datasetsfor each passenger with no cabin check if his independant variables match multiple values correlated with those passengers with cabin per cabin_prefixfor passenger_no_cabin in dataframe_of_passengers_with_no_cabin function function input series output Cabin_Prefix estimation check if the passenger has independant variables that fit each cabin prefix for each independant variable execute function input pd. 20 687 Cabin values are missing large majority of Cabin numbers are missing. Techniques for completing of 1 values vs. fillna missing_embarked_value inplace True train_df train_df Embarked. pivot columns x index y values ppscore sns. Sex and Family_Size have much higher VIF than 5. preprocessing import OneHotEncoder LabelEncoder onehotencoder OneHotEncoder nominal_categorical_features Cabin_Prefix Pclass Embarked Sex train_df_copy train_df. Complete Cabin by mean estimators grouping binning banding Completing techniques WIP 1. Data Analysis Insights https www. Resource https www. Huge Family 8 12 people Cleaning things up pre feature selection 7. 4 Target Encoding There are too many unique Ticket values to analyze so grouping them up by their frequencies makes things easier. symmetric_difference train_df. Some persons might share the same surname indicating family relations. get_dummies train_df_copy columns nominal_categorical_features drop_first True dummies from category_encoders import TargetEncoder enc TargetEncoder cols Name_of_col Another_name training_set enc. ShuffleSplit note this is an alternative to train_test_split run model 10x with 60 30 split intentionally leaving out 10 create table to compare MLA metrics create table to compare MLA predictions index through MLA and save performance to table set name and parameters score model with cross validation http scikit learn. Focus on sophisticated feature engineering to create the most optimal dataset. We may be able to solve these two missing values by looking at other independent variables of the two rows. com orblat titanic ml or blatt Part 2 Question Problem Definition 3. Complete missing value in Test Fare. j for j in dupli. There are two popular approaches for automatic power transforms they are Box Cox Transform Yeo Johnson Transform Box Cox Transformassumes positive values 0 and negative values are not supported. com questions 21998354 pandas wont fillna inplace train_df. DataFrame encoded_feat columns cols print encoded_df. html barplot using https seaborn. figure figsize 16 8 f. Pre Processing Facts https www. Completing Embarked https www. keys if in j i str i for k in range dupli i 0 1 df i 0 df i 0. com orblat titanic ml or blatt Part 9 Modelling 9. 371 Passengers share these commonalities checks that changes were saved correctly method 1 method 2 df Cabin_Prefix. Sex_is_Female 1 temp_df. 2 Frequency Encoding Resource2 https triangleinequality. Useless PassengerID Integer A running index for the dataset no true numerical or categorical meaning. head from sklearn. Title Correcting and Grouping Rare1. Age AgeSurvival Infant Toddlers age 4 were saved in large numbers irrespective of the class The Women and Child First Policy. loc axis 1 df_column return sliced_description_df_by_cabin_prefix_and_feature top 0 top equalls mode for pd. 05 min_samples_leaf 1 5 10. cross_validate best_model_for_submission train_df train_df_x_calc train_df Target cv cv_split return_train_score True best_model_for_submission. i for i in dupli. pdf 2 https arxiv. This is necessary for grouping passengers in the same family. astype str df df. com orblat titanic ml or blatt 7. org wiki Power_transform 2 https machinelearningmastery. Ticket_Frequency Ordinal Numeric Feature Resource1 https www. 21 327 Cabin values are missing only 91 Cabin numbers were preserved. com sinakhorami titanic best working classifier Classifier Comparisonhttps www. contains B na False temp_df. org stable _static ml_map. copy deep True le LabelEncoder def onehotencode_dataframe df nominal_categorical_features nominal_categorical_features le le df nominal_categorical_features df nominal_categorical_features. ", "id": "orblat/titanic-ml-or-blatt", "size": "39184", "language": "python", "html_url": "https://www.kaggle.com/code/orblat/titanic-ml-or-blatt", "git_url": "https://www.kaggle.com/code/orblat/titanic-ml-or-blatt", "script": "get_array_of_boolean_correlations_for_each_feature_per_cabin_prefix linear_model heatmap pyplot correlate_onehotencoded_feature_to_drop drop_features_bulk_dataframes _show_on_single_plot = train_df.Pclass.map({1 get_key_with_max_values get_columns_containing_null GaussianNB sklearn.neighbors TargetEncoder sklearn.linear_model neighbors matplotlib.pyplot apply_to_multiple_dataframes create_is_married missingno Perceptron sklearn.pipeline scipy.stats Pipeline collections get_boolean_list_for_categorical_correlation_of_passenger_value_and_mode pandas.plotting OneHotEncoder ppscore statsmodels.stats.outliers_influence sklearn.svm XGBClassifier SGDClassifier show_values_on_bars apply_to_train_test_combined_dataframes category_encoders sklearn.naive_bayes astropy.stats calculate_vif gaussian_process sklearn random get_mean_plus_minus_std MinMaxScaler get_estimated_letter_bundle clean_rare_title_names svm pandas get_mean_and_std_of_numerical_variable onehotencode_dataframe LogisticRegression model_selection matplotlib find_best_features_to_drop_for_best_multicollinarity create_family_size_feature variance_inflation_factor find_onehotencoded_feature_to_drop create_cabin_prefix_column sklearn.ensemble create_calculated_fare_feature numpy LinearSVC tree sklearn.tree DecisionTreeClassifier one_hot_encode_dataframe get_list_of_keys_with_max_values shapiro RandomForestClassifier scatter_matrix LabelEncoder discriminant_analysis StandardScaler WOEEncoder Counter concat_duplicate_columns bayesian_blocks get_estimated_letter_based_on_boolean_array float_range xgboost ensemble get_mode_of_categorical_variable seaborn correct_title_names mask_boolean_indexing_of_sex = test_df.Pclass.map({1 SVC corr_heatmap tqdm choose_random_key_from_keys_with_max_values statsmodels.api LeaveOneOutEncoder PowerTransformer naive_bayes KNeighborsClassifier get_boolean_list_for_numerical_correlation_of_passenger_value_mean_and_std extract_families_and_titles_from_name sklearn.preprocessing ", "entities": "(('com analysis ml statistical workflow', 'Cabin_Prefix'), 'masumrumi') (('Names', '65 male top male freq'), 'be') (('histogram blocks', 'histogram https www'), 'bin') (('that', 'execute function input independant variable pd'), 'correlate') (('Methods Gaussian Processes Navies Bayes Nearest Neighbor intentionally 10 Ensemble xgboost', 'xgboost'), 'correlation') (('he', 'Cabin'), 'have') (('We', 'zero values'), 'use') (('column names', 'pd'), 'column') (('Family_Size', 'SibSp Parch'), 'com') (('Standard Deviation Apparantly SD', 'https various medical journals 1 www'), 'use') (('changes', 'commonalities checks'), 'share') (('voting classifier http scikit', 'them'), 'learn') (('Age AgeSurvival Infant Toddlers', 'irrespective class'), 'save') (('fit_transform reshaped_column label_encoder LabelEncoder', 'label_encoder'), 'integer_encoded') (('Total samples', 'board'), 'be') (('variables', 'highly other'), 'learn') (('Family_Survival_Rate', 'Survived test set'), 'calculate') (('isna Cabin 0 SyntaxError', 'call method https 3 www'), 'dataset') (('Sex Feature', 'I it'), 'remove') (('Alone SibSp s who', 'above mentioned ones'), 'Parch') (('even it', 'model'), 'be') (('inf', 'Age False pd'), 'retbin') (('S port', 'samples'), 'have') (('Cabin_Prefix Pclass', 'Sex train_df_copy train_df'), 'preprocesse') (('oob_score GradientBoostingClassifier loss False True False exponential n_estimators', 'True False'), 'bootstrap') (('non bias random then 3 standard deviations', 'statistically 99'), 'capture') (('cabin', 'SD MODE'), 'prebix') (('Most Passengers', '15 35'), 'be') (('l1 l2 elasticnet solver newton lbfgs liblinear sag saga multi_class auto ovr PassiveAggressiveClassifier fit_intercept True True SGDClassifier penalty l2 l1 elasticnet True Perceptron penalty l2 l1 elasticnet True BernoulliNB GaussianNB weights uniform distance algorithm auto brute False False leaf_size', 'True False list'), 'None') (('Tune 12 Model', 'Impact'), 'Tuning') (('print cabin_prefix_dictionary n', 'cabin_prefix_dictionary'), 'print') (('com questions', '21998354 inplace'), 'fillna') (('DataFrame', 'Age values'), 'randint') (('i', 'else key 1 df'), 'key') (('I', 'skewness reduction'), 'post') (('multicollinearity we', 'following 1'), 'be') (('Outliers Fares', '1 as high 512USDs'), 'vary') (('Completing', 'https Titanic www'), 'masumrumi') (('I', 'Jochen Wilhelm'), 'get') (('hyperparameters', 'model score'), 'identify') (('rather large family', 'you'), 'show') (('list_of_dataframes df', 'df'), 'def') (('2 Pclass', 'Pclass'), 'win') (('We', 'then VIF'), 'find') (('data type agnostic asymmetric that', 'non linear two columns'), 'be') (('Fare', 'notebook https amazing www'), 'reduce') (('4', 'crash'), 'survive') (('chance', 'men'), 'sex') (('com power transforms', 'variable more Gaussian'), 'make') (('t', 'submission import pandas_profiling profile'), 'file') (('Ticket_Survival_Rate_NA features', 'also same method'), 'create') (('cabin_prefix passengers_without_cabin_column', 'pd'), 'describe') (('who', 'https here www'), 'for') (('score', '1 perfect predictive power'), 'range') (('merely I', 'Machine Learning'), 'BackgroundAspired') (('fit_transform X_train y_train', 'import LeaveOneOutEncoder enc LeaveOneOutEncoder Another_name training_set Name_of_col enc'), 'col') (('chance', 'Pclass2'), 'be') (('SibSp Integer', 'passenger'), 'mean') (('DataFrame Survived column data test_survived 1st names', 'pd'), 'column') (('we', 'https instead stackoverflow'), 'label') (('I', 'rule bin big size'), 'assume') (('html', '1'), 'attribute') (('data', 'p value'), 'be') (('com startupsci titanic data science', 'www'), 'solutionshttps') (('we', 'evacuation'), 'be') (('Sometimes lift', 'Yeo Johnson prior transform'), 'achieve') (('passengers', 'following independent variables'), 'masumrumi') (('heatmap matrix_df cmap BrBG', 'sklearn'), 'annot') (('LabelEncoder', 'n.'), 'label') (('com questions', 'https www'), 'label') (('This', 'distribution'), 'describe') (('Function', 'same statsmodels'), 'apply') (('Family feature', 'extracted surname'), 'create') (('feature selection Sex Age SibSp Parch Ticket_Frequency Survival_Rate Survival_Rate_NA name aka Embarked Cabin_Prefix Title pretty values', 'number generator https random www'), 'selection') (('FeatureCalculate', 'Mrs.'), 'is_married') (('Complete Age Train Test DataFrames 20 missing values', 'np'), 'use') (('class upper passengers', '2 Pclass'), 'be') (('it', 'https t miro'), 's') (('chances', 'age'), 'sexagesurvival') (('I', 'dataset'), 'Part') (('majority', 'Pclass3 passengers'), 'board') (('Age outliers', 'Power Transforms Fare'), 'feature') (('he', 'younger'), 'child') (('com questions', 'https global variable stackoverflow'), 'define') (('com blog 2020 03 one hot encoding', 'Feature Selectionhttps www'), 'learn') (('We', 'two rows'), 'be') (('reshape len', 'onehot_encoded 1 onehot_encoder'), 'integer_encoded') (('who', 'ship'), 'be') (('Maximum number', '30 40'), 'be') (('number', 'Pclass'), 'increase') (('histogram blocks', 'bayesian_blocks'), 'cut') (('that', 'parts'), 'implement') (('0 large families', 'Pclass3'), 'be') (('character datatype', 'integer datatype Pclass'), 'have') (('Define X independent', 'predictor explanatory etc'), 'feature') (('parameters score cross validation http scikit', 'learn'), 'note') (('Hyperparameters Optimization Grid Search Hyperparameters', 'Impact https www'), 'Tuning') (('Insights Most observations', 'https www'), 'take') (('dataset', 'correlated independent variables'), 'grow') (('so we', 'model'), 'be') (('Age', 'survival 4 high rate'), 'have') (('who', 'class also 3rd passengers'), 'be') (('we', 'ticket survival family survival rate rate'), 'survival_rate_na') (('too insights', 'Cabin_prefix'), 'decide') (('Pclass Integer Meaning', 'status social economic SES'), 'represent') (('Alternatively passengers', 'group purchase'), 'share') (('Dummy Variable Trap', 'multicollinearity'), 'lead') (('This', 'same family'), 'be') (('com yassineghouzam titanic', 'ensemble modeling'), 'top') (('predictor more variables', 'better model'), 's') (('It', 'correlation matrix'), 'use') (('Calculated_Fare bins', 'True labels'), 'cut') (('therefore calculated', 'much situation'), 'seem') (('Few elderly passengers', '65 80'), 'range') (('people', 'family'), 'masumrumi') (('Multicollinearity', 'where independent features'), 'occur') (('which', 'infinite VIF'), 'have') (('It', 'survived'), 'be') (('3 which', 'other Pclasses'), 'look') (('Most passengers', '75 parents'), 'travel') (('who', 'NaN Fare'), 'Fare') (('print get_boolean_list_for_categorical_correlation_of_passenger_value_and_mode cabin_prefix_dictionary n', 'cabin_prefix_dictionary'), 'axis') (('Parch', 'slightly more than 5 VIF'), 'have') (('even majority', 'S.'), 'look') (('he', 'T cabin'), 'be') (('others', 'skewness'), 'outlier') (('here that', 'didn t many Pclass3 around 81 survive'), 'be') (('loc return 1 sliced_description_df_by_cabin_prefix_and_feature top 0 top equalls', 'pd'), 'axis') (('Question Problem Resources Part 2 who', 'titanic incident'), 'com') (('13623707 13623707Feature', 'features'), 'be') (('discrete data', 'distinct values'), 'assume') (('concat data_other_cols encoded_data', '1 return'), 'axis') (('passenger', 'Binary Sex indicator'), 'string') (('Pclass Age Sex Survival Survival chances', 'even Women'), 'be') (('html 100 criterion 50 70 120 gini', 'max_depth'), 'n_estimators') (('Huge Family 8 12 people', 'feature selection'), 'clean') (('us', 'training problem how actual domain'), 'be') (('Methods Gaussian Processes Navies Bayes Nearest Neighbor Discriminant Analysis 56780852 Ensemble xgboost', 'xgboost'), 'com') (('0 negative values', 'power two popular automatic transforms'), 'be') (('python 45003806 pandas', '20'), 'use') (('Sex chance', 'men'), 'SexSurvival') (('passengers', 'ticket'), 'find') (('It', 'kaggle python Docker image https github'), 'come') (('fare', 'chances Money Matters'), 'have') (('persons', 'family relations'), 'share') (('columns', 'DO https www'), 'use') (('get_mode_of_categorical_variable cabin_prefix passengers_without_cabin_column', 'pd'), 'pass') (('extract_surname function', 'Name feature'), 'use') (('Hence I', 'option'), 'convert') (('Then dataset', 'missing Embarked values'), 'represent') (('I', 'VIF'), 'col') (('family survival rate', 'survival rate'), 'imply') (('True gamma auto decision_function_shape ovo ovr break_ties True l1 l2 loss hinge criterion False False squared_hinge dual True False DecisionTreeClassifier gini', 'None 1 max_features auto log2 ExtraTreeClassifier criterion None 1 max_features auto eigen None best random 2 4 min_samples_leaf 2 gini entropy best random 2 4 min_samples_leaf 2 lsqr auto'), 'range') (('One way', 'then transform'), 'be') (('Title onehot_encoder', 'reshaped_column df nominal_categorical_columns column'), 'preprocesse') (('we', 'power two transforms'), 'use') (('import re def 1 concat_duplicate_columns', 'df'), 'concat') (('columns', 'families'), 'add') (('Cabin_Prefix onehotencoder nominal_categorical_features Pclass', 'Sex train_df_copy train_df'), 'preprocesse') (('html split dataset', 'splitter class http scikit'), 'learn') (('skewness better hence we', 'it'), 'handle') (('20 Cabin 687 values', 'Cabin numbers'), 'miss') (('Hyperparameters Optimization Grid We', 'results'), 'Search') (('Survived', 'training'), 'survive') (('fit_transform values', 'integer_encoded dummies'), 'print') (('which', 'correlated independant variables'), 'represent') (('when somebody', 'ship'), 'prove') (('We', 'pipeline'), 'explore') (('anyone', 'https miro'), 'stop') (('consist', 'maids'), 'nanny') (('I', 'thanks Kaggle'), 'aside') (('Setup helper FunctionsClassifiersEnsamblingVotingConfusion 2 matrixcross_validation', 'small datasets'), 'avoid') (('when we', 'male passengers'), 'conserve') (('it', 'zero values'), 'use') (('things', 'frequencies'), 'be') (('value', 'Cabin_Prefix'), 'impute') (('passengers', 'same cabin'), 'find') (('pip install dabl Machine Learning MLA Ensemble Methods Gaussian Navies Bayes Nearest Neighbor Discriminant Analysis Algorithm Selection xgboost', 'xgboost'), 'com') (('that', 'logical sense'), 'mean') (('Methods Gaussian Navies Bayes Nearest Neighbor Discriminant Analysis Ensemble xgboost', 'xgboost'), 'store_covariance') (('Family_Size_bins', 'True labels'), 'cut') (('rational', 'https www'), 'derive') (('Sex', 'Age'), 'seem') (('MLA predictions', 'usage 6 print'), 'let') (('html', 'pip install dabl data first analysis'), 'remember') (('detect correcting outliers', 'Contents1'), '4') (('7 kernel linear poly rbf sigmoid 1 degree', 'True False probability'), 'range') (('SD SEM', 'Cabin right feature'), 'decide') (('inf', 'True labels'), 'retbin') (('that', 'training'), 'create') (('Continious Features BinningFeature Binning 1 Conversion', 'bin size'), 'bin') (('Alternatively several passengers', 'cabin'), 'share') (('Feature Engineering ML twicking Hyperparamaters', 'etc'), 'include') (('PassengerId result', 'column names'), 'concat') (('deviation r f3ee5e0eca32 normal standard SD', 'distribution normal median'), 'com') (('Older More SibSp Younger More Age', 'Port C'), 'parch') (('categorical nominal that', 'already numerical values'), 'be') (('Family Title Title', 'Grouping https www'), 'Correcting') (('com power transforms', 'https resource2 www'), 'learn') (('Standard Deviation SD', 'population'), 'be') (('html Passenger', 'https stackoverflow'), 'change') (('Title_4', 'multicullinarity'), 'let') (('passenger', 'journey'), 'range') (('that', 'test set'), 'create') (('when it', 'model'), 'get') (('Ticket feature', '22 duplicate values'), 'have') (('don t', 'pip install missingno'), 'find') (('com', 'data science Github https github'), 'parulpandey') (('titles', 'age certain group'), 'indicate') (('They', 'LabelEncoder'), 'convert') (('Nearly 30', 'siblings'), 'have') (('i', 'range'), 'key') (('Change Log21 02 21 Kernel', 'suggestions'), 'comment') (('I', '3 independent variables'), 'correlate') (('Parch', 'parents'), 'be') (('Modelling ChooseModel https Part 9 scikit', 'data science Single Predictive Power Score'), 'parulpandey') (('Continious features', 'ML algorithms'), 'dive') (('describe', 'features https stackoverflow'), 'include') (('PassengerId result', 'submit base_results 1 model_selection'), 'concat') (('process', 'features'), 'be') (('integer datatype', '3'), 'feature') (('changes', 'age'), 'update') (('feature', 'families survival also rate'), 'be') (('skewness', 'Multicollinearity Variance Infalction Factor VIF https www'), 'top') (('CategoricalEmbarked', 'Nominal Embarked character'), 'encode') (('Many passengers', 'groups'), 'be') (('Around 38 samples', '32'), 'survive') (('I', 'as well own'), 'contain') (('mode operation', 'larger dataset'), 'use') (('Sex', '5'), 'have') (('Adding', 'end'), 'be') (('survival rate', 'Family_Survival_Rate feature'), 'calculate') (('Split Training Data', 'two datasets'), '25') (('95', 'Pclass3'), 'be') (('survival EmbarkedPclass C 1 better even majority', 'S.'), 'Embarked') (('originally 80 I', 'variance'), 'question') (('so I', 'instead skewness'), 'discover') (('last column', 'ppscore f plt'), 'question') (('weighted probabilities', 'html Hard majority Soft Vote'), 'vote') (('better when distribution', 'Gaussian normal distribution'), 'perform') (('integer Survived Target', 'train dataset'), 'find') (('com', 'chi statistic'), 'parulpandey') (('Most data', 'Machine data data time series Learning perspective numerical categorical data'), 'type') (('Numerical data', 'continuous data'), 'have') (('Test dataset Parch 0 10 Integer', 'passenger'), 'range') (('they', 'same ticket'), 'weren') (('who', 'same prefix'), 'purchase') (('Standard Error', 'population estimated mean'), 'be') (('cabin_estimator Hence function', 'Cabin feature'), 'mislead') (('Outlier 2 detection', '1977 IQR'), 'be') (('com jeffd23 scikit', 'www'), 'learn') (('histogram blocks', 'bayesian_blocks'), 'passengers_without_cabin_column') "}