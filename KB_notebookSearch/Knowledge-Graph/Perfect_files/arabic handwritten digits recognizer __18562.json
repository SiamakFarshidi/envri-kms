{"name": "arabic handwritten digits recognizer ", "full_name": " h1 Arabic Handwritten Digits Recognizer h3 Author Abdelwahed Ashraf h3 Linkedin Link h3 Kaggle Link h2 Keras with CNN h3 My Goal is Accuracy 0 9999 h2 Outline h1 Introduction to CNN Keras Acc 0 999 top 8 h2 1 Introduction h2 2 Data preparation h2 3 CNN h2 4 Evaluate the model h2 5 Prediction and submition h1 1 Introduction h1 2 Data preparation h2 2 1 Load data h2 2 2 Check for null and missing values h2 2 3 Normalization h2 2 4 Reshape h2 2 5 Label encoding h2 2 6 Split training and valdiation set h1 3 CNN h2 3 1 Define the model h2 3 2 Set the optimizer and annealer h2 3 3 Data augmentation h1 4 Evaluate the model h2 4 1 Training and validation curves h2 4 2 Confusion matrix ", "stargazers_count": 0, "forks_count": 0, "description": "e the area size pooled each time more the pooling dimension is high more the downsampling is important. For those who have a 3. CNN section three 1. Arabic Handwritten Digits Recognizer Author Abdelwahed Ashraf Linkedin Link https www. The CNN can isolate features that are useful everywhere from these transformed images feature maps. Predict and Submit results section five a 1. Since i set epochs 2 on this notebook. However it seems that our CNN has some little troubles with the 4 digits hey are misclassified as 9. Evaluate the model section four 1. The second important layer in CNN is the pooling MaxPool2D layer. com kaggle docker python For example here s several helpful packages to load Input data files are available in the read only. The rectifier activation function is used to add non linearity to the network. We can get a better sense for one of these examples by visualising the image and looking at the label. Data preparation section two 1. 4 ReshapeTrain and test images 28px x 28px has been stock into pandas. Can judge whether it is under fitting or over fitting Look at confusion matrix Predict the values from the validation dataset Convert predictions classes to one hot vectors Convert validation observations to one hot vectors compute the confusion matrix plot the confusion matrix Show some wrong results and the difference between the predicted label and the real labe Errors are difference between predicted labels and true labels Probabilities of the wrong predicted numbers Predicted probabilities of the true values in the error set Difference between the probability of the predicted label and the true label Sorted list of the delta prob errors Top 6 errors Show the top 6 errors predict results y_pred Evaluate model select the indix with the maximum probability Save the final result in cnn_mnist_submission. 99286 set input mean to 0 over the dataset set each sample mean to 0 divide inputs by std of the dataset divide each input by its std apply ZCA whitening randomly rotate images in the range degrees 0 to 180 Randomly zoom image randomly shift images horizontally fraction of total width randomly shift images vertically fraction of total height randomly flip images randomly flip images Fit the model Prediction model Draw the loss and accuracy curves of the training set and the validation set. Here we can see that our CNN performs very well on all digits with few errors considering the size of the validation set 4 200 images. Training and validation curves section four a 2. 1 Load data a random split of the train set doesn t cause some labels to be over represented in the validation set. Label encoding section two e 6. Data preparation 2. To start with machine learning course and deep learning specialization 5 courses we suggest coursera courses by Andrew Ng. 2 Confusion matrixConfusion matrix can be very helpfull to see your model drawbacks. OutlineThe following sections are included in this notebook Introduction to CNN Keras Acc 0. fit X_train Y_train batch_size batch_size epochs epochs validation_data X_val Y_val verbose 2 With data augmentation to prevent overfitting accuracy 0. We have to choose the pooling size i. This metric function is similar to the loss function except that the results from the metric evaluation are not used when training the model only for evaluation. Its better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function. The last 9 is also very misleading it seems for me that is a 0. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit. The most important errors are also the most intrigous. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session Import the necessary libs convert to one hot encoding Load the data plot Count numbers Check the data Normalize the data to make CNN faster Reshape image is 3D array height 28px width 28px canal 1 Encode labels to one hot vectors ex 2 0 0 1 0 0 0 0 0 0 0 Set the random seed Split the train and the validation set for the fitting Draw an example of a data set to see Please try with this number to understand how augment you data 1111 111 101 144 663 Creating CNN model print out model look Define the optimizer Compile the model Set a learning rate annealer Audjusting learning rate Turn epochs to 30 to get 0. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima. Check for null and missing values section two b 3. Be carefull with some unbalanced dataset a simple random split could cause inaccurate evaluation during the validation. Introduction section one 2. 17 sklearn versions. In the end i used the features in two fully connected Dense layers which is just artificial an neural networks ANN classifier. I ll show you the training and validation curves i obtained from the model i build with 30 epochs 2h30 The model reaches almost 99 98. It is like a set of learnable filters. Keras requires an extra dimension in the end which correspond to channels. Firstly I will prepare the data handwritten digits images then i will focus on the CNN modeling and evaluation. Our model is very well trained 4. So we can safely go ahead. Once our model is ready we fit the training dataset. The LR is the step by which the optimizer walks through the loss landscape. The validation accuracy is greater than the training accuracy almost evry time during the training. Reshape section two d 5. Sometime it is very difficult to catch the difference between 4 and 9 when curves are smooth. Dataframe as 1D vectors of 784 values. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive monotonically decreasing learning rate. 1 Load dataWe have similar counts for the 10 digits. In order to make the optimizer converge faster and closest to the global minimum of the loss function i used an annealing method of the learning rate LR. 999 top 8 1. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. IntroductionThis is a 5 layers Sequential Convolutional Neural Network for digits recognition trained on MNIST dataset. 2 Set the optimizer and annealerOnce our layers are added to the model we need to set up a score function a loss function and an optimisation algorithm. 0 GPU capabilites from GTX 650 to recent GPUs you can use tensorflow gpu with keras. The Flatten layer is use to convert the final feature maps into a one single 1D vector. Confusion matrix section four b 5. In the last layer Dense 10 activation softmax the net outputs distribution of probability of each class. Moreover the CNN converg faster on 0. I choosed to build it with keras API Tensorflow backend which is very intuitive. With the ReduceLROnPlateau function from Keras. 114 With data augmentation i achieved 99. 9999 Prerequisites fundamental coding skills a bit of linear algebra especially matrix operations and perhaps understanding how images are stored in computer memory. 5 Label encodingLabels are 10 digits numbers from 0 to 9. callbacks i choose to reduce the LR by half if the accuracy is not improved after 3 epochs. This technique also improves generalization and reduces the overfitting. This function will iteratively improve parameters filters kernel values weights and bias of neurons. Evaluate the model 4. The improvement is important Without data augmentation i obtained an accuracy of 98. 1 Training and validation curvesThe code below is for plotting loss and accuracy curves for training and validation. By applying just a couple of these transformations to our training data we can easily double or triple the number of training examples and create a very robust model. Set the optimizer and annealer section three b 3. It is the error rate between the oberved labels and the predicted ones. It combines all the found local features of the previous convolutional layers. Dropout is a regularization method where a proportion of nodes in the layer are randomly ignored setting their wieghts to zero for each training sample. To keep the advantage of the fast computation time with a high LR i decreased the LR dynamically every X steps epochs depending if it is necessary when accuracy is not improved. 7 accuracy on the validation dataset after 2 epochs. It looks at the 2 neighboring pixels and picks the maximal value. com in abdelwahed ashraf 090523169 Kaggle Link https www. 98114 history model. 1 Define the modelI used the Keras Sequential API where you have just to add one layer at a time starting from the input. you found this notebook helpful or you just liked it some upvotes would be very much appreciated That will keep me motivated This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. com abdelwahed43 data image png base64 iVBORw0KGgoAAAANSUhEUgAAAX8AAACECAMAAABPuNs7AAAAflBMVEX 8AAADe3t5QUFDQ0NA1NTV3d3e3t7fb29suLi5LS0u0tLRzc3Ps7Ozj4 OpqamUlJTCwsLz8 NZWVlnZ2cPDw8oKCiIiIgjIyP39 dDQ0PU1NTJyckcHBzo6OhiYmIXFxeenp4 Pj6AgICMjIyYmJgzMzN2dnasrKxtbW2l rdzAAAR50lEQVR4nO2d6aKiOLeGw6CgCAiIEkCUQbfe w0ehgRWwmJ31e7C7q9P3h9VWwIZHkLmlRCipKSkpKSkpKSkpKSkpKSkpKQEFToWKuYcOJhe7uBaoA87OXsY99qhg2uGeu1kgyt94c7M6xwPuRhcXdHVGX46AY8XnqhwcNVxr7N1 Hsaqpg5V7gzIxzgrjf2MO6qsXQ2uGvDEC48zLy 4a6McC5ejdLhf P7eHmDq4 71h lf2bOe9z5e 6nX J 7H9Y3kl0Pf4S xPuivK Bs61 yNiD8f4w9 zv6zO3wepnvGvUxgZnP91M6RskX 87P6F NMwT8V7EP6pS5wd sH JPAWuC E 77h n7MOZz haMzJx YpweGcOyyD qL0H Pomke2T 56jpSnW3TPqfP AfWwv8r EDfgoC 9vhthF8WZn xR9 GizVMv8yg5GZ8TesgpIiH6qLRf4l7f6F BuiS0W5xD8JKC0KRw9ZYL K wTI gv8fV4IDoL8D7ZTijljXf4N 0Ooc7zfwgjI M FoQGtXEffmH8jfbFVt5L5t8G7FpCJSPyv7TNDsuI4n1JyW xL0AxssD FgaFC94 4P OvvpLqZV8hP92unBY4E 4d GPDDGmGL873nblKqHZovAX6sD4sIvXeCftAFFyRD nP mdLt4TXnF B dsXC 4 V bOm7LbGnFt7EP3Z8npL6E xP4ELz2 wTmxRb6Ar57 su7VHhOtQ3vc2cf vovhb4n9tuSMrDF nvyjInNCxsl3hTQTHxj8jo6QJ zWrf 4M4Y0E18d8E7LMwyFjtrck glfOKP Iwq4A5B9bJBMbEpC Ty6tb3bbyjyG7pCZZP53E5ZtkP D8PIhLokO d qLM9fUfc T3Z nvPXCnv8ql4L b t 9lFz Zv2GzkvQamjxpK KPA3glQfnf6RhPif87pFIvDfJPiXesg6y743S7o zf4VL d8bc cxZwci8huV ScdCHPA 2CV7NrZY hD5J0HRvqGzHnLCE 99wdJ9ICEPaUX iXAF56 ZsLEP R9nfTih O9GMkLhBUn8n3kIuwCQv0 MzXg5XOLfTFkYtj8v4YHzDxKEfxT2l5uxtQrKn2KIXFnUGe mf4w Xv60pfQT59 MeskC xsJ67PgLPE EB26Q 6gM3Qhi 2fKrN4GQ75n3MWryte p9YVRfy1wfrX8 4GrXXFlB cF2d 9UFF8Irzn8zNQWk rckUidKbP8c5B6lxP9rkb9PdINfbghd4q9ZIW97Cf2vdxj2pHfZK57zj8ti8Dx26Vbi3zq6YRF0qbrR9 r8hfp3CG7O qWC zbiiy8RKC I K BtJomcT RKkfTeEA sHUtIl0sl3k 6K8lBL7v9YQxonxFflfTVYqtZeHv4T 74aVifcgv67OH7Q s2qBf yBbCy2 5OmbYEGl ENSO3 d1gYGpDE 95 54X9OiQz vcx s20TeL Ovx0xT57123ve LZmkS e HzzJ kXLGf5RBndX5v8efJoOI8C cJf5tbrm0RVj2xvl35QisIeT2j7Y 5Dqhul9J BNCe7BGV4ezepB5LfBv2zAo 7b57poZofxmgX9Nv jzVdb3MhbG33warc1 jDghjyX 2sOeKmBs C0jhJYo 9Z3OHo3498qrrqZlWAj1r9tzeK2om2 4Hczr8XxhyMv4eTxt7oNqxkbXwJ V5867IlHl lrrn1enX JpncYfox BGZM0PHn6OUOkZ PPzTEv3 LvwvyWA4sJ 6bAYnnHca7mNci 42ePVH oiD VLjp0WXxJf5PvZs WHv8uZ DK0cYc 7JZWrlLMy 3Mw UXP 12DqjS7xZxLGf07dvGBzB84of606Dh AL Nvv9Yv2Xlx L 6VJ Yf4H64fxX1bcakPG36Da1j36D 1w4f65f5r 1LNZsH96 gvqY OPM32GP6Z E 8rc05x5z8x 63 P W DvX35p X4m 94i uKz c0B09seXDP04UPJHj58Yc5vtv7EeqNesxUm4QkPmXldHlCvWSvCxuP1YA9 oV6f2ChAgHp94CtflJSUlJSUlJSUlJSUlJSUlJSUlJSUlJSU kLedYPouWfOt MO0d0cXPMEe3jHrTnOmOvmzua ygTzOmGTN26Mue74vNx2h3mdsHk5U4zXc7h5xydvNqjXMZv ctBon48r8cdn2 iS3P E O9ZWv 24PU O 8b15f3FNqfn3 Pa758a8b DUNZgX JFrj m ff6zbd4bTCWeIfvcQXgfJvHGuYFkf5R OcucS eoUwyTj c83i9gP d4slY8bff8P7ZP5xegArj1blH2 7RX6ERnyhrMQ zUshSXP 7oruAcLI5R SnH hkuIDe4T ddD0eF4IR1u gH fWFi9i atqE6vE kn7yygoKMsSp Q2c bbbGTS5 olCwsJP5VyUllNhttdwt0p3zP581Y4E 8YxXscD 7JOy 79bfxvQ4If8b R1R k3VNiEQuB oKTQdTpZJqzKf6 Pv7c3jL9mC2mS He22160jzWnvzTjX7XssnG9q8C LK6aIMA bd9pb9rS8g8PF2oI K P6t1qD41XAf qrEYfWTEj888C2waWg5D KSzq o5P82fYZvytb yvb4Rmj uQVF in761pM7MDBh5CfxDYV8DkX8cZsch6 ZBC9wNAP97aVmObtqe5fjT2kbA3yDlcC0OzCfKP UuJ9gKg z9YeVZWnzE ldL 5J SYH5l8j 4BGfZaPoWEn8s9APiHVv9OlLF jXui8aL8Hyx8uedc 3H07hTnxvx3t5iuq7rsqejvZ6APgfw684QNIQm64IPGP3OPODabPD A 595guhSavARalX kgUso 4fQFBX4W0T4NkT bbXu sm4NcqMv3YqwsLL8nFBIeBftQGAtmII T9AdghG 1VY jeskkhG826Jf0mi2AI1CeAfuQ57gtvOrMrfgAZ4OP8NfWiTIP8qk cwgfxf9pQElH byQyD2GNAgP NAJOb1ldHw vfy htQv4pq7KMkL bmf11qj1zd7SNgvZ3lntl6eQF0Jr89zD74 y1EFbAkH 77Yjm71L9WxPzKTjP 78n8AYBf9eFdmMO3S3wP4U8mpD LqDDaniK89 ZedLFrkT4tx9PF3S3YPcT N8uoYHJ1sWaOP8HBblcKH8CeZcyyf7RJ7nQz5nz10FHAvAPIf t8DEwr0X7L48zFNqfe7dPikG59Yxs 9vFINHHEQDI 0LK3jv6Cf6d a rRcPmdYPFOWZ B1oqkH bd g2 2u7krgXWI7L CMCOAv5f3zlZ4cu2r 3eQO3vyt7eM7oi8D HoT99ddIGPLfFPR2f5C8 AT frDNHbBkQzMRGf xvMkYQCr pTak3P6Po5D40zcy43 EPWXAn7o8xHfBd4ZiXov8z7yjKvKPSdcE0sdOrsB xzpaT JWc 5dqUqJd962bbfP8GdEWWgI xoYwEvtn2IyJcT4a10uIw1vyc34lwvjP1vid6jumwsJee3PvJbtv1gROLM Da7G1EsX GeUJWfH99iQxn Ksjy3lTiP50f457w1jNo ljj LSV2vXlOY1XY E9pE ON73 iHT3QhwX8r5QEvu9bbeU0fj3Ma4l zFpn8viDT3NvGkMQ E82lQ9iVgj QTVlNkBrlv8hLbxWhTc2VBD u9wbKQj82wqWUDO3tnwAG OfbC6 GfSb6M34G4XtR0my21R3sf9V9 Ys1AJjTzj toOM8j87LhgMEu3fpxqpJNki 6tF8vvK LXavKX7VlMBj43 P6YKQB7 YfZIpDCTBf6d0qNzxPhrhl EnpeXR8n Wqs6w6AbHCFiXsv8G5y luxvU8WzNP5 9 tL Pj OSfr279LY2A4 6oZiwl5 DMx0vJQH42o34Zjef4r7gLC5r uu93ufL3GP51 OSzwF7Q4 3Ituiy NP i9RWFsj ttcC Gvbw c Yn 6v8Wf6z Dn6fzR tt Mc 9J bflnfNZfqe 77eDH Je660v7b1NZRMWfdRsWGKkL8YbYLPMFdeV3t4l7zoUDcdazoca DbxPlfe815fFCHxaGKJWUlJSUlJSUlJSUlJSUlJSUlJSUlJT 3aImKn7aaoa65myc3cUf5oP0uCuzXSVejnrNBukp6mryo21t3Gs2SB9KD 1SvHI2 l gzsIS2T nNc6f cD81 sf5tc2LKaX5v v91 uxMf37 98DPkPofmv dn78mzLhi FPXZQcJ YB ww LWoH lp hNuMvWib80 y7HY4zZqcy4 9AG1GE 9uhhP6I 85ogsnwUuS S9O0X32WciulH D WrA 0t7ijRL uNrDdd2fOf XBz8 1H4UGT u8YldsBXkS w3zBjXpF ZLdPHqfyHPKvMjMYfB1tSEf Rt3qIFp2LPDnx4PJ EtCYT0i8W8jYj mFYFr8 eblpMS5y QlPkHhD52S cfcb24yePgOvw422YtQITnH V6ek 6TPhF2BlezOu377mZ6bn2RbA9wMuffDxoahCPaO66OTh6TOS o nx6H7I hQcALxgf2RQWGtJ PfEfi e zXosKsoGQpigf8jFA Hg wfk 3X3mRWFMzrnNTG5rmPjl4B6xGU 8nB Ueu0xBg1Sbyz4o2suZkIL8u 0vf O0PTsDtrxPhkEaJPxUrU8B cxuwlb19x5BYgb8lNwAE vpzM84E84frHnEqgCGje8 4Bgo ze9XfNiSpbIv08csM5bl38HtLz2qcPPH0zodAaRzP8xHhsm87 2XZ624Ni qAKfuBirwFbRLPbCVZ0gH jgnXB5QCGeQ3qXzucfkD Y5SW G DSNtOxwRj M3pfOxV R9aJtmQumLh Ed3yf5U0wqpLzXxr2yakVcZEi8k2e50m PXLnZIvWAy3hDsj2Dh3ptfI wjPR zMNx oOEv4JbzXVxE i v2S3vx xPu Ds6lTSD HvEt0MqRuscub8M9jYF nr8GhOgX9UkD5VlnEIxg6E3P68RQ6ZMjrkL9g1Bf3XMOevHWiGnH9qFPyeg52i K3uWNRbqPOIifxvhUfDbLIAXJP TU0e9K2ybVkf6cv8y kvuzEP 4sOFvUbfaesvJ8 yUX2FnD808ptFvd908g KtsHI4A K8W31rAD9D258Y DuFlKH NcMr9eHTn udvdikMx9W c 4 rChF jVxl85 j5Pr7kGFunvG 5zB lI4 zcDhmERgftvQA LcQMhWP7f3OGEtdRFz38chyUcdvwp1v 9TdXDmvxvfdPE8Mbsj CvYWUotX884j5AUS23Py0Swn6OzP pw IL8m adm33f6xVb4ss7v9zz8h5zl97DYal fh2Rf6lxxL3IMF5kT88AHo1 merM Htd ExlvgbIciLEv N3qZ5Qjyz w2ICw4RnO3 YMOj5SH uAmp7x9L37dJNhgZY y1sRIV Bte299I Gn3A4F onPL3rgk5SL j T LtF5P LODT2x869BYSz3f dtwt28SXH 2tVp V qf9uCGG6zIYz xKfXAPTFd1NC d9Q m0VbDaZNX2YAv DAL2Pnk8X7O8q jH nUKdNyTm O wlJiNvyVGNz4eDu3Ief83 nLZ3nAI 5ba0H7dG9fZ Ge83xt7Ixkb88xrafyNZhj znraAd1rgX jToOf8XGBfzINQKzPfzjndpF CBpA6Pi UerhbP r0bFxiWmkKH tRaju6a55Of9w JmyHeR w 7Lnk1eIPyv9tgCWJX u2USwDYMtv Ad mfRTd lenS FtcemGfTZHx 7Q0zXzIjz hv HEf4O OTMhw8b H MAxLrjD1v IcRllfmv6tgn5s Pf5V8nvSP2z9exiEoZX aC O GT dP87 fWR8memv7X 8wfOnxXz zhM8Wv8F7z R Yfi8rAxBEe9pgqtjzFTLFn9zyqqKtRMf5 inmdsmk4d4OHzLy 7DGvU5YvMjxefAmDgXvNzGYt OFxfkpJSUlJSUlJSUlJSUlJSUlJSUlJSUlJSUnpc o DwttbqVDDroAAAAASUVORK5CYII If this Kernel helped you in any way I would be very much appreciated to your UPVOTES Keras with CNN My Goal is Accuracy 0. Computation will be much much faster For computational reasons i set the number of steps epochs to 2 if you want to achieve 99 of accuracy set it to 30. Some of these errors can also be made by humans especially for one the 9 that is very close to a 4. 6 Split training and valdiation set I choosed to split the train set in two parts a small fraction 10 became the validation set which the model is evaluated and the rest 90 is used to train the model. in order to minimise the loss. We define the loss function to measure how poorly our model performs on images with known labels. Let s investigate for errors. This flattening step is needed so that you can make use of fully connected layers after some convolutional maxpool layers. I want to see the most important errors. Load data section two a 2. 671 of accuracy with this CNN trained in 2h30 on a single CPU i5 2500k. Each filter transforms a part of the image defined by the kernel size using the kernel filter. We use a specific form for categorical classifications 2 classes called the categorical_crossentropy. These are used to reduce computational cost and to some extent also reduce overfitting. For those six case the model is not ridiculous. To avoid that you could use stratify True option in train_test_split function Only for 0. We need to encode these lables to one hot vectors ex 2 0 0 1 0 0 0 0 0 0 0. 2 Check for null and missing valuesI check for corrupted images missing values inside. We reshape all data to 28x28x1 3D matrices. Since we have 60 000 training images of balanced labels see 2. I choosed RMSprop with default values it is a very effective optimizer. 3 Data augmentation In order to avoid overfitting problem we need to expand artificially our handwritten digit dataset. Define the model section three a 2. This Notebook follows three main parts The data preparation The CNN modeling and evaluation The results prediction and submission 2. For RGB images there is 3 channels we would have reshaped 784px vectors to 28x28x3 3D matrices. Prediction and submition section five 1. 3 NormalizationWe perform a grayscale normalization to reduce the effect of illumination s differences. Filters can be seen as a transformation of the image. 9967 accuracy Without data augmentation i obtained an accuracy of 0. Some popular augmentations people use are grayscales horizontal flips vertical flips random crops color jitters translations rotations and much more. The kernel filter matrix is applied on the whole image. For that purpose i need to get the difference between the probabilities of real value and the predicted ones in the results. I plot the confusion matrix of the validation results. Split training and valdiation set section two f 3. The higher LR the bigger are the steps and the quicker is the convergence. Combining convolutional and pooling layers CNN are able to combine local features and learn more global features of the image. The most important function is the optimizer. There is no missing values in the train and test dataset. Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Data augmentation section three c 4. That means that our model dosen t not overfit the training set. For example the number is not centered The scale is not the same some who write with big small numbers The image is rotated. MNIST images are gray scaled so it use only one channel. The first is the convolutional Conv2D layer. We could also have used Stochastic Gradient Descent sgd optimizer but it is slower than RMSprop. I choosed to set 32 filters for the two firsts conv2D layers and 64 filters for the two last ones. The metric function accuracy is used is to evaluate the performance our model. relu is the rectifier activation function max 0 x. We can make your existing dataset even larger. Normalization section two c 4. This layer simply acts as a downsampling filter. 67 of accuracyFor the data augmentation i choosed to Randomly rotate some training images by 10 degrees Randomly Zoom by 10 some training images Randomly shift images horizontally by 10 of the width Randomly shift images vertically by 10 of the height I did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9. ", "id": "abdelwahed43/arabic-handwritten-digits-recognizer", "size": "18562", "language": "python", "html_url": "https://www.kaggle.com/code/abdelwahed43/arabic-handwritten-digits-recognizer", "git_url": "https://www.kaggle.com/code/abdelwahed43/arabic-handwritten-digits-recognizer", "script": "keras.datasets matplotlib.image keras.layers keras.models train_test_split IPython.display confusion_matrix plot_model keras.utils.np_utils numpy keras.preprocessing.image Image seaborn Dropout ImageDataGenerator BatchNormalization Dense keras.callbacks MaxPool2D plot_confusion_matrix RMSprop keras.optimizers ReduceLROnPlateau matplotlib.pyplot Sequential sklearn.model_selection pandas Conv2D to_categorical # convert to one-hot-encoding keras.utils mnist display_errors sklearn.metrics Flatten ", "entities": "(('we', 'Andrew Ng'), 'start') (('We', 'label'), 'get') (('It', 'maximal value'), 'look') (('7 accuracy', '2 epochs'), 'dataset') (('you', 'keras'), '0') (('function metric accuracy', 'performance'), 'use') (('we', 'training dataset'), 'be') (('We', 'one hot vectors'), 'need') (('when someone', 'digit'), 'be') (('activation rectifier function', 'network'), 'use') (('optimizer', 'loss landscape'), 'be') (('perhaps how images', 'computer memory'), '9999') (('gray it', 'only one channel'), 'be') (('image', 'big small numbers'), 'center') (('accuracy', '3 epochs'), 'callback') (('which', 'channels'), 'require') (('technique', 'overfitting'), 'improve') (('i', '99'), '114') (('Training', '2'), 'section') (('validation', 'training set'), 'mean') (('Goal', 'CNN'), 'image') (('which', 'Dense two fully connected layers'), 'use') (('Label 5 encodingLabels', '9'), 'be') (('RMSProp update', 'learning aggressive monotonically decreasing rate'), 'adjust') (('It', 'error oberved labels'), 'be') (('that', 'data augmentation techniques'), 'approach') (('IntroductionThis', 'MNIST dataset'), 'be') (('I', '64 two last ones'), 'choose') (('model', 'almost 99 98'), 'show') (('you', 'Only 0'), 'avoid') (('regularization where proportion', 'training sample'), 'be') (('model', 'six case'), 'be') (('it', 'default values'), 'choose') (('Flatten layer', '1D one single vector'), 'be') (('671', 'CPU i5 single 2500k'), 'train') (('which', 'keras API Tensorflow backend'), 'choose') (('maximum probability', 'cnn_mnist_submission'), 'judge') (('grayscales horizontal vertical flips', 'crops color jitters translations rotations'), 'be') (('i', 'learning rate LR'), 'use') (('OutlineThe following sections', 'Keras Acc'), 'include') (('fit X_train batch_size epochs epochs validation_data X_val batch_size Y_val', 'overfitting accuracy'), 'Y_train') (('that', 'me'), 'seem') (('you', 'maxpool convolutional layers'), 'need') (('This', 'distributed way'), 'drop') (('validation accuracy', 'evry almost time training'), 'be') (('i', 'predicted results'), 'need') (('We', 'categorical classifications 2 classes'), 'use') (('90', 'model'), 'set') (('filter', 'kernel filter'), 'transform') (('when accuracy', 'X steps dynamically epochs'), 'decrease') (('simple random split', 'validation'), 'cause') (('we', 'very robust model'), 'double') (('It', 'kaggle python Docker image https github'), 'find') (('Combining', 'image'), 'be') (('optimizer', 'probably local minima'), 'be') (('that', 'very 4'), 'make') (('then i', 'CNN modeling'), 'focus') (('kernel filter matrix', 'whole image'), 'apply') (('function', 'neurons'), 'improve') (('3 we', '28x28x3 3D matrices'), 'be') (('second important layer', 'CNN'), 'be') (('you', '30'), 'be') (('model dosen', 'training set'), 'mean') (('We', '28x28x1 3D matrices'), 'reshape') (('I', 'validation results'), 'plot') (('3 NormalizationWe', 'differences'), 'perform') (('Load', '10 digits'), 'have') (('we', 'digit artificially handwritten dataset'), 'augmentation') (('60 training 000 images', '2'), 'see') (('test 4 ReshapeTrain 28px 28px', 'pandas'), 'image') (('labels', 'validation set'), 'datum') (('i', '98'), 'be') (('how poorly model', 'known labels'), 'define') (('hey', '9'), 'seem') (('Audjusting', '0'), 'list') (('it', 'such 6'), 'rotate') (('i', '0'), 'accuracy') (('These', 'also overfitting'), 'use') (('it', 'RMSprop'), 'use') (('CNN', '4 200 images'), 'see') (('we', 'loss function'), 'set') (('results', 'only evaluation'), 'be') (('curvesThe 1 Training code', 'training'), 'be') (('Filters', 'image'), 'see') (('layer', 'downsampling simply filter'), 'act') (('Notebook', 'data CNN results prediction'), 'follow') (('Confusion matrixConfusion 2 matrix', 'model very drawbacks'), 'be') (('when curves', '4'), 'be') (('where you', 'input'), 'use') (('It', 'previous convolutional layers'), 'combine') (('that', 'images feature everywhere transformed maps'), 'isolate') "}