{"name": "custom training loop flowers tpu deotte ", "full_name": " h2 my tests h1 TPU or GPU detection h1 Competition data access h1 Configuration h2 Visualization utilities h1 AugMix h1 Deotte s spatial transforms h1 Datasets h1 Dataset visualizations h1 Keras training h2 Model h2 Training h1 Custom training loop h2 Model h2 Step functions h2 Training loop h1 distribute the datset according to the strategy h1 Optimized custom training loop h2 Model h2 Step functions h2 Training loop h1 Confusion matrix h1 test val on ext datasets h1 Predictions h1 Visual validation h1 Timing summary ", "stargazers_count": 0, "forks_count": 0, "description": "Interpolate Interpolation means we always stay within 0 and 255. history val_sparse_categorical_accuracy accuracy 212 Optimized custom training loopOptimized by calling the TPU less often and performing more steps per call Model Step functions Training loop Confusion matrixB0 224x224 ep30 f1 score 0. trainable_variables update metrics train_accuracy. range in the step function. Using an LR ramp up because fine tuning a pre trained model. The name of the dataset is the name of the directory it is mounted in. history val_sparse_categorical_accuracy. Sequential pretrained_model tf. time history model. apply_gradients zip grads model. reset_states if epoch EPOCHS break simple_ctl_training_time time. glob GCS_PATH val. Otherwise subtract 255 from the pixel. If step is zero return the original image. fit the validation data iterator is repeated and it is not reset for each validation run different from model. my tests ex1 base with ep18 EN B5 aug rndflip time 46. On the validation dataset this setting can drop some validation images. history val_sparse_categorical_accuracy accuracy 212 Custom training loop Modelwith strategy. Order does not matter since we will be shuffling the data anyway. However in the optimized version of the custom training loop using tf. time distribute the datset according to the strategytrain_dist_ds strategy. Thanks to the dataset. Adam learning_rate LRSchedule this also works but is not very readable optimizer tf. SparseCategoricalAccuracy valid_accuracy tf. Otherwise build lut from the full histogram and step and then index from it. Data pipeline code is executed on the CPU part of the TPU while the TPU itself is computing gradients. This is done in the C code for image. For GPU training please select 224 x 224 px image size. tfrec TRAINING_FILENAMES_IN17 TRAINING_FILENAMES_IN18 TRAINING_FILENAMES_IN19 TRAINING_FILENAMES_TF tf. Optimizations specific to the TPU optimized custom training loop The training and validation step functions run multiple batches at once. It happens that in this example the validation dataset is used exactly once per validation. 940 maybe restore best weights B5 224x224 ep30 f1 score 0. This notebooks shows three ways of training a model on TPU 1. numpy and matplotlib defaults binary string in this case these are image ID strings If no labels only image IDs return None for labels this is the case for test data data auto squaring this will drop data that does not fit into square or square ish rectangle size and spacing display magic formula tested to work from 1x1 to 10x10 images set up the subplots on the first call input image is one image of size dim dim 3 not a batch of b dim dim 3 fix for size 331 ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS FIND ORIGIN PIXEL VALUES Do addition in float. Sum valid_loss tf. Adam learning_rate lambda lrfn tf. LearningRateSchedule def __call__ self step return lrfn epoch step STEPS_PER_EPOCH optimizer tf. Using Keras and model. get everything as one batch normalized since we are splitting the dataset and iterating separately on images and ids order matters. all in one batch run this cell again for next set of images. convert degrees to radians define rotation matrix For each pixel in the image select the pixel if the value is less than the threshold. history val_sparse_categorical_accuracy 1 lr 0. history val_loss loss 211 display_training_curves history. numpy VALIDATION_STEPS report metrics epoch_time time. Using a custom training loop specifically optimized for TPU Optimization that benefit all three models use dataset. compile r 0 9. history loss history. format simple_ctl_training_time display_training_curves history. GradientTape as tape probabilities model images training True loss loss_fn labels probabilities grads tape. fit get_training_dataset steps_per_epoch STEPS_PER_EPOCH epochs EPOCHS validation_data get_validation_dataset callbacks lr_callback keras_fit_training_time time. This could change numerics. GlobalAveragePooling2D tf. Sum loss as recommended by the custom training loop Tensorflow documentation https www. prefetch AUTO statement in the next function below this happens essentially for free on TPU. The loop will be compiled to thanks to tf. history val_loss 1 val_acc 0. reset_states train_loss. batch BATCH_SIZE drop_remainder True The training dataset is infinitely repeated so drop_remainder True should not be needed. time start_timeprint KERAS FIT TRAINING TIME 0. available image sizes if not SKIP_VALIDATION TRAINING_FILENAMES tf. Starting with a high LR would break the pre trained weights. history sparse_categorical_accuracy. Using a repeated validation set instead. glob GCS_PATH train. group 1 for filename in filenames return np. Losses are reported through Keras metrics. scope pretrained_model tf. numpy STEPS_PER_EPOCH history. For each pixel in the image less than threshold we add addition amount to it and then clip the pixel value to be between 0 and 255. append valid_accuracy. time epoch_start_time print nEPOCH d d. LearningRateScheduler lrfn verbose True model. With a validation at the end of every epoch this would be slow. format keras_fit_training_time display_training_curves history. experimental_run_v2 train_step args images labels print end flush True validation run at the end of each epoch if step 1 STEPS_PER_EPOCH epoch print end flush True validation run for image labels in valid_dist_ds strategy. No parameters necessary if TPU_NAME environment variable is set. default distribution strategy in Tensorflow. 372e 05 steps val_steps 99 29 V1 deotte ep 30 dd 4 augmix ep 30 dd TPU or GPU detection Competition data accessTPUs read data directly from Google Cloud Storage GCS. range aggregating losses returned from multiple batches becomes impractical. compile optimizer adam loss sparse_categorical_crossentropy metrics sparse_categorical_accuracy lr_callback tf. On Kaggle this is always the case. reset_states valid_accuracy. AugMix Deotte s spatial transforms Datasets Dataset visualizations Keras training Modelwith strategy. For optimal performance reading from multiple files at once and disregarding data order. Assumes RGB for now. The value of addition is between 128 and 128. to compute mins and maxes. If you have multiple datasets attached to the notebook you can pass the name of a specific dataset to the get_gcs_path function. trainable True False transfer learning True fine tuning model tf. experimental_distribute_dataset get_training_dataset valid_dist_ds strategy. Scale the image making the lowest value 0 and the highest value 255. summary Trainingstart_time time. sum n Peek at training data run this cell again for next set of images peer at test data run this cell again for next set of images False transfer learning True fine tuning Instiate optimizer with learning rate schedule this also works but is not very readable Instantiate metrics Loss The recommendation from the Tensorflow custom training loop documentation is loss_fn lambda a b tf. you can list the bucket with gsutil ls GCS_DS_PATH At this size a GPU will run out of memory. Extrapolate We need to clip and then cast. reset_states valid_loss. float32 STEPS_PER_EPOCH Instantiate metrics train_accuracy tf. update_state loss tf. is there any better way than manually typing all of these conditions I tried to randomly select transformation from array of functions but tensorflow didn t let me to you can play with these parameters level of transformations as described above in transformations integer from 1 to 10 number of different chains of transformations to be mixed number of transformations in one chain 1 means random from 1 to 3 returns 3x3 transformmatrix which transforms indicies CONVERT DEGREES TO RADIANS ROTATION MATRIX SHEAR MATRIX ZOOM MATRIX SHIFT MATRIX input image is one image of size dim dim 3 not a batch of b dim dim 3 output image randomly rotated sheared zoomed and shifted fix for size 331 GET TRANSFORMATION MATRIX LIST DESTINATION PIXEL INDICES ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS FIND ORIGIN PIXEL VALUES convert image to floats in 0 1 range explicit size needed for TPU tf. Using a custom training loop1. The validation dataset is made to repeat indefinitely because handling end of dataset exception in a TPU loop implemented with tf. functiondef valid_step images labels probabilities model images training False loss loss_fn labels probabilities update metrics valid_accuracy. range is not yet possible. summary Instiate optimizer with learning rate schedule class LRSchedule tf. format lrfn epoch flush True set up next epoch epoch step 1 STEPS_PER_EPOCH epoch_start_time time. Works on CPU and single GPU. the training dataset must repeat for several epochs slighly faster with fixed tensor sizes prefetch next batch while training autotune prefetch buffer size slighly faster with fixed tensor sizes prefetch next batch while training autotune prefetch buffer size prefetch next batch while training autotune prefetch buffer size def count_data_items filenames n int re. 68e 05 steps val_steps 99 29 ex2 ep 30 abort error after ep 21 time 46. This will introduce a slight inaccuracy because the validation dataset now has some repeated elements. experimental_run_v2 valid_step args image labels print end flush True compute metrics history. format epoch_time loss 0. org tutorials distribute custom_training define_the_loss_function This works too and shifts all the averaging to the training loop which is easier update metrics this loop runs on the TPU update metrics this loop runs on the TPU distribute the datset according to the strategy Hitting End Of Dataset exceptions is a problem in this setup. Compute the histogram of the image channel. Clip the counts to be in range. tfrec TRAINING_FILENAMES_IN17 TRAINING_FILENAMES_IN18 TRAINING_FILENAMES_IN19 TRAINING_FILENAMES_TF TRAINING_FILENAMES_OX else TRAINING_FILENAMES tf. 949 21sec ep B7 512x512 ep30 fail in ep2 test val on ext datasets B0 std train ep20 in17 in18 in19 ox tf IN17 18 19 Predictions Visual validation Timing summary Detect hardware return appropriate distribution strategy TPU detection. Dense len CLASSES activation softmax model. sparse_categorical_crossentropy would work the same. history sparse_categorical_accuracy 1 val_loss 0. trainable_variables optimizer. The validation dataset iterator is not reset between validation runs. string means bytestring shape means single element returns a dataset of image label pairs tf. compute_average_loss tf. 932B3 224x224 ep30 f1 score 0. Since the iterator is passed into the step function which is then compiled for TPU thanks to tf. update_state labels probabilities train_loss. time train_accuracy. update_state loss Training loopstart_time epoch_start_time time. append train_accuracy. This is achieved by placing a loop using tf. history sparse_categorical_accuracy history. tfrec TRAINING_FILENAMES_OX predictions on this dataset should be submitted for the competition 00 09 10 19 20 29 30 39 40 49 50 59 60 69 70 79 80 89 90 99 100 102 Learning rate schedule for TPU GPU and CPU. However whith the setting Tensorflow produces batches of a known size and although XLA the TPU compiler can now handle variable batches it is slightly faster on fixed batches. function and executed on TPU. sparse_categorical_crossentropy a b global_batch_size BATCH_SIZE Step functions tf. history loss 1 accuracy 0. 939B5 331x331 ep30 f1 score 0. fit whre the recommendation is to use a non repeating validation dataset run training step validation run at the end of each epoch validation run compute metrics report metrics set up next epoch since we are splitting the dataset and iterating separately on images and labels order matters. the training data iterator is repeated and it is not reset for each validation run same as model. Shift lut prepending with 0. disable order increase speed automatically interleaves reads from multiple files uses data as soon as it streams in rather than in its original order returns a dataset of image label pairs if labeled True or image id pairs if labeled False data augmentation. A possibly cheaper version can be done using cumsum unique_with_counts over the histogram values rather than iterating over the entire image. time start_timeprint SIMPLE CTL TRAINING TIME 0. For the purposes of computing the step filter out the nonzeros. functiondef train_step images labels with tf. numpy history. sparse_categorical_crossentropy a b global_batch_size BATCH_SIZE https www. org tutorials distribute custom_training define_the_loss_function Here a simpler loss_fn tf. SparseCategoricalAccuracy train_loss tf. Validation is adjusted to always use exactly or more than the entire validation dataset. It is possible to return values from step function and return losses in that way. This Kaggle utility will copy the dataset to a GCS bucket co located with the TPU. It is not the case here because the validation dataset happens to contain an integral number of batches. Xception weights imagenet include_top False input_shape IMAGE_SIZE 3 pretrained_model. Use ls kaggle input to list attached datasets. experimental_distribute_dataset get_validation_dataset print Steps per epoch STEPS_PER_EPOCH History namedtuple History history history History history loss val_loss sparse_categorical_accuracy val_sparse_categorical_accuracy epoch 0for step images labels in enumerate train_dist_ds run training step strategy. Scales each channel independently and then stacks the result. Compute the cumulative sum shifting by step 2 and then normalization by step. format epoch 1 EPOCHS print time 0. update_state labels probabilities valid_loss. Configuration Visualization utilitiesdata pixels nothing of much interest for the machine learning practitioner in this section. string means bytestring shape means single element class is missing this competitions s challenge is to predict flower classes for the test dataset returns a dataset of image s Read from TFRecords. loss_fn lambda a b tf. function passing a fresh iterator for every validation run would trigger a fresh recompilation. ", "id": "romanweilguny/custom-training-loop-flowers-tpu-deotte", "size": "10060", "language": "python", "html_url": "https://www.kaggle.com/code/romanweilguny/custom-training-loop-flowers-tpu-deotte", "git_url": "https://www.kaggle.com/code/romanweilguny/custom-training-loop-flowers-tpu-deotte", "script": "shear_x __call__ translate_y solarize_add load_dataset get_training_dataset title_from_label_and_target pyplot pyplot as plt blend inner_loop_body kaggle_datasets confusion_matrix shear_y scale_channel lrfn batch_to_numpy_images_and_labels build_lut numpy read_labeled_tfrecord translate_x efficientnet.tfkeras recall_score decode_image brightness inner_loop_cond f1_score substract_means data_augment get_test_dataset outer_loop_body contrast sample_level KaggleDatasets tensorflow valid_step_fn affine_transform int_parameter train_step int_div_round_up display_training_curves valid_step precision_score augmix read_unlabeled_tfrecord display_one_flower outer_loop_cond get_validation_dataset posterize display_confusion_matrix apply_op count_data_items resize train_step_fn color LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule) get_mat autocontrast matplotlib namedtuple scale_values transformDeotteSpacial normalize rotate display_batch_of_images float_parameter sklearn.metrics collections equalize solarize ", "entities": "(('we', 'always 0'), 'mean') (('validation', 'different model'), 'fit') (('challenge', 'TFRecords'), 'mean') (('epoch EPOCHS', 'simple_ctl_training_time time'), 'reset_states') (('this', 'epoch'), 'be') (('range', 'multiple batches'), 'become') (('which', 'thanks tf'), 'pass') (('all', 'images'), 'run') (('Hitting End', 'setup'), 'distribute') (('STEPS_PER_EPOCH epoch print step 1 end', 'valid_dist_ds strategy'), 'label') (('loop', 'tf'), 'compile') (('validation dataset', 'exactly once validation'), 'happen') (('Validation', 'validation entire dataset'), 'adjust') (('It', 'way'), 'be') (('step', 'zero original image'), 'return') (('ids order', 'separately images'), 'get') (('TRAINING_FILENAMES_IN18 TRAINING_FILENAMES_IN19 TRAINING_FILENAMES_TF TRAINING_FILENAMES_OX', 'TRAINING_FILENAMES_IN17'), 'tfrec') (('custom training step optimized training functions', 'multiple batches'), 'loop') (('notebooks', 'TPU'), 'show') (('group', 'np'), 'return') (('value', 'addition'), 'be') (('train_step functiondef images', 'tf'), 'label') (('drop_remainder infinitely True', 'BATCH_SIZE drop_remainder'), 'batch') (('GPU', 'memory'), 'list') (('Kaggle utility', 'TPU'), 'copy') (('validation dataset iterator', 'validation runs'), 'reset') (('format', 'lrfn True'), 'flush') (('TPU', 'gradients'), 'execute') (('we', '0'), 'add') (('validation', 'same model'), 'repeat') (('possibly cheaper version', 'rather entire image'), 'do') (('three models', 'use dataset'), 'use') (('format', 'keras_fit_training_time history'), 'display_training_curve') (('ORIGIN PIXEL VALUES', 'tf'), 'be') (('experimental_run_v2 valid_step args image labels print end', 'compute metrics True history'), 'flush') (('in19 18 Predictions validation Timing summary Detect ox tf 19 Visual hardware', 'distribution strategy TPU appropriate detection'), 'b7') (('org tutorials', 'custom_training define_the_loss_function'), 'distribute') (('Instantiate metrics also very readable recommendation', 'Tensorflow custom training loop documentation'), 'run') (('validation here dataset', 'batches'), 'be') (('V1 deotte dd GPU detection Competition data 05 99 29 ep 30 dd 4 augmix 30 accessTPUs', 'Google Cloud Storage directly GCS'), 'step') (('Configuration Visualization utilitiesdata', 'section'), 'pixel') (('you', 'get_gcs_path function'), 'pass') (('Starting', 'pre trained weights'), 'break') (('validation', 'now repeated elements'), 'introduce') (('next epoch we', 'separately images'), 'be') (('single element', 'image label pairs'), 'mean') (('This', 'tf'), 'achieve') (('we', 'data'), 'matter') (('prefetch', 'essentially free TPU'), 'happen') (('val_loss', '211'), 'history') (('optimal performance', 'data order'), 'for') (('validation dataset', 'tf'), 'make') (('loss loss_fn labels False probabilities', 'metrics valid_accuracy'), 'label') (('as soon it', 'data False augmentation'), 'return') (('ORIGIN PIXEL VALUES', 'float'), 'default') (('STEPS_PER_EPOCH namedtuple History history History history 0for step images History sparse_categorical_accuracy labels', 'train_dist_ds training step enumerate strategy'), 'step') (('value', 'threshold'), 'define') (('Losses', 'Keras metrics'), 'report') (('time', 'strategytrain_dist_ds strategy'), 'distribute') (('function', 'fresh recompilation'), 'trigger') (('training dataset', 'prefetch buffer def count_data_items next batch autotune size filenames'), 're') (('it', 'slightly fixed batches'), 'whith') (('LearningRateScheduler lrfn', 'True model'), 'verbose') (('TRAINING_FILENAMES_OX tfrec predictions', 'TPU GPU'), 'submit') (('it', 'directory'), 'be') (('Datasets Dataset visualizations Keras', 'strategy'), 'AugMix') (('True loss loss_fn labels probabilities', 'tape'), 'grad') (('trainable_variables', 'metrics train_accuracy'), 'update') "}