{"name": "build gan assignment 1 ", "full_name": " h1 Your First GAN h3 Goal h3 Learning Objectives h2 Getting Started h4 MNIST Dataset h4 Tensor h4 Batches h2 Generator h2 Noise h2 Discriminator h2 Training ", "stargazers_count": 0, "forks_count": 0, "description": "Upload it to Google Drive and open it with Google Colab3. html will be useful here. In general use torch. zeros you ll need to pass device device to them. Note that you may see a loss to be greater than 1 this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess. In addition be warned that this runs very slowly on a CPU. Important You should NOT write your own loss function here use criterion pred true Multiply Multiply Zero out the gradient before backpropagation Calculate discriminator loss Update gradients Check that they detached correctly Update optimizer Check that some discriminator weights changed UNQ_C7 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_gen_loss These are the steps you will need to complete 1 Create noise vectors and generate a batch of fake images. Needed for grading Verify the discriminator class Check there are three parts Check the linear layer is correct Set your parameters Load MNIST dataset as tensors UNQ_C6 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_disc_loss These are the steps you will need to complete 1 Create noise vectors and generate a batch num_images of fake images. randn https pytorch. For example consider changing the size of the hidden dimension or making the networks shallower or deeper by changing the number of layers. Before you train your GAN you will need to create functions to calculate the discriminator s loss and the generator s loss. TensorYou will represent the data using tensors https pytorch. html for stabilization and finally a non linear activation function you use a ReLU here https pytorch. detach on the generator result to ensure that only the discriminator is updated Remember that you have already defined a loss function earlier criterion and you are encouraged to use torch. Train your GAN and visualize the generated images. NoiseTo be able to use your generator you will need to be able to create noise vectors. org docs master generated torch. Remember the generator wants the discriminator to think that its fake images are real Important You should NOT write your own loss function here use criterion pred true Multiply Multiply Check that the loss is reasonable UNQ_C8 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION Whether the generator should be tested Dataloader returns the batches Flatten the batch of real images from the dataset Zero out the gradients before backpropagation Calculate discriminator loss Update gradients Update optimizer For testing purposes to keep track of the generator weights Hint This code will look a lot like the discriminator updates These are the steps you will need to complete 1 Zero out the gradients. zeros_like instead of torch. GeneratorThe first step is to build the generator component. com uc export view id 1ePf0x1EkkK0Ll0V5lxsZQl5aJ1cEOYOr https drive. For example a ground truth tensor for a fake image is all zeros. Getting StartedYou will begin by importing some useful packages and the dataset you will use to build and train your GAN. Finally to use your discrimator s neural network you are given a forward pass function that takes in an image tensor to be classified. 2 Calculate the generator loss assigning it to gen_loss. Verify the generator block function Check the three parts Check the output shape UNQ_C2 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION Generator Build the neural network There is a dropdown with hints if you need them Needed for grading Verify the generator class Check there are six modules in the sequential part Check that the output shape is correct UNQ_C3 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_noise NOTE To use this on GPU with device cuda make sure to pass the device argument to the function you use to generate the noise. 5 minutes MNIST Digits https drive. You will learn more about this in the following lectures REctified Linear Unit ReLU Leaky ReLU https drive. Note that each optimizer only takes the parameters of one particular model since we want each optimizer to optimize only one of the models. After you ve submitted a working version with the original architecture feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. 3 Backprop through the generator update the gradients and optimizer. Now you can initialize your generator discriminator and optimizers. This is how the discriminator and generator will know how they are doing and improve themselves. 2 Get the discriminator s prediction of the fake image. You will learn more about activations and batch normalization later in the course. Note that whenever you create a new tensor using torch. You do not need to do this if you re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input such as torch. Linear https pytorch. One way to run this more quickly is to use Google Colab 1. Each block should include a linear transformation https pytorch. The final layer does not need a normalization or activation function but does need to be scaled with a sigmoid function https pytorch. com uc export view id 1BlfFNZACaieFrOjMv_o2kGqwAR6eiLmN Training dataset Set for testing purposes please do not change UNQ_C1 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_generator_block Hint Replace all of the None with the appropriate dimensions. png You may notice that the images are quite pixelated this is because they are all only 28 x 28 The small size of its images makes MNIST ideal for simple training. Finally you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network. The noise vector z has the important role of making sure the images generated from the same class don t all look the same think of it as a random seed. MNIST DatasetThe training images your discriminator will be using is from a dataset called MNIST http yann. For every batch you will need to update the discriminator and generator using their loss. com uc export view id 1Qp0aiob39MKPGVw6OecYXBhhlx c MpB Now you can use these blocks to make a discriminator The discriminator class holds 2 values The image dimension The hidden dimensionThe discriminator will build a neural network with 4 layers. Since the generator is needed when calculating the discriminator s loss you will need to call. You will probably find torch. This means that your generator will generate an entire batch of images and receive the discriminator s feedback on each before updating the model. It s also often the case that the discriminator will outperform the generator especially at the start because its job is easier. ones 3 3 device device or move it onto the target device using torch. Your First GAN GoalIn this notebook you re going to create your first generative adversarial network GAN for this course Specifically you will build and train a GAN that can generate hand written images of digits 0 9. 3 Calculate the generator s loss. You will start by creating a function to make a single layer block for the generator s neural network. Finally you can put everything together For each epoch you will process the entire dataset in batches. org docs stable index. You need a ground truth tensor in order to calculate the loss. Don t forget to detach the generator Remember the loss function you set earlier criterion. Create generator and discriminator loss functions. Make the runtime type GPU under Runtime Change runtime type Select GPU from the dropdown 4. The documentation may be useful if you re less familiar with PyTorch https pytorch. Beginning with the noise vector the generator will apply non linear transformations via the block function until the tensor is mapped to the size of the image to be outputted the same size as the real images from MNIST. You will need to fill in the code for final layer since it is different than the others. This output classifies whether an image is fake or real. It contains 60 000 images of handwritten digits from 0 to 9 like these MNIST Digits https upload. html to map to another shape a batch normalization https pytorch. The results will get better with later lessons as you learn methods to help keep your generator and discriminator at similar levels. Make sure your get_noise function uses the right device But remember don t expect anything spectacular this is only the first lesson. Optional hints for Generator1. It will start with the image tensor and transform it until it returns a single number 1 dimension tensor output. org wikipedia commons 2 27 MnistExamples. 2 UNQ_C5 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION Discriminator Hint You want to transform the final output into a single value so add one more linear map. As with the generator component you will start by creating a function that builds a neural network block for the discriminator. zeros where possible. org docs stable generated torch. Additionally these images are also in black and white so only one dimension or color channel is needed to represent them more on this later in the course. Build the generator and discriminator components of a GAN from scratch. Replace device cpu with device cuda 5. You should roughly expect to see this progression. 2 Get the discriminator s prediction of the fake image and calculate the loss. Tensors are a generalization of matrices for example a stack of three matrices with the amounts of red green and blue at different locations in a 64 x 64 pixel image is a tensor with the shape 3 x 64 x 64. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. These are called batches. The hints will also often include links to relevant documentation. Optional hint for get_noise1. org the machine learning library you will be using. Now you can build the generator class. Learning Objectives1. You will be using PyTorch in this specialization so if you re not familiar with this framework you may find the PyTorch documentation https pytorch. org docs stable tensors. Note You use leaky ReLUs to prevent the dying ReLU problem which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU which result in a zero gradient. 4 Calculate the discriminator s loss by averaging the real and fake loss and set it to disc_loss. 3 Get the discriminator s prediction of the real image and calculate the loss. The same goes for the discriminator it will calculate its loss on the entire batch of generated images as well as on the reals before the model is updated. In GANs and in machine learning in general you will process multiple images per training step. Sigmoid https pytorch. Verify the noise vector function Make sure a normal distribution was used UNQ_C4 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_discriminator_block Verify the discriminator block function Check there are two parts Check that the shape is right Check that the LeakyReLU slope is about 0. On a GPU this should take about 15 seconds per 500 steps on average while on CPU it will take roughly 1. Note that you do not need a sigmoid after the output layer since it is included in the loss function. randn you either need to create it on the target device e. Feel free to explore them more but you can imagine these as multi dimensional matrices or vectors BatchesWhile you could train your model after generating one image it is extremely inefficient and leads to less stable training. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments. html so the output can be transformed in complex ways. The output size of the final linear transformation should be im_dim but remember you need to scale the outputs between 0 and 1 using the sigmoid function. Tensors are easy to manipulate and supported by PyTorch https pytorch. Make sure to pass the device argument to the noise. DiscriminatorThe second component that you need to construct is the discriminator. You are also provided with a visualizer function to help you investigate the images your GAN will create. TrainingNow you can put it all together First you will set your parameters criterion the loss function n_epochs the number of times you iterate through the entire dataset when training z_dim the dimension of the noise vector display_step how often to display visualize the images batch_size the number of images per forward backward pass lr the learning rate device the device type here using a GPU which runs CUDA not CPUNext you will load the MNIST dataset as tensors using a dataloader. Since multiple images will be processed per pass you will generate all the noise vectors at once. Batches are sets of images that will be predicted on before the loss functions are calculated instead of calculating the loss function after each image. For testing purposes to check that your code changes the generator weights Keep track of the average discriminator loss Keep track of the average generator loss. It will take 3 values The noise vector dimension The image dimension The initial hidden dimensionUsing these values the generator will build a neural network with 5 layers blocks. Remember to pass the device argument to the get_noise function. ones_like and torch. It s important that neither one gets too good that is near perfect accuracy which would cause the entire model to stop learning. ", "id": "amoghjrules/build-gan-assignment-1", "size": "11022", "language": "python", "html_url": "https://www.kaggle.com/code/amoghjrules/build-gan-assignment-1", "git_url": "https://www.kaggle.com/code/amoghjrules/build-gan-assignment-1", "script": "get_gen test_get_noise test_discriminator DataLoader test_disc_loss get_gen_loss test_generator show_tensor_images get_disc MNIST # Training dataset nn test_gen_block tqdm test_gen_reasonable transforms torchvision make_grid matplotlib.pyplot Generator(nn.Module) tqdm.auto forward test_gen_loss Discriminator(nn.Module) get_discriminator_block get_disc_loss torch.utils.data get_noise __init__ torch get_generator_block torchvision.datasets test_disc_block test_disc_reasonable torchvision.utils ", "entities": "(('right LeakyReLU slope', 'discriminator block function'), 'make') (('first step', 'generator component'), 'be') (('you', 'noise vectors'), 'generate') (('you', 'fake images'), 'write') (('you', 'machine learning library'), 'org') (('entire model', 'perfect accuracy'), 's') (('One way', 'Google more quickly Colab'), 'be') (('model', 'as well reals'), 'go') (('you', 'PyTorch documentation https pytorch'), 'use') (('linear activation finally non you', 'https ReLU here pytorch'), 'html') (('single layer', 'neural network'), 'start') (('whenever you', 'torch'), 'note') (('it', 'others'), 'need') (('that', 'image tensor'), 'give') (('ground truth tensor', 'fake image'), 'be') (('it', 'dimension tensor single number 1 output'), 'start') (('dimensionThe hidden discriminator', '4 layers'), 'com') (('You', 'roughly progression'), 'expect') (('you', 'target device e.'), 'randn') (('You', 'batch later course'), 'learn') (('you', 'them'), 'zero') (('look', 'random seed'), 'have') (('GAN', 'images'), 'provide') (('general you', 'training step'), 'process') (('stack', 'shape'), 'be') (('you', 'batches'), 'put') (('how they', 'themselves'), 'be') (('also black only one dimension', 'later course'), 'be') (('generator', 'layers 5 blocks'), 'take') (('TensorYou', 'tensors https pytorch'), 'represent') (('tensors', 'dataloader'), 'put') (('you', 'that'), 'be') (('that', 'discriminator'), 'start') (('you', 'similar levels'), 'get') (('EDIT GRADED DO FUNCTION', 'appropriate dimensions'), 'com') (('cross entropy loss', 'positive sufficiently confident wrong guess'), 'note') (('you', 'torch'), 'detach') (('you', 'GAN'), 'get') (('discriminator', 'dataset'), 'be') (('You', 'loss'), 'need') (('loss functions', 'image'), 'be') (('you', 'noise'), 'verify') (('Now you', 'generator discriminator'), 'initialize') (('you', 'PyTorch https less pytorch'), 'be') (('You', 'following lectures'), 'learn') (('x all only 28 small size', 'simple training'), 'notice') (('code', 'generator average loss'), 'check') (('3', 'loss'), 'get') (('this', 'very slowly CPU'), 'warn') (('so output', 'complex ways'), 'transform') (('it', 'roughly 1'), 'take') (('It', 'MNIST'), 'contain') (('final layer', 'function https sigmoid pytorch'), 'need') (('that', '0 9'), 'GoalIn') (('job', 'especially start'), 's') (('spectacular this', 'anything'), 'make') (('you', 'noise vectors'), 'be') (('networks', 'layers'), 'consider') (('you', 'when loss'), 'need') (('you', 'earlier criterion'), 'forget') (('that', 'such torch'), 'need') (('generator', 'model'), 'mean') (('which', 'zero gradient'), 'note') (('you', 'gradients'), 'remember') (('you', 'fake images'), 'need') (('it', 'loss function'), 'note') (('hints', 'relevant documentation'), 'include') (('tensor', 'MNIST'), 'apply') (('block', 'transformation https linear pytorch'), 'include') (('UNQ_C5 UNIQUE CELL GRADED Discriminator 2 You', 'so one more linear map'), 'IDENTIFIER') (('Tensors', 'PyTorch https pytorch'), 'be') (('how different architectural choices', 'better GANs'), 'submit') (('it', 'extremely less stable training'), 'be') (('optimizer', 'models'), 'note') (('you', 'later lectures'), 'be') (('you', 'loss'), 'need') (('You', 'normal distribution'), 'generate') (('you', 'sigmoid function'), 'be') (('that', 'neural network'), 'give') "}