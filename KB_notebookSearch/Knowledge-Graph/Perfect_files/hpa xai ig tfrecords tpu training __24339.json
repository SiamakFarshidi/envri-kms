{"name": "hpa xai ig tfrecords tpu training ", "full_name": " h1 Human Protein Atlas Single Cell Classification h2 Exploratory Data Analysis EDA h2 TABLE OF CONTENTS h3 0 xa0 xa0 xa0 xa0IMPORTS h3 1 xa0 xa0 xa0 xa0BACKGROUND INFORMATION h3 2 xa0 xa0 xa0 xa0SETUP h3 3 xa0 xa0 xa0 xa0HELPER FUNCTIONS h3 4 xa0 xa0 xa0 xa0DATASET PREPERATION h3 5 xa0 xa0 xa0 xa0MODEL BUILDING h3 6 xa0 xa0 xa0 xa0MODEL TRAINING h3 1 1 WHAT IS EXPLAINABLE AI GENERAL INFO h3 1 2 THE GOAL h3 1 3 CURRENT XAI APPROACHES h3 1 4 WHERE IS XAI NEEDED h3 1 4 Why Is XAI Needed The Case For Growing Global AI Regulation h3 6 1 Model Paramaters Configuration h3 6 2 Model Creation Initialization ", "stargazers_count": 0, "forks_count": 0, "description": "Feature Ablation Perturbation Assigns an importance score to each input feature based on the magnitude changes in model output or loss when those features are replaced by a baseline usually zeros based on an input feature mask. count_nonzero y_pred y axis 0 tf. As an example consider this image https commons. float32 f1 2 tp 2 tp fn fp 1e 16 macro_f1 tf. Args y int32 Tensor targets array of shape BATCH_SIZE N_LABELS y_hat float32 Tensor probability matrix from forward propagation of shape BATCH_SIZE N_LABELS Returns cost scalar Tensor value of the cost function for the batch y tf. 1 WHAT IS EXPLAINABLE AI GENERAL INFO For the purposes of this notebook and my explanation I will be logically seperating explainable AI into two seperate branches. 0737 https arxiv. 2701 Establishes a commission on automated decision making transparency fairness and individual rights. take N_VAL train_ds train_ds. images text structured data ease of implementation theoretical justifications and computational efficiency relative to alternative approaches that allows it to scale to large networks and feature spaces such as images. org tutorials interpretability images IG_fireboat. DeepLiftSHAP Gradient An extension of DeepLift that approximates SHAP values. Human Protein Atlas Single Cell ClassificationExploratory Data Analysis EDA CREATED BY DARIEN SCHETTLERTABLE OF CONTENTS 0 nbsp nbsp nbsp nbsp IMPORTS 1 nbsp nbsp nbsp nbsp BACKGROUND INFORMATION 2 nbsp nbsp nbsp nbsp SETUP 3 nbsp nbsp nbsp nbsp HELPER FUNCTIONS 4 nbsp nbsp nbsp nbsp DATASET PREPERATION 5 nbsp nbsp nbsp nbsp MODEL BUILDING 6 nbsp nbsp nbsp nbsp MODEL TRAINING 0 nbsp nbsp IMPORTS1 nbsp nbsp XAI BACKGROUND INFORMATION1. You would classify this image as a fireboat and might highlight the pixels making up the boat and water cannons as being important to your decision. It adds gaussian noise to each input example samples times selects a random point between each sample and randomly drawn baseline from baselines distribution computes the gradient for it and multiples it with input baseline. Often the explanations arising through this branch are obvious to humans or already known. Use probability values instead of binary predictions. Create a dictionary mapping the feature name to the tf. It has many use cases including understanding feature importances identifying data skew and debugging model performance. arXiv preprint arXiv 1907. pdf PERCEPTIVE In short this is interpretability that can be observed by humans. If a features is located in multiple hyper rectangles the importance scores are averaged across those hyper rectangles. reduce_mean cost average on all labels return macro_cost def macro_f1 y y_hat thresh 0. shard 4 index 2 blue_train_ds train_ds. skip N_VAL val_ds val_ds. IG Example https www. Args y int32 Tensor targets array of shape BATCH_SIZE N_LABELS y_hat float32 Tensor probability matrix of shape BATCH_SIZE N_LABELS Returns cost scalar Tensor value of the cost function for the batch y tf. In it we will walk through an implementation of IG step by step to understand the pixel feature importances of an image classifier. Although occasionally post hoc analysis is required or basic explainability tools. Finally the latter is multiplied by input baseline. GradientSHAP Gradient Approximates SHAP values based on the expected gradients. selected input layer averages them for each output channel and multiplies with the layer activations. ipynb I ntegrated G radients IG aims to explain the relationship between a model s predictions in terms of its features. png 3 nbsp nbsp NOTEBOOK SETUP4 nbsp nbsp HELPER FUNCTIONS5 nbsp nbsp DATASET PREPERATION6 nbsp nbsp MODEL CREATION6. 1 Model Paramaters Configuration 6. If there are any RELUs present in the model their gradients will be overridden so that only positive gradients of the inputs in case of Guided BackProp and outputs in case of deconvnet are back propagated. 3 CURRENT XAI APPROACHES Algorithm Type Description Integrated Gradients Gradient Approximates the integral of gradients along the path straight line from baseline to input sand multiplies with input baseline DeepLift Application Explains differences in the non linear activations outputs in terms of the differences of the input from its corresponding reference. IG is an Explainable AI XAI technique introduced in the paper Axiomatic Attribution for Deep Networks https arxiv. reduce_sum y_hat 1 y axis 0 fn tf. Example compatible data type. VIEW SCHEDULE saving in Tensorflow s SavedModel format BytesList won t unpack a string from an EagerTensor. batch BATCH_SIZE. Layer Conductance Gradient Decomposes integrated gradients via chain rule. 2 nbsp nbsp INTEGRATED GRADIENTS BACKGROUND INFORMATIONThis notebook will show how to implement Integrated Gradients IG for this competition. float32 fp tf. The original image sizes used for every version of EfficientNet are EfficientNetB0 224 224 3 EfficientNetB1 240 240 3 EfficientNetB2 260 260 3 EfficientNetB3 300 300 3 EfficientNetB4 380 380 3 EfficientNetB5 456 456 3 EfficientNetB6 528 528 3 EfficientNetB7 600 600 3 I LL BE USING EfficientNetB2 WITH 512x512x3 INPUT 7 nbsp nbsp MODEL TRAINING8 nbsp nbsp APPENDIX HOW TO RECREATE THE TFRECORDS For later visualization Machine Learning and Data Science Imports Built In Imports Visualization Imports PRESETS TPU detection no TPU found detect GPUs for GPU or multi GPU machines Define the root data directory Define the paths to the training and testing tfrecord and image folders respectively Capture all the relevant full image paths Capture all the relevant full tfrec paths Define paths to the relevant csv files Create the relevant dataframe objects Not the lowest as it is very underrepresented explicit size needed for TPU Defaults are not specified since both keys are required. As an example for the difficulty with perceptive interpretability when a visual evidence is given erroneously the underlying mathematical structure may not seem to provide useful clues on the mistakes. count_nonzero y_pred 1 y axis 0 tf. org wiki File San_Francisco_fireboat_showing_off. We are however not bound by this and can use a smaller larger size if we want. Guided BackProp DeconvNet Gradient Computes the gradients of the model outputs w. com github tensorflow docs blob master site en tutorials interpretability integrated_gradients. 2 Model Creation Initialization The original image size from the EfficientNet paper for EfficientNetB2 is 260x260x3. Final SHAP values represent the expected values of gradients input baseline for each input example. Starting with a high LR would break the pre trained weights. Input Gradient Gradient Multiplies model inputs with the gradients of the model outputs w. float32 fn tf. MATHEMATICAL In short this is interpretability that can only be observed by first applying mathematical manipulations to the data. This is not an exhaustive list of black box models. reduce_sum 1 y_hat y axis 0 tn tf. to the neurons multiplied by the gradients of the neurons w. A survey on explainable artificial intelligence XAI Towardsmedical XAI. For each input example it considers a distribution of baselines and computes the expected value of the attributions based on DeepLift algorithm across all input baseline pairs. co ZXdBQ4D Screen Shot 2020 07 07 at 10 24 16 AM. Layer Internal Influence Gradient Approximates the integral of gradients along the path from baseline to inputs for selected input layer. org wiki T distributed_stochastic_neighbor_embedding 1. Layer Gradient Activation Gradient Computes element wise product of layer activations and the gradient of the output w. reduce_sum 1 y_hat 1 y axis 0 soft_f1_class1 2 tp 2 tp fn fp 1e 16 soft_f1_class0 2 tn 2 tn fn fp 1e 16 cost_class1 1 soft_f1_class1 reduce 1 soft f1_class1 in order to increase soft f1 on class 1 cost_class0 1 soft_f1_class0 reduce 1 soft f1_class0 in order to increase soft f1 on class 0 cost 0. Washington Bill 1655 Establishes guidelines for the use of automated decision systems to protectconsumers improve transparency and create more market predictability. 2 THE GOAL Pull the veil back on black box machine learning models and help users understand how why a model makes the decisions that it does. Average 1 soft F1 across all labels. NoiseTunnel Depends on the choice of above mentioned attribution algorithm 1. Linear Logistic Regression Decision Trees K Nearest Neighbors Rule Based Learners General Additive Models Bayesian ModelsBLACK BOX MODELSThese are models algorithms that are NOT easily interpretable and DO requre XAI. See below list was created roughly a year ago GDPR Article 22 empowers individuals with the right to demand an explanation of how anautomated system made a decision that affects them. It is simply the more common black box models. count_nonzero 1 y_pred y axis 0 tf. float32 tp tf. See this excerpt paper that explains the branches in more detail. An example technique that most are familiar with is clustering t SNE https en. Occlusion Perturbation Assigns an importance score to each input feature based on the magnitude changes in model output when those features are replaced by a baseline usually zeros using rectangular sliding windows and sliding strides. 4 WHERE IS XAI NEEDED Obviously things like the weakly supervised tasks in this competition may require XAI XAI can be used for a wide range of things that we won t get into here protecting against bias protecting against overfitting detecting features etc. They can only be easily perceived once the pattern is brought into lower dimensions abstracting some fine grained information we could not yet prove is not discriminative with measurable certainty. 4 Why Is XAI Needed The Case For Growing Global AI Regulation Many regulatory bodies have begun to encourage or enforce explainability in predictive algorithms used in the public domain. Go to this notebook to see the implementation of IG. the inputs along the path from baseline to inputs. 5 Compute the macro F1 score on a batch of observations average F1 across labels Args y int32 Tensor labels array of shape BATCH_SIZE N_LABELS y_hat float32 Tensor probability matrix from forward propagation of shape BATCH_SIZE N_LABELS thresh probability value above which we predict positive Returns macro_f1 scalar Tensor value of macro F1 for the batch y_pred tf. Massachusetts Bill H. Feature Permutation Perturbation Assigns an importance score to each input feature based on the magnitude changes in model output or loss when those features are permuted based on input feature mask. This can inform on how to improve the model as well as being useful for identifying things like bias and overfitting XAI APPROACHES https i. Tree Ensembles Support Vector Machines Multi Layer Neural Network MLPNN Convolutional Neural Network CNN Recurrent Neural Network RNN 1. Algorithmic Accountability Act 2019 Requires companies to provide an assessment of the risks posed bythe automated decision system to the privacy or security and the risks that contribute to inaccurate unfair biased or discriminatory decisions impacting consumers California Consumer Privacy Act Requires companies to rethink their approach to capturing storing and sharing personal data to align with the new requirements by January 1 2020. pdf The two major categories presented here namely perceptive interpretability and interpretability by mathematical structures appear to present different polarities within the notion of interpretability. To understand what that is we will see the definitions and examples of the terms Transparent and Black Box Models TRANSPARENT MODELSThese are models algorithms that are easily interpretable and DO NOT generally requre XAI. Layer Activation Computes the inputs or outputs of selected layer. IG has become a popular interpretability technique due to its broad applicability to any differentiable model e. reduce_sum 1 y_hat y axis 0 soft_f1 2 tp 2 tp fn fp 1e 16 cost 1 soft_f1 reduce 1 soft f1 in order to increase soft f1 macro_cost tf. Shapely Value Sampling Perturbation Similar to Shapely value but instead of considering all feature permutations it considers only samples random permutations. Create a Features message using tf. Your model will also classify this image as a fireboat later on in this tutorial however does it highlight the same pixels as important when explaining its decision In the images below titled IG Attribution Mask and Original IG Mask Overlay you can see that your model instead highlights in purple the pixels comprising the boat s water cannons and jets of water as being more important than the boat itself to its decision. On the other hand a mathematical analysis of patterns may provideinformation in high dimensions. Saliency Gradient The gradients of the output w. reduce_sum y_hat y axis 0 fp tf. It approximates the integral of gradients defined by a chain rule described as the gradients of the output w. 5 cost_class1 cost_class0 take into account both class 1 and class 0 macro_cost tf. One other place XAI can be used is when working with black box models. greater y_hat thresh tf. Guided GradCam Gradient Computes the element wise product of Guided BackProp and up sampled positive GradCam attributions. shard 4 index 1 36 is length of id always SEEDING KERNEL INIT val_ds train_ds. AUTOTUNE loading in Tensorflow s SavedModel format def macro_double_soft_f1 y y_hat Compute the macro soft F1 score as a cost average 1 soft F1 across all labels. Shapely Value Perturbation Computes feature importances based on all permutations of all input features. 01365 LINKS nbsp nbsp nbsp nbsp Tensorflow Google Colab This Is Heavily Based Off Of https colab. Illinois House Bill 3415 States predictive data analytics determining creditworthiness or hiringdecisions may not include information that correlates with the applicant race or zip code. reduce_mean cost average on all labels return macro_cost def macro_soft_f1 y y_hat Compute the macro soft F1 score as a cost. How will your model generalize to new fireboats What about fireboats without water jets Read on to learn more about how IG works and how to apply IG to your models to better understand the relationship between their predictions and underlying features. LayerGradCam Gradient Computes the gradients of model outputs w. Define a parser None or path to model num_parallel_reads None forces the order to be preserved See an example See examples ANNOYINGLY THIS DOES NOT WORK AS THE QUADRUPLE OF CHANNEL IMAGES IS IN A DIFFERENT ORDER EVERY TIME red_train_ds train_ds. It adds each feature for each permutation one by one to the baseline and computes the magnitudes of output changes for each feature which are ultimately being averaged across all permutations to estimate final attribution score. This version uses the computation of soft F1 for both positive and negative class for each label. jpg of a fireboat spraying jets of water. reduce_mean f1 return macro_f1 Using an LR ramp up because fine tuning a pre trained model. shard 4 index 3 green_train_ds train_ds. float32 y_hat tf. shard 4 index 0 yellow_train_ds train_ds. ", "id": "dschettler8845/hpa-xai-ig-tfrecords-tpu-training", "size": "24339", "language": "python", "html_url": "https://www.kaggle.com/code/dschettler8845/hpa-xai-ig-tfrecords-tpu-training", "git_url": "https://www.kaggle.com/code/dschettler8845/hpa-xai-ig-tfrecords-tpu-training", "script": "Counter plotly.graph_objects preprocess_tfrec_ds tensorflow_addons plotly.express kaggle_datasets augment macro_soft_f1 lrfn numpy Image serialize seaborn get_backbone ListedColormap decode_image _int64_feature tif_gzip_to_png glob get_class_wts plot_rgb tqdm str_2_multi_hot_encoding convert_rgby_to_rgb matplotlib.patches KaggleDatasets tensorflow matplotlib.pyplot plot_ex macro_f1 PIL matplotlib.colors download_and_convert_tifgzip_to_png pandas macro_double_soft_f1 datetime tqdm.notebook get_new_data decode add_head_to_bb _float_feature _bytes_feature collections flatten_list_of_lists load_image ", "entities": "(('erroneously underlying mathematical structure', 'mistakes'), 'seem') (('You', 'decision'), 'classify') (('water jets', 'predictions'), 'generalize') (('which', 'attribution final score'), 'add') (('we', 'image classifier'), 'walk') (('It', 'model data skew performance'), 'have') (('models that', 'easily XAI'), 'Trees') (('Tensorflow Google 01365 nbsp This', 'https Heavily colab'), 'LINKS') (('fn 1 2 16 soft_f1', 'f1 soft macro_cost'), 'reduce_sum') (('QUADRUPLE', 'DIFFERENT ORDER'), 'define') (('when features', 'input feature mask'), 'assign') (('that', 'humans'), 'PERCEPTIVE') (('Often explanations', 'humans'), 'be') (('XAI', 'box when black models'), 'be') (('importance scores', 'hyper rectangles'), 'rectangle') (('it', 'feature such images'), 'ease') (('that', 'data'), 'MATHEMATICAL') (('NoiseTunnel', 'algorithm'), 'depend') (('reduce_mean cost average', 'cost'), 'return') (('reduce_mean cost average', 'macro_cost def'), 'return') (('we', 'smaller larger size'), 'bind') (('most', 'SNE https'), 'cluster') (('BackProp DeconvNet Guided Gradient', 'model outputs w.'), 'compute') (('when features', 'strides'), 'assign') (('that', 'them'), 'create') (('it', 'only random permutations'), 'perturbation') (('Value Perturbation Shapely Computes', 'input features'), 'feature') (('IG', 'Axiomatic Deep Networks https arxiv'), 'be') (('notebook', 'competition'), 'INTEGRATED') (('that', 'more detail'), 'see') (('IG', 'model differentiable e.'), 'become') (('it', 'input baseline pairs'), 'consider') (('Layer Gradient Activation Gradient Computes', 'output w.'), 'element') (('This', 'box exhaustive black models'), 'be') (('This', 'XAI APPROACHES https i.'), 'inform') (('N_LABELS Returns', 'tf'), 'target') (('Finally latter', 'input baseline'), 'multiply') (('example', 'image https commons'), 'consider') (('nbsp HELPER 3 nbsp 4 nbsp nbsp DATASET', 'BACKGROUND nbsp 0 1 2 nbsp SETUP'), 'EDA') (('Model Creation image 2 original size', 'EfficientNetB2'), 'initialization') (('we', 'detecting features overfitting etc'), '4') (('Guided GradCam Gradient', 'GradCam up positive attributions'), 'compute') (('models that', 'easily generally XAI'), 'see') (('It', 'output w.'), 'approximate') (('CURRENT 3 XAI', 'corresponding reference'), 'explain') (('version', 'label'), 'use') (('that', 'SHAP values'), 'Gradient') (('model', 'decision'), 'classify') (('2701', 'transparency fairness'), 'Establishes') (('AUTOTUNE', 'labels'), 'format') (('IG', 'features'), 'ntegrate') (('example samples times', 'input baseline'), 'add') (('Starting', 'pre trained weights'), 'break') (('Washington Establishes 1655 guidelines', 'market more predictability'), 'Bill') (('Layer Activation', 'selected layer'), 'compute') (('cost_class1 5 cost_class0', 'account'), 'take') (('Layer Internal Influence Gradient', 'input selected layer'), 'approximate') (('input selected layer', 'layer activations'), 'average') (('that', 'applicant race'), 'include') (('SHAP Final values', 'example'), 'represent') (('I', 'two seperate branches'), '1') (('it', 'that'), '2') (('mathematical analysis', 'high dimensions'), 'provideinformation') (('that', 'January'), 'require') (('scalar Tensor macro_f1 value', 'batch'), 'Compute') (('Many regulatory bodies', 'public domain'), '4') (('here namely perceptive interpretability', 'interpretability'), 'pdf') (('we', 'yet measurable certainty'), 'perceive') (('1 1e 16 cost_class1 soft_f1_class1', 'class 0 cost'), 'reduce_sum') (('1 i', 'KERNEL always INIT'), 'be') (('Layer Conductance Gradient Decomposes', 'chain rule'), 'integrate') (('only positive gradients', 'deconvnet'), 'be') (('keys', 'TPU Defaults'), 'be') (('Gradient Approximates', 'expected gradients'), 'SHAP') (('VIEW SCHEDULE', 'EagerTensor'), 'win') (('Gradient', 'model outputs w.'), 'LayerGradCam') "}