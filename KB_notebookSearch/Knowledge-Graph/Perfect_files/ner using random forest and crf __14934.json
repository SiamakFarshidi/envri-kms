{"name": "ner using random forest and crf ", "full_name": " h1 Named Entity Recognition using GMB Groningen Meaning Bank corpus h2 Objective h2 Background h3 Understanding Named Entity Recognition h2 Data source h2 Analysis pipeline the OSEMN approach h3 Environment set up and loading dependencies h2 Obtain the data h2 Scrubbing Cleaning the data h2 Exploring Visualizing our data h2 Modeling the data h3 Performance metrics h3 Random Forest classifier h3 Conditional Random Fields classifier h3 Hyperparameter tuning using Randomized CV Search h2 Interpreting the results h2 References ", "stargazers_count": 0, "forks_count": 0, "description": "Using a LSTM neural network and creating a ensemble model with CRF. References SKLearn CRF Documentation https sklearn crfsuite. Commonly Used Types of Named Entity NE Type Examples ORGANIZATION Georgia Pacific Corp. Lets check the dataset again without the O tags. Lets find the number of words in the dataset Lets visualize how the sentences are distributed by their length Lets find out the longest sentence length in the dataset Words tagged as B org Words tagged as I org Words tagged as B per Words tagged as I per Words tagged as B geo Words tagged as I geo Words distribution across Tags Words distribution across Tags without O tag Words distribution across POS Simple feature map to feed arrays into the classifier. Analysis pipeline the OSEMN approach Obtain the data The dataset is an extract of the GMB corpus which is tagged annotated. pdf Survey of different NER techniques https nlp. Random Forest classifier We will use 5 fold cross validation as an input parameter to the classifier i. We can either work on this model alone by improving the features or ensembling it with a more contextual model or use a different model altogether. Exploring Visualizing our data Before going further we will try to understand what the dataset is all about and what all the features mean. This will help us in understanding what each tag type and sub type represents. Whereas a discrete classifier predicts a label for a single sample without considering neighboring samples a CRF can take context into account e. True Positives TP These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. It is in tab separated text file. Classification reports are used to obtain the values of these metrics in a text format per class. We will be using the LGBFS algorithm Gradient descent using the L BFGS method and it works best using a limited amount of computer memory. Using state of the art deep learning models using Keras or Tensor Flow deep learning models are known to outperform every other model when it comes to huge datasets therefore they can be used to create a superior NER classifier. Gradient Descent will be used as an optimization function. In particular a tagger can be built that labels each word in a sentence using the IOB format where chunks are labelled by their appropriate type. for each word and also consider the consecutive words in the list. Secondly RF is considered one of the most accurate classifier available. it is not common in this dataset to have a location right after an organization name i. Background Understanding Named Entity RecognitionNamed Entity Recognition or Named Entity Recognition and Classification NERC is probably the first step towards information extraction that seeks to locate and classify named entities in text into pre defined categories. Named Entity Recognition using GMB Groningen Meaning Bank corpus ObjectiveNamed Entity Recognition for annotated corpus using GMB Groningen Meaning Bank corpus for entity classification with enhanced and popular features by Natural Language Processing applied to the data set. Maybe the model is again remembering words and not taking into the context information completely. Precision TP TP FP Recall Sensitivity Recall is the ratio of correctly predicted positive observations to the all observations in actual class yes. I ORG B LOC has a negative weight. Unfortunately GMB is not perfect. edu sekine papers li07. Since we are dealing with Information Extraction we will use the following metrics to evaluate the models Precision Recall F1 score The metrics mentioned above are calculated using True False positives and True False negatives respectively. It requires huge computing resources to achieve this performance boost unfortunately. The model is basically memorizing words and tags which will not suffice. The precision and recall values of most of the classes were 0. The IOB Tagging system contains tags of the form B CHUNK_TYPE for the word in the Beginning chunk I CHUNK_TYPE for words Inside the chunk O Outside any chunkThe IOB tags are further classified into the following classes geo Geographical Entity org Organization per Person gpe Geopolitical Entity tim Time indicator art Artifact eve Event nat Natural Phenomenon Data sourceThe dataset an extract from GMB corpus which is tagged annotated and built specifically to train the classifier to predict named entities such as name location etc. The corpus is created by using already existed annotators and then corrected by humans where needed. Using a larger corpus from the GMB dataset. Current implementation considers only 2 hyper parameters in CV search however the CRF model offers more parameters which can be further tuned to improve on the performance. Currently it uses Gradient Descent but Stochastic Gradient Descent can be used to create a faster model. Since NER usually deals with a huge corpus neural networks are very efficient in identifying patterns in the data provide a better model. It seems most of the sentences are 20 30 words long and the distribution is normal. ELI5 report has built in support for several ML frameworks and provides a way to explain black box models. It seems the features which require the model to take proper decisions are missing. Precision Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. Now we have a good model fit using the CRF classifier. Although we have a good average score the model performed quite badly. It is possible to improve the model performance from a current 95 F1 score to about 98 using the above techniques. This will help us understand the extract of the dataset. Therefore this score takes both false positives and false negatives into account. CRFsuite CRF models use two kinds of features state features and transition features. Interpreting the results Let s try to understand the report in brief. This can be broken down into two sub tasks identifying the boundaries of the NE and identifying its type. However the precision and recall metrics of the classes individually have not improved. We will make the first row as the heading remove the first row and re index the dataset We have 66161 samples and 5 features. pdf NLTK book chapter 7 http www. Using different algorithms in the CRF model. We can see that there is a scope of improving the model. Quite surprising most of the words are tagged as outside of any chunk. MONEY 175 million Canadian Dollars GBP 10. Dependencies used are below Obtain the data Scrubbing Cleaning the data Seems the dataset does not contain any missing values and we are good to proceed to the exploration step where we will try to understand the data more. Therefore we will train on one subset and test on the other and repeat for every subset so that the classifier classifies correctly on average and the performance estimate is not overly optimistic. Now since we have good model we can take sneak peek into the model and see how it is classifying and what all weights are assigned. remove the O tag from the list. False Positives FP When actual class is no and predicted class is yes. The model is trying to understand the context as well and not just remember words. In order to use CRF we will enhance the feature set and create more features which can be used by the model to predict the tags correctly. We can also see that B ORG I ORG has positive weights which makes sense as the First names are always followed by the inner names the same applies for organisations. Named entity recognition is a task that is well suited to the type of classifier based approach. Scrubbing Cleaning the data Initial data exploration and preparation for analysis Exploring Visualizing our data Basic EDA to understand the data Modeling the data Create classification models for NER Simple tree based model RandomForest State of the art CRF model Hyperparameter tuning Interpreting the results Analysing the performance and further steps to improve Environment set up and loading dependenciesAnaconda is used to do the analysis which is an easy to install free enterprise ready Python distribution for data analytics processing and scientific computing. com abhinavwalia95 entity annotated corpus home Understanding Model performance https blog. The average score has dropped but the individual precision and recall scores have improved. pdf POS tagging and NER https hpi. com all accuracy precision recall f1 score interpretation of performance measures Data analysis Data visualisation Let us take a sneak peak into the dataset first The dataset does not have any header currently. The model is not perfect and it can be further improved by various ways Adding more features to the model e. These words can be considered as fillers and their presence might impact the classifier performance as well. It is important that the classifier has proper features fed in to improve the performance. It is essential that the model is evaluated by these metrics per class to make sure we have a good model. The dataset has the following columns or features Index Index numbers for each word Numeric type Sentence The number of sentences in the dataset We will find the number of sentences below Numeric type Word The words in the sentence Character type POS Parts Of Speech tags these are tags given to the type of words as per the Penn TreeBank Tagset Categorical type Tag The tags given to each word based on the IOB tagging system described above Target variable Categorical type Since the dataset is annotated with POS and Tags we will build a simple class to combine the words into a sentence. True Negatives TN These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. It is not a gold standard corpus meaning that it s not completely human annotated and it s not considered 100 correct. Hyperparameter tuning using Randomized CV Search Now we have a good model with decent precision and recall scores for each class. We can fit this model and use it to predict tags given a sentence. Now that we know the words and sentences lets try to understand what sort of words each tag contains. This is important in order to understand how the classifiers will perform and help us interpret the results. Modeling the dataWith the basic EDA done and understanding the dataset we can move to the modeling stage. we will divide the dataset into 5 subsets and train test on them. Now we will create the Randomized CV search model wherein we will use a modified F1 scorer model considering only the relevant labels define fixed parameters and parameters to search use the same metric for evaluation search Lets check the best estimated parameters and CV score We sort the tags a bit so that they appear in an orderly fashion in the classification report Now we create the model again using the best estimators. Some models like decision trees and neural networks are often be able to get 100 accuracy on the training data but perform much worse on new data. It is a popular algorithm for parameter estimation in machine learning. We will understand them in detail in the exploration step. Named entities are definite noun phrases that refer to specific types of individuals such as organizations persons dates and so on. combination of words with gives a proper meaning e. decision rules that correspond to the translation rules that we intend to capture. the linear chain CRF which is popular in natural language processing predicts sequences of labels for sequences of input samples. The context information behind each word needs to be fed to the model as well so that the predictions are more accurate. Random Forest being one of the most popular tree based models can learn the underlying rules according to which terms are tagged. 40 PERCENT twenty pct 18. It is the harmonic mean of the both Precision and Recall begin equation F1 Score 2 frac Recall Precision Recall Precision end equation For a decent classifier we would prefer high precision and recall values. Also we add new features such as upper lower digit title etc. html ELI5 report documentation https media. Compared to the Random Forest classifier the CRF classifier did better as the scores have improved. We chose Random Forest because of the following reasons Firstly RF is able to automatically construct correlation paths from the feature space i. It is a supervised learning method which has been proven to be better than the tree based models when it comes to NER. html Annotated GMB corpus dataset https www. We can expect that O I ENTITY transitions to have large negative weights because they are impossible. GMB is a fairly large corpus with a lot of annotations. Recall TP TP FN F1 score F1 Score is the weighted average of Precision and Recall. We can use the first row as a header as it has the relevant headings. de fileadmin user_upload fachgebiete plattner teaching NaturalLanguageProcessing NLP2015 NLP04_POS_NER. Performance metricsBefore we move to the modeling part it is important to understand the performance metrics on the basis of which the models will be evaluated. We divide the dataset into train and test sets Lets see how the input array looks like Random Forest classifier Lets check the performance Feature set Creating the train and test set Creating the CRF model We predcit using the same 5 fold cross validation Lets evaluate the mode Tuning the parameters manually setting c1 10 First we select all the tags that are relevant for us i. Simple tree based models have been proven to provide decent performance in building NERC systems. In short we try to provide a sequence of features to the model for each word the sequence containing POS tags capitalisations type of word title etc. Lets check for any missing values in the dataset A class to retrieve the sentences from the dataset This is how a sentence will look like. Since we need to take into account the context as well we create features which will provide consecutive POS tags for each word. WHO PERSON Eddy Bonte President Obama LOCATION Murray River Mount Everest DATE June 2008 06 29 TIME two fifty a m 1 30 p. Conditional Random Fields classifierA Conditional Random Field CRF is a standard model for predicting the most likely sequence of labels that correspond to a sequence of inputs. We will try tuning the model manually to see if we can improve it. It also learned that some transitions are unlikely e. President Obama Park Street etc. Now we shall start with the modeling part where we create new features create the model and evaluate it on the metrics stated above. False Negatives FN When actual class is yes but predicted class in no. So our dataset mostly contains words related to geographical locations geopolitical entities and person names. Since the problem statement is a simple classification problem we will start with a simple tree based model Random Forest using a simple feature map. Maybe we can do it computationally and get a better model. 75 FACILITY Washington Monument Stonehenge GPE South East Asia Midlothian The goal of a named entity recognition NER system is to identify all textual mentions of the named entities. Transition features make sense at least model learned that I ENITITY must follow B ENTITY. ", "id": "shoumikgoswami/ner-using-random-forest-and-crf", "size": "14934", "language": "python", "html_url": "https://www.kaggle.com/code/shoumikgoswami/ner-using-random-forest-and-crf", "git_url": "https://www.kaggle.com/code/shoumikgoswami/ner-using-random-forest-and-crf", "script": "classification_report cross_val_predict sent2features cross_val_score numpy seaborn flat_classification_report word2features matplotlib.pyplot sent2labels sklearn.grid_search scorers metrics sklearn_crfsuite.metrics pandas feature_map RandomForestClassifier getsentence(object) sklearn.cross_validation sklearn_crfsuite RandomizedSearchCV __init__ sklearn.metrics make_scorer CRF sklearn.ensemble ", "entities": "(('CRF Documentation https', 'sklearn crfsuite'), 'sklearn') (('value', 'predicted class'), 'TN') (('we', 'word title etc'), 'try') (('It', 'boost'), 'require') (('features', 'all what'), 'try') (('where we', 'metrics'), 'start') (('We', '66161 samples'), 'make') (('which', 'data analytics processing'), 'scrub') (('Precision Recall F1 metrics', 'True False negatives'), 'use') (('Now we', 'again best estimators'), 'create') (('precision values', 'classes'), 'be') (('tag type', 'what'), 'help') (('noun definite that', 'organizations persons such dates'), 'be') (('Gradient Descent', 'optimization function'), 'use') (('classifier', 'performance'), 'be') (('which', 'proper decisions'), 'seem') (('when it', 'NER'), 'be') (('We', 'different model'), 'work') (('We', 'sentence'), 'fit') (('I', 'B ENTITY'), 'make') (('us', 'results'), 'be') (('us', 'dataset'), 'help') (('manually we', 'it'), 'try') (('CRFsuite CRF models', 'features state features'), 'use') (('Firstly RF', 'feature space i.'), 'choose') (('Now we', 'recall class'), 'tuning') (('WHO', 'Eddy Bonte President Obama LOCATION Murray River Mount Everest DATE'), 'person') (('better scores', 'Random Forest classifier'), 'do') (('it', 'computer memory'), 'use') (('ELI5 report', 'box black models'), 'build') (('corpus', 'then humans'), 'create') (('Random Forest We', 'classifier i.'), 'classifier') (('Quite surprising most', 'as outside chunk'), 'tag') (('We', 'model'), 'see') (('Therefore score', 'false account'), 'take') (('terms', 'which'), 'learn') (('Also we', 'digit title such upper lower etc'), 'add') (('s', 'brief'), 'let') (('which', 'basically words'), 'memorize') (('as well predictions', 'model'), 'need') (('we', 'good model'), 'be') (('that', 'defined categories'), 'be') (('value', 'predicted class'), 'tp') (('However precision metrics', 'classes'), 'improve') (('which', 'word'), 'create') (('It', 'above techniques'), 'be') (('Simple tree based models', 'NERC systems'), 'prove') (('which', 'GMB corpus'), 'pipeline') (('I', 'classifier'), 'find') (('model', 'good average score'), 'have') (('GMB', 'annotations'), 'be') (('weights', 'model'), 'take') (('we', 'translation rules'), 'rule') (('Recall TP TP FN F1 score F1 Score', 'weighted Precision'), 'be') (('sentence', 'how'), 'check') (('we', 'modeling stage'), 'model') (('Named', 'data set'), 'apply') (('model', 'as well just words'), 'try') (('which', 'name such location'), 'contain') (('Stochastic Gradient Descent', 'faster model'), 'use') (('models', 'which'), 'move') (('Now we', 'CRF classifier'), 'have') (('It', 'machine learning'), 'be') (('html ELI5', 'documentation https media'), 'report') (('we', 'train them'), 'divide') (('that', 'classifier based approach'), 'be') (('20 30 words distribution', 'sentences'), 'seem') (('which', 'input samples'), 'predict') (('we', 'sentence'), 'have') (('they', 'large negative weights'), 'expect') (('com abhinavwalia95', 'corpus home Understanding Model performance https entity blog'), 'annotate') (('that', 'inputs'), 'be') (('Classification reports', 'class'), 'use') (('it', 'relevant headings'), 'use') (('that', 'i.'), 'divide') (('models', 'new data'), 'be') (('therefore they', 'NER superior classifier'), 'use') (('first dataset', 'header'), 'com') (('classifier classifies', 'subset'), 'train') (('we', 'high precision values'), 'be') (('fillers', 'classifier performance'), 'consider') (('Maybe model', 'context information'), 'remember') (('it', 'model e.'), 'be') (('We', 'exploration step'), 'understand') (('NER', 'better model'), 'be') (('Precision TP TP FP Recall Sensitivity Recall', 'actual class'), 'be') (('which', 'tags'), 'enhance') (('classification simple we', 'Random feature simple map'), 'be') (('So dataset', 'geographical locations geopolitical entities'), 'contain') (('which', 'further performance'), 'consider') (('it', 'organization right name'), 'be') (('FACILITY Washington Monument Stonehenge GPE South East Asia 75 goal', 'named entities'), 'Midlothian') (('where chunks', 'appropriate type'), 'build') (('where we', 'data'), 'be') (('First names', 'same organisations'), 'see') (('Lets', 'O again tags'), 'check') (('This', 'type'), 'break') (('CRF', 'account e.'), 'predict') (('tag', 'words'), 'now') (('Negatives False When actual class', 'no'), 'fn') (('Secondly RF', 'most accurate classifier'), 'consider') (('Precision Precision', 'positive observations'), 'be') (('Maybe we', 'computationally better model'), 'do') "}