{"name": "tps 08 21 xgboost ", "full_name": " h2 Data import h1 EDA h1 Data preprocessing h1 Hyperparameters optimization h1 Model training h2 Feature importances h2 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "Data preprocessing Hyperparameters optimization The code below is commented in order to save runtime. Model training Feature importances Submission linear algebra data processing CSV file I O e. As you can see f1 feature has the smallest amount of unique values 289. best_value Hyperparameters optimized by Optuna fold_rmse np. split X target_bins X_train X_valid X. Data import EDA There are no missing value in the both datasets. reshape 1 1 print f Trees model. loc valid_idx y_train y_valid y. loc valid_idx time_limit 3600 4 study optuna. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session nrows 10000 train date_time pd. array oof_preds valid_idx. optimize lambda trial train_model_optuna trial X_train X_valid y_train y_valid timeout time_limit print Number of finished trials len study. There are some features with relatively low correlation with target value even comparing with other features Let s visualize each feature vs loss. As you can see the correlation is between 0. So I don t think any feature should be treated as categorical. Let s look at feature correlation. 03 which is pretty small. params print Best score study. reshape 1 1 y_scaler. 2 random_state 42 for train_idx valid_idx in split. suggest_int min_child_weight 10 30 Model loading and training time split StratifiedShuffleSplit n_splits 1 test_size 0. The datasets are pretty well balanced. So the features are weakly correlated. read_csv Pandas setting to display more dataset rows and columns Input data files are available in the read only. Let s check feature values distribution in the both datasets. sqrt mean_squared_error y_scaler. create_study direction minimize study. Let s check target distribution. to_datetime test date_time format Y m d H M S Colors to be used for plots Plot dataframe Mask to hide upper right part of plot as it is a duplicate Making a plot Calculating edges of target bins to be used for stratified split Scaling data A set of hyperparameters to optimize by optuna min_child_weight trial. to_datetime train date_time format Y m d H M S test date_time pd. trials print Best trial parameters study. ", "id": "maximkazantsev/tps-08-21-xgboost", "size": "824", "language": "python", "html_url": "https://www.kaggle.com/code/maximkazantsev/tps-08-21-xgboost", "git_url": "https://www.kaggle.com/code/maximkazantsev/tps-08-21-xgboost", "script": "seaborn mean_squared_error StratifiedKFold KFold StratifiedShuffleSplit matplotlib.pyplot sklearn.preprocessing MinMaxScaler QuantileTransformer train_model_optuna sklearn.model_selection pandas sklearn.metrics StandardScaler xgboost XGBRegressor numpy ", "entities": "(('f1 feature', 'unique values'), 'have') (('s', 'loss'), 'be') (('Model min_child_weight 10 30 loading time', 'n_splits'), 'split') (('it', 'optuna min_child_weight trial'), 'format') (('best_value Hyperparameters', 'Optuna fold_rmse np'), 'optimize') (('code', 'runtime'), 'datum') (('s', 'datasets'), 'let') (('correlation', '0'), 'be') (('Submission', 'linear algebra data CSV file'), 'importance') (('read_csv Pandas', 'columns Input data read'), 'be') (('t', 'train date_time 10000 pd'), 'list') "}