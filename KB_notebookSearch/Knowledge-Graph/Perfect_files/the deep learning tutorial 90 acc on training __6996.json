{"name": "the deep learning tutorial 90 acc on training ", "full_name": " h1 Table of Contents h1 Article on medium publication h1 What would be the workflow h1 Problem Identification h1 What data do we have h1 Exploratory data analysis h1 Additional analysis h1 Data preparation including feature engineering h1 Creating a Model h1 Model evaluation h1 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "csv index_col PassengerId submission Survived y_final. drop labels Survived Sex_male Fsize LargeF SibSp_8 Parch_9 T_WEP Cabin_T Emb_Q axis 1 Creating the model Inputing the first layer with input dimensions Adding an Dropout layer to prevent from overfitting adding second hidden layer Adding another Dropout layer adding the output layer that is binary 0 1 Visualizing the model Creating an Stochastic Gradient Descent Compiling our model Fitting the ANN to the Training set y_preds model. It s all about experimenting and learning. Note that Parch 3 and Parch 1 shows higher survival probabilities. Additional analysis Let s create few additional charts to see how different variables are related. Exploratory data analysis One important aspect of machine learning is to ensure that the variables show almost the same trend across train test data. astype int submission. We will use it as it is. jpg So it is a classification problem and you are expected to predict Survived as 1 and Died as 0. com content uploads 2016 04 blog twenty one business icebergs sink business 280416. com rf image_large Pub p9 AJC 2018 07 12 Images newsEngin. Name Can be used to create new variable Title by extracting the salutation from name. Cabin Alphanumeric variable. Data preparation including feature engineering 5. While Title Age feature represents the Age category of passengers the features like Fare PClass Cabin etc. It shows that a good number of babies young kids survived. drop labels source Sex_male Fsize LargeF SibSp_8 Parch_9 T_WEP Cabin_T Emb_Q axis 1 inplace True You may want to drop some variables to avoid dummy variable trap X_train train. Approximately 62 of Pclass 1 passenger survived followed by 47 of Pclass2. Fare Check the number of missing value Only 1 value is missing so we will fill the same with median Use the numpy fuction log1p which applies log 1 x to all elements of the column Check the new distribution Cabin Replace the missing Cabin number by the type of cabin unknown U Let s plot the survival probability by Cabin Create dummy variables Embarked Find the number of missing values Fill Embarked missing values of dataset set with mode S Create dummy variables Create dummies for PClass Now You may want to drop some variables to avoid dummy variable trap test. Fare Let s check the distribution first. astype int submission pd. This might be a representation of class compartment. 99 on training dataset. See the mix of variable types. We need to transform this variable using log function and make it more normally distributed. What data do we have Let s import necessary libraries bring in the datasets in Python environment first. Name Not relevant from analysis modeling perspective. Write down things like what are you expected to do what data you might need or let s say what all algorithms you plan to use. input gender_submission. jpg Based on data above female passengers had better chances of survival than male passengers Age The insight below connects back to Ladies and Kids First scene of the movie. Embarked represents port of embarkation. Step By Step Tutorial For Beginners http www. Based on findings we can conclude that Age Gender features representing social economic status were primary factors affecting the survival of passenger. If you like this notebook or find this notebook helpful Please upvote and or leave a comment Article on medium publication I also wrote an article on medium on the same topic. Understand the problem first and draft a rough strategy on a piece of paper to start with. Once we have the datasets in Python environment we can slice dice the data to understand what we have and what is missing. The purpose of this kernel is to show how a simple NN model can be constructed. Sex Create dummy variables Age Missing value treatment followed by creating dummy variables SibSP Create dummy variables Parch Create dummy variables Ticket Create dummy variables post feature engineering Fare Missing value treatment followed by log normalization Cabin Create dummy variables post feature engineering Embarked Create dummy variables PassengerID Pclass Name Title Age Sex Creating Family Size variable using SibSp Parch SibSp Parch Ticket Fare Embarked Creating a Model Model evaluation Conclusion Title Sex_Female Fare PClass seems to be common features preferred for classification. com rp1611 step by step tutorial for beginners All data visualization remains the same however instead of emsembling method I have tried simple neural network in this Kernal. to_csv NNPrediction1. predict test y_final y_preds 0. The Fare variable is right skewed. This will help you to stay on track. As we do variable analysis try to replicate wherever applicable the code for test data and see if there is any major difference in data distribution. Let s look at the distribution. Parch Parch indicates number of parents children aboard the Titanic. Model evaluation 7. We will do this during feature engineering process. We can create more features using this Cabin variable. Variable source is a kind of tag which indicates data source in combined data Let s check the data PassengerID Drop PassengerID Pclass Use as it is Name Extract Salutation from Name variable Convert other salutations to fixed Title Drop Name variable Age Index of NaN age rows Sex Create dummy variables combdata Sex combdata Sex. Before you jump to How to do this part like typical Data Scientists understand What Why part. The kernel is purely for learning purpose so I would keep the kernel simple and leave it as it is post training the model. Data preparation including feature engineering What we need to do to process following variables PassengerID No action required PClass Have only 3 numerical values. Embarked C Cherbourg Q Queenstown S SouthamptonLet s explore the variable with Survival rate. I will give you one example here. Table of ContentsThis is my 2nd Kernel for this competition. We will drop this feature later after creating a new variable as Title. 22048809_071418 titanic_Titanic Image 7 2. Link to my previous kernel is pasted below. Now the Titanic challenge hosted by Kaggle is a competition in which the goal is to predict the survival or the death of a given passenger based on a set of variables describing age sex or passenger s class on the boat. So here is the workflow. Exploratory data analysis 4. Problem Identification 2. SibSP 1 and SibSP 2 shows higher chances of survival. As the analysis output below suggests Emabrked C shows high probabilities of survival. The model provides accuracy of 87. What data do we have 3. map male 0 female 1 or Create a variable representing family size from SibSp and Parch Create new feature of family size Analyze the Survival Probability by Fsize Drop FSize variable SibSp Create dummy variables Parch Create dummy variables Ticket Extracting the ticket prefix. Ticket This variable has alphanumeric value which might not be related to Survival directly but we can use this variable to create some additional features. Use the groupby univariate bivariate analysis method to compare the distribution across Train Test data PassengerId Not relevant from modeling perspective so we will drop this variable later Pclass Pclass is categorical variable. com rp1611 model ensembles for survival prediction a3ecc9f7c2ae and access the blog. represents the economic status. Conclusions That s all you need to solve a data science problem. Frankly speaking no one cares. Sex Based on analysis below female had better chances of survival. If you compare the output you will see that missing value percentages do not vary much across train test datasets. 687 missing values in train 327 missing values in test data which needs to be treated. Developing a model 6. Check missing values in train data set Check missing values in train data set Fill empty and NaNs values with NaN Analyze the count of survivors by Pclass Analyze the Survival Probability by Pclass Count the number of passengers by gender Analyze survival count by gender Analyze the Survival Probability by Gender Let s explore the distribution of age by response variable Survived We can also say that the older the passenger the lesser the chance of survival Analyze the count of survivors by SibSP Analyze probability of survival by SibSP Analyze the count of survivors by Parch Analyze the Survival Probability by Parch for some statistics Get the fitted parameters used by the function Now plot the distribution Let s check the unique values Analyze the count of survivors by Embarked variable Analyze the Survival Probability by Embarked Age Pclass Survival Age Embarked Sex Pclass Relation among Pclass Gender Survival Rate Relation among SibSP Gender Survival Rate Relation among Parch Gender Survival Rate Let s combining train test for quick feature engineering. The best way to learn is by practicing so please feel free to tweak the parameters and use the framework to further improve. SibSP This variable refers to number of siblings spouse onboard. You can click this clink https medium. If there is no prefix replace with U Unknown. Problem Identification Best Practice The most important part of any project is correct problem identification. What would be the workflow I will keep it simple crisp rather than using buzz words useless data science frameworks. We have 417 observations 11 columns no response Survived column. If you find this notebook helpful Please upvote and or leave a comment Import the basic python libraries Read the datasets We have 891 observations 12 columns. Example Let s start with finding the number of missing values. If not it would lead to overfitting because model is representing a relationship which is not applicable in the test dataset. ", "id": "rp1611/the-deep-learning-tutorial-90-acc-on-training", "size": "6996", "language": "python", "html_url": "https://www.kaggle.com/code/rp1611/the-deep-learning-tutorial-90-acc-on-training", "git_url": "https://www.kaggle.com/code/rp1611/the-deep-learning-tutorial-90-acc-on-training", "script": "keras.layers Activation keras.models stats skew #for some statistics numpy seaborn SGD Dropout Dense norm scipy keras.optimizers tensorflow matplotlib.pyplot Sequential pandas scipy.stats StandardScaler sklearn.preprocessing ", "entities": "(('Data typical Scientists', 'What'), 'jump') (('classification you', '0'), 'jpg') (('This', 'class compartment'), 'be') (('what', 'what'), 'slice') (('Cabin variables post feature engineering variables Pclass Name Title Age Sex Creating Family Size Create dummy Embarked Create dummy variable', 'common classification'), 'create') (('Now You', 'trap dummy variable test'), 'check') (('s', 'missing values'), 'let') (('directly we', 'additional features'), 'have') (('Problem Identification Best most important part', 'project'), 'practice') (('variable analysis', 'data major distribution'), 'try') (('young kids', 'babies'), 'show') (('Name', 'name'), 'use') (('that', 'Training set'), 'label') (('which', 'test 327 missing data'), 'value') (('SibSP variable', 'siblings'), 'refer') (('We', 'Cabin variable'), 'create') (('you', 'algorithms'), 'write') (('Drop Pclass it', 'Sex Create variables combdata Sex dummy combdata'), 'be') (('best way', 'framework'), 'be') (('Step', 'www'), 'http') (('it', 'log function'), 'need') (('goal', 'boat'), 'be') (('Table', '2nd competition'), 'be') (('You', 'X_train train'), 'Fsize') (('Approximately 62', 'Pclass2'), 'survive') (('Parch Parch', 'Titanic'), 'indicate') (('Parch', 'survival 3 1 higher probabilities'), 'note') (('jpg', 'Kids First movie'), 'have') (('variables', 'train test data'), 'analysis') (('SibSP', 'survival'), 'show') (('s', 'Python environment'), 'Let') (('I', 'same topic'), 'write') (('We', '891 observations'), 'find') (('Embarked C Cherbourg Q Queenstown SouthamptonLet s', 'Survival rate'), 'S') (('We', 'Survived column'), 'have') (('s', 'feature quick engineering'), 'check') (('Title Age feature', 'Fare PClass Cabin etc'), 'represent') (('I', 'Kernal'), 'step') (('We', 'Title'), 'drop') (('Link', 'previous kernel'), 'paste') (('which', 'test dataset'), 'lead') (('We', 'feature engineering process'), 'do') (('PClass', 'only 3 numerical values'), 'preparation') (('you', 'data science problem'), 'conclusion') (('how different variables', 'few additional charts'), 'let') (('value missing percentages', 'train test much datasets'), 'compare') (('Pclass later Pclass', 'variable'), 'use') (('I', 'buzz data science simple rather words useless frameworks'), 'be') (('csv index_col PassengerId submission', 'y_final'), 'Survived') (('NN how simple model', 'kernel'), 'be') (('it', 'model'), 'be') (('Emabrked below C', 'survival'), 'suggest') (('Age Gender', 'passenger'), 'conclude') (('Sex', 'survival'), 'have') "}