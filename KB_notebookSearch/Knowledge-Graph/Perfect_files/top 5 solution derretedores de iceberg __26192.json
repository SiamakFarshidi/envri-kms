{"name": "top 5 solution derretedores de iceberg ", "full_name": " h1 Titanic Competition on Kaggle h1 First Steps h1 Visualizing our data h1 Feature Engineering h3 Surname h3 Title h3 Age Missing h3 Child h3 Family Size h3 Alone h3 Numerical Ticket h3 Group features h4 Group ID h4 Group Type h4 Num Group Survivors and Deceased h3 Split Fare h3 Cabin Letter h3 More than one cabin h1 Exploratory Data Analysis h2 Categorial Values h2 CabinLetter versus Embarked h2 CabinLetter verus Pclass h2 Embarked versus Pclass h2 GroupType and GroupSize h2 MoreCabin verus Pclass h2 New Features h3 Small Group and Family h1 Find inconsistencies h2 Title and Sex h2 Embarked and Group h2 Age and Parch h1 Cleaning data h2 Embarked h2 Fare h2 Age h1 Pre Processing the Data h2 Sex h2 One hot encoding h2 Standartization and Normalization h1 Feature Selection and Basic Machine Learning Base Models h2 Models Performance on Full Feature Set h2 Feature Importances h2 Recursive Feature Elimination RFE h2 Testing Different Feature Sets h3 Settings h3 Testing each subset of similar features h3 Testing my final opinions h2 Getting a final set of features h1 Validations and Hypterparamether Tuning h1 Making the predicting the preparing for submition ", "stargazers_count": 0, "forks_count": 0, "description": "MoreCabin verus PclassHere we see that surprisingly there are about 7 people from class 3 with more than one cabin. Num Group Survivors and DeceasedNow we re going to calculate the number of survivors of a group. There are still some inconsistencies that I did know how to resolve Let s now look at the third inconsistencies IsAlone False but GroupType AloneWe can see that the third inconsistencies are the same as those I said I don t know what to do. So C is not a mistake. RFE uses some kind of metric to give each feature a grade and remove the one with the lowest grade at each step. And they also agree with the exception of the perceptron that Pclass_2 is not a good feature. But they agree that Pclass_1 is a bad feature. Let s check the best sets Here we see that indeed using all the classes or not doesn t affect much and it s better not using Age and Embarked. We can try don t RFE again without Mr We also see that Title_Miss doesn t have much impact And that Title_Master and Title_rare seem to be good features. Indeed the higher rates are in families and groups of size 2 to 4. The rare Title doesn thave a higher change of surviving which is very interesting I was excepting it to be high. This process also helps to get more ideias for the creation process of new features so called Feature Engineering. Feature Engineering Now let s create some new features that might be useful to us. Finally let s look at the last inconsistency IsAlone True but GroupType FamilyWe don t have any cases. While there is 2 embarked missing in the training set and one fare missing in the test set. Before doing that though I found that there s one group that have different fares even buying the same ticket. Another good variable to see is the Parch. I ve already took a closer look at the data and solved all the cases one by one. We want to fill the NaN values and keep the shape of the graph as similar as possible. But at the same time we see that they are very good on some models but very bad or others. Augusta came from S that s a mistake. Titanic Competition on KaggleThis is a Python notebook for the Titanic Competition on Kaggle https www. We see that it s market as the third best feature IsChild is also market as a bad feature probably because the Age is enough MoreCabins is another irrelevant feature AgeMissing and NumericalTicket are also not important we also saw this in the EDA section One very interesting thing is that Title_rare was market as a good feature that seems a bit strange Another very strange thing is the fact that Age is market as a bed feature Let s try to understand better checking only some feature at time. If we simply count the number of people we know survived both groups will have a value of zero Therefore it s important to have not just the number of survivors metric but also the number of deceased. Now talking about families and groups. But we can look at the SplitFare value. Indeed as we suspected the Family and Mixed types have much more Child and Women rates than the other types. To do so we re going to perform what s called Feature Selection selecting only the best features. First the graphs for GroupNumSurvived and GroupNumDeceased are very obvious The only feature that doesn t seen to have much impact is the NumericalTicket column. Titanic is our first competition and we based our work on some other notebooks available online. We ll use 0 for male 1 for female One hot encodingFinally we need to change the non binary categorical values into binary values using one hot encoding. Then group 1 will have a deceased number of 3 while group 2 will have a number of 0. As I tried many different version for the same kind of feature let s try to identify what option is the best for each feature. First there are some exceptions we found previously. The steps we need to do are change the Sex column from string to integer do One Hot Encoding in the categorial columns like Pclass and Embarked scale the numerical features to help the models using MinMax or Standartization. So let s treat that case before proceeding Now we can proceed. We also see that indeed Mr. I ll definely keep CabinLetter_N and we can test keeping letters B D E and maybe G. Both of them survived. You can follow the progress of each model I m commenting because Kaggle takes A LOT OF TIME to run this so I hardcoded the best estimators Selecting the feature set Selecting the estimator Here I ll show the head of the dataframe we re going to submit Total positive predictions. Better Classes try just class 3 Title try each title Better in some case worst in others Small Sizes FamilyAlmost the same EmbarkedWorst Grouptype Bad Ages try just Age CabinLetter try each letter Testing my final opinionsLet s know try some feature sets that I think are going to be good after doing all this analysis. Let s average the values How many people in more than one cabin are there Creating a figure Getting the total amount of each category Getting the survivors count for each category Getting the percentages Plottin the total and survived amounts Plotting the percentages as text on top of each bar Adding title and legend Here I just add some extra space for the percentages labels Showing each image at the end of the for loop As the groupsize is 1 we can fill with the same value Checking if every age was filled out Setting the random_state and n_jobs parameters of the model Creating a Stratified K Fold object to split the data for the Cross Validation Creting empty arrays for the metrics we ll compute Performing the Cross Validation Fitting the model to the training data Getting the prediction values Computating the scores Computing the feature importance if the model has it Now let s compute the aggregation functions Computing the feature importances mean Returning the aggergations and the feature importances Sorting the importances Some models can t be used with RFE so we need to skip them Getting the ranks for each model Getting the overall ranks Let s fix some features that we saw are very good in the majority of the situations For now let s fix these three features. Also I found that indeed a sister of her called Eliza Johnston was on board and also a friend Alice Harknett. Suppose we have two groups both with four people One of the groups have three people that we know died in the accident and the four we need to predict The other groups have four people that we need to predict. Let s correct the values for this family. With this analysis we see that the sex and class are very important to decide if a person will survive or not. Let s set them as variables Validations and Hypterparamether TuningNow that we ve selected the best feature set we could let s start tuning the hyperparameter for each model and try to extract their best before moving on for the ensambles like stacking and voting ensambles. This is probably because Age have all the information we need. Here is a list of them RM W Titanic vF1 https rpmarchildon. The two graphs must look as similar as possible. Embarked Let s take care of these few missing values in embarked and fare first. The test set is the one we need to predict the target variable. Here we see that the third class tend to pay around 7. Categorial ValuesSomethings were very excepted while others are very interesting to realize. Sex We need to transform the Sex column into a numerical column. Also we need to split again our data set into the training set and test set. For instance there could be a higher probabilty of not surving if we today don t know the age of that person. Commeting the best results GradientBossting seem to be the best model we have so far. Linear and other models were also good but tree models are defined getting the best results so we re going to just keep analysing them because we saw that if we find a good feature set for tree models the other models will also benefit from it. Let s start with the easy one CabinLetter_N is one of the best features and we already were expecting that because we saw in the EDA section that there was a huge difference between having a cabin assigned to a row or not CabinLetter_T is quoted as best than letters A and F but that s because some linear classifiers think this is a good but we know that we actually should have already dropped this feature because it s completely useless CabinLetter_A and CabinLetter_F as quoted as the worst features in our dataset. So this shows that indeed there s a correlation between these group sizes and surviving or not. So even if they are not listed as very good features we might want to keep some of them because the tree based models got the better results. Doing some research with the family we see that Mr. We can now check how they performed in the test set according to each metric their positions don t have much if we change the sorting metric. Another thing we see is the fact that AgeMissing and IsChild are not considered good by any model so we probably can remove them. That s a good feature because if we have 5 people in a group and four of them survived and the fifth one we don t know the odds are higher that this person will also survive. Therefore it s fair to create a new column SplitFare using the fare divided by the number of buyers. First let s print the age distribution to see how it is before the changes we ll do. We can try to drop them and see if the scores increase. It s strange to see that we have missing values for people that survived. Testing Different Feature SetsSo now let s try many different feature sets using the information we got from the previous tests. encyclopedia titanica. Finally we see that the type of group really has some influence. Age and ParchWe except that passengers who are kids shouldn t have a Parch greater than 2. Hays Thomas Thomas Elias Elias Elias Elias Giles Giles Wiklund Wiklund Andersson Andersson Andersson Andersson Andersson Kink Kink Kink Braund Braund Hansen s bouhgt three different tickets Hansen s bought three different tickets Hansen s bought three different tickets Jensen s bought four different tickets Jensen s bought four different tickets Jensen s bought four different tickets Jensen s bought four different tickets Kiernan Kiernan Jussila Jussila Larsson Larsson Hagland Hagland Francatelli knows Mr and Mrs Morgan Morgan Lamson Lamson Frolicher Frolicher Thomas Thomas Elias Elias Elias Elias Giles Giles Wiklund Wiklund Andersson Andersson Andersson Andersson Andersson Kink Kink Kink Braund Braund Hansen s bouhgt three different tickets Hansen s bought three different tickets Hansen s bought three different tickets Jensen s bought four different tickets Jensen s bought four different tickets Jensen s bought four different tickets Jensen s bought four different tickets Kiernan Kiernan Jussila Jussila Olsen Olsen Larsson Larsson Davies Davies Duran y More Duran y More Ilmakangas Ilmakangas Hagland Hagland Bowerman Francatelli and Morgan Francatelli and Morgan Hays and Potter Lamson s bought two different tickets Lamson s bought two different tickets Mock Andrews and Longley Frolicher Parrish Walton Thomas Elias Elias Elias Elias Giles Giles Wiklund s bought two different tickets Wiklund s bought two different tickets Andersson Andersson Andersson Andersson Kink Kink Braund Braund Hansen s bouhgt three different tickets Hansen s bought three different tickets Jensen Jensen Jensen Jensen Kiernan Kiernan Eustis Jussila Jussila Olsen Olsen Larsson Larsson Davies Duran y More Duran y More Ilmakangas Ilmakangas Hagland Hagland Thomas Elias Elias Elias Elias Andersson Andersson Braund Braund Hansen Hansen Jensen Jensen Jensen Jensen Kiernan Kiernan Jussila Jussila Larsson Larsson Davies Hagland Hagland Hays Hays Hays Everyone in the group was on a B cabin First we set the GroupId to be the index Now we iterate over the dictionaries that have the GroupId as keys Now we reset the index And iterate over the passenger_id Now let s show the modified dataframe entried. However this number could be misleading. The cell below has the list of the best estimators before traning. AgeWe also need to decide that to do with the NaN ages. We just need to select the estimator and the features set that we consider the best to submit make the predictions and save them into a. When comparing the family features we see that SibSp and SmallFamily got the better ranks but SmallFamily wasn t good on tree based models while FamilySize was so we might want to stay with FamilySize. Right now our stragety is creating the maximum amount of features we can. Them let s visualize them and get some of the main info about them. Perhaps the Pclass have some dependence with other features or the most important class is the third. Cabin LetterFirst I ll fill the NaN Cabins with the letter N. We see that the Age is considered a good feature for many of the Tree Based models but a really bad feature for all linear classifiers that makes the overall rank to be very bad but we don t have to really remove this feature. com masumrumi a statistical analysis ml workflow of titanic by Masum RumiWe hope you like First StepsLet s first of all import our libraries and read the data sets. Title and Sex Embarked and GroupHere we see that we have 3 groups GroupId 63 have a couple with same ticket but one came from S and the other from C. Thus I think it s better to just have a binary value instead of a NumCabins feature. We can visualy see the correlation between the data with stronger colors representing higher correlations. The distribuition of the age after filling the NaN values. But when the divide the families and groups by size we see that very large families and groups also have higher changes of dying. So we re left with just two options S or C. Feature ImportancesAnd let s examine the feature importances that each model gave to each feature and start checking which features we ll keep or not. In the IsAlone column we see that people traveling alone had a lower survival rate. Let s take a look at our original test set with the new column Prediction to understand the decisions of our model. The same thing happens in the IsChild column. She was the personal maid of Ella Holmes. Standartization and NormalizationNow every numberial feature has mean equal 0 and standart deviation equal to 1. A Statistical Analysis ML workflow of Titanic https www. We also see that it s not a normal distribution so let s take the median instead of the mean. Let s define the functions we re going to use to tune the hyperparameters. Let s check if that s the case. Recursive Feature Elimination RFE Let s now try a different way of feature selection. We saw they are indeed the best I ll leave CabinLetter_T out I ll leave Title_rare out Let s also divide our models into Linear and Tree based models because we saw that there are many differences between them Now let s create some functions to help us evaluate each set First let s make feature sets using the fixed set and each other set Let s also add the full set of features Returning the obj we can get all of its attributes Defining some parameters that will be the same for every model Defining the lists we ll use on the tuning function Logistic Regression Classifier Ridge Classifier Stochastic Gradient Descent Decision Tree Classifier ExtraTree Classifier highly randomized version of a Decision Tree Random Forest Classifier SVC Classifier K Nearest Neighbors Classifier Gradient Boosting Classifier Extra Trees Classifier XGBooster AdaBoost on a Decision Tree Classifier Here we choose the traning set we ll tune our models Try using verbose 2 if it s taking too much time. Models Performance on Full Feature SetNow let s apply these functions and get a first view of our models in action when applied to the full feature set. Let s go column by column Surname TitleThere are many titles that have very few examples so we re going to group them under the same category. That probably is a mistake we need to do some research to find where did they come from GroupId is a family with two tickets. Let s visualize the Age against many categorial variables. When comparing group features we see that GroupSize was better in the majority of the situation so we might want to keep it instead of SmallGroup. Here we see that Title_Mr is quoted as one of the best features by many algorithms with some exceptions from both linear and tree based models Ridge Perceptron LinearSVC LDA and ExtraTree but this feature seems to be one of the bests indeed many algorithm cite this as the best feature On the other hand Title_Mrs doesn t seem to be a good feature maybe because Mr already give the information we want. We see Age indeed an horrible feature that could the way the filled the values or something like that I really don t know Embarked is also not good Using or not all the classes doesn t metter much Same thing for the size Using only Mr Master and rare is better. There are a lot that we can create and think. Finally we need to redo the IsChild calculations. So we re going to plot several graphs in order to better understand the trend of the data. Then we ll be able to tell based on the features that we already have if a person is going or not to survive. That s the case of almost all tree based models. We see that there s no difference in Sex and Embarked. Or it could be just that the Mixed group tend to have about 3 members Let s try to compare different features to see have more insights. They would be cousins uncles or something like that We have a higher value for the family size because some families didn t buy the same ticket We have people that don t have the same surname as the parents or spouse Or it can be some other strange case. So we re ready to proceed. That s because Parch equal to 1 or 2 are mainly kids and greater than that will never be a child therefore it can be also used. Feature Selection and Basic Machine Learning Base ModelsSo finally we can start doing some Machine Learning. We already can see that many models are overfitting the data. I ll show you that this was the only group with different fares Here we can clearly see that all tickets cost about 26. The Pclass_1 feature is considered one of the worst features. Finally CabinLetter_B and CabinLetter_G are listed as median features. We can see that many of these features SibSp Parch FamilySize GroupSize and SmallGroup have a better rank on tree models. This can be useful because for instance if a whole family in the training set survived and there s another family member in the test set odds are this person will also survive. And let s define all the base models we re going to test Now let s define some functions that will be useful to evalute our models. In particular we can see that AdaBoost and ExtraTrees have mean accuracy of 99 in the training set but just about 80 in the test set. Issac Gerald embarked in C and that he is a sibling of Dr. Let s try to see the division of the prices by class for the city S. That s very interesting because we saw there s a high correlaction between the classes and the survival hate The Pclass_2 is also considered a very bed feature. B D and E have the higher survival rates. It seems like we have a kid with Parch 3. Using RFE we can visualize the ranks of each feature. After reading some notebooks of other people we came with these features for each original column Name Surname Title Age Is Child Is Age Missing SibSp and Parch Family Size Is Alone Ticket Group features Group Id Group Size Group Type Group Num Survivors Group Num Deceased Is Numerical Ticket Fare Split Fare Fare divided by Group Size Cabin Cabin Letter Multiple Cabins The columns Pclass and Embarked doesn t seen to offer much more than theirselves in their original format. As I was creating a huge section of the notebook I ll just fix them quickly without much explanation. Getting a final set of featuresSo after all this jorney to find a very good set of feature we have four sets of features that we can try to use in order to get the best score we can. FamilySize and GroupSize don t seem to help a lot so let s use Title Pclass Parch and SibSp and just use Sex to untie. Let s look at the whole group to we if we find anything We have a family of five member of which four are kids. New FeaturesMotiveted by the EAD process we can create new features that will reinforce the trends we saw in the data. Cleaning dataLet s now see what we ll do about the null data we have in the data sets. There are many interesting things we visualize here. We can see that it s more probable that they came from city C because the SplitValues are more centred around 40. Talking about the CabinLetter we can see that people with NaN in the cabin entry had a much lower change of surviving than the people that we knew the cabins specially cabins in the lobbies B D and E we ll visualize more about that in the future. The features I ll create are SmallGroup 2 up to 4 members SmallFamily 2 up to 4 members Small Group and Family Find inconsistenciesBefore cleaning the data let s try to find and adjust some inconsistencies in the data. Let s try to look at these entried We see that actually we should have just G or E as they cabin letters so we ll fix that. For the cabin letters we have many cases. com wp content uploads 2018 06 RM W Titanic vF1. Looking at the encyclopedia titanica https www. So it could be right but we also need to do some research GroupId is another family with the same ticket but different cities that also is probably a mistake. One more interesting thing is the Pclass. Now let s try to look just at third class. Now it s time to filter only the ones that will truly help our models to learn the discard those that won t. org we see that indeed Miss. CabinLetter versus Embarked CabinLetter verus Pclass Embarked versus PclassSome conclusions for this part are The only cabins with people from Q are C E and F Decks or lobbies A B and C just have people from the first class Pclass 1 More than 90 of people who embarked from Q are from third class Pclass 3 The other two cities have more passengers and their class distribution is more similar. The types of inconsistencies we can find or at least the ones I tought are GroupSize FamilySize where GroupType Family FamilySize 1 but GroupType Family IsAlone False but GroupType Alone IsAlone True but GroupType FamilyLet s begin with the first one By looking at the data we see that a few thing might be the case for these rows We ve got a lower value for the family size because some people might be from the same family but not exactly were counted by the columns SibSp and Parch. And families of size 4 have amost an equal amount of survivors and deceased. Group features Let s group the tickets and measure some metrics upon these groups. This could explain the trends we saw. Margaret is their mother. Age Missing There are many NaN values in the Age column. Here we see that even though Pclass_3 has an overall rank of 4 it not all models agree that this is a good feature. The train df has two null values in the Embarked column and the test df has a null value in the fare column it might be worth to take a look later. Based on the surname we can classify each ticket as a person that is traveling alone a family a group of friends or a mixed between family and friends. Visualizing our dataJust printing unique values or try to use pandas commands like value_counts or even pivot_table is not enjoy to really get insightful information about the data. FareNow let s look at the NaN Fare value in the test data. Visualizing the predicitons versus the trend in the traning set. Amelia Bissette embarked in C with the other members of her gropus. It got almost 85 in many sets XGB also is very good getting the best score so far 84. Master got very good ranks on linear models but very bad on tree models. Let s now look at the second inconsistency FamilySize 1 but GroupType FamilyLooking at the data we see two main cases People that are from the same family but broght two different tickets most of them are consecutive Groups that are Family but were considered Non Family because many people have a different single name. Settings Testing each subset of similar featuresHere we can see that in many cases the base score is actually better than adding more features. General Data Science Modules Plotting Modules Basical Modules Machine Learning Classifiers Feature and Model Selection For Kaggle uncomment Here I ll be saving the test set for the submition Here I add SibSp and Parch to the categorical_values because there re only few values Thus I can group them to get some insight Creating a figure Getting the total amount of each category Getting the survivors count for each category Getting the percentages Plottin the total and survived amounts Plotting the percentages as text on top of each bar Adding title and legend Here I just add some extra space for the percentages labels Showing each image at the end of the for loop Creating the figure Plotting the first axis total vs survivors by age Plotting the second axis male vs female by age Adding labels titles and legends removing an auto added ylabel by pandas Showing the figure Carrau Minahan Risien Newell Watt Ware Ware Renouf Jefferys and Denbury Hirvonen Lindqvist Christy Jacobsohn Hocking Hocking Richards Gustafsson Gustafsson Backstrom Vander Vander Klasen Klasen Bourke Bourke Strom Persson Hays Davidson Frauenthal Frauenthal Crosby Crosby Ware Ware Christy Jacobsohn Hocking Hocking Richards Gustafsson Gustafsson Backstrom Vander Vander Klasen Klasen Bourke Bourke Strom Persson Frauenthal Frauenthal Crosby Crosby Ware s bought two different tickets Lindqvist is accompained by the Hirvonen s Hocking and Richards are the same family Hocking and Richards are the same family Hocking and Richards are the same family Backstrom and Gustafsson are the same family Backstrom and Gustafsson are the same family Backstrom and Gustafsson are the same family Klasen s bought two different tickets Klasen s bought two different tickets Bourke s bought two different tickets Strom and Persson are the same family Strom and Persson are the same family Frauenthal s bought two different tickets Frauenthal s bought two different tickets Crosby s bought two different tickets Crosby s bought two different tickets Carrau Hocking Hocking Richard Watt Ware Ware Risien One of the Frauenthal s is in cabin D40 so the others might be at least under the D lobby Denbury is traveling with Jefferys Perreault is traveling with Hays Payne is traveling with Hays First we set the GroupId to be the index Now we iterate over the dictionaries that have the GroupId as keys Now we reset the index And iterate over the passenger_id Now let s show the modified dataframe entried. Now let s define the models and hyperparameters we re going to search over. This process is very boring so I ll try to be lean. 5 but larger groups have higher fare because this value is the orginal value multiplied by the number of people that bought the same ticket. More than one cabinThere are just very few examples of people that reserved more than one cabins. We saw that using the ticket we can find many people that were traveling together. Here we see again that some features are very good on tree models while not that good on linear models. We can try to find any pattern that indicate which city these two ladies came from. So we can count the number of survivors of the groups. That could be because there are many classes on those desks and because they have higher chances of surviving. Going back to the EAD section we see that these cabins don t have many examples they have about 20 instances so that could be the reason why they are not good CabinLetter_C on the other hand has about 100 entries but it s also listed as a bad feature. The Title have many differences specially in the Master and rare categories. com company beedata usp mycompany student group of the University of S\u00e3o Paulo Brazil. Child Family Size Alone Numerical TicketA thing we can see in the ticket column is the fact that some tickets just have numbers while others have numbers and letters. Ware s Lindqvist and Hirvonen Hocking s and Richards Backstrom s and Gustafsson s Klasen s Bourke s Strom s and Persson s Frauenthal s Crosby s Francatelli s and Morgan s Lamson s Frolicher s Thomas s Elias s Giles Wiklund Andersson s Kink s Braund s Hansen s Jensen s Kiernan s Jussila s Olsen s Larsson s Davies Duran y More s Ilmakangas Hagland s Here we see the changes in the GroupId on the Richards Hocking s Backstrom s and Gustafsson s We group by the id because we saw many people knew each other but didn t buy the same ticket. Deck F doesn t have any passengers from class 1. That helps almost every non tree based model. A similar thing happens with rare and rare doesn t have much examples so it might be a good idea to drop that feature. Is was produced by a team from the BeeData https www. Where we see that they are two women that were in the cabin B28 which costed 40 and is the same ticket. We see that the higher the class the higher the median age. In the feature I ll be look for the best way to find these NaN values but I don t want to lose the NaNs beucase they can be meaningful. It could also be the case that families will tend to have more child and women than people traveling alone. Split FareWhen a ticket was bought by more than one person the original fare is multipled by the number of people that bought the ticket. Here we can have a great view of how each model perform without any parameter tuning. Here we got amazing scores for the tree models. Group ID Group Type InconsistenciesWe still need to fix some group types and look closer the data in some cases. Later we ll try to select only those that seen be lead to the best results. 96 Let s try to check what of these sets are the best by summing the columns Fortunately almost all sets are better than the base set. Looking at GroupType features we see that none of them were very good nether on linear nor on tree models. Pre Processing the DataThe last step before finally going into the Machine Learning models is pre processing the data. Sex with the exception of Decision Tree is considered a very good feature for all models. Here we see that indeed their tickets are sequential. One interesting thing is that indeed the AgeMissing column seens to have some correlation with the rates of surviving but that sfar from what I personally excepted I was excepting something like we visualized in the CabinLetter. The same thing happens with the SibSp adults will not have more than one Spouse and kids tend to have a few siblings. So let s group those exceptions together Now let s count the number of survivors and deceased. We created lots of potencial features and saw that many of them have a high correlation with your target variable Survived while some others don t seem to help. In the Title we see the same trend for girls and childs. Doolina are siblings and Mrs. So indeed it will be a feature that we want to keep. Exploratory Data AnalysisNow that we ve created lots of features we can plot more graphs to try to visualize correlations between these new features and the target features Survived. It was one of the families we put in the same group. So there are about 20 of missing ages and about 78 of missing cabins. I think it could be because everyone on that desk is from the first class but I don t know CabinLetter_D and CabinLetter_E are listed as good features. If you want to run every cell and don t the searches make sure to comment the cell above and just use the cell below Making the predicting the preparing for submitionIt was being a lot jorney since here but we re finally ready to submit our predictions. So S was a mistake as well. Not everything might be useful so we need to be careful. We can use this as a feature as well. So that means the it might not be necessarly due to the class or city sincethese decks have people from all classes and cities if it was very correlated with the class we would except that the higher survival rates would be in A B and C since these only have first class passengers GroupType and GroupSizeWe see that much the Family and Mixed has more survivors and deceased in sizes of 2 and 3 for family and 3 4 and 5 for mixed. This one will be harder since we don t have the Cabin value to help us. Both dataframes have many null values in the Cabin column and some on the Age column. ", "id": "giatro/top-5-solution-derretedores-de-iceberg", "size": "26192", "language": "python", "html_url": "https://www.kaggle.com/code/giatro/top-5-solution-derretedores-de-iceberg", "git_url": "https://www.kaggle.com/code/giatro/top-5-solution-derretedores-de-iceberg", "script": "RobustScaler RFECV RidgeClassifier sklearn.feature_selection train_test_split MaxAbsScaler RFE sklearn.discriminant_analysis xgboost sklearn.svm numpy XGBClassifier cross_val_score seaborn SGDClassifier accuracy_score ExtraTreeClassifier ExtraTreesClassifier recall_score def_group_size def_split_fare overall_performance f1_score LinearSVC SVC test_feature_set def_rare tune_models get_features_rank_for_model GaussianNB sklearn.neighbors reassign_group is {score_full_class sklearn.naive_bayes sklearn.tree roc_auc_score more_than_one_cabin test_all_feature_sets sklearn.linear_model PCA StratifiedKFold matplotlib.pyplot MinMaxScaler DecisionTreeClassifier def_group_id show_importances = t_df.loc[ precision_score LinearDiscriminantAnalysis sklearn.model_selection pandas get_null_amount tune_model RandomForestClassifier LogisticRegression def_group_type Perceptron train_model_CV get_model_name \\ RandomizedSearchCV KNeighborsClassifier AdaBoostClassifier cabin_letter get_null_perc is_small GridSearchCV sklearn.decomposition get_features_rank sklearn.metrics sklearn.ensemble StandardScaler sklearn.preprocessing ", "entities": "(('what', 'Feature only best features'), 'go') (('it', 'parents'), 'be') (('how model', 'great view'), 'have') (('that', 'best results'), 'try') (('we', 'feature'), 'visualize') (('s', 'test data'), 'let') (('dataframes', 'Age column'), 'have') (('so we', 'probably them'), 'be') (('therefore it', 'mainly that'), 's') (('RFE', 'step'), 'use') (('other models', 'also it'), 'be') (('that', 'ticket'), 'FareWhen') (('we', 'sorting metric'), 'check') (('we', 'CabinLetter'), 'be') (('third class', 'around 7'), 'see') (('s', 'family'), 'let') (('IsAlone GroupType FamilyWe don t', 'cases'), 'let') (('best', 'a.'), 'need') (('base score', 'actually more features'), 'test') (('they', 'don NaNs beucase'), 'look') (('Here list', 'them'), 'be') (('where they', 'two tickets'), 'be') (('Visualizing', 'data'), 'enjoy') (('That', 'almost every non tree based model'), 'help') (('people', 'survival alone lower rate'), 'see') (('I', 'quickly much explanation'), 'fix') (('families', 'survivors'), 'amost') (('it', 'look'), 'have') (('who', 'greater 2'), 'have') (('Fortunately almost all sets', 'base set'), '96') (('it', 'Age'), 'let') (('now s', 'three features'), 'let') (('already person', 'that'), 'be') (('that', 'people'), 's') (('s', 'model'), 'let') (('we', 'data sets'), 'see') (('it', 'just binary value'), 'think') (('Pclass_1 feature', 'worst features'), 'consider') (('MoreCabin we', '3 more than one cabin'), 'verus') (('Sex', 'very good models'), 'consider') (('Titanic Competition', 'Kaggle https www'), 'be') (('tree', 'based better results'), 'list') (('Cabin I', 'letter'), 'LetterFirst') (('Pre', 'data'), 'process') (('B D', 'survival higher rates'), 'have') (('Finally CabinLetter_B', 'median features'), 'list') (('cell', 'traning'), 'have') (('together Now s', 'survivors'), 'let') (('we', 'that'), 'be') (('very we', 'don really feature'), 'see') (('normal so s', 'median'), 'see') (('Family types', 'other types'), 'have') (('s', 'time'), 'see') (('SplitValues', 'city C'), 'see') (('we', 'us'), 'be') (('Hot One Encoding', 'MinMax'), 'scale') (('we', 'features'), 'create') (('which', '40'), 'see') (('already many models', 'data'), 'see') (('others', 'numbers'), 'be') (('2 it', 'too much time'), 'see') (('best we', 'best results'), 'commete') (('it', 'dataset'), 'let') (('very I', 'it'), 'thave') (('that', 'more than one cabins'), 'be') (('we', 'many cases'), 'have') (('Non many people', 'different single name'), 'let') (('hot encodingFinally we', 'one hot encoding'), 'use') (('Now we', 'case'), 'let') (('that much Family', 'family'), 'mean') (('Pclass_2', 'high classes'), 's') (('more passengers', 'class third Pclass'), 'CabinLetter') (('that', 'same ticket'), 'have') (('s', 'feature selection'), 'elimination') (('scores', 'them'), 'try') (('Now s', 'just third class'), 'let') (('we', 'changes'), 'let') (('t know CabinLetter_D', 'good features'), 'think') (('s', 'them'), 'let') (('it', 'good feature'), 'happen') (('Sex We', 'numerical column'), 'need') (('s', 'feature when full set'), 'let') (('B E', 'future'), 'see') (('Feature Basic Machine Base finally we', 'Machine Learning'), 'Selection') (('different that', 'same ticket'), 'be') (('we', 'hyperparameters'), 'let') (('option', 'feature'), 'let') (('we', 'letters B D E'), 'keep') (('So we', 'groups'), 'count') (('clearly tickets', 'about 26'), 'show') (('Amelia Bissette', 'gropus'), 'embark') (('I', 'one one'), 'take') (('Title_Master', 'much impact'), 'try') (('We', 'Sex'), 'see') (('they', 'surviving'), 'be') (('I', 'analysis'), 'try') (('two ladies', 'city'), 'try') (('one', 'C.'), 'see') (('modified dataframe', 'passenger_id'), 'Modules') (('so we', 'FamilySize'), 'feature') (('that', 'family'), 'classify') (('AdaBoost', 'test just about set'), 'see') (('people', 'exactly columns'), 'be') (('we', 'same group'), 'be') (('Perhaps Pclass', 'other features'), 'have') (('again features', 'that linear models'), 'see') (('indeed sister', 'board'), 'find') (('s', 'S.'), 'let') (('we', 'Mr.'), 'see') (('different fares', 'even same ticket'), 'before') (('they', 'very models'), 'see') (('we', 'group'), 'go') (('don today t', 'person'), 'be') (('so s', 'just Sex'), 'seem') (('Indeed higher rates', 'size'), 'be') (('that', 'us'), 'let') (('so we', 'same category'), 'let') (('numberial feature', 'equal 1'), 'mean') (('we', 'that'), 'suppose') (('Also we', 'training set'), 'need') (('we', 'girls'), 'see') (('didn t', 'same ticket'), 's') (('s', 'many categorial variables'), 'let') (('we', 'data'), 'create') (('That', 'almost all tree based models'), 's') (('this', '4'), 'see') (('Pclass_2', 'perceptron'), 'agree') (('Therefore it', 'metric also deceased'), 'survive') (('s', 'groups'), 'feature') (('first we', 'other notebooks'), 'be') (('many', 'tree models'), 'see') (('very large families', 'also higher changes'), 'have') (('I', 'don what'), 'be') (('we', 'Total positive predictions'), 'follow') (('Therefore it', 'buyers'), 's') (('much Same thing', 'Mr only Master'), 'see') (('others', 'target variable'), 'create') (('Deck F doesn t', 'class'), 'have') (('we', 'already information'), 'see') (('type', 'really influence'), 'see') (('we', 'that'), 'let') (('Title', 'specially Master categories'), 'have') (('people', 'more child'), 'be') (('She', 'Ella personal Holmes'), 'be') (('We', 'higher correlations'), 'visualy') (('Finally we', 'IsChild calculations'), 'need') (('Master', 'tree very models'), 'get') (('columns', 'original format'), 'come') (('four', 'which'), 'let') (('we', 'information'), 'be') (('doesn t', 'much impact'), 'be') (('So this', 'group indeed sizes'), 'show') (('we', 'best score'), 'have') (('we', 'Survived'), 'feature') (('very person', 'analysis'), 'see') (('s', 'ensambles'), 'let') (('same thing', 'IsChild column'), 'happen') (('we', 'instead SmallGroup'), 'see') (('we', 'models'), 'let') (('that', 'many people'), 'see') (('more than one Spouse', 'few siblings'), 'happen') (('lot we', 'finally predictions'), 'make') (('AgeWe', 'NaN also ages'), 'need') (('you', 'data sets'), 'masumrumi') (('s', 'more insights'), 'be') (('we', 'feature'), 'let') (('Embarked s', 'embarked'), 'let') (('he', 'Dr.'), 'embark') (('none', 'tree models'), 'feature') (('we', 'previous tests'), 'let') (('process', 'Feature so called Engineering'), 'help') (('group', '0'), 'have') (('So we', 'options just two S'), 'leave') (('person', 'them'), 's') (('Group ID Group Type InconsistenciesWe', 'cases'), 'need') (('We', 'graph'), 'want') (('modified dataframe', 'passenger_id'), 'Elias') (('that', 'those'), 's') (('s', 'data'), 'be') (('that', 'models'), 'let') (('person', 'family test set odds'), 'be') (('Here we', 'tree models'), 'get') (('we', 'Parch'), 'seem') (('XGB', 'also very best score'), 'get') (('So we', 'data'), 'go') (('it', 'also bad feature'), 'go') "}