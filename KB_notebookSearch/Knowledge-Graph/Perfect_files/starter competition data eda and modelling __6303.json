{"name": "starter competition data eda and modelling ", "full_name": " h2 1 Understanding the Comptetion h3 Competition Objective h2 2 Understanding the Data h4 Columns h2 3 EDA and Data Prepataion h3 3 1 EDA h3 Observations h3 3 1 1 Duplicate Id s and dataset labels h4 Note As we can see this 170113f9 399c 489e ab53 2faf5c64c5bc Id is mentioning multiple datasets So for each id in test we ll need to predict all posible datasets used h3 3 1 2 Duplicate pub title and dataset label h4 Note As we observed in the above artifact there are publication titles using multiple datasets h3 3 1 3 Multiple publications having same title h3 3 1 4 Dataset titles and labels h3 3 2 Data Preparation h3 3 2 1 WordCloud of publication titles h3 3 2 2 WordCloud of most frequent words in the texts h2 4 Baseline Model h3 Hypothesis building h3 4 1 Preparing test set h3 4 2 Let s check this hypothesis on training data To check if it s even worth to use this h3 4 3 Making submission file h4 We can see that string matching gives 100 accuracy on train set On submission as well it will probably give a good score This model can definetely serve as a baseline h4 Note Accuracy isn t the actual evaluation metric The actual evaluation metric is Jaccard similatity base FBeta 0 5 score I have prepared this Notebook that implements the evaluation metric and it also evaluates the baseline on actual metric h4 If you found it useful please consider appreciating it by an UPVOTE Thanks ", "stargazers_count": 0, "forks_count": 0, "description": "com pashupatigupta ci how score is calculated jaccard fbeta notebook for a detailed explanation of evaluation process. We can use these details to generate some features for model building. Duplicate Id s and dataset labels Note As we can see this 170113f9 399c 489e ab53 2faf5c64c5bc Id is mentioning multiple datasets. So for each id in test we ll need to predict all posible datasets used. Meaning that there are cases when two different publications from two different authors have same title. 2 Let s check this hypothesis on training data To check if it s even worth to use this Let s check the accuracySuperb This hypothesis gives 100 accuracy on training set. 1 EDA Observations 1 There are duplicate id s meaning that there are some pulications that are using mutiple datasets. Understanding the Comptetion ci https oerc. Understanding the DataList of data file provided as inputWe have a train. Let s look into the folders first then we ll look into files. Meaning that there are some datasets that has multiple labels. Note Accuracy isn t the actual evaluation metric. Look into this Evaluation Process Jaccard FBeta https www. This is about the json files. A single publication is using mutiple datasets. Thanks print print nDescription of numerical variables desc_df_num df list num_df column_name. So using this id column we can have any information about the publication. edu sites oerc themes oerc images projects coleridge. I have prepared this Notebook https www. EDA and Data Prepataion 3. 2 Data PreparationWe ll read the text of a publication from the json file and put it in the train dataframeNow that we have the text content of each publication let s do some wordcloud analysis. The actual evaluation metric is Jaccard similatity base FBeta 0. 1 WordCloud of publication titles 3. BUT this isn t what the competition demands. rename columns index column_name display desc_df_num. Columns id publication id note that there are multiple rows for some training documents indicating multiple mentioned datasets pub_title title of the publication a small number of publications have the same title dataset_title the title of the dataset that is mentioned within the publication dataset_label a portion of the text that indicates the dataset cleaned_label the dataset_label as passed through the clean_text function from the Evaluation pageSo we have id publication_title and cleaned_label columns. Dataset titles and labelsA single dataset can have multiple labels. Let s look into the files to find out what are they Well these json files are full text version of publication. Well interesting 4 There 45 dataset titles but 130 dataet labels. Now let s look at train. This model can definetely serve as a baseline. That s why that id is repeating. We ll look into how these two are related. So by simple string matching we can find out whether a dataset is mentioned in a publication or not. png Background The Coleridge Initiative is a not for profit organization originally established at New York University that is working with governments to ensure that data are more effectively used for public decision making. Competition ObjectiveOne liner We are required to build an algorithm that can find our what are the datasets that a publications uses. csv file that I believe has the labels information. 2 WordCloud of most frequent words in the texts 4. It s also important to understand the evaluation process of this competition because it is little different. We have with us the full text of scientific publications from numerous research areas we ll identify data sets that the publications authors used in their work. So instead of inferering from the publication which datasets are used we ll be finding out if a particular dataset is used in publication or not. For this we need a list of possible datasets and we can get it from the training set. This is just a baseline hypothesis. This type of automation will be very useful in showing what datasets are used in a particular type of publications or the reverse what are the potential usages of a datset. If you found it useful please consider appreciating it by an UPVOTE. Duplicate pub_title and dataset label Note As we observed in the above artifact there are publication titles using multiple datasets. We have a labelled dataset train set that we ll use to develop our algorithm. 2 Same is the case with pub_title. In any publication the authors mentions the names of the datasets that are used in their work. Description In this competition we need to develop an algorithm to automate the discovery of how scientific data are referenced in publications. The unlabelled dataset test set will be used for evaluation of the algorithm. We have some json files in both the directories. Let s what are the sections in a paper Woah So we have each and every detail of a publication available in a json format. On submission as well it will probably give a good score. 3 There is NO one to one mapping of id and pub_title. We also have train and test folders. Let s look into the data to understand the data and the comptetion better. Baseline Model Hypothesis building Instead of directly jumping into models like BERT XLNet GPT 3 let s think simple here. 3 Making submission file We can see that string matching gives 100 accuracy on train set. The id column is same as the json filenames. com pashupatigupta ci how score is calculated jaccard fbeta that implements the evaluation metric and it also evaluates the baseline on actual metric. 1 Preparing test set 4. head row_limit A text cleaning function. Multiple publications having same titleThere is NO one to one mapping of id and pub_title. ", "id": "pashupatigupta/starter-competition-data-eda-and-modelling", "size": "6303", "language": "python", "html_url": "https://www.kaggle.com/code/pashupatigupta/starter-competition-data-eda-and-modelling", "git_url": "https://www.kaggle.com/code/pashupatigupta/starter-competition-data-eda-and-modelling", "script": "clean_text seaborn plotly.graph_objects WordCloud matplotlib.pyplot STOPWORDS basic_eda pandas get_text wordcloud numpy ", "entities": "(('EDA 1 1 that', 'mutiple datasets'), 'observation') (('Dataset', 'labelsA single multiple labels'), 'have') (('how scientific data', 'publications'), 'description') (('s', 'data'), 'let') (('themes oerc images', 'coleridge'), 'oerc') (('when two different publications', 'same title'), 'mean') (('I 2faf5c64c5bc d', 'multiple datasets'), 's') (('single publication', 'mutiple datasets'), 'use') (('we', 'publication multiple datasets'), 'pub_title') (('We', 'model building'), 'use') (('that', 'work'), 'mention') (('as well it', 'probably good score'), 'give') (('XLNet 3 s', 'BERT'), 'let') (('json files', 'text full publication'), 'let') (('dataset', 'publication'), 'find') (('competition', 'isn what'), 't') (('hypothesis', 'training set'), 'let') (('it', 'competition'), 's') (('particular dataset', 'publication'), 'use') (('com pashupatigupta how score', 'evaluation process'), 'ci') (('dataset test unlabelled set', 'algorithm'), 'use') (('d we', 'publication'), 'have') (('we', 'posible datasets'), 'need') (('we', 'training set'), 'need') (('Understanding', 'train'), 'have') (('that', 'multiple labels'), 'mean') (('we', 'algorithm'), 'have') (('publications', 'that'), 'require') (('we', 'Evaluation pageSo'), '-PRON-') (('model', 'definetely baseline'), 'serve') (('Multiple publications', 'd'), 'be') (('first then we', 'files'), 'look') (('datasets', 'potential datset'), 'be') (('We', 'directories'), 'have') (('publications authors', 'work'), 'have') (('So we', 'json available format'), 'let') (('s', 'wordcloud analysis'), 'read') (('it', 'actual metric'), 'ci') (('data', 'decision more effectively public making'), 'Background') (('I', 'labels information'), 'file') (('it', 'UPVOTE'), 'find') (('Thanks print', 'print numerical variables'), 'nDescription') (('string matching', 'train set'), 'see') "}