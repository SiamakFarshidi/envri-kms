{"name": "effective feature engineering ", "full_name": " h1 Effective feature engineering h2 1 Correlation h2 2 Score gain on a simple model h2 3 Feature importances of Tree models h2 4 Permutation importance h2 5 SHAP values h2 6 Score gain on a complex model ", "stargazers_count": 0, "forks_count": 0, "description": "To our dismay we see that the feature importance orderings are very different for each of the three options provided by XGBoost For the cover method it seems like the capital gain feature is most predictive of income while for the gain method the relationship status feature dominates all the others. After selecting important features I start looking at the actual impact on a complex model. Simple models are easy to interpret. That s why it s represented like 0. Question What is a fair way for a colition to divide its payoff Depends on the definition of fairness Approach Identify axioms that express properties of a fair payoff division Symmetry Interchangeable agents should receive the same payments Dummy Players Dummy players should receive nothing Additivity v_1 v_2 S v_1 S v_2 S The author of SHAP found that we can apply this concept to machine learning. Thank you for reading this Kernel. I m still new to this field. Option 1 Give it as category df matchType df matchType. Permutation importance5. I will show an example in another Kernel. pdf 2017 Understanding why a model makes a certain prediction can be as crucial as the prediction s accuracy in many applications. Intuitively it sounds fair. 0033 standard deviation. Score gain on a complex modelChecking on the real model. In here if the value on the target is close to 0 it means that the feature may be irrelevant to the target. We can see the score and the execution time. But it can be another risk if longer training time reduces the number of your trials. I d like to hear how you work on feature engineering. I created a Kernel dedicated for feature engineering for this competition. I d like to share what I ve done so far. Checking correlation is the fastest way to estimate the impact but it doesn t capture the actual contribution of the score. There are several options for measuring importance like split How many times the feature is used to split gain The average training loss reduction gained when using a feature for splitting. Interpretable Machine Learning with XGBoostThis is the background of Interpretable machine learning which is a field receiving a lot of attention recently. pdf 2016 known as LIME The main contribution of SHAP is that they introduced the concept of Shapley Value to measure the contribution. It d be enough if you just want to see the impact of the new feature you added. astype category Option 2 pd. Removing a feature and see the difference. It s the average of combinations of features. Build a complex model5. The implementation is available on GitHub slundberg shap A unified approach to explain the output of any machine learning model https github. Try various features on a simple model4. run_experiments method takes preprocess functions and returns DataFrame. If you successfully setup an effective environment for experiments at the beginning of the competition it puts you at an advantage. The shapley values are calculated like below. A Unified Approach to Interpreting ModelPredictions https arxiv. SHAP also can visualize how the score changes when the feature value is low high on each data. This method was proposed in this paper Why Should I Trust You Explaining the Predictions of Any Classifier https arxiv. Do you always need to wait for 1 hour to see the impact of a newly added feature I talked with experienced Kaggers about feature engineering. Permutation importanceThe basic idea is that observing how much the score decreases when a feature is not available the method is known as permutation importance or Mean Decrease Accuracy MDA. Feature importances of Tree modelsTree models can output feature importances. Shapley Value is a solution concept in cooperative game theory proposed in 1953 by Lloyd Shapley https en. Build a simple model3. SHAP valuesSHAP proposed a new fair way to measure contribution which is justified in game theory. Aside from the main topic of this Kernel it s better to split dataset by match since we predict results by group in match. You can find papers and libraries here lopusz awesome interpretable machine learning https github. this is what I was doing above but more reliable However there is room to discuss how to define measure contribution. Traid off Accurate result The number of trials How they work on a competition is like below. Effective feature engineeringSay you have a model which takes 1 hour to train. get_dummies df matchType axis 1 Option 3 Drop it for now Not the best solution. You can see how important rank features are in this competition. This should make us very uncomfortable about relying on these measures for reporting feature importance without knowing which method is best. Score gain on a complex modelThe upper things are faster but less accurate and lower things are more accurate but slower. Score gain on a simple modelI used LinearRegression during feature engineering since it s simple and fast. Then use the simple model to interpret how it s trained. They said that you get more accurate result when you check on the actual complex model you carefully built. Train with promising featuresThey go back and forth between steps during a competition. ELI5 shuffles the target feature instead of removing it to make it useless so that we don t need to re train the model again. However the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret such as ensemble or deep learning models creating a tension between accuracy and interpretability. I m trying from the top of the list when I come up with a new idea. The figure shows the importance of each feature. org wiki Lloyd_Shapley. In machine learning player is feature and contribution is score. com slundberg shap SHAP values represent the fair score of features depending on their contribution towards the total score in the set of features. Next let s see aggregated features. CorrelationThis is the simplest way to see the relation between features. Score gain on a simple model3. However sometimes it doesn t represent the actual contribution. Feature importances of Tree models4. Let s see player level generated features. They built a simple model which works well only on a local point We don t need to predict on the all points. So what should we do It depends on the requirement and there is a trade off but I d recommend to build an environment to try new ideas quickly and fail quickly You d earn new ideas through the process. For example we have 3 features L M and N. com lopusz awesome interpretable machine learning 4. ", "id": "rejasupotaro/effective-feature-engineering", "size": "7253", "language": "python", "html_url": "https://www.kaggle.com/code/rejasupotaro/effective-feature-engineering", "git_url": "https://www.kaggle.com/code/rejasupotaro/effective-feature-engineering", "script": "lightgbm median_by_team mean_by_team run_experiment headshotKills_over_kills train_test_split min_by_team LinearRegression walkDistance_over_heals players_in_team numpy seaborn teamwork mean_absolute_error sum_by_team total_distance killPlace_over_maxPlace sklearn.linear_model items walkDistance_over_kills LGBMRegressor matplotlib.pyplot rank_by_team pandas PermutationImportance max_by_team original eli5.sklearn run_experiments sklearn.metrics reload ", "entities": "(('t', 'points'), 'build') (('faster less things', 'complex modelThe upper things'), 'be') (('Shapley Value', 'Lloyd Shapley https'), 'be') (('run_experiments method', 'DataFrame'), 'take') (('We', 'score'), 'see') (('method', 'feature importance'), 'make') (('feature when value', 'low data'), 'visualize') (('we', 'L M'), 'have') (('player level', 'features'), 'let') (('it', 'score'), 'be') (('You', 'papers'), 'find') (('Feature importances', 'feature importances'), 'output') (('It', 'features'), 's') (('You', 'Classifier https arxiv'), 'propose') (('which', '1 hour'), 'engineeringSay') (('how you', 'feature engineering'), 'd') (('You quickly quickly d', 'process'), 'depend') (('com slundberg shap SHAP values', 'features'), 'represent') (('you', 'actual complex model'), 'say') (('However sometimes it', 'actual contribution'), 'doesn') (('Option', 'category df matchType df matchType'), 'give') (('I', 'feature engineering'), 'need') (('I', 'measure more However how contribution'), 'be') (('certain prediction', 'many applications'), 'pdf') (('it', 'feature engineering'), 'use') (('we', 'model'), 'shuffle') (('we', 'match'), 's') (('I', 'complex model'), 'start') (('I', 'Kernel'), 'show') (('training loss average reduction', 'splitting'), 'be') (('they', 'contribution'), 'pdf') (('when I', 'new idea'), 'm') (('you', 'new feature'), 'be') (('I', 'competition'), 'create') (('we', 'machine learning'), 'question') (('relationship', 'status feature others'), 'see') (('feature', 'target'), 'in') (('implementation', 'machine learning model https github'), 'be') (('figure', 'feature'), 'show') (('method', 'permutation importance'), 'be') (('featuresThey', 'competition'), 'train') (('rank how important features', 'competition'), 'see') (('how it', 'Then simple model'), 'use') (('longer training', 'trials'), 'be') (('which', 'game theory'), 'propose') (('even experts', 'accuracy'), 'achieve') (('How they', 'competition'), 'result') (('CorrelationThis', 'features'), 'be') (('it', 'advantage'), 'setup') (('which', 'attention'), 'be') "}