{"name": "a detailed regression guide with house pricing ", "full_name": " h1 Goals h1 Importing Necessary Libraries and datasets h1 A Glimpse of the datasets h2 Sample Train Dataset h2 Sample Test Dataset h1 Exponential Data Analysis EDA h2 Missing Values h3 Train h3 Test h4 Relationship between missing values and Sale Price h2 Numerical variables h3 Temporal Variable h3 Discrete Variables h3 Continous Variables h4 SalePrice vs OverallQual h4 SalePrice vs GrLivArea h4 SalePrice vs GarageArea h4 SalePrice vs TotalBsmtSF h4 SalePrice vs 1stFlrSF h4 SalePrice vs MasVnrArea h4 Observations h4 Assumptions of Regression h4 Observation h4 Resources h4 Outliers h3 Categorical Variables h4 Rare Values h1 Feature Engineering h2 Missing Values h2 Year Variables h2 Numeric Variable Transformation h2 Categorical Variables Dealing with rare Labels anticipated rare labels h2 Categorical Variables Converting to String Variables h2 Feature Scaling h1 Feature Selection h2 Dealing with Missing Values h1 Resources h1 Credits h4 If you like to discuss any other projects or just have a chat about data science topics I ll be more than happy to connect with you on h1 If you have come this far Congratulations h1 If this notebook helped you in any way or you liked it please upvote and or leave a comment ", "stargazers_count": 0, "forks_count": 0, "description": "If the noise is not the same across the values of an independent variable like the residual plot above we call that Heteroscedasticity. Predictors can have wildly different results depending on the observations in our sample and small changes in samples can result in very different estimated effects. Some of these categorical variable consists of values that are present in a very small amount. As we discussed before there is a linear relationship between SalePrice and GrLivArea. com help images KurtosisPict. Therefore we can find and fix various assumptions with a few unique techniques. operatorname SST sum_ i 1 n left y_i bar y right 2 Here y_i Each observed data point. There are two types of Skewness Positive and Negative. Let s write the equation for R 2. beta_1 Slope Weight Coefficient of x. Let s try this regression in the housing dataset. Quite simple isn t it. In other words if we see one of these assumptions in the dataset it s more likely that we may come across with others mentioned above. r_ xy the sample Pearson correlation coefficient between observed X and Y I hope most of us know how to calculate all these components from the two equations above by hand. It may seem confusing with multiple similar abbreviations but once we focus on what they each do things will become much more intuitive. Let s find out how the sales price is distributed. The value of R 2 increases as more feature gets added despite the effectiveness of those features in the model. Our target variable is right skewed. Even though it seems like there is a linear relationship between the response variable and predictor variable the residual plot looks more like a funnel. One way to fix this Heteroscedasticity is by using a transformation method like log transformation or box cox transformation. If you would like to improve this result further you can think about the assumptions of the linear regressions and apply them as we have discussed earlier in this kernel. Lasso deals with multicollinearity more brutally by penalizing related coefficients and force them to become zero hence removing them. Create models that are well equipped to predict housing prices. As you can tell it is the opposite of Homoscedasticity. You can also check out this https www. This notebook is always a work in progress. Once we train our algorithm using 2 3 of the train data we start to test our algorithms using the remaining data. The error y_i hat y _i The square of the error y_i hat y _i 2 The sum of the square of the error sum_ i 1 n y_i hat y _i 2 that s the equation on the left. This is a problem since we may think that having a greater R 2 means a better model even though the model didnot actually improved. Fitting model Advanced approach Blending Models Submission Resources Statistics Types of Standard Deviation What is Regression Introduction to Econometrics with R Writing pythonic ways Six steps to more professional data science code Creating a Good Analytics Report Code Smell Python style guides The Best of the Best Practices BOBP Guide for Python PEP 20 The Zen of Python The Hitchiker s Guide to Python Python Best Practice Patterns Pythonic Sensibilities Why Scikit Learn Introduction to Scikit Learn Six reasons why I recommend scikit learn Why you should learn Scikit learn A Deep Dive Into Sklearn Pipelines Sklearn pipelines tutorial Managing Machine Learning workflows with Sklearn pipelines A simple example of pipeline in Machine Learning using SKlearn Credits To GA where I started my data science journey. Continous VariablesLet s find out the continous variables. id column this column the unique identifier for each house. If you have come this far Congratulations If this notebook helped you in any way or you liked it please upvote and or leave a comment This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. one is for sample population and one is for Total population. Let s focus on the numerical variables this time. This metrics is the relationship between y and x. Temporal VariableThere are 4 year variables in the dataset. Elastic Net adds both the sum of the squares errors and the absolute value of the squared error. There tend to be some relationships between the variables and the SalePrice for example some are monotonic like OverallQual some almost monotonic except for a unique values like OverallCond. Let s break this down. However do not let the simplicity of this model fool you as Linear Regression is the base some of the most complex models out there. Once we are confident about the result of our algorithm we may use the model to on the original test data and submit in the challenge. We will get rid of these two in the feature engineering section. Now let s make sure that the target variable follows a normal distribution. Regularization ModelsWhat makes regression model more effective is its ability of regularizing. the Kolmogorov Smirnov test can check for normality in the dependent variable. image https cdn images 1. However we will be using many algorithms as scikit learn modules makes it easy to implement them and get the best possible outcome. The goodness of fit test e. Let s look at another scatter plot with a different feature. In the visualization above SSR is the distance from baseline model to the regression line. This is called Heteroscedasticity more explanation below and is a red flag for the multiple linear regression model. If you want to learn more about the probability plot Q Q plot try this https www. However the value of R 2_ adj decreases if we use a feature that doesn t improve the model significantly. The following part is a work in progress So from the Evaluation section above we know that RSS sum_ i 1 n left y_i hat y _i right 2 And we already know. This residual is the difference between the predicted line and the observed value. jpg resize 375 2C234 You can read more about this from this https codeburst. SalePrice vs GarageAreaAnd the next one. We want to know estimate predict the sale price of a house based on the given area How do we do that One naive way is to find the average of all the house prices. Many of these variables will not be useful for the final model i. Skewness differentiates in extreme values in one versus the other tail. We are using median here because mean would not direct us towards a better assumption as there are some outliers present. X_train y_train first used to train the algorithm. SalePrice vs MasVnrAreaOkay I think we have seen enough. bar y Mean of y value. astype str all_data YearRemodAdd all_data YearRemodAdd. 83 or 83 correlation between GarageYrBlt and YearBuilt. As the name suggests this kernel goes on a detailed analysis journey of most of the regression algorithms. A histogram box plot or a Q Q Plot can check if the target variable is normally distributed. Do a comprehensive data analysis along with visualizations. So simply speaking an error is the difference between an original value y_i and a predicted value hat y _i. The higher the value of this constant the more the impact in the loss function. Resources Assumptions of Linear Regression Assumptions of Multiple Linear Regression Youtube All regression assumptions explained OutliersExtreme values affects the performance of a linear model. In machine learning we call it coefficient. The problem with rare values is that sometimes they are present in the train but not in test sometimes in test but not in train. The code below basically splits the train data into 4 parts X_train X_test y_train y_test. There are many outliers in the scatter plots above that took my attention. I will also describe the model results along with many other tips. RMSE Root Mean Squared Error operatorname RMSE sqrt frac 1 n sum_ i 1 n hat y_i y_i 2 Here y_i Each observed data point. Homoscedasticity describes a situation in which the error term or variance or the noise or random disturbance in the relationship between the independent variables and the dependent variable is the same across all values of the independent variable. However we generally don t use data for example year in their raw format instead we try to get information from them. If we were to write the equation regarding the sample example above it would simply look like the following equation Sale Price beta_0 beta_1 Area epsilon This equation gives us a line that fits the data and often performs better than the average line above. jpeg KurtosisAccording to Wikipedia In probability theory and statistics Kurtosis is the measure of the tailedness of the probability. In statistics language we say SSE is the squared residual that was not explained by the regression line and this is the quantity least square minimizes. If we want to create any linear model it is essential that the features are normally distributed. As you can see the residual variance is the same as the value of the predictor variable increases. We already know that our target variable does not follow a normal distribution. This is to avoid over fitting. the right sides one above is the replacement of hat y _i it is replaced by left beta_0 sum_ j 1 p beta_j x_j right which simply follow s the slope equation y mx b where beta_0 is the intercept. R 2 can be infinitely negative as well. This relationship is exact because we are given X and y beforehand and based on the value of X and y we come up with the slope and y intercept which in turns determine the relationship between X and y. com wp content uploads 2017 01 home sales 701x526. Feature ScalingFeature SelectionDealing with Missing Values Missing data in train and test data all_data So there are no missing value left. So SST describes the distance between the black dot and the average line. Which means the variation in the Y s is completely explained by the regression line causing the value of R 2 to be close to 1. SST TSS SST is the sum of the squared distance from all points to average line bar y. The only difference between left sides equation vs. beta_0 the y intercept it is a constant and it represents the value of y when x is 0. Now these assumptions are prone to happen altogether. Here we are not only renaming the rare labels but also writing codes that deal with labels that may not be in the dataset now but show up in the future. We can now plug them in the linear equation to get the predicted y value. However before doing that I want to find out the relationships among the target variable and other predictor variables. Missing ValuesTrainTestRelationship between missing values and Sale Price These plots compare the median SalePrice in the observations where data is missing vs the observations where a value is available. On the two charts above the left one is the average line and the right one is the regression line. We will explain more on MSE later. b y intercept. To fit an exact slope equation in an inexact relationship of data we introduce the term error. Blue dots are observed data points and red lines are error distance from each observed data points to model predicted line. Numeric Variable TransformationWe are transforming the continous variables. hat y beta_0 beta_1 x_1 beta_2 x_2. Similarly many other features such as BsmtUnfSF FullBath have good correlation with other independent feature. If you are learning about Q Q plots for the first time. It represents the relationship between X and y. Let s put this one in a scatter plot and see how it looks. com masumrumi a statistical analysis ml workflow of titanic. Let s find themYou can see these values are represented in years as we hoped. Numerical variablesLet s find the numerical variables from the dataset. Let s look at the residual plot for independent variable GrLivArea and our target variable SalePrice. Of course it makes sense since we are talking about an error mean squared error. Missing Values Temporal variables year variables Non Gaussian distributed variables variables that do not follow a normal distribution. Any feedback constructive criticism would be genuinely appreciated. We will do the transformation in the feature engineering section. This means that the extreme values of this distribution are similar to that of a normal distribution. This penalty is added to the least square loss function above and looks like this. x and y are the data points located in x_axis and y_axis respectively. However Lasso is well suited for redundant variables. com watch v 9IcaQwQkE9I one if you have some extra time. Please checkout this https www. R 2 The Coefficient of determination R 2 describes the proportion of variance of the dependent variable explained by the regression model. Learn review explain complex data science topics through write ups. Let s see if there is a relationship between year features and SalePriceThese charts seems more like a real life situation case. The two on the top right edge of SalePrice vs. For the sake of understanding this model we will use only two features SalePrice and GrLivArea. One thing to take note here there are some outliers in the dataset. In these cases models doesn t know what to do with the values. Similar to Simple Linear Regression there is an equation for multiple independent variables to predict a target variable. Elastic Net is the combination of these two. Linear regression or multilinear regression requires independent variables to have little or no similar features. To get more in depth of it let us review the least squared loss function. The coefficients will be somewhere between 0 and ones for simple linear regression. If you have any idea suggestions about this notebook please let me know. com statistical guides measures of spread standard deviation. The only difference between Ridge and Lasso is the way they penalize the coefficients. org tutorials intermediate gridspec. Let s look at the YrSold plot This plot should raise an eyebrows. Having a negative indicates that the predictive equation has a greater error than the baseline model. SalePrice vs OverallQual OverallQual is a categorical variable and a scatter plot is not the best way to visualize categorical variables. I am going to only mention the equation of the pearson correlation r_xy here as it may be unknown to some of the readers. The following two examples depict two cases where no or little linearity is present. In the chart above SSE is the distance of the actual data point from the regression line. SSR is also known as ESS Explained Sum of the Squared Error and SSE Sum of the Squared Error RSS Residual Sum of the Squared Error Let s break these down. R 2_ adj 1 frac 1 R 2 n 1 n k 1 here n of datapoints. text minimize RSS Ridge Lasso sum_ i 1 n left y_i left beta_0 sum_ j 1 p beta_j x_j right right 2 lambda_1 sum_ j 1 p beta_j lambda_2 sum_ j 1 p beta_j 2 This equation is pretty self explanatory if you have been following this kernel so far. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. Let s describe the effect of regularization and then we will learn how we can use loss function in Ridge. 1 gbr_model_full_data. I want to focus our attention on the target variable which is SalePrice. In other extreme cases when there is no relation between x and y hence SSR 0 and therefore SSE SST The regression line explains none of the variances in Y causing R 2 to be close to 0. Therefore this regression is called Simple linear regression SLR. Feature EngineeringHere is a list of things we will go over in this section based on this dataset. If the model performs well we dump our test data in the algorithms to predict and submit the competition. gives us statistical info about the numerical variables. A much anticipated decrease in mean squared error mse therefore better predicted model. Year VariablesWe learned from the EDA section that getting the elapsed years of yearBuilt YearRemodAdd and GarageYrBlt actually shows a better relation with median sale prices. If there are any recommendations changes you would like to see in this notebook please leave a comment at the end. Many of us may have learned to show the relationship between two variable using something called y equals mX plus b. Rare ValuesThe presence of rare values can skew the model result. These are the predictor variables sorted in a descending order starting with the most correlated one OverallQual. As we engineer existing features or new features we might use some techniques to learn parameters from the data. x_1 Independent variable simple linear regression variables. Multivariate Normality Normality of Errors The linear regression analysis requires the dependent variable to be multivariate normally distributed. Because this distribution has thin tails it has fewer outliers e. get_dtype_counts let s make a variable that indicates 1 if the observation was missing or zero otherwise let s compare the median SalePrice in the observations where data is missing vs the observations where a value is available let s run the function on each variable with missing data make list of numerical variables visualise the numerical variables let s explore the values of these temporal variables let s explore the relationship between the year variables and the house price in a bit of more detail capture difference between year variable and year in which the house was sold let s make a list of discrete variables Generate a mask for the upper triangle taken from seaborn example gallery cbar False make list of continuous variables Let s go ahead and analyse the distributions of these variables Let s go ahead and analyse the distributions of these variables after applying a logarithmic transformation log does not take 0 or negative values so let s be careful and skip those variables log transform the variable customizing the QQ_plot. In that case we may need to change our function depending on the data to get the best possible fit. Here we are plotting our target variable with two independent variables GrLivArea and MasVnrArea. Fixing Skewness Creating New Features Deleting features Creating Dummy Variables. sum_ j 1 p beta_j 2 is the squared sum of all coefficients. text minimize RSS Lasso sum_ i 1 n left y_i left beta_0 sum_ j 1 p beta_j x_j right right 2 lambda_1 sum_ j 1 p beta_j Here lambda_2 is a constant similar to the Ridge function. GrLivArea seem to follow a trend which can be explained by saying that As the prices increased so did the area. com masumrumi a stats analysis and ml workflow of house pricing kernel at Kaggle. It s pretty apparent from the chart that there is a better linear relationship between SalePrice and GrLivArea than SalePrice and MasVnrArea. No or Little multicollinearity Multicollinearity is when there is a strong correlation between independent variables. Now that we know every nitty gritty details about this equation let s use it for science but before that a couple of things to remember. In previous codes we have seen that log transformation can help us to make features more like normally distribute. Positive Skewness similar to our target variable distribution means the tail on the right side of the distribution is longer and fatter. Once we get the outcomes we compare it with y_test By comparing the outcome of the model with test_y we can determine whether our algorithms are performing well or not. For example running this by clicking run or pressing Shift Enter will list the files in the input directory for some statistics Any results you write to the current directory are saved as output. If the points are perfectly linear then error sum of squares is 0 In that case SSR SST. It looks like a blob of data points and doesn t seem to give away any relationships. You can tell this is not the most efficient way to estimate the price of houses. Here we see that the pre transformed chart on the left has heteroscedasticity and the post transformed chart on the right has Homoscedasticity almost an equal amount of variance across the zero lines. Let s find out if we have any in our variables. Linear Regression We will start with one of the most basic but useful machine learning model Linear Regression. However in real life data is not that simple. Let s discuss what we have found so far. php article for more. As you can see these variables are not normally distributed including our target variable SalePrice. SSR ESS SSR is the sum of the squared residual between each predicted value and the average line. Therefore this equation is called Multiple Linear Regression. Let s bring back the three charts to show our target variable. One of them is mean squared error MSE which we used while comparing two models. To Udemy Course Deployment of Machine Learning. For now I am going to dive right into the R 2. One of the benefits of regularization is that it deals with multicollinearity high correlation between predictor variables well especially Ridge method. So a symmetrical distribution will have a skewness of 0. In negative Skewness the mean and median will be less than the mode. Elastic Net Elastic Net is the combination of both Ridge and Lasso. So please stay tuned for more to come. However if you take your time to read this and other model description sections and let me know how I am doing I would genuinely appreciate it. The average line clearly does not represent all the datapoint and fails to grasp the linear relationship between GrLivArea SalePrice. We want to minimize this error. This kernel is the regression siblings of my other Classification kernel. html sphx glr tutorials intermediate gridspec py link. Platykurtic Platykurtic describes a particular statistical distribution with thinner tails than a normal distribution. now score cv_rmse gbr print gbr. show a finite number of valuesLet s analyze the discrete variables and see how they are related with the target variable SalePrice. MSE Mean Squared Error operatorname MSE frac 1 n sum_ i 1 n hat y_i y_i 2 MAE Mean Absolute Error operatorname MAE frac sum_ i 1 n bar y y_i n RSE Relative Squared Error operatorname RSE frac sum_ i 1 n hat y_i y_i 2 sum_ i 1 n bar y y_i 2 RAE Relative Absolute Error operatorname RAE frac sum_ i 1 n hat y_i y_i sum_ i 1 n bar y y_i and R 2 Coefficient of the determination The evaluation metrics often named in such a way that I find it confusing to remember. RSS sum_ i 1 n left y_i left beta_0 sum_ j 1 p beta_j x_j right right 2 This equation is also known as the loss function. In addition to that this kernel uses many charts and images to make things easier for readers to understand. I will explain more on this later. com max 1600 1 nj Ch3AUFmkd0JUSOW_bTQ. These values can overfit the models significantly. beta_n x_n Here We already know parts of the equation and from there we can keep adding new features and their coefficients with the equations. Any feedback about further improvements would be genuinely appreciated. This way I would review what I know and at the same time help out the community. astype str Check the skew of all numerical features feture engineering a new feature TotalFS create a figure get the axis of that figure plot a scatter plot on it with our data get the axis plot it iterate over predictions plot it iterate over predictions score cv_rmse stack_gen print Stack. now print GradientBoosting gbr_model_full_data gbr. Linearity Correct functional form Linear regression needs the relationship between each independent variable and the dependent variable to be linear. So How do we check regression assumptions We fit a regression line and look for the variability of the response data along the regression line. bar y the sample mean of observed values Y bar x the sample mean of observed values X s_y the sample standard deviation of observed values Y s_x the sample standard deviation of observed values X There are two types of STD s. If you already know enough about Linear Regression you may skip this part and go straight to the part where I fit the model. Often the relationship is unknown to us and even if we know the relationship it may not always be exact. In statistics language we say that SSR is the squared residual explained by the regression line. In that case we don t want to learn from part of the data that will be used to evaluate the model. We will describe more on this in following parts. We will take care of that in the feature engineering selection. That s the sort of relationship we would like to see to avoid some of these assumptions. Fitting model simple approach Train_test split We have separated dependent and independent features We have separated train and test data. So In other words it is the measure of the extreme values outliers present in the distribution. fit X y 0. It is also known as TSS Total Sum of the Squared Error SSR Sum of the Squared Regression is the residual explained by the regression line. This plot above is an excellent example of Homoscedasticity. Before building a multiple linear regression model we need to check that these assumptions below are valid. Some of the other metrics are. distribution of a real valued random variable. Ridge Ridge regression adds penalty equivalent to the square of the magnitude of the coefficients. However The two on the bottom right of the same chart do not follow any trends. However our residual plot is anything but an unstructured cloud of points. Multicollinearity can lead to a variety of problems including The effect of predictor variables estimated by our regression will depend on what other variables are included in our model. If you are reading this in my github page you may find it difficult to follow through as the following section includes mathematical equation. In Mean squared error we subtract the mean of y from each y datapoints and square them. Missing ValuesSuccess It looks like there is no more missing variables. As you can see there is a significant difference in median sale price between where missing value exists and where missing value doesn t exist. The residual will look like an unstructured cloud of points centered around zero. Let s now see how the distribution might look once we do the transformation. We will use log transformation to do the transformation. These three charts above can tell us a lot about our target variable. It adds both the sum of squared coefficients and the absolute sum of the coefficients with the ordinary least square function. We can fix this by using different types of transformation more on this later. We need to be particulary careful for these variables to extract maximum values for a linear model. text minimize RSS Ridge sum_ i 1 n left y_i left beta_0 sum_ j 1 p beta_j x_j right right 2 lambda_2 sum_ j 1 p beta_j 2 Here lambda_2 is constant a regularization parameter. We will do that later. SalePrice vs TotalBsmtSFand the next SalePrice vs 1stFlrSFHow about one more. As we can see the multicollinearity still exists in various features. I will incorporate new concepts of data science as I comprehend them with each update. extreme values three or more standard deviations from the mean than do mesokurtic and leptokurtic distributions. The equation is as follows. When lambda_2 is infty the coefficients become 0 When lambda_2 is between 0 and infty 0 lambda_2 infty The lambda_2 parameter will decide the miagnitude given to the coefficients. Sometimes we may be trying to fit a linear regression model when the data might not be so linear or the function may need another degree of freedom to fit the data. hat y_i Predicted data point for each x_i depending on i. Since we fit a linear model we assume that the relationship is linear and the errors or residuals are pure random fluctuations around the true line. We need to remove them in the Feature engineering section. Categorical Variables Dealing with rare Labels anticipated rare labelsThis is an important step and can save a lot of headache down the road. Success As you can see the log transformation removes the normality of errors which solves most of the other errors we talked about above. epsilon error or residual. If you like this notebook or find this notebook helpful Please feel free to UPVOTE and or leave a comment. The linearity assumption can be tested with scatter plots. Assumptions of Regression Linearity Correct functional form Homoscedasticity Constant Error Variance vs Heteroscedasticity. Let s see it in the function. The reason behind that is when predictors are strongly correlated there is not a scenario in which one variable can change without a conditional change in another variable. org math algebra two var linear equations writing slope intercept equations v graphs using slope intercept form. This penalty is added to the least square loss function and replaces the squared sum of coefficients from Ridge. It is important to check for multicollinearity Lasso Lasso adds penalty equivalent to the absolute value of the sum of coefficients. k of feature used. 89 correlation between GarageCars and GarageArea. We use this function to predict the values of one dependent target variable based on one independent predictor variable. y beta_0 beta_1 x_1 epsilon And this is the equation for a simple linear regression. Using cross validation. Leptokurtic Example of leptokurtic distributions are the T distributions with small degrees of freedom. We expect that the variability in the response dependent variable doesn t increase as the value of the predictor independent increases which is the assumptions of equal variance also known as Homoscedasticity. Let s make a comparison of the pre transformed and post transformed state of residual plots. In order to get around this we use Adjusted R Squared R 2_ adj Adjusted R Squared R 2_ adj R 2_ adj is similar to R 2. We will get rid off them later. Let s talk about those a bit. I am going to share how I work with a dataset step by step from data preparation and data analysis to statistical tests and implementing machine learning models. Regularization models such as Lasso Ridge do the clean up later on. hat y _i is the predicted value. com watch v smJBsZ4YQZw video. There are three types of regularizations. y mX b Here m slope of the regression line. So why do we still have to split our training data If you are curious about that I have the answer. We already know that our target variable is not normally distributed. astype str all_data OverallQual all_data OverallQual. This minimization becomes a balance between the error the difference between the predicted value and observed value and the size of the coefficients. text residual _i y_i hat y _i This error is the only part that s different addition from the slope equation. As you can see the regression line reduces much of the errors therefore performs much better than average line. As we look through these scatter plots I realized that it is time to explain the assumptions of Multiple Linear Regression. In other words there is a constant variance present in the response variable as the predictor variable increases. The best way to solve multicollinearity is to use regularization methods like Ridge or Lasso. In another word it gives weight as to for each x horizontal space how much y vertical space we have to cover. Negative Skewness means the tail on the left side of the distribution is longer and fatter. The price of the houses increases with the overall quality. With very high multicollinearity the inverse matrix the computer calculates may not be accurate. The way we compare between the two predicted lines is by considering their errors. For this competition when we train the machine learning algorithms we use part of the training set usually two thirds of the train data. As you can see most of the continous variables seem to contaion outliers. In positive Skewness the mean and median will be greater than the mode similar to this dataset. We can no longer interpret a coefficient on a variable as the effect on the target of a one unit increase in that variable holding the other variables constant. Categorical VariablesThere is less cardinality in these features. Let s go through some of the correlations that still exists. However there is an apparent relationship between the two features. So this is a guide for me and everyone else who is reading it. In simple terms it shows 1 unit of increase in y changes when 1 unit increases in x. Phew This looks like something we can work with Let s find out the MSE for the regression line as well. One important thing to remember that log doesn t take 0 or negative values therefore we will have to skip variable including values of 0 or lower. It looks like there are quite a bit Skewness and Kurtosis in the target variable. We can see variables are normally distributed Gaussian distribution. A visualization would make things much more clear. checkout this https www. Let s check out some more features to determine the outliers. As you can see from the equation the increase of k feature in the denumerator penilizes the adjusted R 2 value if there is not a significant improvement of R 2 in the numerator. R 2 measures the explanatory power of the model The more of the variance in the dependent variable Y the model can explain the more powerful it is. We can have a target variable predicted by multiple independent variables using this equation. I have tried to show this whole process in the visualization chart below. Generate a mask for the upper triangle taken from seaborn example gallery let s make boxplots to visualise outliers in the continuous variables log does not take negative values so let s be careful and skip those variables determine the of observations per category return categories that are rare print categories that are present in less than 1 of the observations we can re use the function to determine median sale price that we created for discrete variables Let s separate into train and test set Remember to set the seed random_state for this sklearn function we are setting the seed here stratify train SalePrice make a list with the numerical variables that contain missing values print percentage of missing values per variable replace engineer missing values as we described above calculate the mode using the train set add binary missing indicator in train and test replace missing values by the mode in train and test check that we have no more missing values in the engineered variables capture difference between the year variable and the year in which the house was sold function finds the labels that are shared by more than a certain of the houses in the dataset find the frequent categories replace rare categories by the string Rare this function will assign discrete values to the strings of the variables so that the smaller value corresponds to the category that shows the smaller mean house sale price order the categories in a variable from that with the lowest house sale price to that with the highest create a dictionary of ordered categories to integer values use the dictionary to replace the categorical strings by integers capture all variables in a list except the target and the ID count number of variables create scaler fit the scaler to the train set transform the train and test set getting a copy of train all_data OverallCond all_data OverallCond. Skewness is the degree of distortion from the symmetrical bell curve or the normal curve. Heatmap is an excellent way to identify whether there is multicollinearity or not. It is essential to standardize the predictor variables before constructing the models. Let s take a sample of the data and graph it. The term regularizing stands for models ability to structurally prevent overfitting by imposing a penalty on the coefficients. SalePrice vs GrLivAreaAs you can see there are two outliers in the plot above. In order to maximise performance of linear models we can use log transformation. Success It looks like the transformation was successful. Let s find a line with the average of all houses and place it in the scatter plot. But How do we know that Linear regression line is actually performing better than the average line What metrics can we use to answer that How do we know if this line is even the best line best fit line for the dataset If we want to get even more clear on this we may start with answering How do we find the beta_0 intercept and beta_1 coefficient of the equation Finding beta_0 intercept and beta_1 coefficient We can use the following equation to find the beta_0 intercept and beta_1 coefficient hat beta _1 r_ xy frac s_y s_x hat beta _0 bar y hat beta _1 bar x Here. There are three types of Kurtosis Mesokurtic Leptokurtic and Platykurtic. Our target variable SalePrice is not normally distributed. We will transform these variable in the feature engineering section to extract necessary information. Here loss is the sum of squared residuals More on this later. Let s look at the function. This is one of the assumptions of multiple linear regression. To Kaggle community for inspiring me over and over again with all the resources I need. Let s see how mathematicians express this error with the slope equation. sum_ j 1 p beta_j is the absolute sum of the coefficients. This way of model fitting above is probably the simplest way to construct a machine learning model. astype str all_data GarageYrBlt all_data GarageYrBlt. The error plot shows that as GrLivArea value increases the variance also increases which is the characteristics known as Heteroscedasticity. We also assume that the observations are independent of one another No Multicollinearity and a correlation between sequential observations or auto correlation is not there. Ideally if the assumptions are met the residuals will be randomly scattered around the centerline of zero with no apparent pattern. This is what we are trying to estimate solve understand. Which means more houses were sold by less than the average price. Ridge Lasso Elastic Net These regularization methods work by penalizing the magnitude of the coefficients of features and at the same time minimizing the error between the predicted value and actual observed values. Discrete VariablesLet s find the descrete variables i. So we have calculated the beta coefficients. Importing Necessary Libraries and datasetsA Glimpse of the datasets. Mesokurtic is similar to the normal curve with the standard value of 3. The longer the time between the house was built remodeled and sold the lower the sale price. How can we do that Introducing Linear Regression one of the most basic and straightforward models. read_csv Input data files are available in the. This error term accounts for the difference of those points. Homoscedasticity Constant Variance The assumption of Homoscedasticity is crucial to linear regression models. There will be at least a good amount of points where the regression line will not be able to go through for the sake of model specifications linear non linear and bias variance tradeoff more on this later. Let s write the equation for R 2_ adj. As you can see we get a better spread of data once we use log transformation. Sample Train DatasetSample Test DatasetExponential Data Analysis EDA If you want to know more about why we are splitting dataset s into train and test please check out this kernel https www. jpg This kernel is going to solve House Pricing with Advanced Regression Analysis a popular machine learning dataset for beginners. Mean Squared ErrorNow let s get back to our naive prediction and calculate the Mean squared error which is also a metrics similar to RSS helps us determine how well our model is performing. However we will keep them for now for the sake of learning and let the models e. For now let s just say the closer the value of MSE is to 0 the better. This error exists because in real life we will never have a dataset where the regression line crosses exactly every single data point. com This kernel will always be a work in progress. Let s name a few of them. 83 correlation between TotRmsAbvGrd and GrLivArea. Let s apply this to each one of them. If you want to find out more about how to customize charts try this https matplotlib. Therefore we will keep all the features for now. Now we need to introduce a couple of evaluation metrics that will help us compare and contrast models. Here is a picture to make more sense. This is likely because the houses will have an older look and might need repairing. SSE sum_ i 1 n left y_i hat y _i right 2 And the relation between all three of these metrics is SST SSR SSE From the equation above and the R 2 equation from the top we can modify the R 2 equation as the following R 2 1 frac SSE SST More on R 2 R 2 is matric with a value between 0 and 1. Let s refresh our memory and call upon on that equation. If I were using only multiple linear regression I would be deleting these features from the dataset to fit better multiple linear regression algorithms. beta_j is the coefficient of the feature x_j. hat y beta_0 beta_1 x epsilon Let s plug in hat Y equation in the RSS equation and we get. Independence of Errors vs Autocorrelation Multivariate Normality Normality of Errors No or little Multicollinearity. Here y Dependent variable. GoalsThis kernel hopes to accomplish many goals to name a few. There are many evaluation metrics. Modeling the Data Before modeling each algorithm I would like to discuss them for a better understanding. If you like to discuss any other projects or just have a chat about data science topics I ll be more than happy to connect with you on LinkedIn Github Kaggle masumrumi. In other words the amount of unique value for each feature is not so much that we need to tackle them. There are multiple outliers in the variable. io 2 important statistics terms you need to know in data science skewness and kurtosis 388fef94eeaa article. We call this the total variation in the Y s of the Total Sum of the Squares SST. com wp content uploads 2016 12 anat. Check out this https statistics. When lambda_2 is 0 the loss funciton becomes same as simple linear regression. I have used and modified some of the code from this course to help making the learning process intuitive. Categorical variables remove rare labels Categorical variables convert strings to numbers Standarise the values of the variables to the same rangeBefore we began to engineer the features it is important to separate the data into train and test set. r_ xy frac sum x_i bar x y_i bar y sqrt sum x_i bar x 2 sum y_i bar y 2 Let s get on with calculating the rest by coding. In addition to that we can also check the residual plot which tells us how is the error variance across the true line. If you would like to know more about this equation Please check out this video https www. png In this visualization above the light green line is the average line and the black dot is the observed value. It is also known as alpha. However Let s dive deep into some more complex regression. However lets go into more details. astype str all_data YearBuilt all_data YearBuilt. This slope equation gives us an exact linear relationship between X and y. Let use one of the evaluation regression metrics and find out the Mean Squared Error more on this later of this line. SSR sum_ i 1 n left hat y_i bar y right 2 SSE RSS RSS is calculated by squaring each residual of the data points and then adding them together. Let s put both of the model s side by side and compare the errors. It is imperative to check for outliers since linear regression is sensitive to outlier effects. Let s investigate if there are any labels that are present only in a small number of houses. Observations Our target variable shows an unequal level of variance across most predictor independent variables. then X_test is used in that trained algorithms to predict outcomes. Clearly these categorical variables shows promising information. Categorical Variables Converting to String VariablesSuccess It looks like we have successfully converted all the categorical variables and the monotonic relationship is pretty apperant. operatorname R 2 frac SSR SST Here SST Sum of the Total Squared Error is the total residual. We will get rid of some of these outliers in the feature engineering section. Now that we have our predicted y values let s see how the predicted regression line looks in the graph. As the year increases the price of the houses seems to be decreasing which in real time is quite unusual. Ordinary least squared loss function minimizes the residual sum of the square RSS to fit the data text minimize RSS sum_ i 1 n y_i hat y _i 2 sum_ i 1 n left y_i left beta_0 sum_ j 1 p beta_j x_j right right 2 Let s review this equation once again Here y_i is the observed value. ", "id": "masumrumi/a-detailed-regression-guide-with-house-pricing", "size": "47950", "language": "python", "html_url": "https://www.kaggle.com/code/masumrumi/a-detailed-regression-guide-with-house-pricing", "git_url": "https://www.kaggle.com/code/masumrumi/a-detailed-regression-guide-with-house-pricing", "script": "boxcox1p RobustScaler fixing_skewness replace_categories lightgbm customized_scatterplot StackingCVRegressor train_test_split find_frequent_labels overfit_reducer LinearRegression Lasso ElasticNetCV boxcox_normmax analyze_transformed_continuous skew  # for some statistics rmsle Ridge SVR xgboost sklearn.svm numpy cross_val_score sklearn.cluster seaborn make_pipeline stats skew analyze_na_value blend_models_predict sklearn.manifold mlxtend.regressor find_outliers mean_absolute_error analyze_continuous missing_percentage XGBRegressor analyse_year_features analyze_discrete scipy sklearn.linear_model PCA StratifiedKFold RidgeCV KFold LGBMRegressor matplotlib.pyplot analyse_rare_labels plotting_3_chart MinMaxScaler sklearn.model_selection pandas matplotlib.style matplotlib.gridspec datetime scipy.special KMeans cv_rmse missingno sklearn.pipeline ElasticNet mean_squared_error scipy.stats LassoCV elapsed_years TSNE sklearn.metrics sklearn.decomposition GradientBoostingRegressor sklearn.ensemble StandardScaler sklearn.preprocessing ", "entities": "(('more we', 'others'), 's') (('Therefore we', 'a few unique techniques'), 'find') (('error term', 'points'), 'account') (('GoalsThis kernel', 'few'), 'hope') (('recommendations you', 'end'), 'leave') (('we', 'best possible fit'), 'need') (('here it', 'readers'), 'go') (('it', 'predictor variables'), 'be') (('residual', 'predicted line'), 'be') (('you', 'https www'), 'try') (('feedback', 'further improvements'), 'appreciate') (('plot', 'eyebrows'), 'let') (('we', 'log transformation'), 'see') (('difference', 'original value'), 'be') (('us', 'loss least squared function'), 'let') (('some', 'OverallCond'), 'tend') (('you', 'extra time'), 'watch') (('regression where line', 'dataset'), 'exist') (('s', 'regression line'), 'Phew') (('_ 1 beta_0 sum _ n left left j 1 right equation', 'loss also function'), 'p') (('we', 'RSS equation'), 'beta_1') (('I', 'many other tips'), 'describe') (('So symmetrical distribution', '0'), 'have') (('s', 'equation'), 'let') (('No one Multicollinearity', 'sequential observations'), 'assume') (('SSR SST operatorname R 2 Here Sum', 'Total Squared Error'), 'frac') (('We', 'coefficient hat beta_0 intercept beta'), 'know') (('you', 'two plot'), 'SalePrice') (('plot', 'above excellent Homoscedasticity'), 'be') (('how they', 'target variable'), 'show') (('variables', 'target normally variable'), 'distribute') (('when x', 'y'), 'beta_0') (('new we', 'data'), 'use') (('why we', 'kernel https www'), 'EDA') (('where no linearity', 'two cases'), 'depict') (('we', 'enough'), 'SalePrice') (('So we', 'beta coefficients'), 'calculate') (('minimization', 'observed coefficients'), 'become') (('s', 'these'), 'know') (('We', 'parts'), 'describe') (('regression where line', 'more this'), 'be') (('that', 'correlations'), 'let') (('notebook', 'comment'), 'like') (('Kolmogorov Smirnov test', 'dependent variable'), 'check') (('target already variable', 'normal distribution'), 'know') (('data points', 'x_axis'), 'x') (('increase', '2 numerator'), 'penilize') (('when 1 unit', 'x.'), 'show') (('We', 'train'), 'split') (('scatter categorical plot', 'best categorical variables'), 'be') (('I', 'over over resources'), 'to') (('SSR _ 1 bar right SSE RSS n left y 2 RSS', 'then them'), 'sum') (('Create that', 'housing well prices'), 'model') (('readers', 'many charts'), 'use') (('learning process', 'course'), 'use') (('it', 'train set'), 'remove') (('particulary variables', 'linear model'), 'need') (('instead we', 'them'), 'try') (('you', 'video https www'), 'like') (('most', 'above hand'), 'xy') (('I', 'answer'), 'have') (('Elastic Net', 'absolute squared error'), 'add') (('s', 'errors'), 'let') (('I', 'right R'), 'go') (('we', 'train data'), 'use') (('s', 'numerical variables'), 'let') (('how we', 'Ridge'), 'let') (('which', 'target variable'), 'want') (('that', 'model'), 'want') (('that', 'very small amount'), 'consist') (('Lasso Lasso', 'coefficients'), 'add') (('metrics', 'y'), 'be') (('it', 'Multiple Linear Regression'), 'realize') (('target', 'back three charts'), 'let') (('r _ xy frac sqrt bar y bar 2 2 s', 'rest'), 'sum') (('I', 'genuinely it'), 'appreciate') (('we', 'earlier kernel'), 'think') (('term', 'coefficients'), 'stand') (('statistics 2 important you', 'data science kurtosis skewness article'), 'term') (('s', 'housing dataset'), 'let') (('Elastic Net', 'two'), 'be') (('You', 'https'), 'resize') (('ValuesThe Rare presence', 'model result'), 'skew') (('prices', 'so area'), 'seem') (('sum _ p j 1 beta_j', 'absolute coefficients'), 'be') (('you', 'first time'), 'learn') (('s', 'scatter plot'), 'let') (('We', 'more this'), 'fix') (('We', 'feature engineering section'), 'rid') (('loss funciton', 'linear simple regression'), 'become') (('Here we', 'variables two independent GrLivArea'), 'plot') (('I', 'better understanding'), 'like') (('R', '2 0'), 'in') (('We', 'regression line'), 'check') (('data points', 'predicted line'), 'observe') (('Linear We', 'machine learning most basic model'), 'regression') (('target variable', 'normal distribution'), 'let') (('Elastic Net Elastic Net', 'Ridge'), 'be') (('_ hat 2 that', 'left'), 'y_i') (('you', 'https matplotlib'), 'try') (('x_1 beta_0 this', 'linear simple regression'), 'beta_1') (('Mesokurtic', '3'), 'be') (('However Lasso', 'well redundant variables'), 'be') (('values', 'models'), 'overfit') (('s', 'outliers'), 'let') (('log', 'QQ_plot'), 'let') (('sometimes they', 'train'), 'be') (('inverse matrix', 'very high multicollinearity'), 'be') (('error how variance', 'true line'), 'check') (('stack_gen', 'Stack'), 'check') (('s', 'it'), 'let') (('following section', 'mathematical equation'), 'find') (('error perfectly then sum', 'case'), 'be') (('features', 'linear model'), 'be') (('therefore we', '0'), 'have') (('residuals', 'apparent pattern'), 'scatter') (('Many', 'something'), 'learn') (('model even didnot', '2 better model'), 'be') (('Regularization models', 'clean'), 'do') (('it', 'best possible outcome'), 'use') (('much anticipated decrease', 'therefore better model'), 'predict') (('form Linear Linearity Correct functional regression', 'independent variable'), 'need') (('how mathematicians', 'slope equation'), 'let') (('We', 'feature engineering section'), 'do') (('linearity assumption', 'scatter plots'), 'test') (('Numeric Variable TransformationWe', 'continous variables'), 'transform') (('kernel', 'always progress'), 'com') (('kernel', 'regression Classification other kernel'), 'be') (('s', 'different feature'), 'let') (('notebook', 'always progress'), 'be') (('we', 'two models'), 'mean') (('other variables', 'model'), 'lead') (('linear regression', 'outlier effects'), 'be') (('Temporal VariableThere', 'year 4 dataset'), 'be') (('me', 'notebook'), 'let') (('residual plot', 'more funnel'), 'seem') (('regression how predicted line', 'graph'), 'let') (('it', 'often way'), 'frac') (('they', 'things'), 'seem') (('This', 'linear regression more explanation red multiple model'), 'call') (('regression model', 'more regularizing'), 'make') (('values extreme outliers', 'distribution'), 'be') (('Linear Regression', 'most complex models'), 'let') (('kernel', 'beginners'), 'jpg') (('much', 'therefore much better average line'), 'reduce') (('2 Coefficient', 'regression model'), 'r') (('we', 'error'), 'make') (('extreme values', 'mesokurtic distributions'), 'do') (('more it', 'dependent variable'), 'measure') (('errors', 'pure random true line'), 'assume') (('where I', 'data science journey'), 'approach') (('SSR', 'regression squared line'), 'say') (('So SST', 'black dot'), 'describe') (('getting', 'sale median prices'), 'learn') (('One naive way', 'house prices'), 'want') (('how well model', 'also similar RSS'), 'let') (('how it', 'scatter plot'), 'let') (('where I', 'model'), 'skip') (('Predictors', 'very different estimated effects'), 'have') (('we', 'variables'), 'let') (('we', 'it'), 'call') (('html sphx glr', 'link'), 'tutorial') (('they', 'coefficients'), 'be') (('blob', 'doesn relationships'), 'look') (('else who', 'it'), 'be') (('s', 'independent variable GrLivArea'), 'let') (('predictive equation', 'baseline model'), 'indicate') (('_ beta_0 sum _ lambda_1 sum _ p Here 1 n left left j 1 right right 2 j 1 lambda_2', 'Ridge constant function'), 'minimize') (('It', 'target bit variable'), 'look') (('we', 'other errors'), 'Success') (('once again Here observed value', 'equation'), 'square') (('best way', 'Ridge'), 'be') (('rare labelsThis', 'road'), 'anticipate') (('We', 'transformation'), 'use') (('here mean', 'better assumption'), 'use') (('It', 'X'), 'represent') (('which', 'also Homoscedasticity'), 'expect') (('Ridge Ridge regression', 'coefficients'), 'add') (('The longer time', 'sale the lower price'), 'build') (('each', 'i.'), 'y_i') (('I', 'visualization'), 'try') (('that', 'normal distribution'), 'miss') (('SSR ESS SSR', 'predicted value'), 'be') (('models', 'learning'), 'keep') (('that', 'now future'), 'rename') (('OutliersExtreme values', 'linear model'), 'Assumptions') (('However two', 'trends'), 'follow') (('We', 'necessary information'), 'transform') (('we', 'term error'), 'fit') (('post', 'zero lines'), 'have') (('we', 'dataset'), 'be') (('Many', 'i.'), 'be') (('I', 'community'), 'review') (('residual', 'zero'), 'look') (('X_train y_train', 'first algorithm'), 'use') (('How we', 'most basic models'), 'do') (('read_csv Input data files', 'the'), 'be') (('I', 'update'), 'incorporate') (('It', 'models'), 'be') (('so function', 'data'), 'try') (('houses', 'repairing'), 'be') (('_ hat 1 n left right we', 'Evaluation So section'), 'be') (('we', 'log transformation'), 'in') (('only that', 'slope different equation'), '_') (('org math', 'slope intercept form'), 'algebra') (('Leptokurtic Example', 'freedom'), 'be') (('we', 'only two features'), 'use') (('doesn', 'model'), 'decrease') (('average black dot', 'light green line'), 'png') (('Similarly many other features', 'other independent feature'), 'have') (('three charts', 'target variable'), 'tell') (('we', 'equations'), 'x_n') (('we', 'years'), 'let') (('one', 'Total population'), 'be') (('we', 'how much vertical space'), 'give') (('this', 'regression line'), 'say') (('we', 'solve understand'), 'be') (('SST TSS SST', 'line average bar'), 'be') (('way', 'machine learning above probably simplest model'), 'be') (('It', 'python docker image https kaggle github'), 'come') (('you', 'output'), 'list') (('so much we', 'them'), 'be') (('However residual plot', 'points'), 'be') (('one variable', 'variable'), 'be') (('It', 'ValuesSuccess'), 'miss') (('sum _ p j 1 beta_j', '2 squared coefficients'), 'be') (('it', 'relationship'), 'be') (('Numerical variablesLet', 'dataset'), 'find') (('s', 'things'), 'let') (('d this', 'house'), '-PRON-') (('Learn review', 'write ups'), 'explain') (('Clearly categorical variables', 'promising information'), 'show') (('average line', 'GrLivArea SalePrice'), 'represent') (('t', 'sale significant median price'), 'be') (('error term', 'independent variable'), 'describe') (('features', 'us'), 'see') (('most', 'contaion outliers'), 'seem') (('more feature', 'model'), 'add') (('beta_j', 'feature'), 'be') (('tail', 'distribution'), 'mean') (('median', 'similar dataset'), 'be') (('median', 'mode'), 'be') (('multicollinearity', 'still various features'), 'exist') (('It', 'SalePrice'), 's') (('This', 'linear multiple regression'), 'be') (('s', 'life situation SalePriceThese more real case'), 'let') (('we', 'linear SalePrice'), 'discuss') (('target variable', 'most predictor independent variables'), 'show') (('coefficients', 'linear somewhere between simple regression'), 'be') (('These', 'most correlated one OverallQual'), 'be') (('penalty', 'Ridge'), 'add') (('We', 'predictor one independent variable'), 'use') (('that', 'often better average line'), 'look') (('We', 'normally Gaussian distribution'), 'see') (('You', 'houses'), 'tell') (('statistics Kurtosis', 'probability'), 'KurtosisAccording') (('linear regression analysis', 'dependent variable'), 'Normality') (('we', 'transformation'), 'let') (('then X_test', 'outcomes'), 'use') (('penalty', 'above this'), 'add') (('We', 'feature engineering selection'), 'take') (('We', 'Feature engineering section'), 'need') (('which', 'real time'), 'seem') (('I', 'target variable'), 'want') (('Platykurtic Platykurtic', 'normal distribution'), 'describe') (('we', 'challenge'), 'use') (('you', 'Homoscedasticity'), 'be') (('we', 'errors'), 'be') (('assumptions', 'linear regression multiple model'), 'before') (('just the closer value', '0'), 'let') (('We', 'y predicted value'), 'plug') (('we', 'remaining data'), 'train') (('we', 'R'), 'use') (('However s', 'deep more complex regression'), 'let') (('It', 'regression line'), 'know') (('how I', 'machine learning models'), 'go') (('Fixing', 'Dummy Variables'), 'feature') (('we', 'Heteroscedasticity'), 'call') (('code', '4 parts'), 'split') (('that', 'us'), 'need') (('Categorical VariablesThere', 'less features'), 'be') (('where value', 'observations'), 'miss') (('Therefore we', 'features'), 'keep') (('more houses', 'average price'), 'mean') (('kernel', 'regression algorithms'), 'go') (('which', 'X'), 'be') (('Homoscedasticity Constant assumption', 'regression models'), 'variance') (('value', '2 1'), 'mean') (('s', 'residual plots'), 'let') (('also which', 'Heteroscedasticity'), 'show') (('extreme values', 'normal distribution'), 'mean') (('We', 'Squares SST'), 'call') (('One way', 'log box transformation transformation'), 'be') (('pretty self you', 'kernel'), 'minimize') (('Linear regression', 'little similar features'), 'require') (('slope y mx simply where beta_0', 'sum _ left beta_0 j'), 'be') (('I', 'LinkedIn Github Kaggle masumrumi'), 'be') (('I', 'linear regression better multiple algorithms'), 'delete') (('Discrete VariablesLet s', 'descrete variables'), 'find') (('successfully categorical variables', 'String Categorical VariablesSuccess'), 'Variables') (('that', 'attention'), 'be') (('it', 'outliers fewer e.'), 'have') (('Here loss', 'More this'), 'be') (('Skewness', 'bell symmetrical curve'), 'be') (('We', 'feature engineering section'), 'get') (('sample', 'STD two s.'), 'mean') (('price', 'overall quality'), 'increase') (('scaler', 'train all_data OverallCond all_data OverallCond'), 'generate') (('that', 'houses'), 'let') (('residual variance', 'predictor variable increases'), 'be') (('Ridge Lasso Elastic regularization methods', 'predicted value'), 'net') (('Multicollinearity', 'when strong independent variables'), 'be') (('SSE More', '0'), 'leave') (('slope equation', 'X'), 'give') (('we', 'competition'), 'dump') (('com statistical', 'spread standard deviation'), 'guide') (('we', 'them'), 'square') (('lambda_2 parameter', 'coefficients'), 'infty') (('It', 'ordinary least square function'), 'add') (('Continous VariablesLet s', 'continous variables'), 'find') (('We', 'equation'), 'have') (('other variables', 'variable'), 'interpret') (('algorithms', 'test_y'), 'determine') (('we', 'assumptions'), 's') "}