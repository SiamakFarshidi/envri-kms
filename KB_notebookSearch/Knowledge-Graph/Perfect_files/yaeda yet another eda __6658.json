{"name": "yaeda yet another eda ", "full_name": " h2 Overview h2 Packages h3 To do h2 To be continued ", "stargazers_count": 0, "forks_count": 0, "description": "There also doesn t seem to be any missing values in the test set. That looks pretty sparse to me but let s see how much variation is there between different columns. 0 for 75 another indication that we are probably dealing with sparse data. Wow not very diverse at all Most of the values are heavily concentrated around 0. This will probably be another major concern when it comes to feature selection engineering. Now let s take a closer look at the shape and content of the train data. This is IMHO overall a good thing although a lot of times there is some signal in the missing values that s valuable and worth exploring. Unfortunately there was no way to tell how this model would perform on the unseen data. A few things immediately stand out 1. Sanity check is always a good thing and at leas at this level hte Kaggle people did not mess things up. Only marginal improvement there is a verly small bump close to 15. These will need to be eliminated. Now let s plot it to see how diverse the numerical values are. This is going to be a very very interesting competition. According to Pandas there are no int values in the test set. We ll start with a simple LighGBM regression and see if that yields any results. Target variable ranges over 4 orders of magnitude. The standard deviation for most features seems larger than the feature mean. Now let s take a look at the test dataset. Here we see that the number of features in the test set 4992 matches the number in the train set. This submission scored 1. There are a LOT of features Almost 5000 And they outnumber the number of rows in the training set 4. This is probably one of the main reasons why the metric that we are trying to optimize for this competition is RMSLE root mean square logarithmic error. We ll take a look at naturally the target variable. As expected this distribution looks much more ahem normal. First let s set our target variable to be the log of 1 target. Now I m really curious about those. Let s see how would that look on a plot. Features seem sparse. We ll have to investigate this further. Now let s do some plotting. This is yet another byproduct of having a very small dataset. Now let s import and take a glimpse at these files. Again we see that these distributions look similar but they are definitely not the same. Maybe if we used the log plot things would be better. In fact there are fewer rows than columns which means we ll have to invest a lot of effort into feature selection feature engineering. Most features seem to have similarly wide spread of values as the target variable. So as we suspected almost 97 of all values in the train dataframe are zeros. We see a similar distribution of various statistical aggregates but by no means the same seems like there soem substantial distribution shifts between the train and test sets. Let s take a look at the statistics of the Log 1 target We see that the statistical properties of teh Log 1 Target distribution are much more amenable. Here we find all the relevant files for this competition. However as mentioned earlier most of these columns seem to be filled predominatly with zeros. Now let s take a look at columns with constant value. Most features have 0. First let s make a histogram of its raw value. Do feature importance analysis To be continued. This is a highly skewed distribution so let s try to re plot it with with log transform of the target. There are a few features such as d5308d8bc c330f1a67 that seem to be filled with zeros. We ll have to investigate this later and possibly do some reverse engineering. It seems that for this competition we don t have to do any complicated combination and mergers of files. So it seems that the vast majority of columns have 95 percent of zeros in them. Now let s see some basic descriptive statistics for the train and test dataframes. We also see that the number of rows in the test set far surpasses the number of rows in the train set. csv and sample_submission. Another way of looking at the same distribution is with the help of violinplot. We want to get a better numerical grasp of the true extent of zeros. This section will keep growing in subsequent versions of this EDA. And it looks like a fairly nice distribution albeit still fairly asymetrical. There doesn t appear to be any missing values in the train set. For most problems it would be useful to take a look at the description of these columns but in this competition they are anonymized and thus would not yield any useful information. So this is interesting there are 256 constant columns in the train set but none in the test set. Yes a very interesting competition indeed. We see that the input folder only contains three files train. 53 on Public Leaderboard. So let s get started PackagesFirst let s load a few useful Python packages. The memory size of the train dataset is fairly large 170 MB which is to be expected. These constant columns are thus most likely an artifact of the way that the train and test sets were constructed and not necessarily irrelevant in their own right. OverviewThe purpose of this kernel is to take a look at the data come up with some insights and attempt to create a predictive model or two. A few things to notice 1. Well that s great we made a prediction on the test set and saved it to a file which we were able to submit to the competition. Can the violin plot help Not really the plot looks nicer but the overall shape is pretty much the same. As advertised features are numeric and anonymized. Let s do the same thing with the test data. Let s try to get a better sense of this data. OK that s much more interesting. OK let s try to do some modeling. If we treat all the train matrix values as if they belonged to a single row vector we see a huge amount of varience far exceeding the similar variance for the target variable. OK let s take a look at the distribution of non zero values. It is possible that some of those int features are one hot encoded or label encoded categorical variables. Now let s look at the test set. So we have the total of 4735 columns to work with. There are less than 5000 training rows. Pandas is treating 1845 features as float and 3147 as integer. Now let us look at the input folder. So let s subset the colums that we d use to just those that are not constant. ", "id": "tunguz/yaeda-yet-another-eda", "size": "6658", "language": "python", "html_url": "https://www.kaggle.com/code/tunguz/yaeda-yet-another-eda", "git_url": "https://www.kaggle.com/code/tunguz/yaeda-yet-another-eda", "script": "seaborn lightgbm mean_squared_error xgboost scipy.stats KFold matplotlib.pyplot sklearn.model_selection pandas sklearn.metrics describe numpy ", "entities": "(('that', 'missing values'), 'be') (('s', 'data'), 'let') (('First s', '1 target'), 'let') (('We', 'train sets'), 'see') (('input folder', 'files only three train'), 'see') (('feature', 'most features'), 'seem') (('that', 'zeros'), 'be') (('how numerical values', 'it'), 'let') (('highly skewed so s', 'target'), 'let') (('We', 'later possibly reverse engineering'), 'have') (('we', 'feature selection feature engineering'), 'be') (('Now s', 'train dataframes'), 'let') (('way', 'violinplot'), 'be') (('s', 'test data'), 'let') (('This', 'yet very small dataset'), 'be') (('t', 'test missing set'), 'seem') (('Now s', 'train data'), 'let') (('section', 'EDA'), 'keep') (('that', 'results'), 'start') (('that train sets', 'necessarily own right'), 'be') (('we', 'competition'), 's') (('vast majority', 'them'), 'seem') (('s', 'Python a few useful packages'), 'let') (('We', 'zeros'), 'want') (('they', 'thus useful information'), 'be') (('We', 'target naturally variable'), 'take') (('First s', 'raw value'), 'let') (('So we', '4735 columns'), 'have') (('we', 'probably sparse data'), '0') (('Now s', 'files'), 'let') (('it', 'fairly nice distribution'), 'look') (('fairly large 170 which', 'train dataset'), 'be') (('So this', 'test set'), 'be') (('number', 'train set'), 'see') (('almost 97', 'train dataframe'), 'be') (('probably major when it', 'feature selection engineering'), 'be') (('how much variation', 'there different columns'), 'look') (('purpose', 'predictive model'), 'be') (('statistical properties', 'Target 1 distribution'), 'let') (('However earlier most', 'predominatly zeros'), 'seem') (('target', 'values'), 'seem') (('Target variable', 'magnitude'), 'range') (('we', 'files'), 'seem') (('Pandas', 'integer'), 'treat') (('some', 'one hot categorical variables'), 'be') (('Unfortunately how model', 'unseen data'), 'be') (('also number', 'train set'), 'see') (('s', 'non zero values'), 'let') (('Almost they', 'training'), 'be') (('Now s', 'test dataset'), 'let') (('that', 'just those'), 'let') (('we', 'target variable'), 'see') (('Now s', 'constant value'), 'let') (('root', 'square logarithmic error'), 'mean') (('Here we', 'competition'), 'find') (('things', 'log plot'), 'be') (('doesn There t', 'train missing set'), 'appear') (('hte Kaggle people', 'things'), 'be') "}