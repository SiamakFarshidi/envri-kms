{"name": "red wine quality classification ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "In the data set there 1599 different wine as row data and 12 features as columns. Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Of course brand and price feature was evaluated on this research therefore it is not a good analysis for saying it is good wine. Due to privacy and logistic issues only physicochemical inputs and sensory the output variables are available e. During this research four important machine learning techniques was used Logistic regression Support Vector Machine Decision Tree Random ForestFrom all algorithms it was obvious that for this dataset SVM and then Random Forest algorithm gave the best model and accuracy means that those algorithms predict correctly test data. The cost function is used to understand how model split and predict the split dataset classifications. Random Forest Ordinal Dataset Analyze grid_search cross validation feature selection for noise try with taking important features then look what happens. In other words separating dataset into labeled training and test categorize dataset with labeled data it could be better with work with as binary classification. The table above is a summary of some statistical measures for each numeric predictor of the dataset count indicates the number of records for each attribute that corresponds to the number of wines. For large values of C the optimization will choose a smaller margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. For this reason it is more convenient to start with making data balanced. In order to separate classes it can be drawn many different lines but by choosing best line it is considered that margin should be maximum in between support vectors which are the closest points with different classes. The standard deviations or variance are the minimum and maximum values of variables. Constructing a decision tree is all about finding attribute that returns the highest standard deviation reduction. The outcome is measured with a dichotomous variable in which there are only two possible outcomes. It gives results using data sets that show a loss or unbalanced distribution. Decision Tree Tree models where the target variable can take a discrete set of values are called classification trees in these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels. For that reason correlation map helps to understand these relations in a single representation. 0 means There is no relationship between dependent and independent variables. The C parameter tells the SVM optimization how many misclassifying point of each training example. Volitile_acidicity has a strong negative relationship between citric_acid Volitile_acidicity has weak negative relationship between fixed_acidicity and sulphates Volitile_acidicity has almost no relationship between residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density Density has positive relationship between fixed_acidicity Density has negative relationship between density Density has almost no relationship between volitile_acidicity free_sulfur_dioxide total_sulfur_dioxide Citric_acid has positive relationship between fixed_acidicity Citric_acid has negative relationship between volitile_acidicity pH Citric_acid has almost no relationship between residual_sugar free_sulfur_dioxide total_sulfur_dioxideIt seems very hard to analyze dataset like that therefore it is better to go deep down analyzing with some other visualizations. Formulation of the logistic regression P 1 1 e b_0 b_1 x 2. mean indicates the average value around which each group of attributes is attested. Lastly the best estimator will be decided according to cross validation score and conculusion. Classificaiton and Cross Validation 1. For more information read Cortez et al. The measures of central tendency and variability or distribution are some commonly used measures to define the data set. Correlation map is made by calculating the covariance of each features with respect to others then each covariance value is divided by standard deviation of each variables and get results between 1 0 1. The end of the branch that doesn t split anymore it is the decision leaf tree means that they are the last classification nodes qualities. After all work it is obvious that working with binary classification is more better the predict good or bad wines. corr 0 Alcohol has a positive relationship between quality and weakly pH Alcohol has a negative relationship between density Alcohol has almost no relationship between fixed_acidicity residual_sugar free_sulfur_dioxide sulphates Volitile_acidicity has a weak positive relationship between pH. After each standard deviation calculations standard deviation reduction is used to classify dataset. If someone wants to analyze similar data like that it is better to work SVM or Random Forest. Logistic regression is analyzing a dataset in which there are one or more independent variables that determine an outcome. 1 means There is a positive relationship between dependent and independent variables. In this procedure all the features are considered and different split points are tried and tested using a cost function. Introduction Data Introduction Data upload and some enhancements Data visualization Classification and Cross Validation Results 1. For very tiny values of C there are misclassified examples often even if training data is linearly separable. As they use a collection of results to make a final decision. Then I can modify quality column values with by value column values 3 Categorical Analysis Apply all previous classification algorithms Conclusion For this work it was aimed that the analyzing which psychochemical are more related with wine quality and which approach is good for prediction of wine quality better. max and min indicate the attribute that I assume the highest and lowest value for each attribute group. Data is trained with those class labels and then it is predicted with test data set then calculate accuracy how the algorithm predicts test data correctly. Then some classification and one regression technique will be applied to the data and with cross validation accuracy will be evaluated. the purpose to get good accuracy classification. Conversely a very small value of C will cause the optimizer to look for a larger margin separating hyperplane even if that hyperplane misclassifies more points. The standard deviation reduction is based on the decrease in standard deviation after a dataset is split on an attribute. read_csv good visualizing correlation map view How many wine quality number is realted with how many unique wines Check the outliers for each feature with respect to output value Check the outliers for each feature with respect to output value Fucntion Part Normalization x_norm x mean std it gives for each value the same value intervals means between 0 1 Train and Test splitting of data Logistic Regression We automatically get the logistic regression with the best parameters. The standard deviation is used to calculate the homogeneity of a numerical sample. SVM SVM Support Vector Machine is a discriminative classifier by a separating hyperplane and supervised learning technique for Machine Learning. For this data set firstly using ordinal logistic regression then it is better to using binary logistic regression with modified dataset. It is used to predict a binary outcome 1 0 Yes No True False given a set of independent variables. Hence those algorithms variances are found better with high margin terminology therefore with multiclass analysis those algorithms will give the best accuracy. Additionally SVM is useful for non linear classification by increasing dimension then find a new line classification in increased dimension. After the analysis of this dataset some features have more effect to deciding quality of the wine there are some insights about the criteria about wine quality you can compare just looking the some psychochemical on the label of wines Should be higher Alcohol is the most important feature to decide quality of the wine. Furthermore there is no nun value to deal with it and all values are numeric means that input values are float and only output value is integer. Apply cross validation to estimate the skills of models with 10 split with using best parameters Predict the test data as selected classifier print confusion matrix and accuracy score before best parameters Now seperate the dataset as response variable and feature variabes Train and Test splitting of data Plot the decision tree Add a new feature according to mean of the quality Good wine represented by 1 bad wine represented by 0 Check the outliers for each feature with respect to output value Categorical distribution plots Now seperate the dataset as response variable and feature variabes Train and Test splitting of data Plot the decision tree Add a new feature according to mean of the quality Good wine represented by 2 average 1 and bad wine represented by 0 Check the outliers for each feature with respect to output value Categorical distribution plots Now seperate the dataset as response variable and feature variabes Train and Test splitting of data Plot the decision tree. If the alcohol percentage is high enough it means that quality of the wine should be better Sulphates is another selecting criteria for good wines with high percentage sulphates wine quality is increasing Citric Acid is another selecting criteria it should be higher to decide more better wine Should be lower Volatile Acidity should be less in the good wine Sulfur dioxide is another effect to decreasing wine quality and also it causes head ache therefore ifthere is less sulfur dioxide in wine it should be selected Chlorides value has very less effect to quality of the wine but again it is obvious more value of itcauses bad quality of the wineAdditionally for marketing point of view if a customer wants to buy a wine just looking with some psychochemical values can decide what s he needs to buy. A decision tree is drawn upside down with its root at the top involves partitioning the data into subsets that contain instances with similar values homogenous then on the middle there are condition internal node based on the tree split into branches edges. Logistic Regression 2. Data visualization This data set has many different features and it is important to understand relationship between these in order to analyze dataset better. Predictions from all trees are pooled to make the final prediction the mode of the classes for classification or the mean prediction for regression. The base algorithm of the decision tree recursive binary splitting. The split with the best cost or lowest cost is selected. According to these information it can be made a good analyze about dataset and columns. Random forest algorithm contains many variables and many categorical variables with a large number of class labels. Content of the paper is starting with introduction of the data set and some data visualization. Red Wine Quality Classification In this analysis of the work it will be determined which physiochemical properties make red wine good by using some machine learning techniques. std indicates the standard deviation of each attribute group from which we can guess the degree of data dispersion around the average. Support Vector Classifier SVC best estimator DecisionTree Classifier tree best estimator FEATURE IMPORTANCE FOR DECISION TREE forest best estimator FEATURE IMPORTANCE FOR DECISION TREE Create applying classification funciton Find the best parameters and get the classification with the best parameters as return valu of grid search Plotting the learning curve score curves each time with 30 data randomly selected as a validation set. Now my data seems with the value dimension as balanced because the count of those two classes seems very equal with each other Good wine dataset equals to 855 and bad wine dataset equals to 744. According to figure in above Quality has a positive relationship between alcohol Quality has a negative weak relationship between volitile_acidicity Quality has almost no relationship between residual_sugar free_sulfur_dioxide and pH. Logistic Regression The logistic regression is a predictive analysis of statistical method. Bootstraping the sample data creating some mini sample dataset with less variable then calcuating with regression to the gini then pick the highly correlated therefore first 2 3 split will be the same because of the central limit theorem because the variance of the sum is decrreasing Binary dataset Analyze This dataset can be proper for classification and regression techniques because the data is multivariate contains numeric values as an input and output value is ordered and not balanced means that there are much more normal wines than excellent or poor ones. In more detail SVM uses margin and hyperplane instead of line to separate data into two or more different class. While SVM algorithm is working it follows two rules which are firstly classify correctly then increase the margins in hyperplane. The measures used to define the central tendency are mean median and mode. Differently from the unsupervised learning algorithms there is a dataset belongs to different classes labels. As it seems from the figure in above data is not balanced and the ranges are between 3 8 score. there is no data about grape types wine brand wine selling price etc. Random Forest Best Classifier according to classfication accuraies Conculusion Data Introduction In this dataset there are specifically red wine variants of Poteguese Vinho Verde wine. 1 means There is a negative relationship between dependent and independent variables. When new trees are added into the random forest algorithm updates itself with decreasing the loss by eliminating noises. Random Forest Random forest baging pf the decision treeRandom forests construct many individual decision trees at training and it uses the simplicity of decision trees with flexibility resulting in improvement the accuracy. However it can give some idea for the people who do not have more knowledge about wine for selecting the good wine maybe for just dinner or gift for friends linear algebra data processing CSV file I O e. ", "id": "sevilcoskun/red-wine-quality-classification", "size": "13003", "language": "python", "html_url": "https://www.kaggle.com/code/sevilcoskun/red-wine-quality-classification", "git_url": "https://www.kaggle.com/code/sevilcoskun/red-wine-quality-classification", "script": "train_test_split IPython.display confusion_matrix grid_search sklearn.svm cross_val_score Image numpy seaborn SVC tree train_test sklearn.tree sklearn.linear_model learning_curve sklearn normalization matplotlib.pyplot sklearn.externals.six plot_learning_curve DecisionTreeClassifier StringIO sklearn.model_selection pandas RandomForestClassifier LogisticRegression export_graphviz ShuffleSplit GridSearchCV sklearn.metrics sklearn.ensemble apply_classification ", "entities": "(('measures', 'median'), 'mean') (('algorithms', 'test correctly data'), 'use') (('therefore it', 'deep down other visualizations'), 'have') (('covariance then value', '1'), 'make') (('feature splitting', 'decision tree'), 'cross') (('1', 'positive dependent variables'), 'mean') (('True 1 0 False', 'independent variables'), 'use') (('approach', 'wine quality'), 'modify') (('who', 'algebra data CSV file'), 'give') (('Logistic regression', 'classes'), 'be') (('ranges', '8 score'), 'balanced') (('how model', 'dataset split classifications'), 'use') (('training misclassified often even data', 'C'), 'be') (('1', 'negative dependent variables'), 'mean') (('C parameter', 'training example'), 'tell') (('working', 'binary classification'), 'be') (('more data', 'reason'), 'be') (('split', 'best cost cost'), 'select') (('cross validation accuracy', 'regression one data'), 'apply') (('Content', 'data set'), 'start') (('0', 'dependent variables'), 'mean') (('red wine', 'machine learning techniques'), 'Classification') (('group', 'attributes'), 'indicate') (('good it', 'research'), 'evaluate') (('then it', 'modified dataset'), 'be') (('deviation deviation standard calculations standard reduction', 'dataset'), 'use') (('he', 'what'), 'mean') (('it', 'accuracy'), 'forest') (('different split points', 'cost function'), 'consider') (('almost no relationship', 'pH.'), 'have') (('Additionally SVM', 'increased dimension'), 'be') (('that', 'branches edges'), 'be') (('it', 'SVM'), 'want') (('output value', 'much more normal excellent ones'), 'be') (('dataset', 'attribute'), 'base') (('it', 'binary classification'), 'be') (('We', 'best parameters'), 'view') (('algorithms', 'best accuracy'), 'find') (('they', 'that'), 'end') (('how algorithm', 'test data'), 'train') (('that', 'loss'), 'give') (('which', 'hyperplane'), 'follow') (('SVM', 'two different class'), 'use') (('that', 'class labels'), 'call') (('outcome', 'which'), 'measure') (('Lastly best estimator', 'validation cross score'), 'decide') (('I', 'attribute group'), 'indicate') (('we', 'average'), 'indicate') (('hyperplane', 'training points'), 'choose') (('then what', 'important features'), 'cross') (('you', 'wine'), 'be') (('one independent that', 'outcome'), 'analyze') (('they', 'final decision'), 'use') (('only physicochemical inputs', 'output variables'), 'be') (('count', '744'), 'seem') (('best estimator', 'validation randomly set'), 'SVC') (('Introduction Data Introduction Data some', 'visualization Data Classification'), 'upload') (('correlation map', 'single representation'), 'help') (('standard deviation', 'numerical sample'), 'use') (('hyperplane even misclassifies', 'hyperplane'), 'cause') (('it', 'dataset'), 'visualization') (('forest Random algorithm', 'class labels'), 'contain') (('which', 'closest different classes'), 'draw') (('negative weak relationship', 'residual_sugar free_sulfur_dioxide'), 'have') (('measures', 'data commonly used set'), 'be') (('final prediction', 'mean regression'), 'pool') (('that', 'wines'), 'be') (('that', 'deviation highest standard reduction'), 'be') (('SVM SVM Support Vector Machine', 'Machine Learning'), 'be') (('standard deviations', 'minimum variables'), 'be') (('Logistic logistic regression', 'predictive statistical method'), 'Regression') (('When new trees', 'noises'), 'update') (('it', 'good dataset'), 'make') (('output only value', 'Furthermore nun it'), 'be') "}