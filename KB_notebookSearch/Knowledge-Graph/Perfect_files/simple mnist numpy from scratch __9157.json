{"name": "simple mnist numpy from scratch ", "full_name": " h1 Summary h4 References h2 Data input h2 Some examples of the input data h1 Network structures h2 Softmax activation prediction and the loss function h1 Backpropagation Chain rule h2 Hyper parameters and network initialization h1 Gradient Descent training of the network h3 Remark h1 Predictions for testing data ", "stargazers_count": 0, "forks_count": 0, "description": "edu tutorial supervised MultiLayerNeuralNetworks. Softmax activation prediction and the loss functionFrom the hidden layer Layer 2 to the output layer layer 3 the weight matrix W 1 is of shape 256 10 the form of which is as follows W 1 begin pmatrix boldsymbol theta _1 boldsymbol theta _2 cdots boldsymbol theta _K end pmatrix which maps the activation from Layer 2 to Layer 3 output layer and there is no bias because a constant can be freely added to the activation without changing the final output. tag 1 Denote the data sample matrix X mathbf x 1 dots mathbf x N top its label vector as mathbf y y 1 dots y N and then the final loss has an extra L 2 regularization term for the weight matrices not for bias L W b X mathbf y frac 1 N sum_ i 1 N J_i frac alpha 2 Big W 0 2 W 1 2 Big tag 2 where alpha 0 is a hyper parameter determining the strength of the regularization the bigger the alpha is the smaller the magnitudes of the weights will be after training. org tutorials beginner pytorch_with_examples. The code is vectorized and is adapted from the Softmax regression and Neural Network lectures used in UCI Math 10 https github. Step 2 Derivatives for W 1 recall that W 1 boldsymbol theta _1 cdots boldsymbol theta _K and denote mathbf z 2 big z 2 _1 dots z 2 _K big W 1 top mathbf a big boldsymbol theta top _1 mathbf a cdots boldsymbol theta top _K mathbf a big for the k th output unit in the output layer Layer 3 then delta 2 _k frac partial J partial z_k 2 Big P big y k mathbf a W 1 big 1_ y k Big sigma_k 1_ y k and frac partial J partial boldsymbol theta _k frac partial J partial z_k 2 frac partial z_k 2 partial boldsymbol theta _k delta 2 _k mathbf a. Then mathbf a is the activation from the hidden layer Layer 2 can be written as mathbf a mathrm ReLU big W 0 top mathbf x mathbf b big where the ReLU activation function is mathrm ReLU z max z 0 and can be implemented in a vectorized fashion as follows. Hyper parameters and network initialization Gradient Descent training of the networkIn the training we use a GD variant of the RMSprop for mathbf w which stands for the parameter vector in our model Choose mathbf w _0 eta gamma epsilon and let g_ 1 1 For k 0 1 2 cdots M nbsp nbsp nbsp nbsp g_ k gamma g_ k 1 1 gamma left partial_ mathbf w L mathbf w _k right 2 nbsp nbsp nbsp nbsp displaystyle mathbf w _ k 1 mathbf w _k frac eta sqrt g_ k epsilon partial_ mathbf w L mathbf w _k Remark The training takes a while since we use the gradient descent for all samples. This is a simple demonstration mainly for pedagogical purposes which shows the basic workflow of a machine learning algorithm using a simple feedforward neural network. tag ast Denote the label of the i th input as y i and then the sample wise loss function is the cross entropy measuring the difference of the distribution of this model function above with the true one 1_ y i k denote W W 0 W 1 b mathbf b let mathbf a i be the activation for the i th sample in the hidden layer Layer 2 J_i J W b mathbf x i y i sum_ k 1 K left 1_ left y i k right log P big y i k mathbf a i W 1 big right. At the last layer a softmax activation is used which can be written as follows combining the weights matrix W 1 that maps the activation mathbf a from the hidden layer to output layer P big y k mathbf a W 1 big sigma_k mathbf a W 1 frac exp big boldsymbol theta top _k mathbf a big sum_ j 1 K exp big boldsymbol theta top _j mathbf a big. The training is through a standard gradient descent with step size adaptively changing by Root Mean Square prop RMSprop and there is no cross validation set reserved nor model averaging for simplicity. Caveat lector fluency in linear algebra and multivariate calculus. P big y k mathbf a W 1 big _ k 1 K is the probability distribution of our model which estimates the probability of the input mathbf x s label y is of class k. The model is a 3 layer feedforward neural network FNN in which the input layer has 784 units and the 256 unit hidden layer is activated by ReLU while the output layer is activated by softmax function to produce a discrete probability distribution for each input. Lastly our prediction hat y for sample mathbf x can be made by choosing the class with the highest probability hat y operatorname argmax _ k 1 dots K P big y k mathbf a W 1 big. read_csv Read the data Set up the data hide the axes ticks relu activation function THE fastest vectorized implementation for ReLU layer 1 input layer layer 1 input layer layer 2 hidden layer add one more layer layer 2 activation layer 2 hidden layer layer 3 output layer the output is a probability for each sample loss_sample stores the cross entropy for each sample in X convert y_true from labels to one hot vector encoding loss_sample is a dimension N array for the final loss we need take the average layer 1 input layer layer 1 input layer layer 2 hidden layer layer 2 activation one more layer layer 2 hidden layer layer 3 output layer layer 2 layer 3 weights derivative delta2 is partial L partial z2 of shape N K layer 1 layer 2 weights derivative delta1 is partial a2 partial z1 layer 2 activation s weak derivative is 1 z1 0 Student project extra layer of derivative no derivative for layer 1 the alpha part is the derivative for the regularization regularization 0. An overview of gradient descent optimization algorithms http ruder. sum W 0 2 dW 0 is W 0 s derivative and dW 1 is W 1 s derivative similar for db regularization RMSprop RMSprop number of iterations of gradient descent number of neurons in the hidden layer number of pixels in an image initialization sanity check 1 sanity check 2 sanity check 3 reset RMSprop predictions Generating submission using pandas for grading. SummaryThis note is an MNIST digit recognizer implemented in numpy from scratch. We denote this distribution by a vector boldsymbol sigma sigma_1 dots sigma_K top. Predictions for testing dataThe prediction labels are generated by ast. References Stanford Deep Learning tutorial in MATLAB http ufldl. Thus frac partial J partial w_ ji x_j delta_i 1 frac partial J partial b_ i delta_i 1 text and frac partial J partial mathbf w _ i delta_i 1 mathbf x frac partial J partial mathbf b boldsymbol delta 1. linear algebra data processing CSV file I O e. io optimizing gradient descent. The derivative at the backpropagation stage is computed explicitly through the chain rule. Learning PyTorch with examples https pytorch. The middle layer Layer 2 is the hidden layer. The neural network in the figure above has 2 input units not counting the bias unit 3 hidden units and 1 output unit. com scaomath UCI Math10. Layer 1 is the input layer and Layer 3 is the output layer. Network structuresThe figure above is a simplication of the neural network used in this example. In this actual computation below the input layer has 784 units the hidden layer has 256 units and the output layers has 10 units K 10 classes. Step 3 Derivatives for W 0 mathbf b recall that W 0 boldsymbol w _1 cdots boldsymbol w _ n_2 mathbf b b_1 dots b_ n_2 where n_2 is the number of units in the hidden layer Layer 2 and denote mathbf z 1 z_1 1 dots z_ n_2 1 W 0 top mathbf x mathbf b big mathbf w top _1 mathbf x b_1 cdots mathbf w top _ n_2 mathbf x b_ n_2 big for each node i in the hidden layer Layer 2 i 1 dots n_2 then delta 1 _i frac partial J partial z 1 _i frac partial J partial a_i frac partial a_i partial z 1 _i frac partial J partial mathbf z 2 cdot left frac partial mathbf z 2 partial a_i frac partial a_i partial z 1 _i right left sum_ k 1 K frac partial J partial z 2 _k frac partial z 2 _k partial a_i right f z 1 _i left sum_ k 1 K w_ ki delta 2 _k right 1_ z 1 _i 0 where 1_ z 1 _i 0 is ReLU activation f s weak derivative and the partial derivative of the k th component before activated by the softmax in the output layer z 2 _k with respect to the i th activation a_i from the hidden layer is the weight w 1 _ ki. We hope that this estimate is as close as possible to the true probability 1_ y k that is 1 if the sample mathbf x is in the k th class and 0 otherwise. Backpropagation Chain rule The derivative of the cross entropy J in 1 for a single sample and its label mathbf x y with respect to the weights and the bias is computed using the following procedure Step 1 Forward pass computing the activations mathbf a a_1 dots a_ n_2 from the hidden layer Layer 2 and boldsymbol sigma sigma_1 dots sigma_K from the output layer Layer 3. Data input Some examples of the input dataWe randomly choose 10 samples from the training set and visualize it. The loss function model hypothesis function and the gradient of the loss function are all implemented from ground up in numpy in a highly vectorized fashion no FOR loops. The weight matrix W 0 mapping input mathbf x from the input layer Layer 1 to the hidden layer Layer 2 is of shape 784 256 together with a 256 bias. The circles labeled 1 are the bias units. ", "id": "scaomath/simple-mnist-numpy-from-scratch", "size": "9157", "language": "python", "html_url": "https://www.kaggle.com/code/scaomath/simple-mnist-numpy-from-scratch", "git_url": "https://www.kaggle.com/code/scaomath/simple-mnist-numpy-from-scratch", "script": "loss softmax matplotlib.pyplot backprop relu pandas numpy ", "entities": "(('alpha 1 part', 'regularization regularization'), 'Read') (('which', 'feedforward neural simple network'), 'be') (('derivative', 'chain explicitly rule'), 'compute') (('log P k', 'i W 1 big right'), 'ast') (('output layer', 'input'), 'be') (('constant', 'final output'), 'layer') (('the magnitudes', 'training'), 'tag') (('sample mathbf', 'k class'), 'hope') (('output layers', '10 units K 10 classes'), 'have') (('circles', '1'), 'be') (('neural network', 'bias unit'), 'have') (('training', 'cross simplicity'), 'be') (('We', 'vector boldsymbol sigma sigma_1 dots'), 'denote') (('examples', 'it'), 'input') (('loss function model hypothesis function', 'loops'), 'implement') (('probability', 'label class k.'), 'mathbf') (('Predictions', 'ast'), 'generate') (('code', 'Neural Network https UCI Math 10 github'), 'vectorize') (('k', '_ y boldsymbol k Big 1 partial J partial theta'), 'derivative') (('_ 2 k', 'hidden layer'), 'recall') (('we', 'samples'), 'epsilon') (('overview', 'ruder'), 'http') (('SummaryThis note', 'scratch'), 'be') (('hat y operatorname k y _ 1 K big k', 'W 1 big'), 'lastly') (('References Stanford Deep Learning tutorial', 'ufldl'), 'http') (('Network structuresThe figure', 'example'), 'be') (('x mathbf ReLU activation mathrm big W 0 top big where function', 'mathrm ReLU z z 0 vectorized fashion'), 'mathbf') (('W 0 2 0 W 0 s dW', 'grading'), 'be') (('computing', 'boldsymbol sigma sigma_1 output Layer 2 layer'), 'rule') (('_ top j', 'big'), 'use') "}