{"name": "tps nov 21 extracting the power of naive bayes ", "full_name": " h1 Understand the Models You Love h1 About Naive Bayes h1 Importing Packages and Sample Data h1 Splitting Data h1 Making Pipelines with Transformers h1 Gaussian Naive Bayes h1 Analysing the X train data Experimenting h1 Best Variable Transformation h1 Var Smoothing h1 Permutation Importance h1 Converting to Discrete h1 References ", "stargazers_count": 0, "forks_count": 0, "description": "com better naive bayes 6. com rayhanlahdji tps 1121 naive bayes for naive souls by Rayhan Lahdji2. Permutation ImportanceThe permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. For speed I am choosing a simple test split of 30 size on 10000 samples. This procedure breaks the relationship between the feature and the target thus the drop in the model score is indicative of how much the model depends on the feature. shape 1 print d. s f f 1 features indices f importances indices f fitting our pipeline with a subset of the data with these variables first 39 variables have positive importance hence ignoring them. png It perform wells in case of categorical input variables compared to numerical variable s. org naive bayes classifiers 2. com watch v H3EjCKtlVog on Gaussian Naive Bayes2. com blog 2017 09 naive bayes explained 4. Understand the Models You LoveIn this month s TPS I am checking out some new algorithms I came across. I did a similar experiment last month with Random Forest https www. Bernoulli Used when features are binary in nature 0s and 1s We will use Gaussian NB after transforming our features. com markosthabit tbs november naive bayes by Markos Thabit3. Since our dataset has weakly correlated variables NB might work well here. Importing Packages and Sample Data Splitting Data Making Pipelines with Transformers Gaussian Naive BayesMethods of Improvement http www. https machinelearningmastery. Please note that these parameter observations are made independent of each other and only for the current data we have. com 8ebe947b0538431322197ecd5324bade_l3. Multinomial Used for discrete counts3. org stable modules generated sklearn. For numerical variable normal distribution is assumed bell curve which is a strong assumption hence we will do variable transformation first. I am choosing Naive Bayes right now. In simple terms a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Laplace Smoothing is introduced to solve the problem of zero probability. Following are the types of Naive Bayes algorithms 1. com wp content uploads 2015 09 Bayes_rule 300x172 300x172. in Feature Importance in Naive Bayes Classifiers 5qob5d5sFWNotebooks 1. svg Analysing the X_train data Experimenting Best Variable TransformationQuantile Transformer works best because it helps variables assume normal distribution useful for NB Var SmoothingPortion of the largest variance of all features that is added to variances for calculation stability. com wp content ql cache quicklatex. com introduction to na C3 AFve bayes classifier fa59e3e24aaf3. I would make changes to the important parameters and mention their impact. Gaussian Assumes that features follow a normal distribution. I have shared references towards the end. https towardsdatascience. ReferencesLinks 1. com raahulsaxena tps oct 21 understand random forest parameters Feel free to run your own experiments and upvote if you find this code useful About Naive BayesIt is a classification technique based on Bayes Theorem with an assumption of independence among predictors. Converting to Discrete6 bins without any scaling give good results even better than simple quantile transformer scaling on continuous variables. com watch v O2L2Uv9pdDA on understanding NB importing evaluation and data split packages importing modelling packages taking only 10000 rows as sample fit a probability distribution to a univariate data sample estimate parameters fit distribution sort data into classes calculate priors create PDFs Print the feature ranking for f in range X_test. https scikit learn. com prashant111 naive bayes classifier in python by Prashant BanerjeeVideos 1. The purpose of this exercise is to become better at extracting maximum power from it and see if non NN models can be used too. In statistics Laplace Smoothing is a technique to smooth categorical data. ", "id": "raahulsaxena/tps-nov-21-extracting-the-power-of-naive-bayes", "size": "4055", "language": "python", "html_url": "https://www.kaggle.com/code/raahulsaxena/tps-nov-21-extracting-the-power-of-naive-bayes", "git_url": "https://www.kaggle.com/code/raahulsaxena/tps-nov-21-extracting-the-power-of-naive-bayes", "script": "RobustScaler train_test_split sklearn.inspection stats numpy seaborn datatable MultinomialNB GaussianNB sklearn.naive_bayes roc_auc_score scipy StratifiedKFold permutation_importance matplotlib.pyplot QuantileTransformer MinMaxScaler percentiler sklearn.model_selection pandas sklearn.pipeline fit_distribution scipy.stats Pipeline sklearn.metrics StandardScaler sklearn.preprocessing ", "entities": "(('strong hence we', 'variable transformation'), 'assume') (('presence', 'other feature'), 'assume') (('we', 'only current data'), 'note') (('Laplace Smoothing', 'zero probability'), 'introduce') (('Converting', 'continuous variables'), 'give') (('I', 'Random Forest https last month www'), 'do') (('feature when single value', 'model score'), 'define') (('NN non models', 'it'), 'be') (('com blog 2017 09 naive bayes', '4'), 'explain') (('code', 'predictors'), 'tps') (('priors', 'range'), 'calculate') (('I', 'impact'), 'make') (('Sample Data Splitting Making', 'www'), 'import') (('first 39 variables', 'hence them'), 'index') (('that', 'normal distribution'), 'follow') (('png It', 'variable numerical s.'), 'perform') (('model', 'feature'), 'break') (('I', 'end'), 'share') (('types', 'Naive Bayes algorithms'), 'be') (('I', 'new algorithms'), 'understand') (('We', 'features'), 'use') (('I', '10000 samples'), 'choose') (('Laplace Smoothing', 'categorical data'), 'be') (('that', 'calculation stability'), 'work') "}