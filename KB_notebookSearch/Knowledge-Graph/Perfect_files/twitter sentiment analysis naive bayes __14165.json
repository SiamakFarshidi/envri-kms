{"name": "twitter sentiment analysis naive bayes ", "full_name": " h1 Naive Bayes h1 Part 1 Process the Data h2 Part 1 1 Implementing your helper functions h4 Instructions ", "stargazers_count": 0, "forks_count": 0, "description": "So how do you train a Naive Bayes classifier The first part of training a naive bayes classifier is to identify the number of classes that you have. In this freqs dictionary the key is the tuple word label The value is the number of times it has appeared. Calculate V You can then compute the number of unique words that appear in the freqs dictionary to get your V you can use the set function. One way for us to define the level of positiveness or negativeness without calculating the log likelihood is to compare the positive to negative frequency of the word. Words Positive word count Negative Word Count glad 41 2 arriv 57 4 1 3663 0 378 Implement get_words_by_threshold freqs label threshold If we set the label to 1 then we ll look for all words whose threshold of positive negative is at least as high as that threshold or higher. Similarly use the lookup function to get the negative count of that word. Emojis like and words like me tend to have a negative connotation. However please remember to include the logprior because whenever the data is not perfectly balanced the logprior will be a non zero value. In other words the positive frequency of a word is the number of times the word is counted with the label of 1. Calculate D D_ pos D_ neg Using the train_y input list of labels calculate the number of documents tweets D as well as the number of positive documents tweets D_ pos and number of negative documents tweets D_ neg. This means that the ratio of positive to negative 1 and the logprior is 0. If we set the label to 0 then we ll look for all words whose threshold of positive negative is at most as low as the given threshold or lower. Notice how the words i and am are not saved since it was removed by process_tweet because it is a stopword. Implement test_naive_bayes Instructions Implement test_naive_bayes to check the accuracy of your predictions. Remember to check if the key in the dictionary exists before adding that key to the dictionary or incrementing its value. For each tweet sum up loglikelihoods of each word in the tweet. For example given a list of tweets i am rather excited you are rather happy and the label 1 the function will return a dictionary that contains the following key value pairs rather 1 2 happi 1 1 excit 1 1 Notice how for each word in the given string the same label 1 is assigned to each word. Use the formulas as follows and store the values in a dictionary P D_ pos frac D_ pos D tag 1 P D_ neg frac D_ neg D tag 2 Where D is the total number of documents or tweets in this case D_ pos is the total number of positive tweets and D_ neg is the total number of negative tweets. download stopwords nltk. Calculate N_ pos and N_ neg Using freqs dictionary you can also compute the total number of positive words and total number of negative words N_ pos and N_ neg. P D_ pos is the probability that the document is positive. Similarly we can also filter a subset of words that have a maximum ratio of positivity negativity or lower words that are at least as negative or even more negative than a given threshold. We have given you the function process_tweet that does this for you. Append a dictionary to a list where the key is the word and the dictionary is the dictionary pos_neg_ratio that is returned by the get_ratio function. get word label 0 print cleaned tweet UNQ_C1 UNIQUE CELL IDENTIFIER DO NOT EDIT define the key which is the word and label tuple if the key exists in the dictionary increment the count else if the key is new add it to the dictionary and set the count to 1 Testing your function Build the freqs dictionary for later uses UNQ_C2 UNIQUE CELL IDENTIFIER DO NOT EDIT calculate V the number of unique words in the vocabulary calculate N_pos and N_neg if the label is positive greater than zero Increment the number of positive words by the count for this word label pair else the label is negative increment the number of negative words by the count for this word label pair Calculate D the number of documents Calculate D_pos the number of positive documents hint use sum Calculate D_neg the number of negative documents hint compute using D and D_pos Calculate logprior For each word in the vocabulary. 2f p_category Feel free to check the sentiment of your own tweet below UNQ_C8 UNIQUE CELL IDENTIFIER DO NOT EDIT use lookup to find positive counts for the word denoted by the integer 1 use lookup to find negative counts for the word denoted by integer 0 calculate the ratio of positive to negative counts for the word UNQ_C9 UNIQUE CELL IDENTIFIER DO NOT EDIT get the positive negative ratio for a word if the label is 1 and the ratio is greater than or equal to the threshold. download twitter_samples Part 1 Process the DataFor any machine learning project once you ve gathered the data the first step is to process it to make useful inputs to your model. Remove noise You will first want to remove noise from your data that is remove words that don t tell you much about the content. Calculate the ratio of positive divided by negative counts ratio frac text pos_words 1 text neg_words 1 Where pos_words and neg_words correspond to the frequency of the words in their respective classes. The reason for doing this is because we want to treat words with or without the punctuation as the same word instead of treating happy happy happy happy and happy. InstructionsCreate a function count_tweets that takes a list of tweets as input cleans all of them and returns a dictionary. org wiki Additive_smoothing explains more about additive smoothing. You will also implement a lookup helper function that takes in the freqs dictionary a word and a label 1 or 0 and returns the number of times that word and label tuple appears in the collection of tweets. Other words like glad community and arrives tend to be found in the positive tweets. In other words if we had no specific information and blindly picked a tweet out of the population set what is the probability that it will be positive versus that it will be negative That is the prior. We will use this dictionary in several parts of this assignment. Calculate freq_ pos and freq_ neg Using your freqs dictionary you can compute the positive and negative frequency of each word freq_ pos and freq_ neg. Expected Output The expected output is around 1. In other words we ll treat motivation motivated and motivate similarly by grouping them within the same stem of motiv. It takes a short time to train and also has a short prediction time. 9940 Expected Output I am happy 2. The prior is the ratio of the probabilities frac P D_ pos P D_ neg. 41 great great great great 8. You will create a probability for each class. 09089 Part 3 Test your naive bayesNow that we have the logprior and loglikelihood we can test the naive bayes function by making predicting on some tweets Implement naive_bayes_predict Instructions Implement the naive_bayes_predict function to make predictions on tweets. 57 The sentiment is positive. Assume that the result dictionary that is input will contain clean key value pairs you can assume that the values will be integers that can be incremented. The function takes in the tweet logprior loglikelihood. 55 Part 4 Filter words by Ratio of positive to negative counts Some words have more positive counts than others and can be considered more positive. Once we re able to calculate these ratios we can also filter a subset of words that have a minimum ratio of positivity negativity or higher. See you next week add folder tmp2 from our local workspace containing pre downloaded corpora files to nltk s data path get the sets of positive and negative tweets split the data into two pieces one for training and one for testing validation set avoid assumptions about the length of all_positive_tweets remove stock market tickers like GE remove old style retweet text RT remove hyperlinks remove hashtags only removing the hash sign from the word tokenize tweets remove stopwords remove punctuation tweets_clean. V is the number of unique words in the entire set of documents for all classes whether positive or negative. Compute the positive probability of each word P W_ pos negative probability of each word P W_ neg using equations 4 5. append word stemming word freqs. The function takes in your test_x test_y log_prior and loglikelihood It returns the accuracy of your model. Likewise some words can be considered more negative than others. Add the pos_neg_ratio to the dictionary otherwise do not include this word in the list do nothing Test your function find negative words at or below a threshold Test your function find positive words at or above a threshold Some error analysis done for you Test with your own tweet feel free to modify my_tweet. Also add the logprior to this sum to get the predicted sentiment of that tweet. It returns the probability that the tweet belongs to the positive or negative class. that would not give us enough information on the sentiment. The key in the dictionary is a tuple containing the stemmed word and its class label e. Prior and LogpriorThe prior probability represents the underlying probability in the target population that a tweet is positive versus negative. It is good practice to check the datatype before incrementing the value but it s not required here. The value the number of times this word appears in the given collection of tweets an integer. You can then compute the loglikelihood log left frac P W_ pos P W_ neg right tag 6. Calculate the probability that a document tweet is positive P D_ pos and the probability that a document tweet is negative P D_ neg Calculate the logprior the logprior is log D_ pos log D_ neg Calculate log likelihood Finally you can iterate over each word in the vocabulary use your lookup function to get the positive frequencies freq_ pos and the negative frequencies freq_ neg for that specific word. This wiki article https en. We ll use these to compute the positive and negative probability for a specific word using this formula P W_ pos frac freq_ pos 1 N_ pos V tag 4 P W_ neg frac freq_ neg 1 N_ neg V tag 5 Notice that we add the 1 in the numerator for additive smoothing. Implement get_ratio Given the freqs dictionary of words and a particular word use lookup freqs word 1 to get the positive count of the word. We can calculate the ratio of positive to negative frequencies of a word. We can take the log of the prior to rescale it and we ll call this the logprior text logprior log left frac P D_ pos P D_ neg right log left frac D_ pos D_ neg right. Log likelihoodTo compute the loglikelihood of that very same word we can implement the following equations text loglikelihood log left frac P W_ pos P W_ neg right tag 6 Create freqs dictionary Given your count_tweets function you can compute a dictionary called freqs that contains all the frequencies. 5 Notice the difference between the positive and negative ratios. Congratulations on completing this assignment. P D_ neg is the probability that the document is negative. 0 means that when we add the logprior to the log likelihood we re just adding zero to the log likelihood. Expected Accuracy 0. Note that log frac A B is the same as log A log B. 29 this movie should have been great. Why do you think the misclassifications happened Were there any assumptions made by the naive bayes model Part 6 Predict with your own tweetIn this part you can predict the sentiment of your own tweet. Add the pos_neg_ratio to the dictionary If the label is 0 and the pos_neg_ratio is less than or equal to the threshold. 1 Implementing your helper functionsTo help train your naive bayes model you will need to build a dictionary where the keys are a word label tuple and the values are the corresponding frequency. UNQ_C6 UNIQUE CELL IDENTIFIER DO NOT EDIT return this properly if the prediction is 0 the predicted class is 1 otherwise the predicted class is 0 append the predicted class to the list y_hats error is the average of the absolute values of the differences between y_hats and test_y Accuracy is 1 minus the error UNQ_C7 UNIQUE CELL IDENTIFIER DO NOT EDIT You do not have to input any code in this cell but it is relevant to grading so please do not change anything Run this cell to test your function print s f tweet naive_bayes_predict tweet logprior loglikelihood print f tweet p. N_ pos and N_ neg are the total number of positive and negative words for all documents for all tweets respectively. So the logprior can also be calculated as the difference between two logs text logprior log P D_ pos log P D_ neg log D_ pos log D_ neg tag 3 Positive and Negative Probability of a WordTo compute the positive probability and the negative probability for a specific word in the vocabulary we ll use the following inputs freq_ pos and freq_ neg are the frequencies of that specific word in the positive or negative class. Finally you want to use stemming to only keep track of one variation of each word. InstructionsGiven a freqs dictionary train_x a list of tweets and a train_y a list of labels for each tweet implement a naive bayes classifier. 28 great great great 6. Notice how the word rather appears twice in the list of tweets and so its count value is 2. Use the get_ratio function to get a dictionary containing the positive count negative count and the ratio of positive to negative counts. P W_ pos frac freq_ pos 1 N_ pos V tag 4 P W_ neg frac freq_ neg 1 N_ neg V tag 5 Note We ll use a dictionary to store the log likelihoods for each word. You may find it useful to use the zip function to match each element in tweets with each element in ys. get the positive and negative frequency of the word calculate the probability that each word is positive and negative calculate the log likelihood of the word UNQ_C3 UNIQUE CELL IDENTIFIER DO NOT EDIT You do not have to input any code in this cell but it is relevant to grading so please do not change anything UNQ_C4 UNIQUE CELL IDENTIFIER DO NOT EDIT process the tweet to get a list of words initialize probability to zero add the logprior check if the word exists in the loglikelihood dictionary add the log likelihood of that word to the probability UNQ_C5 UNIQUE CELL IDENTIFIER DO NOT EDIT You do not have to input any code in this cell but it is relevant to grading so please do not change anything Experiment with your own tweet. Expected Output 0. These include all common words like I you are is etc. Naive BayesIf you are running this notebook in your local computer don t forget to download the twitter samples and stopwords from nltk. First use naive_bayes_predict function to make predictions for each tweet in text_x. You also want to remove all the punctuation from a tweet. We ll also remove stock market tickers retweet symbols hyperlinks and hashtags because they can not tell you a lot of information on the sentiment. Expected Output happi 1 1 trick 0 1 sad 0 1 tire 0 2 Part 2 Train your model using Naive BayesNaive bayes is an algorithm that could be used for sentiment analysis. p logprior sum_i N loglikelihood_i NoteNote we calculate the prior from the training data and that the training data is evenly split between positive and negative labels 4000 positive and 4000 negative tweets. Part 5 Error AnalysisIn this part you will see some tweets that your model missclassified. The key is the word the value is the log likelihood of that word. Note that we can also use the log likelihood calculations to compare relative positivity or negativity of words. An example key value pair would have this structure happi positive 10 negative 20 ratio 0. Hints Please use the process_tweet function that was imported above and then store the words in their respective dictionaries and sets. Note that the labels we ll use here are 1 for positive and 0 for negative. ", "id": "pratikbarua/twitter-sentiment-analysis-naive-bayes", "size": "14165", "language": "python", "html_url": "https://www.kaggle.com/code/pratikbarua/twitter-sentiment-analysis-naive-bayes", "git_url": "https://www.kaggle.com/code/pratikbarua/twitter-sentiment-analysis-naive-bayes", "script": "TweetTokenizer os nltk.tokenize numpy nltk.stem get_words_by_threshold twitter_samples count_tweets train_naive_bayes getcwd PorterStemmer get_ratio pandas stopwords naive_bayes_predict test_naive_bayes nltk.corpus process_tweet test_lookup lookup ", "entities": "(('particular word', 'word'), 'get_ratio') (('pos_neg_ratio', 'threshold'), 'add') (('word', 'tweets'), 'implement') (('it', 'function print'), 'IDENTIFIER') (('they', 'sentiment'), 'remove') (('it', 'own tweet'), 'get') (('word', 'integer'), 'appear') (('_ pos _ neg', 'frac D'), 'take') (('ratio', '1'), 'mean') (('input', 'dictionary'), 'InstructionsCreate') (('You', 'class'), 'create') (('threshold', 'at most as given threshold'), 'look') (('value', 'log word'), 'be') (('that', 'you'), 'give') (('threshold', 'at least as threshold'), 'glad') (('that', 'value clean key pairs'), 'assume') (('stopwords', 'punctuation tweets_clean'), 'add') (('we', 'here positive negative'), 'note') (('that', 'sentiment'), 'give') (('function', 'tweet logprior loglikelihood'), 'take') (('perfectly balanced', 'logprior'), 'remember') (('us', 'word'), 'be') (('tweet', 'positive class'), 'return') (('ratio', 'threshold'), '2f') (('word', '1'), 'be') (('it', 'times'), 'be') (('words', 'others'), 'word') (('dictionary that', 'get_ratio function'), 'append') (('you', 'set function'), 'calculate') (('org wiki Additive_smoothing', 'additive smoothing'), 'explain') (('we', 'positive class'), 'calculate') (('you', 'that'), 'train') (('Other words', 'positive tweets'), 'tend') (('it', 'value'), 'be') (('Emojis', 'negative connotation'), 'tend') (('it', 'process_tweet'), 'notice') (('N _ neg Using you', 'total negative words'), 'dictionary') (('number', 'vocabulary'), 'get') (('Using', 'tweets _ D negative documents'), 'd') (('We', 'assignment'), 'use') (('tweet', 'target population'), 'represent') (('Finally you', 'word'), 'want') (('it', 'ys'), 'find') (('you', 'word freq _ pos'), 'pos') (('training data', 'evenly positive labels 4000 positive 4000 negative tweets'), 'logprior') (('V', 'classes'), 'be') (('That', 'population'), 'set') (('key', 'value'), 'remember') (('test_y It', 'model'), 'take') (('list', 'bayes naive classifier'), 'InstructionsGiven') (('You', 'frac P W _ pos P W _ neg'), 'compute') (('we', 'instead happy happy'), 'be') (('Implement Instructions', 'tweets'), '09089') (('error analysis', 'own tweet'), 'include') (('log frac A B', 'log log B.'), 'note') (('part you', 'own tweet'), 'think') (('same label', '1 word'), 'be') (('you', 'I'), 'include') (('prior', 'probabilities'), 'be') (('word label values', 'dictionary'), '1') (('we', 'words'), 'note') (('that', 'sentiment analysis'), 'happi') (('Instructions', 'predictions'), 'implement') (('first step', 'model'), 'be') (('P W _ neg frac _ V V 4 _ neg 1 5 we', 'additive smoothing'), 'use') (('lower that', 'at least as even more given threshold'), 'filter') (('we', 'motiv'), 'treat') (('count so value', 'tweets'), 'notice') (('5', 'positive ratios'), 'notice') (('P W _ neg frac _ neg V V 4 _ neg 1 5 We', 'word'), 'freq') (('don t', 'content'), 'noise') (('It', 'prediction also short time'), 'take') (('frac text 1 1 Where pos_words', 'respective classes'), 'calculate') (('we', 'log likelihood'), 'mean') (('model', 'that'), 'AnalysisIn') (('You', 'tweet'), 'want') (('D _ pos', 'D _ total negative tweets'), 'use') (('N _ pos', 'tweets'), 'be') (('that', 'respective dictionaries'), 'use') (('that', 'positivity negativity'), 'filter') (('that', 'frequencies'), 'compute') (('We', 'word'), 'calculate') (('positive frequencies', 'specific word'), 'calculate') (('Naive BayesIf you', 'nltk'), 'run') (('key', 'stemmed word'), 'be') (('Likewise words', 'more others'), 'consider') "}