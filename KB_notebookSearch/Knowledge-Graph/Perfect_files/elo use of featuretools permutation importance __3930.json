{"name": "elo use of featuretools permutation importance ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "In built feature importance of Light GBM2. Adding relationships to the entity set is so natural in featuretools just like anybody would be dealing with databases. I started by developing ther baseline model using the features available in the training dataset achieved score of around 3. Possible that Feature importances and SHAP values can give better results but I do not understand their maths very well especially the SHAP values. One problem that I am facing not able to resolve completely due to the fact that automtaed feature engineering has generated so many junk features that any single feature is having very less impact on overall accuracy. This is simply selecting the features based on a threshold from Permutation Importance. To increase accuracy further I see two options please let me know if you have other ideas as well 1. Now comes the interesting part of running featuretools to get automated feature engineering. A better approach could be to label encode all the features before performing will make that optimization going forward. Most of the features are categorical in datasets but marked as numerical so manually I had to change the variable type of many features to Id and Categorical to avoid getting rubbish features. Permutation Importance is easy to comprehend and a natural way to remove useless features if you randomly shuffle a feature and it doesn t reduce your accuracy then that feature is not a good indicator. Permutation Importance3. Same featuretools engineering for the test set. With more depth you get transformation of transformations that can be useful at times but are very hard to interpret so avoiding that. This library might take a little while to understand completely but an excellent and elegant solution. linear algebra data processing CSV file I O e. 98 implying that the features in training dataset were not very useful. While using featuretools for automated feature engineering below are some of the challenges I encountered 1. As I mentioned above due to memory constraints I had to do this twice by deleting the intermediary files in between. Comes the final part of feature engineering DFS or Deep Feature Synthesis where different features get stacked up that s mean the term deep comes from. These are some features that contained string data so label encoding them below. Performed operations on each dataset and then immediately deleted them to retain precious memory. Hyper tune the paramters in Light GBM XGBoost model. Performed featuretools feature engineering twice on train and test dataset that I could have done in 1 go by combining the two sets. If someone had to do all this manually this is the way to go about it. So even with a low threshold of 0. Everytime I was running out of RAM and the kernel died. One I need to thoroughly understand how cards business work but then lot of features are anonymized and it is difficult to comprehend them. To select features I am using the 3 methods 1. Perform manual feature engineering based on domain knowledge. Next I did some manual feature engineering by aggregating purchase amount feature from historical and new merchant transactions getting an accuracy of roughly 3. Using Light GBM generally I prefer XGBoost but they provide similar accuracy. Then I decided to try featuretools for automated feature engineering and use Permutation Importance to select the relevant features. Turned off GPU to increase RAM from 14GB to 17GB. SHAP valuesI prefer to perform feature selection based on Permutation Importance simply because that s the best I understand. I will try learning them better. Tackling this I had to take some redundant steps but yes they worked. The biggest problem by far at least in this competition was the large size of historical transactions file. I have kept the maximum depth as 1. 001 I remove almost 70 of the features generated from featuretools. ", "id": "tandonarpit6/elo-use-of-featuretools-permutation-importance", "size": "3930", "language": "python", "html_url": "https://www.kaggle.com/code/tandonarpit6/elo-use-of-featuretools-permutation-importance", "git_url": "https://www.kaggle.com/code/tandonarpit6/elo-use-of-featuretools-permutation-importance", "script": "SelectFromModel PermutationImportance lightgbm sklearn.feature_selection mean_squared_error matplotlib sklearn.preprocessing train_test_split eli5.sklearn pyplot pyplot as plt featuretools LabelEncoder sklearn.model_selection pandas sklearn.metrics numpy ", "entities": "(('I', 'featuretools'), '001') (('they', 'redundant steps'), 'tackle') (('biggest problem', 'transactions large historical file'), 'be') (('library', 'little while'), 'take') (('it', 'them'), 'one') (('I', 'Permutation Importance'), 'prefer') (('then feature', 't accuracy'), 'be') (('Hyper', 'GBM XGBoost Light model'), 'tune') (('This', 'Permutation Importance'), 'select') (('I', '1'), 'be') (('single feature', 'overall accuracy'), 'problem') (('Performed operations', 'precious memory'), 'delete') (('I', '3 methods'), 'select') (('features', 'training dataset'), 'imply') (('I', 'around 3'), 'start') (('just anybody', 'databases'), 'be') (('that', 'so them'), 'be') (('that', 'very so that'), 'get') (('Then I', 'relevant features'), 'decide') (('I', 'twice intermediary files'), 'have') (('kernel', 'RAM'), 'run') (('optimization', 'features'), 'be') (('I', 'two sets'), 'feature') (('I', 'SHAP very well especially values'), 'possible') (('numerical so manually I', 'rubbish features'), 'be') (('term', 'that'), 'come') (('they', 'similar accuracy'), 'prefer') (('Now interesting part', 'feature automated engineering'), 'come') (('Next I', 'roughly 3'), 'do') (('you', 'as well 1'), 'see') (('manually this', 'it'), 'be') "}