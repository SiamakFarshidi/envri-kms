{"name": "notebook7fb74e7e82 ", "full_name": " h1 Predicting House Prices on Kaggle h2 Downloading and Caching Datasets h2 Kaggle h2 Accessing and Reading the Dataset h2 Data Preprocessing h2 Training h2 K Fold Cross Validation h2 Model Selection h2 Submitting Predictions on Kaggle h2 Summary h2 Exercises ", "stargazers_count": 0, "forks_count": 0, "description": "png raw 1 width 400px label fig_house_pricing Accessing and Reading the DatasetNote that the competition data is separatedinto training and test sets. If they do it is time to upload them to Kaggle. Second because we do not know a priori which features will be relevant we do not want to penalize coefficientsassigned to one feature more than on any other. First we maintain a dictionary DATA_HUB that maps a string the name of the dataset to a tuple containing both the URL to locate the datasetand the SHA 1 key that verifies the integrity of the file. We use pandas to load the two csv files containing training and test data respectively. For example the year of constructionis represented by an integer the roof type by discrete categorical assignments and other features by floating point numbers. One way to address this problem is tomeasure the discrepancy in the logarithm of the price estimates. Let us start with the numerical features. Improve the score by improving the model e. Kaggle Kaggle https www. com d2l ai d2l pytorch colab blob master img kaggle. It is considerably larger than the famous Boston housing dataset https archive. But this added complexity might obfuscate our code unnecessarilyso we can safely omit it here owing to the simplicity of our problem. Then to put all features on a common scale we standardize the data byrescaling features to zero mean and unit variance x leftarrow frac x mu sigma. Throughout training you will want to monitor both numbers. 2011 covers house prices in Ames IA from the period of 2006 2010. png raw 1 width 400px label fig_kaggle_submit2 Summary Real data often contain a mix of different data types and need to be preprocessed. The following code will generate a file called submission. data by default and returns the name of the downloaded file. Model SelectionIn this example we pick an untuned set of hyperparametersand leave it up to the reader to improve the model. Rescaling real valued data to zero mean and unit variance is a good default. layers weight decay and dropout. com d2l ai d2l pytorch colab blob master img house_pricing. We also implement two additional utility functions one is to download and extract a zip or tar fileand the other to download all the datasets used in this book from DATA_HUB into the cache directory. TrainingTo get started we train a linear model with squared loss. So you will want to make sure that you have pandas installedbefore proceeding further. Predicting House Prices on Kaggle label sec_kaggle_house Now that we have introduced some basic toolsfor building and training deep networksand regularizing them with techniques includingweight decay and dropout we are ready to put all this knowledge into practiceby participating in a Kaggle competition. One nice sanity check is to seewhether the predictions on the test setresemble those of the K fold cross validation process. The Data tab on the competition tabin numref fig_house_pricing has links to download the data. We can see that in each example the first feature is the ID. Improve the score on Kaggle by tuning the hyperparameters through K fold cross validation. Submitting Predictions on KaggleNow that we know what a good choice of hyperparameters should be we might as well use all the data to train on it rather than just 1 1 K of the datathat are used in the cross validation slices. For instance if our prediction is off by USD 100 000when estimating the price of a house in Rural Ohio where the value of a typical house is 125 000 USD then we are probably doing a horrible job. Downloading and Caching DatasetsThroughout the book we will train and test modelson various downloaded datasets. We first need a function that returnsthe i mathrm th fold of the datain a K fold cross validation procedure. The following additional libraries are needed to run thisnotebook. The steps are quite simple Log in to the Kaggle website and visit the house price prediction competition page. The features consist of various data types. Let us take a look at the first four and last two featuresas well as the label SalePrice from the first four examples. Can you improve your model by minimizing the logarithm of prices directly What happens if you try to predict the logarithm of the price rather than the price 1. Each record includes the property value of the houseand attributes such as street type year of construction roof type basement condition etc. ai t 107 save save Hit cache save save If pandas is not installed please uncomment the following line pip install pandas save save After standardizing the data all means vanish hence we can set missing values to 0 Dummy_na True considers na missing value as a valid feature value and creates an indicator feature for it To further stabilize the value when the logarithm is taken set the value less than 1 as 1 The Adam optimization algorithm is used here Apply the network to the test set Reformat it to export to Kaggle. Saving the predictions in a csv filewill simplify uploading the results to Kaggle. What happens if we do not standardize the continuous numerical features like what we have done in this section Discussions https discuss. All such datasets are hosted at the sitewhose address is DATA_URL. com d2l ai d2l pytorch colab blob master img kaggle_submit2. Click the Upload Submission File button in the dashed box at the bottom of the page and select the prediction file you wish to upload. Dropping the MSZoning feature two new indicator features MSZoning_RL and MSZoning_RM are created with values being either 0 or 1. While leaderboard chasing often spirals out of control with researchers focusing myopically on preprocessing stepsrather than asking fundamental questions there is also tremendous value in the objectivity of a platformthat facilitates direct quantitative comparisonsamong competing approaches as well as code sharingso that everyone can learn what did and did not work. It proceeds by slicing out the i mathrm th segmentas validation data and returning the rest as training data. We hope that through a hands on approach you will gain some intuitions that will guide youin your career as a data scientist. Hence we remove it from the datasetbefore feeding the data into the model. The pandas package does this automatically for us. While this is convenient it does not carryany information for prediction purposes. This indicates that we are overfitting. Finding a good choice can take time depending on how many variables one optimizes over. Intuitively we standardize the datafor two reasons. We replace them by a one hot encodingin the same way that we previously transformedmulticlass labels into vectors see numref subsec_classification problem. First we apply a heuristic replacing all missing valuesby the corresponding feature s mean. Click the Make Submission button at the bottom of the page to view your results. And here is where reality complicates things for some examples some data are altogether missingwith the missing value marked simply as na. Is it always a good idea to replace missing values by their mean Hint can you construct a situation where the values are not missing at random 1. And if things work the linear model will serve as a baselinegiving us some intuition about how close the simple modelgets to the best reported models giving us a senseof how much gain we should expect from fancier models. Submit your predictions for this section to Kaggle. Note that running on Colab is experimental please report a Githubissue if you have any problem. Next we deal with discrete values. However if we try an unreasonably large number of optionswe might just get lucky and find that our validationperformance is no longer representative of the true error. If we cannot do better than random guessing here then there might be a good chancethat we have a data processing bug. The following download function downloads a dataset caching it in a local directory. This helps the model identify each training example. The price of each house is includedfor the training set only it is a competition after all. png raw 1 width 400px label fig_kaggle On the house price prediction competition page as illustratedin numref fig_house_pricing you can find the dataset under the Data tab submit predictions and see your ranking The URL is right here https www. If you want to participate in a Kaggle competition you will first need to register for an account see numref fig_kaggle. We will put this to good use to select the model designand to adjust the hyperparameters. This leads to the following root mean squared error between the logarithm of the predicted price and the logarithm of the label price sqrt frac 1 n sum_ i 1 n left log y_i log hat y _i right 2. So is replacing missing values with their mean. The model that we obtain in this waycan then be applied to the test set. This includes features such as MSZoning. The training dataset includes 1460 examples 80 features and 1 label while the test datacontains 1459 examples and 80 features. After all a small value delta for log y log hat y leq delta translates into e delta leq frac hat y y leq e delta. On the other hand if we err by this amountin Los Altos Hills California this might represent a stunningly accurate prediction there the median house price exceeds 4 million USD. com is a popular platformthat hosts machine learning competitions. Fortunately if you are reading in Jupyter we can install pandas without even leaving the notebook. In this section we will walk you through details ofdata preprocessing model design and hyperparameter selection. If a file corresponding to this datasetalready exists in the cache directoryand its SHA 1 matches the one stored in DATA_HUB our code will use the cached file to avoidclogging up your internet with redundant downloads. Unlike in previous sections our training functionswill rely on the Adam optimizer we will describe it in greater detail later. Logarithms are useful for relative errors. names of Harrison and Rubinfeld 1978 boasting both more examples and more features. Not surprisingly our linear model will not leadto a competition winning submissionbut it provides a sanity check to see whetherthere is meaningful information in the data. Finally via the values attribute we can extract the NumPy format from the pandas formatand convert it into the tensorrepresentation for training. Notice that sometimes the number of training errorsfor a set of hyperparameters can be very low even as the number of errors on K fold cross validationis considerably higher. Next as demonstrated in numref fig_kaggle_submit2 we can submit our predictions on Kaggleand see how they compare with the actual house prices labels on the test set. First it proves convenient for optimization. For convenience we can download and cachethe Kaggle housing datasetusing the script we defined above. The main appeal of this optimizer is that despite doing no better and sometimes worse given unlimited resources for hyperparameter optimization people tend to find that it is significantly less sensitiveto the initial learning rate. Each competition centers on a dataset and manyare sponsored by stakeholders who offer prizesto the winning solutions. We will need to preprocess the data before we can start modeling. You can see that this conversion increasesthe number of features from 79 to 331. To get started we will read in and process the datausing pandas which we have introduced in numref sec_pandas. K Fold Cross ValidationYou might recall that we introduced K fold cross validationin the section where we discussed how to dealwith model selection numref sec_model_selection. We will want to partition the training setto create a validation set but we only get to evaluate our models on the official test setafter uploading predictions to Kaggle. Less overfitting might indicate that our data can support a more powerful model. com c house prices advanced regression techniques The house price prediction competition page. Submitting data to Kaggle https github. The platform helps users to interactvia forums and shared code fostering both collaboration and competition. How good are your predictions 1. We can use K fold cross validation to select the model and adjust the hyperparameters. Click the Submit Predictions or Late Submission button as of this writing the button is located on the right. Massive overfitting might suggest that we can gainby incorporating regularization techniques. With a large enough dataset and the normal sorts of hyperparameters K fold cross validation tends to bereasonably resilient against multiple testing. Note that this is not the most efficient way of handling dataand we would definitely do something much smarterif our dataset was considerably larger. With house prices as with stock prices we care about relative quantitiesmore than absolute quantities. For instance MSZoning assumes the values RL and RM. To verify that this indeed transformsour feature variable such that it has zero mean and unit variance note that E frac x mu sigma frac mu mu sigma 0 and that E x mu 2 sigma 2 mu 2 2 mu 2 mu 2 sigma 2. The training and verification error averages are returnedwhen we train K times in the K fold cross validation. According to one hot encoding if the original value of MSZoning is RL then MSZoning_RL is 1 and MSZoning_RM is 0. The house price prediction competitionis a great place to start. Data PreprocessingAs stated above we have a wide variety of data types. Here we implement several utility functionsto facilitate data downloading. Transforming categorical features into indicator features allows us to treat them like one hot vectors. Thus we tend to care more aboutthe relative error frac y hat y y than about the absolute error y hat y. edu ml machine learning databases housing housing. The data are fairly generic and do not exhibit exotic structurethat might require specialized models as audio or video might. In fact this is also the official error measureused by the competition to evaluate the quality of submissions. This dataset collected by Bart de Cock in 2011 cite De Cock. ", "id": "georgemitchellswe/notebook7fb74e7e82", "size": "14330", "language": "python", "html_url": "https://www.kaggle.com/code/georgemitchellswe/notebook7fb74e7e82", "git_url": "https://www.kaggle.com/code/georgemitchellswe/notebook7fb74e7e82", "script": "download train_and_pred download_all train k_fold download_extract log_rmse d2l torch as d2l torch get_net get_k_fold_data torch.nn pandas numpy ", "entities": "(('First it', 'optimization'), 'prove') (('us', 'numerical features'), 'let') (('we', 'cross validation'), 'be') (('we', 'greater detail'), 'rely') (('altogether missing value', 'examples'), 'be') (('we', 'data types'), 'state') (('Intuitively we', 'datafor two reasons'), 'standardize') (('1', 'MSZoning'), 'be') (('E frac mu sigma', 'zero mean'), 'note') (('one', 'how many variables'), 'take') (('we', 'fancier models'), 'serve') (('we', 'test then set'), 'apply') (('We', 'hyperparameters'), 'use') (('we', 'squared loss'), 'start') (('Model example we', 'model'), 'SelectionIn') (('you', 'rather price'), 'improve') (('Hence we', 'model'), 'remove') (('that', 'data scientist'), 'hope') (('us', 'label last two well first four examples'), 'let') (('here then good we', 'data processing bug'), 'do') (('unit variance', 'zero mean'), 'be') (('that we', 'subsec_classification numref problem'), 'see') (('button', 'right'), 'click') (('model', 'training example'), 'help') (('1 test', '1459 examples'), 'include') (('One way', 'price estimates'), 'be') (('data', 'more powerful model'), 'indicate') (('it', 'learning significantly less initial rate'), 'be') (('we', 'Kaggle'), 'want') (('we', 'more other'), 'second') (('png raw 1 width 400px Summary Real data', 'data different types'), 'label') (('following code', 'file'), 'generate') (('Logarithms', 'relative errors'), 'be') (('Adam optimization less than 1 as algorithm', 'Kaggle'), 'save') (('sanity One nice check', 'validation cross process'), 'be') (('we', 'test various downloaded datasets'), 'Downloading') (('2011', '2006'), 'cover') (('Data tab', 'data'), 'have') (('you', 'pandas installedbefore proceeding'), 'want') (('URL', 'ranking'), 'label') (('cross validation', 'bereasonably multiple testing'), 'fold') (('this', 'submissions'), 'be') (('We', 'hyperparameters'), 'put') (('It', 'Boston housing dataset considerably famous https'), 'be') (('us', 'one hot vectors'), 'allow') (('names', 'both more examples'), 'boast') (('how they', 'test set'), 'submit') (('such datasets', 'sitewhose address'), 'host') (('Discussions https', 'section'), 'happen') (('where values', 'random'), 'be') (('first feature', 'example'), 'see') (('This', 'label price sqrt'), 'lead') (('Here we', 'data downloading'), 'implement') (('they', 'Kaggle'), 'be') (('fairly exotic structurethat', 'audio might'), 'be') (('one', 'cache directory'), 'implement') (('we', 'even notebook'), 'install') (('pandas package', 'automatically us'), 'do') (('we', 'regularization incorporating techniques'), 'suggest') (('we', 'Kaggle script'), 'download') (('corresponding feature', 'missing valuesby'), 'apply') (('record', 'construction roof type basement condition etc'), 'include') (('roof', 'point other floating numbers'), 'represent') (('1 rather just 1 K', 'cross validation slices'), 'submit') (('dataset', 'De Cock'), 'cite') (('data', 'zero mean'), 'put') (('where we', 'model selection numref how sec_model_selection'), 'Fold') (('You', '331'), 'see') (('just validationperformance', 'longer true error'), 'get') (('following additional libraries', 'thisnotebook'), 'need') (('MSZoning_RL', 'values'), 'feature') (('we', 'training'), 'extract') (('code', 'redundant downloads'), 'use') (('whetherthere', 'meaningful data'), 'leadto') (('we', 'Kaggle competition'), 'introduce') (('you', 'problem'), 'note') (('MSZoning', 'values'), 'assume') (('we', 'problem'), 'obfuscate') (('125 000 then we', 'probably horrible job'), 'for') (('steps', 'house price prediction competition page'), 'be') (('what', 'platformthat facilitates direct quantitative comparisonsamong competing approaches'), 'be') (('platform', 'shared collaboration'), 'help') (('It', 'training data'), 'proceed') (('features', 'data various types'), 'consist') (('competition data', 'png raw 1 width 400px DatasetNote'), 'label') (('that', 'file'), 'maintain') (('very even number', 'validationis'), 'notice') (('we', 'data'), 'need') (('we', 'numref sec_pandas'), 'read') (('we', 'ofdata model design selection'), 'walk') (('only it', 'house'), 'be') (('We', 'training'), 'use') (('you', 'numbers'), 'want') (('who', 'winning solutions'), 'center') (('it', 'prediction purposes'), 'carryany') (('K', 'validation procedure'), 'need') (('much dataset', 'definitely something'), 'note') (('you', 'prediction file'), 'click') (('Thus we', 'absolute error'), 'tend') (('you', 'numref fig_kaggle'), 'need') (('download following function', 'local directory'), 'download') (('we', 'absolute quantities'), 'with') (('median house there price', '4 million USD'), 'represent') "}