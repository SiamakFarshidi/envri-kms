{"name": "grapheme mind reader panv12 nogpu ", "full_name": " h1 Reading the Robot Mind h2 Executive Summary h2 Details h3 Please support by upvoting h3 references h2 Basic Model h2 Now let s do some mind reading h3 To determine this h1 What can we conclude h2 Note Re evaluated in light of fixes through version PANv12f h2 Big problem with the last Conv2D layer h2 Impact of Borders h2 Impact of Resolution h2 Examining the Filters Themselves h1 An that s it for the Mind Reader h3 Please support by upvoting ", "stargazers_count": 0, "forks_count": 0, "description": "com pnussbaum grapheme mind reader panv00 PANv01 through v12 Added mind reader code and removed extraneous code. Specifically Column 1 Original Input Columns 2 though 8 Recreation of input based on output of Conv2D layerIn the case of the above these recreations are done without any adjustment for biasing normalization or even max pooling. edu etd 558 PANv00 Forked from the notebook Bengali Graphemes Starter EDA Multi Output CNN https www. Examining the Filters ThemselvesIt is also worthwhile to examine the filters themselves. We need to process only the output of each convolutional layer and then reverse the process to create an image. Naturally these both add to precious resource usage for this contest. PANv12f now have a list of filter patches one for each convolutional layer number of filters in each Conv2D layer the size of each filter assume they are square for now the size of the border around the center pixel of the square assume filters are odd numbered dimensions like 3 5 7 etc. Corrected various bugs PANv12f Simplified code. 9353Pointer to version 00 https www. This is a wonderful method of determining which features most greatly impact the final classification coding. Now shows all convolutional layers Basic Model Now let s do some mind readingGiven only the output of one of the convolutional layers is enough information being kept for subsequent layers to process correctly To determine this. It uses a reverse calculation and attempts to recreate the original data in this case the handwritten grapheme from an internal state of the neural network. imshow scratch cmap gray here is if we want to show the full reconstruction Now create the output from only the first convolutional layer using out explore model second and third In this section we reverse the convolution layer to recreate a picture display the original and re created images original image Recreate image from combination of this and all of the prior layers of filters multiplied by the output of this layer The size of output plus borders Find the color patch attributable to this Filter multiplied by the output value it generated PANv12f6 the border we choose not to display because it s blank due to padding same may be too big when things scale down due to strides or max pooling. Another method uses autoencoders 2 as a data compression means which also can point to flaws in a lossy internal representation of the data. This will help bring attention to seeing these or similar functions get incorporated within the Keras framework and wherever it seems it will do the most good for future researchers. I am a full time teacher and as such I can only devote spare time to this effort. There is a HUGE degredation before the last convolution layer perhaps due to MaxPooling from 32x32 down to 16x16 perhaps due to the 5x5 filter size or perhaps something else Big problem with the last Conv2D layerIs this because it s closest to the flatten and dense layers morphed away from the original image becuase of this or simply due to one too many MaxPoolings leaving it at a 16x16 size from the original 64x64 or the increase in filter size from prior Conv2D at 3x3 to final Conv2D at 5x5 More investigation is needed Impact of BordersIn an earlier version of this kernel I indicated there may be a problem with borders but improvements to the recreation of the original input algorithm have shown me that this is not the case. In all convolutional layers there seem to be no duplicate filters no filters in the same convolutional layer that look identical however in the later convolutional layers some look very similar to one another. There may be a method to perform pruning and use fewer filters. the seven Convolutional layers are layers 1 2 5 7 10 11 and 14 PANv12f added number of convolutional layers the layer in which the convolutional filter data is stored any scaling due to strides or max pooling that took place just prior to or at that conv layer not cumulative list of models that stop at each convolutional layer list of filters for convolutional layer list of filter biases for convolutional layer These are the filters These are the biases PANv12f now we use a list of models that can be traversed. ax i col. Need to make this automated. Impact of ResolutionLooking at the set of twelve all correct and twelve all incorrect examples we see the impact of resolution gets worse and worse with each layer of convolution. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. In this way an expert or any child with 10 years of schooling in reading and writing Bengali can clearly see if important information has been lost what was lost and even what layer within the neural network it was lost. PANv12d e Corrected terminology used in comments. It looks as though the all correct examples lose some information but enough remains to identify the feature. Possible solutions would be to reduce the use of MaxPooling and also possibly keep the starting image at a higher resolution. We can then compare that to the original image to seee if too much information has been discarded. The cumulative effect of the borders from prior Conv3D layers The mind reading visualization of each filter equal to the filter at the first Conv2D layer but more complex later filter Size Border size For now draw eight filters on a row for display purposes temporary variables for filter size and cumulative border size display the filter Now create the output from only the first convolutional layer using out explore model second and third In this section we reverse the convolution layer to recreate a picture display the original and re created images original image Recreate image from combination of this and all of the prior layers of filters multiplied by the output of this layer The size of output plus borders Find the color patch attributable to this Filter multiplied by the output value it generated PANv12f6 the border we choose not to display because it s blank due to padding same may be too big when things scale down due to strides or max pooling. An expert can clearly see if important information has been lost what was lost and even what layer within the neural network it was lost. An that s it for the Mind ReaderThe above diagrams show the original test image and then in subsequent pairs of images shows the output of the first second and 3rd convolutional layer each followed by a reconstruction of the original image based on that output. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. For example notice the image degredation when the first max pooling goes from 64x64 to 32x32 between columns 3 and 4 or equivalently between the 2nd and 3rd Conv2D. model predictions placeholder row_id place holder. com pnussbaum grapheme mind reader panv00 Visuallize the network Just take the first Parquet for speed CNN takes images in shape batch_size h w channels so reshape the images for i j in enumerate all_incorrect print i j Visualize few samples of current training dataset PANv12f cleaned up the code to support any number of convolutional layers NOTE This importing of some weights section uses apriori knowledge of the model that we are trying to make subsets of FUTURE WORK Make this importing of NN subsets automatic without apriori knowledge Let s make a new networks whose outputs are the convolution layers NOTE This is manually configured. Please support by upvotingIf you feel this is an important area for further research please upvote this notebook. Also a human expert can determine if sufficient information has been retained to perform the classification task. graphical images of the kernels themselves as well as reverse processing of the output of internal layers back to an estimation of the original image to look for flaws. Reading the Robot Mind Executive Summary Examines one of the most popular public kernels in this contest Draws pictures of the internal workings of the neural network Explains how these pictures can be used to improve the score Proposes this for the Keras feature set DetailsIn this contest as in others participants struggle to improve their leaderboard LB scores by optimizing parameters and methods. com ml engine docs ai explanations overview 2 https en. imshow scratch cmap gray here is if we want to show the full reconstruction This structure will be needed to create the contest output Clean up data to prep for contest entry PANv00 Test on all 4 parquets to generate output and get a LB score. references 1 https cloud. com kaushal2896 bengali graphemes starter eda multi output cnn Courtesy Kaushal Shah saved trained model for use in this no GPU analysis kernel LEADERBOARD SCORE of 0. The only adjustment made for max pooling is the amount the displayed image is zoomed in so that it retains the original size of the input for comparison. read_csv Input data files are available in the. What can we conclude Note Re evaluated in light of fixes through version PANv12fIf you examine the eight 8 columns above we see the original 64x64 input in the leftmost column and moving left to right we see a recreation of the input based on the output of consecutive convolution layers Conv2D layers and dense layers. I therefore retract the suggestion that borders need to be added to pad the data. This Jupyter notebook proposes a third method to read the robot mind. In the all incorrect some important detail features are entirely lost by the third convolution column 4 which occurs after the first MaxPooling layer. remove the picture of the correct grapheme not relevant to the contest Make the correct classifications unsigned 8 bit integers the images are resized to 64 pixels square the images have only black and white one color channel read in an image from the storage directory and resize it Load the model trained in version 00 NOTE You must first select File then add or upload data from the public kernel available here https www. This is a natural impact of the initial scaling of images to 64x64 the MaxPooling layers as well as the strides parameter not used here. Now the notebook reads in a trained model from v00 and creates additional neural netowrks that are subsets of the trained NN in order to look into the internal representations of the convolutional layers. Perhaps others have come to the same conclusions and are already working similar efforts in which case I am also happy to help where I can. Nevertheless please feel free to reach out to me in this regard. In a recent AI Explainability Whitepaper 1 from Google feature attribution is stressed. org wiki AutoencoderIt also builds upon work I have published in US Patents and my 2013 PhD Dissertation Method and apparatus for developing a neural network for phoneme recognition US 5 749 066 Method and apparatus for interfacing and training a neural network for phoneme recognition US 5 809 462 2013 Signal Processing of EEG for the Detection of Attentiveness towards Short Training Videos https scholarscompass. One aspect of this optimization is the ability to peer into the internal representation of the trained neural network to let a human expert determine if important information has been lost or is being misinterpreted somewhere along the way. For example running this by clicking run or pressing Shift Enter will list all files under the input directory Any results you write to the current directory are saved as output. ", "id": "pnussbaum/grapheme-mind-reader-panv12-nogpu", "size": "8073", "language": "python", "html_url": "https://www.kaggle.com/code/pnussbaum/grapheme-mind-reader-panv12-nogpu", "git_url": "https://www.kaggle.com/code/pnussbaum/grapheme-mind-reader-panv12-nogpu", "script": "matplotlib.image keras.layers keras.models train_test_split pyplot pyplot as plt keras confusion_matrix get_dummies plot_model keras.preprocessing.image numpy seaborn ImageDataGenerator glob PIL.Image keras.callbacks tqdm keras.optimizers tensorflow tqdm.auto sklearn.model_selection pandas resize map matplotlib keras.utils sklearn.metrics ", "entities": "(('too much information', 'original image'), 'compare') (('that', 'convolutional layers'), 'read') (('We', 'image'), 'need') (('each', 'output'), 's') (('sufficient information', 'classification task'), 'determine') (('I', 'Short Training Videos https scholarscompass'), 'wiki') (('this', 'notebook'), 'support') (('also where I', 'case'), 'come') (('4 which', 'first MaxPooling layer'), 'lose') (('Examining', 'ThemselvesIt also filters'), 'be') (('size', 'odd numbered 3 5 7 etc'), 'have') (('Courtesy Kaushal cnn Shah', '0'), 'com') (('Jupyter notebook', 'robot mind'), 'propose') (('too when things', 'strides'), 'effect') (('which', 'data'), 'use') (('Naturally these', 'contest'), 'add') (('read_csv Input data files', 'the'), 'be') (('recreations', 'normalization'), 'Columns') (('correct examples', 'enough feature'), 'look') (('Basic Now s', 'correctly this'), 'show') (('This', 'MaxPooling 64x64 layers'), 'be') (('this', 'me'), 'be') (('it', 'comparison'), 'be') (('00 You', 'public kernel'), 'remove') (('it', 'future researchers'), 'help') (('some', 'very one'), 'seem') (('too when things', 'strides'), 'be') (('it', 'neural network'), 'see') (('that', 'models'), 'be') (('It', 'python docker image https kaggle github'), 'come') (('how pictures', 'parameters'), 'explain') (('you', 'output'), 'list') (('convolution This', 'new networks'), 'Visuallize') (('important information', 'somewhere way'), 'be') (('Possible solutions', 'higher resolution'), 'be') (('max when first pooling', '3 2nd'), 'notice') (('wonderful which', 'classification most greatly final coding'), 'be') (('we', 'convolution consecutive layers'), 'conclude') (('borders', 'data'), 'retract') (('contest output', 'LB score'), 'be') (('time full as I', 'effort'), 'be') (('It', 'neural network'), 'use') (('impact', 'convolution'), 'impact') "}