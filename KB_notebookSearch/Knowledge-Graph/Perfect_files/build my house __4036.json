{"name": "build my house ", "full_name": " h3 Import Libraries h1 Data Visualization on training test set h1 Feature Engineering h2 Onehot encoding on categorical data h1 Linear Regression h3 Train Test Split h3 Creating and Training the Model h2 confusion Recall Precision matrix function for performance checking h3 Model Evaluation h3 Predictions from our Model h3 Regression Evaluation Metrics h1 Gradient Boosting Regression h1 Decision Tree Regression h1 Support Vector Machine Regression h1 Random Forest Regression h1 Model Comparison ", "stargazers_count": 0, "forks_count": 0, "description": "sum axis 0 axis 0 corresonds to columns and axis 1 corresponds to rows in two diamensional array C. A logical understanding of loss function would depend on what we are trying to optimise. T 1 3 2 4 C. read_csv database connection Categorical boolean mask filter categorical columns using mask and turn it into alist i in range len categorical_cols data i data i. prediction with SVM Model linear algebra data processing CSV file I O e. We are going to see the error rate. RMSE is even more popular than MSE because RMSE is interpretable in the y units. Data Visualization on training test set Heatmap for train set Pair plot CDF And Pdf for yearbuilt feature LMPLOT for yearbuilt Box plot for GarageCars feature Feature EngineeringWe have to convert all columns into numeric or categorical data. We are trying to predict the sales prices by using a regression then the loss function would be based off the error between true and predicted house prices. which one is better Now we will use test data. Decision Tree Regression The decision tree is a simple machine learning model for getting started with regression tasks. sum axix 0 4 6 C C. While the AdaBoost model identifies the shortcomings by using high weight data points gradient boosting performs the same by using gradients in the loss function y ax b e e needs a special mention as it is the error term. We will toss out the Address column because it only has text info that the linear regression model can t use. sum axix 1 3 7 C. Background A decision tree is a flow chart like structure where each internal non leaf node denotes a test on an attribute each branch represents the outcome of a test and each leaf or terminal node holds a class label. The topmost node in a tree is the root node. Predictions from our Model Let s grab predictions off our test set and see how well it did Regression Evaluation Metrics Here are three common evaluation metrics for regression problems Mean Absolute Error MAE is the mean of the absolute value of the errors 1\ud835\udc5b \ud835\udc56 1\ud835\udc5b \ud835\udc66\ud835\udc56 \ud835\udc66 \ud835\udc56 Mean Squared Error MSE is the mean of the squared errors 1\ud835\udc5b \ud835\udc56 1\ud835\udc5b \ud835\udc66\ud835\udc56 \ud835\udc66 \ud835\udc56 2 Root Mean Squared Error RMSE is the square root of the mean of the squared errors 1\ud835\udc5b \ud835\udc56 1\ud835\udc5b \ud835\udc66\ud835\udc56 \ud835\udc66 \ud835\udc56 Comparing these metrics MAE is the easiest to understand because it s the average error. Not suited for large dataset because of it complexity Support Vector Machine Regression Random Forest Regression Model Comparison We can say the best working model by loking MSE rates The best working model is Support Vector Machine. The major difference between AdaBoost and Gradient Boosting Algorithm is how the two algorithms identify the shortcomings of weak learners eg. C 9 9 matrix each cell i j represents number of points of class i are predicted class j divid each element of the confusion matrix with the sum of elements in that column C 1 2 3 4 C. sum axis 1 axis 0 corresonds to columns and axis 1 corresponds to rows in two diamensional array C. MSE is more popular than MAE because MSE punishes larger errors which tends to be useful in the real world. The loss function is a measure indicating how good are model s coefficients are at fitting the underlying data. Let s get started Building my first Kernel Hope you like it Import LibrariesWhat is the data trying to say to us We are going to convert train ad test data. sum axis 0 1 4 2 6 3 4 4 6 representing A in heatmap format representing B in heatmap format print the intercept prepare models making dataframe. Train Test Split Now let s split the data into a training set and a testing set. All of these are loss functions because we want to minimize them. We will train out model on the training set and then use the test set to evaluate the model. Creating and Training the Model confusion Recall Precision matrix function for performance checking Model Evaluation Let s evaluate the model by checking out it s coefficients and how we can interpret them. see here for more details. And We are going to drop SalePrice column for predict. T 1 3 2 3 3 7 4 7 sum of row elements 1 divid each element of the confusion matrix with the sum of elements in that row C 1 2 3 4 C. sum axis 1 1 3 3 7 2 3 4 7 C. fillna we are going to scale to data This function plots the confusion matrices given y_i y_i_hat. Gradient Boosting Regression Gradient Boosting trains many models in a gradual additive and sequential manner. Filling NULL values Onehot encoding on categorical data Linear Regression Let s now begin to train out regression model We will need to first split up our data into an X array that contains the features to train on and a y array with the target variable in this case the Price column. ", "id": "jacckashakash/build-my-house", "size": "4036", "language": "python", "html_url": "https://www.kaggle.com/code/jacckashakash/build-my-house", "git_url": "https://www.kaggle.com/code/jacckashakash/build-my-house", "script": "Counter OneVsRestClassifier CalibratedClassifierCV train_test_split sklearn.calibration LinearRegression confusion_matrix scipy.sparse sklearn.utils sklearn.ensemble normalized_mutual_info_score mlxtend.classifier precision_recall_curve accuracy_score CountVectorizer numpy sklearn.svm seaborn cross_val_score TfidfVectorizer SGDClassifier ensemble RandomForestRegressor sklearn.manifold create_engine # database connection SVC sklearn.feature_extraction.text MultinomialNB hstack sklearn.neighbors GaussianNB sklearn.naive_bayes sklearn.metrics.classification plot_confusion_matrix sklearn.tree sklearn.linear_model sklearn auc matplotlib.pyplot sklearn.multiclass metrics defaultdict sklearn.model_selection sqlalchemy pandas shuffle DecisionTreeRegressor datetime RandomForestClassifier LogisticRegression roc_curve r2_score stopwords TruncatedSVD SVR model_selection nltk.corpus mean_squared_error KNeighborsClassifier StackingClassifier normalize TSNE sklearn.decomposition GridSearchCV sklearn.metrics collections StandardScaler log_loss sklearn.preprocessing ", "entities": "(('how we', 'them'), 'let') (('it', 'special mention'), 'perform') (('We', 'train ad test data'), 'let') (('linear regression model', 'that'), 'toss') (('B', 'dataframe'), 'axis') (('working best model', 'MSE rates'), 'suited') (('Decision Tree decision tree', 'machine learning regression simple tasks'), 'regression') (('how s coefficients', 'underlying data'), 'be') (('class j', 'column'), 'matrix') (('we', 'what'), 'depend') (('sum', 'two diamensional array'), 'corresond') (('function', 'y_i y_i_hat'), 'go') (('Gradient Boosting Regression Gradient Boosting', 'gradual additive'), 'train') (('We', 'predict'), 'go') (('it', 'squared errors'), 'let') (('topmost node', 'tree'), 'be') (('Gradient Boosting how two algorithms', 'learners weak eg'), 'be') (('sum', 'two diamensional array'), 'axis') (('prediction', 'SVM Model linear algebra data CSV file'), 'process') (('loss we', 'them'), 'be') (('Data Visualization', 'numeric categorical data'), 'set') (('i', 'range len categorical_cols data'), 'connection') (('loss then function', 'house prices'), 'base') (('Now s', 'training set'), 'let') (('RMSE', 'y units'), 'be') (('branch', 'terminal class label'), 'be') (('Now we', 'test data'), 'use') (('We', 'test then model'), 'train') (('that', 'Price column'), 'value') (('which', 'real world'), 'be') "}