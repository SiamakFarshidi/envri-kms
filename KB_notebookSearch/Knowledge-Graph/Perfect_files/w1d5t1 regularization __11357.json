{"name": "w1d5t1 regularization ", "full_name": " h1 Tutorial 1 Regularization techniques part 1 h1 Tutorial Objectives h1 Setup h2 Install dependencies h2 Figure Settings h2 Loading Animal Faces data h2 Loading Animal Faces Randomized data h2 Plotting functions h2 Set random seed h2 Set device GPU or CPU Execute set device h1 Section 0 Defining useful functions h1 Section 1 Regularization is Shrinkage h2 Video 1 Introduction to Regularization h2 Video 2 Regularization as Shrinkage h2 Coding Exercise 1 Frobenius Norm h1 Section 2 Overfitting h2 Video 3 Overparameterization and Overfitting h2 Section 2 1 Visualizing Overfitting h3 Animation Run Me h3 Plot the train and test losses h3 Think 2 1 Interpreting losses h4 Student Response h2 Section 2 2 Overfitting on Test Dataset h4 Validation Dataset h1 Section 3 Memorization h2 Data Visualizer h1 Section 4 Early Stopping h2 Video 4 Early Stopping h2 Coding Exercise 4 Early Stopping h2 Think 4 Early Stopping h3 Student Response h1 Summary h2 Airtable Submission Link h1 Bonus Train with randomized labels h2 Think Bonus Does it Generalize ", "stargazers_count": 0, "forks_count": 0, "description": "Read more here https pytorch. A model with big weights can fit more data perfectly whereas a model with smaller weights tends to underperform on the train set but can surprisingly do very well on the test set. Isn t it surprising to see that the NN was able to achieve 100 training accuracy on randomly shuffled labels This is one of the reasons why training accuracy is not a good indicator of model performance. 1 Visualizing OverfittingLet s create some synthetic dataset that we will use to illustrate overfitting in neural networks. If you have time left you can learn how a model behaves when is trained with randomized labels. Video 2 Regularization as ShrinkageOne way to think about Regularization is to think in terms of the magnitude of the overall weights of the model. Set the arguments Intialize the model Train the model train and test acc plot markdown Plotting them all together Run Me. We have learned about overfitting one of the worst caveats in deep learning and finally we learned a method of reducing overfitting in our models called early stopping. ANNs memorize some but generalize well3. Install dependencies Figure Settings Loading Animal Faces data Loading Animal Faces Randomized data Plotting functions Set random seed Executing set_seed seed seed you are setting the seed Set device GPU or CPU. Notice how we split the data. Section 2 Overfitting Time estimate 15 mins Video 3 Overparameterization and Overfitting Section 2. begin equation A _F sqrt sum_ i 1 m sum_ j 1 n a_ ij 2 end equation This is just a measure of how big the matrix is analogous to how big a vector is. com NeuromatchAcademy course content dl tree main tutorials W1D5_Regularization solutions W1D5_Tutorial1_Solution_1f125e8e. inform the user if the notebook uses GPU or CPU. Set the arguments Intialize the model Train the model Train and Test accuracy plot title Video 4 Early Stopping add event to airtable Fill in all missing code below. py Also it is interesting to note that sometimes the model trained on slightly shuffled data does slightly better than the one trained on pure data. named_parameters Click for solution https github. 6572162508964539 Apart from calculating the weight size for an entire model we could also determine the weight size in every layer. The test function takes in the current model after every epoch and calculates the accuracy on the test dataset. Hint Use functions model. org docs stable notes randomness. parameters or model. Section 4 Early Stopping Time estimate 20 mins Video 4 Early StoppingNow that we have established that the validation accuracy reaches the peak well before the model overfits we want to somehow stop the training early. imshow expects imput to be in numpy format and in the format P_x P_y 3 where P_x and P_y are the number of pixels along axis x and y respectively. The more complex the model the better it fits the training data but if it is too complex it generalizes less well it memorizes the training data but is less accurate on future test data. The train function takes in the current model along with the train_loader and loss function and updates the parameters for a single pass of the entire dataset. We have therefore hidden the code and shown the resulting outputs. py Random seed 2021 has been set. In this section we train three MLPs one each on 1. One then needs to regularize them to make the models fit complex enough but not too complex. Regularization as shrinkage of overparameterized models early stopping SetupNote that some of the code for today can take up to an hour to run. Frobenious norm of the modelFinally you can compare the Frobenius norm per layer in the model before and after training. Then we would be in serious trouble. Have a look how it works Using the last function calculate_frobenius_norm we can also obtain the Frobenius Norm per layer for a whole NN model and use the plot_weigts function to visualize them. If we overfit our training data there is always the evaluation on test data to keep us honest. Click for solution https github. Let s create an overparametrized Neural Network that can fit on the dataset that we just created and train it. First let s create the required dataloaders for all three datasets. com NeuromatchAcademy course content dl tree main tutorials W1D5_Regularization solutions W1D5_Tutorial1_Solution_c705db1a. Animal Faces Dataset2. 2 Overfitting on Test DatasetIn principle we should not touch our test set until after we have chosen all our hyperparameters. But if we overfit the test data how would we ever know Note that there is another kind of overfitting you do honest fitting on one set of images or posts or medical records but it may not generalize to other sets of images posts or medical records. t to train and test losses Where do you see the minimum of these losses What does it tell us about the model we trained Student Response Click for solution https github. Now let s define a model which has many parameters compared to the training dataset size and train it on these datasets. For this we can modify our calculate_frobenius_norm function as shown below. In case that DataLoader is used title Set device GPU or CPU. We train on a fraction of the dataset as it will be faster to train and will overfit more clearly. Were we to use the test data in the model selection process there is a risk that we might overfit the test data. Here we have classes cat dog wild. py Example output Think 4 Early StoppingDiscuss among your pod why or why not Do you think early stopping can be harmful for training your network Student Response Click for solution https github. Animation Run Me Plot the train and test losses Think 2. Before training our BigAnimalNet calculate the Frobenius norm again. Tutorial 1 Regularization techniques part 1 Week 1 Day 5 Regularization By Neuromatch Academy __Content creators __ Ravi Teja Konkimalla Mohitrajhu Lingan Kumaraian Kevin Machado Gamboa Kelson Shilling Scrivo Lyle Ungar__Content reviewers __ Piyush Chauhan Siwei Bai Kelson Shilling Scrivo__Content editors __ Roberto Guidotti Spiros Chavlis__Production editors __ Saeed Salehi Spiros Chavlis Our 2021 Sponsors including Presenting Sponsor Facebook Reality Labs Tutorial Objectives1. com NeuromatchAcademy course content dl tree main tutorials W1D5_Regularization solutions W1D5_Tutorial1_Solution_8f1d49a8. html Call set_seed function in the exercises to ensure reproducibility. Validation DatasetA common practice to address this problem is to split our data in three ways using a validation dataset or validation set to tune the hyperparameters. A Completely Noisy Dataset Random Shuffling of all labels 3. Execute set_device especially if torch modules used. Frobenius norm per layer before and after training Section 2. title Tutorial slides markdown These are the slides for the videos in this tutorial title Install dependencies generate airtable form Imports title Figure Settings title Loading Animal Faces data title Loading Animal Faces Randomized data title Plotting functions unnormalize title Set random seed markdown Executing set_seed seed seed you are setting the seed for DL its critical to set the random seed so that students can have a baseline to compare their results to expected results. Early stopping stops training when the validation accuracies stop increasing. Now that we have finished training let s see how the model has evolved over the training process. Network Class Animal Faces sum up batch loss get the index of the max log probability title Video 1 Introduction to Regularization add event to airtable title Video 2 Regularization as Shrinkage add event to airtable Fill in all missing code below. Execute set_device Section 0 Defining useful functionsLet s start the tutorial by defining some functions which we will use frequently today such as AnimalNet train test and main. Set the arguments Initialize the network Train the network Train and Test accuracy plot markdown Frobenius norm for AnimalNet before and after training Choose the datapoint you would like to visualize choose that datapoint using index and permute the dimensions and bring the pixel values between 0 1 Convert the torch tensor into numpy Call the function Here we have 100 completely shuffled train data. You should have also observed from the above plots that the train test loss on real data is not very smooth and hence you might guess that the choice of epoch can play a very large role on the val test accuracy of your model. then remove or comment the line below to test your function raise NotImplementedError Define calculate_frobenius_norm function Sum the square of all parameters Take a square root of the sum of squares of all the parameters add event to airtable Seed added for reproducibility Frobenius Norm per Layer initialization of variables Sum all the parameters Take a square root of the sum of squares of all the parameters Plots the weights Creates a new model Calculates the forbenius norm per layer Plots the weights title Video 3 Overparameterization and Overfitting add event to airtable creating train data input output adding small error in the data visualizing trian data creating test dataset Network Class 2D train the network on toy dataset Calculates frobenius before training initializing variables losses model norm Initializing variables to store weights frobenius norm per epoch training title Animation Run Me create a figure and axes organizing subplots title Plot the train and test losses title Student Response markdown Frobenious norm of the model Change title to Frobenious norm of the model markdown Frobenius norm per layer before and after training Dataloaders for the Dataset defining number of examples for train val test using pathlib to be compatible with all OS s Dataloaders for the Original Dataset For reproducibility Creating train_loader and Val_loader Dataloaders for the Random Dataset For reproducibility splitting randomized data into training and validation data using pathlib to be compatible with all OS s Randomized train and validation dataloader Dataloaders for the Partially Random Dataset For reproducibility Splitting data between training and validation dataset for partially randomized data using pathlib to be compatible with all OS s Training and Validation loader for partially randomized data Network Class Animal Faces Here we have 100 true train data. This week we use the sum of Frobenius Norm of all the tensors in the model as a measure of the size of the model. You should see that the value of weights increases over the epochs. then remove or comment the line below to test your function raise NotImplementedError Complete the early_stopping_main function Number of successive epochs that you want to wait before stopping training process in number of epcos Keps track of number of epochs during which the val_acc was less than best_acc train the model calculate training accuracy calculate validation accuracy add event to airtable Set the arguments Initialize the model title Student Response title Airtable Submission Link Here we have 15 partially shuffled train data. com NeuromatchAcademy course content dl tree main tutorials W1D5_Regularization solutions W1D5_Tutorial1_Solution_62d845ba. Then run the code below to validate your implementation. Frobenius Norm of Single Linear Layer 0. Coding Exercise 4 Early StoppingReimplement the main function to include early stopping as described above. Section 1 Regularization is Shrinkage Time estimate 20 mins Video 1 Introduction to RegularizationA key idea of neural nets is that they use models that are too complex complex enough to fit all the noise in the data. This is bad however because it will mean that the model will fail when presented with new data. Shuffling some of the data is a form of regularization one of many ways of adding noise to the training data. Airtable Submission Link Bonus Train with randomized labelsIn this part let s train on a partially shuffled dataset where 15 of the labels are noisy. Having the weights too small can also be an issue as it can then underfit the model. A partially Noisy Dataset Random Shuffling of 15 labels Now think for a couple of minutes as to what the train and test accuracies of each of these models might be given that you train for sufficient time and use a powerful network. Section 3 Memorization Time estimate 20 mins Given sufficiently large networks and enough training Neural Networks can achieve almost 100 train accuracy by remembering each training example. First let s build the model architecture Next let s define the different parameters for training our model At this point we can now train our model. Now train our BigAnimalNet model Frobenius norm for AnimalNet before and after training Data VisualizerBefore we train the model on a data with random labels let s visualize and verify for ourselves that the data is random. Ideally we would only touch the test data once to assess the very best model or to compare a small number of models to each other real world test data is seldom discarded after just one use. 1 Interpreting lossesRegarding the train and test graph above discuss among yourselves What trend do you see w. Now let s train the network on the shuffled data and see if it memorizes. com NeuromatchAcademy course content dl tree main tutorials W1D5_Regularization solutions W1D5_Tutorial1_Solution_683d27d3. Plotting them all together Run Me Think Bonus Does it Generalize Given that the Neural Network fit memorize the training data perfectly Do you think it generalizes well What makes you think it does or doesn t Click for solution https github. py SummaryIn this tutorial you have been introduced to the regularization technique where we have described it as shrinkage. py Now let s visualize the Frobenious norm of the model as we trained. Big ANNs are efficient universal approximators due to their adaptive basis functions2. Coding Exercise 1 Frobenius NormBefore we start let s define the Frobenius norm sometimes also called the Euclidean norm of an m n matrix A as the square root of the sum of the absolute squares of its elements. ", "id": "joseguzman/w1d5t1-regularization", "size": "11357", "language": "python", "html_url": "https://www.kaggle.com/code/joseguzman/w1d5t1-regularization", "git_url": "https://www.kaggle.com/code/joseguzman/w1d5t1-regularization", "script": "torch.nn.functional set_device zipfile IPython.display early_stop_plot frame seed_worker numpy train main set_seed Net(nn.Module) BiliVideo(IFrame) matplotlib.animation on_button_clicked torch.nn IPython evaltools.airtable tqdm transforms AirtableForm torchvision matplotlib.pyplot IFrame tqdm.auto ImageFolder forward display as IPydisplay print_function visualize_data ipywidgets torch.optim __future__ test HTML clear_output widgets imshow __init__ BigAnimalNet(nn.Module) display early_stopping_main torchvision.datasets calculate_frobenius_norm AnimalNet(nn.Module) plot_weights YouTubeVideo ZipFile ", "entities": "(('we', 'train frequently today such AnimalNet test'), 'section') (('perfectly model', 'test surprisingly very well set'), 'fit') (('sometimes model', 'pure data'), 'py') (('we', 'test data'), 'be') (('torch especially modules', 'set_device'), 'Execute') (('Student Response Airtable Submission Here we', 'train 15 partially shuffled data'), 'remove') (('P_y 3 where P_x', 'x'), 'expect') (('we', 'now model'), 'let') (('some', 'hour'), 'regularization') (('week we', 'model'), 'use') (('we', 'layer'), '6572162508964539') (('we', 'hyperparameters'), 'overfitting') (('Neural enough Networks', 'training example'), 'estimate') (('we', 'us'), 'be') (('data', 'ourselves'), 'train') (('common practice', 'hyperparameters'), 'be') (('which', 'datasets'), 'let') (('you', 'when randomized labels'), 'learn') (('we', 'calculate_frobenius_norm function'), 'modify') (('test function', 'test dataset'), 'take') (('finally we', 'models'), 'learn') (('we', 'solution https github'), 't') (('accuracy plot Train 4 Early Stopping', 'airtable Fill'), 'Set') (('where we', 'shrinkage'), 'py') (('model', 'when new data'), 'be') (('we', 'just it'), 'let') (('how model', 'training process'), 'let') (('early stopping', 'Student Response solution https github'), 'think') (('value', 'epochs'), 'see') (('Ideally we', 'seldom just one use'), 'touch') (('it', 'shuffled data'), 'let') (('fit', 'then them'), 'need') (('Video 2 Regularization', 'model'), 'be') (('choice', 'model'), 'observe') (('train function', 'entire dataset'), 'take') (('We', 'resulting outputs'), 'hide') (('we', 'one 1'), 'train') (('we', 'neural networks'), '1') (('less well it', 'test less future data'), 'model') (('we', 'model'), 'let') (('Big ANNs', 'basis efficient universal due adaptive functions2'), 'be') (('medical it', 'images posts'), 'know') (('you', 'w.'), 'interpret') (('students', 'expected results'), 'slide') (('Shuffling', 'training data'), 'be') (('DataLoader', 'case'), 'in') (('it', 'dataset'), 'train') (('Here we', 'train 100 true data'), 'remove') (('you', 'seed'), 'randomized') (('training why accuracy', 'model good performance'), 't') (('you', 'before training'), 'norm') (('notebook', 'GPU'), 'inform') (('where 15', 'labels'), 'let') (('that', 'data'), 'be') (('s', 'elements'), 'let') (('how vector', 'equation _ F sqrt sum'), 'begin') (('Shrinkage', 'airtable Fill'), 'sum') (('it', 'solution https github'), 'plot') (('you', 'powerful network'), 'think') (('too small also it', 'then model'), 'be') (('Here we', 'train 100 completely shuffled data'), 'initialize') (('First s', 'three datasets'), 'let') (('we', 'somehow training'), 'estimate') (('we', 'them'), 'obtain') "}