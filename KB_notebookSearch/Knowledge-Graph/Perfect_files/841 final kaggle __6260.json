{"name": "841 final kaggle ", "full_name": " h2 Packages Helper Functions h2 File Path Operations if using images without Stain norm h2 Stain Normalization done pre run h2 Data Loaders Sampling Normalization Setup h4 torch datasets and dataloaders h4 Balancing strategies using samplers h4 Visualization of patch distribution per patient done pre run h4 Find means and sds for all channels done pre run h2 Patch Classifier Selection Training h4 DenseNet121 ResNet18 VGG11 h4 Optimizer Loss function h4 Training with early stopping learning rate scheduler h2 Patch level Inference Postprocessing h3 If only postprocessing Download checkpoints of all runs h3 Get patch level prediction performance feature vectors for image level test h3 Get feature vectors labels for image level training h2 Image Classifier with Max Vote SVM XGB h3 Max Voting h3 SVM votes h3 XGB votes h3 SVM features h3 XGB features h2 References ", "stargazers_count": 0, "forks_count": 0, "description": "So for each patient we count the three way votes and divide them by the sum of votes. edu mn sites default files macenko2009. And to balance with respect to patients we implement our own sampler by modifying the script by 2. Accessed 21 Dec 2021 This cell imports helper functions from the linked dataset histopathsn custom pytorch dataset class reset the layers in the network add a patient column for the metadata split train and validation by creating two subset copies of the annotation csvs Max 512 add simple augmentations to the training set for all datasets perform resize and normalization with means stds pre computed means and stds differ across stain normalized patches and normal patches get the data loaders if balancing strategy class Downsample majority class and oversample minority class if balancing strategy patient Downsample majority patients and oversample minority patients specify model_name choose among ResNet VGG or DenseNet use gpu if available freeze up lower level layers freeze up lower level layers freeze up lower level layers create an optimizer object mean squared error loss get the val accu history for plotting validate at the end of epoch update the early stopper and record val accuracy print end of epoch results if early stopper says stop then stop update scheduler register the final layer before classifier so that the model outputs that layer as well during loader iterations The outputs from this layer would serve as the feature vector for the non neural model to learn predict get the best model of all training epochs prepare patch level statistics for test for the non neural model prepare patch level statistics for training for the non neural model. 1 Say a patient has m patches then the patch classifier would output m labels that we call votes each label could be 0 1 or 2. Sun Deep Residual Learning for Image Recognition 2015. 2 Another method is to extract a feature vector for each patch using the final layer of the convolutional network right before the fully connected layers. Yang Imbalanced Dataset Sampler GitHub 2021. Accessed 21 Dec 2021 8 T. com ufoym imbalanced dataset sampler. After that we reduce the learning rate and increase the number of trees to get the final XGBoost classifier on the feature vectors. van der Maaten and K. Koyama Optuna A Next generation Hyperparameter Optimization Framework 2019. However with experiments we discover that running optuna on Colab and Kaggle kernels might generate different results. Thomas A Method for Normalizing Histology Slides for Quantitative Analysis University of North Carolina Chapel Hill 2009. Available https www. For both the training and test sets we perform the same postprocesses and only pass the results from the training side to the patient classifier. the patch classifier. com content_cvpr_2017 papers Huang_Densely_Connected_Convolutional_CVPR_2017_paper. Both are tree level bootstrapping hyperparameters. Guestrin XGBoost A Scalable Tree Boosting System 2016. pdf Data Loaders Sampling Normalization Setup torch datasets and dataloaders Balancing strategies using samplersTo balance with respect to target labels we adopt an external script for the sampler written by 2. Tizhoosh Fine Tuning and Training of DenseNet for Histopathology Image Representation Using TCGA Diagnostic Slides 2021. Initially fix some main hyperparameters tune max_depth and min_child_weight firstThen we tune the gamma minimum loss reduction required to make a split Then we tune subsample proportion of dataset in the bootstrapped data and colsample_bytree proportion of features in the bootstrapped data. Finally we tune alpha which is the L1 regularization factor. Packages Helper Functions File Path Operations if using images without Stain norm Stain Normalization done pre run Should be conducted with both training test sets. Available http wwwx. DenseNet121 ResNet18 VGG11Original paper of DenseNets 3 The selection of layers to freeze is based on 4 Original paper of ResNets 5 Original paper of VGG networks 6 Optimizer Loss function Training with early stopping learning rate scheduler Patch level Inference PostprocessingAfter training the patch classifier we d like to extract info from the well trained supposedly patch classifier to train a patient or image level classifier. Available https github. Weinberger Densely Connected Convolutional Networks 2016. org kdd2016 papers files rfp0697 chenAemb. Normalize functions in the data loader cell for the outputs Patch Classifier Selection TrainingChoose one of the following models as the patch level classifier. So we then compute the element wise average of the feature vectors so that regardless of how many patches a patient has their input for the patient classifier would always be of size L. Original paper 1 http wwwx. Now for a patient with m patches they would have three numbers as the input into the patient classifier m_1 m m_2 m m_3 m with m_i being the vote count for the i th label. Available https arxiv. Accessed 21 Dec 2021 6 K. Zisserman Very Deep Convolutional Networks for Large Scale Image Recognition 2015. Accessed 21 Dec 2021 7 T. Accessed 21 Dec 2021 2 M. Find means and sds for all channels done pre run See the torchvision. To rerun the three cells below simply choose one balancing strategy and create train_loader in the cell above. Search spaces are intuitively determined. If only postprocessing Download checkpoints of all runs Get patch level prediction performance feature vectors for image level test Get feature vectors labels for image level training Image Classifier with Max Vote SVM XGB Max VotingReturns the label that has the maximum votes in each patient SVM votes Runs SVM on the votes. Accessed 17 Dec 2021 3 G. The x axis represents different patients and the y axis is the number of patches. XGB votes Runs XGBoost on the votes Original paper of XGBoost 7 SVM features Runs SVM on the feature vectors XGB features Runs XGBoost on the feature vectors We use a random search package optuna 8 and stratified k fold to tune hyperparameters for the XGBoost classifier. org openaccess content_cvpr_2016 papers He_Deep_Residual_Learning_CVPR_2016_paper. Visualization of patch distribution per patient done pre run Here we plot bar graphs of the number of patches per patient. There are two approaches to extract such info. For a patient with m patches they will have m feature vectors with length L dependent on the CNN model i. Accessed 21 Dec 2021 4 A. Accessed 21 Dec 2021 5 K. Available https openaccess. ", "id": "jaredfeng/841-final-kaggle", "size": "6260", "language": "python", "html_url": "https://www.kaggle.com/code/jaredfeng/841-final-kaggle", "git_url": "https://www.kaggle.com/code/jaredfeng/841-final-kaggle", "script": "ImageData objective torch.backends.cudnn DataLoader *  # custom pytorch dataset class pyplot pyplot as plt EarlyStopper confusion_matrix ImbalancedDatasetSampler xgboost accuracy_score numpy sklearn.svm XGBClassifier seaborn torchvision.io tune4 SVC nn tune3 BalancingPatientPatchSampler roc_auc_score CustomSampler summary transforms read_image torchvision hook StratifiedKFold sklearn.model_selection pandas torchsummary tune1 time matplotlib torch.utils.data Dataset get_activation * tune2 optim torch torchsampler sklearn.metrics reset_parameters ", "entities": "(('Here we', 'patient'), 'do') (('Tizhoosh Fine Tuning', 'TCGA Diagnostic Slides'), 'use') (('Find', 'torchvision'), 'do') (('learning rate scheduler Patch level Inference early stopping PostprocessingAfter', 'level patient classifier'), 'paper') (('they', 'CNN dependent model'), 'have') (('Stain norm Stain Normalization', 'training test pre sets'), 'conduct') (('input', 'label'), 'have') (('running optuna', 'Kaggle different results'), 'discover') (('x axis', 'y patches'), 'represent') (('we', 'feature vectors'), 'reduce') (('patch level statistics', 'non neural model'), 'access') (('input', 'always size'), 'compute') (('that', 'votes'), 'get') (('we', '2'), 'balance') (('Both', 'tree hyperparameters'), 'be') (('org kdd2016 papers', 'rfp0697 chenAemb'), 'file') (('We', 'XGBoost classifier'), 'Runs') (('we', 'patient classifier'), 'perform') (('which', 'alpha'), 'tune') (('we', '2'), 'dataset') (('Then we', 'bootstrapped data'), 'fix') (('method', 'right fully connected layers'), '2') (('label', 'votes'), 'say') (('we', 'votes'), 'count') "}