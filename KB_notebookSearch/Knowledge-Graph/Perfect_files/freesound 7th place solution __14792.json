{"name": "freesound 7th place solution ", "full_name": " h1 Freesound Audio Tagging 2019 h2 Contents h2 Data Augmentation h3 pitch shift h3 Fade h3 Reverb h3 Treble and Bass h3 Equalizing h2 Crop Policy h3 Uniform Crop h3 Strength Adaptive Crop h2 Model Structure h3 Custom CNN h4 kernel size h4 The nuber of convolution blocks h4 Cutting output feature map to arbitrary size h3 Inception v3 h2 Augmentation in Batch h3 blend image augmentation h2 Training Strategy h3 Pretrain with Noisy Data h3 Training with Curated Data h3 TTA for Validation h2 Ensemble h2 What didn t work ", "stargazers_count": 0, "forks_count": 0, "description": "We especially want to thank daisukelab https www. Ensemblemany models Inception CustomCNN v1 CustomCNN v2 Se ResNeXt DenseNet sec to make time steps 128 500 cent pitch down 500 cent pitch up fade in and out cyclically emphasize weaken Stack X as X X X Standardize Normalize to 0 255 Just zero all 0 batch_size feature_maps time_stpes batch_size feature_maps Attention Layers changed kernel size we don t know why but this is better than 3 3 kernels. In this block we ll get the primitive pattens connected in the frequency axis. Spatial and Channel Squeeze and Exitation Added both pool. EqualizingThis augmentation adjust custom frequency range. But that isn t the case when the degree of the translation is not so big. We tried both policy and found it surprising that policy 2 was slightly better than policy 1. We tried to crop the place where the total of db is high preferentially. Validation Stratified K fold for Multilabel Models DenseNet XceptionNet SE Inception ResNet ResNeXt SE ResNeXt MobileNet ConvLSTM CapsuleNet We ve tried many models but we didn t find them better than Custom CNN or Inception v3 model. The cycle was 10 epochs. Note that this score was calcurated with Custom CNN model and the effect of the augmentation varies between models. We created a pitch augmented dataset from train_noisy dataset and used it only for training. We altered the model structure like the model below. Especially since we use random resized crop tta is very important. html additivegaussiannoise SpecAugment http ai. Split noisy data with train_test_split. Third Convolution Block channel 64 128 1 3 pooling. 5 ConvBlocks Changed pooling policy. At first it wa cropped uniformly as mhiro s kernel. 04 when learning only the last few seconds. We used normal KFold to get cross validation and split the dataset into 5 subsets. Unfortunately we don t know how much the LB lwlrap score improved with Custom CNN but the LB lwlrap score of Inception v3 improved 0. com c freesound audio tagging 2019 discussion 89382 latest 534349. com mhiro2 for sharing excellent training framework and sailorwei https www. Cutting output feature map to arbitrary sizeWe found that the position in the feature map is important when treating spectrogram. Train with all the noisy data validate with curated data. co IgUKfFOYaa moving the pattern in the frequency can be problematic. Pretrain with Noisy DataWe used noisy data to pre train our model. Clearly spectrogram has different axis and we need to treat them separately. This block allows patterns to move in the time axis. html All the above had no impact on CV. png Thank you for all the competitors kaggle teams and the host of this competition We enjoyed a lot during the competition and learned many things. Here s the table which summarize how much each augmentation worked on the score. 88 with Custom CNN and 0. This is because we designed our model considering the meaning of each blocks although we re not sure whether it is working as we imaged originally. We simply used the same indices for splitting original dataset and augmented datasets. EnsembleWe end up with simply blending the output with the same weight which means CustomCNN Inception v3 3ch Inception v3 1ch 3 but we tried a lot to use many models with comparatively low scores around 0. We prepare strength_list in advance and apply strength adaptive crop in FATTrainDataset. 03247 OctaveConv https arxiv. Ensemble Ensemble Data AugmentationWe created augmented training dataset with sox http sox. io en stable reference Image. com 2019 04 specaugment new data augmentation. This block allows patterns to move in the frequency axis a little. com c tensorflow speech recognition challenge forums t 47715 2nd place solution forumMessageId 270205 post270205The design philosophy behind the placement of convolution blocks are as follows. We used sox from Python using subprocess package and created augmented features with kaggle kernels. The original audio data and its melspectrogram representation are below. Some sample has it in the middle of crip. com kaggle competitions kaggle 10700 logos header. com daisukelab cnn 2d basic solution powered by fast ai https www. htmlSo We got mixed Image dataWe define below function to apply to above strategy in our training Training StrategyThis is our pipline. We applied both pitch up and pitch down. With pre training local lwlrap jumped from 0. io en latest source augmenters. We put this block to collect primitive patterns of the log melspectrogram. Data Augmentation Data_Augmentation pitch shift pitch_shift fade Fade reverb Reverb treble and bass Treble_and_Bass equalizing Equalizing 2. All the works for this competition were done with kaggle kernels and we d like to thank a lot to kaggle for giving us great opportunities and computing resources. In the stage2 pre training we didn t changed any settings except for the initial weight of the network. We used mhiro2 s way https www. One thing to note is that we didn t used augmented dataset for validation. The score drops about 0. We had two policy to train and validate with noisy data. 005 upThis sample has important information earlier but there is a little information from the end of sound. com sailorwei fat2019 2d cnn with mixup lb 0 673We ve created a kernel to explain team kaggler ja Shirogane s solution. You can see how we did this in this kernel https www. Model Structure Custom CNNAt first we worked a lot on our custom CNN model. The original CVSSP Baseline model structure is below. The same pattern at the bottom of the image and at the top of the image are totally different. And the transformed audio data is like this. Here s one of the kernel which created some of the augmented features with train_curated dataset. So we used tta for validation to ensure that validation can be properly evaluated. pitch shiftThis augmentation changes the pitch of the sound. We used those augmentation which certainly alter what it sounds like but still can be recoginized as the same class. com daisukelab for his clear instruction with great kernels and datasets mhiro2 https www. com daisukelab fat2019_prep_mels1 We used noisy data for warm up just like pre training the model and curated data for fine tuning the model which was trained in earlier stage with noisy data. Fourth Convolution Block channel 128 256 4 1 pooling. Training with Curated DataWe used curated data to finetune the models which were pre trained with noisy data. com hidehisaarai1213 freesound dameoshi aug fold5 s one of the kernel to conduct learning with curated data. get mix data blend_arr_x blend_arr_y idx_ex _get_blend_arr x_trn y_trn lam np. pdf to catch the local features. 87 with Inception v3. With this trick CV lwlrap score improved 0. Random crop got a little better cv than the first 2 seconds cropped. When we train the model with curated data we used all of these augmented features so the size of the train dataset was 8 times larger 7 augmentations 1 original than the original curated dataset. We d like to appreciate to phalanx https www. Both train and validate are done with only noisy data. Finaly we decided to use 20 TTA for each model and blend 5 fold 3models 15model with equally weighted manner. We can use sox like this. Fifth Convolution Block channel 256 512 1 3 pooling. One is all the same as the original Inception v3 architecture so we simply used the one in torchvision. As explained in this blog post https t. TTA for ValidationIt is very effective to use tta because we use random crop. pdf We thought that the position in the image was very important in this competition and tried CoordConv because it says it can handle the position but we found it not useful. com ybabakhin kaggle_salt_bes_phalanx. The degree of pitch shift was fixed to 500 cent. score augmentation no aug no warm up pitch down fade reverb treble and bass down no warm up pitch down fade reverb treble and bass down all CV we don t remember 0. Augmentation in Batch blend image augmentationWe used below augmentation strategy in batchhttps pillow. We believe it was just a seed magic but the model trained with policy12 were at least as better as the model trained with policy1. We used Log Melspectrogram features made with the manner introduced by daisukelab in his great dataset. We should not allow patterns translate along the frequency axis. This method is very effective because most samples contain important information in places where the sound is loud. Also we faced the problem of Blend Many Models vs Increasing the number of TTA. We also tried OctaveConv but couldn t find it useful for this competition. Cutting the feature map to arbitrary size. Here s the types of augmentation. We added some modification in the ConvBlock and the model structure. Strength Adaptive CropMany sound crip has important information at the first few seconds. com mhiro2 simple 2d cnn classifier with pytorch to train which means change learning rate with CosineAnnealingLR cyclickally in the range between 1e 3 and 1e 5. kernel sizeWe don t know why but the size 2 2 was the best. Freesound Audio Tagging 2019 Logo https storage. In short our solution is 3 models ensemble one is Custom CNN and the other two are Inception v3 models. com knjcode pytorch finetuner GaussianNoise https imgaug. Since the size of each augmented dataset is all the same as the original one which means 4970 and the order of them are all the same it was easy to avoid data leak. We borrowed nice implementation from https github. We changed kernel size number of Convolution Blocks Pooling policy and the structure of the fully connected layer. Then We introduce Strength Adaptive Crop. In the pre training of Inception models we applied RandomResizedCrop to the data and it required us to train longer so we introduced stage 2 pre training. Second Convolution Block channel 32 64 4 1 pooling. com hidehisaarai1213 convnet noisy all pitch pretraining. com phalanx for these implementations. 720 We didn t checked all the score improvement of Inception v3 model with these augmentation but between no aug and with aug there exist around 0. The reason is simple the size was to large. Crop Policy Crop_Policy Uniform Crop Uniform_Crop Strength Adaptive Crop Strength_Adaptive_Crop 3. We started from improving CVSSP Baseline https www. We applied these augmentations to the whole curated data. Sox is an open source synthesis tool which is easy to use from command line interface and is installed to Kaggle kernel by default. Treble and BassThis augmentation emphasize or weaken treble and bass range of the sound. We used augmented dataset with original train_curated dataset which made the whole training dataset 8 times larger than the original one. We found omalleyt https www. In this block we ll get the primitive patterns connected to the patterns nearby in the time axis. Inception v3We used two Inception v3 models. When RandomResizedCrop is used val score fluctuate so if val tta is not used an appropriate epoch can not be selected. 725 Crop Policy Uniform CropWe use random crop because we use fixed image size 128 128. That is a little bit different from treating natural images. Now check the result of audio transformation. None of us had experience with audio data and we also didn t get used to use Nerual Networks. Augmentation in Batch Augmentation_in_Batch blend image augmentation blend_image_augmentation 5. score augmentation no aug pitch down pitch up fade reverb treble and bass down treble and bass up equalize pitch down fade reverb treble and bass down all CV no warm up 0. We were helped a lot with public kernels below shared by them. com mhiro2 simple 2d cnn classifier with pytorch https www. com omalleyt s 2nd place solution for Tensorflow Speech Recognition Competition very helpful. Specific CNN structures CoordConv https arxiv. ReverbThis augmentation add reverberation to the original sound. 825 CV warm up 0. With careful experiment we found that Inception model was comparatively unstable and requires comparatively large number of TTA which was around 20 25 times. We only used train_noisy dataset for validation. com hidehisaarai1213 freesound sox pitch fade reverb treble basWe created 7 augmented datasets whose size are 4970 each which means the same size as curated one. The difference from the original model is. The other one was 1ch version of the original Inception v3 so we just simply altered the first layer. optimize BCEWithLogitLossWe also used augmented data in this stage. We took careful not to aggregate the feature map along to the time axis and to the frequency axis simultaneously. First Convolution Block channel 1 32 no pooling. We also added spatial and channel attention. However due to the nature of recording it is rare to have important information at th end of sounds. Model Structure Model_Structure Custom CNN Custom_CNN Inception v3 Inception_v3 4. FadeThis augmentation alter the volume of the sound. What didn t work Augmentation CutMix https github. Training Strategy Training_Strategy Pretrain with Noisy Data Pretrain_with_Noisy_Data Training with Curated Data Training_with_Curated_Data TTA for Validation TTA_for_Validation 6. The score of the 1ch version was not as stable as 3ch version but we found that model a little bit different from the original one. com sailorwei for showing his Inception v3 model in his public kernel. Xception model may have been better but we couldn t train it properly as we didn t have enough time to try it in the last period of the competition. The nuber of convolution blocksWe used 5 blocks to get 512 channels feature map. To treat the position in the feature map we used spatial pyramid pooling https arxiv. We also applied pitch shift to the noisy data. ", "id": "hidehisaarai1213/freesound-7th-place-solution", "size": "14792", "language": "python", "html_url": "https://www.kaggle.com/code/hidehisaarai1213/freesound-7th-place-solution", "git_url": "https://www.kaggle.com/code/hidehisaarai1213/freesound-7th-place-solution", "script": "torch.nn.functional show_before_and_after audio_to_melspectrogram show_melspectrogram pathlib _get_strength_list transform_and_compare _get_strength IPython.display array2img mono_to_color GAB(nn.Module) master_bar Cnn_9layers_AvgPooling(nn.Module) Image numpy ImageOps progress_bar init_weights SCse(nn.Module) FATTrainDataset(Dataset) subprocess Path read_as_melspectrogram conf init_weight _get_blend_arr FATValidDataset(Dataset) tqdm_notebook torch.nn tqdm ConvBlock(nn.Module) convert_wav_to_image matplotlib.pyplot read_audio forward PIL pandas fastprogress torch.utils.data __len__ Dataset ConvNet(nn.Module) __init__ __getitem__ SpatialAttention2d(nn.Module) ", "entities": "(('how we', 'kernel https www'), 'see') (('We', 'great dataset'), 'use') (('com kaggle competitions', 'logos 10700 header'), 'kaggle') (('We', 'https www'), 'd') (('We', 'https github'), 'borrow') (('ReverbThis augmentation', 'original sound'), 'add') (('which', 'train_curated dataset'), 's') (('nuber', 'channels feature 512 map'), 'use') (('when degree', 'translation'), 't') (('We', 'axis frequency'), 'take') (('we', 'network'), 'change') (('patterns', 'time'), 'allow') (('Then We', 'Strength Adaptive Crop'), 'introduce') (('which', 'noisy data'), 'use') (('We', 'daisukelab https especially www'), 'want') (('whole training', '8 times original one'), 'use') (('LB lwlrap score', 'Inception v3'), 'know') (('We', 'only training'), 'create') (('we', 'torchvision'), 'be') (('We', 'whole curated data'), 'apply') (('we', 'validation'), 'be') (('effect', 'models'), 'note') (('so size', '8 times 7 augmentations 1 original original curated dataset'), 'be') (('dameoshi aug hidehisaarai1213 fold5', 'curated data'), 'com') (('We', 'many things'), 'thank') (('all it', 'data leak'), 'be') (('we', 'little bit different original one'), 'be') (('com place 2nd solution', 'Tensorflow Speech Recognition Competition'), 'helpful') (('We', 'CVSSP Baseline https www'), 'start') (('Inception v3We', 'Inception v3 two models'), 'use') (('pitch', 'CV'), 'augmentation') (('seed just model', 'policy1'), 'believe') (('We', 'model'), 'alter') (('where total', 'db'), 'try') (('we', 'Nerual Networks'), 'have') (('We', '5 subsets'), 'use') (('which', 'noisy data'), 'daisukelab') (('transformed audio data', 'this'), 'be') (('difference', 'original model'), 'be') (('Random crop', 'seconds first 2 cropped'), 'get') (('optimize BCEWithLogitLossWe', 'stage'), 'use') (('com', 'public kernel'), 'sailorwei') (('We', 'kaggle kernels'), 'use') (('We', 'also spatial attention'), 'add') (('Pretrain', 'model'), 'use') (('wa', 'uniformly kernel'), 'crop') (('Spatial Squeeze', 'pool'), 'add') (('first we', 'custom CNN lot model'), 'work') (('we', 'pooling https spatial pyramid arxiv'), 'use') (('We', 'validation'), 'use') (('longer so we', 'pre stage 2 training'), 'in') (('Third Convolution Block', '64 128 1 3 pooling'), 'channel') (('Fifth Convolution Block', '256'), 'channel') (('sample', 'crip'), 'have') (('it', 'sounds'), 'be') (('0 ve', 'solution'), 'sailorwei') (('policy', '2 slightly policy'), 'try') (('Treble', 'sound'), 'emphasize') (('We', 'below them'), 'help') (('FadeThis augmentation', 'sound'), 'alter') (('which', '1e'), 'com') (('same pattern', 'image'), 'be') (('we', 'don 0'), 'augmentation') (('which', 'one'), 'com') (('it', 'position'), 'pdf') (('degree', '500 cent'), 'fix') (('patterns', 'frequency'), 'allow') (('it', 'still same class'), 'use') (('upThis sample', 'sound'), '005') (('augmentation', 'sound'), 'pitch') (('we', 'image fixed size'), 'use') (('where sound', 'places'), 'be') (('com speech recognition challenge place design c tensorflow t 47715 2nd forumMessageId 270205 philosophy', 'convolution blocks'), 'forum') (('how much augmentation', 'score'), 's') (('Also we', 'TTA'), 'face') (('source synthesis open which', 'default'), 'be') (('moving', 'frequency'), 'be') (('That', 'little bit natural images'), 'be') (('validation', 'validation'), 'use') (('EqualizingThis augmentation', 'custom frequency range'), 'adjust') (('we', 'nearby time'), 'get') (('We', 'FATTrainDataset'), 'prepare') (('sizeWe position', 'when spectrogram'), 'find') (('we', 'great opportunities'), 'do') (('Data Augmentation Data_Augmentation pitch shift', 'fade Fade reverb Reverb pitch_shift treble'), 'equalizing') (('we', 'frequency axis'), 'get') (('didn them', 'Custom CNN'), 'fold') (('didn 720 We t', 'there around 0'), 'check') (('We', 'ConvBlock'), 'add') (('use', 'crop random resized tta'), 'be') (('Finaly we', '5 3models 15model equally weighted manner'), 'decide') (('Strength Adaptive CropMany sound crip', 'first few seconds'), 'have') (('train', 'only noisy data'), 'do') (('why this', 'kernel size'), 'model') (('noisy data', 'curated data'), 'train') (('We', 'noisy data'), 'have') (('we', 'blocks'), 'be') (('v3 1ch 3 we', '0'), 'end') (('We', 'original dataset'), 'use') (('didn t work Augmentation', 'What'), 'github') (('Fourth Convolution Block', '128'), 'pooling') (('we', 'them'), 'have') (('We', 'fully connected layer'), 'change') (('which', 'TTA'), 'find') (('We', 'noisy data'), 'apply') (('we', 'random crop'), 'be') (('Training StrategyThis', 'training'), 'htmlSo') (('Ensemble Data Ensemble AugmentationWe', 'sox http sox'), 'create') (('We', 'log melspectrogram'), 'put') (('properly we', 'competition'), 'be') (('couldn it', 'competition'), 'try') (('we', 'just simply first layer'), 'be') "}