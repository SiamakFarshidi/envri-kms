{"name": "python ml breast cancer diagnostic data set ", "full_name": " h1 A brief tutorial on using Python to make predictions Breast Cancer Wisconsin Diagnostic Data Set h3 de Freitas R C h1 1 Introduction h1 2 Preparing the data h1 3 Visualizing the data h1 4 Machine learning h2 4 1 Using all mean values features h3 4 1 1 Stochastic Gradient Descent h3 4 1 2 Support Vector Machines h3 4 1 3 Nearest Neighbors h3 4 1 3 Naive Bayes h3 4 1 4 Forest and tree methods h2 4 2 Using the selected features h3 4 2 1 Stochastic Gradient Descent h3 4 2 2 Support Vector Machines h3 4 2 3 Nearest Neighbors h3 4 2 4 Naive Bayes h3 4 2 5 Forest and tree methods h1 5 Improving the best model h2 5 1 Naive Bayes h2 5 2 Forest and tree methods ", "stargazers_count": 0, "forks_count": 0, "description": "The test set will correspond to 20 of the total data test_size 0. Diagnosis M malignant B benign 3 32 Ten real valued features are computed for each cell nucleus a radius mean of distances from center to points on the perimeter b texture standard deviation of gray scale values c perimeter d area e smoothness local variation in radius lengths f compactness perimeter 2 area 1. org stable modules generated sklearn. org 2 https matplotlib. In order to avoid Overfitting 1 we will use the function train_test_split to split the data randomly random_state 42 into a train and a test set. In all cases the procedure will be the following 1. org wiki OverfittingNext we will use nine different classifiers all with standard parameters. the accuracy 2 of the predictions is measured. A brief tutorial on using Python to make predictions Breast Cancer Wisconsin Diagnostic Data Set de Freitas R. Still another form of doing this could be using box plots which is done below. During the data set loading a extra column was created. Now we can count how many diagnosis are malignant M and how many are benign B. Other packages will be loaded as necessary. Those parameters are called hyper parameters and are passed as arguments to the constructor of the classifier. org stable modules svm. 4 Forest and tree methods 4. 4 Naive Bayes 4. org Below we will use Seaborn to create a heat map of the correlations between the features. The red dots correspond to malignant diagnosis and blue to benign. 1 Stochastic Gradient Descent 4. 1 http scikit learn. To choose the right estimator algorithm we used the flowchart 2 found in the Scikit learn web page. org As can bee seen above except for the diagnosis that is M malignant or B benign all other features are of type float64 and have 0 non null numbers. After running the piece of codes below it will be presented the accuracy the cross validation score and the best set of parameters. 3 Visualizing the data In this section we will build visualizations of the data in order to decide how to proceed with the machine learning tools. We can also see how the malignant or benign tumors cells can have or not different values for the features plotting the distribution of each type of diagnosis for each of the mean features. html accuracy score 4. 3 Naive Bayes The Naive Bayes algorithm applies Bayes theorem with the assumption of independence between every pair of features. the predictions are found using X_test 4. the classifier clf is fitted with the train data set X_train and y_train 3. 1 Stochastic Gradient Descent The first classifier is the Stochastic Gradient Descent 1. 1 https seaborn. The grid search will be done only on the best models which are Naive Bayes Random Forest Extra Trees and Decision Trees. Look how in some cases reds and blues dots occupies different regions of the plots. The function f will be construct by the machine learning algorithm based on the ys and Xs that are already known. As described in UCI Machine Learning Repository 1 the attribute informations are 1. 2 Support Vector Machines 4. To do that we will need to use the Seaborn 1 and the Matplotlib 2 packages. Each estimator has a different set of hyper parameters which can be found in the corresponding documentation. 1 https archive. For this reason we will transform the categories M and B into values 1 and 0 respectively. edu ml datasets Breast Cancer Wisconsin 28Diagnostic 29 2 Preparing the data We will start loading some of the packages that will help us organize and visualize the data. 0 g concavity severity of concave portions of the contour h concave points number of concave portions of the contour i symmetry j fractal dimension coastline approximation 1 The mean standard error and worst or largest mean of the three largest values of these features were computed for each image resulting in 30 features. With help of Pandas 1 we will load the data set and print some basic informations. In the end we will compare the accuracy the cross validation score for the selected set and the complete set of features. 1 IntroductionThe aim of this notebook is to me and others to understand the process of organizing and preparing the data selecting the features choosing and applying the machine learning tools comparing selecting and improving the best models. 2 Forest and tree methods As can be seen in one case Extra Trees both accuracy and cross validations score were improved but only by some few percents and with the cost of more computational resources and time. html grid search 5. the accuracy is estimated with help of cross validation 1 5. We will select those features to use in the next section. 2 Support Vector Machines Now we will use three different Support Vector Machines 1 classifiers. It is also possible to create a scatter matrix with the features. the classifier clf is initialized 2. We can search for the best performance of the classifier sampling different hyper parameter combinations. org stable 2 http scikit learn. cross_val_score 2 http scikit learn. 3 Nearest Neighbors 4. We will use the code below to delete this entire column. As we saw above some of the features can have most of the times values that will fall in some range depending on the diagnosis been malignant or benign. 5 Improving the best model Not all parameters of a classifier is learned from the estimators. htmlThe algorithms will process only numerical values. org stable modules grid_search. The necessary tools will be loaded as needed. To remember those features are radius_mean perimeter_mean area_mean concavity_mean concave points_mean. org stable modules model_evaluation. The problem we are dealing with here is a classification problem. We are interested mainly in the mean values of the features so we will separate those features in the list below in order to make some work easier and the code more readably. This will be done with an exhaustive grid search 1 provided by the GridSearchCV function. 1 Using all mean values features Our aim is to construct a function y f X such that the value of y 1 or 0 will be determined once we input the values X into f. For this we will use Scikit learn 1 package. org stable tutorial machine_learning_map index. 1 Naive Bayes 5. 5 Forest and tree methods As can be seen in the table above using only some of the mean features reduced in most of the cases both accuracy and cross validation scores. 4 Machine learning In this section we will test and analyze machine learning algorithms for classification in order to identify if the tumor is malignant or benign based on the cell features. At the end the results are presents in along with the total time needed to run all the process. The features from the data set describe characteristics of the cell nuclei and are computed from a digitized image of a fine needle aspirate FNA of a breast mass. For instance field 3 is Mean Radius field 13 is Radius SE field 23 is Worst Radius. 3 Nearest Neighbors The nearest neighbors classifier finds predefined number of training samples closest in distance to the new point and predict the label from these. 2 Using the selected features In this section we will apply the same classifiers for the data with the features that were previously selected based on the analysis of section 3. In other cases only the accuracy or the cross validation score could be improved. After training our machine learning algorithm we need to test its accuracy. ", "id": "rcfreitas/python-ml-breast-cancer-diagnostic-data-set", "size": "8298", "language": "python", "html_url": "https://www.kaggle.com/code/rcfreitas/python-ml-breast-cancer-diagnostic-data-set", "git_url": "https://www.kaggle.com/code/rcfreitas/python-ml-breast-cancer-diagnostic-data-set", "script": "train_test_split NuSVC sklearn.svm cross_val_score accuracy_score numpy seaborn SGDClassifier ExtraTreesClassifier SVC LinearSVC GaussianNB sklearn.neighbors sklearn.naive_bayes sklearn.tree sklearn.linear_model matplotlib.pyplot DecisionTreeClassifier sklearn.model_selection pandas RandomForestClassifier KNeighborsClassifier GridSearchCV sklearn.metrics sklearn.ensemble ", "entities": "(('Naive Naive Bayes 3 algorithm', 'features'), 'Bayes') (('parameters', 'classifier'), 'call') (('we', 'web page'), 'use') (('which', 'corresponding documentation'), 'have') (('2 Forest methods', 'more computational resources'), 'improve') (('we', 'machine learning how tools'), '3') (('We', 'next section'), 'select') (('results', 'process'), 'be') (('test set', 'data total test_size'), 'correspond') (('only accuracy', 'other cases'), 'improve') (('train data', 'X_train'), 'fit') (('org wiki OverfittingNext we', 'all standard parameters'), 'use') (('extra column', 'data'), 'create') (('1 mean standard error', '30 features'), 'severity') (('tumors also how malignant cells', 'mean features'), 'see') (('parameters', 'estimators'), '5') (('tumor', 'cell features'), 'test') (('Support Vector 2 Now we', 'Support three different Vector Machines 1 classifiers'), 'Machines') (('that', 'section'), 'apply') (('This', 'GridSearchCV 1 function'), 'do') (('blues dots', 'plots'), 'look') (('we', 'Seaborn'), 'need') (('1 we', 'f.'), 'feature') (('Nearest neighbors 3 nearest classifier', 'these'), 'neighbor') (('htmlThe algorithms', 'only numerical values'), 'process') (('we', 'features'), 'use') (('predictions', 'X_test'), 'find') (('It', 'features'), 'be') (('radius_mean perimeter_mean area_mean concavity_mean', 'points_mean'), 'be') (('we', '1 package'), 'use') (('1 we', 'basic informations'), 'load') (('procedure', 'cases'), 'be') (('that', 'ys'), 'construct') (('we', 'accuracy'), 'need') (('We', 'below entire column'), 'use') (('we', 'values'), 'transform') (('M benign', 'non 0 null numbers'), 'org') (('us', 'data'), 'dataset') (('red dots', 'malignant diagnosis'), 'correspond') (('We', 'classifier hyper parameter different combinations'), 'search') (('features', 'breast mass'), 'describe') (('attribute 1 informations', 'UCI Machine Learning Repository'), 'be') (('which', 'only best models'), 'do') (('which', 'box plots'), 'use') (('that', 'diagnosis'), 'have') (('accuracy', 'cross validation'), 'estimate') (('we', 'complete features'), 'compare') (('cell nucleus', 'radius lengths'), 'compute') (('code', 'below order'), 'be') (('IntroductionThe aim', 'best models'), '1') (('accuracy', '2 predictions'), 'measure') (('1 we', '42 train'), 'use') "}