{"name": "step by step explanation of scoring metric ", "full_name": " h3 Rationale h3 Picking a test image h3 Intersection Over Union for a single Prediction GroundTruth comparison h3 Thresholding the IoU value for a single GroundTruth Prediction comparison h3 Single threshold precision for a single image h3 Multi threshold precision for a single image h3 Mean average precision for the dataset ", "stargazers_count": 0, "forks_count": 0, "description": "The number of true positives is equal to the number of predictions with a hit on a true object. FN a ground truth object had no associated predicted object. 5 a predicted object is considered a hit if its intersection over union with a ground truth object is greater than 0. Picking a test imageI m going to pick a sample image from the training dataset load the masks then create a mock predict set of masks from it by moving and dilating each individual nucleus mask. This tells us there are a few different steps to getting the score reported on the leaderboard. Here s the dataset Intersection Over Union for a single Prediction GroundTruth comparison The IoU of a proposed set of object pixels and a set of true object pixels is calculated as IoU A B frac A B A B Let s take one of the nuclei masks from our GroundTruth and Predicted volumes. The metric used for this competition is defined as the mean average precision at different intersection over union IoU thresholds. Now for each prediction mask P we ll get a comparison with every ground truth mask GT. Multi threshold precision for a single image The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold Avg. Precision frac 1 n_ thresh sum_ t 1 nprecision t Here we simply take the average of the precision values at each threshold to get our mean precision for the image. Therefore the leaderboard metric will simply be the mean of the precisions across all the images. Single threshold precision for a single imageNow in our example we ve created 7 prediction masks P_i to compare with 7 ground truth masks GT_j. At each threshold we will have a 7 7 matrix showing whether there was a hit with that object. RationaleI found the explanation for the scoring metric on this competition a little confusing and I wanted to create a guide for those who are just entering or haven t made it too far yet. So for this set of masks the IoU metric is calculated as IoU A B frac A B A B frac 564 849 0. 664 Thresholding the IoU value for a single GroundTruth Prediction comparison Next we sweep over a range of IoU thresholds to get a vector for each mask comparison. Calculate the mean of the average precision for each image. Hope you found this helpful I know it helped me to work through it Get image Get masks Make messed up masks Plot the objects Minor ticks and turn grid on. The number of false positives is equal to the number of predictions that don t hit anything. Their intersections and unions look like this Notice how the intersection will always be less than or equal to the size of the GroundTruth object and the Union will always be greater than or equal to that size. The threshold values range from 0. Mean average precision for the dataset Lastly the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset. Average the precision across thresholds. At each threshold calculate the precision across all your submitted masks. 95 with a step size of 0. Calculate whether this mask fits at a range of IoU thresholds. Precision t frac TP t TP t FP t FN t TP a single predicted object matches a ground truth object with an IoU above the threshold FP a predicted object had no associated ground truth object. In other words at a threshold of 0. For each submitted nuclei prediction calculate the Intersection of Union metric with each ground truth mask in the image. In most cases this will be zero since nuclei shouldn t overlap but this also allows flexibility in matching up each mask to each potential nucleus. The number of false negatives is equal to the number of ground truth objects that aren t hit. The precision value is based on the number of true positives TP false negatives FN and false positives FP in this hit matrix. ", "id": "stkbailey/step-by-step-explanation-of-scoring-metric", "size": "4116", "language": "python", "html_url": "https://www.kaggle.com/code/stkbailey/step-by-step-explanation-of-scoring-metric", "git_url": "https://www.kaggle.com/code/stkbailey/step-by-step-explanation-of-scoring-metric", "script": "scipy get_iou_vector Path ndimage pathlib matplotlib.pyplot iou_thresh_precision pandas numpy ", "entities": "(('precision value', 'hit matrix'), 'base') (('mask', 'IoU thresholds'), 'calculate') (('Intersection', 'image'), 'calculate') (('metric', 'union IoU thresholds'), 'define') (('predicted object', 'ground truth associated object'), 'frac') (('who', 'haven just it'), 'find') (('Union', 'always size'), 'look') (('number', 'true object'), 'be') (('mask P we', 'ground truth mask GT'), 'get') (('masks', 'nucleus individual mask'), 'create') (('intersection', '0'), 'consider') (('we', 'ground truth masks 7 GT_j'), 'precision') (('score', 'test dataset'), 'precision') (('aren', 'that'), 'be') (('this', 'potential nucleus'), 'be') (('Therefore leaderboard metric', 'images'), 'be') (('s', 'Predicted volumes'), 's') (('average precision', 'Avg'), 'precision') (('don t', 'anything'), 'be') (('masks', 'Minor grid'), 'hope') (('we', 'object'), 'have') (('Next we', 'mask comparison'), '664') (('a few different score', 'leaderboard'), 'tell') (('IoU A B frac A B A B', 'masks'), 'calculate') (('FN', 'ground truth associated predicted object'), 'have') (('1 _ thresh _ 1 nprecision n sum Here we', 'image'), 'frac') "}