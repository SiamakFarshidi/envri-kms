{"name": "titanic data science solutions ", "full_name": " h1 Titanic Data Science Solutions h3 This notebook is a companion to the book Data Science Solutions h2 Workflow stages h2 Question and problem definition h2 Workflow goals h2 Refactor Release 2017 Jan 29 h3 User comments h3 Porting issues h3 Best practices h2 Acquire data h2 Analyze by describing data h3 Assumtions based on data analysis h2 Analyze by pivoting features h2 Analyze by visualizing data h3 Correlating numerical features h3 Correlating numerical and ordinal features h3 Correlating categorical features h3 Correlating categorical and numerical features h2 Wrangle data h3 Correcting by dropping features h3 Creating new feature extracting from existing h3 Converting a categorical feature h3 Completing a numerical continuous feature h3 Create new feature combining existing features h3 Completing a categorical feature h3 Converting categorical feature to numeric h3 Quick completing and converting a numeric feature h2 Model predict and solve h3 Model evaluation h2 References ", "stargazers_count": 0, "forks_count": 0, "description": "Certain titles mostly survived Mme Lady Sir or did not Don Rev Jonkheer. We may analyze by visualizing data. We can consider correlating Embarked Categorical non numeric Sex Categorical non numeric Fare Numeric continuous with Survived Categorical numeric. Cabin is alphanumeric. Reference Wikipedia https en. The perceptron is an algorithm for supervised learning of binary classifiers functions that can decide whether an input represented by a vector of numbers belongs to some specific class or not. Logistic regression measures the relationship between the categorical dependent variable feature and one or more independent variables features by estimating probabilities using a logistic function which is the cumulative logistic distribution. Although there was some element of luck involved in surviving the sinking some groups of people were more likely to survive than others such as women children and the upper class. Embarked takes three possible values. Around 38 samples survived representative of the actual survival rate at 32. We should consider Age our assumption classifying 2 in our model training. We can not create FareBand. Which features are mixed data types Numerical alphanumeric data within same feature. We also combine these datasets to run certain operations on both datasets together. Complete the Age feature for null values completing 1. Correlating numerical and ordinal featuresWe can combine multiple features for identifying correlations using a single plot. Confirms classifying 1. Consider banding Fare feature. Let us start by converting Sex feature to a new feature called Gender where female 1 and male 0. mean age_std guess_df. Nearly 30 of the passengers had siblings and or spouse aboard. It is a type of linear classifier i. Supply or submit the results. 5 among Pclass 1 and Survived classifying 3. We will prefer method 2. Name feature is relatively non standard may not contribute directly to survival so maybe dropped. Translated 32 survival rate. What is the distribution of numerical feature values across the samples This helps us determine among other early insights how representative is the training dataset of the actual problem domain. Perform a stage multiple times in our workflow. Fares varied significantly with few passengers 1 paying as high as 512. Note that where applicable we perform operations on both training and testing datasets together to stay consistent. The objective of this notebook is to follow a step by step workflow explaining each step and rationale for every decision we take during solution development. Five features are strings object. Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset. We can only do so at this stage for features which do not have any empty values. A histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. One way to do this is to detect any outliers among our samples or features. A simple way is to generate random numbers between mean and standard deviation https en. Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations. A journey through Titanic https www. Exception in Embarked C where males had higher survival rate. Complete and add Embarked feature to model training. Here are the highlights to note. Ticket is a mix of numeric and alphanumeric data types. We will first do this for the Age feature. What are the data types for various features Helping us during converting goal. So median Age for Pclass 1 and Gender 0 Pclass 1 and Gender 1 and so on. Cabin Age Embarked features contain a number of null values in that order for the training dataset. This can be done by calculating the coefficient of the features in the decision function. org wiki Support_vector_machine. Confirms our assumption for creating 4 fare ranges. Within numerical features are the values discrete continuous or timeseries based Among other things this helps us select the appropriate plots for visualization. The algorithm allows for online learning in that it processes elements in the training set one at a time. By dropping features we are dealing with fewer data points. We may also want to understand the implications or correlation of different classes with our solution goal. In our case we note correlation among Age Gender and Pclass. Quick completing and converting a numeric featureWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We also do not need the PassengerId feature in the training dataset. Seven features are integer or floats. Workflow stagesThe competition solution workflow goes through seven stages described in the Data Science Solutions book. Few elderly passengers 1 within age range 65 80. Consider Pclass for model training. org wiki Median values for Age across sets of Pclass and Gender feature combinations. We may want to classify or categorize our samples. Categorical Survived Sex and Embarked. S port used by most passengers top S Ticket feature has high ratio 22 of duplicate values unique 681. Our submission to the competition site Kaggle results in scoring 3 883 of 6 082 competition entries. We may want to create new feature for Age bands. When we plot Title Age and Survived we note the following observations. More accurate way of guessing missing values is to use other correlated features. How to select the right visualization plots and charts depending on nature of the data and the solution goals. In pattern recognition the k Nearest Neighbors algorithm or k NN for short is a non parametric method used for classification and regression. com omarelgabry titanic a journey through titanic Getting Started with Pandas Kaggle s Titanic Competition https www. Given a set of training samples each marked as belonging to one or the other of two categories an SVM training algorithm builds a model that assigns new test samples to one category or the other making it a non probabilistic binary linear classifier. Most titles band Age groups accurately. Which features within the dataset contribute significantly to our solution goal Statistically speaking is there a correlation https en. Let us now execute our decisions and assumptions for correcting creating and completing goals. This way Age Class is a good artificial feature to model as it has second highest negative correlation with Survived. Can we create new features based on an existing feature or a set of features such that the new feature follows the correlation conversion completeness goals. Model evaluationWe can now rank our evaluation of all the models to choose the best one for our problem. The model confidence score is the highest among models evaluated so far. The RegEx pattern w. Workflow goalsThe data science solutions workflow solves for seven major goals. Any suggestions to improve our score are most welcome. We may also want to develop some early understanding about the domain of our problem. For modeling stage one needs to prepare the data. Which features are available in the dataset Noting the feature names for directly manipulating or analyzing these. thanks Reinhard Porting issues Specify plot dimensions bring legend into plot. Males had better survival rate in Pclass 3 when compared with Pclass 2 for C and Q ports. Pclass varies in terms of Age distribution of passengers. Analyze by visualizing dataNow we can continue confirming some of our assumptions using visualizations for analyzing the data. Add Sex feature to model training. Correlating certain features may help in creating completing or correcting features. This is required by most model algorithms. So is Title as second highest positive correlation. Visualize report and present the problem solving steps and final solution. Confirms our classifying assumption 2. Confirms correlating 1 and completing 2. Let us replace Age with ordinals based on these bands. There are 60 predictive modelling algorithms to choose from. Converting a categorical featureNow we can convert features which contain strings to numerical values. We may combine mulitple workflow stages. Confirms our classifying assumption 3. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Which features may contain errors or typos This is harder to review for a large dataset however reviewing a few samples from a smaller dataset may just tell us outright which features may require correcting. Drop a stage altogether. The workflow indicates general sequence of how each stage may follow the other. Most passengers in Pclass 1 survived. Combine methods 1 and 2. We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board. Inversely as Pclass increases probability of Survived 1 decreases the most. org wiki Standard_deviation. We can not remove the AgeBand feature. Total samples are 891 or 40 of the actual number of passengers on board the Titanic 2 224. We simply fill these with the most common occurance. Review Parch distribution using percentiles. This helps us answer questions relating to specific bands Did infants have better survival rate Note that x axis in historgram visualizations represents the count of samples or passengers. Now we can safely drop the Name feature from training and testing datasets. We decide to include this feature in our model. User comments Combine training and test data for certain operations like converting titles across dataset to numerical values. Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Note that the model generates a confidence score which is higher than Logistics Regression model. Let us drop Parch SibSp and FamilySize features in favor of IsAlone. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. Using multiple plots instead of overlays for readability. Our problem is a classification and regression problem. So for instance converting text categorical values to numeric values. 8 SibSp distribution. We may want to complete Age feature as it is definitely correlated to survival. Survival among Title Age bands varies slightly. Large number of 15 25 year olds did not survive. The expand False flag returns a DataFrame. We want to know how well does each feature correlate with Survival. A sample is classified by a majority vote of its neighbors with the sample being assigned to the class most common among its k nearest neighbors k is a positive integer typically small. If k 1 then the object is simply assigned to the class of that single nearest neighbor. org wiki K nearest_neighbors_algorithm. KNN confidence score is better than Logistics Regression but worse than SVM. 69 Age and Fare. This turns a continous numerical feature into an ordinal categorical feature. ReferencesThis notebook has been created based on great work done solving the Titanic competition and other sources. We may not need supply stage to productize or service enable our dataset for a competition. We can replace many titles with a more common name or classify them as Rare. Wrangle prepare cleanse the data. Note that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The model generated confidence score is the lowest among the models evaluated so far. uniform age_mean age_std age_mean age_std Convert random age float to nearest. What is the distribution of categorical features Names are unique across the dataset count unique 891 Sex variable as two possible values with 65 male top male freq 577 count 891. In machine learning naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes theorem with strong naive independence assumptions between the features. FacetGrid train_df col Embarked hue Survived palette 0 k 1 w grid sns. Refactor Release 2017 Jan 29We are significantly refactoring the notebook based on a comments received by readers b issues in porting notebook from Jupyter kernel 2. Analyze by describing dataPandas also helps describe the datasets answering following questions early in our project. Higher fare paying passengers had better survival. Positive coefficients increase the log odds of the response and thus increase the probability and negative coefficients decrease the log odds of the response and thus decrease the probability. Discrete SibSp Parch. Model predict and solve the problem. Alternatively several passengers shared a cabin. Infant passengers in Pclass 2 and Pclass 3 mostly survived. With these two criteria Supervised Learning plus Classification and Regression we can narrow down our choice of models to a few. Which features are numerical Which features are numerical These values change from sample to sample. Creating new feature extracting from existingWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival before dropping Name and PassengerId features. It may be best to derive a feature or a set of features from these individual features creating 1. Create new feature combining existing featuresWe can create a new feature for FamilySize which combines Parch and SibSp. We can also create an artificial feature combining Pclass and Age. Model algorithms may work best when there are no missing values. We may also want round off the fare to two decimals as it represents currency. There are several excellent notebooks to study data science competition entries. We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. So far we did not have to change a single feature or value to arrive at these. We do this in a single line of code. Pclass We observe significant correlation 0. On April 15 1912 during her maiden voyage the Titanic sank after colliding with an iceberg killing 1502 out of 2224 passengers and crew. Women Sex female were more likely to have survived. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. Acquire dataThe Python Pandas packages helps us work with our datasets. Completing a numerical continuous featureNow we should start estimating and completing features with missing or null values. Analyze identify patterns and explore the data. Children Age were more likely to have survived. Correlating categorical and numerical featuresWe may also want to correlate categorical features with non numeric values and numeric features. Correlating numerical featuresLet us start by understanding correlations between numerical features and our solution goal Survived. Ticket feature may be dropped from our analysis as it contains high ratio of duplicates 22 and there may not be a correlation between Ticket and survival. std age_guess rnd. PassengerId may be dropped from training dataset as it does not contribute to survival. Visualize stage may be used multiple times. Port of embarkation correlates with survival rates. For example Master title has Age mean of 5 years. Ports of embarkation have varying survival rates for Pclass 3 and among male passengers. We may analyze data before and after wrangling. We decide to retain the new Title feature for model training. Sex We confirm the observation during problem definition that Sex female had very high survival rate at 74 classifying 1. Note the confidence score generated by the model based on our training dataset. These are candidates for correcting goal. We can create another feature called IsAlone. Best practices Performing feature correlation analysis early in the project. Now we iterate over Sex 0 or 1 and Pclass 1 2 3 to calculate guessed values of Age for the six combinations. In the following code we extract Title feature using regular expressions. We can convert the categorical titles to ordinal. matches the first word which ends with a dot character within Name feature. Most passengers are in 15 35 age range. Wrangle dataWe have collected several assumptions and decisions regarding our datasets and solution requirements. Acquire training and testing data. We decide to use this model s output Y_pred for creating our competition submission of results. We may validate these assumptions further before taking appropriate actions. We start by acquiring the training and testing datasets into Pandas DataFrames. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Correlating categorical featuresNow we can correlate categorical features with our solution goal. Let us create Age bands and determine correlations with Survived. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results. 62 knowing our problem description mentions 38 survival rate. Infants Age 4 had high survival rate. 7 to Kaggle kernel 3. 5 age Logistic Regression Support Vector Machines Gaussian Naive Bayes Perceptron Linear SVC Stochastic Gradient Descent Decision Tree Random Forest submission. Continous Age Fare. We may also add to our assumptions based on the problem description noted earlier. The next model Random Forests is one of the most popular. FacetGrid train_df col Pclass hue Survived grid sns. Doing so will also help us in achieving the feature completing goal. Method 1 and 3 will introduce random noise into our models. Guess Age values using median https en. Model predict and solveNow we are ready to train a model and predict the required solution. Data preparation may also require us to estimate any missing values within a feature. We should band age groups creating 3. So instead of guessing age values based on median use random numbers between mean and standard deviation based on sets of Pclass and Gender combinations. This will enable us to drop Parch and SibSp from our datasets. We may want to complete the Embarked feature as it may also correlate with survival or another important feature. Analyze by pivoting featuresTo confirm some of our observations and assumptions we can quickly analyze our feature correlations by pivoting features against each other. org wiki Logistic_regression. Completing a categorical featureEmbarked feature takes S Q C values based on port of embarkation. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived not necessarily direct correlation between Embarked and Survived. Tree models where the target variable can take a finite set of values are called classification trees in these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels. These include Logistic Regression KNN or k Nearest Neighbors Support Vector Machines Naive Bayes classifier Decision Tree Random Forrest Perceptron Artificial neural network RVM or Relevance Vector MachineLogistic Regression is a useful model to run early in the workflow. org wiki Random_forest. One can approach the problem based on available features within the training dataset. org wiki Correlation among a feature and solution goal As the feature values change does the solution state change as well and visa versa This can be tested both for numerical and categorical features in the given dataset. 5 and c review of few more best practice kernels. Titanic Data Science Solutions This notebook is a companion to the book Data Science Solutions https www. thanks Sharan Naribole Correct observation nearly 30 of the passengers had siblings and or spouses aboard. We can consider three methods to complete a numerical continuous feature. Convert the Fare feature to ordinal values based on the FareBand. Naive Bayes classifiers are highly scalable requiring a number of parameters linear in the number of variables features in a learning problem. Assumtions based on data analysisWe arrive at following assumptions based on data analysis done so far. Question and problem definitionCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. Which features contain blank null or empty values These will require correcting. com sinakhorami titanic titanic best working classifier data analysis and wrangling visualization machine learning preview the data Review survived rate using percentiles. It also makes sense doing so only for features which are categorical Sex ordinal Pclass or discrete SibSp Parch type. We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. We may also want to create a Fare range feature if it helps our analysis. This can be done with numerical and categorical features which have numeric values. We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. These feature names are described on the Kaggle data page here https www. Further qualifies our classifying assumption 2. Correcting by dropping featuresThis is a good starting goal to execute. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. Which features are categorical These values classify the samples into sets of similar samples. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. com Data Science Solutions Startup Workflow dp 1520545312. org wiki Decision_tree_learning. FacetGrid train_df col Embarked grid sns. thanks Reinhard Correctly interpreting logistic regresssion coefficients. This result only accounts for part of the submission dataset. However there are use cases with exceptions. Cabin Age are incomplete in case of test dataset. SibSp and Parch These features have zero correlation for certain values. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. We want to identify relationship between output Survived or not with other variables or features Gender Age Port. Cabin values have several dupicates across samples. Speeds up our notebook and eases the analysis. org wiki Naive_Bayes_classifier. This simple analysis confirms our assumptions as decisions for subsequent workflow stages. The notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle. Sex is highest positivie coefficient implying as the Sex value increases male 0 to female 1 the probability of Survived 1 increases the most. Decision trees where the target variable can take continuous values typically real numbers are called regression trees. While both Decision Tree and Random Forest score the same we choose to use Random Forest as they correct for decision trees habit of overfitting to their training set. Name feature may contain errors or typos as there are several ways used to describe a name including titles round brackets and quotes used for alternative or short names. Pclass 3 had most passengers however most did not survive. We may want to engineer the Name feature to extract Title as a new feature. Converting categorical feature to numericWe can now convert the EmbarkedFill feature by creating a new numeric Port feature. FacetGrid train_df col Pclass hue Gender age_mean guess_df. Our training dataset has two missing values. Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster can our model determine based on a given test dataset not containing the survival information if these passengers in the test dataset survived or not. This is described on the Kaggle competition description page here https www. The question or problem definition for Titanic Survival competition is described here at Kaggle https www. Female passengers had much better survival rate than males. Question or problem definition. Survived is a categorical feature with 0 or 1 values. We want to do this early in our project and match these quick correlations with modelled correlations later in the project. Perform a stage earlier than indicated. Random forests or random decision forests are an ensemble learning method for classification regression and other tasks that operate by constructing a multitude of decision trees n_estimators 100 at training time and outputting the class that is the mode of the classes classification or mean prediction regression of the individual trees. Not bad for our first attempt. Within categorical features are the values nominal ordinal ratio or interval based Among other things this helps us select the appropriate plots for visualization. Most passengers 75 did not travel with parents or children. Six in case of test dataset. The upper class passengers Pclass 1 were more likely to have survived. Oldest passengers Age 80 survived. The results from multiple executions might vary. This result is indicative while the competition is running. com c titanic details getting started with random forests Titanic Best Working Classifier https www. The completion goal achieves desired requirement for model algorithm to operate on non null values. Based on our assumptions and decisions we want to drop the Cabin correcting 2 and Ticket correcting 1 features. This model uses a decision tree as a predictive model which maps features tree branches to conclusions about the target value tree leaves. ", "id": "startupsci/titanic-data-science-solutions", "size": "27064", "language": "python", "html_url": "https://www.kaggle.com/code/startupsci/titanic-data-science-solutions", "git_url": "https://www.kaggle.com/code/startupsci/titanic-data-science-solutions", "script": "sklearn.svm numpy seaborn SGDClassifier SVC LinearSVC GaussianNB sklearn.neighbors sklearn.naive_bayes sklearn.tree sklearn.linear_model random matplotlib.pyplot DecisionTreeClassifier pandas RandomForestClassifier LogisticRegression Perceptron KNeighborsClassifier sklearn.ensemble ", "entities": "(('Names', '65 male top male freq'), 'be') (('we', 'solution goal'), 'correlate') (('one', 'data'), 'need') (('We', 'samples'), 'want') (('Model', 'problem'), 'rank') (('Female passengers', 'males'), 'have') (('Total samples', 'board'), 'be') (('We', 'training dataset'), 'need') (('We', 'board'), 'want') (('This', 'submission dataset'), 'result') (('We', 'Gender Age Port'), 'want') (('maps', 'target value tree leaves'), 'use') (('Correlating', 'non numeric values'), 'want') (('Sex female', '74'), 'confirm') (('Bayes naive classifiers', 'features'), 'be') (('We', 'numerical continuous feature'), 'consider') (('how stage', 'other'), 'indicate') (('Nearest Neighbors Support Vector Naive Bayes classifier Decision Tree Random Forrest network Relevance Vector MachineLogistic Perceptron neural Regression', 'useful early workflow'), 'include') (('we', 'data fewer points'), 'deal') (('that', 'most frequently feature'), 'complete') (('uniform age_mean', 'nearest'), 'age_mean') (('however most', '3 most passengers'), 'have') (('Cabin values', 'samples'), 'have') (('predictions', 'feature vector'), 'algorithm') (('non numeric Fare Sex Numeric', 'Categorical Survived numeric'), 'consider') (('KNN confidence score', 'SVM'), 'be') (('us', 'Pclass Gender combinations'), 'let') (('These', 'goal'), 'be') (('Random next model Forests', 'most popular'), 'be') (('features', 'directly these'), 'be') (('suggestions', 'score'), 'be') (('probability', '1 most'), 'increase') (('More accurate way', 'other correlated features'), 'be') (('Completing', 'embarkation'), 'take') (('Large number', 'year 15 25 olds'), 'survive') (('histogram', 'automatically defined bins'), 'indicate') (('which', 'empty values'), 'do') (('One', 'training dataset'), 'approach') (('simple analysis', 'workflow subsequent stages'), 'confirm') (('input', 'specific class'), 'be') (('submission', '6 competition 082 entries'), 'result') (('training dataset', 'two missing values'), 'have') (('Now we', 'six combinations'), 'iterate') (('where banding', 'useful patterns'), 'be') (('Naive Bayes classifiers', 'learning problem'), 'be') (('Analyze', 'early project'), 'help') (('that', 'individual trees'), 'be') (('Doing', 'feature completing goal'), 'help') (('it', 'one category'), 'mark') (('We', 'feature'), 'create') (('Now we', 'training datasets'), 'drop') (('This', 'ordinal categorical feature'), 'turn') (('data types', 'goal'), 'be') (('ReferencesThis notebook', 'Titanic competition'), 'create') (('we', 'Age Gender'), 'note') (('such new feature', 'correlation conversion completeness goals'), 'create') (('us', 'bands'), 'let') (('it', 'definitely survival'), 'want') (('This', 'decision function'), 'do') (('User comments', 'numerical values'), 'Combine') (('Name feature', 'alternative names'), 'contain') (('we', 'only single value'), 'note') (('Survived', 'categorical 0 values'), 'be') (('We', 'categorical titles'), 'convert') (('Cabin Age', 'test dataset'), 'be') (('We', 'Pandas DataFrames'), 'start') (('results', 'multiple executions'), 'vary') (('This', 'Kaggle competition description page'), 'describe') (('empty These', 'blank null'), 'feature') (('This', 'necessarily direct Embarked'), 'be') (('outright features', 'just us'), 'contain') (('expand False flag', 'DataFrame'), 'return') (('us', 'Survived'), 'let') (('we', '1 features'), 'want') (('We', 'problem'), 'want') (('We', 'model'), 'decide') (('Higher fare', 'better survival'), 'have') (('Infant passengers', 'Pclass'), 'survive') (('assumption', 'model training'), 'consider') (('This', 'datasets'), 'enable') (('k', 'most common k nearest neighbors'), 'classify') (('We', 'later project'), 'want') (('features', 'data mixed Numerical alphanumeric same feature'), 'be') (('Few elderly passengers', '65 80'), 'range') (('Master title', '5 years'), 'have') (('which', 'Name feature'), 'match') (('it', 'currency'), 'want') (('Most passengers', '75 parents'), 'travel') (('So Title', 'second highest positive correlation'), 'be') (('Pclass', 'passengers'), 'vary') (('Question', 'test dataset'), 'define') (('We', 'goals'), 'use') (('where applicable we', 'datasets'), 'note') (('us', 'training problem how actual domain'), 'be') (('values', 'similar samples'), 'classify') (('Ticket', 'data numeric types'), 'be') (('S port', '22 duplicate values'), 'have') (('We', 'new feature'), 'want') (('This', 'model most algorithms'), 'require') (('nearly 30', 'siblings'), 'thank') (('us', 'datasets'), 'help') (('Supervised we', 'few'), 'narrow') (('Cabin Age Embarked features', 'training dataset'), 'contain') (('We', 'errors'), 'analyze') (('We', 'Rare'), 'replace') (('Correlating', 'features'), 'help') (('us', 'numerical features'), 'survive') (('they', 'training set'), 'score') (('wrangling visualization machine data Review', 'percentiles'), 'com') (('problem description', 'survival 38 rate'), 'mention') (('it', 'time'), 'allow') (('we', 'which'), 'understand') (('We', 'Age bands'), 'want') (('that', 'classification analysis'), 'model') (('We', 'subsequent goals'), 'want') (('Survival', 'Title Age bands'), 'vary') (('Port', 'survival rates'), 'correlate') (('us', 'visualization'), 'be') (('Data preparation', 'feature'), 'require') (('that', 'class labels'), 'call') (('we', 'following observations'), 'note') (('Refactor Jan 2017 29We', 'Jupyter kernel'), 'Release') (('numerical values', 'sample'), 'be') (('we', 'required solution'), 'predict') (('confidence score', 'models'), 'generate') (('1 then object', 'single nearest neighbor'), 'assign') (('Titanic Data Science notebook', 'book Data Science Solutions https www'), 'Solutions') (('Ports', '3 male passengers'), 'have') (('Fares', '1 as high 512'), 'varied') (('Correlating', 'single plot'), 'combine') (('shipwreck', 'enough passengers'), 'be') (('Method', 'models'), 'introduce') (('Reinhard Porting plot Specify dimensions', 'plot'), 'thank') (('features', 'certain values'), 'SibSp') (('passengers', 'test dataset'), 'know') (('where males', 'survival higher rate'), 'exception') (('visa as well This', 'given dataset'), 'do') (('So far we', 'these'), 'have') (('completion goal', 'non null values'), 'achieve') (('which', 'Logistics Regression model'), 'note') (('it', 'significantly results'), 'discard') (('it', 'also survival'), 'want') (('Workflow stagesThe competition solution workflow', 'Data Science Solutions book'), 'go') (('we', 'other'), 'confirm') (('Wrangle dataWe', 'datasets'), 'collect') (('We', 'Pclass'), 'create') (('we', 'regular expressions'), 'extract') (('Converting', 'numeric Port new feature'), 'convert') (('question', 'Kaggle https here www'), 'describe') (('Alternatively several passengers', 'cabin'), 'share') (('we', 'solution development'), 'be') (('We', 'Age feature'), 'do') (('model confidence score', 'models'), 'be') (('we', 'data'), 'continue') (('axis', 'samples'), 'help') (('Name feature', 'Name'), 'want') (('it', 'survival'), 'drop') (('1 probability', '1 most'), 'be') (('Certain titles', 'Mme Lady mostly Sir'), 'survive') (('it', '22 Ticket'), 'drop') (('notebook', 'Kaggle'), 'walk') (('We', 'further appropriate actions'), 'validate') (('We', 'model training'), 'decide') (('One way', 'samples'), 'be') (('feature names', 'Kaggle data page'), 'describe') (('notebooks', 'experts'), 'skip') (('Assumtions', 'data analysis'), 'arrive') (('it', 'analysis'), 'want') (('Males', '2 C ports'), 'have') (('We', 'Survival'), 'want') (('Nearly 30', 'siblings'), 'have') (('us', 'IsAlone'), 'let') (('Pclass We', 'significant correlation'), 'observe') (('which', 'Parch'), 'create') (('Correcting', 'featuresThis'), 'be') (('Analyze', 'data'), 'identify') (('Positive coefficients', 'thus probability'), 'increase') (('Sex', 'new feature'), 'let') (('service', 'competition'), 'need') (('it', 'training'), 'drop') (('simple way', 'deviation https mean en'), 'be') (('which', 'numerical values'), 'convert') (('features', 'solution significantly goal'), 'be') (('which', 'so only features'), 'make') (('It', '1'), 'be') (('Around 38 samples', '32'), 'survive') (('We', 'results'), 'decide') (('we', 'missing values'), 'start') (('us', 'goals'), 'let') (('We', 'datasets'), 'combine') (('Name feature', 'relatively non directly survival'), 'be') (('model algorithm one', 'numerical equivalent values'), 'require') (('We', 'most common occurance'), 'fill') (('We', 'problem description'), 'add') (('which', 'numeric values'), 'do') (('typically real numbers', 'continuous values'), 'call') (('We', 'solution goal'), 'want') (('good artificial it', 'Survived'), 'be') (('We', 'data'), 'analyze') (('which', 'logistic function'), 'measure') (('supervised we', 'given dataset'), 'perfome') "}