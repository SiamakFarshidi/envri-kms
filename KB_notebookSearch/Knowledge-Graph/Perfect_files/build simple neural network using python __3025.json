{"name": "build simple neural network using python ", "full_name": " h1 Let s understand Neural Network more Mathematically and Programmatically h3 Creation of Feed Forward Neural Networks using Python only h3 Let s design a code of single neuron with 3 inputs example h3 3 neuron layer with 4 inputs h3 Using the dot product for a neuron s calculation h3 Using the dot product with a layer of neurons h3 This error is just to show why shapes are important You can execute the below cells now h3 Data often comes in input batch h3 Example of what an array of a batch of samples looks like compared to a single sample h2 More hidden layers and non linear data h3 Matrix product with row and column vectors with a batch of inputs to the neural network h4 Q 1 How many neurons we have in the second layer h3 Extracting the data h3 Why how two or more hidden layers w nonlinear activation functions works with neural networks deep learning h4 Dense layer h3 Activation Functions h3 Softmax Activation Function h3 One hot encoded values ", "stargazers_count": 0, "forks_count": 0, "description": "71828182846 need to do e x e 2. Use Softmax Activation function with deep learning. How non linearity comes will see later. Doing dot product with a layer of neurons and multiple input4. Log 1 0 and to get ve value of Log we added minus sign because we want log function as positive2. 71828182846 now normalize them to simplify to single value per sample use keep_dims To prevend exploding values let s substract the max value from each array to retain the values between infinity and 1 Softmax Activation forward pass Dense Layer initialize weight and biases forwards pass test class create the layer perform a forward pass perform activation of Relu perform softmax see output our numpy solution Common loss class calculates data and reg loss given model output and truth values calculate sample losses mean loss Categorical Cross Entropy Loss forward pass number of samples in a batch clip data to remove log 0 and negative loss probs for target values if categorical labels. There are generally 2 types of activation functions used in NN. Now we need to multiply this group of input data with group of weights both are matrix Example of what an array of a batch of samples looks like compared to a single sample. Step activation function 2. Creation of a basic neuron with 3 and different inputs 2. You gonna see here is 1. Generally there are following types 1. Rectified Linear Units ReLU Activation simple code We can see the values have been clipped to 0. Creation of a simple layer of neurons with 4 inputs3. Softmax Activation FunctionFor batchesSoftmax Activation CodeExponentFinal CodeCategorical Cross Entropy1. To have some measure of how wrong the model is we will usea loss function. We use activation function because is activation function is non linear it allows for neural networks with 2 or more layers to map non linear functions. More hidden layers and non linear dataA deep neural network is a neural network with 2 or more hidden layersSo for each layer we will have a different set of weights Matrix product with row and column vectors with a batch of inputs to the neural network Q 1 How many neurons we have in the second layer Extracting the data Why how two or more hidden layers w nonlinear activation functions works with neural networks deep learning Dense layer Activation FunctionsActivation function is applied to the output of a neuron which modifies outputs. How to use Activation Function5. Sigmoid activation function4. And we are using Log because Log 1 is 0 and that means 100 where our Loss will be 0 and we have to approach to this value only and here we have achieve the concept of Categorical Cross EntropyCalculating LossSoftmax output array is giving out the values which which are representing the corresponding to it s class One hot encoded values. Let s understand Neural Network more Mathematically and Programmatically Creation of Feed Forward Neural Networks using Python only No any other libaries are used here. Let s design a code of single neuron with 3 inputs exampleWhat if we have 4 inputs 3 neuron layer with 4 inputs Using the dot product for a neuron s calculation Using the dot product with a layer of neurons This error is just to show why shapes are important. Problems with log add a very small value to the actual values like 1e 7Final Code add bias add to final Dense Layer initialize weight and biases forwards pass test class create the layer perform a forward pass see output Relu Activation Class forward pass calculate max of 0 input values Dense Layer initialize weight and biases forwards pass test class create the layer perform a forward pass perform activation of Relu see output Using e x because this function converts negative values to positive values Min value 0 Max value infinity as e x is monotonic funtion need to do e x e 2. One in the hidden layers and 1 in the final output layer. What is a batch a group of input data at a time. Linear activation function Last Layer for regression 3. You can execute the below cells now Data often comes in input batch. In our case with a softmax classifier so I used categorical cross entropy. ", "id": "rahuldogra/build-simple-neural-network-using-python", "size": "3025", "language": "python", "html_url": "https://www.kaggle.com/code/rahuldogra/build-simple-neural-network-using-python", "git_url": "https://www.kaggle.com/code/rahuldogra/build-simple-neural-network-using-python", "script": "Activation_Relu calculate Loss Loss_CategoricalCrossEntropy(Loss) matplotlib.pyplot nnfs.datasets Activation_Softmax __init__ Layer_Dense forward spiral_data numpy ", "entities": "(('we', 'positive2'), 'add') (('Categorical Cross Entropy loss Loss', 'categorical labels'), 'normalize') (('which', 'class One hot encoded values'), 'use') (('only other libaries', 'Python'), 'let') (('just why shapes', 'neurons'), 'let') (('values', '0'), 'code') (('array', 'single sample'), 'need') (('we', 'loss function'), 'have') (('which', 'outputs'), 'layer') (('neural networks', 'linear non functions'), 'use') (('Use Softmax Activation', 'deep learning'), 'function') (('monotonic funtion', 'e e 2'), 'add') (('now Data', 'input often batch'), 'execute') (('batch', 'time'), 'be') (('I', 'cross categorical entropy'), 'use') "}