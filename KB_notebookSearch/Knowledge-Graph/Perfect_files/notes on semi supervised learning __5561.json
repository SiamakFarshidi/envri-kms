{"name": "notes on semi supervised learning ", "full_name": " h1 Notes on semi supervised learning h2 Introduction h2 Demonstration h3 Synthetic benchmark h3 Application benchmark h1 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "Notes on semi supervised learning IntroductionFor some kinds of data you will run into the problem of having many samples but not having labels for all of those samples only for a subset of them. This problem gets its own name likely because it is so commonly encountered in research and dataset generation contexts. First we ll use the following synthetic dataset. The requisite section of the sklearn documentation http scikit learn. Thus these algorithms generalize to a wide variety of spaces like e. seed 42 y_unlabeled_pred trans. So these algorithms can be used to do that as well. In those cases perhaps try applying machine learning to the problem directly. It s a useful tool to know about more generally for missing data imputation from a limited sample size but the algorithms have poor performance characteristics for larger samples. For this synthetic dataset it will not make much of a difference but the higher the level of noise in the dataset the more important parameter tuning becomes. I would argue that the two problems are just rephrasings of one another semi supervised learning is classifying missing dependent variables while missing data imputation is classifying missing predictor variables. circles http scikit learn. A set of techniques and algorithms specific to this problem exists. If you are familiar with the sklearn style this should seem intimately familiar by now Synthetic benchmarkNow some code In the plots above I show class clusters with different levels of class separation from almost none to almost complete. Note that the presence of a handful of wayward points in the dominant clusters blue and white is actually a suprising artifact of make_classification. The clusters will be tuned to have different degrees of class separation but will basically all be distributed according to the following template Here is the recipe for applying LabelPropagation to the data. These are the semi supervised learning techniques. If the number of labels available is very small however the performance of these algorithms can be suboptimal. However this turned out to be foolish as the class clusters were not sufficiently well distributed. predict X_unlabeled import missingno as msno msno. In a semi supervised learning problem you don t have all the labels or none of them only some of them. png On the other hand they are very computationally expensive. As you can see class differenciability is extremely important for successful labeling. fit X_labeled y_labeled np. Here I demonstrate the LabelSpreading algorithm. Respective to using standard machine learning algorithms for this task the one adaptation that these semi supervised algorithms make is that they generalize better when there are very few labels. The accuracy was nearly 0. For this reason I include them in my notes on Simple techniques for missing data imputation https www. DemonstrationI will demonstrate these algorithms in action. In this plot I show for each of these levels the performance of the LabelPropagation labeling algorithm how well it performs on each class and also which points exactly it gets wrong. org stable _images sphx_glr_plot_label_propagation_structure_001. These are LabelPropagation and LabelSpreading. org stable modules label_propagation. Since imputing missing data is likely to be a vastly more common problem than generalizing labels it is worth keeping these techniques in mind especially for missing data imputation from a small number of samples. Thus these algorithms do not scale well All that being said semi supervised learning is not that different from missing data imputation. If the set of labels is sufficiently large standard machine learning algorithms like kNN and SVM classification may be used to assign classes to unlabeled data. html semi supervised is a wee bit light on the details but basically 1 LabelSpreading is a regularized version of LabelPropagation and 2 they work by building a self similarity matrix https en. If there are n points in the dataset building a self similarity matrix requires O n 2 comparison operations. Application benchmarkI tried applying this algorithm to a real dataset the Open Powerlifting Database to assign Division information to fields lacking it. As you can see the performance of this algorithm on this test dataset is almost synonymous with that of LabelPropagation and again note that the misclassified points in 3 and 4 are mostly make_classification artifacts. Oh well Here s the code for this ConclusionSemi supervised learning is a restatement of the missing data imputation problem which is specific to the small sample missing label case. This is known as the semi supervised learning problem. Note that the algorithms provide regularization and kernel hyperparameters that I will not tune or explore here. This situation occurs particularly often in research contexts where it s often easy to get a small number of labelled data points via hand labelling but significantly harder to gather the full dataset if the full dataset is sufficiently large. org wiki Self similarity_matrix on the dataset then classifying each unlabeled point by finding the existing labeled point it is most similar to. This hopefully isn t surprising. com residentmario simple techniques for missing data imputation. semi_supervised import LabelPropagation trans LabelPropagation trans. It is semi supervised because it lies in between unsupervised learning which does not use labels and supervised learning which requires them. sklearn implements a pair of classifiers for this task. ", "id": "residentmario/notes-on-semi-supervised-learning", "size": "5561", "language": "python", "html_url": "https://www.kaggle.com/code/residentmario/notes-on-semi-supervised-learning", "git_url": "https://www.kaggle.com/code/residentmario/notes-on-semi-supervised-learning", "script": "make_classification seaborn missingno LabelSpreading sklearn.datasets matplotlib.pyplot label LabelPropagation pandas sklearn.semi_supervised numpy ", "entities": "(('2 they', 'self similarity matrix https'), 'be') (('it', 'samples'), 'be') (('available very however performance', 'algorithms'), 'suboptimal') (('requisite section', 'sklearn documentation http scikit'), 'learn') (('it', 'generation contexts'), 'get') (('which', 'label small sample missing case'), 's') (('semi', 'learning problem'), 'know') (('Application benchmarkI', 'it'), 'try') (('parameter more important tuning', 'dataset'), 'make') (('they', 'task'), 'be') (('you', 'them'), 'have') (('set', 'specific problem'), 'exist') (('supervised which', 'them'), 'supervised') (('you', 'them'), 'supervised') (('it', 'existing labeled point'), 'similarity_matrix') (('sklearn', 'task'), 'implement') (('Here I', 'LabelSpreading algorithm'), 'demonstrate') (('missing', 'predictor missing variables'), 'argue') (('full dataset', 'significantly full dataset'), 'occur') (('class differenciability', 'extremely successful labeling'), 'be') (('they', 'other hand'), 'png') (('algorithms', 'larger samples'), 's') (('These', 'learning techniques'), 'be') (('exactly it', 'class'), 'show') (('self similarity matrix', 'comparison n 2 operations'), 'require') (('So algorithms', 'that'), 'use') (('I', 'hyperparameters'), 'note') (('I', 'almost none'), 'seem') (('Here recipe', 'data'), 'tune') (('First we', 'following synthetic dataset'), 'use') (('supervised learning', 'data that imputation'), 'be') (('presence', 'blue actually suprising make_classification'), 'note') (('I', 'data imputation https missing www'), 'include') (('again misclassified points', '3'), 'be') (('Thus algorithms', 'e.'), 'generalize') (('DemonstrationI', 'action'), 'demonstrate') (('SVM classification', 'unlabeled data'), 'be') "}