{"name": "understanding cross lingual models ", "full_name": " h1 Introduction h1 Setup TPU configuration h1 Part 1 Understanding cross lingual models h1 XLM is based on several key concepts h1 Training a Masked Language Model MLM for BERT h1 Unsupervised Cross lingual Representation Learning at Scale h1 Part 2 Implementation using TPU Multiprocessing h1 More Text Cleaning h1 Tokenize encode comments h1 Focal Loss h1 Build the model and check summary h1 Visualize model architecture h1 Learning rate schedule h1 Training h1 Make Submission ", "stargazers_count": 0, "forks_count": 0, "description": "net profile Jeremy_Barnes5 publication 309312650 figure fig1 AS 669424235323406 1536614583578 The process of cross lingual sentiment classification We assume that the opinion units_W640. In a machine translation application it would take a sentence in one language and output its translation in another. Training a Masked Language Model MLM for BERTlet s say the problem statement is Given an input sequence we will randomly mask some words. com xhlulu jigsaw tpu distilbert with huggingface and keras Jigsaw TPU BERT with Huggingface and Keras https www. check the below pictures https www. The Spanish test set is mapped accordingly and the classifier is tested on this cross lingual test set. Each one is broken down into two sub layers https jalammar. Feel free to correct me if I made any mistakes in this kernel. We also present a detailed empirical evaluation of the key factors that are required to achieve these gains including the trade offs between 1 positive transfer and capacity dilution and 2 the performance of high and low resource languages at scale. 8 average accuracy on XNLI 12. A Transformer includes two parts an encoder that reads the text input and generates a lateral representation of it e. ChangeLog version 1 training xlm roberta base for 1 epoch including validation english data in training set version 2 training for 4 epoch poor lb close to 0. The process of cross lingual sentiment classification. These are the language the XLM model supports en es fr de zh ru pt it ar ja id tr nl pl simple fa vi sv ko he ro no hi uk cs fi hu th da ca el bg sr ms bn hr sl zh_yue az sk eo ta sh lt et ml la bs sq arz af ka mr eu tl ang gl nn ur kk be hy te lv mk zh_classical als is wuu my sco mn ceb ast cy kn br an gu bar uz lb ne si war jv ga zh_min_nan oc ku sw nds ckb ia yi fy scn gan tt am. png Popping open that Optimus Prime goodness we see an encoding component a decoding component and connections between them. A win win for everyone in NLP. com miklgr500 jigsaw tpu bert with huggingface and keras 8 Excellent Pretrained Models to get you Started with Natural Language Processing NLP https www. The upgraded BERT is denoted as Translation Language Modeling TLM while the vanilla BERT with BPE inputs is denoted as Masked Language Modeling MLM. num_replicas_in_sync and keeping everything else as it was before version 10 as we see version 9 model diverged and it seems like loss focal_loss gamma 1. io images t The_transformer_encoder_decoder_stack. 0 for focal loss reducing train non toxic comment samples removing translated validation data from train set patience 1 and epoch 6 bug version 4 same as version 3 but trying to fix the problem for 3 epoch i was not monitoring val auc correctly in version 3 version 5 error version 6 patience 2 extra 2 epochs using validation set version 7 using translated validation data focal_loss gamma 1. 25 worked just fine for xlmr base but not with large so will just try binary_crossentropy instead just to make sure whether or not my assumption is correct leaving everything else as it is saving model based on maximum validation accuracyand using EarlyStopping ModelCheckpoint LearningRateScheduler training for 4 epochs version 11 trying to solve error of version 10 version 12 trying to solve bugs of version 11 version 13 adding translated spanish data in training set version 14 oversampling validation english data reducing spanish non english sample a bit for training for 5 epochs within 3 hours tpu limit failed couldn t finish commit within 3 hours tpu limit version 15 trying for 4 epoch and oversampling positive samples of translated validation set version 16 more data for training 958870 total and using 2 epochs because it will take time Note one thing i found in this competition is large subset of trainset helps the learning algorithm perform better here instead of choosing small subset for long training version 17 in version 16 you can see i got NotImplementedError after first epoch and i am unable to save best checkpoint in previous versions i also tried monitor val_acc and monitor val_accuracy but none of them saving best checkpoint for me where am i making mistakes in version 17 i will try to get rid of the error and will train again if you know why i am unable to save best checkpoint please help me in the comment box thanks in advance Imports Setup TPU configuration Part 1 Understanding cross lingual modelsthe paper titled Cross lingual Language Model Pretraining https arxiv. Introduction In this kernel i will try to share my understanding and findings of cross lingual models. How XLM works The paper Cross lingual Language Model Pretraining https arxiv. a vector for each word and a decoder that produces the translated text from that representation. com maroberti fastai with transformers bert roberta Jigsaw Multilingual Toxicity EDA Models https www. This is what our model will try to predict. As in BERT the goal of the model is to predict the masked tokens however with the new architecture the model can use the context from one language to predict tokens in the other as different words are masked words in each language they are chosen randomly. The outputs of the self attention layer are fed to a feed forward neural network. 3 average F1 score on MLQA and 2. The complete XLM model was trained by training both MLM and TLM and alternating between them. The English train set is used to train a classifier. 07291 presents two innovative ideas a new training technique of BERT for multilingual classification tasks and the use of BERT as initialization of machine translation models. com max 1400 0 lBYVNRe1esIXn1qE. Finally we show for the first time the possibility of multilingual modeling without sacrificing per language performance XLM Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. implementation is adapted from tarunpaparaju s kernel Jigsaw Multilingual Toxicity EDA Models https www. We assume that the opinion units have already been determined. 9151 where the only difference in version 9 was to add extra text cleaning techniques now in version 9 i will move to xlmr large model with maxlen 192 BATCH_SIZE 16 strategy. Check out the below comparison https cdn. com dimitreoliveira flower classification with tpus eda and baseline notebook. com facebookresearch XLM The Illustrated Transformer https jalammar. A High Level Look Let s begin by looking at the model as a single black box. Let s now use BertForMaskedLM to predict a masked token This was a small demo of training a Masked Language Model on a single input sequence. XLM R performs particularly well on low resource languages improving 11. 8 in XNLI accuracy for Swahili and 9. The model outperforms other models in a cross lingual classification task sentence entailment in 15 languages and significantly improves machine translation when a pre trained model is used for initialization of the translation model. Now that our data is rightly pre processed for BERT we will create a Masked Language Model. The model significantly outperforms other prominent models Unsupervised Cross lingual Representation Learning at ScaleAbstract This paper Unsupervised Cross lingual Representation Learning at Scale https arxiv. Each training sample consists of the same text in two languages whereas in BERT each sample is built from a single language. We will make XLM R code data and models publicly available. com tarunpaparaju jigsaw multilingual toxicity eda models Flower Classification with TPUs EDA and Baseline https www. jpg here L1 means language 1 and L2 means language 2 https slideplayer. validation and testing dataset Focal Loss Build the model and check summary Define Define ReduceLROnPlateau callback Visualize model architecture Learning rate schedule Trainingwe train it for 2 more epochs on the validation set which is significantly smaller but contains a mixture of different languages. io images t The_transformer_encoders_decoders. words to the left and right of a masked word. XLM is based on several key concepts Transformers The Transformer architecture is at the core of almost all the recent major developments in NLP. io images t Transformer_encoder. The decoding component is a stack of decoders of the same number. 1 average F1 score on NER. The decoder has both those layers but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence similar what attention does in seq2seq models. jpg First instead of using word or characters as the input of the model it uses Byte Pair Encoding BPE that splits the input into the most common sub words across all languages thereby increasing the shared vocabulary between languages. png This Transformer architecture outperformed both RNNs and CNNs convolutional neural networks. Make Submission References Jigsaw TPU XLM Roberta https www. We ll look closer at self attention later in the post. com xhlulu jigsaw tpu xlm roberta Jigsaw TPU DistilBERT with Huggingface and Keras https www. png The next step would be to convert this into a sequence of integers and create PyTorch tensors of them so that we can use them directly for computation Notice that we have set MASK at the 8th index in the sentence which is the word Hensen. only the predecessors of each word in 2018 updated BERT used the Transformer s encoder to learn a language model by masking dropping some of the words and then trying to predict them allowing it to uses the entire context i. io images t the_transformer_3. The exact same feed forward network is independently applied to each position. png The encoders are all identical in structure yet they do not share weights. 2 for Urdu over the previous XLM model. png To assess the contribution of the model the paper presents its results on sentence entailment task classify relationship between sentences using XNLI dataset that includes sentences in 15 languages. Part 2 Implementation using TPU MultiprocessingEven though i am a pytorch lover but not sure if the video below is true for 2020 also or not. Clean the text remove usernames and links More Text Cleaning applying text cleaning techniques like clean_text replace_typical_misspell handle_contractions fix_quote on train test and validation set we can see from above 2 cells that text cleaning for train validation and test set takes 8 minutes that means we are losing some of our vital times for training on tpu which is 3 hours max. 07291 by Facebook AI named XLM presents an improved version of BERT to achieve state of the art results in both classification and translation tasks. so it would be a good idea if we create another kernel and save above 2 cells newly updated train val and test_data as kernels output then using those files we can quickly import our new train test and validation data here which will save time for training model on TPU Roc Auc Evaluation metric Tokenize encode comments Load bert tokenizer Encode comments previously we lost 8 minutes and here 12 minutes sum them and we have lost 20 minutes which is almost 1 epoch training time here. The model then should predict the original value of the masked words based on the context provided by the other non masked words in the sequence. We train a Transformer based masked language model on one hundred languages using more than two terabytes of filtered CommonCrawl data. com blog 2019 07 pytorch transformers nlp python First let s prepare a tokenized input from a text string using BertTokenizer This is how our text looks like after tokenization https cdn. For quick demonstration purpose i will use code from analyticsvidhya s article Introduction to PyTorch Transformers An Incredible Library for State of the Art NLP with Python code https www. png The below animation wonderfully illustrates how Transformer works on a machine translation task https cdn. The computational resources required to train models were reduced as well. net publication 309312650_Exploring_Distributional_Representations_and_Machine_Translation_for_Aspect based_Cross lingual_Sentiment_Classification Fastai with Transformers BERT RoBERTa. The model also receives the language ID and the order of the tokens in each language i. The new metadata helps the model learn the relationship between related tokens in different languages. com slide 12311059 73 images 13 Cross lingual Document Classification. It introduced an attention mechanism that processes the entire text input simultaneously to learn contextual relations between words or sub words. io images t Transformer_decoder. Second it upgrades the BERT architecture in two manners 1. the Positional Encoding separately. png The encoder s inputs first flow through a self attention layer a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. com wp content uploads 2019 07 Screenshot from 2019 07 18 15 18 42. 02116 shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross lingual transfer tasks. XLM uses a known pre processing technique BPE and a dual language training mechanism with BERT in order to learn relations between words in different languages. com wp content uploads 2019 03 transformercomparison. Our model dubbed XLM R significantly outperforms multilingual BERT mBERT on a variety of cross lingual benchmarks including 13. gif The vanilla Transformer has only limited context of each word i. 25 version 8 More text cleaning everything else is left same as version 7 so that we can compare version 7 with version 8 s result for additional text cleaning clean_text replace_typical_misspell handle_contractions fix_quote version 9 as you can see now that in version 7 lb 0. io illustrated transformer Exploring Distributional Representations and Machine Translation for Aspect based Cross lingual Sentiment Classification https www. com blog 2019 07 pytorch transformers nlp python facebookresearch XLM https github. com tarunpaparaju jigsaw multilingual toxicity eda models XLM R handles the following 100 languages Afrikaans Albanian Amharic Arabic Armenian Assamese Azerbaijani Basque Belarusian Bengali Bengali Romanized Bosnian Breton Bulgarian Burmese Burmese Catalan Chinese Simplified Chinese Traditional Croatian Czech Danish Dutch English Esperanto Estonian Filipino Finnish French Galician Georgian German Greek Gujarati Hausa Hebrew Hindi Hindi Romanized Hungarian Icelandic Indonesian Irish Italian Japanese Javanese Kannada Kazakh Khmer Korean Kurdish Kurmanji Kyrgyz Lao Latin Latvian Lithuanian Macedonian Malagasy Malay Malayalam Marathi Mongolian Nepali Norwegian Oriya Oromo Pashto Persian Polish Portuguese Punjabi Romanian Russian Sanskri Scottish Gaelic Serbian Sindhi Sinhala Slovak Slovenian Somali Spanish Sundanese Swahili Swedish Tamil Tamil Romanized Telugu Telugu Romanized Thai Turkish Ukrainian Urdu Urdu Romanized Uyghur Uzbek Vietnamese Welsh Western Frisian Xhosa Yiddish. png The encoding component is a stack of encoders the paper stacks six of them on top of each other there s nothing magical about the number six one can definitely experiment with other arrangements. 8987 and in version 8 we got lb 0. com blog 2019 03 pretrained models get started nlp Introduction to PyTorch Transformers An Incredible Library for State of the Art NLP with Python code https www. com wp content uploads 2019 03 transform20fps. 89 version 3 gamma 2. ", "id": "mobassir/understanding-cross-lingual-models", "size": "16537", "language": "python", "html_url": "https://www.kaggle.com/code/mobassir/understanding-cross-lingual-models", "git_url": "https://www.kaggle.com/code/mobassir/understanding-cross-lingual-models", "script": "build_lrfn textblob Fore plotly.express stats nltk.stem.wordnet date transformers regularizers build_model layers scipy clean_numbers matplotlib.cm tensorflow.keras.backend matplotlib.pyplot tensorflow.keras.callbacks ModelCheckpoint PIL metrics Timestamp tensorflow.keras.activations Model TreebankWordTokenizer plotly.subplots constraints tensorflow.keras WordCloud googletrans model_to_dot init torch.nn.functional TweetTokenizer Style make_subplots plotly.graph_objects regular_encode networkx gensim.models kaggle_datasets BertModel accuracy_score tensorflow.keras.models initializers sklearn.cluster Adam Dropout TextBlob nltk.stem torch.nn BertForMaskedLM Embedding sklearn KaggleDatasets tensorflow wordnet nltk pandas nltk.corpus handle_contractions GRU optimizers colorama IPython.core.display IPython.display nltk.tokenize.treebank sklearn.utils lrfn nltk.tokenize pytorch_transformers numpy Image SpatialDropout1D fix_quote Word2Vec Translator on_epoch_end replace_typical_misspell tokenizers tensorflow.keras.initializers SentimentIntensityAnalyzer ReduceLROnPlateau RocAucEvaluation(Callback) CSVLogger Detector tensorflow.keras.constraints shuffle clean_data tqdm.notebook WordNetLemmatizer activations plotly.figure_factory HTML tensorflow.keras.optimizers tensorflow.keras.regularizers sklearn.metrics clean_text focal_loss BertTokenizer tensorflow.keras.layers _get_mispell LearningRateScheduler polyglot.detect seaborn Back TfidfVectorizer SVG clean Dense Callback sklearn.feature_extraction.text STOPWORDS backend as K tqdm roc_auc_score nltk.sentiment.vader replace LSTM PCA backend KMeans datetime EarlyStopping callback stopwords Input keras.utils Conv1D __init__ BertWordPieceTokenizer word_tokenize focal_loss_fixed sklearn.decomposition wordcloud ", "entities": "(('We', 'later post'), 'look') (('We', 'CommonCrawl filtered data'), 'train') (('wonderfully how Transformer', 'machine translation task https cdn'), 'png') (('they', 'language'), 'be') (('com blog', 'Python code https www'), 'start') (('it', 'loss'), 'num_replicas_in_sync') (('i', 'cross lingual models'), 'introduction') (('This', 'input single sequence'), 'let') (('Transformer architecture', 'NLP'), 'base') (('pytorch video', 'below 2020'), 'implementation') (('encoding component', 'decoding them'), 'png') (('XLM R', '11'), 'perform') (('model', 'sequence'), 'predict') (('Transformer png architecture', 'RNNs convolutional neural networks'), 'outperform') (('io', 'Machine Sentiment Classification https Aspect based Cross lingual www'), 'illustrate') (('Khmer Korean Kurdish Kurmanji Kyrgyz Macedonian Malagasy Malay Malayalam Marathi Mongolian Nepali Norwegian Oriya Oromo Scottish Gaelic Serbian Sindhi Sinhala Slovak Slovenian Somali Sundanese Swahili Swedish Tamil Tamil Telugu Latin Latvian Lithuanian Pashto Persian Polish Portuguese Punjabi Romanian Russian Telugu', 'Hungarian Icelandic Japanese Javanese Kannada Indonesian Irish Italian Kazakh'), 'com') (('that', 'words'), 'introduce') (('accordingly classifier', 'cross test lingual set'), 'map') (('how text', 'tokenization https cdn'), 'blog') (('s', 'single black box'), 'let') (('Second it', 'two manners'), 'upgrade') (('one', 'sub layers https two jalammar'), 'break') (('they', 'weights'), 'png') (('when pre trained model', 'translation model'), 'outperform') (('which', 'sentence'), 'png') (('XLM', 'different languages'), 'use') (('Excellent Pretrained 8 you', 'Natural Language Processing NLP https www'), 'bert') (('language', 'https 1 language 2 slideplayer'), 'mean') (('train English set', 'classifier'), 'use') (('i', 'Python code https www'), 'use') (('pretraining', 'cross transfer lingual tasks'), 'show') (('it', 'specific word'), 'png') (('that', 'languages'), 'first') (('model', 'what'), 'be') (('decoding component', 'same number'), 'be') (('model', '13'), 'dub') (('we', 'Masked Language Model'), 'create') (('How XLM', 'paper lingual Language Model Pretraining https Cross arxiv'), 'work') (('9 you', '7 lb 0'), 'leave') (('which', 'different languages'), 'Build') (('model', 'different languages'), 'help') (('com xhlulu jigsaw', 'Huggingface'), 'tpu') (('vanilla Transformer', 'word'), 'gif') (('gan tt', 'en de it'), 'be') (('we', 'randomly words'), 'train') (('Submission References Jigsaw', 'XLM Roberta https www'), 'make') (('sample', 'single language'), 'consist') (('XLM complete model', 'them'), 'train') (('I', 'kernel'), 'feel') (('model', 'Unsupervised Representation Scale https Cross lingual arxiv'), 'outperform') (('that', 'representation'), 'vector') (('that', '15 languages'), 'png') (('07291', 'classification tasks'), 'present') (('that', 'scale'), 'present') (('which', 'tpu'), 'clean') (('s', 'six definitely other arrangements'), 'png') (('com xhlulu jigsaw', 'keras Huggingface'), 'tpu') (('ChangeLog training xlm roberta version 1 base', 'close 0'), 'set') (('implementation', 'kernel'), 'adapt') (('that', 'it'), 'include') (('it', 'another'), 'take') (('opinion', 'sentiment cross lingual classification'), 'figure') (('BERT', 'Masked Language Modeling MLM'), 'denote') (('07291', 'machine translation models'), 'present') (('Finally we', 'GLUE benchmarks'), 'show') (('9 i', 'maxlen 192 BATCH_SIZE 16 strategy'), '9151') (('attention', 'seq2seq models'), 'have') (('i', 'validation 7 translated data'), '0') (('which', '20 minutes'), 'be') (('model', 'language'), 'receive') (('exact same feed forward network', 'independently position'), 'apply') (('Imports Setup', 'Understanding modelsthe Part 1 cross lingual paper'), 'work') (('computational resources', 'models'), 'reduce') (('com pytorch 2019 07 transformers', 'python facebookresearch XLM https github'), 'blog') (('outputs', 'feed forward neural network'), 'feed') (('it', 'entire context'), 'use') "}