{"name": "featuretools for good ", "full_name": " h1 Featuretools for Good h2 Automated Feature Engineering h3 Data Preprocessing h2 Missing Values h2 Domain Knowledge Feature Construction h3 Remove Squared Variables h2 Remove Highly Correlated Columns h1 Establish Correct Variable Types h1 EntitySet and Entities h1 Normalize Household Table h3 Table Relationships h1 Deep Feature Synthesis h1 Feature Selection h3 Training and Testing Data h1 Correlations with the target h2 Subset to Relevant Data h3 Labels for Training h2 Custom Evaluation Metric for LightGBM h1 Modeling with Gradient Boosting Machine h2 Feature Importances h1 Custom Primitive h3 Range Primitive h3 Correlation Primitive h1 More Featuretools h1 Post Processing Function h2 Results After Post Processing h1 Increase number of features h2 Remove Zero Importance Features h1 Add in Divide Primitive h2 Increase to 1500 features h2 Go to 2000 h1 Try Modeling with more folds h1 5000 Features h3 5000 features with 10 fold modeling h1 Comparison of Models h1 Save Data h1 Conclusions ", "stargazers_count": 0, "forks_count": 0, "description": "Featuretools for GoodIn this notebook we will implement automated feature engineering with Featuretools https docs. In this case the instances are households. I think we should be able to add more features as long as we continue to impose feature selection. Modeling with Gradient Boosting MachineThe hyperparameters used here _have not been optimized_. Below we convert the Boolean variables to the correct type. This is a little tedious but also necessary. html or an introductory blog post here. Remove one out of every pair of columns with a correlation threshold above the correlation_threshold 6. To start with we use the default agg and trans primitives in a call to ft. There are two types of primitives which are operations applied to data Transforms applied to one or more columns in a _single table_ of data Aggregations applied across _multiple tables_ using the relationships between tablesWe generate the features by calling ft. I m not running the GBM with a random seed so the same set of features can produce different cross validation results. The next step is to optimize the model for these features. The cross validation accuracy continues to increase as we add features. These are created because some of transform primitives might have affected the Target. com willkoehrsen a complete introduction and walkthrough. Deep Feature SynthesisHere is where Featuretools gets to work. Feature SelectionWe can do some rudimentary feature selection removing one of any pair of columns with a correlation greater than 0. Although these correlations only show linear relationships they can still provide an approximation of what features will be useful to a machine learning model. This is meant only as a first pass at modeling with these features. These shows the predictions on an individual not household level we set all individuals to 4 if they did not have a head of household. Finally the same with the ordinal variables. https towardsdatascience. The objective of this data science for good problem is to predict the poverty of households in Costa Rica. Post Processing FunctionThere are a number of steps after generating the feature matrix so let s put all of these in a function. EntitySet and EntitiesAn EntitySet in Featuretools holds all of the tables and the relationships between them. We need to make sure the length of the labels matches the length of the training dataset. Extract the training and testing data along with labels and ids needed for making submissions Results After Post Processing Increase number of features Remove Zero Importance Features Add in Divide PrimitiveNext we ll add a divide transform primitive into the deep feature synthesis call. There still might be additional gains to increasing the number of features and or using different custom primitives. This build features using any of the applicable primitives for each column in the data. Eventually we re probably going to be overfitting to the training data but the we can address that through regularization and feature selection. com automated feature engineering in python 99baf11cc219 We ll read in the data and join the training and testing set together. Since these are the same for all members of a household we can directly add these as columns in the household table using additional_variables. Household variables Boolean Yes or No Ordered Discrete Integers with an ordering Continuous numericBelow we manually define the variables in each category. Table RelationshipsNormalizing the entity automatically adds in the relationship between the parent household and the child ind. Defining a custom evaluation metric for Light GBM is not exactly straightforward but we can manage. Training and Testing Data Correlations with the targetFeaturetools has built features with moderate correlations with the Target. Remove columns with a missing percentage above the missing_threshold 4. The index of the household table is idhogar which uniquely identifies each household. To prevent featuretools from building the exact same features we already have we can add drop_exact and pass in the feature names as strings using the get_name functionality. Go to 2000This is getting ridiculous. Range PrimitiveWe can also make a custom primitive that calculates the correlation coefficient between two columns. Remove any duplicated columns. Individual Variables these are characteristics of each individual rather than the household Boolean Yes or No 0 or 1 Ordered Discrete Integers with an ordering2. For anyone new to featuretools check out the documentation https docs. We ll call the first table data since it contains all the information both at the individual level and at the household level. Only one way to find out through data Let s look at the performance of models so far. The gradient boosting machine seems very good at cutting through the swath of features. This relationship links the two tables and allows us to create deep features by aggregating individuals in each household. ConclusionsFeaturetools certainly can make our job easier for this problem Adding features continues to improve the validation score with mixed effects on the public leaderboard. Increase to 1500 features1000 is clearly not enough Most of these features are highly correlated but we can still find useful features as evidenced by the feature importances. This library is extremely easy to get started with and very powerful as the score from this kernel illustrates. Then we convert the float variables. However why go to the trouble if Featuretools can do that for us That one call alone gave us 147 features to train a model This was only using the default primitives as well. We need to remove any columns containing derivations of the Target. Correlation Primitive More FeaturetoolsWhy stop with 150 features Let s add in a few more primitives and start creating more. I look forward to seeing what the community can come up with for this problem. Most of these features are aggregations we could have made ourselves. For example it will automatically aggregate the individual level data at the household level. At first we ll limit the features to 1000. Automated Feature EngineeringAutomated feature engineering should be a _default_ part of your data science workflow. Normalize Household TableNormalization allows us to create another table with one unique row per instance. Remove columns with only a single unique value. At the moment Featuretools is the only open source Python library available for automated feature engineering. com minute quick start for the Costa Rican Household Poverty Challenge. At the moment we only have a single table but we can create multiple tables through normalization. Subset to Relevant Data Labels for TrainingWe ll now get into modeling. Remove Highly Correlated Columns Establish Correct Variable TypesWe need to specify the correct variables types 1. We can use more primitives or write our own to build more features. The distribution is close to what we observe in the training labels which are provided on the household level. Using feature primitives Deep Feature Synthesis can build hundreds or 1000s as we will later see of features from the relationships between tables and the columns in tables themselves. Manual feature engineering is limited both by human creativity and time constraints but automated methods have no such constraints. Featuretools uses the table relationships to aggregate features as required. The new table is derived from the data table and we need to bring along any of the household level variables. All of the variable types have already been confirmed. Save DataWe can save the final selected featuretools feature matrix created with a maximum of 2000 features. com getting_started install. This will be used for Bayesian optimization of model hyperparameters. 5000 Features 5000 features with 10 fold modeling Comparison of ModelsAt this point we might honestly ask if there is any benefit to increasing the number of features. We should also make sure the len of test_ids the idhogar of the testing households is the same as the length of the testing dataset. Feature ImportancesThe utility function below plots feature importances and can show us how many features are needed for a certain cumulative level of importance. Missing Values Domain Knowledge Feature Construction Remove Squared VariablesThe gradient boosting machine does not need the squared version of variables it if already has the original variables. The gradient boosting machine implemented in LightGBM usually does well Custom Evaluation Metric for LightGBMThis is the F1 Macro score used by the competition. We ll write a simple function that finds the range of a numeric column. The three columns not in the above lists are Id Idhogar and Target. The cells below remove any columns that aren t in the data these may have been removed due to correlation. All that s left is to model The cell below runs the gradient boosting machine model and saves the results. Try Modeling with more foldsAs a final model we ll increase the number of folds to 10 and see if this results in more stable predictions across folds. My focus is now going to shift to modeling but I encourage anyone to keep adjusting the featuretools implementation. __Featuretools should be a default part of your data science workflow. __ The tool is incredibly simple to use and delivers considerable value creating features that we never would have imagined. It s concerning that there is so much variation between folds but that is going to happen with a small imbalanced testing set. Data Preprocessing These steps are laid out in the kernel A Complete Introduction and Walkthrough https www. A random seed would ensure consistent results but may have a singificant effect on the predictions. Custom PrimitiveTo expand the capabilities of featuretools we can write our own primitives to be applied to the data. Replace infinite values with np. They involve correcting missing values creating a few features that Featuretools can build on top of. ", "id": "willkoehrsen/featuretools-for-good", "size": "10466", "language": "python", "html_url": "https://www.kaggle.com/code/willkoehrsen/featuretools-for-good", "git_url": "https://www.kaggle.com/code/willkoehrsen/featuretools-for-good", "script": "Counter lightgbm range_calc featuretools.primitives IPython.display p_corr_calc numpy seaborn macro_f1_score f1_score most to least plot_feature_importances model_gbm StratifiedKFold matplotlib.pyplot post_process featuretools sklearn.model_selection pandas featuretools.variable_types s_corr_calc make_agg_primitive display sklearn.metrics make_scorer collections ", "entities": "(('Featuretools', 'aggregate features'), 'use') (('Data Training Correlations', 'Target'), 'build') (('com', 'complete introduction'), 'willkoehrsen') (('This', 'default only primitives'), 'go') (('that', 'testing small imbalanced set'), 's') (('we', 'ft'), 'use') (('us', 'household'), 'link') (('highly we', 'feature importances'), 'be') (('these', 'correlation'), 'remove') (('Adding features', 'public leaderboard'), 'make') (('This', 'model hyperparameters'), 'use') (('us', 'instance'), 'allow') (('some', 'Target'), 'create') (('we', 'additional_variables'), 'add') (('instances', 'case'), 'be') (('we', 'regularization selection'), 'go') (('which', 'household level'), 'be') (('Below we', 'correct type'), 'convert') (('they', 'household'), 'show') (('random seed', 'predictions'), 'ensure') (('next step', 'features'), 'be') (('we', 'data'), 'expand') (('we', 'category'), 'variable') (('this', 'folds'), 'try') (('we', 'features'), 'continue') (('length', 'training dataset'), 'need') (('s', 'more'), 'stop') (('Data', 'Complete Introduction'), 'lay') (('boosting gradient machine', 'already original variables'), 'miss') (('that', 'numeric column'), 'write') (('This', 'data'), 'build') (('Save DataWe', 'features'), 'save') (('Feature SelectionWe', 'greater 0'), 'do') (('feature Manual engineering', 'time automated such constraints'), 'limit') (('how many features', 'importance'), 'feature') (('it', 'household level'), 'call') (('idhogar', 'testing dataset'), 'make') (('notebook we', 'Featuretools https docs'), 'featuretool') (('s', 'models'), 'way') (('This', 'features'), 'mean') (('boosting gradient machine', 'features'), 'seem') (('Featuretools', 'Python feature only open available automated engineering'), 'be') (('We', 'more features'), 'use') (('we', 'household level variables'), 'derive') (('which', 'uniquely household'), 'be') (('these', 'Ordered Discrete ordering2'), 'be') (('we', 'themselves'), 'build') (('we', 'feature synthesis primitive deep call'), 'extract') (('point we', 'features'), 'feature') (('features', 'machine learning model'), 'provide') (('We', 'Target'), 'need') (('community', 'problem'), 'look') (('Featuretools', 'top'), 'involve') (('Table', 'parent household'), 'add') (('we', 'that'), '_') (('Subset', 'now modeling'), 'get') (('same set', 'cross validation different results'), 'run') (('that', 'two columns'), 'make') (('Custom Evaluation usually well Metric', 'F1 Macro competition'), 'do') (('now I', 'featuretools implementation'), 'go') (('as long we', 'feature selection'), 'think') (('Automated Feature EngineeringAutomated feature engineering', '_ _ data science workflow'), 'be') (('already we', 'get_name functionality'), 'prevent') (('99baf11cc219 We', 'training'), 'com') (('_', 'ft'), 'be') (('we', 'ourselves'), 'be') (('objective', 'Costa Rica'), 'be') (('it', 'household level'), 'aggregate') (('we', '1000'), 'limit') (('we', 'normalization'), 'have') (('so s', 'function'), 'be') (('Remove', 'variables correct types'), 'need') (('three columns', 'above lists'), 'be') (('_ _ Featuretools', 'data science default workflow'), 'be') (('All', 'variable types'), 'confirm') (('s', 'results'), 'be') (('extremely very score', 'kernel'), 'be') (('EntitySet EntitySet', 'them'), 'hold') (('exactly we', 'Light GBM'), 'be') "}