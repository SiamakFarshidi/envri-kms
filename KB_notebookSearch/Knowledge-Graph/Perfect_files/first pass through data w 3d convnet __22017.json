{"name": "first pass through data w 3d convnet ", "full_name": " h2 Applying a 3D convolutional neural network to the data h2 Quick introduction to Kaggle h2 About this specific competition h2 Requirements and suggestions for following along h2 Alright let s get started h1 Section 1 Handling Data h1 Section 2 Processing and viewing our Data h1 Section 3 Preprocessing our Data h1 Section 4 3D Convolutional Neural Network h2 Moment o truth h1 Section 5 Concluding Remarks h2 Now what ", "stargazers_count": 0, "forks_count": 0, "description": "com c data science bowl 2017 discussion 27922 1 for how an actual doctor reads a CT scan. net neural networks machine learning tutorial 4 https pythonprogramming. net matplotlib intro tutorial Alright let s get started Section 1 Handling Data Assuming you ve downloaded the data what exactly are we working with here The data consists of many 2D slices which when combined produce a 3 dimensional rendering of whatever was scanned. 62Epoch 9 completed out of 10 loss 122785997. size of window movement of window as you slide about 5 x 5 x 5 patches 32 channels 64 features to compute. 6Epoch 3 completed out of 10 loss 5744945978. 6Epoch 6 completed out of 10 loss 1014763813. Now I will just make a slight modification to all of the code up to this point and add some new final lines to preprocess this data and save the array of arrays to a file Section 4 3D Convolutional Neural Network Moment o truth Okay we ve got preprocessed normalized data. The real number we need to beat is if our network was to always predict a single class. if you have improvements share them This is going to stay pretty messy. That s why this is a competition. This current competition is a 2 stage competition where you have to participate in both stages to win. This notebook is my actual personal initial run through this data and my notes along the way. Now we re set to train the network. In our test that s the real number to beat. I ve never had data to try one on before so I was excited to try my hand at it Before we can feed the data through any model however we need to at least understand the data we re working with. Stage one has you competing based on a validation dataset. You probably already have numpy if you installed pandas but just in case numpy is pip install numpy 1 https pythonprogramming. We actually don t have to have all of the data prepared before we go through the network. If at all possible I prefer to separate out steps in any big process like this so I am going to go ahead and pre process the data so our neural network code is much simpler. org wiki DICOM 2 https pypi. these images are just WAY too big for a convolutional neural network to handle without some serious computing power. For example you can check out this tutorial https www. Example output that I got Epoch 1 completed out of 10 loss 195148607547. Consider neural network variables like layers learning rate activation functions optimizer. Someone feel free to enlighten me how one could actually calculate this number beforehand. Now let s see what an actual slice looks like. 150 is still going to wind up likely being waaaaaaay to big. Then there will be actual blind or out of sample testing data that you will actually use your model on which will spit out an output CSV file with your predictions based on the input data. see what happens Now I am not about to stuff a neural networks tutorial into this one. for you To install the CPU version of TensorFlow just do pip install tensorflow To install the GPU version of TensorFlow you need to get alllll the dependencies and such. With pictures we can do all sorts of things. That s fine we can play with that constant more later we just want to know how to do it. Thus we already know out of the gate that we re going to need to downsample this data quite a bit AND somehow make the depth uniform. holy moly 512 x 512 This means our 3D rendering is a 195 x 512 x 512 right now. Thus we can hopefully just average this slice together and maybe we re now working with a centimeter or so. com c data science bowl 2017 discussion 27922 2 https www. 54Epoch 8 completed out of 10 loss 289082075. Applying a 3D convolutional neural network to the data Welcome everyone to my coverage of the Kaggle Data Science Bowl 2017. Finishing accuracy Accuracy 0. com questions 312443 how do you split a list into evenly sized chunks stage 1 for real. We will also be making use of Pandas for some data analysis 2 Matplotlib for data visualization 3 You do not need to go through all of those tutorials to follow here but if you are confused it might be useful to poke around those. We re sorting by the actual image position in the scan. Let s say we want to have 20 scans instead. This is just a theory it has to be tested. AFAIK it s the padding that causes this to not be EXACTLY 50 000 50 x 50 x 20 is the size of our actual input data which is 50 000 total. We can though In this case it s pretty close. It s still possible to cheat. net data analysis python pandas tutorial introduction 3 https pythonprogramming. Run this locally train_neural_network x. In this case that s the chest cavity of the patient. net how to cuda gpu tensorflow deep learning tutorial 2 https www. Neural networks are capable of amazing things only when we have the datasets to support it. When running locally make sure your training data is NOT the sample images it should be the stage1 images. We have about 200 slices though I d feel more comfortable if I saw a few more. 9992289899768697 Section 5 Concluding Remarks So how did we do Well we overfit almost certainly. 64 features image X image Y image Z If you are working with the basic sample data use maybe 2 instead of 100 here. You can submit up to 3 entries a day so you want to be very happy with your model and you are at least slightly disincentivised from trying to simply fit the answer key over time. Thus we have to begin by simply trying things and seeing what happens I have a few theories about what might work but my first interest was to try a 3D Convolutional Neural Network. I couldn t think of anything off the top of my head for this so I Googled how to chunk a list into a list of lists. Do note that now to have a bigger window your processing penalty increases significantly as we increase in size obviously much more than with 2D windows. Being a realistic data science problem we actually don t really know what the best path is going to be. Your submission is scored based on the log loss of your predictions. com questions 312443 how do you split a list into evenly sized chunks 1 we ve got ourselves a nice chunker generator. It s going to take a while. Luckily for us there already exists a Python package for reading dicom files Pydicom 2. Awesome That s actually a decently large hurdle. org data for another 888 scans. You will also need numpy here. 505a20caf6ab6df4643644c923f06a5eb 0. In this case the submission file should have two columns one for the patient s id and another for the prediction of the liklihood that this patient has cancer like id cancer01e349d34c02410e1da273add27be25c 0. I ll have us stick to just the base dataset again mainly so anyone can poke around this code in the kernel environment. you don t have enough data to really do this I am passing for the sake of notebook space but we are getting 1 shaping issue from one input tensor. net loading images python opencv tutorial Section 2 Processing and viewing our Data Alright so we re resizing our images from 512x512 to 150x150. com c data science bowl 2017 discussion 27666 2 official external data thread. My thought is that what we have is really a big list of slices. I expect that with a large enough dataset this wouldn t be an actual issue but with this size of data it might be of huge importance. So what changes EVERYTHING OMG IT S THE END OF THE WORLD AS WE KNOW IT It s not really all too bad. It might be the case our testing set has more cancerous examples or maybe less we really don t know. This is what you will upload to kaggle and your score here is what you compete with. net introduction to python programming 2 https pythonprogramming. 7Epoch 4 completed out of 10 loss 3268944715. In effort to not turn this notebook into an actual book however we re going to move forward We can now see our new data by doing Section 3 Preprocessing our Data Okay so we know what we ve got and what we need to do with it. Not sure why will have to look into it. com gzuidhof data science bowl 2017 full preprocessing tutorialAlright so above we just went ahead and grabbed the pixel_array attribute which is what I assume to be the scan slice itself we will confirm this soon but immediately I am surprised by this non uniformity of slices. I am not going to ask the Kaggle online kernel to even bother building this computation graph so I will comment out the line to actually run this. No cancer always would give us 72 accuracy. We might just simply not have enough data. Also there s no good reason to maintain a network in GPU memory while we re wasting time processing the data which can be easily done on a CPU. 6Epoch 5 completed out of 10 loss 1916325681. com questions 312443 how do you split a list into evenly sized chunks some patients don t have labels so we ll just pass on this for now again some patients are not labeled but JIC we still want the error if something else is wrong with our code Credit Ned Batchelder Link http stackoverflow. If you do cheat you wont win anything since you will have to disclose your model for any prizes. If you do not have opencv do a pip install cv2 Want to learn more about what you can do with Open CV Check out the Image analysis and manipulation with OpenCV and Python tutorial 1. I can at least think of A way and that s all we need. just how much data do we have here Oh. Create new data from the existing data by adding noise. How about our accuracy Due to the lower amount of data on Kaggle I have no idea what number you re seeing just know it s probably not all that great. Bottom line There are tons of options here. Let s see what the best score our classifer could get is if it just always picked the most common class So actually our dataset has 1035 non cancer examples and 362 cancerous examples. Before just adding any noise it would be wise to research what exactly doctors do when analyzing CT scans for cancerous tumors. There are numerous ways that we could go about creating a classifier. Above we iterate through each patient we grab their label we get the full path to that specific patient inside THAT path contains 200ish scans which we also iterate over BUT also want to sort since they wont necessarily be in proper order. 1 http 20http stackoverflow. com watch v r7 WPbx8VuY 3 https pythonprogramming. Just in case you are new how does all this work In general Kaggle competitions will come with training and testing data for you to build a model on where both the training and testing data comes with labels so you can fit a model. In many more realistic examples in the world however your dataset will be so large that you wouldn t be able to read it all into memory at once anyway but you could still maintain one big database or something. If you do not have matplotlib do pip install matplotlib Want to learn more about Matplotlib Check out the Data Visualization with Python and Matplotlib tutorial 1. Overlaying other pictures on top rotating messing with edges adding various transforms. We ve got to actually figure out a way to solve that uniformity problem but also. Regardless this much data wont be an issue to keep in memory or do whatever the heck we want. You wouldn t want to add noise that conflicts here most likely. Bring in more outside data. com gzuidhof data science bowl 2017 full preprocessing tutorial the first slice Credit Ned Batchelder Link http stackoverflow. At the release of stage 2 the validation set answers are released and then you make predictions on a new test set that comes out at the release of this second stage. This is how real programming is happens. This is a raw look into the actual code I used on my first pass there s a ton of room for improvment. I am by no means an expert data analyst statistician and certainly not a doctor. There s always a sample submission file in the dataset so you can see how to exactly format your output predictions. Do a pip install pydicom and pip install pandas and let s see what we ve got 1 https en. At the end you can submit 2 final submissions allowing you to compete with 2 models if you like. You will need to accept the terms of the competition to proceed with downloading the data. net tensorflow introduction machine learning tutorial 5 https pythonprogramming. Are we totally done. If any of you would like to improve this chunking averaging code feel free. Now we re ready to feed it through our 3D convnet and. All this said this specific competition isn t actually just right or wrong. This might be problematic and we might need to actually normalize this dataset. My goal here is that anyone even people new to kaggle can follow along. 63Epoch 2 completed out of 10 loss 14236109414. com c data science bowl 2017 discussion 27666 for reading dicom files for doing directory operations for some simple data analysis right now just to load in the labels data and quickly reference it Change this to wherever you are storing your data IF YOU ARE FOLLOWING ON KAGGLE YOU CAN ONLY PLAY WITH THE SAMPLE DATA WHICH IS MUCH SMALLER a couple great 1 liners from https www. Now we can begin to iterate through the patients and gather their respective data. Welcome to datascience Okay next question is. Awesome Thanks Ned Okay once we ve got these chunks of these scans what are we going to do Well we can just average them together. If we re going to be successful with a neural network we need more data. Do note that if you do wish to compete you can only use free datasets that are available to anyone who bothers to look. If there s a growth there it should still show up on scan. It might also just simply be the case that a neural network isn t the best model of choice here. Just uncomment it locally and it will run. This initial pass is not going to win the competition but hopefully it can serve as a starting point or at the very least you can learn something new along with me. Guessing it s one of the depths that doesn t come to 20. Let s look at the first 12 and resize them with opencv. 46Epoch 7 completed out of 10 loss 680146186. As per Ned Batchelder via Link http stackoverflow. net convolutional neural network cnn machine learning tutorial 6 https pythonprogramming. If you are completely new to data science I will do my best to link to tutorials and provide information on everything you need to take part. net cnn tensorflow convolutional nerual network machine learning tutorial Now we re ready for the network itself Why 54080 magic number To get this I simply run the script once and see what the error yells at me for the expected size multiple. What we need is to be able to just take any list of images whether it s got 200 scans 150 scans or 300 scans and set it to be some fixed number. As we continue through this however you re hopefully going to see just how many theories we come up with and how many variables we can tweak and change to possibly get better results. I found the torrent to download the fastest so I d suggest you go that route. We can either hunt for more outside datasources or we can engage in adding some noise to the data. To be honest I don t know of any super smooth way of doing this but that s fine. Okay the Python gods are really not happy with me for that hacky solution. Even if it was what was the number to beat Was it 50 since it s either cancer or not Not quite. When you create an account head to competitions in the nav bar choose the Data Science Bowl then head to the data tab. If you can preprocess all of the data into one file and that one file doesn t exceed your available memory then training should likely be faster so you can more easily tweak your neural network and not be processing your data the same way over and over. I d start here https www. All of our images are the same size but the slices arent. Now what Well likely the largest issue here is our size of data. By this I mean while training the network we can actually just loop over our patients resize the data then feed it through our neural network. We re almost certainly going to need to do some preprocessing of this data but we ll see. Your training file should be 700mb with 1400 total labeled samples. Later we could actually put these together to get a full 3D rendering of the scan. We ve got CT scans of about 1500 patients and then we ve got another file that contains the labels for this data. Even if we do a grayscale colormap in the imshow you ll see that some scans are just darker overall than others. For example you can grab data from the LUNA2016 challenge https luna16. Your job is to predict chance of cancer it s not so binary. 50d12f1c627df49eb223771c28548350e 0. org pypi pydicomAt this point we ve got the list of patients by their IDs and their associated labels stored in a dataframe. Really any of this code. com gzuidhof data science bowl 2017 full preprocessing tutorial 1 One immediate thing to note here is those rows and columns. Okay what you re about to see you shouldn t attempt if anyone else is watching like if you re going to show your code to the public. If you see something that you could improve share it with me Quick introduction to Kaggle If you are new to kaggle create an account and start downloading the data. For the actual dependency installs and such I will link to them as we go. Thus an algorithm that always predicted no cancer with our model would be 74 accurate 1035 1397. We know the scans are in this dicom format but what is that If you re like me you have no idea what that is or how it will look in Python You can learn more about DICOM from Wikipedia 1 if you like but our main focus is what this will actually be in Python terms. About this specific competition At its core the aim here is to take the sample data consisting of low dose CT scan information and predict what the liklihood of a patient having lung cancer is. If you re already familiar with neural networks and TensorFlow great If not as you might guess I have a tutorial. Okay so now what I think we need to address the whole non uniformity of depth next. I am going to do my best to make this tutorial one that anyone can follow within the built in Kaggle kernels Requirements and suggestions for following along I will be using Python 3 and you should at least know the basics 1 of Python 3. There are many ways to add noise to data. com gzuidhof data science bowl 2017 full preprocessing tutorial a couple great 1 liners from https www. the list goes on and on. But hey we did it We figured out a way to make sure our 3 dimensional data can be at any resolution we want or need. Installation tutorials Installing the GPU version of TensorFlow in Ubuntu 1 Installing the GPU version of TensorFlow on a Windows machine 2 Using TensorFlow and concept tutorials Introduction to deep learning with neural networks 3 Introduction to TensorFlow 4 Intro to Convolutional Neural Networks 5 Convolutional Neural Network in TensorFlow tutorial 6 Now the data we have is actually 3D data not 2D data that s covered in most convnet tutorials including mine above. Do note here that the actual scan when loaded by dicom is clearly not JUST some sort of array of values instead it s got attributes. That s not in my plans here since that s already been something covered very well see this kernel https www. This isn t quite ideal and will cause a problem later. 1595 in real data 20 if you re in the Kaggle sample dataset Well that s also going to be a challenge for the convnet to figure out but we re going to try Also there are outside datasources for more lung scans. The dataset is pretty large at 140GB just in initial training data so this can be somewhat restrictive right out of the gate. com questions 312443 how do you split a list into evenly sized chunksThe struggle is real. 57Epoch 10 completed out of 10 loss 96427552. Keep hacking away 1 https www. In terms of a 3D rendering these actually are not the same size. Being 512 x 512 I am already expecting all this data to be the same size but let s see what we have from other patients too 1 https www. It s unclear to me whether or not a model would appreciate that. One major issue is these colors and ranges of data. We d definitely want to confirm our testing set actually has this ratio before assuming anything. Our dataset is only 1500 even less if you are following in the Kaggle kernel patients and will be for example 20 slices of 150x150 image data if we went off the numbers we have now but this will need to be even smaller for a typical computer most likely. 1 https pythonprogramming. There are a few attributes here of arrays but not all of them. Your convolutional window padding strides need to change. Consider other models. How can we do this Well first we need something that will take our current list of scans and chunk it into a list of lists of scans. This is certainly not the right way to go about it but that s my 100 honest method and my first time working in a 3D convnet. My theory is that a scan is a few millimeters of actual tissue at most. That s huge Alright so we already know that we re going to absolutely need to resize this data. We have a few options at this point we could take the code that we have already and do the processing online. net matplotlib intro tutorial Now I am not a doctor but I m going to claim a mini victory and say that s our first CT scan slice. ", "id": "sentdex/first-pass-through-data-w-3d-convnet", "size": "22017", "language": "python", "html_url": "https://www.kaggle.com/code/sentdex/first-pass-through-data-w-3d-convnet", "git_url": "https://www.kaggle.com/code/sentdex/first-pass-through-data-w-3d-convnet", "script": "train_neural_network mean numpy tensorflow maxpool3d conv3d matplotlib.pyplot convolutional_neural_network process_data pandas chunks ", "entities": "(('it', 'huge importance'), 'expect') (('we', 'depth'), 'okay') (('which', 'input actual data'), 's') (('we', '20 scans'), 'let') (('Credit Ned Batchelder Link', 'stackoverflow'), '312443') (('net', 'https deep learning tutorial 2 www'), 'tensorflow') (('6Epoch 6', '10 loss'), 'complete') (('stage 2 where you', 'stages'), 'be') (('neural just WAY too convolutional network', 'computing serious power'), 'be') (('6Epoch 3', '10 loss'), 'complete') (('we', 'lung more scans'), '1595') (('57Epoch 10', '10 loss'), 'complete') (('Now we', '3D convnet'), 're') (('i', '0'), 'have') (('63Epoch 2', '10 loss'), 'complete') (('we', 'network'), 'have') (('any', 'averaging chunking code'), 'feel') (('Later we', 'scan'), 'put') (('We', 'anything'), 'want') (('we', 'actually dataset'), 'be') (('now this', 'even typical computer'), 'be') (('there it', 'still scan'), 'show') (('you', 'new me'), 'go') (('so I', 'actually this'), 'go') (('submission', 'predictions'), 'score') (('Credit Ned Batchelder Link', 'stackoverflow'), 'bowl') (('one you', 'validation dataset'), 'have') (('more I', 'a few more'), 'have') (('we', 'it'), 'go') (('instead it', 'attributes'), 'note') (('list', 'associated dataframe'), 'pypi') (('I', 'means'), 'be') (('training file', '700 1400 total labeled samples'), 'be') (('we', 'them'), 'link') (('also also they', 'necessarily proper order'), 'grab') (('how you', 'chunksThe evenly sized struggle'), '312443') (('that', 'test'), 's') (('that', 'chest patient'), 's') (('we', 'file 3D Convolutional Neural Network Moment o Section 4 truth'), 'make') (('it', 'cancer'), 'be') (('62Epoch 9', '10 loss'), 'complete') (('case just numpy', 'https 1 pythonprogramming'), 'numpy') (('it', 'those'), 'make') (('we', 'just them'), 'thank') (('we', 'depth bit somehow uniform'), 'know') (('We', 'uniformity problem'), 'get') (('I so you', 'route'), 'find') (('actual slice', 'what'), 'let') (('already we', 'absolutely data'), 's') (('it', 'case'), 'can') (('you', 'OpenCV tutorial'), 'do') (('that', 'model'), 'be') (('that', 'scans'), 'do') (('that', 'data'), 'get') (('com gzuidhof data science full preprocessing', '1 One immediate thing'), 'bowl') (('lung cancer', 'patient'), 'be') (('these', '3D rendering'), 'be') (('this', 'Python actually terms'), 'know') (('you', 'prizes'), 'win') (('Python gods', 'hacky solution'), 'be') (('we', 'already processing'), 'have') (('we', 'things'), 'do') (('you', 'challenge LUNA2016 https'), 'grab') (('Now we', 'respective data'), 'begin') (('which', 'easily CPU'), 's') (('It', 'network isn also just simply neural best choice'), 'be') (('net data analysis', 'python tutorial https pandas 3 pythonprogramming'), 'introduction') (('So actually dataset', 'non cancer 1035 examples'), 'let') (('data', 'neural network'), 'loop') (('we', 'https too 1 www'), 'expect') (('network', 'always single class'), 'be') (('model', 'that'), 's') (('more later we', 'just how it'), 's') (('you', 'data'), 'share') (('you', 'here what'), 'be') (('pip', 'Python tutorial'), 'install') (('already something', 'kernel https very well www'), 's') (('network neural code', 'ahead data'), 'prefer') (('else you', 'public'), 'about') (('I', 'improvment'), 'be') (('you', 'part'), 'do') (('same slices', 'images'), 'be') (('we', 'resolution'), 'do') (('that', 'mine'), 'tutorial') (('s', 'opencv'), 'let') (('we', 'more data'), 'need') (('you', 'dependencies'), 'install') (('how you', '1 real'), '312443') (('I', 'tutorial'), 're') (('You', 'data'), 'need') (('you', 'time'), 'submit') (('don really t', 'more cancerous examples'), 'be') (('54Epoch 8', '10 loss'), 'complete') (('This', 'them'), 'have') (('Now likely largest issue', 'here data'), 'be') (('so I', 'lists'), 'think') (('that', 'second stage'), 'release') (('2017 27922 1 how actual doctor', 'CT scan'), 'c') (('46Epoch 7', '10 loss'), 'complete') (('we', 'at least data'), 'have') (('We', 'just simply enough data'), 'have') (('locally it', 'Just it'), 'run') (('whatever', 'when 3 dimensional rendering'), 'tutorial') (('error', 'size expected multiple'), 'tutorial') (('we', 'what'), 'install') (('3 you', '1 Python'), 'go') (('ourselves', 'evenly sized chunks'), '312443') (('anyway you', 'still one big database'), 'be') (('exactly doctors', 'cancerous tumors'), 'be') (('loading images python opencv Data net tutorial Section 2 so we', '150x150'), 'Processing') (('significantly we', '2D obviously much windows'), 'note') (('it', '200 150 300 it'), 'be') (('that', 'this'), 'know') (('likely you', 'data'), 'be') (('who', 'anyone'), 'note') (('6Epoch 5', '10 loss'), 'complete') (('together maybe we', 'now centimeter'), 'average') (('One major issue', 'data'), 'be') (('we', 'data'), 'hunt') (('WHICH', 'https MUCH couple great 1 www'), 'change') (('just it', 'idea'), 'about') (('again mainly anyone', 'kernel environment'), 'have') (('you', 'about 5 5 5 patches 32 64 features'), 'size') (('I', '1 10 loss'), 'complete') (('you', 'output how exactly predictions'), 's') (('cancer', 'always 72 accuracy'), 'give') (('notebook', 'way'), 'be') (('how one', 'actually number'), 'feel') (('we', 'at least way'), 'think') (('scan', 'few actual tissue'), 'be') (('only when we', 'it'), 'be') (('that', '100 honest first 3D convnet'), 'be') (('you', 'https tutorial www'), 'check') (('we', 'input one tensor'), 'have') (('doesn t', '20'), 'guess') (('we', 'possibly better results'), 'go') (('best path', 'really what'), 'be') (('you', '2 models'), 'submit') (('this', 'somewhat right gate'), 'be') (('we', 'data'), 'go') (('that', 'mini victory'), 'tutorial') (('7Epoch 4', '10 loss'), 'complete') (('have', 'really big slices'), 'be') (('you', 'model'), 'be') (('It', 'IT'), 's') (('scans', 'just darker others'), 'see') (('Now I', 'one'), 'see') (('we', 'classifier'), 'be') (('you', 'input data'), 'be') (('We', 'scan'), 'sort') (('conflicts', 'noise'), 'wouldn') (('even people', 'new kaggle'), 'be') (('soon immediately I', 'slices'), 'bowl') (('features X Y you', 'sample data basic use'), 'image') (('we', 'heck'), 'be') (('Ned Batchelder', 'stackoverflow'), 'http') (('Data Science Bowl', 'data then tab'), 'choose') (('first interest', 'Convolutional Neural 3D Network'), 'have') "}