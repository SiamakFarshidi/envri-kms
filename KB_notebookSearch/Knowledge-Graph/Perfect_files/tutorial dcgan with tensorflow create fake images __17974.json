{"name": "tutorial dcgan with tensorflow create fake images ", "full_name": " h3 Special Note Due to Out of memory limits in Kaggle I could NOT run this Notebook inside Kaggle with any larger dataset than 1000 images Hence the resultant fake images generated at the end will almost be like noise But this Notebook is mostly written as a Tutorial to show the flow of DCGAN h2 Link to my Youtube Video Explaining this whole Notebook h1 Facial Attribute prediction h3 Getting to know CELEB A Dataset h4 Data Files h2 The Facial Attributes of CELEB A Dataset h3 A quick tip in this regard if you are using local machine to handle this dataset challenge h1 Fundamental way GAN Works h3 The basic objective function of a vanilla GAN model is the following h1 Regular Neural Network vs CNN h4 Convolutional Neural Networks have three important architectural features h1 Deep Convolutional GANs DCGANs h2 Generator Architecture h2 The architecture of a discriminator h3 Several tricks had to be applied to make DCGANs stable and useful h1 How Reverse ConvNets works in DCGAN h3 Scaling images h1 Transposed Convolutions h1 UpSampling2D vs Conv2DTranspose in Keras h1 Generator h4 I use Leaky relu activations in the hidden layer neurons and sigmoids for the output layers Originally ReLU was recommend for use in the generator model and LeakyReLU was recommended for use in the discriminator model although more recently the LeakyReLU is recommended in both models h2 Use of Batch Normalization h3 For understanding parameters look at this blog h3 Lets see how our generator is creating images from noise Lets generate a sample image with the below code h1 Discriminator h1 DCGAN Combining Generator and Discriminator h2 Setting discriminator trainable to False h1 Final DCGAN Training h3 Final Training Architecture of a DCGAN h2 DCGAN is super sensitive h3 Since we are training two models at once the discriminator and the generator we can t rely on Keras fit function Instead we have to manually loop through each epoch and fit the models on batches ", "stargazers_count": 0, "forks_count": 0, "description": "The architecture of a discriminator Imgur https imgur. img_list glob. It is added to the model after the hidden layer but before the activation such as LeakyReLU. There are 5 landmarks left eye right eye nose left mouth right mouth list_attr_celeba. The generator consists of convolutional transpose layers batch normalization layers and ReLU activations. png The 2 2 kernel produces a 2 2 output when convolving over a 3 3 image. Hence the resultant fake images generated at the end will almost be like noise. Convolutional Neural Networks have three important architectural features. csv Image landmarks and their respective coordinates. Discriminator takes true of fake images and output the probability estimate ranging between 0 and 1. Link to my Youtube Video Explaining this whole Notebook https youtu. Special Note Due to Out of memory limits in Kaggle I could NOT run this Notebook inside Kaggle with any larger dataset than 1000 images. As training progresses the generated images will look increasingly real just like original celeb a images. crop function argument box which is the crop rectangle as a left upper right lower tuple. Convolutions are performed by sliding one or more filters the smaller Matrix which is the Kernel of the Current Layer over the input layer. min 255 training_images i. All kernels inconvolution layers are of a size of 4 x 4. Generator Architecture Imgur https imgur. csv Attribute labels for each image. input img_align_celeba img_align_celeba. So the training loop begins with generator receiving a random seed as input. But this Notebook is mostly written as a Tutorial to show the flow of DCGAN. com understand transposed convolutions and build your own transposed convolution layer from scratch 4f5d97b2967 Lets see how our generator is creating images from noise. The ReLU function takes one input x and returns the maximum of 0 x. Why would we want to do this Well we aren t going to be training the generator model directly we are going to be combining the generator and discriminator into a single model then training that. The noise variable is the code equivalent of the variable z which we discussed earlier. The standard process to train a DCGAN network is to first train the discriminator on the batch of samples. Becausewhen you are training neural networks on a very large dataset whichcannot fit in the GPU memory the reading speed from physical drivescould be the bottleneck of your training performance. min training_images axis 0 print np. To generate fake samples create a latent vector of a shape of 100 over a uniform distribution. Final DCGAN TrainingTraining GANs is an art form itself as incorrect hyperparameter settings lead to mode collapse. When we train this network we don t want to train the discriminator network so make it non trainable before we add it to the adversarial model. Imgur https imgur. trainable to False will only affect the copy of the discriminator in the combined model. ReLU x argmax x 0. Similarly we got to specify strides output_padding UpSampling2D vs Conv2DTranspose in KerasUpSampling2D is just a simple scaling up of the image by using nearest neighbour or bilinear upsampling so nothing smart. At the beginning of the training the generated images will look like random noise. Several tricks had to be applied to make DCGANs stable and useful Batch normalization was applied to both the generator and the discriminator networkDropout is used as a regularization techniqueThe generator needed a way to upsample the random input vector to an output image. This introduces a similar effect to tanh x or sigmoid x as non linearities to increase the model s expressive power. jpg file The purpose to crete this array is to be able to select only limited number of training_images to train from this array When running in Kaggle full_image_train_path. To combine the generator and discriminator we will be calling the discriminator on the output of the generator. Final Training Architecture of a DCGAN1. This decreases anyunwanted interdependence between parameters across layers which helps speed up the network training process andincrease its robustness especially when it comes to networkparameter initialization. glob full_image_train_path image_path_50k img_list 0 500 print len image_path_50k For the Image. On the contrary the discriminator tries to become better at distinguishing samples coming from G from samples coming from the real distribution. Array to collect loss for the discriminator model Array to collect loss for generator model Loop over each batch in the dataset Benefits of Double Division Operator over Single Division Operator in Python The Double Division operator in Python returns the floor value for both integer and floating point arguments after division. Deep learning based facial attribute analysis consists of two basic sub issues facial attribute estimation FAE which recognizes whether facial attributes are present in given images andfacial attribute manipulation FAM which synthesizes or removes desired facial attributes. png The discriminator network of a DCGAN consists of 4 hidden layers again we treat theinput layer as the 1st hidden layer and 1 output layer. csv Recommended partitioning of images into training validation testing sets. There are 40 attributes. Sometimes the generator only generates a handful of sample varieties. The assumption made here is that the input data has spatial significance or in the example of computer vision the relationship between two distant pixels is probably less significant than two close neighbors. Local Connectivity Neurons in one layer are only connected to neurons in the next layer that are spatially close to them. Facial attribute analysis has received considerable attention when deep learning techniques maderemarkable breakthroughs in this field over the past fewyears. The output layer is also a transposed convolution layer and Tanh isused as the activation function. As image sizesincrease in deeper layers the numbers of channels are increasing by twice. Remember that the transpose layer simply swaps the backwards and forward pass keeping the rest of the operations the same As the transposed convolution will also slide over the input we must specify a kernel_size as with the normal convolution. By forcing the neurons of one layer to share weights the forward pass feeding data through the network becomes the equivalent of convolving a filter over the image to produce a new image. This is so that the discriminator will always receive images as input real and fake that have pixel values in the same range. png The Dataset is considerably large to manage with even Google Colab s Pro version. Typically image data is loaded as a NumPy array such that pixel values are 8 bit unsigned integer uint8 values in the range 0 255. Deep Convolutional GANs DCGANs Deep Convolutional GANs DCGANs introduced convolutions to the generator and discriminator networks. So play with different hyperparameters to obtain better results. To load an image from a file we use the open function in the Image module passing it the path to the image. The 1st layerhas a padding value of 0 and the other layers have a padding value of 1. All hidden layers have a stride value of 2 and a paddingvalue of 1 so that their output image sizes will be half the input images. input img_align_celeba img_align_celeba When running in Local Machine full_image_train_path content gdrive MyDrive celeba dataset img_align_celeba img_align_celeba When running in Colab with data in G Drive full_image_train_path content img_align_celeba img_align_celeba For reading from Colab sesstion storage as g drive is failing for large 200k files of CelebA Now from this array print image_path_50k Model Constants Alternative way to build the numpy array for all the training images For getting the list of training_images I COULD REPLACE THE ABOVE os. Each filter has a relatively small receptive field width height but always extends through theentire depth of the input volume. That seed is used to produce an image. Advantage is it s cheap. png The basic objective function of a vanilla GAN model is the following Imgur https imgur. Batch normalization limits the amount by which updating theparameters in the previous layers can affect the distribution ofinputs received by the current layer. We do this by first sampling some random noise from a random uniform distribution then getting the generator s predictions on the noise. Fundamental way GAN WorksGAN contains two networks which has two competing objectives Generator the generator generates new data instances that are similar to the training data in our case celebA images. The 2nd 3rd and 4th hidden layers and the output layer have a stride value of 2. This is a common convention in the architecture design of neuralnetworks. To do this we need fake samples as well as real samples. Lets generate a sample image with the below code. width and height represent the width and height of bounding box list_landmarks_align_celeba. LeakyReLU activation functions are used in the hidden layersand Sigmoid is used for the output layer. After about 300 epochs they resemble almost the original. Since we are training two models at once the discriminator and the generator we can t rely on Keras. For understanding parameters look at this bloghttps towardsdatascience. Generator takes random latent vector and output a fake image of the same size as our reshaped celebA image. This is called mode collapse. The GAN training process consists of a two player minimax game in which D is adapted to minimize the discrimination error between real and generated samples and G is adapted to maximize the probability of D making a mistake. First create a random noise print generated_image It is just plain noise. Regular Neural Network vs CNNUnlike a regular feed forward neural network whose neuronsare arranged in flat fully connected layers layers in aConvNet are arranged in three dimensions width height depth. This is good If the copy of the discriminator in the combined model were trainable it would update itself to be worse at classifying images. For example either a generator or a discriminator could overfit if one or the otherdoes not converge. As the formula shows the generator optimizes for maximally confusing the discriminator by trying to make it output high probabilities for fake data samples. Convolutional Neural Networks CNNs are neural networks with architectural constraints to reduce computational complexity and ensure translational invariance the network interprets input patterns the same regardless of translation in terms of image recognition a banana is a banana regardless of where it is in the image. When G does a good enough job to fool D the output probability should be close to 1. But for the generator convolutions are replaced with up convolutions so the representation at each layer of the generator is actually successively larger as it maps from a low dimensional latent vector onto a high dimensional image. The Conv2DTranspose layer learns a number of filters similar to the regular Conv2D layer. png Here D refers to the discriminator network while G obviously refers to the generator. The output layer has a stride value of 1 and apadding value of 0. The architecture of the discriminator is shown inthe following The input channel can be either 1 or 3 depending on whether you are dealing withgrayscale images or color images. Transposed Convolutions Imgur https imgur. Using Conv2DTranspose will also upsample its input but the key difference is the model should learn what is the best upsampling for the job. Feedthis latent vector to the untrained generator network. zip All the face images cropped and aligned list_eval_partition. Convolution layers are used in alllayers which are followed by batch normalization layers except that the first layer does nothave batch normalization. Shared Weights This is the concept that makes CNNs convolutional. So overall the core to the DCGAN architecture uses a standard CNN architecture on the discriminative model. Doing this reduces the size of the output and requires no added parameters to learn so pooling layers are often used to regulate the size of the network and keep the system below a computational limit. The generator network will generate fake samplesthat we use to train our discriminator network. Instead we have to manually loop through each epoch and fit the models on batches. In DCGAN under the Generator part to generate an image by using the ConvNet architecture we reverse the process instead of taking an image and processing it into a vector we take avector and up size it to an image. The outputchannel can be either 1 or 3 depending on whether you want to generate grayscale imagesor color images. But I want to work in the opposite direction i. We already have the real samples so we nowneed to generate the fake samples. The discriminator is then used to classify real images drawn from the training set and fakes images produced by the generator. The output will be a 3x64x64 RGB image. 1 represents positive while 1 represents negative The Facial Attributes of CELEB A Dataset Imgur https imgur. Initially both of the networks are naive and have random weights. Concatenate the real images and the fake images to create a new set of sample images. Use of Batch NormalizationBatch normalization standardizes the activations from a prior layer to have a zero mean and unit variance. Sometimes thespeed up of SSD reading samples at 50 MB s over the traditional harddrive 5 MB s can save you a big chunk of training time. A lot of things could go wrong in the training ofGANs. crop 30 55 150 175 croping size for the image so that only the face at centre is obtainedtraining_images np. However this was not simply a matter of adding convolutional layers to the model since training became even more unstable. Discriminator the discriminator evaluate the authenticity of provided images it classifies the images from the generator and the original image. The training of CNNs then becomes the task of learning filters deciding what features you should look for in the data. Now I am taking real x_train data by sampling a batch of real images from the set of all image Create Labels First training on real image Next train the discriminator network on real images and real labels Now training on fake image Creating variables to make ready the whole adversarial network Image Label vector that has all the values equal to 1 To fool the Discriminator Network. x_1 and y_1 represent the upper left point coordinate of bounding box. Setting discriminator. A quick tip in this regard if you are using local machine to handle this dataset challenge. Originally ReLU was recommend for use in the generator model and LeakyReLU was recommended for use in the discriminator model although more recently the LeakyReLU is recommended in both models. Generator I use Leaky relu activations in the hidden layer neurons and sigmoids for the output layers. print training_images training_images is a 4 D array print training_images 500 64 64 3 print np. If you have a Solid State Drive SSD with enough space plugged in yourmachine I highly recommend you move all of your training samples tothe SSD especially when you have a powerful graphics card. listdir with glob like below full_image_train_path. Batch normalization is used after the activation of convolution in the discriminator modeland after transpose convolutional layers in generator model. We also need tocreate an array of labels label 1 for real images and label 0 for fake images. This allows the generator to understand the discriminator so it can update itself more effectively. Start by sampling a batch of noise vectors from a uniform distribution generator receives a random seed as input which is used to produce an image. csv Bounding box information for each image. that they train at a similar rate. Getting to know CELEB A Dataset202 599 number of face images of various celebrities10 177 unique identities but names of identities are not given40 binary attribute annotations per image5 landmark locations Data Files img_align_celeba. As theimage feature map sizes increase by two in deeper layers the numbers of channels aredecreasing by half. be csQj1e6Oj38 Imgur https imgur. I got out of memory quite a few times. be csQj1e6Oj38 Facial Attribute predictionFacial Attribute prediction is a Computer Vision CV task about deducing the set of attributes belonging to a face. This has the effect of stabilizing the training process. min training_images i training_images i 2 1 uncomment this if activation is tanh for generator last layer training_images np. The right can also be represented as left width and lower can be represented as upper height. It maps 4 x 4 feature maps to single values so that the Sigmoid functioncan transform the value into prediction confidence. DCGAN is super sensitiveHere even when we only train a GAN to manipulate 1D data we have to use multipletechniques to ensure a stable training. png Thats where the Transposed convolutions in the Keras API comes to help. max training_images axis 0 Generator will upsample our seed using convolutional transpose layers upsampling layers Random noise to 4x4x512 image Next add a reshape layer to the network to reshape the tensor from the last layer to a tensor of a shape of 4 4 512 BatchNormalization is added to the model after the hidden layer but before the activation such as LeakyReLU. Images 1 162770 are training 162771 182637 are validation 182638 202599 are testing list_bbox_celeba. Example attributes are the color of hair hairstyle age gender etc. But the fact that it can create an image from a random noise array proves the model s power. The crop method from the Image module takes four coordinates as input. Pooling and ReLU CNNs have two non linearities pooling layers and ReLU functions. The loss is calculated for each of these modelsIn the process defined below I am training the generator and discriminator simultaneously. Pooling layers consider a block of input data and simply pass on the maximum value. png The generator network of a DCGAN contains 4 hidden layers we treat the input layer asthe 1st hidden layer for simplicity and 1 output layer. resize 64 64 for path in image_path_50k for i in range len training_images training_images i training_images i training_images i. DiscriminatorHere I am creating a Sequential model incrementally via the add method DCGAN Combining Generator and DiscriminatorThe combined model is stacked generator and discriminator Setting discriminator. Conv2DTranspose is a convolution operation whose kernel is learnt just like normal conv2d operation while training your model. linear algebra First I am creating an array containing all the full path names for each of the. Transposed convolution layers areused in hidden layers which are followed by batch normalization layers and ReLUactivation functions. Could not work at all with the full set of 200k images. All kernel sizes of transposed convolution layers are set to 4 x 4. This design trims the vast majority of connections between consecutive layers but keeps the ones that carry the most useful information. At every step as it slides across the input each filter outputs a single activation value the dot product between the input values and the filter entries. Transposing convolutional layers is employed hereLeakyRelu and TanH activations are used throughout both networks How Reverse ConvNets works in DCGANConvNets have traditionally been used for image classificationtasks in which the network takes in an image with thedimensions height width number of color channels asinput and through a series of convolutional layers outputs asingle vector of class scores with the dimensions 1 n wheren is the number of class labels. array training_images Scaling images it is also recommended that real images used to train the discriminator are scaled so that their pixel values are in the range 1 1. to use a smaller input and to learn its larger representation being the following Imgur https imgur. It s important that the generator and discriminator do not overpower each other e. ", "id": "paulrohan2020/tutorial-dcgan-with-tensorflow-create-fake-images", "size": "17974", "language": "python", "html_url": "https://www.kaggle.com/code/paulrohan2020/tutorial-dcgan-with-tensorflow-create-fake-images", "git_url": "https://www.kaggle.com/code/paulrohan2020/tutorial-dcgan-with-tensorflow-create-fake-images", "script": "Reshape tensorflow.keras.layers plot_model numpy tensorflow.keras.models Image discriminator_model Dropout preprocessing BatchNormalization Dense LeakyReLU tensorflow tensorflow.keras.utils matplotlib.pyplot Sequential PIL pandas Conv2D generator_model Conv2DTranspose tensorflow.keras Flatten ", "entities": "(('We', 'noise'), 'do') (('neuronsare', 'dimensions height three width depth'), 'arrange') (('Conv2DTranspose layer', 'Conv2D similar regular layer'), 'learn') (('Similarly we', 'nearest neighbour'), 'get') (('face images', 'list_eval_partition'), 'zip') (('crop method', 'input'), 'take') (('that', 'most useful information'), 'trim') (('that', 'Discriminator Network'), 'take') (('Notebook', 'DCGAN'), 'write') (('numbers', 'channels'), 'increase') (('Sigmoid', 'output layer'), 'use') (('you', 'dataset challenge'), 'tip') (('lot', 'training wrong ofGANs'), 'go') (('best upsampling', 'job'), 'upsample') (('I', 'i.'), 'want') (('basic objective function', 'vanilla GAN model'), 'png') (('theimage feature map', 'half'), 'size') (('amount', 'current layer'), 'limit') (('it', 'itself'), 'allow') (('trainable', 'combined model'), 'affect') (('convolution also transposed Tanh', 'activation function'), 'be') (('generated images', 'random noise'), 'look') (('Use', 'unit zero mean variance'), 'standardize') (('directly we', 'then that'), 'want') (('especially when you', 'graphics powerful card'), 'have') (('generator', 'normalization layers'), 'consist') (('filter', 'input volume'), 'have') (('crop', 'left upper right lower tuple'), 'box') (('Next add', 'such LeakyReLU'), 'axis') (('filter', 'input values'), 'output') (('generate', 'uniform distribution'), 'create') (('parameters', 'bloghttps towardsdatascience'), 'look') (('we', 'discriminator network'), 'generate') (('it', 'data fake samples'), 'show') (('which', 'image'), 'receive') (('Images', '1 162770 162771 182637 182638 202599 list_bbox_celeba'), 'be') (('discriminator', 'generator'), 'use') (('I', 'generator'), 'calculate') (('generator', 'input'), 'begin') (('ReLU function', '0 x.'), 'take') (('D', 'mistake'), 'consist') (('network', '1 class n labels'), 'employ') (('width', 'box list_landmarks_align_celeba'), 'represent') (('Convolutional GANs DCGANs Deep Convolutional GANs Deep DCGANs', 'generator networks'), 'introduce') (('G', 'obviously generator'), 'refer') (('it', 'generator'), 'evaluate') (('2nd 3rd hidden layers', '2'), 'have') (('that', 'case celebA images'), 'contain') (('output image sizes', '1'), 'have') (('we', 'generator'), 'call') (('Pooling layers', 'simply maximum value'), 'consider') (('ABOVE', 'training_images'), 'input') (('2 png 2 kernel', '3 when over a 3 image'), 'produce') (('pixel such values', '8 bit unsigned integer uint8 range'), 'load') (('So overall core', 'discriminative model'), 'use') (('Convolutional Neural Networks', 'three important architectural features'), 'have') (('eye right nose', 'mouth'), 'be') (('i', 'i.'), 'resize') (('linear First I', 'the'), 'algebra') (('first layer', 'nothave batch normalization'), 'use') (('where Transposed convolutions', 'Keras API'), 'png') (('last layer', 'np'), 'training_images') (('techniqueThe generator', 'output image'), 'have') (('one', 'example'), 'overfit') (('kernel sizes', '4 4'), 'set') (('Example attributes', 'hair hairstyle age gender etc'), 'be') (('training', 'model'), 'be') (('relationship', 'probably less two close neighbors'), 'be') (('Generator I', 'output layers'), 'use') (('kernels inconvolution layers', '4 4'), 'be') (('we', 'fake samples'), 'have') (('Lets', 'below code'), 'generate') (('Batch normalization', 'generator model'), 'use') (('real that', 'same range'), 'be') (('Sometimes generator', 'sample varieties'), 'generate') (('we', 'normal convolution'), 'remember') (('it', 'classifying images'), 'be') (('pooling layers', 'computational limit'), 'reduce') (('feeding forward pass data', 'new image'), 'become') (('I', '1000 images'), 'note') (('Double Division operator', 'point floating division'), 'Array') (('especially when it', 'networkparameter initialization'), 'decrease') (('pixel values', 'range'), 'training_images') (('We', '0 fake images'), 'need') (('synthesizes', 'facial attributes'), 'consist') (('that', 'spatially them'), 'connect') (('we', 'stable training'), 'be') (('standard process', 'samples'), 'be') (('we', 'adversarial model'), 'want') (('we', 't Keras'), 'rely') (('Pooling CNNs', 'layers functions'), 'have') (('This', 'neuralnetworks'), 'be') (('more recently LeakyReLU', 'models'), 'recommend') (('discriminator', 'real distribution'), 'try') (('only face', 'centre'), 'crop') (('how generator', 'noise'), 'transpose') (('you', 'withgrayscale images'), 'show') (('Facial predictionFacial Attribute csQj1e6Oj38 Attribute prediction', 'face'), 'be') (('This', 'expressive power'), 'introduce') (('they', 'almost original'), 'resemble') (('Dataset', 'Google Pro considerably even version'), 'png') (('actually successively it', 'high dimensional image'), 'replace') (('print training_images training_images', 'print 64 64 3 np'), 'be') (('smaller which', 'input layer'), 'perform') (('art hyperparameter incorrect settings', 'mode collapse'), 'be') (('you', 'data'), 'become') (('Sigmoid functioncan', 'prediction confidence'), 'map') (('regardless where it', 'image'), 'be') (('we', 'simplicity'), 'contain') (('we', 'fake samples'), 'need') (('1', 'CELEB A Dataset Imgur https imgur'), 'represent') (('This', 'training process'), 'have') (('DiscriminatorHere I', 'discriminator'), 'create') (('it', 'power'), 'prove') (('again we', '1st hidden layer'), 'png') (('purpose', 'Kaggle When full_image_train_path'), 'file') (('which', 'batch normalization layers'), 'areuse') (('reading speed', 'training performance'), 'be') (('when deep learning', 'past fewyears'), 'receive') (('generator', 'other e.'), 's') (('we', 'which'), 'be') (('Discriminator', '0'), 'take') (('generated images', 'increasingly just original celeb'), 'look') (('x_1', 'box'), 'represent') (('we', 'image'), 'load') (('convolution kernel', 'model'), 'be') (('It', 'noise print First random generated_image'), 'create') (('we', 'image'), 'reverse') (('It', 'such LeakyReLU'), 'add') (('you', 'grayscale imagesor color images'), 'be') (('output probability', '1'), 'do') (('Initially both', 'random weights'), 'be') (('other layers', '1'), 'layerha') (('right', 'upper height'), 'represent') (('Generator', 'celebA reshaped image'), 'take') (('output layer', '0'), 'have') (('Hence resultant fake images', 'almost noise'), 'be') (('CELEB Dataset202 599 number', 'given40 attribute image5 landmark locations Data binary Files'), 'be') (('Instead we', 'batches'), 'have') "}