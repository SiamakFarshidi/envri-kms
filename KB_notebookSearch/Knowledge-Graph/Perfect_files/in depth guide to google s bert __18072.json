{"name": "in depth guide to google s bert ", "full_name": " h1 Understanding BERT for Disaster NLP h1 Introduction h3 Welcome to this kernel In this kernel we are going to explore Google s BERT I will try to cover all the topics which helps to understand BERT clearly Finally we will see how to implement BERT for disaster NLP h3 Without wasting time lets get started h2 Table of Contents h3 References and credits h1 1 Intution behind RNN based Sequence to Sequence Model h1 1 1 Limitations of RNN S h1 2 Introduction to the Transformer h2 2 1 Transformer s Model Architecture h2 2 2 Understanding self attention h2 2 3 Limitations of the Transformer h1 3 Understanding BERT Bidirectional Encoder Representations from Transformers h2 3 1 How Does BERT Work h3 3 1 1 BERT s Architecture h2 3 2 Text processing for BERT h3 1 Masked LM MLM h3 2 Next Sentence Prediction NSP h2 3 3 How to use BERT Fine tuning h2 3 4 Takeaways h2 3 5 Compute considerations training and applying h1 4 Implementation of BERT using TFHub h4 Credits This part was taken from this kernel please refer to this awesome kernel and consider upvoting Thanks to the author of the kernel xhlulu h2 4 1 Importing necessary modules h3 Getting tokenizer h2 4 2 Helper Functions h2 4 3 Loading BERT from the Tensorflow Hub h2 4 4 Loading data h2 4 5 Loading tokenizer from the bert layer h2 4 6 Encoding the text into tokens masks and segment flags h2 4 7 Model Build Train Predict Submit h1 Please consider Upvoting if you like this kernel h1 Thank you for reading this kernel h1 Happy learning sharing ", "stargazers_count": 0, "forks_count": 0, "description": "2 Understanding self attention 2. Such a comprehensive embedding scheme contains a lot of useful information for the model. 2 Helper Functions 4. BERT_large with 345 million parameters is the largest model of its kind. Finally we will see how to implement BERT for disaster NLP. Can you figure out what the term it in this sentence refers to Is it referring to the street or to the animal It s a simple question for us but not for an algorithm. In the above example all the tokens marked as EA belong to sentence A and similarly for EB Token Embeddings These are the embeddings learned for the specific token from the WordPiece token vocabularyFor a given token its input representation is constructed by summing the corresponding token segment and position embeddings. It is demonstrably superior on small scale tasks to BERT_base which uses the same architecture with only 110 million parameters. The decoder on the other hand has an extra Masked Multi Head Attention. Using BERT a Q A model can be trained by learning two extra vectors that mark the beginning and the end of the answer. We currently have two variants available BERT Base 12 layers transformer blocks 12 attention heads and 110 million parameters BERT Large 24 layers transformer blocks 16 attention heads and 340 million parameters https s3 ap south 1. Now focus on the below image. png The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non masked words. Vanishing Gradient Problem Issue of increasing gradients at each step called as exploding gradients. Introduction to the Transformer The Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequence aligned RNNs or convolution. I will try to cover all the topics which helps to understand BERT clearly. 1 Importing necessary modules Getting tokenizer 4. com blog 2019 09 demystifying bert groundbreaking nlp framework 4. 2 Text processing for BERT 3. This helps the decoder focus on the appropriate parts of the input sequence. Thanks to the author of the kernel xhlulu https www. How do Transformers Work in NLP A Guide to the Latest State of the Art Models https www. 1 Importing necessary modules 4. 3 Loading BERT from the Tensorflow Hub 4. 3 How to use BERT Fine tuning Using BERT for a specific task is relatively straightforward BERT can be used for a wide variety of language tasks while only adding a small layer to the core model 1. Both the encoder stack and the decoder stack have the same number of units. To help the model distinguish between the two sentences in training the input is processed in the following way before entering the model 1. BERT is a deeply bidirectional model. Introduction to the Transformers 2. The encoder and decoder blocks are actually multiple identical encoders and decoders stacked on top of each other. The concept and implementation of positional embedding are presented in the Transformer paper. png Let s see how this setup of the encoder and the decoder stack works The word embeddings of the input sequence are passed to the first encoder These are then transformed and propagated to the next encoder The output from the last encoder in the encoder stack is passed to all the decoders in the decoder stack as shown in the figure below An important thing to note here in addition to the self attention and feed forward layers the decoders also have one more layer of Encoder Decoder Attention layer. 1 Limitations of RNN S Dealing with long range dependencies is still challenging The sequential nature of the model architecture prevents parallelization. com av blog media wp content uploads 2019 06 Screenshot from 2019 06 17 22 31 11. Implementation of BERT using TFHub Credits This part was taken from this kernel https www. 1 Limitations of RNN S 2. 3 How to use BERT Fine tuning 3. com av blog media wp content uploads 2019 06 Screenshot from 2019 06 17 19 53 10. 1 BERT s Architecture The BERT architecture builds on top of Transformer. For example if a sentence is split from the middle then a significant amount of context is lost. Understanding BERT 3. Let s see an example to illustrate this. Demystifying BERT A Comprehensive Guide to the Groundbreaking NLP Framework https www. com xhlulu disaster nlp keras bert using tfhub please refer to this awesome kernel and consider upvoting. This masking combined with fact that the output embeddings are offset by one position ensures that thepredictions for position i can depend only on the known outputs at positions less than i. The outputs are concatenated and linearly transformed as shown in the figure below https s3 ap south 1. Attention Is All You Need https arxiv. Similar to the encoder we employ residual connections around each of the sub layers followed by layer normalization. com av blog media wp content uploads 2019 09 bert_encoder. 2 Understanding self attention Self attention sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. Multiplying the output vectors by the embedding matrix transforming them into the vocabulary dimension. com av blog media wp content uploads 2019 06 seq2seq. com max 875 1 LgbpLsRUGbtTPmMSUO2Drw. 7 Model Build Train Predict Submit References and credits 1. As a result the pre trained BERT model can be finetuned with just one additional output layer to create state of the art models for a wide range of tasks such as question answering and language inference without substantial taskspecific architecture modifications. One way to deal with this is to consider both the left and the right context before making a prediction. When training the BERT model Masked LM and Next Sentence Prediction are trained together with the goal of minimizing the combined loss function of the two strategies. Now that we know the overall architecture of BERT let s see what kind of text processing steps are required before we get to the model building phase. BERT s bidirectional approach MLM converges slower than left to right approaches because only 15 of words are predicted in each batch but bidirectional training still outperforms left to right training after a small number of pre training steps. 3 Limitations of the Transformer3. com blog 2019 06 understanding transformers nlp state of the art models 3. Calculating the probability of each word in the vocabulary with softmax. The assumption is that the random sentence will be disconnected from the first sentence. read_csv Input data files are available in the. 3 Limitations of the Transformer Attention can only deal with fixed length text strings. Self attention is computed not once but multiple times in the Transformer s architecture in parallel and independently. 5 Loading tokenizer from the bert layer 4. That s exactly what BERT does. png To predict if the second sentence is indeed connected to the first the following steps are performed 1. https s3 ap south 1. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2. png For bert every input embedding is a combination of 3 embeddings Position Embeddings BERT learns and uses positional embeddings to express the position of words in a sentence. com av blog media wp content uploads 2019 06 Screenshot from 2019 06 17 20 03 14. com av blog media wp content uploads 2019 06 Screenshot from 2019 06 17 20 01 32. Masked LM MLM Before feeding word sequences into BERT 15 of the words in each sequence are replaced with a MASK token. These are added to overcome the limitation of Transformer which unlike an RNN is not able to capture sequence or order information Segment Embeddings BERT can also take sentence pairs as inputs for tasks Question Answering. 1 Transformer s Model Architecture https s3 ap south 1. Let s take a simple example of a sequence to sequence model. com av blog media wp content uploads 2019 09 sent_context. Without wasting time lets get started. When training language models there is a challenge of defining a prediction goal. To overcome this challenge BERT uses two training strategies 1. In Named Entity Recognition NER the software receives a text sequence and is required to mark the various types of entities Person Organization Date etc that appear in the text. 7 Model Build Train Predict Submit Please consider Upvoting if you like this kernel. Thank you for reading this kernel. We employ a residual connection around each ofthe two sub layers followed by layer normalization. Intution behind RNN based Sequence to Sequence Model 1. Adding a classification layer on top of the encoder output. 0 when trained on 1M steps 128 000 words batch size compared to 500K steps with the same batch size. It is therefore referred to as Multi head Attention. Calculating the probability of IsNextSequence with softmax. Table of Contents 1. As a consequence the model converges slower than directional models a characteristic which is offset by its increased context awareness. Many models predict the next word in a sequence a directional approach which inherently limits context learning. Decoder The decoder is also composed of a stack of N 6 identical layers. A CLS token is inserted at the beginning of the first sentence and a SEP token is inserted at the end of each sentence. In addition to the two sub layers in each encoder layer the decoder inserts a third sub layer which performs multi head attention over the output of the encoder stack. 1 the software receives a question regarding a text sequence and is required to mark the answer in the sequence. With enough training data more training steps higher accuracy. Disaster NLP Keras BERT using TFHub https www. Understanding BERT Bidirectional Encoder Representations from Transformers BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. The text has to be split into a certain number of segments or chunks before being fed into the system as inputThis chunking of text causes context fragmentation. There are two sentences in this example and both of them involve the word bank https s3 ap south 1. com max 986 0 ViwaI3Vvbnd CJSQ. BERT Explained State of the art language model for NLP https towardsdatascience. 1 How Does BERT Work 3. That is the output of each sub layer is LayerNorm x Sublayer x where Sublayer x is the function implemented by the sub layeritself. That s why it learns a unique embedding for the first and the second sentences to help the model distinguish between them. com max 1773 0 KONsqvDohE7ytu_E. A sentence embedding indicating Sentence A or Sentence B is added to each token. Check out the below illustration https s3 ap south 1. png Let s first focus on the Encoder and Decoder parts only. This contains information about the input sequence This context vector is then passed to the decoder and it is then used to generate the target sequence English phrase If we use the Attention mechanism then the weighted sum of the hidden states are passed as the context vector to the decoder 1. The idea behind Transformer is to handle the dependencies between input and output with attention and recurrence completely. Let s break it down Both Encoder and Decoder are RNNs At every time step in the Encoder the RNN takes a word vector xi from the input sequence and a hidden state Hi from the previous time step The hidden state is updated at each time step The hidden state from the last unit is known as the context vector. png Encoder The encoder is composed of a stack of N 6 identical layers. 1 Transformer s Model Architecture 2. 5 Compute considerations training and applying 4. png All of these Transformer layers are Encoder only blocks. Happy learning sharing Go to TOP This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. Model size matters even at huge scale. com av blog media wp content uploads 2019 06 Screenshot from 2019 06 17 22 47 53. A positional embedding is added to each token to indicate its position in the sequence. In this kernel we are going to explore Google s BERT. 2 Text processing for BERT https s3 ap south 1. In Question Answering tasks e. The model then attempts to predict the original value of the masked words based on the context provided by the other non masked words in the sequence. During training 50 of the inputs are a pair in which the second sentence is the subsequent sentence in the original document while in the other 50 a random sentence from the corpus is chosen as the second sentence. Using BERT a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label. The number of encoder and decoder units is a hyperparameter https s3 ap south 1. The entire input sequence goes through the Transformer model. gif The above seq2seq model is converting a German phrase to its English counterpart. The output of the CLS token is transformed into a 2 1 shaped vector using a simple classification layer learned matrices of weights and biases. Classification tasks such as sentiment analysis are done similarly to Next Sentence classification by adding a classification layer on top of the Transformer output for the CLS token. Intution behind RNN based Sequence to Sequence Model Sequence to sequence seq2seq models in NLP are used to convert sequences of Type A to sequences of Type B. com max 1321 0 m_kXt3uqZH9e7H4w. Bidirectional means that BERT learns information from both the left and the right side of a token s context during the training phase. The Encoder block has 1 layer of a Multi Head Attention followed by another layer of Feed Forward Neural Network. These combinations of preprocessing steps make BERT so versatile. com bert explained state of the art language model for nlp f8b21a9b6270 2. 5 Compute considerations training and applying https miro. The bidirectionality of a model is important for truly understanding the meaning of a language. png If we try to predict the nature of the word bank by only taking either the left or the right context then we will be making an error in at least one of the two given examples. Thanks you very much for you open source contibutions. com av blog media wp content uploads 2019 09 bert_emnedding. For example translation of English sentences to German sentences is a sequence to sequence task. BERT Pre training of Deep Bidirectional Transformers for Language Understanding https arxiv. png Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions. The first is a multi head self attention mechanism and the second is a simple positionwise fully connected feed forward network. Implementation of BERT using TFHub 4. Self attention allows the model to look at the other words in the input sequence to get a better understanding of a certain word in the sequence. Each layer has two sub layers. We also modify the self attentionsub layer in the decoder stack to prevent positions from attending to subsequent positions. In technical terms the prediction of the output words requires 1. Understanding BERT for Disaster NLP Introduction Welcome to this kernel. Next Sentence Prediction NSP In the BERT training process the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. For instance on the MNLI task the BERT_base accuracy improves by 1. com xhlulu disaster nlp keras bert using tfhub Thanks to all the above blogs and article contributors. When the model is processing the word it self attention tries to associate it with animal in the same sentence. To facilitate these residual connections all sub layers in the model as well as the embeddinglayers produce outputs of dimension dmodel 512. 6 Encoding the text into tokens masks and segment flags 4. In other words the text is split without respecting the sentence or any other semantic boundary But undoutedly transformer inspired BERT and all the following breakthroughs in NLP. png Take a look at the above image. For example running this by clicking run or pressing Shift Enter will list all files under the input directory Any results you write to the current directory are saved as output. ", "id": "ratan123/in-depth-guide-to-google-s-bert", "size": "18072", "language": "python", "html_url": "https://www.kaggle.com/code/ratan123/in-depth-guide-to-google-s-bert", "git_url": "https://www.kaggle.com/code/ratan123/in-depth-guide-to-google-s-bert", "script": "Model Adam Input tensorflow Dense tensorflow.keras.callbacks ModelCheckpoint tensorflow.keras.models bert_encode build_model tensorflow.keras.optimizers tensorflow.keras.layers pandas tensorflow_hub numpy ", "entities": "(('One way', 'right prediction'), 'be') (('Transformer', 'RNNs'), 'introduction') (('you', 'kernel'), 'consider') (('com bert', 'nlp f8b21a9b6270 2'), 'explain') (('that', 'text'), 'receive') (('Self attention', 'parallel'), 'compute') (('sentence', 'Sentence token'), 'add') (('which', '110 only million parameters'), 'be') (('LayerNorm Sublayer where x', 'sub'), 'be') (('attention', 'same sentence'), 'try') (('models', 'Type B.'), 'use') (('characteristic which', 'context increased awareness'), 'converge') (('Classification tasks', 'CLS token'), 'do') (('prediction', '1'), 'require') (('hidden state', 'context vector'), 'let') (('com xhlulu disaster nlp keras bert', 'awesome kernel'), 'refer') (('we', 'BERT'), 'go') (('Understanding', 'right layers'), 'design') (('png encoder', 'N 6 identical layers'), 'Encoder') (('that', 'NER label'), 'train') (('as well embeddinglayers', '512'), 'produce') (('BERT', 'NLP https towardsdatascience'), 'explain') (('which', 'BERT'), 'try') (('Text 2 processing', 'BERT https s3'), 'ap') (('BERT pre trained model', 'language taskspecific architecture substantial modifications'), 'finetune') (('model', 'sequence'), 'attempt') (('model', 'sequence'), 'allow') (('Vanishing', 'exploding gradients'), 'call') (('concept', 'Transformer paper'), 'present') (('other semantic undoutedly transformer', 'following NLP'), 'split') (('random sentence', 'first sentence'), 'be') (('bidirectionality', 'language'), 'be') (('order Segment Embeddings BERT', 'tasks Question Answering'), 'add') (('read_csv Input data files', 'the'), 'be') (('It', 'python docker image https kaggle github'), 'go') (('RNN S 1 Dealing', 'model architecture prevents parallelization'), 'limitation') (('training enough data', 'training more steps higher accuracy'), 'with') (('idea', 'attention'), 'be') (('right then we', 'given examples'), 'png') (('Compute 5 considerations', 'https miro'), 'train') (('that', 'answer'), 'train') (('sometimes called', 'sequence'), 'be') (('overcome', 'training two strategies'), 'use') (('It', 'algorithm'), 'figure') (('part', 'kernel https www'), 'implementation') (('multi', 'encoder stack'), 'layer') (('other 50 random sentence', 'second sentence'), 'be') (('second sentence', 'subsequent original document'), 'NSP') (('then significant amount', 'context'), 'lose') (('Sentence embeddings', '2'), 'be') (('chunking', 'context fragmentation'), 'have') (('number', 'encoder units'), 'be') (('software', 'sequence'), 'receive') (('Disaster NLP Keras', 'TFHub https www'), 'BERT') (('output', 'weights'), 'transform') (('Finally we', 'disaster NLP'), 'see') (('decoders', 'Encoder Decoder Attention layer'), 'let') (('BERT_base accuracy', '1'), 'improve') (('following steps', 'indeed first'), 'png') (('embedding comprehensive scheme', 'model'), 'contain') (('outputs', 'https s3'), 'concatenate') (('All', 'Transformer layers'), 'png') (('Compute 5 considerations', '4'), 'train') (('Masked Next Sentence Prediction', 'two strategies'), 'train') (('above model', 'English counterpart'), 'gif') (('Encoder block', 'Feed Forward Neural Network'), 'have') (('why it', 'them'), 's') (('i', 'less i.'), 'ensure') (('you', 'output'), 'list') (('s', 'first Encoder parts'), 'let') (('encoder blocks', 'other'), 'be') (('then weighted sum', 'decoder'), 'contain') (('decoder', 'input sequence'), 'help') (('BERT loss png function', 'non masked words'), 'take') (('both', 'word bank https s3'), 'be') (('BERT', 'steps'), 'make') (('Model Build 7 Train', 'Submit References'), 'predict') (('positional embedding', 'sequence'), 'add') (('How Transformers', 'Art Models https www'), 'work') (('encoder stack', 'units'), 'have') (('relatively straightforward BERT', 'core model'), 'be') (('model', 'different positions'), 'allow') (('only 15', 'training pre steps'), 'approach') (('decoder', 'N 6 identical layers'), 'decoder') (('110 layers million Large 24 transformer', 'attention 16 heads'), 'have') (('3 Limitations', 'length text only fixed strings'), 'deal') (('input representation', 'corresponding token segment embeddings'), 'mark') (('therefore Multi', 'Attention'), 'refer') (('translation', 'task'), 'be') (('we', 'layer normalization'), 'employ') (('com understanding 2019 06 transformers', 'art models'), 'blog') (('input entire sequence', 'Transformer model'), 'go') (('input embedding', 'sentence'), 'be') (('s', 'sequence'), 'let') (('BERT', 'training phase'), 'mean') (('We', 'layer normalization'), 'employ') (('We', 'subsequent positions'), 'modify') (('which', 'context inherently learning'), 'predict') (('SEP token', 'sentence'), 'insert') (('BERT 1 architecture', 'Transformer'), 'architecture') (('decoder', 'Masked Multi Head extra Attention'), 'have') (('we', 'model building phase'), 'now') (('input', 'model'), 'help') "}