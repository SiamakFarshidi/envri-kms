{"name": "automated feature selection with sklearn ", "full_name": " h1 Automated feature selection with sklearn h2 Discussion h2 GenericUnivariateSelect h2 Model based feature selection SelectFromModel h2 Recursive feature elimination RFE and RFECV h1 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "While the number of features is small or you have time to sit down and consider them all feature selection is mainly a hand driven process. html univariate feature selection points out the best options for this approach linear_model. That concludes this notebook In the next notebook I will step beyond sklearn built in to consider a feature selection algorithm implemented as an sklearn contrib module boruta. This is due to laziness as there is a boundary to how many variables your average notebook author is willing to deal with and how much time they re willing to dedicate to the task at hand. Here s all you need to do Using this function revolves around inputting a function that takes the X and y arrays performs some kind of statistical test on the values and then returns the score per feature in X. To demonstrate I ll use the Kepler dataset. Many machine learning related Kaggle notebooks for example are based on easily processed data or only on well understood subsets of data. Given this fact we might decide that a worthwhile strategy would be to first preprocess the data to remove the pixel values that are mostly whitespace. I prefer the mutual_info_ tests because they just work out of the box. Measuring feature importance this way is simple These results agree decently well with the results we got from the information theoretic approach in the previous section. I find almost all the sklearn feature selection algorithms to be useful if not to use then at least to think about so this notebook has very similar coverage to the material covered in the official documentation http scikit learn. In this notebook I examine and demonstrate some tools available in Python for performing automated feature selection. However this is not always possible. In scenarios where the number of variables are overwhelming or your time is limited automated or semi automated feature selection can speed things up. com residentmario lasso regression with tennis odds by taking the coef_ of features deemed insignificant to 0 and decision trees which feature expose Gini importance via a feature_importances_ class variable. It appears that I accidentally chose a good number of predictor variables at least as far as RFECV is concerned While I won t spend any more time treating RFECV here you can read a bit more on cross validation and get some insight into how this algorithm works by looking at this notebook https www. To demonstrate the API let s first try GenericUnivariateSelect out on a simple character recognition problem. Recursive feature elimination RFE and RFECV Recursive feature elimination is the sklearn sort of implementation of a really old idea stepwise selection that used to be very popular. The following list considers the 3 possible pairs of feature choices and the difference in the sets chosen by the selectors for the Kepler dataset That these different approaches came to such deviant conclusions is not unexpected it s mainly a signal that the weaker of the 20 features we ve selected mostly come down to noise as all of the feature importance and ranking visualizations we have constructed so far have shown. Here s one example of a glyph from the dataset In training on this dataset it may naturally occur to us that hey most of the space in the image is actually whitespace the space around the character not taken up by the character itself. We can perform feature selection by using the feature ranks generated by such machine learning models as a ranking and then pruning the features based on that ranking. This approach is good because it is non parametric basing our selection on features that get picked by an actual machine learning algorithm not a statistical test makes a lot of sense. These are For regression f_regression mutual_info_regression For classification chi2 f_classif mutual_info_classif All of these tests in some way attempt to infer the relatedness of the predictor variables with the target variables. There are two classes of these tests one implemented for regression and the other implemented for classification. GenericUnivariateSelect GenericUnivariateSelect is a sklearn. fpr is the false positive rate we may use this argument to select columns with a certain level of risk e. If your goal is to build a logistic model or a linear support vector machine then it makes sense to make your feature selection decisions based on outputs from those two models but in general decision trees are by far the most popular choice for model based feature selection. This dataset consists of several thousand 32 by 32 pixel images of handwritten Arabic characters. Up to you whether or not you re comfortable using this toolchain. For the official documentation on GenericUnivariateSelect click here http scikit learn. The obvious example is linear regression which works by applying a coefficient multiplier to each of the features. com residentmario cross validation schemes with food consumption. We can apply mutual_info_classif by running a simple built in. RFECV is a slight tweak to RFE that uses cross validation to determine the optimal stopping point in terms of numbers of columns eliminating user choice in the n_features_to_select attribute. Feature selection is the process of tuning down the number of predictor variables used by the models you build. Here is a heatmap demonstrating what implementing this strategy would look like This heatmap shows the mean value of each pixel in the dataset. For example when faced with two models with the same or nearly the same score but with the latter model using more variables your immediate instinct should be to choose the one with fewer variables. Unsurprisingly the variable that best predicts whether or not Kepler will classify an object as a planet or not is the predisposition mdash that is whether or not the planet is already considered a CANDIDATE or not usually the alternative is FALSE POSITIVE and Kepler is just double checking. com questions and answers 46335 on the Kaggle Q A Forum. html univariate feature selection. Defining the metric is up to you you pass it a function and it does the rest. This methodology is implemented in RFE and is basically just a looped alternative to the SelectFromModel all at once approach. By looking at this point you might reasonably conclude that curating the feature space by 50 would be appropriate. Keep doing this until the desired number of features is reached. Then we would do So far we ve been using GenericUnivariateSelect with percentile arguments however the available selection options are actually percentile k_best fpr fdr fwe. But for automated feature selection we probably want to avoid coming up with our own expressions. This operator s main feature is its lack of opinion. This results in sometimes different sometimes very different column selections depending on the algorithm. This dataset contains information on potential planets observed and confirmed or unconfirmed by the Kepler Space Observatory. Of course in this case the choice of the algorithm you use matters a lot. Datasets may easily have hundreds or even thousands of variables quickly outrunning human comprehension. DecisionTreeClassifier for classification. And even when you do have the incentive to hand roll and curate your features automated feature selection provides some useful early directions for exploration during the exploratory process. The second of these is obviously give me the N best features but the remaining three options take a bit of explaining. Tuning the number of parameters in a model is a natural part of data science in practice and is something that comes naturally as part of the model building process. PS you may also like this thread https www. Given a set of data build a model on that data then assess the importance of the variables and prune the weakest feature. Automated feature selection with sklearn DiscussionThe optimal machine learning problem approach is to take a dataset perform extensive EDA on it and understand many to most of the important properties of the predictors _before_ getting as far as seriously training models on these variables. GenericUnivariateSelect makes it easy to implement this strategy. For datasets with many variables relatively strongly correlated with one another and relatively weakly correlated with the target variable this approach may result in slightly different feature choices from those made by naive model based selection. Luckily sklearn defines a handful of different pre built statistical tests that we may use. It s really useful to able to hand roll your metrics like this. Other times there just isn t enough time. These are non parametric tests which use k nearest neighbors to measure the scale of the relationship between the predictor values and the target variable. This is because they provide an easily interpretable measurement Gini efficiency and have been found to work great at ranking feature importance in practice. they are designed to select all columns which pass a certain p value threshold. Suppose we want to work with just the top fifty percent of the columns here. The sklearn documentation describing this approach here http scikit learn. The advantage of this approach is that it will not remove variables which were deemed insignificant at the beginning of the process but become more and more significant as lesser features are removed. Obviously the higher the coefficient the more valuable the feature. Sometimes the dataset has too many variables. com residentmario ml visualization with yellowbrick 1. LinearSVC and trees. Finally fwe is family wide error rate this can be used to control the level of risk that at least one of the columns returned is not actually correlated. Here I ve plotted out the resulting per variable score in a heatmap. DecisionTreeRegressor for regression and one of linear_model. ConclusionAlthough the different approaches agree on the most important variables in the dataset they disagree on the finer points. With that function in tow here is the entire process It doesn t come equipped with the cool statistical doo dads that GenericUnivariateSelect has access to but in practice this approach is probably used far more often. That s a lot of statistical trickery. Here are the columns that we selected Note the use of the get_support method on the GenericUnivariateSelect object to get the indices of the columns that were carried over. That model is simpler to train simpler to understand easier to run and less likely to be leaky. fdr is the false discovery rate and can be specified to select columns with a certain level of risk that actually correlated columns will be rejected. Other good examples are lasso regression which features built in variable selection I show an application of this feature here https www. 25 that they not correlated after all. The F test statistics only find linear relationships making them only appropriate for performing linear regression and the chi squared test requires that the data be appropriately scaled which it rarely is and non negative which it only sometimes is. com residentmario automated feature selection with boruta. It supports selecting columns in one of a few different configurations k for when you want a specific number of columns percentile for when you want to a percentage of the total number of columns and so on. Note I demonstrated how to build this exact feature importance chart in even fewer steps using the yellowbricks ML visualization library in this previous notebook https www. LogisticRegression and svm. feature_selection tool that allows you to select features from a dataset using a scoring function. sklearn comes with a simple wrapper for turning these feature importance scores into a feature subselection called SelectFromModel. As we go further right we retain fewer and fewer of the original pixels. Model based feature selection SelectFromModel Some machine learning algorithms naturally assign importance to dataset features in some way. White hot pixels are the ones that most often have data a glyph passes through them while black pixels lack any data. The disadvantage is that since you have to train the model many times this approach is multiplicatively slower than the one and done. See that notebook here https www. All three of them are statistical significance tests e. drop str columns drop columns with greater than 500 null values. org stable modules feature_selection. Rebuild the classifier especially in later steps the coefficient coverage will change then repeat the process. ", "id": "residentmario/automated-feature-selection-with-sklearn", "size": "13003", "language": "python", "html_url": "https://www.kaggle.com/code/residentmario/automated-feature-selection-with-sklearn", "git_url": "https://www.kaggle.com/code/residentmario/automated-feature-selection-with-sklearn", "script": "seaborn SelectFromModel sp sklearn.feature_selection RFECV mutual_info_classif matplotlib.pyplot GenericUnivariateSelect DecisionTreeClassifier RFE pandas sklearn.tree numpy ", "entities": "(('heatmap', 'dataset'), 'be') (('decision which', 'feature_importances _ class variable'), 'regression') (('Sometimes dataset', 'too many variables'), 'have') (('false positive we', 'risk e.'), 'be') (('that', 'pixel values'), 'decide') (('Datasets', 'quickly human comprehension'), 'have') (('feature automated selection', 'exploratory process'), 'provide') (('approach', 'naive model based selection'), 'correlate') (('coefficient coverage', 'then process'), 'rebuild') (('dataset', 'handwritten Arabic characters'), 'consist') (('feature selection decisions', 'feature by far most popular model based selection'), 'make') (('you', 'scoring function'), 'tool') (('We', 'ranking'), 'perform') (('rarely non it', 'only linear regression'), 'find') (('I', 'notebook https previous www'), 'demonstrate') (('further right we', 'original pixels'), 'retain') (('sklearn', 'feature subselection'), 'come') (('it', 'strategy'), 'make') (('immediate instinct', 'fewer variables'), 'be') (('feature selection', 'time'), 'be') (('they', 'hand'), 'be') (('Here I', 'heatmap'), 'plot') (('scikit', 'documentation official http'), 'find') (('main feature', 'opinion'), 'be') (('we', 'previous section'), 'be') (('approach', 'practice'), 'be') (('we', 'columns'), 'suppose') (('which', 'p value certain threshold'), 'design') (('selection however available options', 'percentile arguments'), 'do') (('_', 'variables'), 'selection') (('These', 'target variables'), 'be') (('they', 'practice'), 'be') (('that', 'actually columns'), 'be') (('you', 'toolchain'), 'up') (('com residentmario', 'boruta'), 'automate') (('statistical test', 'sense'), 'be') (('many times approach', 'multiplicatively one'), 'be') (('most', 'character'), 's') (('you', 'matters'), 'in') (('black pixels', 'data'), 'be') (('that', 'model building process'), 'be') (('I', 'sklearn contrib module boruta'), 'conclude') (('com residentmario', 'food consumption'), 'cross') (('s', 'character recognition simple problem'), 'demonstrate') (('We', 'simple'), 'apply') (('http here scikit', 'sklearn approach'), 'documentation') (('dataset', 'Kepler Space Observatory'), 'contain') (('reasonably curating', '50'), 'conclude') (('feature selection Model based machine learning algorithms', 'way'), 'selectfrommodel') (('arrays', 'X.'), 's') (('we', 'that'), 'define') (('that', 'n_features_to_select attribute'), 'be') (('other', 'classification'), 'be') (('they', 'just box'), 'prefer') (('I', 'https here www'), 'be') (('how algorithm', 'notebook https www'), 'appear') (('they', 'finer points'), 'agree') (('three', 'them'), 'be') (('Kaggle related notebooks', 'data'), 'base') (('we', 'probably own expressions'), 'want') (('http here scikit', 'GenericUnivariateSelect click'), 'for') (('more more lesser features', 'process'), 'be') (('remaining three options', 'explaining'), 'give') (('that', 'idea stepwise really old selection'), 'elimination') (('drop str columns', 'greater than 500 null values'), 'drop') (('PS you', 'thread https also www'), 'like') (('I', 'Kepler dataset'), 'demonstrate') (('when you', 'columns'), 'support') (('at least one', 'columns'), 'be') (('non parametric which', 'predictor values'), 'be') (('It', 'this'), 's') (('it', 'rest'), 'be') (('desired number', 'features'), 'keep') (('that', 'columns'), 'be') (('I', 'feature automated selection'), 'examine') (('linear which', 'features'), 'be') (('feature automated selection', 'things'), 'speed') (('ranking we', 'feature importance'), 'consider') (('feature html univariate selection', 'approach linear_model'), 'point') (('you', 'models'), 'be') (('methodology', 'basically just looped SelectFromModel'), 'implement') (('already usually alternative', 'planet'), 'unsurprisingly') "}