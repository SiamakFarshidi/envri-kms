{"name": "welcome to deep learning cnn 99 ", "full_name": " h2 Convolutional Neural Networks h2 Load the data h2 Train the model h2 Evaluate h2 Submit ", "stargazers_count": 0, "forks_count": 0, "description": "Good luck linear algebra convert to one hot encoding example Increase this when not on Kaggle kernel 1 for ETA 0 for silent For speed. Batch Normalization is a technical trick to make training faster. Keras is already available here in the kernel and on Amazon deep learning AMI. a 10x1 array with one 1 and nine 0 s with the position of the 1 showing us the value. 3 by averaging over 5 good runs and you can get higher than that if you train overnight. This reduces the size of the image by half and by combining convolutional and pooling layers the net be able to combine its features to learn more global features of the image. We then speed things up only to reduce the learning rate by 10 every epoch. A fully connected net just treats all these values the same but a CNN treats it as a 28x28 square. Here they have 16 32 filters that use nine weights each to transform a pixel to a weighted average of itself and its eight neighbors. As the same nine weights are used over the whole image the net will pick up features that are useful everywhere. The MaxPooling layers just look at four neighboring pixels and picks the maximal value. Dropout is a regularization method where the layer randomly replaces a proportion of its weights to zero for each training sample. As optimizer we could have used ordinary stochastic gradient descent SGD but Adam is faster. SubmitTo easily get to the top half of the leaderboard just follow these steps go to the Kernel s output and submit submission. io Load the dataAs always we split the data into a training set and a validation set so that we can evaluate the performance of our model. We need to convert these to one hot encoding i. EvaluateWe only used a subset of the validation set during training to save time. We will the Sequential API where you just add on one layer at a time starting from the input. I will show you how it is done in Keras which is a user friendly neural network library for python. If you have Keras installed for Theano backend you might start seeing some error message soon related to channel ordering. We now reshape all data this way. relu is the activation function x max x 0. If you instead wants to increase accuracy try adding on two more layers or increase the number of filters. If this had been RGB images there would have been 3 channels but as MNIST is gray scale it only uses one. This means generating more training data by randomly perturbing the images. With proper training we should get really good results. Keras wants an extra dimension in the end for channels. It has been sweeping the board in competitions for the last several years but perhaps its first big success came in the late 90 s when Yann LeCun 1 used it to solve MNIST with 99. This forces the net to learn features in a distributed way not relying to much on a particular weight and therefore improves generalization. The model needs to be compiled before training can start. io backend set_image_dim_orderingIt would be possible to train the net on the original data with pixel values 0 to 255. Train the modelKeras offers two different ways of defining a network. If you train this model over hundreds of epochs augmentation will definitely improve your performance. If we use the standard initialization methods for weights however data between 0 and 1 should make the net converge faster. Here in the Kernel we will only look at each image 4 5 times so the difference is smaller. This notebook is written for the tensorflow channel ordering. Thes two graphs explain the difference It s easy to understand why a CNN can get better results. If done in the right way it can force the net to only learn translation invariant features. In contrast what I will show you here is nearly state of the art. If you then ensemble over several runs you should get close to the best published accuracy of 99. It s easy however to create a net that overfits with perfect results on the training set and very poor results on the validation data. The labels were given as integers between 0 and 9. Now let s check performance on the whole validation set. It relies on either tensorflow or theano so you should have these installed first. We train once with a smaller learning rate to ensure convergence. In fact we have only gone through the training data approximately five times. We use a Keras function for augmentation. Ignore the 100 results on the leaderboard they were created by learning the test set through repeat submissions Here goes 1 http yann. If you ve successfully come this far you can now create similar CNN for all kinds of image recognition problems. com exdb lenet If you don t already have Keras 1 you can easily install it through conda or pip. In the Kernel 20 minutes training we will achieve 99 but if you train it overnight or with a GPU you should reach 99. Not too bad considering the minimal amount of training so far. Another important method to improve generalization is augmentation. Metrics is only used for evaluation. Getting convergence should not be a problem unless you use an extremely large learning rate. Many other notebooks here use a simple fully connected network no convolution to achieve 96 97 which is a poor result on this dataset. Convolutional Neural NetworksIf you want to apply machine learning to image recognition convolutional neural networks CNN is the way to go. If this happens you could try increasing the Dropout parameters increase augmentation or perhaps stop training earlier. Keras has a function for this We will use a very small validation set during training to save time in the kernel. As it is only nine weights we can stack many convolutional layers on top of each other without running out of memory time. The most important part are the convolutional layers Conv2D. In the end we use the features in two fully connected Dense layers. This can easily be solved 1. I now select the class with highest probabilitySubmitting from this notebook usually gives you a result around 99 with some randomness depending on weight initialization and test train data split. As you can see there are quite a few parameters that could be tweaked number of layers number of filters Dropout parameters learning rate augmentation settings. csv y_hat consists of class probabilities corresponding to the one hot encoding of the training labels. This is often done with trial and error and there is no easy shortcut. Each data point consists of 784 values. As our loss function we use logloss which is called categorical_crossentropy in Keras. See the example with the position of the 1 showing the correct value for the digit in the graph above. ", "id": "toregil/welcome-to-deep-learning-cnn-99", "size": "6749", "language": "python", "html_url": "https://www.kaggle.com/code/toregil/welcome-to-deep-learning-cnn-99", "git_url": "https://www.kaggle.com/code/toregil/welcome-to-deep-learning-cnn-99", "script": "keras.layers keras.models train_test_split confusion_matrix LearningRateScheduler keras.utils.np_utils numpy keras.preprocessing.image Adam Dropout ImageDataGenerator BatchNormalization Dense keras.callbacks MaxPool2D keras.optimizers matplotlib.pyplot Sequential sklearn.model_selection Conv2D to_categorical # convert to one-hot-encoding sklearn.metrics Flatten ", "entities": "(('same CNN', '28x28 square'), 'treat') (('Adam', 'SGD'), 'use') (('We', 'encoding one hot i.'), 'need') (('you', 'that'), '3') (('We', '10'), 'speed') (('I', 'test train data split'), 'select') (('Keras', 'channels'), 'want') (('We', 'augmentation'), 'use') (('you', 'definitely performance'), 'improve') (('that', 'features'), 'use') (('net converge', 'however 0'), 'datum') (('io backend', '255'), 'be') (('just steps', 'submission'), 'get') (('MaxPooling layers', 'maximal value'), 'look') (('you', '99'), 'ensemble') (('that', 'validation very poor data'), 's') (('we', 'model'), 'split') (('labels', '0'), 'give') (('where you', 'input'), 'will') (('We', 'convergence'), 'train') (('you', 'filters'), 'try') (('these', 'tensorflow'), 'rely') (('they', 'http Here 1 yann'), 'ignore') (('Keras', 'deep learning AMI'), 'be') (('you', 'learning extremely large rate'), 'be') (('This', 'often trial'), 'do') (('This', 'randomly images'), 'mean') (('we', 'training only data'), 'go') (('This', 'therefore generalization'), 'force') (('you', 'perhaps training'), 'try') (('regularization where layer', 'training sample'), 'be') (('that', 'itself'), 'have') (('gray it', 'only one'), 'be') (('Batch Normalization', 'technical training'), 'be') (('only nine we', 'memory time'), 'stack') (('We', 'kernel'), 'have') (('Yann when LeCun', '99'), 'sweep') (('csv', 'training labels'), 'consist') (('which', 'neural network user friendly python'), 'show') (('we', 'Dense two fully connected layers'), 'use') (('which', 'Keras'), 'use') (('important method', 'generalization'), 'be') (('it', 'only translation invariant features'), 'force') (('Now s', 'validation whole set'), 'let') (('you', '99'), 'achieve') (('CNN', 'recognition convolutional neural networks'), 'want') (('which', 'poor dataset'), 'use') (('show', 'here nearly art'), 'be') (('Dropout', 'rate augmentation settings'), 'be') (('far you', 'image recognition problems'), 'create') (('1 you', 'conda'), 'lenet') (('luck linear Good algebra', '0 silent speed'), 'convert') (('net', 'image'), 'reduce') (('we', 'really good results'), 'get') (('Train', 'network'), 'offer') (('why CNN', 'better results'), 'Thes') (('you', 'channel soon ordering'), 'start') (('notebook', 'channel tensorflow ordering'), 'write') (('4 5 times difference', 'only image'), 'look') "}