{"name": "i commonlit explore xgbrf repeatedfold model ", "full_name": " h1 CommonLit Target Understanding and Text FE h2 The problem is more complex than you think h1 1 Introduction h3 Libraries h3 Custom Functions Below h1 2 The Data h1 3 The Target h1 4 The Error h3 Story Time h2 Understanding the Standard Error h2 Target vs Error Comparison h2 Target Segmentation h3 We ll segment the target into 3 groups h1 5 The Word Embeddings h3 What does tokenization mean h2 I Doc2Vec h2 II SentenceBERT h1 6 Text Preprocessing h1 7 English Word Frequency Model Submission h2 I Data Preprocessing h2 II Create More Features h2 III RAPIDS XGBoost h2 IV XGBRF Cross Validation h1 My Specs ", "stargazers_count": 0, "forks_count": 0, "description": "To test how effective is our Embedding method Read in data Plot Save data to W B Dashboard Plot Log plot into W B Plot sns. Text PreprocessingAnother thing to be done besides words embeddings is preprocessing our text in a manner that it would be much cleaner and easier for the models to digest. The Error Story TimeLet s forget for a second about the classification problem and the fact that we want to train an AI to distinguish between a more complex text and a rather easy one. If humans dissagreed half a point on so many paragraphs how is our AI going to perform Target vs Error Comparison Note How do we interpret this plot When the target is 1 so the complexity is quite neutral the error decreases a little. It will ask for the API key to login which you can get from your W B profile click on Profile Settings scroll to API keys. There is subjectivity in our text too. load open tfidfvectorizer. Create More FeaturesNow we can create some features from the new created dataset. Note We can see that freq_sum freq_min and freq_mean are the most important features although we have more than 11 700 columns for the words in our texts. Quite long I would say. Note The target in our case has a distribution very close to normal. If we ll choose to create embeddings of 100 numbers a vector and we have 200 words in a paragraph and we have 2 834 paragraphs then we ll end up with and object of size 100 x 200 x 2 834 56 680 000. Create Train the model Make prediction TRAIN Save the model Libraries for models Convert data to CPU Create Folds TRAIN Save the model Plot. Oooook so we looked at the target and its error to observe what we re dealing with in terms of text. After our first experiment the W B Dashboard looks like this Target SegmentationNow let s look a bit at the standard_error in terms of segmentation. But problems are NEVER that easy. What does this mean It means that if in the test set we ll have more examples that have the target value more towards the end of the histogram then our Machine Learning might have even more troubles classifying them. com ayuraj experiment tracking with weights and biases. Coders are people that are qualified to assess images and follow a strict set of rules to classify if for example in an image the person is happy or sad. Libraries RAPIDS info here https rapids. What does tokenization mean Tokenization is a fancy word for saying splitting sequences into words. This way we can choose afterwards which one to choose when creating the more complex models more details are coming in my second notebook. Meaning that we have MANY texts with more than 0. 82 I would call this a win especially because we didn t really do much to our dataset. How hard it is and how much dissagreement is between the coders or raters. Bonus So we can t retrain the TfIdfVectorizer when we submit the data. kdeplot train standard_error fill my_colors 0 color my_colors 0 lw 0. Some could find a paragraph being a bit more easy than others. 67 the highest possible difficulty text I can t understand myself and stops at 1. Now try for another one. Now let s take 2 Word Embeddings one by one as our possible methods for the models. Try to give a score to one of the texts. beautifly beautiful beautify beautification beautiful 7. hashmap Tokenize full text Get word count for each word Save data to W B Dashboard Get sum mean std etc. Let s see how this fares We can also look at feature importance to see which features out of the ones we ve already created are the most important. RAPIDS XGBoostI will try just a basic XGBoost as it is usually the best performing one. from the text frequencies Get more info from text itself Scale these features as they are HUGE TFIDF Vectorizer Create final X variable containing all info This is how our data looks now Libraries for models Basic Data Validation Create DMatrix is optimized for both memory efficiency and training speed. Understanding the Standard ErrorOur error is very skewed to the left. We ll segment the target into 3 groups high complexity medium complexity low complexity We ll follow the natural distribution of the histogram and we ll segment the data into 3 thirds. The Data Let s observe the structure of the data first 3. And who can blame the machine if even the humans are in such dissagreement We also have a little guy completely off charts. IntroductionYet another amazing competition brought by Kaggle My mother is a teacher and I know the struggle of keeping kids involved and interested in reading so I can say this competition is a bit closer to my heart. 5 on a normal rating. CommonLit Target Understanding and Text FEThe problem is more complex than you think 1. However I wanted to see how a very simple baseline would perform. ai Link to my W B Dashboard here https wandb. Competition Goal Rating the complexity of literary passages from grades 3 to 12. com rtatman english word frequency what would be the RMSE score Using embeddings and more advanced techniques will MOST DEFINITELY render better results. pkl rb X_test transformer. On a second look there s more to it than it allows to show on the surface. Custom Functions Below 2. So the coders dissagreed. It means that the word_frequencies dataset is actually helpful Yay This is a big improvement The RMSE dropped from a value of 2. I ll try to improve the model by choosing a different method making a simple RepeatedKFold on the data. So now our distribution would look like this Ok now let s look at the standard_error in terms of segmentation. This competition differenciates itself from others because there is only 1 feature to be used the text which can be highly subjective. ai andrada commonlit workspace user andrada Learn more on why and how to use W B here Experiment Tracking using W B https www. This will later help us understand how long we ll need to make the embeddings. Let s look at a few more examples of text and the target difficutly that it was given 4. 17 Leaderboard And the leaderboard score for the XGBRF Model using Repeated Folds is 0. We don t want to turn into a smoothie after the shakeup as Laura Fink https www. com analytics vidhya text classification using word embeddings and deep learning in python classifying tweets from 6fe644fcfc81. You can observe that indeed the error decreases a little for medium complexity. Note If this line throws an error try using wandb. In this case we ll have an error. However don t be rush in throwing it into a model. XGBRF Cross ValidationLet s try a different approach. com blog 2020 08 top 4 sentence embedding techniques using python and this one https medium. 71 which is the lowest difficulty dinosaurs and pretty things. Note It seems that our paragraphs have between 140 and 210 words. Usually a face can express many more feelings like happiness love disgust and a little bit of surprise in the same time. It is an outlier with the complexity set to 0. Let s assume that WE as people not as Data Scientists need to rate these texts by hand. Note I also chose to use a TfIdfVectorizer for the text feature to add more information to the models. I will try to improve this one a bit but I was just curious to see if the dataset would help. English Word Frequency Model SubmissionI was super curious to see if I would create a SUPER simple model using only some basic features from text the word frequencies dataset here https www. This one looks simple in terms of understanding the problem goal and competition metric. 93 if you have any questions on how to submit don t hesitate to ask don t forget to name your submission submission. Note My Inspo from this article https www. 55 Log plot into W B Plot Arrow Log plot into W B Create segments Plot Plot show_values_on_bars ax h_v v space 1 Log into W B Plot Log plot into W B Tokenize each paragraph Represents a document along with a tag Train model Example of new paragraph Encode the paragraphs Test with an example Print the most similar sentence to our example Tokenize convert to lower case Remove punctuation non alphabetic characters from each word Filter out stopwords Lemmatizer Example Apply to the entire text This cell was taking me 3 hours to run on how I wrote the code very poorly But adityaecdrid came to the resque with this amazing script Now it runs in less than a second English Word Frequencies Dataset Convert it into a dict i. Is the second one simpler or more complex And how big is the difference between the 2 For picking up emotion in images expressions like anger fear sadness coders are trained to score the images. However we can still make some adjustments lower casing all words removing punctuation filtering stopwords lemmatization of the tokens bringing the word to the root e. is the test set following the same pattern The CV technique in this competition will prove very valuable. Let s first see how many words we usually have in a paragraph. My Specs Z8 G4 Workstation 2 CPUs 96GB Memory NVIDIA Quadro RTX 8000 RAPIDS version 0. The target might be missleading as well as it might behave differently in the test data than what we see in the training data. The RMSE is huge considering the fact that we have numbers between 3 and 2. The Word Embeddings What is a Word Embedding Embedding words is the process of vectorizing text meaning that we re changing the characters we understand to numbers that the computer can understand. However at the ends of the distribution the standard error increases slightly meaning that there is more dissagreement between coders in these cases. The texts are very clean as they re paragraphs from academia. In front of such picture even the best rules and the most skilled coders can fail. 0 and the error as well. so you don t have to IV. The TargetOur target variable starts at 3. fit_transform X_test III. There is no correlation between the target and standard_error. csv so you won t get an error CSS STYLE Libraries GPU Libraries Environment check Secrets Custom colors Set Style Cosine Similarity Get similarity between 2 vectors. However we can state that usually we ll encounter a standard_error of 0. as the answer becomes subjective. So I saved the trained information into a pickle. To use it for inference follow the code below transformer TfidfTransformer saved CountVectorizer decode_error replace vocabulary pickle. Luckly we don t have to do much in this case. Data PreprocessingWe ll use parallelization to append to each word the frequency from the english_word_frequency dataset. ", "id": "andradaolteanu/i-commonlit-explore-xgbrf-repeatedfold-model", "size": "10664", "language": "python", "html_url": "https://www.kaggle.com/code/andradaolteanu/i-commonlit-explore-xgbrf-repeatedfold-model", "git_url": "https://www.kaggle.com/code/andradaolteanu/i-commonlit-explore-xgbrf-repeatedfold-model", "script": "wandb_callback RepeatedKFold cuml.metrics pearsonr clean_paragraph train_test_split IPython.core.display pandarallel gensim.models.doc2vec css_styling xgboost nltk.tokenize numpy seaborn _show_on_single_plot TfidfVectorizer create_wandb_hist cuml.preprocessing.model_selection wandb.xgboost Doc2Vec show_values_on_bars nltk.stem sklearn.feature_extraction.text save_dataset_artifact XGBRegressor SentenceTransformer matplotlib.patches TaggedDocument sentence_transformers matplotlib.pyplot kaggle_secrets sklearn.model_selection pandas WordNetLemmatizer stopwords color train_xgb_model nltk.corpus mean_squared_error matplotlib scipy.stats HTML UserSecretsClient create_wandb_plot cosine_similarity word_tokenize sklearn.metrics StandardScaler XGBRFRegressor sklearn.preprocessing ", "entities": "(('workspace user andrada', 'Experiment W B https here www'), 'commonlit') (('We', 'completely off charts'), 'blame') (('much models', 'manner'), 'preprocesse') (('we', '3'), 'be') (('how long we', 'embeddings'), 'help') (('Tokenization', 'words'), 'mean') (('It', '0'), 'be') (('now s', 'segmentation'), 'look') (('then we', '100 200'), 'choose') (('MOST more advanced DEFINITELY', 'better results'), 'frequency') (('Embedding how method', 'W B Plot sns'), 'test') (('still adjustments', 'root e.'), 'make') (('Basic Data Validation Create DMatrix', 'memory efficiency'), 'get') (('Data PreprocessingWe', 'english_word_frequency dataset'), 'use') (('we', 'text'), 'Oooook') (('which', 'only 1 text'), 'differenciate') (('leaderboard score', 'Repeated Folds'), 'be') (('you', 'API keys'), 'ask') (('sadness coders', 'images'), 'be') (('we', 'new created dataset'), 'create') (('paragraphs', 'between 140 words'), 'note') (('more details', 'second notebook'), 'come') (('Specs GB Memory Z8 G4 Workstation 2 CPUs 96 Quadro', 'RAPIDS 8000 version'), 'NVIDIA') (('we', 'training data'), 'missleade') (('I', 'data'), 'try') (('error standard increases', 'cases'), 'mean') (('paragraph', 'bit more others'), 'find') (('Usually face', 'same time'), 'express') (('usually we', '0'), 'state') (('t', 'Laura Fink https www'), 'don') (('s', 'data'), 'let') (('Secrets Custom colors', '2 vectors'), 'csv') (('target', 'very close normal'), 'note') (('competition', 'bit heart'), 'introductionyet') (('just dataset', 'one'), 'try') (('highest possible I', '1'), 'difficulty') (('don However t', 'model'), 'be') (('difficutly it', '4'), 'let') (('Now s', 'models'), 'let') (('word frequencies', 'text'), 'be') (('CountVectorizer decode_error', 'vocabulary pickle'), 'follow') (('Machine then Learning', 'them'), 'mean') (('we', '3 thirds'), 'segment') (('we', 'more complex text'), 'forget') (('quite error', 'little'), 'go') (('very they', 'academia'), 'be') (('person', 'image'), 'be') (('W B Dashboard Get sum', 'data'), 'Tokenize') (('So I', 'pickle'), 'save') (('Competition Goal', '3 12'), 'rate') (('how much dissagreement', 'coders'), 'be') (('beautifly beautiful beautify beautification', '7'), 'beautiful') (('Understanding', 'very left'), 'be') (('we', 'more than 0'), 'mean') (('CV technique', 'competition'), 'be') (('big RMSE', '2'), 'mean') (('we', 'usually paragraph'), 'let') (('more you', '1'), 'be') (('most important we', 'texts'), 'note') (('one', 'problem goal'), 'look') (('it', 'surface'), 's') (('it', 'just basic XGBoost'), 'try') (('TRAIN', 'Plot'), 'train') (('XGBRF Cross ValidationLet s', 'different approach'), 'try') (('computer', 'that'), 'Embeddings') (('when we', 'data'), 'Bonus') (('indeed error', 'medium complexity'), 'observe') (('Luckly we', 'case'), 'have') (('we', 'ones'), 'let') (('s', 'segmentation'), 'look') (('especially we', 'dataset'), 'call') (('line', 'wandb'), 'note') (('TargetOur target variable', '3'), 'start') (('you', 'submission submission'), 'hesitate') (('Data Scientists', 'hand'), 'let') (('I', 'models'), 'note') (('Now it', 'dict i.'), 'show_values_on_bars') "}