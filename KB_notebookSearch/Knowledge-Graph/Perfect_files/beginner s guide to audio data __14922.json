{"name": "beginner s guide to audio data ", "full_name": " h1 Freesound General Purpose Audio Tagging Challenge h3 Contents h2 1 Exploratory Data Analysis h3 Loading data h3 Distribution of Categories h3 Reading Audio Files h3 Audio Length h2 2 Building a Model using Raw Wave h3 Keras Model using raw wave h4 Some sssential imports h4 Configuration h4 DataGenerator Class h4 Normalization h4 Training 1D Conv h4 Ensembling 1D Conv Predictions h2 3 Introuction to MFCC h4 Generating MFCC using Librosa h2 4 Building a Model using MFCC h3 Preparing data h4 Normalization h4 Training 2D Conv on MFCC h4 Ensembling 2D Conv Predictions h2 5 Ensembling 1D Conv and 2D Conv Predictions h2 Results and Conclusion h2 Coming Soon ", "stargazers_count": 0, "forks_count": 0, "description": "We don t hear loudness on a linear scale. Let s listen to an audio file in our dataset and load it to a numpy arrayLet s plot the audio framesLet s zoom in on first 1000 frames Audio LengthWe shall now analyze the lengths of the audio files in our datasetWe observe 1. NormalizationNormalization is a crucial preprocessing step. Our 1D Conv model is fairly deep and is trained using Adam Optimizer with a learning rate of 0. Introuction to MFCCAs we have seen in the previous section our Deep Learning models are powerful enough to classify sounds from the raw audio. One of the challenges is that not all labels are manually verified. Let s take a tour of the data visualization and model building through this kernel. Training on Manually Verified Labels Change this to True to replicate the result To play sound in the notebook Hi hat Using wave library Using scipy Read and Resample the audio Random offset Padding Normalization Other Preprocessing Make a submission file Hi hat Random offset Padding Make a submission file Make a submission file. com zaffnet images master images raw_model. If we want to perform some action after each epoch like shuffle the data or increase the proportion of augmented data we can use the on_epoch_end method. We also generate a submission file. com miscellaneous machine learning guide mel frequency cepstral coefficients mfccs. We use some Keras callbacks to monitor the training. So if the duration of the audio file is 3. predict X_train batch_size 64 verbose 1 np. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators. 0001 Training 1D ConvIt is important to convert raw labels to integer indicesHere is the code for 10 fold training We use from sklearn. The first model will take the raw audio 1D array as input and the primary operation will be Conv1D2. input freesound audio tagging audio_train val_set. clear_session X y X_val y_val X_train train_split y_train train_split X_train val_split y_train val_split checkpoint ModelCheckpoint best_ d. h5 i Save train predictions predictions model. h5 i monitor val_loss verbose 1 save_best_only True early EarlyStopping monitor val_loss mode min patience 5 tb TensorBoard log_dir. predict X_test batch_size 64 verbose 1 np. label_idx n_folds config. predict_generator train_generator use_multiprocessing True workers 6 max_queue_size 20 verbose 1 np. input freesound audio tagging audio_train X_test prepare_data test config. In other words we should extract features that depend on the content of the audio rather than the nature of the speaker. input freesound audio tagging audio_train train. org wikipedia commons 3 3c Freesound_project_website_logo. org wiki Audio_bit_depth with a bit depth https en. png Bit depth 16 The amplitude of each sample in the audio is one of 2 16 65536 possible values. save PREDICTION_FOLDER test_predictions_ d. npy i predictions Make a submission file top_3 np. 809 2D Conv on MFCC verified labels only 168 361 0. mkdir PREDICTION_FOLDER if os. Let s now analyze the frame length distribution in Train and Test. For those interested here is the detailed explanation http practicalcryptography. logs PREDICTION_FOLDER fold_ i i write_graph True callbacks_list checkpoint early tb print 50 print Fold i model get_2d_conv_model config history model. A creative solution should be able to partially rely on these weak annotations. If you wish to train the bigger model change COMPLETE_RUN True at the beginning of the kernel. We do not require any complex feature engineering. Taking these things into account Davis and Mermelstein came up with MFCC in the 1980 s. n_folds for i train_split val_split in enumerate skf K. 785 2D Conv on MFCC 168 361 0. We fit the model using DataGenerator for training and validation splits. The aim of this competition is to classify audio files that cover real world sounds from musical instruments humans animals machines etc. org wiki Sampling_ 28signal_processing 29 of 44. iloc train_split val_set train. save PREDICTION_FOLDER train_predictions_ d. fit_generator train_generator callbacks callbacks_list validation_data val_generator epochs config. Once initialized with a batch_size it computes the number of batches in an epoch. npy i predictions Save test predictions predictions model. The second model will take the MFCCs as input. The __getitem__ method takes an index which is the batch number and returns a batch of the data both X and y after calculating the offset. If we just want to classify some sound we should build features that are speaker independent. The dummy model is just for debugging purpose. The distribution of audio length across labels is non uniform and has high variance. npy i predictions Save test predictions test_generator DataGenerator config. Model Number of Trainable parameters Public LB score 1D Conv on Raw wave 360 513 0. 895 As we can see 2D Convolution on MFCC performs better than 1D Convolution on Raw waves. It is useful for preprocessing and feeding the data to a Keras model. Building a Model using MFCC 2d_model_building Preparing Data 2d_data Normalization 2d_normalization Training 2D Conv on MFCC 2d_training Ensembling 2D Conv Predictions 2d_ensembling 5. Ensembling 1D Conv and 2D Conv Predictions Results and ConclusionSo far we have trained two models. It turns out that these techniques are still useful. Few of the labels are Trumpet Squeak Meow Applause and Finger_sapping. Before we jump to MFCC let s talk about extracting features from the sound. Anything that is global as far as the training is concerned can become the part of Configuration object. For 10 fold CV the number of prediction files should be 10. rmtree logs PREDICTION_FOLDER skf StratifiedKFold train. argsort predictions axis 1 3 predicted_labels. n_classes Normalization pythonmean np. Exploratory Data Analysis Loading data Distribution of CategoriesWe observe that 1. label_idx batch_size 64 preprocessing_fn audio_norm val_generator DataGenerator config. ModelCheckpoint saves the best weight of our model using validation data. We will ensemble these predictions later. input freesound audio tagging audio_test test. fit X y validation_data X_val y_val callbacks callbacks_list batch_size 64 epochs config. 2 seconds the audio will consist of 44100 3. load_weights best_ d. org wikipedia commons thumb b bf Pcm. 1 kHz Each second in the audio consists of 44100 samples. input freesound audio tagging audio_test y_train to_categorical train. Happy Kaggling Contents1. png http recognize speech. Samplig rate 44. The __len__ method tells Keras how many batches to draw in each epoch. Let s analyze their relative complexity and strength. org wiki Audio_bit_depth of 16 and a sampling rate https en. We will try Geometric Mean averaging and see what will be our Public LB score. We get both training and test predictions and save them as. MFCC mimics the logarithmic perception of loudness and pitch of human auditory system and tries to eliminate speaker dependent characteristics by excluding the fundamental frequency and their harmonics. label_idx num_classes config. n_folds for i train_split val_split in enumerate skf train_set train. pythonPREDICTION_FOLDER predictions_1d_conv if not os. Majority of the audio files are short. We use this weight to make test predictions. to_csv PREDICTION_FOLDER predictions_ d. The underlying mathematics is quite complicated and we will skip that. During test time only X is returned. max_epochs use_multiprocessing True workers 6 max_queue_size 20 model. Freesound General Purpose Audio Tagging Challenge Logo https upload. predict_generator test_generator use_multiprocessing True workers 6 max_queue_size 20 verbose 1 np. png Freesound is a collaborative database of Creative Commons Licensed sounds. Let s compute the MFCC of an audio file and visualize it. Building a Model using Raw WaveWe will build two models 1. DataGenerator ClassThe DataGenerator class inherits from keras. Any feature that only gives information about the speaker like the pitch of their voice will not be helpful for classification. 1 kHz 16 bit PCM https upload. Some sssential imports ConfigurationThe Configuration object stores those learning parameters that are shared between data generators models and training functions. Preparing data pythonX_train prepare_data train config. input freesound audio tagging audio_train train_set. Building a Model using MFCCWe will build now build a 2D Convolutional model using MFCC. One such technique is computing the MFCC Mel Frquency Cepstral Coefficients from the raw audio. Results and Conclusion conclusion 1. We will explain MFCC later Keras Model using raw waveOur model has the architecture as follows raw https raw. The minimum number of audio samples in a category is 94 while the maximum is 300 2. png Generating MFCC using LibrosaThe library librosa has a function to calculate MFCC. Note Sequence are a safer way to do multiprocessing. The number of audio samples per category is non nform. Instead of a linear scale our perception system uses a log scale. index val_set. Ensembling 1D Conv and 2D Conv Predictions 1d_2d_ensembling 6. If we want to double the perceived loudness of a sound we have to put 8 times as much energy into it. 844 1D Conv 2D Conv Ensemble N A 0. If you like this work please show your support by upvotes. But before the Deep Learning era people developed techniques to extract features from audio signals. Building a Model using Raw Wave 1d_model_building Model Discription 1d_discription Configuration configuration DataGenerator class data_generator Normalization 1d_normalization Training 1D Conv 1d_training Ensembling 1D Conv Predictions 1d_ensembling 3. mean X_train axis 0 std np. join list x for x in top_3 test label predicted_labels test label. csv i Ensembling 1D Conv PredictionsNow that we have trained our model it is time average the predictions of 10 folds. std X_train axis 0 X_train X_train mean stdX_test X_test mean std Training 2D Conv on MFCC pythonPREDICTION_FOLDER predictions_2d_conv if not os. csv i Ensembling 2D Conv Predictions 5. logs PREDICTION_FOLDER fold_ d i write_graph True callbacks_list checkpoint early tb print Fold i print 50 if COMPLETE_RUN model get_1d_conv_model config else model get_1d_dummy_model config train_generator DataGenerator config. index train_set. EarlyStopping stops the training once validation loss ceases to decrease TensorBoard helps us visualize training and validation loss and accuracy. Exploratory Data Analysis eda Loading data loading_data Distribution of Categories distribution Reading Audio Files audio_files Audio Length audio_length 2. Also a good feature extraction technique should mimic the human speech perception. label_idx batch_size 64 preprocessing_fn audio_norm history model. exists logs PREDICTION_FOLDER shutil. The simplest method is rescaling the range of features to scale the range in 0 1. StratifiedKFold for splitting the trainig data into 10 folds. jpg Important Due to the time limit on Kaggle Kernels it is not possible to perform 10 fold training of a large model. exists PREDICTION_FOLDER os. Introduction to MFCC intro_mfcc Generating MFCC using Librosa librosa_mfcc 4. There are four abnormal length in the test histogram. Reading Audio FilesThe audios are Pulse code modulated https en. index batch_size 128 preprocessing_fn audio_norm predictions model. http recognize speech. iloc val_split checkpoint ModelCheckpoint best_ d. I have trained the model locally and uploaded its output files as a dataset. h5 i Save train predictions train_generator DataGenerator config. com images FeatureExtraction MFCC MFCC_Flowchart. Also the proportion of maually_verified labels per category is non uniform. ", "id": "fizzbuzz/beginner-s-guide-to-audio-data", "size": "14922", "language": "python", "html_url": "https://www.kaggle.com/code/fizzbuzz/beginner-s-guide-to-audio-data", "git_url": "https://www.kaggle.com/code/fizzbuzz/beginner-s-guide-to-audio-data", "script": "optimizers Config(object) get_1d_conv_model get_2d_dummy_model wavfile keras.layers softmax IPython.display keras losses LearningRateScheduler to_categorical GlobalAveragePooling1D (EarlyStopping (Convolution2D numpy seaborn Dropout BatchNormalization Dense on_epoch_end (Convolution1D Sequence tqdm_notebook audio_norm GlobalAveragePooling2D keras.callbacks backend as K models tqdm get_2d_conv_model StratifiedKFold __data_generation matplotlib.pyplot backend relu pandas get_1d_dummy_model sklearn.cross_validation Flatten scipy.io keras.utils __len__ DataGenerator(Sequence) __init__ keras.activations __getitem__ prepare_data ", "entities": "(('Taking', '1980 s.'), 'come') (('Few', 'labels'), 'be') (('pythonX_train prepare_data', 'data'), 'config') (('com zaffnet images master', 'raw_model'), 'image') (('i', 'submission file'), 'make') (('2 seconds audio', '44100'), 'consist') (('Deep Learning models', 'raw audio'), 'be') (('s', 'sound'), 'let') (('that', 'features'), 'want') (('it', 'time 10 folds'), 'Ensembling') (('MFCC', 'fundamental frequency'), 'mimic') (('second model', 'input'), 'take') (('fit_generator train_generator', 'callbacks_list validation_data val_generator epochs config'), 'callback') (('2D Conv Predictions far we', 'two models'), 'train') (('We', 'training splits'), 'fit') (('feature extraction Also good technique', 'speech human perception'), 'mimic') (('that', 'classification'), 'be') (('Note Sequence', 'safer multiprocessing'), 'be') (('png Freesound', 'Creative Commons collaborative Licensed'), 'sound') (('creative solution', 'partially weak annotations'), 'be') (('what', 'Geometric Mean averaging'), 'try') (('you', 'kernel'), 'wish') (('number', 'category'), 'be') (('frames Audio first 1000 LengthWe', 'datasetWe'), 'analyze') (('label_idx', '64 preprocessing_fn audio_norm history model'), 'batch_size') (('as far training', 'Configuration object'), 'become') (('distribution', 'non high variance'), 'be') (('X_train batch_size', '64 1 np'), 'predict') (('humans', 'machines'), 'be') (('number', 'prediction files'), 'be') (('which', 'generators'), 'guarantee') (('2D 809 Conv', 'labels'), 'verify') (('s', 'it'), 'let') (('s', 'model kernel'), 'let') (('predict_generator use_multiprocessing True workers', '6 20 1 np'), 'test_generator') (('argsort predictions', '1 3 predicted_labels'), 'axis') (('s', 'relative complexity'), 'let') (('test label predicted_labels', 'test label'), 'list') (('Building', 'two models'), 'build') (('we', 'it'), 'want') (('dummy model', 'just purpose'), 'be') (('Exploratory Data Analysis Loading data Distribution', '1'), 'observe') (('h5 i', 'train predictions train_generator DataGenerator config'), 'save') (('COMPLETE_RUN get_1d_conv_model else model config model', '50'), 'fold') (('quite we', 'that'), 'be') (('that', 'speaker'), 'extract') (('we', 'Raw waves'), '895') (('1D Conv model', '0'), 'be') (('Bit 16 amplitude', '2 16 65536 possible values'), 'depth') (('s', 'Train'), 'let') (('labels', 'challenges'), 'be') (('perception system', 'log scale'), 'use') (('Building', 'MFCC'), 'build') (('One such technique', 'raw audio'), 'compute') (('com images FeatureExtraction', 'MFCC_Flowchart'), 'MFCC') (('it', 'epoch'), 'compute') (('We', 'them'), 'get') (('ModelCheckpoint', 'validation data'), 'save') (('We', 'test predictions'), 'use') (('I', 'dataset'), 'train') (('early tb', 'get_2d_conv_model config history model'), 'fold') (('EarlyStopping val_loss min 1 save_best_only early patience', 'val_loss'), 'verbose') (('simplest method', '0'), 'rescale') (('We', 'predictions'), 'ensemble') (('submission file', 'submission file'), 'change') (('input freesound audio', 'audio_test y_train train'), 'tag') (('We', 'linear scale'), 'don') (('We', 'feature complex engineering'), 'require') (('png Generating MFCC', 'MFCC'), 'have') (('us', 'training'), 'stop') (('So duration', 'audio file'), 'be') (('i', 'test predictions test_generator DataGenerator config'), 'save') (('that', 'data generators models'), 'object') (('Keras later Model', 'https raw raw'), 'explain') (('maximum', 'category'), 'be') (('It', 'Keras model'), 'be') (('input', 'audio 1D raw array'), 'take') (('Also proportion', 'category'), 'be') (('it', 'large model'), 'Important') (('i', 'test predictions predictions model'), 'save') (('_ _ len _ _ method', 'epoch'), 'tell') (('we', 'on_epoch_end method'), 'want') (('interested here detailed explanation', 'practicalcryptography'), 'be') (('you', 'upvotes'), 'show') (('people', 'audio signals'), 'develop') (('h5 i', 'train predictions predictions model'), 'save') (('We', 'training'), 'use') (('We', 'sklearn'), '0001') (('Training 2D Conv', 'MFCC pythonPREDICTION_FOLDER predictions_2d_conv'), 'std') (('input freesound audio', 'audio_train'), 'tag') (('Exploratory Data Analysis', 'Files Audio Length Audio audio_length'), 'read') (('Model Number', 'Public LB 1D Conv'), 'score') (('X_test prepare_data', 'input freesound audio_train'), 'audio') (('index', 'preprocessing_fn audio_norm predictions 128 model'), 'batch_size') (('which', 'offset'), 'take') "}