{"name": "how i taught myself deep learning vanilla nns ", "full_name": " h1 1 Introduction h1 2 Before we start h1 3 What are Neural Networks h2 3 1 Youtube Videos that will save you time h2 3 2 The Perceptron h2 3 3 How does a plain Vanilla Neural Network look h2 3 4 Why so deep Deep vs Shallow Networks h1 4 The Data MNIST h1 5 Vanilla FNN Neural Network h2 5 1 Activation Functions h3 Rectifier Linear Unit ReLu h3 Sigmoid h3 Tanh h2 5 2 Making a Forward Pass h2 5 3 Backpropagation h3 5 3 1 Loss and Optimizer Functions h3 5 3 2 MNISTClassifier trainable parameters h3 5 3 3 Do 1 backpropagation Compute the LOSS and OPTIMIZE for our images example h1 6 Training the Neural Network h2 6 1 Batches h3 6 1 1 Training the Example Network on a batch instead of image by image h2 6 2 Accuracy of the Classifier h2 6 3 Iterations vs Epochs h2 6 4 Predefined Functions Accuracy and Training Loop h3 6 4 1 Predefined Accuracy Function h3 6 4 2 Predefined Training Function h1 7 Model Evaluation h1 8 Overfitting h2 8 1 Data Augmentation h3 8 1 1 Training on Augmented Data h2 8 2 Weight Decay and Learning Rate h2 8 3 Dropout and Layer Optimization h3 8 3 1 Dropout Function h3 8 3 2 Layer Optimization h3 8 3 3 Changing the Structure of our MNISTClassifier h4 Improved Model Structure h1 9 Bonuses h2 9 1 Confusion Matrix h1 Other How I taught myself Deep Learning Notebooks h1 References ", "stargazers_count": 0, "forks_count": 0, "description": "What is Backpropagation really doing Cheers again to 3Blue1Brown for his amazing structured videos. 1 Confusion Matrix Other How I taught myself Deep Learning Notebooks How I taught myself Deep Learning ConvNet CNNs https www. These Activation Functions squish the neuron s output between the 2 values preventing big numbers becoming much bigger and small numbers becoming much smaller. Size 10 20 10 weights or parameters for each 20 neurons 10x20 weights in total 6 torch. L1Loss MSE torch. Input An image of a cat with height 10 pixels length 20 pixels and channels 1 because is B W for RGB images the number of channels is 3. Please bear with me. Took 1 image through the network and create prediction3. Therefore the FNN is a Classifier. Let s start programming. 1 Training the Example Network on a batch instead of image by image 6. This is used because in general we would want to train the network for longer. Layer1 Because the input in the FNN must be linear the matrix is vectorized by reshaping it into a vector of size 200. This disables the gradients Dropout function etc and sets the model in evaluation mode. 3 Changing the Structure of our MNISTClassifier Now let s change our Neural Net a bit nn. com andradaolteanu how i taught myself deep learning convnet cnns How I taught myself Deep Learning Recurrent NNs https www. Size 50 50 biases 3 torch. Training the Neural NetworkOur purpose now that we have the structure in place and the data is to make the Vanilla FNN perform well. Epoch number of times all training data was used once to update the parameters. Large weights mean that the prediction relies heavily on the content of one or multiple pixels. Hence in different iterations of training we will drop out a different set of neurons. Size 50 784 50 weights or parameters for each 28x28 neurons 28x28x50 weights in total 2 torch. They are interpreted as probabilities probability of input to be digit 1 probability of input to be digit 2 etc. The next chapters will be dedicated to training the network and improving it. com andradaolteanu how i taught myself deep learning recurrent nns If you have any questions please do not hesitate to ask. com understand the dynamics of learning rate on deep learning neural networks Import torch Create some tensors Imports to access build in functions to build the NN to access activation functions to access the MNIST dataset to build out optimizer for plotting Load in the data from torchvision datasets train True to access training images and train False to access test images We also transform to Tensors the images How the object looks Check a sample of the images We need to import again because our train and test data are Tensors already Creating the Network nn. HingeEmbeddingLoss Cosine Loss torch. For example when you re building a RandomForestClassifier sklearn already has that made for you. We can change that by making it changable during training so eventually we can apply Grid Search and find the best combination possible. So be aware Tensors Instead of working with tabular data or numpy arrays we ll be working with tensors. Batches can have different sizes one extreme is batch_size 1 meaning that we compute the loss and update after EACH image so we have 60 000 batches of size 1 a batch_size 60 means that for 60 000 training images we ll have 1000 batches of size 60 the other extreme is batch_size 60 000 when we input ALL images and do 1 backpropagation we have 1 batch of size 60 000 images The actual batch size that we choose depends on many things. It is known that Deep Neural Nets thin and tall are better than Shallow ones fat and short. html What the hell is a Perceptron https towardsdatascience. This happens because the deep ones can learn more and more abstract representations the deeper you go. Layer2 the first layer is multiplied by the weights and formes the second layer composed by 300 neurons5. Size 10 value of the final neurons the log probabilities 5. How I taught myself Deep Learning ConvNet CNNs https www. The Data MNISTWe ll be working in MNIST Dataset which is usually the go to dataset when starting in Neural Networks. We want our batch size to be large enough to not be too noisy but not so large as to make each iteration too expensive to run. Look at the prediction vs actual and computed the loss4. Pro Tip Use this site http alexlenail. That way we can compute the average loss across a mini batch of multiple images and take a step to optimize the average loss. For Neural Networks is different they can be so volatile depending on the structure of your input eg. In FNNs we train using gradient descent to update the weights. html to construct your own FNN it can be dreadful to draw lines by yourself. 2 Accuracy of the ClassifierDuring Training we would usually want to check for the accuracy of the model to see how good or how bad is performing. The beauty in Deep Learning is actually the amount of data and computations that can be handled much better than in an usual ML algorithm. Function where you take the image though the FNN Flatten image from 1 28 28 to 784 Create Log Probabilities set the random seed set random seed in numpy Selecting 1 image with its label Creating an instance of the model Creating the log probabilities Choose maximum probability and then select only the label not the prob number Creating LOSS and Optimizer instances Loss is the function that calculates how far is the prediction from the true value Using this loss the Optimizer computes the gradients of each neuron and updates the weights Let s also look at how many parameters weights and biases are updating during 1 single backpropagation Parameter Understanding set the random seed set random seed in numpy Clear gradients always needs to be called before backpropagation Compute loss Compute Gradients Update weights After this 1 iteration the weights have updated once Create trainloaders for train and test data We put shuffle True so the images shuffle after every epoch Inspect Trainloader Select First Batch 60 1 28 28 60 images of size 1 28 28 actual labels for the 60 images 60 labels in total set the random seed set random seed in numpy Stop after 3 iterations Prediction Update weights or parameters Computes the gradient of current tensor Performs a single optimization step. Until now in this notebook we haven t completed yet a full epoch. By calling this function we make this possible. In addition it usually works with images and text while ML usually works with tabular data. The learning rate controls how much to change the model in response to the estimated error each time the model weights are updated https machinelearningmastery. 3 Do 1 backpropagation Compute the LOSS and OPTIMIZE for our images_example Until now we 1. What are Neural Networks How do Neural Networks learn 3. com 2018 05 wtf tensor. Embedding Loss functions whether 2 inputs are similar or not Hinge Loss torch. When you evaluate the model call your_model. 3 How does a plain Vanilla Neural Network look Plain vanilla Neural Networks or Feed Forward Neural Networks FNNs for the lazy people have the most simple architecture in the Neural Networks realm but their basics will help you understand much more complicated dinosaurs moving on. Cuz this is just it numbers. com watch v aircAruvnKk t 1007s What is Backpropagation really doing https www. Model EvaluationNow that we have our functions ready we can start training on the ENTIRE dataset. Explain code along the way to the best of my ability Note Deep learning coding is VERY different in structure than the usual sklearn for machine learning. A tensor is a container which can house data in N dimensions https www. Before we start This is my first notebook in the series How I taught myself Deep Learning. So here we will create our own neural net Note you can ignore the prints I usually put them to understand what the network does when you click run Note2 super function is there because the MNISTClassifier class inherits attributes from it s parent class nn. 1 Loss and Optimizer Functions These 2 are like brother and sister work hand in hand during the neural network training. Optimizer Function torch. For each model we dropout drop out zero out remove etc. 1 Activation FunctionsAn activation function is a fancy way of saying that we are making the output of each neuron nonlinear because we WANT to learn non linear relationships between the input and the output. Note During training it is highly important to set the model into training mode by calling your_model. The average loss across multiple training inputs is going to be less noisy than the loss for a single input and is less likely to provide bad information because of a bad input. OverfittingAs any other Machine Learning Model Neural Nets can suffer from overfitting. 3 Dropout and Layer Optimization 8. In other words weights are encouraged to be strong and independent. com what the hell is perceptron 626217814f53 3Blue1Brown videos But what is a Neural Network https www. com andradaolteanu how i taught myself deep learning convnet cnns Why ConvNets Convolutions Explained Computing Activation Maps Kernels Padding Stride AlexNet MNIST Classification using Convolutions2. 2 Predefined Training Function 7. I highly recommend taking 20 minutes and watching them before going any further it is always better to visualise than rather read to understand abstract concepts. optim updates the weights and biases to REDUCE the loss Examples Stochastic Gradient Descent SGD Adam Adam Adagrad Adagrad Different neural networks and purposes can require different loss and optimizer functions. com what the hell is perceptron 626217814f53 for some very good explanations. CrossEntropyLoss Binary Cross Entropy Loss torch. Create Model Instance Train. How I taught myself Deep Learning Recurrent NNs https www. In the above example instead of having 70 noisy losses we ll have just 7 averaged losses. Learning Rate This one is probably not new. There are maaany types of activation functions but some of them are Rectifier Linear Unit ReLu The function is linear when the activation is above zero and is equal to zero otherwise. You can also build multiple neural networks and then combine them in another one for example in a Sequence2Sequence RNN. a portion of neurons from each training iteration. Size 20 20 biases 5 torch. So to not complicate ourselves tensors are very similar with numpy arrays but they offer much better GPU support so they are faster. 2 MNISTClassifier trainable parameters 1 torch. com andradaolteanu how i taught myself deep learning recurrent nns 1 Layer RNNs Multiple Neurons RNN Vanilla RNN for MNIST Classification Multilayer RNNs Tanh Activation Function Multilayer RNN for MNIST LSTMs and Vanishing Gradient Problem Bidirectional LSTMs LSTM for MNIST Classification Pytorch This is the library we will be using it is allegedly much easier than Keras and it s starting to make a breakthrough nowadays. com understand the dynamics of learning rate on deep learning neural networks. For somebody that starts in this area with no background whatsoever it can be very confusing especially because I seem to be unable to find code with many explanations and comments. What are Neural Networks 3. They change by the case but their main purpose is the same Loss Function criterion given an output and an actual it computes the difference between them Regressive loss functions MAE torch. Size 20 50 20 weights or parameters for each 50 neurons 50x20 weights in total 4 torch. TanhA variation of the Sigmoid but it outputs values between 1 and 1. 4 Predefined Functions Accuracy and Training LoopNow let s create some functions so our trainin process will become easier 6. functions including loss optimizer functions https pytorch. But first to make the training faster we will select 500 training images and 500 testing images batch_size will be by default set to 20 images batch we ll iterate through the data 200 times num_epochs 200 8. The image composed by numbers as you saw in the above videos is therefore a matrix of shape 1 10 20. html to check all of them. This enables gradients training the Dropout function etc. html Impact of Learning Rate in NNs https machinelearningmastery. Output It is composed of 1 single neuron which fires the value 1 if the image is a cat or 0 otherwise. Classification loss functions Cross Entropy Loss torch. IntroductionThis notebook is just me being frustrated on deep learning and trying to understand in baby steps what is going on here. Let s break down the steps 1. 4 Why so deep Deep vs Shallow NetworksFirst of all the FNN isn t really doing anything special that a simple ML can t achieve. Using the loss we updated the weights and biasesThis is called training. 3 Iterations vs Epochs Iterations number of iterations is the number of times we update the weights parameters of the FNN. For images is not really necessary as they all have the same structure but I threw it here just for reference. 1 Training on Augmented Data transforms. First we make sure we disable Gradient Computing Model in Evaluation Mode Add 1 more dimension for batching. eval when we want to evaluate. Clears the gradients of all optimized Instantiate 2 variables for total cases and correct cases Sets the module in evaluation mode VERY IMPORTANT Just show first 3 batches accuracy Choose maximum probability and then select only the label not the prob number Number of correct cases we first see how many are correct in the batch then we sum then convert to integer not tensor Total cases Sets the model in evaluation mode Creates the dataloader Is formed by 20 images by default with 10 probabilities each Choose maximum probability and then select only the label not the prob number First check how many are correct in the batch then we sum then convert to integer not tensor Total cases Create dataloader for training dataset so we can train on multiple batches Shuffle after every epoch Create criterion and optimizer Losses Iterations to keep all losses during training for plotting Train and test accuracies to keep their values also for plotting Train the data multiple times Set model in training mode Create log probabilities Clears the gradients from previous iteration Computes loss how far is the prediction from the actual Computes gradients for neurons Updates the weights Save information after this iteration Compute accuracy after this epoch and save Show Accuracies Show the last accuracy registered Create plots Select images Training and Testing selection 500 training images 500 test images Create Model Instance Train. For example above we did 3 iterations. So we penalize them by adding and extra term to the criterion function. If we choose a lr too large we might overshoot the local minima while using a lr too small we might wait longer for the model to train as the steps are tinier. 4 each neuron has 40 chance of being dropped layer1_size size of the first hidden layer layer2_size size of the second hidden layer Improved Model Structure 9. Click here https pytorch. 1 Data AugmentationWhy try to collect more data when you can create some on your own Data Augmentation generates more data points from our existing data set by Flipping each image horizontally or vertically won t work for digit recognition but might for other tasks Shifting each pixel a little to the left or right Rotating the images a little Adding noise to the imageFor our example we ll rotate the images randomly up to 35 degrees. Normalize means to scale the input features of a neural network so that all features are scaled similarly. SigmoidThe sigmoid function has a tilted S shape and its output is always between 0 and 1. We have 1 example of an FNN our network is already trained to receive an B W image and create the output 1 if the image is a cat and 0 otherwise. Module is a subclass from which we inherit Here you define the structure Create first layer from 784 neurons to 50 Call activation function Second layer from 50 neurons to 20 Call Activation function Last layer from 20 neurons to 10 10 because we have 10 categories of numbers from which we need to pick 1 If we would have wanted to classify images labeled dog cat crocodile the final layer would have had 3 neurons. html WTF is a Tensor https www. Vanilla FNN Neural NetworkWhen working with neural networks you actually NEED to define a class for yourself. Pytorch has a different structure than the normal machine learning code in sklearn. com watch v Ilg3gGewQ5U list PLZHQObOWTQDNU6R1_67000Dx_ZCJB 3pi index 3 Gradient Descent how Neural Networks learn https www. 1 Youtube Videos that will save you time There are 2 super informative videos on Youtube from 3Blue1Brown that explain very well what neural networks are and what is an FNN Feed Foward Neural Network. Let s take a look 3. 2 Layer OptimizationOur MNISTClassifier had until now 3 layers with a fixed number on neurons in each layer. 3 BackpropagationSo the purpose is to UPDATE the weights and biases in the neural network so it learns to recognize the digits and accurately classify them. Structure of the FNN Dropout for first layer From 784 neurons to layer1_size Activation Function Dropout for second layer From layer1_size neurons to layer2_size Activation Function Dropout for last layer Output layer Taking the image through the NN Flatten the matrix to a vector Log Probabilities output Training on the newly network Create Model Instance Train. I highly suggest reading this blog post https towardsdatascience. This notebook is made to bring more clear understanding of concepts and coding so this would also help me add modify and improve it. I am by no means a teacher but in this notebook I will 1. Created a Vanilla FNN2. Removing it would lead to an error. So if you are frustrated just like I was when I started this stuff I hope the following guidelines will help you. 1 BatchesWith an artificial neural network we may want to use more than one image at one time. So you just call the object from the library and afterwards just fine tune the hyperparameters. Before going any further I highly recommend watching the following video which explains the concept of Backpropagation. an image of shape 3 500 250 number of hidden layers number of neurons in each hidden layer weather or not you want to call the Dropout functions etc. If you liked this upvote Cheers References Create your own FNN http alexlenail. 2 Making a Forward PassA forward pass is when you take the images one by one or batch by batch we ll come back to this and we put them through the neural network which outputs for each a log probability 10 in out case. This way we prevent the weight from being overly dependent on eachother for example for one weight to be unnecessarily large to compensate for another unnecessarily large weight with the opposite sign. 2 Weight Decay and Learning Rate Weight Decay The idea of weight decay is to penalize large weights. Let s look at 1 example The prediction is wrong Keep in mind the model is NOT trained yet so of course the prediction is not accurate. Overfitting is when a neural network model learns about the quirks of the training data rather than information that is generalizable to the task at hand. com watch v IHZwWFHWa w All torch. Predefined Function that shows 20 images Create original and rotated set Show images Creating a personalized transform First Rotates then transforms to tensor then normalizes the images Import the MNIST data aplying the transformations We select first 500 images as our training Training the model Create Model Instance Train. Nevertheless you can apply the following principles on any datasets images text tabular data audio data as all data can be represented in numbers. Layer3 The last hidden layer is composed of 400 neurons6. So please be patient with yourself and if you don t understand something right away continue reading coding and it will all make sense in the end. 2 The Perceptron A Perceptron is a single layer neural network while a Multi Layer Perceptron is called a Neural Network. In Feed Foward Neural Nets the hidden layers gradually increase decrease in hidden size number of neurons so more and more details of the input images text etc. Also the number of parameters is smaller so the training is faster. 1 Predefined Accuracy Function 6. 1 Dropout FunctionThis technique builds many models and then averages their prediction at test time this is why it is very important to call model. Share articles videos I watched that TRULY helped2. CosineEmbeddingLoss etc. This is done during backpropagation when the model literally goes back and updates the parameters weights a little bit. ", "id": "andradaolteanu/how-i-taught-myself-deep-learning-vanilla-nns", "size": "19857", "language": "python", "html_url": "https://www.kaggle.com/code/andradaolteanu/how-i-taught-myself-deep-learning-vanilla-nns", "git_url": "https://www.kaggle.com/code/andradaolteanu/how-i-taught-myself-deep-learning-vanilla-nns", "script": "torch.nn.functional seaborn torch.optim torchvision transforms        # to access the MNIST dataset MNISTClassifier_improved(nn.Module) train_network matplotlib.pyplot show20 __init__ get_accuracy forward torch.nn datasets get_confusion_matrix MNISTClassifier(nn.Module) numpy ", "entities": "(('also me', 'it'), 'make') (('FNN Feed Foward Neural Network', '3Blue1Brown'), 'be') (('Pytorch', 'sklearn'), 'have') (('1 Loss 2', 'network neural training'), 'function') (('deeper you', 'more abstract representations'), 'happen') (('general we', 'network'), 'use') (('you', 'Dropout functions'), 'image') (('last hidden layer', '400 neurons6'), 'Layer3') (('SGD Adam Adam Adagrad Adagrad Different', 'neural different loss functions'), 'update') (('they', 'input eg'), 'be') (('training', 'Create Model Instance Train'), 'transform') (('it', 'accurately them'), 'be') (('1 artificial neural we', 'one time'), 'BatchesWith') (('model time weights', 'https updated machinelearningmastery'), 'control') (('Neural How Networks', '3'), 'be') (('enables', 'etc'), 'gradient') (('training test Training selection 500 images 500 images', 'Model Instance Train'), 'clear') (('class there MNISTClassifier inherits', 'parent class nn'), 'create') (('we', 'neurons'), 'drop') (('what', 'baby steps'), 'be') (('Backpropagation', 'amazing structured videos'), 'do') (('I', 'here just reference'), 'be') (('you', 'therefore shape'), 'be') (('we', 'FNN'), 'be') (('prediction', 'one pixels'), 'mean') (('final layer', '3 neurons'), 'be') (('trainin process', 'functions'), '4') (('we', 'just 7 averaged losses'), 'in') (('which', 'out case'), 'be') (('hidden layers', 'input images'), 'increase') (('Deep learning coding', 'machine learning'), 'explain') (('So we', 'criterion extra function'), 'penalize') (('we', 'many things'), 'have') (('any further it', 'rather abstract concepts'), 'recommend') (('2 MNISTClassifier trainable', '1 torch'), 'parameter') (('average loss', 'bad input'), 'go') (('image', '1'), 'output') (('you', 'questions'), 'andradaolteanu') (('output', 'always 0'), 'have') (('How I', 'Deep Learning NNs https Recurrent www'), 'teach') (('ML', 'usually tabular data'), 'work') (('prediction', 'yet course'), 'let') (('I', 'notebook'), 'be') (('Machine Learning Model Neural OverfittingAs other Nets', 'overfitting'), 'suffer') (('4 neuron', 'second hidden layer'), 'have') (('Neural Deep Nets', 'ones Shallow fat'), 'know') (('How I', 'Deep Learning ConvNet CNNs https www'), '1') (('When you', 'your_model'), 'evaluate') (('Learning Weight 2 Weight Decay idea', 'large weights'), 'decay') (('Do 1 backpropagation', 'we'), 'compute') (('way we', 'opposite sign'), 'prevent') (('1 image', 'output'), 'have') (('hell', 'very good explanations'), 'com') (('dropout', 'etc'), 'drop') (('training data', 'once parameters'), 'use') (('ConvNets Why Convolutions', 'Convolutions2'), 'andradaolteanu') (('much bigger numbers', 'big numbers'), 'squish') (('literally back parameters', 'backpropagation'), 'do') (('this', 'function'), 'make') (('it', 'loss functions MAE Regressive torch'), 'change') (('Inspect Trainloader Select First 28 60 1 28 60 images', 'optimization single step'), 'set') (('Tensors', 'Network already nn'), 'understand') (('So you', 'library'), 'call') (('it', 'breakthrough'), 'andradaolteanu') (('How I', 'Deep Learning ConvNet CNNs https www'), 'teach') (('how how bad', 'model'), 'want') (('com', 'deep learning neural networks'), 'understand') (('we', 'randomly up to 35 degrees'), '1') (('How I', 'Deep Learning NNs https Recurrent www'), 'andradaolteanu') (('they', 'GPU much better support'), 'be') (('I', 'blog post https highly towardsdatascience'), 'suggest') (('data', 'numbers'), 'apply') (('it', 'your_model'), 'note') (('one', 'Rate'), 'learn') (('when activation', 'zero'), 'be') (('we', 'batching'), 'make') (('which', 'Neural usually when Networks'), 'work') (('way we', 'average loss'), 'compute') (('you', 'yourself'), 'FNN') (('already that', 'you'), 'make') (('special simple ML', 'really anything'), '4') (('much more complicated dinosaurs', 'Neural Networks realm'), '3') (('Vanilla FNN', 'place'), 'train') (('numpy we', 'tensors'), 'be') (('which', 'N dimensions https www'), 'be') (('Layer OptimizationOur 2 MNISTClassifier', 'layer'), 'have') (('biasesThis', 'weights'), 'update') (('which', 'Backpropagation'), 'recommend') (('Neural how Networks', 'https www'), 'list') (('best combination', 'Grid Search'), 'change') (('it', 'yourself'), 'be') (('very especially I', 'many explanations'), 'be') (('next chapters', 'it'), 'dedicate') (('we', 'input'), 'be') (('steps', 'longer model'), 'overshoot') (('we', 'ENTIRE dataset'), 'EvaluationNow') (('it', '1'), 'variation') (('matrix', 'size'), 'Layer1') (('They', 'digit 1 input'), 'interpret') (('we', 'data'), 'make') (('You', 'Sequence2Sequence RNN'), 'build') (('training', 'parameters'), 'be') (('rather that', 'hand'), 'be') (('number', 'channels'), 'input') (('first layer', '300'), 'Layer2') (('we', 'weights'), 'train') (('it', 'end'), 'be') (('haven t', 'yet full epoch'), 'complete') (('Backpropagation', 'https really www'), 'watch') (('why it', 'very model'), 'build') (('following guidelines', 'you'), 'be') (('that', 'ML much better usual algorithm'), 'be') (('Dropout', 'evaluation mode'), 'disable') (('features', 'neural network'), 'mean') (('How I', 'Deep Learning'), 'be') (('Now s', 'bit nn'), 'let') "}