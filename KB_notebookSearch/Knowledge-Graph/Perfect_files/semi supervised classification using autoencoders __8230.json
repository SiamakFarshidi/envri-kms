{"name": "semi supervised classification using autoencoders ", "full_name": " h1 Semi Supervised Classification using AutoEncoders h2 Introduction h1 Fraud Detection using Semi Supervised Learning ", "stargazers_count": 0, "forks_count": 0, "description": "T SNE t Distributed Stochastic Neighbor Embedding is a dataset decomposition technique which reduced the dimentions of data and produces only top n components with maximum information. AutoEncoders to the rescue What are Autoencoders Autoencoders are a special type of neural network architectures in which the output is same as the input. In this technique the model aims to find the most relevant patterns in the data or the segments of data. Visualize Fraud and NonFraud Transactions Let s visualize the nature of fraud and non fraud transactions using T SNE. Semi Supervised Classification using AutoEncoders IntroductionBy definition machine learning can be defined as a complex process of learning the best possible and most relevant patterns relationships or associations from a dataset which can be used to predict the outcomes on unseen data. This can be accessed by the weights of the trained model. Let s look at the distribution of target. Consider only 1000 rows of non fraud cases 2. com arthurtok introduction to ensembling stacking in python Feature Engineering Credits 1. The dataset consists of 28 anonymized variables 1 amount variable 1 time variable and 1 target variable Class. I will use the popular titanic dataset for this purpose. a dataset in which the target variable is known. Now we dont need any complex model to classify this even the simpler models can be used to predict. In this kernel I have explained how to perform classification task using semi supervised learning approach. We will create another network containing sequential layers and we will only add the trained weights till the third layer where latent representation exists. Unsupervised Learning is a process of training a machine learning model on a dataset in which target variable is not known. png We will create an autoencoder model in which we only show the model non fraud cases. Dataset Preparation 2. Fraud Detection using Semi Supervised Learning I am using the dataset of Credit Card Fraud Detection https www. Create a network with one input layer and one output layer having identical dimentions ie. If you liked it please upvote. 17 cases are fraud transactions. Supervised Learning is a process of training a machine learning model on a labelled dataset ie. More about Autoencoders If you want to gain more understanding about autoencoders you can refer to the following kernel https www. But the advantage of the representation learning approach is that it is still able to handle such imbalance nature of the problems. The two axis are the components extracted by tsne. com shivamb how autoencoders work intro and usecases https i. An autoencoder is a regression task where the network is asked to predict its input in other words model the identity function. These low level features are then deformed back to project the actual data. Examples of supervised learning are classification regression and forecasting. Create the model architecture by compiling input layer and output layers. One of the biggest challenge of this problem is that the target is highly imbalanced as only 0. Additionally We do not need to run this model for a large number of epochs. Broadly their exists three different machine learning processes 1. We will use keras package. Visualize Fraud Vs Non Fraud Transactions 3. The same model will be used to generate the representations of fraud cases and we expect them to be different from non fraud ones. Performing Some Feature Engineering Used in this Competition. These networks has a tight bottleneck of a few neurons in the middle forcing them to create effective representations that compress the input into a low dimensional code that can be used by the decoder to reproduce the original input. A number of kagglers have shared different approaches such as dataset balancing anomaly detection boosting models deep learning etc but this approach is different. Examples of unsupervised learning are clustering segmentations dimensionality reduction etc. Non Fraud transactions are represented as Green while Fraud transactions are represented as Red. We will use only 2000 rows of non fraud cases to train the autoencoder. The beauty of this approach is that we do not need too many samples of data for learning the good representations. Later I am also applying the same technique on Titanic https www. com arthurtok introduction to ensembling stacking in python2. Dataset PreparationFirst we will load all the required libraries and load the dataset using pandas dataframe. For our use case let s take only about 1000 rows of non fraud transactions. Obtain the Latent Representations Now the model is trained. The following Feature Engineering Code is derived from this kernel https www. With more data one can definately expect improvements. To distinguish these characteristics we need to show the autoencoders only one class of data. com mlg ulb creditcardfraud by ULB machine learning group. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. AutoEncoders Latent Representation Extraction 4. From the above graph we can observe that there are many non_fraud transactions which are very close to fraud transactions thus are difficult to accurately classify from a model. Obtain the Latent Representations 5. com sinakhorami titanic titanic best working classifierNext define the autoencoder modelTrain the modelObtain the Hidden RepresentationTrain the classifier Thus we can see that approach gives a decent results. What a perfect graph we can observe that now fraud and non fraud transactions are pretty visibile and are linearly separable. We are intereseted in obtaining latent representation of the input learned by the model. This is because the autoencoder will try to learn only one class and automaticlly distinuish the other class. In this technique the model aims to find the relationships among the independent and dependent variable. Visualize the latent representations Fraud Vs Non FraudNow we will create a training dataset using the latent representations obtained and let s visualize the nature of fraud vs non fraud cases. Applying to a different dataset Titanic Let s Apply this approach to another dataset. This approach makes use of autoencoders to learn the representation of the data then a simple linear classifier is trained to classify the dataset into respective classes. In this approach the properties of unspervised learning are used to learn the best possible representation of data and the properties of supervised learning are used to learn the relationships in the representations which are then used to make predictions. Generate the hidden representations of two classes non fraud and fraud by predicting the raw inputs using the above model. Explanation The choice of small samples from the original dataset is based on the intuition that one class characteristics non fraud will differ from that of the other fraud. com c titanic dataset. Visualize Latent Representations Fraud vs Non Fraud 6. Simple Linear Classifier 7. Here is the before and after view of Fraud and Non Fraud transactions. Also add the optimizer and loss function I am using adadelta as the optimizer and mse as the loss function. Now we can just train a simple linear classifier on the dataset. Semi Supervised Learning is combination of supervised and unsupervised learning processes in which the unlabelled data is used for training a model as well. The model will try to learn the best representation of non fraud cases. the shape of non fraud cases. Before training let s perform min max scaling. Applying the same technique on Titanic Dataset 1. Every dot in the following represents a transaction. ", "id": "shivamb/semi-supervised-classification-using-autoencoders", "size": "8230", "language": "python", "html_url": "https://www.kaggle.com/code/shivamb/semi-supervised-classification-using-autoencoders", "git_url": "https://www.kaggle.com/code/shivamb/semi-supervised-classification-using-autoencoders", "script": "classification_report keras.layers keras.models train_test_split IPython.display keras accuracy_score numpy Image seaborn preprocessing sklearn.manifold Dense regularizers tsne_plot sklearn.linear_model sklearn matplotlib.pyplot Sequential sklearn.model_selection pandas LogisticRegression Model Input display HTML get_title TSNE sklearn.metrics ", "entities": "(('target variable', 'which'), 'be') (('linear then simple classifier', 'respective classes'), 'train') (('dataset', '28 anonymized variables'), 'consist') (('it', 'problems'), 'be') (('one', 'definately improvements'), 'expect') (('model', 'independent variable'), 'aim') (('target variable', 'which'), 'dataset') (('Examples', 'supervised learning'), 'be') (('Fraud transactions', 'Red'), 'represent') (('decomposition dataset which', 'maximum information'), 'be') (('I', 'learning supervised approach'), 'explain') (('approach', 'decent results'), 'com') (('model', 'data'), 'aim') (('s', 'target'), 'let') (('approach', 'deep etc'), 'share') (('we', 'data'), 'distinguish') (('even simpler models', 'complex model'), 'need') (('Additionally We', 'epochs'), 'need') (('which', 'unseen data'), 'define') (('We', 'autoencoder'), 'use') (('that', 'original input'), 'have') (('autoencoder', 'automaticlly other class'), 'be') (('unlabelled data', 'model'), 'be') (('you', 'kernel https following www'), 'More') (('we', 'non fraud only model cases'), 'create') (('input', 'identity function'), 'be') (('We', 'model'), 'interesete') (('Examples', 'segmentations dimensionality reduction'), 'cluster') (('I', 'Credit Card Fraud Detection https www'), 'detection') (('This', 'trained model'), 'access') (('class non one characteristics fraud', 'other fraud'), 'explanation') (('target', 'highly only 0'), 'be') (('s', 'fraud T non SNE'), 'let') (('com how autoencoders', 'https i.'), 'shivamb') (('s', 'non fraud transactions'), 'let') (('Supervised Learning', 'dataset labelled ie'), 'be') (('them', 'non fraud ones'), 'use') (('Now model', 'Latent Representations'), 'train') (('output', 'input'), 'autoencoder') (('model', 'non fraud cases'), 'try') (('two axis', 'tsne'), 'be') (('Later I', 'https Titanic www'), 'apply') (('non_fraud many which', 'thus accurately model'), 'observe') (('level low features', 'then back actual data'), 'deform') (('s', 'non fraud cases'), 'visualize') (('s', 'dataset'), 'apply') (('Here before', 'Non Fraud transactions'), 'be') (('I', 'loss function'), 'add') (('Now we', 'dataset'), 'train') (('Dataset PreparationFirst we', 'pandas dataframe'), 'load') (('I', 'purpose'), 'use') (('s', 'scaling'), 'let') (('Autoencoders', 'input data'), 'train') (('we', 'good representations'), 'be') (('dot', 'transaction'), 'represent') (('which', 'then predictions'), 'use') (('Feature Engineering following Code', 'kernel https www'), 'derive') (('where latent representation', 'third layer'), 'create') "}