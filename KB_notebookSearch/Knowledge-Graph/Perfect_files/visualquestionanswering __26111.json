{"name": "visualquestionanswering ", "full_name": " h1 Visual Question Answering ML Model on CLEVR dataset h1 Related work h1 Solution h1 Import libraries h1 Loading the data from the JSON files h1 Text encoder h1 Image recognition with Machine Learning h1 What is a pixel exactly h1 Image Type h1 Normalization image in Python h1 Tensorlfow s Neural Network Convolution h1 Activation function h1 What is padding and why do we need it h1 Conv layers might induce spatial hierarchy h1 Padding avoids the loss of spatial dimensions h1 Types of padding h1 Constant padding h1 Reflective padding h1 Replication padding symmetric padding h1 Feature Map h1 Strides h1 Data Set tensorflow h1 Convolutional neural network architecture h1 Architecture mobilenet v2 h1 Creating the Model h1 Training the model on the prepared data ", "stargazers_count": 0, "forks_count": 0, "description": "If alpha 1 default number of filters from the paper are used at each layer. The below example shows various convolution image after applying different types of filters Kernels. The channels argument represents the number of integers per pixel. If you would do sowith a Conv layer this would become problematic as you d reduce the size of your feature maps and hence would produce outputs unequal in size to your inputs. png What you see on the left is an RGB input image width W height H and three channels. Constant paddingA type of padding that really resembles same padding is constant padding. The first part however serves as a feature extraction mechanism it transforms the original inputs into bits of information which ensures that the Dense layers perform better. In this article we would like to validate is whether the operation on highlevel and abstract facts extracted from the image might improve the accuracy of the system in a neutural network system. The library I will use is TensorFlow 2. The architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters followed by 19 residual bottleneck layers described in the Table 2. However rather than zeros which is what same padding does constant padding allows you to pad with a user specified constant value that will be described in the image 5 and in the code 1 the implemention with tensorflow image. Object Based Reasoning in VQA. We start with the question and answer consisting of several words image. decode_image supports GIF decoding which results in an output shape of num_frames height width channels. png image 8 Architecture mobilenet_v2MobileNetV2 is a general architecture and can be used for multiple use cases. Hence optimization a ConvNet involves computing a loss value for the model and subsequently using an optimizer to change the weights. What is padding and why do we need it Let s first take a look at what padding is. For example this is the case when you re training an autoencoder. To deployment architecture propose in this architecture we use a tensorflow and panda that will be describe below. Recent advances indicate that using high level abstract facts extracted from the inputs might facilitate reasoning. This occurs due to the fact that the feature map slides over the input and computes the element wise multiplications but is too large in order to inspect the edges of the input. 0 proportionally increases the number of filters in each layer. As the solutions inherently require to combine visual and natural language processing with abstract reasoning the problem is considered as AI complete. Here you ve got one although it s very generic image. Sometimes though you don t want your input to become smaller in the case of an autoencoder for example where you just want to converge the feature maps into one Sigmoid activated output. Lake Tahoe NV IEEE 2018. We normally represent a pixel as a single integer or multiple integers. On the other hand it would be unwise to interpret a colored image using grayscale pixels since the pixels won t be able to capture any of the actual colors. This is why convolutional layers are known as feature extractors. The size of the filter affects the encoded output volume so stride is often set to a whole integer rather than a fraction or decimal. For each dimension D of input paddings D 0 indicates how many values to add before the contents of tensor in that dimension and paddings D 1 indicates how many values to add after the contents of tensor in that dimension. As in VQA there are two distinct input modalities image and text which makes this problem similar to the problems found in other multi modal domains. the height and width of the image. The decoding function that we use depends on the format of the image. MobileNets support any input size greater than 32 x 32 with larger image sizes offering better performance. Visual Question Answering ML Model on CLEVR dataset Visual Question Answering VQA is a novel problem domain where multi modal inputs must be processed in order to solve the task given in the form of a natural language. png Image 10Convolution of an image with different filters can perform operations such as edge detection blur and sharpen by applying filters. Fully connected layer applies weights over the input generated by the feature analysis to predict an accurate label. io post convolution in autoregressive neural networks. So for row 1 with 3 5 1 this will be 1 5 3 being added. Fully connected output layer generates the final probabilities to determine a class for the image. Pooling layer downsampling scales down the amount of information the convolutional layer generated for each feature and maintains the most essential information the process of the convolutional and pooling layers usually repeats several times. Deep Learning with Python. Naturally as the stride or movement is increased the resulting output will be smaller. png Image 11 StridesStride is a component of convolutional neural networks or neural networks tuned for the compression of images and video data. Data normalization is used in machine learning to make model training less sensitive to the scale of features. How a Convolutional Neural Network Works The CNN layers A CNN is composed of several kinds of layers Convolutional layer creates a feature map to predict the class probabilities for each feature by applying a filter that scans the whole image few pixels at a time. It should have exactly 3 inputs channels and width and height should be no smaller than 32. For example we could interpret a black and white image with RGB pixel values but it is more efficient to view it as a grayscale image 3x fewer integers used. png image 4 Types of paddingThis operation pads a tensor according to the paddings you specify. Stride is a parameter that works in conjunction with padding the feature that adds blank or empty pixels to the frame of the image to allow for a minimized reduction of size in the output layer. png image 12 Data Set tensorflowfrom_tensors combines the input and returns a dataset with a single element that will be implement below from_tensor_slices creates a dataset with a separate element for each row of the input tensor that will be implement below Convolutional neural network architectureA Convolutional Neural Network CNN is a deep learning algorithm that can recognize and classify features in images for computer vision. The architecture and modelling this implementation is represent on the figure 1. png image 7 Replication padding symmetric padding Replication padding looks like reflection padding but you simply take a copy and mirror it. Multimodal Compact Bilinear pooling MCB method that performed joint embedding of visual and text features or Relational Networks RN where embedded question was concatenated with features extracted from pairs of image regions enabling the system to reason about the relation between objects being present in those regions. As you can see since we only pad 2 elements in width there are 1 and 5 but 3 falls off the padding. It extends the Encoder Decoder architecture which originally consisted of two RNNs the first one used for encodinga sequence of input symbols into a fixed length representation and the other for decoding that representation into another sequence of output symbols. Activation function In artificial neural networks the activation function of a node defines the output of that node given an input or set of inputs. Roughly it is a way of increasing the size of an image to counteract the fact that stride reduces the size. MobileNetV2 is very similar to the original MobileNet except that it uses inverted residual blocks with bottlenecking features. alpha Controls the width of the network. Image recognition with Machine Learning In this part of article I will focus on image processing specifically how we can convert images from JPEG or PNG files to usable data for our neural networks. Finally we pass the encoded words one by one as inputs to the LSTM to produce a list of encoded output image. How does Stride work Imagine a convolutional neural network is taking an image and analyzing the content. It has a drastically lower parameter count than the original MobileNet. This is known as the width multiplier in the MobileNet paper. A standard integrated circuit can be seen as a digital network of activation functions that can be ON 1 or OFF 0 depending on input. RGBA An extension of RGB with an added alpha field. Globalvectors for word representation. Now this is very nice but how do we get from input whether image or feature map to a feature map This is through kernels or filters actually. Retrieved from https pytorch. Tensorlfow s Neural Network ConvolutionTraining Convolutional Neural Networks means that your network is composed of two separate parts most of the times. Retrieved from https keras. Figure 1 General architecture of the proposed system image. png Source DESTAIn the next cell we prepare a vocabulary set for questions and answers present in the dataset it will be used to create an encoder conforme describe above. In the image 3 you have a schematic representation of a substantial hierarchy and a less substantial one which is often considered to be less efficient image. The alpha field represents the opacity of an image and in this Lab we ll represent a pixel s alpha value as an integer from 0 255 with 0 being fully transparent and 255 being fully opaque. Hence this layer is likely the first layer in your model in any other scenario you d have feature maps as the input to your layer. Depending on the use case it can use different input layer size and different width factors. What is the next value Simple you copy the entire row mirror it and start adding it as padding values horizontally. Now in order to find out about how padding works we need to study the internals of a convolutional layer first. Possibly this occurs because of how zero based padding i. Each pixel is made up of 3 integers between 0 255 where the integers represent the intensity of red green and blue respectively for the pixel. This in return effectively means that a spatial hierarchy is created the more one moves towards the right when inspecting the model architecture the smaller the inputs and hence feature maps become. The parameters will be list below input_shape Optional shape tuple only to be specified if include_top is False otherwise the input shape has to be 224 224 3 with channels_last data format or 3 224 224 with channels_first data format. The neurons within a CNN are split into a three dimensional structure with each set of neurons analyzing a small region or feature of the image. These weights are equal to the weights of a classic neural network but are structured in a different way. png image 2 As it cannot capture the edges it won t be able to effectively end at the final position of your row resulting in a smaller output width and or height. Now when they slide over the input from left to right horizontally then moving down vertically after a row has been fully captured they perform element wise multiplications between what s currently under investigation within the input data and the weights present within the filter. CNNs use the predictions from the layers to produce a final output that presents a vector of probability scores to represent the likelihood that a specific feature belongs to a certain class. decoding any image format we use tensorflow. Padding and stride are the foundational parameters of any convolutional neural network. Exemplary approaches developed for the VQA problem domain include e. Stride is a parameter of the neural network s filter that modifies the amount of movement over the image or video. The image 8 ilustrate the process. For example the projection of user and item embeddings into the common representation space in neural recommender systems. Applying it effectively adds space around your input data or your feature map or more precisely extra rows and columns with some instantiation Chollet 2017. png Image 9Then the convolution of 5 x 5 image matrix multiplies with 3 x 3 filter matrix which is called Feature Map that represent in image 10. io layers convolutional PyTorch. Convolutions in Autoregressive Neural Networks. To store the information will be load by json files will be use tow arrays that will be transform by panda. png image 6 Reflective paddingReflective padding seems to improve the empirical performance of your model. For PNG images we re also allowed to set channels to 4 corresponding to RGBA images. We use ReLU6 as the non linearity because of its robustness when used with low precision computation. Now what is a feature map That s the yellow block in the image. Neural computation 9 8 1735 1780 1997 J. Let s first see the implementation in Python using a image of clever database. More specifically our ConvNet because that s where you ll apply padding pretty much all of time time. For generic decoding i. These filters you configure some number N per convolutional layer slide strictly convolve over your input data and have the same number of channel dimensions as your input data but have much smaller widths and heights. This allows different width models to reduce the number of multiply adds and thereby reduce inference cost on mobile devices. You need the output images to be of the same size as the input yet need an activation function like e. This is achieved by element wise multiplications between the slice of input data the filter is currently hovering over and the weights present within the filter. When the input file is an image the output of tensorflow. png image 3 Padding avoids the loss of spatial dimensionsSometimes however you need to apply filters of a fixed size but you don t want to lose width and or height dimensions in your feature maps. In the image 7 you have the example of reflection padding on a image. Pixels take a specific form based on the interpretation of the image which is usually one of the following Grayscale Viewing the image as shades of black and white. 200 200 3 would be one valid value. Pooling layer downsampling scales down the amount of information the convolutional layer generated for each feature and maintains the most essential information the process of the convolutional and pooling layers usually repeats several times. Loading the data from the JSON files Loading the json files using JSON module and using pandas to create a dataframe consisting of the path to the images and respective questions and answers to the images. If the filter size is 3x3 pixels the contained nine pixels will be converted down to 1 pixel in the output layer. In 2018 IEEE Winter Conference on Applications of Computer Vision WACV 1814 23. Setting channels to 2 is invalid. The last part of your network which often contains densely connected layers but doesn t have to generates a classification or regresses a value based on the inputs received by the first Dense layer. Related work Research on VQA has resulted in many interesting solutions. paddings is an integer tensor with shape n 2 where n is the rank of tensor. png image 5In the image 6 have the exemple in the transformation of image. The principal archicture of mobilenetve is the Convolutional Neural Network CNN architecture is inspired by the organization and functionality of the visual cortex and designed to mimic the connectivity pattern of neurons within the human brain. We want the original output and the original output only. It is a multi layer neural network designed to analyze visual inputs and perform tasks such as image classification segmentThere are two main parts to a CNN A convolution tool that splits the various features of the image for analysisA fully connected layer that uses the output of the convolution layer to predict the best description for the image action and object detection which can be useful for autonomous vehicles. Retrieved from https theblog. Convolutional Layers. We can change the pixel format of the decoded image via the channels keyword argument. the same padding and constant based padding alter the distribution of your dataset. This can be achieved with the padding mechanism which is precisely what we ll cover in this problem. Fully connected input layer flattens the outputs generated by previous layers to turn them into a single vector that can be used as an input for the next layer. Image Type Now that have learn how to load an image it is time to decode the image data into pixel data using TensorFlow. In the image 8 you have a example with images and below you have the code to implemation. Import libraries First of all need to import the components and verify the gpu process. Each pixel is an integer between 0 255 where 0 is completely black and 255 is completely white. decode_image but if the input is a JPEG image we use tensorflow. read_file takes the file name as its required argument and returns the contents of the file as a tensor with type tensorflow. From this it gets clear straight away why we might need it for training our neural network. In the images RGB have a pixel is made up of 3 integers between 0 255 to normalizate the values of image i will share the matriz witt 255. Fully connected output layer generates the final probabilities to determine a class for the image. 0 as it provides a variety of utility functions to obtain image data from files resize the images and even transform a large set of images all at once. Long short term memory. By altering the image size and alpha parameter all 22 models from the paper can be built with ImageNet weights provided. The default value for channels is 0 which means the decoding function uses the interpretation specified from the raw data. For example if a neural network s stride is set to 1 the filter will move one pixel or unit at a time. The function tensorflow. Padding helps you solve this problem. Another reason is that tensorflow. The main models implemented in the tensorflow will be described below. png According Hochreiter where W denotes the number of words constituting a given question and answer that we use GloVe Global Vectors for Word Representation word embedding model to encode question words. Conv layers might induce spatial hierarchyIf the width and or height of your kernels is 1 you ll see that the width and height of the feature map being output gets smaller. It s a collection of N one dimensional maps that each represent a particular feature that the model has spotted within the image. Here the outcome can be the same the output will have the same shape as the input. read_file will be the raw byte data of the image file. What is a pixel exactly A pixel is essentially just a point on an image with a specific shade color and or opacity. We always use kernel size 3 3 as is standardfor modern networks and utilize dropout and batch normalization during training. Following that direction we decided to develop a solution combining state of the art object detection and reasoning modules. For example for the scenario above a filter may be 3 x 3 pixels wide and high but always has 3 channels as our input has 3 channels too. Fully connected input layer flattens the outputs generated by previous layers to turn them into a single vector that can be used as an input for the next layer. org docs stable _modules torch nn modules padding. This allows our model to converge to better weights and in turn leads to a more accurate model. This is illustrated in the image 2 where the red position is impossible to take and the green one is part of the path of the convolution operation. Typically Convolutional layers are used as feature extractors. Like this You re at the first row again at the right. png Image 8 Feature Map Consider a image 5 x 5 whose image pixel values are 0 1 and filter matrix 3 x 3 as shown in the image 9. decode_image when we also need to resize the image with tensorflow. Setting channels to 1 specifies a grayscale image while setting channels to 3 specifies an RGB image. Through these weights as you may guess the model learns to detect the presence of particular features which once again are represented by the feature maps. In Proceedings of the 2014conference on empirical methods in natural language processing EMNLP pages 1532 1543 2014. Sigmoid in order to generate them. Adding the extra space now allows us to capture the position we previously couldn t capture and allows us to detect features in the edges of your input this desmontrated on the image 4. png attachment image. Text encoderThe diagram for question processing is shown in Fig. input_shape will be ignored if the input_tensor is provided. The number of parameters and number of multiply adds can be modified by using the alpha parameter which increases decreases the number of filters in each layer. Normalization image in Python Normalization refers to rescaling real valued numeric attributes into a 00 to 1 range. Those however could not have been developed without the existence of proper datasets and metrics for evaluation and comparison of the results. We call this a spatial hierarchy. The height and width are based on number of pixels. Indeed convolutional layers may cause a hierarchy like flow of data through the model. The efforts in joint embedding focus on the methods for combining multi modal representations. Since the function can return data with different shapes we can t use tensorflow. If you ve ever looked at an image file s properties before it ll show the dimensions of the image i. That s not what we want when we create an autoencoder. Below you have the implemation of this feature. For example if the dimensions of an image are 400x300 width x height then the total number of pixels in the image is 120000. png Figure 2 Dataflow diagram of question encoding image. Through optimization these layers learn kernels which slide or convolve over the input data generating a number of feature maps that can subsequently be used for detecting certain patterns in the data. Creating the Model Training the model on the prepared dataReferences Desta Mikyas T. We can choose to interpret an image however we want but there is usually one interpretation that is optimal. By consequence the system as a whole allows you to feed it raw inputs which are processed internally while you get a probability distribution over a set of classes in return. This closes the circle with respect to how a convolutional layer works. Larry Chen e Tomasz Kornuta. RGB The default interpretation for color images. There are two main parts to a CNN A convolution tool that splits the various features of the image for analysis A fully connected layer that uses the output of the convolution layer to predict the best description for the image Basic Convolutional Neural Network Architecture CNN architecture is inspired by the organization and functionality of the visual cortex and designed to mimic the connectivity pattern of neurons within the human brain. In other words each group of neurons specializes in identifying one part of the image. New York NY Manning Publications. There are two main parts to a CNN A convolution tool that splits the various features of the image for analysis A fully connected layer that uses the output of the convolution layer to predict the best description for the image. Fully connected layer applies weights over the input generated by the feature analysis to predict an accurate label. png Source Dased on DESTA 2018 Solution The Architecture that is used on this post are presented in Fig. Although the raw byte output represents the image s pixel data it cannot be used directly. ", "id": "marcelosabaris/visualquestionanswering", "size": "26111", "language": "python", "html_url": "https://www.kaggle.com/code/marcelosabaris/visualquestionanswering", "git_url": "https://www.kaggle.com/code/marcelosabaris/visualquestionanswering", "script": "scheduler tensorflow Path FormatarEndereco pathlib tensorflow_datasets matplotlib.pyplot preprocess encode_fn create_pipeline pandas numpy ", "entities": "(('alpha 22 models', 'ImageNet weights'), 'build') (('Hence layer', 'layer'), 'be') (('We', 'words several image'), 'start') (('network', 'times'), 'mean') (('process', 'convolutional layers'), 'repeat') (('we', 'RGBA images'), 'allow') (('the smaller inputs', 'model when architecture'), 'mean') (('height then total number', 'image'), 'be') (('specifically how we', 'neural networks'), 'recognition') (('that', 'time'), 'work') (('that', 'tensorflow image'), 'allow') (('you', 'implemation'), 'have') (('width', 'feature map'), 'induce') (('Architecture 8 mobilenet_v2MobileNetV2', 'use general multiple cases'), 'be') (('model training', 'features'), 'use') (('which', 'output symbols'), 'extend') (('activation function', 'inputs'), 'function') (('which', 'feature once again maps'), 'learn') (('increases', 'layer'), 'modify') (('it', 'image'), 'look') (('Typically Convolutional layers', 'feature extractors'), 'use') (('feature That', 'yellow image'), 'be') (('We', 'channels keyword argument'), 'change') (('Import libraries', 'gpu process'), 'need') (('height', '32'), 'have') (('architecture', 'figure'), 'represent') (('decoding function', 'raw data'), 'be') (('model', 'image'), 's') (('stride', 'size'), 'be') (('architecture', 'Table'), 'contain') (('Dense layers', 'information'), 'serve') (('This', 'kernels'), 'be') (('green one', 'convolution operation'), 'illustrate') (('default alpha 1 number', 'layer'), 'use') (('0', 'layer'), 'increase') (('deep learning that', 'computer vision'), 'combine') (('7 you', 'image'), 'have') (('output', 'input'), 'be') (('when you', 'autoencoder'), 'be') (('it', 'input layer different size'), 'use') (('height', 'pixels'), 'base') (('pixels', 'actual colors'), 'be') (('paddings 1 how many values', 'dimension'), 'indicate') (('Padding', 'neural foundational convolutional network'), 'be') (('MobileNets', 'better performance'), 'support') (('it', 'encoder conforme'), 'DESTAIn') (('we', 'different shapes'), 'return') (('Hence ConvNet', 'weights'), 'optimization') (('that', 'next layer'), 'flatten') (('Simple you', 'values'), 'be') (('below example', 'filters Kernels'), 'show') (('work Related Research', 'many interesting solutions'), 'result') (('how convolutional layer', 'respect'), 'close') (('output Fully connected layer', 'image'), 'generate') (('neurons', 'image'), 'split') (('that', 'tensorflow'), 'propose') (('read_file', 'byte image raw file'), 'be') (('Normalization image', '00 to 1 range'), 'refer') (('we', 'problem'), 'achieve') (('see', 'left'), 'png') (('n 2 where n', 'tensor'), 'be') (('convolutional neural network', 'content'), 'work') (('It', 'original MobileNet'), 'have') (('specific feature', 'certain class'), 'use') (('This', 'MobileNet paper'), 'know') (('Indeed convolutional layers', 'model'), 'cause') (('IEEE Winter Conference', 'Computer Vision'), 'WACV') (('padding how we', 'convolutional layer'), 'need') (('exactly pixel', 'shade specific color'), 'be') (('width different models', 'mobile devices'), 'allow') (('s', 'first look'), 'pad') (('which', 'black'), 'take') (('Exemplary approaches', 'e.'), 'include') (('where you', 'time time'), 'ConvNet') (('You', 'again right'), 're') (('where you', 'one Sigmoid activated output'), 'sometimes') (('group', 'image'), 'specialize') (('We', 'single integer'), 'represent') (('3', 'padding'), 'be') (('Applying', 'feature precisely extra instantiation'), 'add') (('We', 'original output'), 'want') (('when we', 'autoencoder'), 's') (('channels argument', 'pixel'), 'represent') (('problem', 'modal other multi domains'), 'be') (('you d', 'inputs'), 'sowith') (('fully 255', '0'), 'represent') (('that', '0 input'), 'see') (('we', 'image'), 'depend') (('it', 'bottlenecking features'), 'be') (('Finally we', 'output encoded image'), 'pass') (('png image', 'image'), 'image') (('Possibly this', 'padding how zero based i.'), 'occur') (('completely 255', '0'), 'be') (('png 11 StridesStride', 'images'), 'Image') (('that', 'Fig'), 'dase') (('slide', 'much smaller widths'), 'configure') (('which', 'autonomous vehicles'), 'be') (('input When file', 'tensorflow'), 'output') (('straight away why we', 'neural network'), 'get') (('what', 'present filter'), 'capture') (('you', 'paddings'), 'pad') (('which', 'height num_frames width channels'), 'support') (('Text encoderThe diagram', 'Fig'), 'show') (('filter currently weights', 'filter'), 'achieve') (('usually one that', 'image'), 'want') (('which', 'Dense first layer'), 'have') (('input', '3 channels'), 'be') (('png image 6 Reflective paddingReflective padding', 'model'), 'seem') (('we', 'question words'), 'png') (('so stride', 'often whole integer'), 'affect') (('when we', 'tensorflow'), 'decode_image') (('efforts', 'modal multi representations'), 'focus') (('that', 'data'), 'learn') (('3x fewer integers', 'grayscale image'), 'interpret') (('read_file', 'type tensorflow'), 'take') (('we', 'image format'), 'decode') (('Fully connected layer', 'accurate label'), 'apply') (('it', 'output smaller width'), 'image') (('we', 'modules'), 'decide') (('objects', 'regions'), 'Bilinear') (('input', 'e.'), 'need') (('Below you', 'feature'), 'have') (('feature map', 'input'), 'occur') (('that', 'output layer'), 'be') (('i', 'matriz witt'), 'have') (('less substantial which', 'substantial hierarchy'), 'have') (('same padding based padding', 'dataset'), 'alter') (('Feature that', 'image'), '9Then') (('Those', 'results'), 'develop') (('main models', 'tensorflow'), 'describe') (('png Image', 'filters'), 'perform') (('it', 'pixel data'), 'represent') (('that', 'image'), 'be') (('filter', 'time'), 'move') (('image pixel 5 values', 'filter 1 3 3 image'), 'Consider') (('using', 'reasoning'), 'indicate') (('you', 'feature maps'), 'avoid') (('it', 'images'), 'resize') (('this', 'image'), 'allow') (('255 where integers', 'respectively pixel'), 'make') (('We', 'training'), 'use') (('operation', 'network neutural system'), 'be') (('model', 'more accurate model'), 'allow') (('it', 'TensorFlow'), 'type') (('that', 'really same padding'), 'be') (('3x3 contained nine pixels', 'output layer'), 'be') (('AI', 'abstract reasoning'), 'consider') (('problem modal novel where multi inputs', 'natural language'), 'Model') (('weights', 'different way'), 'be') (('input otherwise shape', 'channels_first data 3 224 format'), 'have') (('internally you', 'return'), 'allow') (('it', 've one'), 'get') (('We', 'precision when low computation'), 'use') (('you', 'it'), 'look') (('first implementation', 'clever database'), 'let') (('Basic Convolutional Neural Network Architecture CNN architecture', 'human brain'), 'be') (('Convolutional Neural Network CNN architecture', 'human brain'), 'be') (('why convolutional layers', 'feature extractors'), 'be') (('that', 'panda'), 'be') (('1', '5 1 3 1'), 'be') "}