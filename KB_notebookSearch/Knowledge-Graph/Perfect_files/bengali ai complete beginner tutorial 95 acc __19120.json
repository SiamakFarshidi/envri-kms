{"name": "bengali ai complete beginner tutorial 95 acc ", "full_name": " h1 NOTE IF you are looking at this like a beginner s tutorial please read every line carefully Download and edit for better view h3 While going through tutorials we all have been there getting bored and skipping lines just to get to the core of it includig me but lees we hat the core is in the lines I ll try my best to provide as much information as I can about the code and process because this is what I want to learn and how I have learned what is this line of code doing why this parameter how is this happening why did we do this and all the waves of questions that arise from the very process of implementation of a code be it a simple Hello World or a very Deep Network h2 Another Note h4 I just assume that you know and have practiced a bit of if you are here I m sure you know it all in Python 3 Numpy jupyter lab Pandas and basics of Machine Learning sklearn Deep Learning Neural Networks Convolution Neural Networks keras If not just in case I highly recommend learning these in the order h3 Last Important Note h4 This might not be the most elegent notebook and efficient code out there but WE as a beginners care less about the efficiency but the leraning curve I myself have self studued all the concepts by MYSELF withthe help of this online coding community so here is a little effort for that coomunity and the Data Science enthusiasts yet to come If you see something that you can not understand in a loop method just implement it pieces by pieces yourself You ll develop the core understanding of the libraries as well as the process of how to do it If I can save one fellow coder a trouble I de be repaying my debt to the community Please feel free to correct comment and contact me if you see anything wrong or something that can be replaced with something better as I am a learner myslf too h2 The Problem h2 Solution h2 Import Libraries h2 Files Paths Outputs h2 Importing Images h2 Labels info h2 Plotting h1 Model h3 NOTE h2 Custom metric h2 Compiling h2 Callbacks h2 Helper Functions h3 Resize h3 Generator h2 Training h2 Garbage Collection h2 Predictions and Submissions h2 What next h2 What Next h3 Thank you all for coping with my gramatical errors ", "stargazers_count": 0, "forks_count": 0, "description": "So Check the the data under your commit histories and you can use the data of that kernal for a new kernal. NOTE Before you go on searching and wondering about why these many or so less layers why these many numbers of neurons why droput is set to be at this fraction why normalization and why why not s of ocean these are just trial and error parameter tunings and testing with permutations which cost us very less computational power and time. That is why we had ordered_labels and key_lengths It ll extract the elements ordering vise else there will be conflict just a random dataframe nothing much columns to be dropped after merging with dataframe so that we can have only numerical pixel values calling garbage collector to free up memory from last delete Pandas automically assigns the correct columns by looking at the values in image_id so we don t have to worry about alignment if the image_id in train_classes are correct to the respect classes get the numerical values of classes and convert them to categorical so that 0 1 2 becomes 1 0 0 0 1 0 0 0 1 where the size of the later depends on the maximum number that is present in the original matrix. See all the important description about classes. If you have queries regarding what and why about this Layer model you can see the stackoverflow answer https stackoverflow. We need custom metrices because it is the problem s demand. shape 0 in our case split the data in training and test to cross validate our model s performance. And some amazing visual explanations of Working of Gradients Significance of Eignveectors and EignValues and why and how are they used How Convolution Pooling ACTUALLY works and so on. While going through tutorials we all have been there getting bored and skipping lines just to get to the core of it includig me but lees we hat the core is in the lines. You can follow custom loss function example https stackoverflow. You can surely try different dimensions. Download and edit for better view. Importing Images DO NOT IMPORT ALL OF. First one is image_id and from the second column to last in our case it is marked as 0 32331 we have pixels of image ranging in 0 255. More epochs than required can lead model to overfitting. You have to check it for yourself and see what fits better. It ll be equal to number of elements or df. 15 will zoom in or out the image propotional to original image width_shift_range 0. padding SAME means that at each side o s are padded to each image so that the corner bits get same attention as the middle ones. In my case it has been a journey of luck. Flatten Flatten Flatten layer performs a flatten operation on a tensor reshapes the tensor to have the shape that is equal to the number of elements contained in tensor non including the batch dimension. 15 Get maximum value of every 2 by 2 matrix present in matrix it gets and return a new matrix of these values Drop weights from 30 of neurons from previous layer Convert into 1 D tensor names of output layers. random_state produces the same data every time for reproducibility so it is kind of pseudo randomness featurewise_center would have set input mean to 0 over the dataset if stated True samplewise_center would have set each sample mean to 0 if stated True featurewise_std_normalization would have divided inputs by std of the dataset if set True samplewise_std_normalization would have divided each input by its std if True rotation_range 8 will rotate the image in range 8 degrees zoom_range 0. We want model to show or do something while in training we do not want to interrupt training we use callbacks. 00001 and give us little insight on what has happened verbose 1 stop the model from fitting data if validation loss has not decreased by 0. It is simply a measure of how accurae we were in predictions. number of unique classes in each number. Sequential Model Easy to implement but not customizable and using Functional API Very customizable and relatively not so difficult to implement. Rather than computing learnable parameters and tweaking them using gradients Search for it in case. IT LL MAKE YOUR KERNAL CRASH AT ONE POINT OF TIME LATER. There is not panacea. For this very demo you can see a very good example of using muti loss multi metrics for multi output NN https github. Ex some have 2 classes red yellow but other can have 4 Audi BMW Ferrari Toyota so we have to keep track due to inner working of super. The ProblemWe have a set of almost 250000 single channel gray scale handwritten images of dimensions 137 236 converted to digital format and then converted to pixel values where pixel values range from 0 255 after exploring the dataset we ll know and saved as the 32332 columns 137 236 32332 Every color is made up of Red Green Blue 3 channels and in computers we represent these colors by numbers ranging from 0 255 where 0 represents White and 255 represents Black. We will count the total execution time of this script This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. com playlist list PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU where mazing people giving you INSIGHTS of what happens ACTUALLY INSIDE the mathemetics of something how things work what are PRACTICAL SIGNIFICANCE of formulas we use and all the things. Normalization BatchNormalization Normalize the activations of the previous layer at each batch i. Out N subplots will acquire the areas this area accordingly each one having same area first subplot of the 1 row 2 columns subplots set title of the first subplot show the image which is as index i of the input DataFrame. We are using Adam as an optimizer by defining optimizer adam. For that we ll build a Neural Network and more specifically a Convolution Network. One from the DataFrame and other one from the grapheme used in. Out of Notebook question 0 marks scored. For example if 1 3 is original array then one hot encoded array will look something like 0 0 0 0 0 1 0100 2 0 0 0 0 3 0 0 0 1 NOTE this is not the actual representatin but the values are correct. 15 will shift the width 0. The community still stands divided on this whether to delete and force collector free up memory. epoch is the iteration for which we will train the model. If you have any queries you can check the keras official github keras documentation. Sotmax activation softmax Softmax activation function gives the probability of each class. You have to compile it before you can train your model. com playlist list PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL. So we ll be using Functional API. Accuracy is the metric we are using. There is not BEST size our model processes data in the batches. With a Leaky ReLU you won t face the dead ReLU or dying ReLU problem which happens when your ReLU always have values under 0. Last Important Note This might not be the most elegent notebook and efficient code out there but WE as a beginners care less about the efficiency but the leraning curve. com channel UCYO_jab_esuFRV4b17AJtAw playlists view 50 sort dd shelf_id 20 and Fundamentals of Deep Learning and Neural Networks https www. basic info such rows columns each one s data type and memory used there are 5 columns corresponding to each image. For images we use 2D and for videos and sound 3D. Obviously we lose a bit of information but the ratio between computation memory to the performace information will be decreased greatly. It is a method of preventing Overfitting. I have nothing to get from this but I had a sudden revealation from these such as Neural Networks JUST find the patterns and we have high hopes that they ll find and to our surprise they even just do. Each image has a unique image_id and belongs to atleast one of the 3 classes. You can access flattened layers by model. For example a tensor samples 10 20 1 will be flattened to samples 10 20 1. To get a better understanding of how does all of this makes sense just visit this link https www. Categorical loss is used as the loss function. Why do we need to Good one So common that even Facebook has the answer literally. Please feel free to correct comment and contact me if you see anything wrong or something that can be replaced with something better as I am a learner myslf too. com 2017 06 01 hidden layers. Wish you all the best Thank you all for coping with my gramatical errors. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. ttf file is a font file format created by Apple but used on both Macintosh and Windows platforms. html Custom metricIn keras we can implement a custom metric as only few of the common metrices are available though you can get those metrices later too. Optimizers help in reducing the number of computations drastically. Last columns is what tell us which grapheme is used in image. In other words these layers summarize the presence of features in an input image or extract features. it searches for parameters in batches. This process goes from the output layer to input layer and is called Backpropagation. So we ll be applying the process as1. It ll stop the model from training if given parameters like accuracy is not improving or loss is not decreasing significantly. I ll try my best to provide as much information as I can about the code and process because this is what I want to learn and how I have learned what is this line of code doing why this parameter how is this happening why did we do this and all the waves of questions that arise from the very process of implementation of a code be it a simple Hello World or a very Deep Network. using LeakyRelu as a new layer. NOTE IF you are looking at this like a beginner s tutorial please read every line carefully. Overfitting is cramming in a sense. 15 points respective to total width this will JUST calculate parameters required PCA ZCA and others if True no transformations performed gives you 1 at which epoch model stopped due to early stopping labels are givesn as a dictonary parameter to the fit_generator because it is multi output problem. Position of 1 defines the presence of the elemnt in the list. just for training data these are the final dimensions of an image. e MetricToWatch_LayerName ReduceLROnPlateau will reduce the leraning rate of optimizer by a defined factor if any of the output metric average metric hits plateau. IF Dropout is set to. Put any value of i that exists in df. If you see something that you can not understand in a loop method just implement it pieces by pieces yourself. 03530 tells that a Neural Network crams the wrong data too much that it tells them correctly. com jeffheaton s research on The Number of Hidden Layers https www. So we use 137 236 specified by one who gave us the data. To get the best of your model and knowledge read more apply the most. parquet FILES AT ONCE. com profile Yoshua Bengio on Practical recommendations for gradient based training of deep architectures https arxiv. As we have a continuous values of pixels we have to convert it to the dimensions of the given input already given to us. Dropout may be implemented on any or all hidden layers in the network as well as the visible or input layer. Must read paper from Chiyuan Zhang http pluskid. If given enough time data and wrong label to a network it ll learn to classify dog as a car with 100 accuracy So if it learn to classify a dog that is in the center of image everytime facing right it won t be able to classify the same dog in corner or facing left and so on. backend For Later CompilingUnlike other frameworks like PyTorch Keras is not so pythonic in case of dynamic behaviour. First method can not perform well on or above images which are resized as 64 64 but with second we lose the element of randomness as we just have 1 4 of the original sample to choose from. I am just trying to give you the best of I have after searching through tons of tutorials and blogs. Kaggle saves the output only when you commit and can be accessed as the output of that successful comitted version. In our case the organizers have already given us the metric on which they ll be evaluating our scores so why not to use it from start when we will be judged in the end on this basis Wish we could have same for the life too. 15 means move the mean and variance by a factor of 0. Metrices is keras can be implemented using keras. fit final line of our model construction code tell the system only the start and end it ll find the path if validation loss of out_1 is not decreasing for 3 consecutive epochs decrease the learning rate by 0. com and obviously you can always follow and ask questions on Stackoverflow https stackoverflow. batch 1 single image train_classes have our respective label for every image import any random image file given in input print the first 5. Predictions and Submissions What next What Next From this point onwards you can make the models more robust and efficient using different techniques such as adding more layers until the model reaches saturation point using dimentionality reduction techniques such as PCA ISOmap to reduce the overhead by keepong the most significant data pixels and tweaking the parameters of your model. Not getting it right Well for example Flatten will take a tensor of any shape and transform it into a one dimensional tensor plus the samples dimension but keeping all values in the tensor. com questions 59603353 using multi output labels in keras imagedatagenerator flow and using model fit and custom metric in keras example https datascience. Right So we will be asking from our models to do the common work so that we can use the common works done by common the lower layers and use it in out output. You an not load all the images resize reshape train test split and fit the model. This is not a good practice though I did it just to show how to do what I ll do later in one go Labels infotrain_classe will tell us how many classes are there what are the min max classes and some other information about the Data distrubution on the data we have Plotting This function plots the images side by side. ReLu activation relu ReLu will convert every negative value to 0 and use the ive value as it is. We have 32333 columns. We could also use Relu as layer too instead of activation relu momentum 0. Let us just get the summary of layers and activation functions as they appear in our network. I myself have self studued all the concepts by MYSELF withthe help of this online coding community so here is a little effort for that coomunity and the Data Science enthusiasts yet to come. com questions 59420425 meaning and working of a function fvariable v a function followed by varia regarding this one. Not in this case Try doing it. Model For this problem we have to answer what is better Taking 3 different vehicles or sharing a common with friends when all we have to do is to go to a common place common theatre but side by side seats. collect method to force the collector but it is thought to be a bad practice to use del because it removes the reference of that object from the memory. We do not have to apply a lot of load because we have memorized and learned to memorize the patterns from billions of years of evolution and we are trying to teach this this to computers within what a hundred years or less. com watch v bwb4r3UVKko SolutionWe are trying to find the root vowel and consonent in each image that will be presented to us. flow final dictonary that ll be yielded keeps count of the ordering of the labels and their lengths Extract to from the range of length of labels values. I think the later one got your attention. Few of the videos on Keras are available here https www. com questions 45165 how to get accuracy f1 precision and recall for a keras model CallbacksCallbacks are a method of invisibly inserting our commands basically if else conditions to show stop or do something when some conditions are met. What it does is to take all the image files either from directory or in form of rank 4 metrices number_of_pictures width height channel and maps to the corresponding labels. The input layer is the very beginning of the workflow for the artificial neural network. EarlyStopping is used to avoid resource wastage basically. At each epoch same images will be shown to model and the weights and biases will be adjusted accordingly. We can pass in the images ourselves too but what this generator best is that it augments the images. com keras team keras issues 10306 We can also define our own custom metrices and loss functions. Layers in Keras are by default named as layer_type_position suc as first Conv2d and Dense are named as conn2d_1 and dense_1 unless specified by name something_else in layer. NN CNN works on finding and memorizing the patterns like we humans do but subconciously. 9 will be outputted to 0 but 1 2 76 0. If I can save one fellow coder a trouble I de be repaying my debt to the community. 5533 and Jeff Heaton https github. TrainingWe do our work in loop. Simply N D tensor collapsed into 1 D. 4 it ll randomly drop the weights 40 of neurons which were provided by the previous layer. For example if the values from the previous layers in range 100 to 100000 it ll normalize the entire batch. There are 2 methods that I could come up with. You can also use the Adam imported already and set the learning rate. It means that if validation loss is not improving or if accuracy is not improving decrease the learning rate. So to make our model robust we need augmentations. index We will delete this dataframe later and import it again. You ll develop the core understanding of the libraries as well as the process of how to do it. org on deep learning requires rethinking https arxiv. For example in case of Cancer detection we can use Recall rather than accuracy because a False Positive model says you have Cancer when you actually don t will lead to few more tests from doctors but a False Negative model says you are fit but you are not rally will lead to a delayed treatment that can be harmful. Simply it ll tell us the average difference between the classes that were actually there and those were predicted by the model and if we get a difference in the values scold our model and ask every neuron in process to tweak their output so that the resulting somehow becomes the original. read_csv plotting to plot the font see progress bar Darw picture from font Resizing of image resizng of image image augmentation on training images ONLY splitting the data for custom metrices implementations and other processes that we define keras layers Model class Call backs acts like milestones and if else while model is being trained garbage collector Input data files are available in the. Convuolution Layer Conv2D Convolutional layers are the layers where filters are applied to the original image or to other feature maps in a deep CNN. We can use any number of callbacks but we ll be using 3 in our case. 137 236 32332 pixels take more memory and computational power than that of 74 74 5476 pixels. 0025 in the last 5 epochs and restore best weights for the next time save the weights in a file name specified only if the validation loss of out_1 layer has improved from last save. It allows a small gradient when the unit is not active. Each color has it s own values in case of 3 channel images but in grayscale we just have one channel so we get B W pictures. It is not used on the output layer. 1 will be outputted as they are 1 2 76 0. This is not necessary but it is to show that dataframe has very good method for showing important values we will set the file path in arguments get the character used in glapheme set the figure size s dimensions to width 10 height 3. Repeat Garbage Collection Even though Grabage collection is done automatically in Python by the gc and in periods but we ll use the gc. If not just in case I highly recommend learning these in the order. Compilation will calculate all the required numbers of parameters to train the loss function to use assign them to each layer get a memory space from os assign accuracy metric etc. It can resize crop shift flip rotate iamges for us and so on. Classes ranges are in different. Not Fair And I can not give you a whole lot of Deep Learning in this mere notebook but before we start into the solution I highly rcommend and I repeat I HIGHLY RECOMMEND to watch these Neural Network series https www. We resize the image so that it fits in out memory and uses less computational power. One class has over 160 values but second one merely has 7 classes. Let us suppose that in each class in our problem we have 169 11 7 classes respectively for each label we get a probability the class assigned to the image will be the class with the highest probability. I can not plot a 2 D array in s remove all the columnsthat are not of use resize the images and then divide each pixel by 255 to limit the range from 0 255 to 0 1 and convet as float32 memory efficient reshape into rank 4 matrix 1 checks for the best dimension. So instead of viewing the results from the original images for EDA perform it on the grapheme column and view some results by converting characters to images using matplotlib Below is a code just to see that there is no difference betwwen the two. 1 LeakyRelu LeakyRelu Leaky version of a Rectified Linear Unit ReLu. Pooling MaxPool2D Pooling layers provide an approach to down sampling feature maps by summarizing the presence of features in patches of the feature map. Every neuron involved in producing that final value will ask every other previous value to tell teak parameters. Only if the 3 labels are independent of each other and do not affect each other s output in any sense We can build our model by 2 different styles. But for more details on this one you can read research paper from Yoshua Bengio https www. Dropout Dropout Dropout Layer will drop n proportion of values it gets from the previous layers. Import LibrariesImport all the needed libraries all at one place so that you don t have to be confused. It can be resized to any size without losing quality and looks the same when printed as it does on the screen. We have many reasons for these but the main 2 reasons in our case are computational power and memory. Sometimes I get the freed space sometimes I don t but we are forcing the garbage collector to collect and free up space for the sake of it. nothing saving nothing is like committing when offline and preparing a log if crash happens load the best weights so far so that we don t have over fitting by the time the mode lhad stopped model predictions placeholder row_id place holder. We define the significance ModelCheckPoint is used to save the whole model or just the weights if our model improves by the criteria of improvement defined. Helper Functions ResizeWe need to resize our images. We can get a very good result by losing a very little information. out_1 because it s recall matters twice empty dictonary which stores the values of pixels for each image iterate through all the images in dataframe apply resize transformations on per row basis reshape accordingly resizing swaps the rows to columns so Transpose sets to default all the labels array will be concatenated in this single array define a dict which maps the key y1 y2 etc to lengths of corresponding label_array to store the ordering in which the labels Y were passed in this class for the first time loop it s empty so insert first element concat each array of y_labels key lengths will be different for different range of classes in each class due to_categorical ONE HOT encodings. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. The moment you need some new library you come up here and import it Files Paths OutputsIf you have any problem while finding a file data and path you can find it easily on the screen printed above. NN don t even know what they found. We are using callbacks as whole but can use these on each individual output layers too as monitor val_loss_out_1 i. cmap gray depicts Black White second subplot of the 1 row 2 columns get the properties of the fonts plot the text as an image generalises the function for any df so a bit slow Input layer just takes into account of the size of the input filter size is 64 and kernal is a 3 3 matrix. Another Note I just assume that you know and have practiced a bit of if you are here I m sure you know it all in Python 3 Numpy jupyter lab Pandas and basics of Machine Learning sklearn Deep Learning Neural Networks Convolution Neural Networks keras. We need these names as they act as the keys for mapping output to each later. Neural network is no magic but it just learns the images somehow. Input Layer Input The input layer of a neural network is composed of artificial input neurons and brings the initial data into the system for further processing by subsequent layers of artificial neurons. Import data resize merge in 4 iterations and then train the whole data at once Import data resize train in 4 different iterationsEach method has its own drawback. Generator ImageDataGenerator is a very good tool for generating and augmenting images. I chose to go for second. 5 given if learning rate is above 0. You can follow Medium blogs and one of my favourites blogs is Machine Leraning Mastry https machinelearningmastery. We can use multiple metrices and define each output layer s loss and metric seperately by using the loss layer_1_name list of metrices. Max Pooling calculate the maximum value for each patch of the feature map or in very very simple words for example it ll return a Maximum value of each 2 2 matrix Max 00 01 10 11 that is present in each N N it encounters IFF step 1. com and Cross Validated https stats. For example running this by clicking run or pressing Shift Enter will list all files under the input directory Any results you write to the current directory are saved as output. ", "id": "deshwalmahesh/bengali-ai-complete-beginner-tutorial-95-acc", "size": "19120", "language": "python", "html_url": "https://www.kaggle.com/code/deshwalmahesh/bengali-ai-complete-beginner-tutorial-95-acc", "git_url": "https://www.kaggle.com/code/deshwalmahesh/bengali-ai-complete-beginner-tutorial-95-acc", "script": "skimage.transform ImageDataGenerator  # image augmentation on training images ONLY keras.layers keras.models tqdm # see progress bar keras.preprocessing.image numpy resize_image Dropout BatchNormalization Dense PIL.Image cv2 plot_comp keras.callbacks MaxPool2D LeakyReLU # keras layers keras.backend keras.optimizers ReduceLROnPlateau matplotlib.pyplot ModelCheckpoint Adam #optimizer matplotlib.font_manager tqdm.auto CustomDataGenerator(ImageDataGenerator) resize pandas sklearn.model_selection Model #Model class EarlyStopping Conv2D flow resize # Resizing of image Input train_test_split  # splitting the data resize as cv2_resize # resizng of image Flatten ", "entities": "(('we', '3D.'), 'for') (('You', 'learning already rate'), 'use') (('15', 'propotional original image'), 'zoom') (('Position', 'list'), 'define') (('what', 'yourself'), 'have') (('Dropout', 'network'), 'implement') (('We', 'too monitor'), 'use') (('it', 'luck'), 'be') (('We', 'metrices'), 'use') (('you', 'model'), 'have') (('EarlyStopping', 'resource wastage'), 'use') (('I', 'tutorials'), 'try') (('where filters', 'deep CNN'), 'be') (('resulting', 'output'), 'tell') (('Helper Functions ResizeWe', 'images'), 'need') (('We', 'optimizer adam'), 'use') (('core', 'lines'), 'be') (('I', 'order'), 'recommend') (('we', 'life'), 'give') (('100 to 100000 it', 'entire batch'), 'normalize') (('BEST model', 'batches'), 'be') (('layers', 'features'), 'summarize') (('best it', 'images'), 'pass') (('You', 'favourites blogs'), 'follow') (('backend', 'dynamic behaviour'), 'be') (('array', 'class'), 'concatenate') (('It', 'elements'), 'be') (('Categorical loss', 'loss function'), 'use') (('We', '2 different styles'), 'be') (('255 where 0', '255 Black'), 'have') (('Compilation', 'metric etc'), 'calculate') (('you', 'easily screen'), 'moment') (('you', 'line'), 'NOTE') (('does', 'corresponding labels'), 'be') (('who', 'data'), 'use') (('when it', 'screen'), 'resize') (('Flatten', 'tensor'), 'get') (('we', 'B W pictures'), 'have') (('Pooling MaxPool2D Pooling layers', 'feature map'), 'provide') (('we', 'gc'), 'Collection') (('org', 'https arxiv'), 'require') (('it', 'previous layers'), 'drop') (('garbage collector Input data files', 'the'), 'see') (('humans', 'we'), 'work') (('we', 'case'), 'use') (('Drop', 'output layers'), '15') (('We', 'custom also own metrices'), 'issue') (('32331 we', '0'), 'be') (('I', 'HIGHLY Neural Network series https www'), 'fair') (('model', 'model'), 'prediction') (('15', '0'), 'mean') (('10 20 1', 'samples'), 'sample') (('it', 'just images'), 'be') (('such rows', '5 image'), 'column') (('later one', 'attention'), 'think') (('you', 'keras documentation'), 'github') (('we', 'augmentations'), 'need') (('too much it', 'them'), 'tell') (('You', 'as well how it'), 'develop') (('corner bits', 'middle ones'), 'mean') (('right it', 'left'), 'give') (('you', 'pieces'), 'implement') (('you', 'Import needed all one place'), 'LibrariesImport') (('image', '3 classes'), 'have') (('sense', 'link https just www'), 'get') (('input layer', 'neural artificial network'), 'be') (('mode lhad', 'model predictions placeholder'), 'be') (('Optimizers', 'computations'), 'help') (('shape', 'performance'), 'split') (('you', 'output NN https multi github'), 'see') (('which', 'input'), 'show') (('It', 'us'), 'resize') (('main 2 reasons', 'case'), 'have') (('beginners', 'less efficiency'), 'note') (('Few', 'Keras'), 'be') (('com channel UCYO_jab_esuFRV4b17AJtAw playlists', 'Deep shelf_id Learning'), 'view') (('137 236 32332 pixels', '74 74 5476 pixels'), 'take') (('It', 'Overfitting'), 'be') (('you', 'Yoshua Bengio https www'), 'for') (('Normalization BatchNormalization', 'batch i.'), 'Normalize') (('Layers', 'layer'), 'be') (('I', '2 that'), 'be') (('Input Layer input layer', 'artificial neurons'), 'input') (('we', 'out output'), 'ask') (('validation loss', '0'), 'fit') (('Import data once resize', 'own drawback'), 'merge') (('We', 'very little information'), 'get') (('It', 'python docker image https kaggle github'), 'count') (('activation they', 'network'), 'let') (('it', 'IFF step'), 'calculate') (('KERNAL CRASH', 'TIME LATER'), 'MAKE') (('we', 'side common seats'), 'model') (('keras', 'keras'), 'be') (('that', 'labels values'), 'keep') (('We', 'activation relu too instead momentum'), 'use') (('validation loss', '0'), '00001') (('grapheme', 'image'), 'be') (('which', 'very less computational power'), 'note') (('More epochs', 'overfitting'), 'lead') (('index We', 'later it'), 'delete') (('when unit', 'small gradient'), 'allow') (('that', 'delayed treatment'), 'use') (('second one', 'merely 7 classes'), 'have') (('we', 'a hundred years'), 'have') (('that', 'batch dimension'), 'perform') (('Generator ImageDataGenerator', 'very good images'), 'be') (('it', 'code'), 'try') (('these', 'final image'), 'be') (('actual values', '0'), 'look') (('it', 'less computational power'), 'resize') (('any', 'plateau'), 'reduce') (('delete collector', 'memory'), 'stand') (('so here little effort', 'coomunity'), 'studue') (('we', 'model'), 'be') (('it', 'value'), 'convert') (('validation only loss', 'last save'), 'save') (('character', '10 height'), 'be') (('we', 'it'), 'get') (('You', 'model'), 'access') (('weights', 'epoch same images'), 'show') (('you', 'output'), 'list') (('final value', 'teak parameters'), 'neuron') (('only when you', 'successful comitted version'), 'save') (('You', 'custom loss function example https stackoverflow'), 'follow') (('that', 'activation standard close 1'), 'apply') (('remove', 'best dimension'), 'plot') (('when ReLU', '0'), 'win') (('when conditions', 'something'), 'com') (('Numpy jupyter lab 3 Pandas', 'Deep Learning Neural Networks Convolution Neural Networks keras'), 'note') (('so we', 'super'), 'have') (('it', 'memory'), 'collect') (('we', 'side side'), 'be') (('it', 'custom metrices'), 'need') (('simply how we', 'predictions'), 'be') (('So even Facebook', 'answer'), 'need') (('that', 'us'), 'v') (('we', 'original sample'), 'perform') (('image random file', 'first 5'), 'batch') (('images', 'model'), 'load') (('better I', 'something'), 'feel') (('we', 'already us'), 'have') (('loss', 'accuracy'), 'stop') (('it', 'fit_generator'), 'calculate') (('we', 'PRACTICAL formulas'), 'list') (('class', 'highest probability'), 'let') (('ratio', 'performace information'), 'lose') (('One', 'other grapheme'), 'use') (('accuracy', 'learning rate'), 'mean') (('64', 'input filter size'), 'depict') (('Convolution Pooling How ACTUALLY', 'Eignveectors'), 'explanation') (('we', 'Neural Network'), 'build') (('that', 'original matrix'), 'be') (('which', 'previous layer'), '4') (('Layer about you', 'stackoverflow answer https stackoverflow'), 'see') (('we', 'callbacks'), 'want') (('process', 'input layer'), 'go') (('obviously you', 'Stackoverflow https stackoverflow'), 'com') (('you', 'metrices'), 'implement') (('they', 'later'), 'need') (('just model', 'improvement'), 'define') (('True rotation_range', 'zoom_range'), 'produce') (('you', 'new kernal'), 'check') (('ttf file', 'Macintosh platforms'), 'be') (('I', 'community'), 'save') (('they', 'surprise'), 'have') (('Sotmax activation softmax Softmax activation function', 'class'), 'give') "}