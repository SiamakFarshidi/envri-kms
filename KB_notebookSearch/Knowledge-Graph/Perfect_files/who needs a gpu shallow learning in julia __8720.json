{"name": "who needs a gpu shallow learning in julia ", "full_name": " h1 COLLABORATION WELCOME h2 CHANGELOG h3 Update 2021 03 27 h2 Introduction h3 Goals h3 Backstory h2 Installing Julia h2 Installing home baked dependencies h1 TODO Walk through individual components h1 Putting it all together at least the Julia part h2 Modeling in Python h2 Conclusion h3 Opportunities for future work ", "stargazers_count": 0, "forks_count": 0, "description": "A less unconventional next step would be to get some autograd and convolutions going but in Julia rather than Python. The Python code below basically goes through the typical GBM baseline approach to non computer vision Kaggle competitions. Unfortunately this buries the logic for segmentation a bit. One thing to try would be finding the top target interactions i. Second it uses no deep learning. Modeling in PythonI ve decided to switch over to Python for the modeling section here at the end. I got interested in this competition from a friend just after wrapping up my first project in Julia and I thought to myself hey let s take a crack at doing this in Julia just as a learning exercise. 1 subsample 0. When it comes to improving my submission I think there are some fairly simple things to try adding to the GBM modeling. I ve been a career long Python programmer and this is only my second project with the language my first being a library to download and parse stock data from IEX https github. Putting it all together at least the Julia part Below is a script that handles the loading segmentation and feature encoding of all the cell images. jl seems like a fun time but also like another big chunk of work to dive into so I ll have to see. There s also the interesting technical challenge of seeing how much of my Julia code I can port over to run on GPU. com lukemerrick julia download. TODO Walk through individual componentsIn this latest rounds of updates I m pushing to getting my work running end to end including submission of predictions. Not to devalue all the creativity and endless tweaking that goes into making a competitive deep learning submission but I hope more folks out there will try more way out there approaches like this. jl https juliaimages. However I have started losing steam on this project and doing the remainder in Julia would take much more work than just using Python. com CellProfiling HPA Cell Segmentation project but it gives solid results and runs quite efficiently 1sec to segment a 400x400 resized image on a single core of a laptop CPU most of the runtime is actually from doing higher resolution feature computation. I hope this work is inspiring to the Julia community. I show off my RLE encoding decoding skills to move segmentation masks from Julia to Python I use numba to JIT compile Python code to get a 100x speedup for reading RLE masks IntroductionThis notebook is a bit atypical for a Kaggle image competition. I ve moved the modeling over to LightGBM in Python. Installing JuliaBefore we can run any code we have to install the Julia language. COLLABORATION WELCOME If the ideas code features or predictions below are interesting or helpful to you please reach out Given how different this appraoch is to the common Python Deep Learning approach there might be opportunity for us to team up and ensemble our models. If this sounds interesting to you please continue reading GoalsThe primary goal of this notebook is to take a crack at the competition from scratch segmentation and prediction with a an atypical set of methods and a different programming language and package ecosystem. Filtering the training data would be good too since a lot of cells in images may not actually match the label for the whole image. In order to make this notebook work offline I separated most of the installation steps into another notebook which you can take a look at if you re curious https www. Opportunities for future workCollaboration welcome The first thing I want to do is go back and explain what all the Julia code does with lots of images showing the intermediate steps of segmentation spatial feature engineering etc. cannot use precompiled packages with pyjulia on linux but we can use a system image with PyCall pre compiled to speed that up confirm we re using more than one thread restore channel axes if missing drop er channel if we re skipping it optionally rescale each channel so its color histogram matches that of the average across all channels red green and blue are channels directly we treat yellow is as an even mix of red and green by adding to both channels downscale image segment nuclei using marker based watershep segment full cells upscale segmentation maps create evenly spaced colors palette using HSV across various brightness levels define function to max out saturation of nuclei pixels local modules angle slices squeeze univariate summaries of pixel information quantiles bi channel correlations protein speckle information drop background which is value 0 drop empty fraction of total speckles in this region copy the cell nucleus summary if no nonnucleus region exists happens when cell on edge of image only push the RLE mask if featurization succeeds load train. It would probably require re implementing the underlying Watershed algorithm from ImageSegmentation. Instead it s much easier to export a more standard RLE encoding of the masks into JSON from Julia and then to use Python to re encode via the function provided by the organizers and the pycocotools library. 9 subsample_freq 1 small model for testing train models on each class run predictions create submission let s choose thresholds so that the rate we guess a particular label is a fixed multiple of the class frequency. In future updates I plan to come back and show step by step how the segmentation and feature engineering work. Certainly I doubt my segmentation code gives results as clean as those from the offical HPA Cell Segmentation https github. Sadly even though Julia is the Ju in Ju pyter Kaggle no longer supports Julia Kernels directly so we have to use Julia through a Python kernel which is a bit painful. I m releasing my code and ideas publicly here but if you use significant parts of this to make your solution better please consider inviting me to your team _ CHANGELOG Update 2021. Installing home baked dependenciesIn addition to making a semi polished public package I also created modules to organize utility functions. com louismullie watershed cuda. It can run offline thanks to the downloads being moved to another notebook. I started working locally on my laptop which means no GPU. In a previous version of this notebook I just used a Newton Raphson Logistic Regression solver I wrote from scratch in Julia. It s harder but possible to write Julia code for GPU than CPU anyhow and as I m just starting to figure things out I figured I d try to make my approach efficient enough to run on a laptop CPU. I ve implemented some hefty feature engineering including spatially aware features in Julia. csv and parse multihot encode labels resulting in columns id 0 1. For one I m familiar with several popular ML libraries in Python but I m not yet familiar with any of the Julia ML ecosystem. The secondary goal is to try and run everything fast on a CPU. First it s written purely in Julia not Python. I m planning to come back and run the lower level functions and better explain the segmentation details but for now I m working to getting things going end to end. The Hessian inversion in the full Newton Raphson solver could cost 30 3 27 000 times the compute and so it doesn t seem feasible to continue using that solver. Additionally it seems pretty nonsensical to try and re implement in Julia the complicated segmentation mask encoding function needed for submission. To any more veteran members of the Julia community who are reading through I would deeply appreciate any feedback and constructive criticism on my code. One simple approach would be to get a set of cross validated predictions using the full data and then drop the labels from cells whcih are predicted with significantly lower confidence than the other cells in the same image. pairs of classes that show up together and training a model that predicts the presence absence of the interaction. 02 num_leaves 127 colsample_bytree 0. Since it s clunky actually developing Julia in Kaggle notebooks no autocomplete can t interrup long running computation etc. com lukemerrick InvestorsExchange. Thus the goals above came about naturally. 20 imagesegmentation which seems both daunting and interesting. BackstoryI m new to the Julia programming language but I m very excited about it. Why I have now completed Julia code to segment the images and compute spatially aware statistics that encode each cell as a fixed length numeric feature vector and the last step of fitting a model is relatively small after all. As output we get feature vectors for all the cells identified by segmenting all the train and test images as well as JSON files which contain RLE encoded segmentation masks for all the test cells. Already I feel that I can try out so many things that just aren t feasible in Python numpy like non ML segmentation algorithms hand implemented logistic regression code and building custom visualizations of images. There is a little nuance in that we train one model per class independently a naive approach ignoring class interaction ConclusionSo here we have it no pre training no GPU all thanks to Julia. We load the features into DataFrames feed that into LightGBM and boom we get predictions. I m still a beginner trying to learn and every tip helps I also hope this work is inspiring to folks who aren t afraid to think creatively in the face of cookie cutter deep learning model frameworks. Unfortunately with my more serious feature enginering code I now have over 500 features per image about 30x as many features as before. It would be good to take into account the correlation between classes and the fact taht when multiple classes are present together the features look different than when the classes are present individually. precompile trigger parallel processing of load segment extract featurize remove empty results create dataframe of features giant list of masks parallel processing of load segment extract featurize remove empty results create dataframe of features giant list of masks JSON requires the entire file to be read at once so we need to chunk it if we want the reader to be able to read some at a time load and align the training data load test data load and re encode masks note it s faster to convert the lists to numpy arrays rather than having the numba jitted code take in Python lists convert input mask to expected COCO API input RLE encode mask compress and base64 encoding Train one model per class model_params dict n_estimators 1_000 learning_rate 0. 27I ve overhauled the notebook to be aimed more at an actual submission. It seems possible https github. ", "id": "lukemerrick/who-needs-a-gpu-shallow-learning-in-julia", "size": "8720", "language": "python", "html_url": "https://www.kaggle.com/code/lukemerrick/who-needs-a-gpu-shallow-learning-in-julia", "git_url": "https://www.kaggle.com/code/lukemerrick/who-needs-a-gpu-shallow-learning-in-julia", "script": "lightgbm pathlib functools numpy njit Path _mask reduce in enumerate(tqdm(target_df.columns)) tqdm _mask as coco_mask KFold binary_mask_to_ascii sklearn.model_selection numba pandas _rle_to_mask_1d Julia reencode_mask fast_rle_to_mask average_precision_score pycocotools multiprocessing rle_to_mask cpu_count sklearn.metrics julia.api ", "entities": "(('segmentation code', 'HPA Cell Segmentation https offical github'), 'doubt') (('work', 'Julia community'), 'hope') (('GoalsThe primary goal', 'methods'), 'continue') (('things', 'end'), 'm') (('I', 'Julia ML ecosystem'), 'for') (('Train one model', '1_000 learning_rate'), 'featurize') (('us', 'models'), 'welcome') (('First it', 'purely Julia'), 'write') (('I', 'work'), 'seem') (('most', 'resolution feature actually higher computation'), 'com') (('t', 'solver'), 'cost') (('first', 'stock IEX https parse github'), 'be') (('ConclusionSo here we', 'thanks Julia'), 'be') (('Additionally it', 'submission'), 'seem') (('I', 'step'), 'plan') (('approach', 'laptop enough CPU'), 's') (('work', 'predictions'), 'walk') (('particular label', 'class fixed frequency'), 'create') (('losing', 'just Python'), 'start') (('that', 'interaction'), 'pair') (('when classes', 'classes'), 'be') (('Julia code', 'feature engineering segmentation spatial etc'), 'welcome') (('I', 'Julia'), 'implement') (('s', 'learning just exercise'), 'get') (('who', 'code'), 'appreciate') (('I', 'as many features'), 'have') (('I', 'GPU'), 's') (('I', 'over Python'), 'move') (('IntroductionThis notebook', 'Kaggle image bit competition'), 'show') (('I', 'GBM fairly simple modeling'), 'think') (('Unfortunately this', 'segmentation'), 'bury') (('Instead it', 'organizers'), 's') (('too lot', 'whole image'), 'be') (('we', 'Julia language'), 'run') (('i', 'multihot encode parse columns'), 'csv') (('that', 'model'), 'be') (('I', 'very it'), 'm') (('Python code', 'computer vision Kaggle competitions'), 'go') (('we', 'predictions'), 'load') (('I', 'this'), 'devalue') (('just aren', 'images'), 'feel') (('t', 'long computation'), 'develop') (('which', 'test cells'), 'encode') (('I', 'utility functions'), 'bake') (('I', 'Julia'), 'use') (('who', 'learning model cookie cutter deep frameworks'), 'm') (('which', 'GPU'), 'start') (('27I ve', 'more actual submission'), 'overhaul') (('secondary goal', 'fast CPU'), 'be') (('Modeling', 'here end'), 'decide') (('autograd', 'rather Python'), 'be') (('One thing', 'target top interactions'), 'find') (('It', 'ImageSegmentation'), 'require') (('One simple approach', 'same image'), 'be') (('Below that', 'feature cell images'), 'be') (('It', 'notebook'), 'run') (('solution', 'team'), 'm') (('you', 'https curious www'), 'separate') (('featurization', 'load train'), 'use') (('which', 'Python kernel'), 'support') "}