{"name": "practical machine learning with pytorch tbu ", "full_name": " h1 Practical Machine Learning ML with PyTorch TBU h4 Credits Thanks to Practical AI Goku Mohandas and other contributers for such wonderful work h3 Here are some of my kernel notebooks for Machine Learning and Data Science as follows Upvote them if you like them h2 Kernel Notebook Content h2 Basics h2 Deep Learning h2 Advanced h2 Topics h2 1 Basics h2 1 1 Introduction to Python h3 Variables h3 Lists h3 Tuples h3 Dictionaries h3 If statements h3 Loops h3 Functions h3 Classes h3 Additional resources h2 1 2 NumPy h3 NumPy basics h3 Indexing h3 Array math h3 Advanced h3 Additional resources h2 1 3 Pandas h3 Uploading the data h3 Loading the data h3 Exploratory Dats Analysis EDA h3 Data Preprocessing h3 Feature Engineering h3 Saving data h2 1 4 Linear Regression h3 Overview h3 Training h3 Data h3 Scikit learn Implementation h3 Evaluation h3 Inference h3 Interpretability h3 Proof for unstandardizing coefficients h3 Regularization h3 Categorical variables h3 TODO h2 1 5 Logistic Regression h1 Training h1 Data h1 Scikit learn implementation h1 To Be Updated Soon h2 Credits Reference h2 License h3 Please UPVOTE my kernel if you like it or wanna fork it h4 I am open to have your feedback for improving this kernel h3 Thanks for visiting my Kernel and please UPVOTE to stay connected and follow up the further updates ", "stargazers_count": 0, "forks_count": 0, "description": "Proof for unstandardizing coefficients Note that both X and y were standardized. Categorical variablesIn our example the feature was a continuous variable but what if we also have features that are categorical One option is to treat the categorical variables as one hot encoded variables. 5 Logistic RegressionIn the previous lesson we saw how linear regression works really well for predicting continuous outputs that can easily fit to a line plane. 4 Linear RegressionIn this lesson we will learn about linear regression. Support vector machines SVMs https towardsdatascience. I am open to have your feedback for improving this kernel Hope you enjoyed this kernel Thanks for visiting my Kernel and please UPVOTE to stay connected and follow up the further updates Numerical example Text example int variable float variable text variable boolean variable int variables string variables Making a list Adding to a list Accessing items at specific location in a list the last item the second to last item Slicing Length of a list Replacing items in a list Combining lists Creating a tuple Adding values to a tuple Trying to change a tuples value you can t Creating a dictionary Changing the value for a key Adding new key value pairs Length of a dictionary If statement If statment with a boolean For loop goes from i 0 to i 2 same as x x 1 printing with multiple variables While loop same as x x 1 Create a function Use the function Function with multiple inputs Use the function Create the function Initialize the class For printing Example function Making an instance of a class Using a class s function Set seed for reproducability Scalars scalar 1 D Array notice the float datatype 3 D array matrix Functions Indexing Slicing Integer array indexing Boolean array indexing Basic math or x y or x y or x y Dot product we can specify dtype Sum across a dimension adds all elements add numbers in each column add numbers in each row Transposing Tile Broadcasting Reshaping Removing dimensions squeeze dim 1 Adding dimensions expand dim 1 Read from CSV to Pandas DataFrame First five items Describe features Histograms Unique values Selecting data by feature Filtering only the female data appear Sorting Grouping Selecting row iloc gets rows or columns at particular positions in the index so it only takes integers Selecting specific value Selecting by index loc gets rows or columns with particular labels from the index Rows with at least one NaN value Drop rows with Nan values removes rows with any NaN values reset s row indexes in case any rows were dropped Dropping multiple rows we won t use text features for our initial basic models Map feature values Lambda expressions to create new features Reorganize headers Saving dataframe to CSV See your saved file Arguments Set seed for reproducability Generate synthetic data Generate random linear data Scatter plot Import packages Create data splits Standardize the data mean 0 std 1 using training data Apply scaler on training and test data Check mean should be 0 std should be 1 Initialize the model Train Predictions unstandardize them Train and test MSE Figure size Plot train data Plot test data Show plots Feed in your own inputs Unstandardize coefficients 3. frac partial J partial W_j frac partial J partial y frac partial y partial W_j frac 1 y frac partial y partial W_j frac 1 frac e W_yX sum e XW frac sum e XW e W_yX 0 e W_yX e W_jX X sum e XW 2 frac Xe W_jX sum e XW XP frac partial J partial W_y frac partial J partial y frac partial y partial W_y frac 1 y frac partial y partial W_y frac 1 frac e W_yX sum e XW frac sum e XW e W_yX X e W_yX e W_yX X sum e XW 2 frac 1 P XP XP 2 X P 1 5. Credits Thanks to Practical AI Goku Mohandas and other contributers for such wonderful work Here are some of my kernel notebooks for Machine Learning and Data Science as follows Upvote them if you like them Awesome Deep Learning Basics and Resources https www. is highly recommended by Google as it s developed by googlers along with Notebooks for exercises. DataWe re going to the load the titanic dataset we looked at in lesson 03_Pandas. Compare the predictions hat y ex. The softmax classifier will use the linear equation z XW and normalize it to product the probabiltiy for class y given the inputs. Feed inputs X into the model to receive the logits z XW. com arunkumarramanan awesome data science for beginners Tensorflow Tutorial and House Price Prediction https www. DictionariesDictionaries are python objects that hold key value pairs. MSE J theta frac 1 2 sum_ i hat y _i y_i 2 4. com learn learn python and Kaggle Learn https www. To Be Updated Soon Go to top top Credits Reference practicalAI Goku Mohandas https github. SGD linear regression Go to top top 1. Uploading the dataWe re first going to get some data to play with. If statementsYou can use if statements to conditionally do something. hat y frac 1 1 e XW where hat y prediction in mathbb R NX1 N is the number of samples X inputs in mathbb R NXD D is the number of features W weights in mathbb R DX1 This is the binomial logistic regression. J theta frac 1 2 sum_ i X_iW y_i 2 frac lambda 2 sum sum W 2 frac partial J partial W X hat y y lambda W W W alpha frac partial J partial W where lambda is the regularzation coefficientRegularization didn t help much with this specific example because our data is generation from a perfect linear equation but for realistic data regularization can help our model generalize well. Overview hat y XW where hat y prediction in mathbb R NX1 N is the number of samples X inputs in mathbb R NXD D is the number of features W weights in mathbb R DX1 Objective Use inputs X to predict the output hat y using a linear model. When you want to use numerical operations on then they need to be compatible. When we have more than two classes we need to use multinomial logistic regression softmax classifier. For example if there are three classes the predicted class probabilities could look like 0. stochastic gradient descent. J theta ln hat y_i ln frac e X_iW_y sum_i e X_iW 4. that are calculated you need to separate the training and test set first before spplying those operations. 4 with the actual target values y ex. These are the diferent features pclass class of travel name full name of the passenger sex gender age numerical age sibsp of siblings spouse aboard parch number of parents child aboard ticket ticket number fare cost of the ticket cabin location of room emarked port that the passenger embarked at C Cherbourg S Southampton Q Queenstown survived survial metric 0 died 1 survived Exploratory Dats Analysis EDAWe re going to explore the Pandas library and see how we can explore and process our data. t to the model weights. Additional resourcesThis was a very quick look at python and we ll be learning more in future lessons. The main idea is to take the outputs from the linear equation z XW and use the sigmoid logistic function frac 1 1 e z to restrict the value between 0 1. Highly interpretable. However we are going to use Scikit learn s SGDRegressor class which uses stochastic gradient descent. hat y frac e XW_y sum e XW where hat y prediction in mathbb R NX1 N is the number of samples X inputs in mathbb R NXD D is the number of features W weights in mathbb R DXC C is the number of classes Objective Predict the probability of class y given the inputs X. A dictionary cannot have two of the same keys. ClassesClasses are a fundamental piece of object oriented Python programming. There are also other types of regularization like L1 lasso regression which is useful for creating sparse models where some feature cofficients are zeroed out or elastic which combines L1 and L2 penalties. W_i W_i alpha frac partial J partial W_i 6. Note If you have preprocessing steps like standardization etc. LinearRegression with pros and cons vs. com topics awesome License MIT https img. Let s assume that our classes are mutually exclusive a set of inputs could only belong to one class. Repeat steps 2 4 until model performs well. EvaluationThere are several evaluation techniques to see how well our model performed. Scikit learn implementation Note The LogisticRegression class in Scikit learn uses coordinate descent to solve the fit. We need to standardize our data zero mean and unit variance in order to properly use SGD and optimize quickly. Scikit learn Implementation Note The LinearRegression class in Scikit learn uses the normal equation to solve the fit. This is very easy to do with Pandas and once you create the dummy variables you can use the same steps as above to train your linear model. Loading the dataNow that we have some data to play with let s load into a Pandas dataframe. We won t learn everything about Python but enough to do some basic machine learning. Inference InterpretabilityLinear regression offers the great advantage of being highly interpretable. com learn python Go to top top 1. io badge license MIT brightgreen. Besides MSE when we only have one feature we can visually inspect the model. J theta sum_i y_i ln hat y_i sum_i y_i ln frac e X_iW_y sum e X_iW y 0 0 1 hat y 0. Now you can concat this with your continuous features and train the linear model. Go to top top 1. com arunkumarramanan data science with r awesome tutorials Data Science and Machine Learning Cheetcheets https www. com arunkumarramanan data science and machine learning cheatsheets Awesome ML Frameworks and MNIST Classification https www. DataWe re going to create some simple dummy data to apply linear regression on. com arunkumarramanan practical machine learning with pytorch Practical Machine Learning ML with TensorFlow The above highlighted work will soon be released and also be based on the below coursework Google Machine Learning Crash Course https developers. Disadvantages The model will perform well only when the data is linearly separable for classification. 4 This simplifies our cross entropy objective to the following J theta ln hat y_i negative log likelihood. Note Since we standardized our inputs and outputs for gradient descent we need to apply an operation to our coefficients and intercept to interpret them. com arunkumarramanan awesome data science ipython notebooks Machine Learning Engineer s Toolkit with Roadmap https www. 65 10 Create data with categorical features Arguments Set seed for reproducability Upload data from GitHub to notebook s local drive Read from CSV to Pandas DataFrame Import packages Preprocessing Drop rows with NaN values Drop text based features we ll learn how to use them in later lessons pclass sex and embarked are categorical features Preprocess the dataset Split the data Separate X and y Standardize the data mean 0 std 1 using training data Apply scaler on training and test data don t standardize outputs for classification Check mean should be 0 std should be 1 Initialize the model Train Probabilities Predictions unstandardize them. We will also look at ways of interpreting the linear model. Below is L2 regularization ridge regression. Kernel Notebook Content Basics 1 Python 1 NumPy 1 Pandas 1 Linear Regression 1 Logistic Regression 1 Random Forests 1 KMeans Clustering Deep Learning 2 PyTorch 2 Multilayer Perceptrons 2 Data Models 2 Object Oriented ML 2 Convolutional Neural Networks 2 Embeddings 2 Recurrent Neural Networks 2 Advanced 3 Advanced RNNs 3 Highway and Residual Networks Autoencoders Generative Adversarial Networks Spatial Transformer Networks Topics 4 Computer Vision 4 Time Series Analysis Topic Modeling Recommendation Systems Pretrained Language Modeling Multitask Learning Low Shot Learning Reinforcement Learning 1. Training Steps 1. com arunkumarramanan machine learning engineer s toolkit with roadmap Hands on ML with scikit learn and TensorFlow https www. We re going to load the titanic dataset from the public link below. This function calculates the difference between the predicted and target values and squares it. Training data X y is used to train the model and learn the weights W using stochastic gradient descent SGD. 2 NumPyIn this lesson we will learn the basics of numerical analysis using the NumPy package. com machine learning crash course Machine Learning ML Crash Course with TensorFlow APIs Google s fast paced practical introduction to machine learning A self study guide for aspiring machine learning practitioners Machine Learning Crash Course features a series of lessons with video lectures real world case studies and hands on practice exercises. com ArunkumarRamanan practicalAI master LICENSE Please UPVOTE my kernel if you like it or wanna fork it. the frac 1 2 is just for convenicing the derivative operation. com arunkumarramanan data science with python awesome tutorials Awesome TensorFlow and PyTorch Resources https www. If you are curious about more checkout the NumPy reference manual https docs. Let s look at how to make some variables. 1 Introduction to PythonIn this lesson we will learn the basics of the Python programming language version 3. It s good practice to know what types your variables are. Feed inputs X into the model to receive the predictions hat y. A common objective function for logistics regression is cross entropy loss. Usually not used for classification and only for regression. If you want to learn more right now before diving into machine learning check out this free course Free Python Course https www. Feedback If you have any ideas or you want any other content to be added to this curated list please feel free to make any comments to make it better. Apply the softmax operation on the logits to get the class probabilies hat y in one hot encoded form. A common objective function for linear regression is mean squarred error MSE. In the example dictionary below the keys are the name and eye_color variables. There are many forms of regularization but they all work to reduce overfitting in our models. Pandas is a great python library for data analysis. Note Regularization is not just for linear regression. ListsLists are objects in python that can hold a ordered sequence of numbers and text. But linear regression doesn t fare well for classification asks where we want to probabilititcally determine the outcome for a given set on inputs. We want to use this optimization approach because we will be using this for the models in subsequent lessons. frac mathbb E y hat y sigma_y W_0 sum_ j 1 k W_jz_j z_j frac x_j bar x _j sigma_j hat y _ scaled frac hat y _ unscaled bar y sigma_y hat W_0 sum_ j 1 k hat W _j frac x_j bar x _j sigma_j hat y _ unscaled hat W _0 sigma_y bar y sum_ j 1 k hat W _j frac sigma_y sigma_j bar x _j sum_ j 1 k frac sigma_y sigma_j x_j RegularizationRegularization helps decrease over fitting. Having certain weights with high magnitudes will lead to preferential bias with the inputs and we want the model to work with all the inputs and not just a select few. This is because we cannot apply any knowledge gained from the test set accidentally during preprocessing training. com arunkumarramanan data scientist s toolkits awesome ds resources Awesome Computer Vision Resources TBU https www. 3 PandasIn this notebook we ll learn the basics of data analysis with the Python Pandas library. NumPy basics Indexing Array math Advanced Additional resourcesYou don t to memorize anything here and we will be taking a closer look at NumPy in the later lessons. Implement basic ML algorithms and deep neural networks with PyTorch. Miscellaneous Softmax classifier is going to used widely in neural network architectures as the last layer since it produces class probabilities. The simplified intuition is that the gradient tells you the direction for how to increase something so subtracting it will help you go the other way since we want to decrease loss J theta. com arunkumarramanan awesome tensorflow and pytorch resources Awesome Data Science IPython Notebooks https www. com support vector machine vs logistic regression 94cc2975433f are a good alternative to counter outliers. Calculate the gradient of loss J theta w. We will first understand the basic math behind it and then implement it in Python. 4 J theta sum_i y_i ln hat y_i sum_i y_i ln frac e X_iW_y sum e X_iW sum_i 0 ln 0. LoopsYou can use for or while loops in python to do something repeatedly until a condition is met. Advantages Can predict class probabilities given a set on inputs. VariablesVariables are objects in python that can hold anything with numbers or text. com topics awesome GitHub Machine Learning Topic https github. They each have a value associated with them. com arunkumarramanan awesome deep learning ml tutorials Data Science with Python Awesome Tutorials https www. J theta frac 1 2 sum_ i hat y _i y_i 2 frac 1 2 sum_ i X_iW y_i 2 frac partial J partial W X hat y y 4. W W alpha frac partial J partial W 5. The softmax classifier normalizes the linear outputs to determine class probabilities. TODO polynomial regression simple example with normal equation method sklearn. com arunkumarramanan hands on ml with scikit learn and tensorflow Practical Machine Learning with PyTorch https www. Apply backpropagation to update the weights W using a learning rate alpha and an optimization technique ie. FunctionsFunctions are a way to modularize reusable pieces of code. Learn object oriented ML to code for products not just tutorials. We can interpret our coefficient as follows By increasing X by 1 unit we increase y by W 3. Randomly initialize the model s weights W. 65 10 Initialize the model with L2 regularization Train Predictions unstandardize them Train and test MSE Unstandardize coefficients 3. Practical Machine Learning ML with PyTorch TBU This kernel is empowering you to use machine learning to get valuable insights from data. You can use it to regualr any model s weights including the ones we will look at in future lessons. com arunkumarramanan awesome machine learning ml frameworks Awesome Data Science for Beginners with Titanic Exploration https kaggle. com arunkumarramanan awesome deep learning resources Data Science with R Awesome Tutorials https www. The model will be a line of best fit that minimizes the distance between the predicted and target outcomes. However we are going to use Scikit learn s SGDClassifier class which uses stochastic gradient descent. com GokuMohandas practicalAI GitHub Awesome Lists Topic https github. TuplesTuples are also objects in python that can hold data but you cannot replace values for this reason tuples are called immutable whereas lists are known as mutable. Miscellaneous You can also use linear regression for binary classification tasks where if the predicted continuous value is above a threshold it belongs to a certain class. But we will cover better techniques for classification in future lessons and will focus on linear regression for continuos regression tasks only. The updates will penalize the probabiltiy for the incorrect classes j and encourage a higher probability for the correct class y. Apply backpropagation to update the weights W using gradient descent. Advantages Computationally simple. Can account for continuous and categorical features. class 2 would look like 0 0 1 with the objective cost function to determine loss J. com arunkumarramanan awesome computer vision resources to be updated Machine Learning and Deep Learning Awesome Tutorials https www. With L2 regularization we are penalizing the weights with large magnitudes by decaying them. com topics deep learning GitHub Awesome Lists Topic https github. Compare the predictions hat y with the actual target values y with the objective cost function to determine loss J. Each feature has a coefficient which signifies it s importance impact on the output variable y. Disadvantages Sensitive to outliers since objective is minimize cross entropy loss. Data Preprocessing Feature Engineering Saving data Go to top top 1. com topics machine learning GitHub Deep Learning Topic https github. com arunkumarramanan tensorflow tutorial and examples Data Scientist s Toolkits Awesome Data Science Resources https www. ", "id": "arunkumarramanan/practical-machine-learning-with-pytorch-tbu", "size": "20504", "language": "python", "html_url": "https://www.kaggle.com/code/arunkumarramanan/practical-machine-learning-with-pytorch-tbu", "git_url": "https://www.kaggle.com/code/arunkumarramanan/practical-machine-learning-with-pytorch-tbu", "script": "change_name train_test_split Namespace preprocess add_two numpy SGDClassifier join_name sklearn.preprocessing Pets(object) get_family_size generate_data sklearn.linear_model matplotlib.pyplot sklearn.model_selection pandas SGDRegressor argparse __str__ __init__ StandardScaler sklearn.linear_model.stochastic_gradient ", "entities": "(('main idea', '0'), 'be') (('They', 'them'), 'have') (('i', 'frac'), 'theta') (('above highlighted work', 'coursework Google Machine Learning Crash Course https soon also below developers'), 'com') (('Feed', 'logits'), 'input') (('Note Regularization', 'just linear regression'), 'be') (('This', 'mathbb R DX1'), 'frac') (('class', 'loss'), 'look') (('we', 'more future lessons'), 'be') (('com arunkumarramanan', 'Python Awesome Tutorials https www'), 'awesome') (('softmax classifier', 'inputs'), 'use') (('we', 'continuos regression tasks'), 'cover') (('Overview hat y hat where prediction', 'linear model'), 'XW') (('Now you', 'linear model'), 'concat') (('lesson we', 'linear regression'), 'learn') (('DataWe', 'linear regression'), 'go') (('Randomly', 'weights'), 'initialize') (('we', 'them'), 'note') (('We', 'public link'), 'go') (('common objective function', 'linear regression'), 'be') (('com arunkumarramanan', 'R Awesome Tutorials https www'), 'awesome') (('we', 'lesson'), 'go') (('hat W _ j sigma_y sigma_j bar _ j _ k sigma_j frac 1 sigma_y RegularizationRegularization', 'fitting'), 'mathbb') (('it', 'certain class'), 'use') (('we', 'accidentally training'), 'be') (('Saving data', 'top top 1'), 'go') (('updates', 'correct class'), 'penalize') (('you', 'Free Python Course https www'), 'check') (('you', 'wanna it'), 'com') (('lesson we', 'NumPy package'), 'learn') (('machine practitioners Machine Learning Crash learning Course', 'world case practice real exercises'), 'course') (('class three predicted probabilities', '0'), 'for') (('it', 'curated list'), 'feedback') (('LinearRegression class', 'fit'), 'learn') (('Scikit SGDClassifier which', 'gradient stochastic descent'), 'go') (('1 y frac partial y partial W_y', 'frac e sum e XW frac XW 1 e e'), 'w_j') (('model', 'perfect linear equation'), 'frac') (('frac', '1 just derivative operation'), 'be') (('Training X y', 'descent stochastic gradient SGD'), 'datum') (('We', 'linear model'), 'look') (('number', 'inputs'), 'XW') (('then they', 'numerical operations'), 'need') (('you', 'linear model'), 'be') (('ClassesClasses', 'Python programming'), 'be') (('where we', 'inputs'), 'ask') (('LogisticRegression class', 'fit'), 'learn') (('com data arunkumarramanan learning', 'Awesome ML Frameworks'), 'cheatsheet') (('they', 'models'), 'be') (('We', 'machine basic learning'), 'win') (('we', 'logistic regression softmax multinomial classifier'), 'have') (('Advantages', 'inputs'), 'predict') (('kernel', 'data'), 'ML') (('it', 'class probabilities'), 'go') (('Scikit SGDRegressor which', 'gradient stochastic descent'), 'go') (('that', 'line easily plane'), 'RegressionIn') (('lesson we', 'Python programming language version'), 'introduction') (('how we', 'data'), 'be') (('s', 'Pandas dataframe'), 'load') (('you', 'them'), 'be') (('FunctionsFunctions', 'code'), 'be') (('lists', 'reason'), 'be') (('you', 'standardization'), 'note') (('3 we', 'Python Pandas library'), 'PandasIn') (('it', 'exercises'), 'recommend') (('We', 'properly SGD'), 'need') (('here we', 'later lessons'), 'basic') (('s', 'how variables'), 'let') (('model', 'inputs'), 'lead') (('function', 'it'), 'calculate') (('cross', 'J log following theta ln hat negative likelihood'), '4') (('X', 'coefficients'), 'note') (('com', 'python'), 'learn') (('well only when data', 'linearly classification'), 'perform') (('Feed', 'predictions'), 'input') (('that', 'one hot encoded variables'), 'variablesIn') (('Train 0 1 Predictions', 'Unstandardize coefficients'), 'be') (('we', 'y W'), 'interpret') (('we', 'future lessons'), 'use') (('mutually set', 'only one class'), 'let') (('we', 'subsequent lessons'), 'want') (('65 10', 'MSE Unstandardize coefficients'), 'initialize') (('other way we', 'loss J theta'), 'be') (('you', 'first operations'), 'calculate') (('statements', 'conditionally something'), 'use') (('that', 'numbers'), 'be') (('Pandas', 'python data great analysis'), 'be') (('Train Probabilities 0 1 Predictions', 'them'), 'Set') (('that', 'predicted outcomes'), 'be') (('Disadvantages Sensitive', 'entropy loss'), 'minimize') (('repeatedly condition', 'something'), 'use') (('we', 'them'), 'penalize') (('Inference InterpretabilityLinear regression', 'great advantage'), 'offer') (('dictionary', 'same keys'), 'have') (('common objective function', 'logistics regression'), 'be') (('you', 'reference manual https NumPy docs'), 'be') (('python that', 'value key pairs'), 'be') (('softmax classifier', 'class probabilities'), 'normalize') (('dictionary', 'keys'), 'be') (('data', 'first going'), 'upload') (('which', 'L1 penalties'), 'be') (('we', 'visually model'), 'besides') (('94cc2975433f', 'good outliers'), 'machine') (('it', 'importance output variable'), 'have') (('We', 'Python'), 'understand') "}