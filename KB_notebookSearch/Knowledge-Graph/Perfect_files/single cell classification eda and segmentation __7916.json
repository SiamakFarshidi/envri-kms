{"name": "single cell classification eda and segmentation ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "For each image in the test set you must predict a list of instance segmentation masks and their associated detection score Confidence. This notebook contains some basic EDA and notes about segmentation as it pertains to the HPA Single Cell Classification competition. The training labels are given at the image level and we are tasked with predicting labels at the cell level. Because our task is closer to instance segmentation there is quite a lot of post processing to be done on the output of a U Net in order to produce masks for the individual cells in our images. The challenge here will be segment the images correctly and then find a way to use the imprecise labels to classify our segmented cells accurately. So in other words in some of the Negative samples one or two of the cells in the image may in fact have a protein pattern but most will not. A segmentation solution is a partition of V into multiple connected components. Appreciate any comments advice to visualise classes will take images with single class labels. We will therefore have to identify each cell in each image segment them and classify them individually. org These are both pre trained models based on variations of convolutional neural networks called U Nets. An edge has a weight which is given by the distance between the two vertices pixels in the edge. Oversampling may be necessary. Defining the problem This is a weakly supervised classification problem. The statement below made by Trang Le lnhtrang in our notebook explaining the patterns is correct for how the test set was annotated. excluding same rectangle with different segments excluding regions smaller than 2000 pixels distorted rects draw rectangles on the original image. It is built on top of a segmentation algorithm such as Felzenszwalb. ImageBID ImageBWidth ImageBHeight LabelB1 ConfidenceB1 EncodedMaskB1 LabelB2 ConfidenceB2 EncodedMaskB2 Note that a mask MAY have more than one class. If that is the case predict separate detections for each class using the same mask. Can see most labels are weakly correlated with each other with some for example 4 and 0 with strong negative correlation. If you compare the image level label with the precise pattern observed in any given cell from this group of images the label will be correct for the vast majority of cells but perhaps not for all of them as in your example 1 and 3 above. How to best carry out segmentation on this dataset Popular models being used for this competition includehttps github. As well we need a model that predicts the masks as well as just identifying the objects. Hence the accuracy of these labels is much better and will be correct for each cell in every image. Selective Search is an algorithm used to propose regions which contain objects. The above shows the output from the semantic segementation performed by U Net. TBC when I have the time. The basic idea is that a if the distance metric between two components is small compared to the internal difference of both those components then merge the two components otherwise do nothing. This means that the labels present in the majority of the images will be annotated. The problem can therefore be framed as an instance segmentation problem. com watch v FppOzcDvaDILabels are in the form shown above will change it to a list of intsVast majority of images either have 1 2 or 3 labels with at least one image with 5 labels. During annotation the image level labels are set per sample i. edu people pfelzens papers seg ijcv. The submissions are evaluated using Mean Average Precision or mAP. The labels themselves refer to the subcellur protein localization patterns of single cells. Another approach would be to use a purpose built instance segmentation model such as Mask R CNN which outputs individual object masks bounding boxes and class predictions for each objects. e per a group of up to 6 images from the same sample. For negative samples it is not uncommon that lets say 4 images show no staining while the remaining 2 show some unspecific staining or some granular pattern. Even after playing around with the parameters the set of parameters which correctly identify the different cells in one image will result in far too many regions or too few in the next. I am a beginner in this field and explaining my work as I go is as much for my benefit as anyone else s so I appreciate any advice corrections etc. Note while this algorithm returns bounding boxes not the image masks we will need for submission it will be interesting to see if it can reliably identify individual cells especially when their boundaries overlap. Below will be the masks once post processing is complete. sigma is a Gaussian blur parameter to smooth the image. It is a hierarchical grouping algorithm iteratively grouping together the most similar regions which were obtained by Felzenszwalb until a stopping point determined by the min_size parameter. For more information on how the algorithm works see http cs. So higher k will give us larger components segments. It involves representing our image as an undirected graph G V E of vertices and edges with each vertex being a single pixel and each edge connecting a pair of two vertices. We need a method more robust than this for our dataset of thousands of images of cells. com CellProfiling HPA Cell Segmentation. So we are submitting a list of masks their respective labels and the respective confidences with which these label predictions were made. As you can see the results are pretty unsatisfactory. com CellProfiling HPA Cell Segmentation https www. We have 17 classes of these patterns and a negative class. 2 Single cell label accuracy in test set The test set consists of images where each single cell has been annotated independently. How best to produce these masks Why do we need a pre trained CNN at all Thresholding pretty clearly isn t going to be the answer given that the task is to segment the individual cells in the image and the background is already completely distinct. The distance metric is the difference in colour intensity and location. The task we have is to perform single cell classification therefore we are going to have to produce masks over each cell we identify and classify them individually. The higher the scale factor k the higher the threshold for a decision to label two components to be separate. Starting with each pixel in its own component the segmentation is arrived at through a bottom up technique. A U Net is a form of CNN designed for semantic segmentation. Luckily a lot of that work has been done for us by the creators of https github. The submission csv file uses the following format ImageID ImageWidth ImageHeight PredictionString ImageAID ImageAWidth ImageAHeight LabelA1 ConfidenceA1 EncodedMaskA1 LabelA2 ConfidenceA2 EncodedMaskA2. Which are more common do they tend to occur together Quite imbalanced labels 0 is by far the most common and 11 18 are close to 0. Before going near either of those as a newcomer to the field of computer vision and object identification I want to explore some more basic segmentation techniques in order to appreciate the need for the tools listed above. 19 different training labels want to look at their distributions in the training set. The first segmentation technique that I will implement is a graph based technique first proposed by Felsenzwalb. Not many strong positive correlations. csv has four associate imagesBlue Nucleus Red Microtubules Yellow Endoplasmic reticulum Green Protein of interestFirst lets visualise a few of these separately then find a way to combine them A note about Negative labels from the organiser 1 Image level label uncertainty The image level labels are what we refer to as weak or noisy. pdfAs we can see this approach runs into problems where cells overlap as is the case with much of our dataset. ", "id": "stephenfenel/single-cell-classification-eda-and-segmentation", "size": "7916", "language": "python", "html_url": "https://www.kaggle.com/code/stephenfenel/single-cell-classification-eda-and-segmentation", "git_url": "https://www.kaggle.com/code/stephenfenel/single-cell-classification-eda-and-segmentation", "script": "filters label_cell selective_search_regions numpy seaborn skimage scipy.ndimage label_nuclei matplotlib.patches util to_rgb transform skimage.io load_images matplotlib.pyplot hpacellseg.cellsegmentator pandas closing skimage.morphology disk hpacellseg.utils (binary_erosion measure ", "entities": "(('you', 'instance segmentation masks'), 'predict') (('We', 'them'), 'have') (('label predictions', 'which'), 'submit') (('where single cell', 'images'), 'accuracy') (('challenge', 'segmented cells'), 'be') (('sigma', 'blur Gaussian image'), 'be') (('more they', '0 most 18 0'), 'tend') (('it', 'HPA Single Cell Classification competition'), 'contain') (('task', 'images'), 'be') (('test how set', 'patterns'), 'be') (('labels', 'single cells'), 'refer') (('image level labels', 'sample i.'), 'set') (('which', 'min_size parameter'), 'be') (('We', 'cells'), 'need') (('So higher k', 'components larger segments'), 'give') (('problem', 'instance segmentation therefore problem'), 'frame') (('which', 'objects'), 'be') (('Luckily lot', 'https github'), 'do') (('which', 'edge'), 'have') (('Hence accuracy', 'image'), 'be') (('I', 'graph based first Felsenzwalb'), 'be') (('are', '5 labels'), 'change') (('segmentation solution', 'multiple connected components'), 'be') (('most labels', '4 strong negative correlation'), 'see') (('we', 'them'), 'be') (('background', 'image'), 'produce') (('It', 'such Felzenszwalb'), 'build') (('especially when boundaries', 'reliably individual cells'), 'note') (('then two components', 'otherwise nothing'), 'be') (('which', 'too next'), 'result') (('label', 'example'), 'be') (('most', 'protein pattern'), 'have') (('algorithm how works', 'http cs'), 'for') (('segmentation', 'bottom technique'), 'arrive') (('distance', 'colour metric intensity'), 'be') (('ImageBWidth EncodedMaskB2 mask MAY', 'more than one class'), 'ImageBID') (('else so I', 'advice corrections'), 'be') (('visualise classes', 'class single labels'), 'appreciate') (('labels', 'images'), 'mean') (('we', 'what'), 'have') (('vertex', 'two vertices'), 'involve') (('above', 'U Net'), 'show') (('submission csv file', 'following format'), 'use') (('case', 'same mask'), 'predict') (('case', 'dataset'), 'see') (('submissions', 'Mean Average Precision'), 'evaluate') (('We', 'patterns'), 'have') (('rects', 'original image'), 'distort') (('which', 'class objects'), 'be') (('training 19 different labels', 'training set'), 'want') (('we', 'cell level'), 'give') (('4 images', 'staining'), 'be') (('U Net', 'semantic segmentation'), 'be') (('I', 'tools'), 'want') (('that', 'as well just objects'), 'need') (('These', 'convolutional neural networks'), 'org') (('This', 'problem'), 'be') "}