{"name": "lyft eda training ", "full_name": " h1 Overview h1 Data Format h1 l5kit h2 Data h1 Visualization h3 Raw Data h3 Data Abstraction h1 PyTorch Training h3 Model h3 Training Loop h1 Inference h1 Training Parameter EDA h3 Raster Size h3 Pixel Size h3 Changing Raster Size and Pixel Size h1 Augmentation h1 To be continued ", "stargazers_count": 0, "forks_count": 0, "description": "com pestipeti pytorch baseline train. com tuckerarrants lyft inference resnet50 edit but I included the general procedure here for completeness Here you upload your freshly trained model Now we get our predictions and save them as a csv file with write_pred_csv Training Parameter EDA Let s see which parameters in our configuration file are relevant to training Let s explore two important training specific parameters raster_size and pixel_size Raster Size We can see that different raster sizes affect how much of a scene is rasterized meaning that our model sees different agents depending on the raster size. Note that only 4 of the 17 possible agent label_probabilities are present in this dataset. For a more thorough explanation see Peter s comment here https www. We need to change the input channels of the ResNet to match the rasterizer output. We can combat this by using a smaller pixel_size Changing Raster Size and Pixel Size If you want the model to see more of a scene you can change either raster_size or pixel_size. zarr files is fine but l5kit also provides abstract data classes to generate inputs and targets with ease. Now this probably isn t that useful but it is cool so I will demo it anyway. The EgoDataset iterates only over the AV annotations and the AgentDataset iterates everything but the AV annotations. Let s first visualize the AV using an EgoDataset Once in an EgoDataset we can plot the ground truth trajectory by converting the EgoDataset. The below is a ResNet18 pre trained on ImagNet taken from Peter https www. zarr file to create a scatter plot of all the AV henceforth AV locations like so I have this d out because it takes around 40 minutes to complete and is not nearly as insightful as the below visualizations will be Data Abstraction Working with raw. We can add some weather augmentations which doesn t make much sense to do with the py_semantic raster images but I guess one could use it for py_satellite raster images. zarr file for training we follow the below procedure 1. com pestipeti pytorch baseline inference Since I have internet enabled in this notebook which is not allowed for submission notebooks in this comp I carried out the prediction part in this notebook https www. com ilu000 expected error inaccuracy from rasterization you ll see that there is some inherent innacuracy from the rasterization process defined by pixel_size. It is a great place to start for this competition. More importantly if you want the same scene in a higher resolution you need to change both raster_size and pixel_size. com c lyft motion prediction autonomous vehicles discussion 186492 for explanation of new architecture and training loop changes We can speed up our training by placing our computations on the GPU with the torch. Taken from the competition Data description each. We can then use the EgoDataset and AgentDataset objects in l5kit to iterate over the rasterizer object we just defined to return RGB images. l5kit uses PyTorch DataLoaders so we will use albumentations to compose these augmentations. In test the mask provided in files as mask. zarr files the scenes. Data Data is expected to live in the folder that we set using the L5KIT_DATA_FOLD env variable. agents_mask a mask that for train and validation masks out objects that aren t useful for training. python basics for deep learning for scene visualization set env variable for data let s see what one of the objects looks like for idx_coord idx_data in enumerate tqdm range len frames desc getting centroid to plot trajectory X Y coords for the future positions output shape batch_sizex50x2 You can add more layers here. In particular l5kit allows us to Load driving scenes from zarr files Read aerial and semantic maps Create birds eye view images that repesent a scene around an automonous vehicle other vehicles And most importantly train neural networks and visualize their results Let s add l5kit now and start to explore how we can use it for the task at hand. com lyft l5kit blob master examples. zarr files which can be loaded with the Python zarr module. com corochann lyft training with multi mode confidence convert to batch_size num_modes future_len num_coords add modes add modes and cords error batch_size num_modes future_len reduce coords and use availability when confidence is 0 log goes to inf but we re fine with it error batch_size num_modes reduce time use max aggregator on modes for numerical stability error batch_size num_modes error are negative at this point so max gives the minimum one reduce modes print error error pred bs x time x 2D coords bs x mode 1 x time x 2D coords create confidence bs x mode 1 Backward pass save model during training display training progress Rasterizer Test dataset dataloader Build Rasterizer reset to default reset to default reset to default for augmentations for better visualization get a random sample. We will define our optimizer and metric here as well Training Loop Once you are happy with your training you can save the model state to your disk and upload the pre trained weights offline for an inference submission using torch. 5 4 for each direction of each predicted position. com pestipeti s notebook here https www. This is why Lyft has provided us the l5kit module as a utility script called kaggle_l5kit. com lyft l5kit tree master examples agent_motion_prediction And now we are ready to explore how to train a neural network with this. The following code is again taken from Peter https www. This is yet another import training parameter to experiment with especially when we are feeding these rasterized images to a CNN If pixel_size. traffic_light_faces traffic light information. We can reduce this similarity between frames by introducing augmentations like CutOut CourseDropout and more. 5 this means that we are at most able to capture objects no less than half a meter in size if the object is smaller than half a meter it will not be detected as a pixel so to view these objects we need a higher resolution lower pixel_size As you can read about in this notebook https www. This is largely because some images are very similar e. zarr files of interest are in train_data_loader key Visualization Main source https github. frames snapshots in time of the pose of the vehicle. Each history position lane and other agents are encoded into the pixels and if the raster has pixel_size. save Inference And now after all that work we can predict with this baseline model. com lyft l5kit tree master examples visualisation Raw Data. l5kit l5kit is a Python library developed by Lyft Level 5 with functionality for developing and training learned prediction planning and simulation models for autonomous driving applications. Pass AgentDataset into a torch Dataloader Let s do this now Model We can create a simple model to feed this DataLoader to. When looking at the above configuration dictionary we see that the. zarr files support most traditional NumPy array operations. So this is the value to tweak if you want to change the resolution of the image. zarr file into a ChunkedDataset object2. For example we can iterate over the frames in our. 5 we ought to expect a mean error of. Overview Most of the contents of this notebook can be found on the l5kit GitHub repository in the examples folder here https github. Most importantly we can use a rasterizer which takes these chunked. the frames of the agent vehicle at a red light are much the same in comparison to frames in which it is moving at full speed. zarr files and processes them into rectangular grid of pixels a raster image so that we can view them regularly on a computer screen. Wrap ChunkedDataset object into an AgentDataset which inhereits from the PyTorch Dataset class3. npz masks out any test object for which predictions are NOT required. zarr file contains a set of scenes driving episodes acquired from a given vehicle. agents a generic entity captured by the vehicle s sensors. This is a powerful hyper parameter should our model focus on agents far away Or should it focus on agents closer to it Pixel Size From the l5kit GitHub repository the pixel size of the raster image is the raster s spatial resolution meters per pixel the size in the real world one pixel corresponds to. Data Format The data in this competition is packed into. Luckily this is exactly how our data is set up in this Kaggle kernel so all we need to do is this Now we can iteract with l5kit via the below configuration. zarr file and store it as a ChunkedDataset object. com nxrprime lyft understanding the data and eda Pretty cool right We are able to follow our AV as it traverses threw this scene PyTorch Training Main source https github. 2 Forward pass from https www. We also need to change the output size to X Y number of future states Update see discussion post here https www. This folder should contain subfolders for the aerial and semantic maps as well as the. Among other things this library allows us to predict the future movement of cars based on historical observations. We can change the rasterizer by building a new one and an accomodating dataset for it like so And now we can do the exact same thing to view the agents by switching from an EgoDataset to an AgentDataset The below animation code is taken from here https www. Note that as per the competition rules internet is not allowed in this competition so we cannot use pip to install packages. com c lyft motion prediction autonomous vehicles discussion 178323 Augmentation It seems there is a large gap between training loss and validation. It will become clear how to manipulate this cfg as we go through the notebook This configuration file is used to load the data by using a LocalDataManager object which resolves relative paths from the configuration file using the L5KIT_DATA_FOLDER env that we set earlier to extract the. target_position into pixel coordinates and then call draw_trajectory which can be used to draw predicted trajectories as well. ", "id": "tuckerarrants/lyft-eda-training", "size": "9555", "language": "python", "html_url": "https://www.kaggle.com/code/tuckerarrants/lyft-eda-training", "git_url": "https://www.kaggle.com/code/tuckerarrants/lyft-eda-training", "script": "l5kit.evaluation pytorch_neg_multi_log_likelihood_batch DataLoader albumentations pyplot pyplot as plt IPython.display animate run_prediction build_rasterizer ChunkedDataset l5kit.geometry TARGET_POINTS_COLOR numpy write_pred_csv animation sample_dataset animate_solution l5kit.visualization resnet18 nn draw_trajectory tqdm l5kit.data transform_points Dict typing pytorch_neg_multi_log_likelihood_single LyftModel(nn.Module) LocalDataManager forward AgentDataset show_images Tensor rc matplotlib EgoDataset torch.utils.data display clear_output HTML __init__ l5kit.rasterization optim torch torchvision.models.resnet l5kit.dataset ", "entities": "(('EgoDataset', 'AV annotations'), 'iterate') (('zarr which', 'Python zarr module'), 'file') (('we', 'RGB just images'), 'use') (('predictions', 'which'), 'mask') (('we', 'packages'), 'note') (('which', 'predicted trajectories'), 'target_position') (('raster', 'other pixels'), 'encode') (('we', 'augmentations'), 'use') (('we', 'below procedure'), 'file') (('mask', 'mask'), 'provide') (('now Model We', 'DataLoader'), 'pass') (('you', 'raster_size'), 'need') (('which', 'chunked'), 'use') (('below', 'Peter https www'), 'be') (('Update', 'discussion post'), 'need') (('lyft l5kit tree master examples', 'Raw Data'), 'com') (('now how we', 'hand'), 'allow') (('l5kit', 'ease'), 'be') (('lyft l5kit tree master now we', 'this'), 'com') (('We', 'rasterizer output'), 'need') (('Data data', 'competition'), 'format') (('We', 'CutOut CourseDropout'), 'reduce') (('us', 'historical observations'), 'allow') (('It', 'great competition'), 'be') (('import training yet especially when we', 'CNN'), 'be') (('only 4', 'dataset'), 'note') (('it', 'Data raw'), 'file') (('one pixel', 'real world'), 'be') (('model', 'raster size'), 'edit') (('it', 'full speed'), 'be') (('we', 'our'), 'iterate') (('training loop We', 'torch'), 'discussion') (('you', 'torch'), 'define') (('zarr files', 'NumPy array most traditional operations'), 'support') (('I', 'notebook https www'), 'enable') (('Most', 'examples https folder here github'), 'overview') (('Now we', 'below configuration'), 'be') (('one', 'raster images'), 'add') (('you', 'pixel_size'), 'com') (('now after we', 'baseline model'), 'save') (('why Lyft', 'utility script'), 'be') (('so I', 'it'), 't') (('zarr file', 'given vehicle'), 'contain') (('it', 'PyTorch Training source https Main github'), 'com') (('You', 'more layers'), 'set') (('you', 'image'), 'be') (('you', 'raster_size'), 'combat') (('you', 'notebook https www'), '5') (('we', 'earlier the'), 'become') (('we', 'EgoDataset'), 'let') (('l5kit l5kit', 'driving autonomous applications'), 'be') (('that', 'useful training'), 'agents_mask') (('Rasterizer Test dataset dataloader Build Rasterizer', 'random sample'), 'training') (('folder', 'aerial maps'), 'contain') (('zarr files', 'Visualization Main source https train_data_loader key github'), 'be') (('following code', 'Peter https again www'), 'take') (('com lyft motion vehicles prediction autonomous 178323 It', 'training large loss'), 'c') (('animation below code', 'https www'), 'change') (('which', 'PyTorch Dataset'), 'object') (('we', 'the'), 'dictionary') (('we', 'L5KIT_DATA_FOLD env variable'), 'expect') (('we', 'computer regularly screen'), 'file') "}