{"name": "handling multimodal distributions fe techniques ", "full_name": " h1 Tabular Playground 1 Bimodal Regression h1 Problem Definition h1 Modality h1 Data Overview h2 Target h2 cont Features h1 Model Baseline h1 Feature Engineering Techniques h2 Gaussian Mixture Modelling GMM h2 Binning h2 Statistical Features h2 Deep Feature Synthesis h2 Summary h1 EDA h1 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "Therefore we do not necessarily need to transform the target to fit a normal distribution. It is an unsupervised learning algorithm. 00081 DFS divide_numeric 0. For example if you have two features hours_spent_writing_kernels and number_of_kernels you could combine them by dividion to get a new feature average_time_to_write_a_kernel hours_spent_writing_kernels number_of_kernels. read_csv Configurations Unimodal Bimodal instantiate a second axes that shares the same x axis otherwise the right y label is slightly clipped Multimodal instantiate a second axes that shares the same x axis otherwise the right y label is slightly clipped Drop one outlier instantiate a second axes that shares the same x axis otherwise the right y label is slightly clipped for i var in enumerate train_df. This is called multimodal distribution. Hopefully you now have a few ideas on how to investigate the features. Discussion When you have bimodal distribution https www. In this section we will be exploring different feature engineering techniques. The most commonly known distribution is unimodal with only one peak. what is going on We definitely need to dig deeper here before we can start building our model. The baselibe in Tawara https www. We should probably drop it since it is only one single data point. com ttahara tps jan 2021 gbdts baseline instantiate a second axes that shares the same x axis we already handled the x label with ax1 otherwise the right y label is slightly clipped Initialize variables Prepare training and validation data Define model Calculate evaluation metric Root Mean Squared Error RMSE Make predictions Calculate evaluation metric for out of fold validation set Average predictions over all folds instantiate a second axes that shares the same x axis otherwise the right y label is slightly clipped ncols ncols. com c bnp paribas cardif claims management discussion. Model BaselineAlothough we did not do any feature engineering so far let s create a simple LightGBM baseline to get a benchmark first. If we look at the distributions of our predictions versus the ground truth we can see that our model is doing quite poorly. Let s try another idea What happens if we use the classes we got from the GMM to separate each feature BinningWork in progress Statistical FeaturesAnother common approach is to create new features from the data s statistics mean sum std etc. You can list out all possibilities with the function list_primitives. The out of fold OOF RMSE score is 0. Differencing means that we take the difference between two consecutive datapoints of a column. com ttahara tps jan 2021 gbdts baseline will be the base for our experiments. com willkoehrsen automated feature engineering tutorial. Without further context we are given some features with continuous values to predict a continuous target. For below plots we can see some odd distributions all cont features show multiple peaks with no sign of a normal distribution. However in this challenge we do not have any information about the features. This is probably also the most comfortable distribution to work with. com c santander customer transaction prediction discussion 86951 Since the features have multimodal distributions it would be worthwhile checking what happens if we add a GMM feature for each cont feature. 000089 Statistical Features 0. If you need more inspiration I recommend the discussion section of the BNP Paribas Cardif Claims Management Competition https www. Decision trees are insensitive to the targets distribution. Now we have some feature distributions with multiple peaks. EDAFrom above section we understand the urgency of gaining some insights from the data. Also don t get it confused with multimodal learning which describes problems with mixed feature modalities such as pictures and text. probplot train_df var plot ax i 2 Baseline model parameters copied from https www. they are highly correlated to each other e. 000094 GMM separated 0. Data Overview Target It looks like an overlay of two different distributions when the data distribution has two peaks it is called bimodal distribution. You can see that we end up with 196 features. 000115 Binning 0. com ttahara s notebook TPS Jan 2021 GBDTs Baseline https www. If you have two peaks it is called bimodal and if you have three or more peaks then it is called multimodal. There is no missing data but instead we have a different obstacle that we have to overcome bimodal distribution of the target variable and multimodal distributions of the features. Feature Engineering TechniquesWe have seen that our baseline model in theory is able to model the bimodal distribution of our target. boxplot train_df var ax ax i 1 stats. Log transformFrom the above experiments we can see that cont2 cont3 cont4 and cont13 seem to be the most important features for our model. How about dividing each feature by another feature using the primitive divide_numeric. In the Data Overview Data Overview we saw that the features have a low absolute correlation to the target. Let s see if this is true. If you wanto read more about this topic in depth I highly recommend this kernel Automated Feature Engineering Tutorial https www. In the following minimal example we have three features and a target that has a bimodal distribution as shown in the plot. Let s use our baseline model as is and use it for modelling the data. We can somewhat automate this with Deep Feature Synthesis. Let s see if we can get any interesting insights if we difference the features with np. Can tree based models handle bimodal distributions TL DR Yes tree based models in general should be able to handle bimodal distributions. For simplicity reasons the target equals feature1 if feature3 0 and else the targets equals feature2. Let s try a couple of primitives. Therefore we would expect tree based models to be able to handle bimodal distributions without any transformations as well. There is exactly one data point with a target value of 0 this looks very much like an outlier. If we were given the names of each feature we could start by creating new features based on our intuition. startswith cont sns. With the newly added features we get a RMSE of 0. This score is quite bad since RMSE of 0 would be ideal. 000094 improvement to our baseline and our distribution is still not close to the distribution of the ground truth. However for 14 features that will take a lot of time. For this purpose we will concatenate the training and the testing data and sort them by the column id. Deep Feature SynthesisAnother common approach is creating new features by combining features with basic mathematical operations addition subtraction multiplication division. 703148 this is our benchmark for the next steps. Additionally there seems to be a highly correlated cluster consisting of cont1 and cont6 through cont13. Tabular Playground 1 Bimodal Regression Problem Definition Problem Definition Modality Modality Data Overview Data Overview Target Target cont Features cont Features Model Baseline Model Baseline Feature Engineering Techniques Feature Engineering Techniques Gaussian Mixture Modelling Gaussian Mixture Modelling Binning Binning Statistical Features Statistical Features Deep Feature Synthesis Deep Feature Synthesis Summary Summary EDA EDA Problem DefinitionIn this challenge we are asked to build a regression model. Let s also try it for multiplication SummarySo far the feature engineering techniques only gave us minor improvements OOF RMSE Delta to Benchmark Benchmark 0. Gaussian Mixture Modelling GMM We can use Gaussian Mixture Modelling to separate the two distributions. The data does not seem to be skewed and therefore does not necessarily need to be transformed if non tree based models are used for tree based models this would not matter anyways. cont FeaturesLet s look at the cont features in bulk. distplot train_df var ax ax i 0 sns. From a top level point of view we can see that none of the features seem to be correlated to the target and cont14. However we can also see that this highly depends on the quality of our features. 00004 In contrast to the other feature engineering techniques with the binning features we get a worse score than with the other techniques. We already saw that our target variable has a bimodal distribution. This way we can practice on focussing on the data without requiring any specific domain knowledge because the column names cont do not indicate any further information. An idea would be to randomly combine features and hoping to see a correlation to our target. Happy Kaggling Submission linear algebra data processing CSV file I O e. 00003 DFS multiply_numeric 0. ModalityIn this challenge we will learn about the modality of a distribution. 703148 n a GMM class 0. Let s explore them a little more. From the below results we can see that the LightGBM model is able to handle the bimodal distribution. There are unimodal bimodal and multimodal distributions. Therefore we cannot create new features by intuition. we lose some information when binning features. This is probably due to the fact that although we have more features now a. You can find the modality of a distribution by counting the number of its peaks. collinearity between cont1 and cont1_bin10 and b. ", "id": "iamleonie/handling-multimodal-distributions-fe-techniques", "size": "8923", "language": "python", "html_url": "https://www.kaggle.com/code/iamleonie/handling-multimodal-distributions-fe-techniques", "git_url": "https://www.kaggle.com/code/iamleonie/handling-multimodal-distributions-fe-techniques", "script": "sklearn.mixture seaborn scipy GaussianMixture lightgbm mean_squared_error KFold get_gmm_class_feature matplotlib.pyplot run_model visualize_results featuretools sklearn.model_selection pandas sklearn.metrics stats numpy ", "entities": "(('s', 'data'), 'let') (('idea', 'target'), 'be') (('I', 'BNP Paribas Cardif Claims Management Competition https www'), 'recommend') (('distribution', 'ground truth'), 'improvement') (('baseline model', 'target'), 'see') (('I', 'highly kernel'), 'recommend') (('this', 'very much outlier'), 'be') (('we', 'column'), 'concatenate') (('You', 'peaks'), 'find') (('cont3 cont2 cont4', 'most important model'), 'log') (('Hopefully you', 'how features'), 'have') (('Now we', 'multiple peaks'), 'have') (('we', 'np'), 'let') (('quite RMSE', '0'), 'be') (('that', 'time'), 'however') (('tree', 'transformations'), 'expect') (('features', 'target'), 'see') (('we', 'other techniques'), '00004') (('we', 'continuous target'), 'give') (('Therefore we', 'intuition'), 'create') (('we', 'features'), 'have') (('we', 'feature engineering different techniques'), 'explore') (('LightGBM model', 'bimodal distribution'), 'see') (('com ttahara tps gbdts jan 2021 baseline', 'experiments'), 'be') (('we', 'data'), 'understand') (('most commonly known distribution', 'only one peak'), 'be') (('none', 'target'), 'see') (('target already variable', 'bimodal distribution'), 'see') (('Decision trees', 'targets distribution'), 'be') (('we', 'sum std statistics mean etc'), 'let') (('Statistical Deep Feature Synthesis Deep Feature Synthesis Summary EDA Problem Summary we', 'regression model'), 'Model') (('we', 'distribution'), 'modalityin') (('slightly i', 'enumerate'), 'instantiate') (('we', 'column'), 'mean') (('they', 'highly other e.'), 'correlate') (('we', 'when features'), 'lose') (('we', '196 features'), 'see') (('it', 'two peaks'), 'Target') (('model', 'ground truth'), 'see') (('Therefore we', 'normal distribution'), 'need') (('so far s', 'benchmark'), 'do') (('cont features', 'normal distribution'), 'see') (('which', 'such pictures'), 'get') (('this', 'next steps'), '703148') (('column names cont', 'further information'), 'practice') (('Happy Kaggling Submission', 'linear algebra data CSV file'), 'process') (('We', 'Deep Feature Synthesis'), 'automate') (('feature engineering far techniques', 'OOF RMSE Benchmark Benchmark'), 'let') (('we', 'cont feature'), 'com') (('we', 'intuition'), 'start') (('it', 'probably it'), 'drop') (('then it', 'three peaks'), 'call') (('also this', 'features'), 'see') (('feature3 0 else targets', 'feature2'), 'equal') (('When you', 'bimodal distribution https www'), 'discussion') (('we', 'more features'), 'be') (('You', 'list_primitives'), 'list') (('that', 'plot'), 'have') (('Deep Feature SynthesisAnother common approach', 'operations addition subtraction multiplication basic mathematical division'), 'create') (('Gaussian Modelling We', 'two distributions'), 'mixture') (('we', 'features'), 'be') (('this', 'tree based models'), 'seem') (('definitely deeper here we', 'model'), 'need') (('var plot Baseline model probplot train_df i 2 parameters', 'https www'), 'ax') (('y same axis otherwise right label', 'ncols slightly clipped ncols'), 'instantiate') (('TL DR Yes based models', 'bimodal general distributions'), 'handle') (('we', '0'), 'get') (('you', 'new feature'), 'combine') "}