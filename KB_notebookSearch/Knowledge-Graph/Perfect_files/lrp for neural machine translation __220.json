{"name": "lrp for neural machine translation ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. read_csv Input data files are available in the. the below is the mathematical model for LSTM and dense layer we are directly sending the word embedings of input into the for loop where in the above cell we have taken the word embedings for a input sequence. 0 can be used for sanity check word embedding dimension number of classes gate g only gate g only CASE 1 positive AND negative scores occur deepest color is positive deepest color is negative CASE 2 ONLY positive scores occur CASE 3 ONLY negative scores occur forward pass number of classes initialize gate g only gate g only format reminder lrp_linear hin w b hout Rout bias_nb_units eps bias_factor. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. add Dense tar_vocab activation softmax model. add Embedding src_vocab n_units input_length src_timesteps mask_zero True attention_probs model. 0 and bias_nb_units D global network relevance conservation if bias_factor 1. add Dense tar_vocab activation softmax compile model summarize defined model ATTENTION PART STARTS HERE ATTENTION PART FINISHES HERE prepare training data prepare validation data define model fit model generate target given source sequence evaluate the skill of the model translate encoded source text calculate BLEU score initialize gates i g f o pre activation gates i g f o activation gates i g f o pre activation gates i g f o activation gates i g f o pre activation gates i g f o activation word embedding dimension indices of the gates i f o for i in hout 5 print i shape 1 M shape D M print Numerator shape 1 M shape D M shape D Note local layer relevance conservation if bias_factor 1. load doc into memory open the file as read only read all text close the file split a loaded document into sentences clean a list of lines prepare regex for char filtering normalize unicode characters tokenize on white space convert to lowercase remove punctuation from each token remove non printable chars form each token remove tokens with numbers in them store as string save a list of clean sentences to file load dataset split into english german pairs clean sentences save clean pairs to file spot check load a clean dataset save a list of clean sentences to file load dataset reduce dataset size random shuffle split into train test save load a clean dataset load datasets fit a tokenizer max sentence length prepare english tokenizer prepare german tokenizer encode and pad sequences integer encode sequences pad sequences with 0 values one hot encode target sequence define NMT model define NMT model summarize defined model compile model model. ", "id": "sitaramireddy1994/lrp-for-neural-machine-translation", "size": "220", "language": "python", "html_url": "https://www.kaggle.com/code/sitaramireddy1994/lrp-for-neural-machine-translation", "git_url": "https://www.kaggle.com/code/sitaramireddy1994/lrp-for-neural-machine-translation", "script": "numpy.random create_tokenizer lrp_linear keras.layers load_doc keras.models keras.preprocessing.sequence html_heatmap word_for_id IPython.display TimeDistributed to_categorical plot_model keras.utils.vis_utils getRGB numpy lrp define_model newaxis nltk.translate.bleu_score predict_sequence array Dense clean_pairs build_model argmax pad_sequences keras.callbacks pickle span_word Tokenizer Embedding unicodedata LSTM corpus_bleu max_length matplotlib.pyplot RNN ModelCheckpoint Sequential evaluate_model shuffle pandas save_clean_data to_pairs load_model load keras.preprocessing.text Model RepeatVector Input encode_output merge display HTML keras.utils rescale_score_by_abs normalize load_clean_sentences encode_sequences rotate dump newaxis as na soft_max ", "entities": "(('we', 'input sequence'), 'be') (('NMT model', 'model compile model defined model'), 'open') (('It', 'python docker image https kaggle github'), 'come') (('you', 'output'), 'list') (('read_csv Input data files', 'the'), 'be') (('forward number', 'format lrp_linear hin w only b'), 'use') (('1 M shape D print D M Numerator shape 1 M shape M shape D local layer', 'conservation'), 'add') "}