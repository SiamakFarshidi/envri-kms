{"name": "masters final project model lb 1 392 ", "full_name": " h1 Kaggle LANL Earthquake Prediction Modeling h3 Kevin Maher h3 Regis University MSDS696 Data Science Practicum II h3 Associate Professor Dr Robert Mason h4 May 2 2019 h4 Spring 2019 In partial fullfillment of the Master of Science in Data Science degree Regis University Denver CO h3 Introduction h3 Publication h3 Problem Approach h3 Processing Issues h3 Code Setup h3 Feature Creation h3 Feature Creation using Wavelets h3 Models h3 Feature Selection h3 Individual Model Cross Validation Results h3 Hyperparameter Tuning h3 Model Stacking h3 Lessons Learned h3 Future Research Possibilities h3 Conclusions h3 Acknowledgements h3 Test Environment h3 Author and License Information h3 References ", "stargazers_count": 0, "forks_count": 0, "description": "These are the LightGBM and XGBoost models both run with substantially different cross validation methods. These were CatBoost Prokhorenkova Gusev Vorobev Dorogush Gulin 2017 and genetic programming via the gplearn library Stephens 2016. LightGBM training times then became almost identical to those of XGBoost. Consequently the individual functions often require many hours or days to run even when using multiprocessing. When done this provided 20 features that passed a p value test with a threshold of at or below 0. 2011 XGBoost Chen Guestrin 2016 and LightGBM Ke et al. The sta_lta refers to the short term average divided by the long term average. CatBoost given its good reputation and being a modern gradient boosting machine is also worth further study. One should be cautious in doing so it has proven all to easy to drastically overfit the training data and trials too numerous to document here have resulted in superb CV scores but lessened Kaggle leader board results. Because this 150k sample overlap is so small compared to the 100 million plus samples in each 1 6th slice of the signal leakage is effectively negligible. First there are four models with 24 000 data rows that have performed well on the Kaggle leader board. 001 MAE in the Kaggle public leader board score. Building an effective model for a Kaggle challenge where the data is noisy and the leader board that utilizes only 13 of the potential test data is a significant challenge. EDA showed that most if not all of the signal above the 20 000 frequency line was likely to be noise so the frequency bands will concentrate on the region below that. The data above may be too small to make many observations regarding the relationship between CV score and Kaggle public leader board results. Only a result function containing the genetic algorithm s output mathematical functions relationships and coefficients seem to be given by the author. When scripts take overnight or even days to run having more computers available is clearly advantageous and allows trying more ideas on the project. After the competition is over and the full test set is made available for scoring this problem would be resolved for non contest entries. Also working on data chunks that represent only a portion of the very large input data set means that the whole data set is not loaded into memory multiple times once for each process. Build six sets of random indices. 7 Anaconda environments. Takes overnight to run 6 processes on the input data. 440 under similar conditions. The metric used by Kaggle in this competition is Mean Absolute Error MAE and thus a lower value is better with zero representing a perfect fit Bilogur. However the script CV scores appear to be just above 2. The Preda 2019 script references a script by Andrew and one by a Kaggle user named Scirpus. Code SetupBelow are the imports needed to run the code. Please see the table below for examples of CV and public leader board score. Using 6 fold cross validation with slices that do not overlap eliminates the leakage because of the splitting of the signal into 6 segments before the random selection was performed. Exploratory data analysis EDA is performed in a separate Jupyter notebook located with this file in the github repository https github. Ensemble Methods Elegant Techniques to Produce Improved Machine Learning Results. XGBRegressor n_estimators 1000 learning_rate 0. ModelsRun a LightGBM model and save for a submission to Kaggle. While the Kaggle public leader board appears to be the best test set for model ranking currently available there might be a lot of variance in the results when the remaining 87 of the test data is revealed. Long run times for many functions and algorithms made the project more of a challenge than it otherwise might have been. best_iteration_ mean absolute error root mean squared error training for over fit num_iteration model. Kaggle limits submission models to two per day and this proved to become a limitation for experimentation as the project due date approached. This allowed some parallel development to take place and helped when there were more ideas available than CPU power to investigate them. Removing some 150 features by this method provided a very tiny increase 0. com artgor earthquakes fe more features and samples Mart A. Model StackingSeveral issues affect possible model stacking given the state of this project. Genetic programming in Python with a scikit learn inspired API. Many of the feature creation ideas here appear to owe their origins to the Lukayenko 2019 script and it s cited predecessors. Partly because of the extensive exploration of slicing the data into 4194 non overlapping slices in the Kaggle kernels by other challenge participants and partly to set out on an individual exploration of modeling this data a different approach is tried in the primary models presented here. As noted below these models usually scored higher individual scores on the Kaggle leader board. While indices for slicing data out of the model were chosen randomly they were chosen from 6 slices of the original model data. This code will work best if there is available at least a four core 8 hyper thread CPU it was primarily tested on a Windows 10 operating system with an AMD Ryzen 7 CPU 8 cores 16 logical threads and 16GB of RAM. Keeping up will probably require new breakthroughs. Note the use of the Scikit Learn Pedregosa et al. A model stack built by simple averaging was submitted to the Kaggle leader board for scoring using the two best models by this author plus output from the Scirpus 2019 script. io en stable van der Walt S. Scipy considers this p value to be reasonably reliable for sample sizes above 500 which clearly is true for the models presented here Scipy 2019. Large Scale Machine Learning on Heterogeneous Systems. When I tried the Preda 2019 script run from an IDE this author obtained a public leader board score on Kaggle of 1. First it tends to help spread the random generation of data across the signal without risk of bunching too many slices into a compact region of the original signal. Code was tested using an IDE not this notebook. 24 000 data rows were created in a stratified and then randomized manner. Convert to float Copy for LTA Compute the STA and the LTA Pad zeros Avoid division by zero by setting zero values to tiny float low pass filter high pass filter band pass filter FFT transform values train_X create_features_pk_det seg_id seg train_X start_idx end_idx 1 on success 0 if fail just tqdm in IDE train_X create_features_pk_det seg_id seg train_X start_idx end_idx do something like this in the IDE call the functions above in order if __name__ __main__ function name predictions mean absolute error root mean squared error index needed it is seg id do this in the IDE call the function if __name__ __main__ lgb_base_model model xgb. The Scipy pearsonr function provides a p value that takes account of the sample size of the model. Both Preda 20190 and Scirpus 2019 scripts report lower Kaggle public leader board scores than their CV scores. In spite of the information leakage and possible overfitting this method produced the best individual model Kaggle public leader board score in this report of 1. The Scirpus 2019 genetic programming model result is also as obtained by this author in testing and agrees with the Kaggle leader board result reported for this script by its original author at the time the script was run by this author. Retrieved from https scikit learn. Feature CreationFunction to split the raw data into 6 groups for later multiprocessing. The predecessor scripts can be found from citation links in these scripts and full links are referenced below. IPython A System for Interactive Scientific Computing. This is probably caused by information leakage between the samples because they are derived from signals that overlap in time. 082 average using LightGBM. Similarly the number of leaves was decreased and the minimum data in a leaf increased for the same reason in LightGBM. The remainder are reserved for the final scoring that will be done after the competition concludes and after this course is finished. It would have been helpful to try more hyperparameter tuning. Feature reduction via Pearson s correlation appears to have had a moderate beneficial effect upon most of the individual models as evidenced by Kaggle public leader board scores for equivalent individual models. 9 colsample_bytree 0. Function to restructure wavelet signal peak detection so that the peaks are ordered by index rather than peak value. The script by Scirpus is interesting in being a very effective genetic programming model Scirpus 2019. It seems though that CV score is not a good predictor of the eventual Kaggle public leader board score and this has caused significant challenges throughout the project especially with hyperparameter tuning. These are obtained from 6 simple slices of the original data each slice is used to randomly create 4k data rows. This was actually not true for the randomly sampled CV model with Light GBM. The feature builder function took so long that it was run as 6 concurrent processes in order to speed it up. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD 16 785 794 New York NY USA. 556 for the LightGBM model presented in that script. com tensorflow tensorflow 0. In the table below only one model fell in Kaggle scoring when the feature set was reduced and the divergence was only 0. Thus we note that these are laboratory earthquakes not real earthquakes. An example of this is the Preda 2019 kernel but there are many other excellent scripts using this approach that the reader might review on Kaggle. Feature Creation using WaveletsFeature creation by using wavelets to extract peak value and index information from the signal was also explored. Hyperparameter tuning was therefore less extensive than would be desirable. The acoustic data provided is used to create statistical features which are fed into supervised learning algorithms which then seek to predict the time until an earthquake from test signals. Unfortunately the C code that the genetic algorithm has been written in does not appear to be publicly available. The test set 2624 samples was run as a single process over two days. Lessons LearnedSeveral modeling types that this author had no previous experience with were tried on the 24k row data features. Ensembles of models may therefore perform best where their individual weaknesses and variance tend to somewhat cancel out Demir. Filter design helper functions. Individual Model Cross Validation ResultsIndividual model cross validation CV results are shown below both for the models presented in this project and two reference models taken from the Kaggle Kernels. I have used the Jupyter notebook here only for documentation and presentation purposes. Problem ApproachThis problem has been approached here by regression modeling. These were sourced from a Kaggle kernel script Preda 2019. WIndowed features were not subjected to the digital filters since the windowing is a type of filter. 0 tensorflow g3doc index. Processing IssuesMany of the processes and functions below are very long running possibly taking overnight or days to complete. Retrieved from https chromium. PublicationIn an effort to comply with both university and Kaggle requirements this Jupyter notebook is being published on GitHub and on Kaggle. Retrieved from https keras. SciPy Open Source Scientific Tools for Python Retrieved from http www. This function joins the results of the multiprocessing build into one training set for model building. While the features remain in the model it is arguable that their benefit was not worth 5 days of compute time. It is probably not worthwhile to spend time on Support Vector Machines and Nearest Neighbors algorithms in other regression models within the author s experience these seem antiquated and appear to under perform newer gradient boosting decision tree based methods such as LightGBM or XGBoost. com Upvotes and or github stars appreciated This code herein has been released under the span style color 337AB7 text decoration none text underline none Apache 2. Due to extremely high computational time this was only performed for the 24 000 sample models. Experience below will show both the successes and challenges of this alternate method of feature creation. Yet the XGBoost model in question had a Kaggle public leader board MAE of 1. Examination of other scripts on the Kaggle kernels section leads this author to believe that this is typical for scripts where the data rows were bread sliced from the original acoustic signal and the 4914 resulting data rows do not overlap or leak information. The parameter proc_id is the multiprocessing identifier passed in by the multiprocessing caller. The difference does not appear to be significant and might change if further parameter tuning were performed. If it fails part way down due to a path name being wrong then it is not necessary to re run every function. Manager function to build the feature fields that are extracted from the acoustic signal for the training set only. Since statisticians generally consider p values below 0. It is currently unknown whether this will change when the full private leader board is revealed at the end of the competition. CatBoost unbiased boosting with categorical features. While there are 2624 test signals provided by Kaggle only 13 341 are used for the public leader board Rouet Leduc et. A problem is that this algorithm is very computationally expensive. This training data is accompanied by a ground truth time to failure time until the next earthquake for each acoustic sample. These scripts appear to underfit the public leader board in the sense that cross validation CV scores tend to be higher worse than the public leader board score. The Preda 2019 model is his LightGBM single model as run by this author. Function to add a slope value that adds a slope representing the peak vs its distance from ths signal end. Second it helped greatly with computational time and memory usage because multiprocessing can then be employed. Retrieved from http papers. The idea of creating the models in this way was taken from the Preda 2019 script as well as many others too numerous to cite that are present on the Kaggle website. This model scored 1. set for local environment set for local environment the test signals are 150k samples long Nyquist is thus 75k. This appeared to help even with gradient boosted decision tree algorithms and is necessary with many other machine learning algorithms. This slicing accomplishes several objectives. cc paper 6907 lightgbm a highly efficient gradient boosting decision tree Prokhorenkova L. While 60k estimators was eventually chosen from experience the model would appear to continue to train up to 100k estimators or beyond. Model Fit Metrics undated. Multiprocessing in Python on Windows and Jupyter Ipython Making it work. 437 for the XGBoost models. AcknowledgementsI feel compelled to note again the contributions of the Preda 2019 Lukayenko 2019 and Scirpus 2019 scripts to this work. The output is a usable training set for both features and targets the earthquake prediction times. com machine learning ensemble methods machine learning Jones E. Some of these functions can take a long time to run so it is recommended that it be done from an IDE and one function at a time. The training data set contains more than 629 million acoustic signal samples and is 10GB in size so there is a lot of data to process. LightGBM A Highly Efficient Gradient Boosting Decision Tree. The test signals are all 150k samples in length thus it seems best to extract 150k sample sets from the training data. Note that the signal is 150k lines long hence by the Nyquist criteria there are 75k valid frequency lines before aliasing. Put the feature creation functions together and create the features. Another item to note is that the models mentioned herein are averaged ensembles of a cross validation CV. Hyperparameter TuningEffective hyperparameter tuning proved to be a very large challenge in this project. Retrieved from https www. com gpreda lanl earthquake eda and prediction Pedregosa et al. All of this helps to avoid crashes and allow the feature building portion of the script to run overnight. Because it uses genetic programming rather than the decision trees used by LightGBM and XGBoost it offers possible diversity to the model. Retrieved from https docs. ConclusionsIt might be tempting to build and evaluate models based strictly on the training data in projects outside of the Kaggle competition. As noted above the feature importance from the LightGBM model was abandoned as a feature selection mechanism in favor of Pearson s correlation. Retrieved from https gplearn. Define some constants. The author s inexperience with both of these algorithms appear to be the primary culprit. More success was achieved by calculating the Pearson s correlation of the features with the target time to failure. It would be easy to keep more computers busy on this project and if it were to be repeated I would try to locate more resources. Changes the Fourier transform to evaluate based on magnitude and phase and also to do so in a bandwidth limited manner as compared to the reference scripts. This is for several reasons. The code is written using Python 3. These were added to allow for obtaining statistics on the signal in a bandwidth limited manner. Or one could continue to use the Pearson s approach to feature generation and experiment with various cutoff values. Andrews Script plus a Genetic Program Model. Multiprocessing allowed the main feature creation to run overnight instead of possibly requiring days which might have been required with a single process. Manager function to call the create features functions in multiple processes. At the time of submission this was good for the top 1 of 3200 plus competitors. This makes the feature building more memory efficient. As an alternate view one could argue that the CV is needed anyway so why not take advantage of it as a direct model. ReferencesBilogur A. Keras The Python Deep Learning library. The goal of the project is to predict the time that an earthquake will occur in a laboratory test. This allows usage of all of the training data for creating a Kaggle submission file while still reserving validation holdout sets. LANL Earthquake Prediction. 667 where only 2 3 of the columns are selected for a split at each tree level was chosen because the documentation for XGBoost indicates that this helps with overfitting. The author please requests a citation for the use or derivation of this work. It was fortunate to have two reasonably powerful computers available for much of the project. Then 24 000 data rows are created with 900 features extracted from the signal. There are two possible approaches to model cross validation CV because of the stratification used by the multiprocessing. This is based on the EDA where the magnitude of the Fourier transform looks important but the phase response seems to be mostly noise. Slicing the data into 4194 chunks avoids overlap and possible information leakage between these slices as they then do not share any signal information. 0 random_state 777 fold_ n_jobs 12 verbosity 2 model. The exploratory data analysis notebook for this project will also be published in the same manner. The simulated earthquakes tend to occur somewhat periodically because of the test setup but this periodicity is not guaranteed to the researcher attempting to predict the time until an earthquake. Many of these libraries request a citation when used in an academic paper. This algorithm uses a Mexican Hat wavelet in the SciPy library and by interference with Mexican Hat wavelets finds the peak and peak index locations for the signal. 67 reg_lambda 1. While Keras and Tensorflow work very well on speech and vision applications it does not to this author apppear fully competitive with the best tree based gradient boosting models in a regression problem. 0 open source license. The main function to create features. Random CV row selection on 24k rows of data where the rows do overlap in the original signal changes the picture significantly from CV results reported for the above. Machine Learning 45 1 5 32. Alternate choices for the width and number of frequency bands have not been investigated and might prove worthwhile. This method also helps create more accurate models by averaging the results of different splits of the training data into training and validation sets. This resulted in worse overfitting and a lower CV score. Also there is the Scirpus 2019 script to consider based upon 4194 sample rows. Build features from the Kaggle test data files. Please review the EDA for additional perspective on the problem. An XGBoost model improved from 1. Scikit learn Machine Learning in Python. Retrieved from https arxiv. Helper functions for feature generation. SciPy is utilized to provide signal processing functions especially filtering and for Pearson s correlation metrics Jones E. Proceedings of the 9th Python in Science Conference 51 56. Robert Mason May 2 2019 Spring 2019 In partial fullfillment of the Master of Science in Data Science degree Regis University Denver CO IntroductionPresented here are a set of models for the Kaggle LANL Earthquake Challenge Rouet Leduc et. This perhaps could have been more easily acomplished with the skiprows and nrows parameters of the Python Pandas read csv function rather than creating 6 new files. However it was difficult to know where to set a threshold for feature removal due to their being few obvious cut points in the feature scores. The true origin of this modeling approach is unknown to this author. It appears that for this challenge problem the CV score is not a reliable predictor of public leader board score at least for the small current public leader board test data sample. Added frequency bandwidth limiting to the time domain features. Kaggle leader board values are those obtained by this author in testing. In order to avoid having to load the whole 629m data set into memory 6 times only the smaller slices with 1 6 of the data were loaded one into each process. com residentmario model fit metrics Brieman L. This is a problem where it is difficult to score well. It is hoped that the diversity of model types and feature generation will help to stabilize the predictions submitted to Kaggle such that the model does generalize well when the full leader board is revealed at the end of the competition and after this university course has completed. Numpy is utilized to provide many numerical functions for feature creation van der Walt Colbert Varoquaux 2011. 05 as representing significance this value was chosen for the model s feature reduction algorithm. com scirpus andrews script plus a genetic program model Singhal G. The notebook was designed for a university course. It has not been tested and probably will not run in the Kaggle environment. Retrieved from https medium. For much of the semester the author worked with the random cross validation strategy. The indices did not survive the process but mostly the peak values and some slope values appear to have some merit. Thus in addition to random selection for cross validation working with 5 slices for training and 1 slice for validation as a 6 fold CV is also an option. A 6 fold XGBoost model where the folds did not overlap in the training signal time domain improved from 1. Also tried was increasing the number of data rows to 40 000. Computing in Science Engineering 13 22 30 DOI 10. com Vettejeep MSDS696 Masters Final Project. Given around 629m potential training samples one challenge is how best to extract effective but still computationally tractable training sets from the given signal. com blog 2016 01 07 big p little n Demir N. best_iteration_ num_iteration model. Future Research PossibilitiesPrincipal components analysis PCA appears to be worth trying especially if one were to apply the mass feature generation used here on 4194 sample data. This possibly could help with the large p small n issue that might arise if a model with only 4194 training rows was tried and 900 features obtained from splitting the signal up with digital filters. Note for example that this author s XGBoost model with essentially non overlapped cross validation had a CV score of 2. 437 better than obtained for the Preda 2019 LightGBM model. The Random Forest also did not perform as well as LightGBM and XGBoost on the feature generation set presented here. Butterworth 4 pole IIR filters are utilized to obtain the signal split into frequency bands. Actually in this model there is very slight leakage because this author was not fully cognizant of these possible effects and allowed a small 150k sample overlap between the 6 segments. The laboratory test applies shear forces to a sample of earth and rock containing a fault line. More features and samples. Semester time constraints made tuning efforts difficult as it would have required too many days to perform effective grid searches on the problem. It ensures relatively even coverage of the width of the input signal and it allows for multiprocessing so that the script runs in a reasonable time. Both suffered from long training times in this scenario and CV scores that were not encouraging. Most of the published kernel scripts that this author has reviewed on Kaggle use a data row creation method that slices the 629 million row acoustic input data evenly into 4194 non overlapping chunks of data that are equivalent in length to the 150k sample size of the Kaggle test samples. This changes the CV relationship back to that of the 4194 sample models reported and again the CV score is much worse than the Kaggle leader board score. This is a common regression metric. This is the variant of the model with feature elimination performed by Pearson s correlation. In terms of obtaining a good leader board score and to hopefully generalize well to the full leader board 3 models were averaged here and submitted to Kaggle. Realization that a sectionalized cross validation was also practical shortened LightGBM training times because the model eventually reached a point where it stopped improving on the validation data. These models required 6 hours to train with LightGBM and 30 minutes with XGBoost. By combining the models a score of 1. This produces the test file that will be used for prediction and submission to Kaggle. This Kaggle competition comes with cash prizes and this attracts many fine competitors so it will not be surprising that this result will fall as more entrants submit models. The Kaggle submission shown here was made on April 28th 2019. The training signal is provided by Kaggle in the form of a continuous acoustic signal that is over 629m samples long. Also tried were Random Forest Brieman 2001 decision tree based algorithm and a model based on the Keras Tenorflow Deep learning library Chollet et al. 6 and the library dependencies as noted below near the end of this document and in the imports. Even the current Kaggle public leader board result obtained is suspect because of the small amount of the test data that it contains but a better test set does not appear to be available at present. Journal of Machine Learning Research. This will be explored when model results are presented below. LANL Earthquake EDA and Prediction. org doc scipy reference generated scipy. Accuracy and Kaggle scoring position appear to be gained by using this technique at a significant cost in additional model training time and complexity. Inspired by script from Preda 2019 and Lukayenko 2019. best_iteration_ index needed it is seg id do this in the IDE call the function above if __name__ __main__ lgb_trimmed_model. Stratified random sampling will be performed on the data. This will also output feature importance. Instead of the Kaggle public leader board score being better than the CV score now the CV score is very low for random CV sampling. These features may have has a very small beneficial effect upon the model and a significant number of these features were deemed statistically significant by the Pearson s correlation performed in the feature reduction section of the modeling. Pandas is very helpful for its ability to support data manipulation and feature creation McKinney 2010. The user is left to decide how to extract information from the test signal in order to provide training data for their chosen machine learning algorithms. If the create_features_pk_det function is called to obtain wavelet generated peak detection features it may take two days to run. The function below can be called for either the training or test sets. Data Structures for Statistical Computing in Python. Best public leader board scores for LightGBM is 1. For example a LightGBM model with 8 fold random cross validation improved from an MAE of 1. Test Environment Author and License InformationKevin Maher Email Vettejeep365 gmail. This discloses my code which is being submitted and shared to my professor and class for grading. Because of limited time and observed overfitting hype parameter tuning was performed by theoretical changes in directions that might reduce overfitting. The author did not have time to fully investigate it and may have been hampered by a lack of experience with the algorithm. The signal constants define how the signal and Fourier transforms will be filtered to produce bandwidth limited features. Without them and their predecessor kernel scripts on Kaggle any progress made by this effort would have been far more difficult. XGBoost A scalable tree boosting system. com c LANL Earthquake PredictionScipy 2019. The trend feature is a linear regression on a portion of the signal. Feature SelectionEarly on feature selection was performed via feature ranking output from a LightGBM model. Another area of exploration is that the frequency bands used to create additional features were selected somewhat arbitrarily except for the understanding of the general frequency range desired that was obtained in the EDA by Fourier analysis. Running 6 processes on 24 000 samples required 3 days to complete. For this and the models that follow remember to adjust the number of jobs treads or processes based on the CPU capabilities available. This allows for selection of the section of the overall data on which to work. 0 seems best within 0. I believe that the Andrew script is the one by Andrew Lukayenko Lukayenko 2019. This may help the machine learning see the peaks in a more time ordered manner. 2017 libraries for machine learning and support. It is be best to transfer them to an IDE in order to run them. General Chairs Thirty first Conference on Neural Information Processing Systems NIPS 2017. Also some of the code uses multiprocessing and this can be troublesome if run from Jupyter Singhal 2018. If the create_features_pk_det function is called to obtain wavelet generated peak detection features it may take three days to run. For example colsample_bytree in XGBoost was set to 0. Because this made a computationally intensive approach to the problem even more computationally difficult this effort was quickly dropped. 1 max_depth 6 subsample 0. The Jupyter environment in which this project is presented is a descendant of the IPython environment originated by P\u00e9rez Granger 2007. These two outside models are the ones noted by Preda 2019 and Scirpus 2019. Computing in Science Engineering 9 21 29 DOI 10. fit X_tr y_tr predictions num_iteration model. The code has been written and run in Python 3. These methods give very different and opposing CV results but very similar Kaggle public leader board scores. Hyperparamater tuning was particularly difficult because of the disconnect between leader board score and cross validation scores. The NumPy Array A Structure for Efficient Numerical Computation. com grvsinghal speed up your python code using multiprocessing on windows and jupyter or ipython 2714b49d6facStevens T. Kaggle LANL Earthquake Prediction Modeling Kevin Maher Regis University MSDS696 Data Science Practicum II Associate Professor Dr. No final stopping point was ever found for this model. The Preda 2019 reference script had a CV of 2. ", "id": "vettejeep/masters-final-project-model-lb-1-392", "size": "33831", "language": "python", "html_url": "https://www.kaggle.com/code/vettejeep/masters-final-project-model-lb-1-392", "git_url": "https://www.kaggle.com/code/vettejeep/masters-final-project-model-lb-1-392", "script": "create_features build_fields lightgbm create_features_pk_det LinearRegression des_bw_filter_hp scale_fields split_raw_data xgboost stats numpy add_trend_feature mean_absolute_error tqdm hilbert scipy sklearn.linear_model lgb_base_model KFold lgb_trimmed_model hann convolve join_mp_build sklearn.model_selection pandas scipy.signal des_bw_filter_lp run_mp_build des_bw_filter_bp mean_squared_error multiprocessing classic_sta_lta build_test_fields sklearn.metrics StandardScaler build_rnd_idxs sklearn.preprocessing ", "entities": "(('signal', 'Nyquist 150k long hence criteria'), 'note') (('More success', 'failure'), 'achieve') (('function', 'model building'), 'join') (('user', 'machine learning chosen algorithms'), 'leave') (('that', 'test potential data'), 'build') (('data whole set', 'multiple times once process'), 'mean') (('result Only function', 'author'), 'seem') (('it', 'Lukayenko 2019 script'), 'appear') (('one', 'cutoff various values'), 'continue') (('very slight author', '6 segments'), 'be') (('Code SetupBelow', 'code'), 'be') (('that', 'CPU treads capabilities'), 'remember') (('Jupyter it', 'Windows'), 'make') (('methods', 'leader board very Kaggle public scores'), 'give') (('validation CV scores', 'leader board higher public score'), 'appear') (('table', 'leader board CV public score'), 'see') (('Exploratory data analysis EDA', 'github repository https github'), 'perform') (('so long it', 'it'), 'take') (('value', 'feature reduction algorithm'), '05') (('as well many too numerous that', 'Kaggle website'), 'take') (('error absolute root', 'fit num_iteration model'), 'square') (('model stack', 'Scirpus 2019 script'), 'submit') (('author', '24k row data features'), 'try') (('Keeping', 'probably new breakthroughs'), 'require') (('machine learning', 'more time ordered manner'), 'help') (('multiprocessing identifier', 'multiprocessing caller'), 'be') (('model', 'up to 100k estimators'), 'choose') (('author', 'cross validation random strategy'), 'for') (('where it', 'validation data'), 'realization') (('which', 'test signals'), 'use') (('algorithm', 'signal'), 'use') (('divergence', 'Kaggle scoring'), 'fall') (('one challenge', 'given signal'), 'be') (('author', 'algorithm'), 'have') (('that', 'Kaggle leader well board'), 'be') (('Regis University Denver CO IntroductionPresented', 'Kaggle LANL Earthquake Challenge Rouet Leduc et'), 'Mason') (('This', 'which'), 'allow') (('This', 'rather 6 new files'), 'read') (('Build', 'Kaggle test data files'), 'feature') (('This', 'many other machine learning algorithms'), 'appear') (('when remaining 87', 'test data'), 'appear') (('i', '_ _ name'), 'need') (('randomly they', 'model original data'), 'choose') (('periodicity', 'earthquake'), 'tend') (('Model Cross Validation ResultsIndividual model cross validation CV Individual results', 'reference Kaggle two Kernels'), 'show') (('It', 'project'), 'be') (('phase response', 'Fourier transform'), 'base') (('It', 'Kaggle probably environment'), 'test') (('random_state', '777 _ 12 verbosity 2 model'), '0') (('author', '1'), 'obtain') (('which', 'grading'), 'disclose') (('slice', '4k data randomly rows'), 'obtain') (('laboratory test', 'fault line'), 'apply') (('project', 'experimentation'), 'limit') (('This', 'Light GBM'), 'be') (('First it', 'original signal'), 'tend') (('therefore best where individual weaknesses', 'somewhat Demir'), 'perform') (('CV now score', 'CV very random sampling'), 'be') (('parallel development', 'them'), 'allow') (('Problem ApproachThis problem', 'regression here modeling'), 'approach') (('as more entrants', 'models'), 'come') (('556', 'script'), 'present') (('project', 'P\u00e9rez Granger'), 'be') (('Feature SelectionEarly', 'LightGBM model'), 'perform') (('seem', 'such LightGBM'), 'be') (('Processing', 'processes'), 'be') (('This', 'feature also importance'), 'output') (('Genetic programming', 'inspired API'), 'learn') (('XGBoost model', '2'), 'note') (('that', 'training'), 'function') (('true origin', 'author'), 'be') (('so it', 'CV too numerous here superb scores'), 'be') (('peak mostly values', 'slope merit'), 'survive') (('models', 'cross validation CV'), 'be') (('XGBoost both', 'cross validation substantially different methods'), 'be') (('where folds', '1'), 'improve') (('leader when full private board', 'competition'), 'be') (('random selection', '6 segments'), 'use') (('overnight even more computers', 'project'), 'take') (('output', 'earthquake prediction times'), 'be') (('models', '30 XGBoost'), 'require') (('even more computationally effort', 'problem'), 'drop') (('XGBoost model', '1'), 'have') (('frequency bands', 'that'), 'show') (('difficult it', 'problem'), 'make') (('only 13 341', 'leader public board'), 'be') (('Code', 'notebook'), 'test') (('24 data 000 rows', 'then manner'), 'create') (('peaks', 'peak rather value'), 'function') (('training data', 'so data'), 'contain') (('that', 'continuous acoustic signal'), 'provide') (('Consequently individual functions', 'even when multiprocessing'), 'require') (('where rows', 'above'), 'change') (('memory multiprocessing', 'greatly computational time'), 'help') (('that', 'ths signal end'), 'function') (('Numpy', 'feature creation van der Walt Colbert Varoquaux'), 'utilize') (('It', 'them'), 'be') (('CatBoost', 'boosting modern gradient also further study'), 'give') (('These', 'Kaggle kernel script Preda'), 'source') (('Andrew script', 'Andrew Lukayenko Lukayenko'), 'believe') (('ConclusionsIt', 'Kaggle competition'), 'tempting') (('test', 'two days'), 'run') (('script', 'reasonable time'), 'ensure') (('SciPy', 'correlation metrics Jones especially E.'), 'utilize') (('that', 'overfitting'), 'perform') (('script', 'Scirpus'), 'be') (('LightGBM training times', 'XGBoost'), 'become') (('inexperience', 'algorithms'), 'appear') (('that', 'Fourier analysis'), 'be') (('com grvsinghal', 'windows'), 'speed') (('that', 'Kaggle'), 'produce') (('leader board Best public scores', 'LightGBM'), 'be') (('this', 'hyperparameter especially tuning'), 'seem') (('data', 'CV score'), 'be') (('Feature reduction', 'equivalent individual models'), 'appear') (('Alternate choices', 'frequency bands'), 'investigate') (('function', 'below training sets'), 'call') (('sample 150k overlap', 'signal leakage'), 'be') (('Jupyter notebook', 'Kaggle'), 'PublicationIn') (('that', '0'), 'provide') (('notebook', 'university course'), 'design') (('this', '24 sample only 000 models'), 'perform') (('CV again score', 'Kaggle leader board much score'), 'change') (('course', 'final scoring'), 'reserve') (('reference Preda 2019 script', '2'), 'have') (('this', 'Jupyter Singhal'), 'use') (('training only 4194 rows', 'digital filters'), 'help') (('Hyperparamater tuning', 'validation scores'), 'be') (('full links', 'scripts'), 'find') (('slicing', 'several objectives'), 'accomplish') (('However it', 'feature scores'), 'be') (('These', 'bandwidth limited manner'), 'add') (('This', 'correlation'), 'be') (('I', 'presentation here only documentation purposes'), 'use') (('i', 'function'), 'convert') (('The', 'term long average'), 'refer') (('com artgor earthquakes', 'Mart A.'), 'fe') (('progress', 'effort'), 'be') (('statisticians', '0'), 'consider') (('how signal', 'bandwidth limited features'), 'define') (('LightGBM model', '1'), 'improve') (('decision rather LightGBM it', 'model'), 'use') (('it', 'gradient boosting regression based problem'), 'work') (('Kaggle submission', 'April here 28th'), 'make') (('it', 'two days'), 'call') (('author', 'work'), 'request') (('frequency Added bandwidth', 'time domain features'), 'limit') (('This', 'worse overfitting'), 'result') (('windowing', 'filter'), 'subject') (('AcknowledgementsI', '2019 work'), 'feel') (('script CV However scores', 'just 2'), 'appear') (('that', 'model'), 'pearsonr') (('it', 'test better present'), 'be') (('Hyperparameter TuningEffective hyperparameter tuning', 'very large project'), 'prove') (('thus it', 'training data'), 'be') (('Experience', 'feature creation'), 'show') (('CatBoost', 'unbiased categorical features'), 'boost') (('it', 'three days'), 'call') (('scoring Accuracy position', 'model training additional time'), 'appear') (('that earthquake', 'laboratory test'), 'be') (('training data', 'acoustic sample'), 'accompany') (('it', 'challenge'), 'make') (('This', 'validation holdout still sets'), 'allow') (('significant number', 'modeling'), 'have') (('which', 'single process'), 'allow') (('stopping final point', 'ever model'), 'find') (('reader', 'Kaggle'), 'be') (('CV that', 'scenario'), 'suffer') (('zero', 'fit perfect Bilogur'), 'be') (('script', 'author'), 'be') (('Stratified random sampling', 'data'), 'perform') (('colsample_bytree', '0'), 'set') (('Many', 'when academic paper'), 'request') (('also as well XGBoost', 'feature generation set'), 'perform') (('These', 'CatBoost Prokhorenkova Gusev Vorobev Dorogush gplearn 2017 genetic library'), 'be') (('I', 'more resources'), 'be') (('then it', 'function'), 'be') (('feature building portion', 'script'), 'help') (('CV', 'direct model'), 'argue') (('data analysis exploratory notebook', 'also same manner'), 'publish') (('they', 'then signal information'), 'slice') (('especially one', 'sample here 4194 data'), 'appear') (('that', 'time'), 'cause') (('benefit', 'compute 5 days time'), 'be') (('cc paper', 'gradient boosting decision 6907 highly efficient tree'), 'lightgbm') (('Preda 2019 script', 'Kaggle user'), 'reference') (('Model StackingSeveral issues', 'project'), 'affect') (('that', 'Kaggle test samples'), 'use') (('it', 'one time'), 'take') (('two outside models', 'Preda'), 'be') (('code herein', 'none text none Apache'), 'com') (('this', '3200'), 'be') (('university course', 'competition'), 'hope') (('test over full set', 'contest non entries'), 'resolve') (('which', 'clearly models'), 'consider') (('It', 'more hyperparameter tuning'), 'be') (('data where rows', 'information'), 'lead') (('this', 'overfitting'), '667') (('24 data Then 000 rows', 'signal'), 'create') (('Pandas', 'data manipulation'), 'be') (('code', 'Python'), 'write') (('150k samples long Nyquist', 'local environment'), 'set') (('scripts Preda 20190 2019 report', 'CV scores'), 'low') (('method', 'training sets'), 'help') (('Feature Creation', 'index signal'), 'explore') (('hyper thread best at least a four core 8 it', '16 RAM'), 'work') (('Preda 2019 model', 'LightGBM single author'), 'be') (('minimum data', 'LightGBM'), 'decrease') (('Removing', 'very tiny increase'), 'provide') (('CV score', 'leader board test data at least small current public sample'), 'appear') (('leader hopefully well full board 3 models', 'here Kaggle'), 'average') (('trend feature', 'signal'), 'be') (('pole IIR Butterworth 4 filters', 'frequency bands'), 'utilize') (('Kaggle leader board values', 'testing'), 'be') "}