{"name": "kernel tricks to build efficient classifier model ", "full_name": " h1 Author Pradeep Sathyamurthy h1 Date Started Oct 15 2017 h1 Last Modified Date Oct 28 2017 h1 Topic Focussed SVM Kernals h1 Dataset Voice Dataset for Gender Regonition from Kaggle h1 Introduction h3 1 SVM is a kernal trick which can be used for both supervised and unsupervised learning h3 2 As part of this case study I am going to apply SVM for a supervised learning as I am aware of the class labels to be classified h3 3 Thus in this notebook I will be using the voice dataset obtained from URL sighted below to classify if the parameters for a particular instances is a male or a female h1 Objective of case study h3 1 My main objective is to apply SVM and its different kernals and observe how the margin defined helps in improving the classification accuracy h3 2 I will try to tune different parameters in Kernal and choose the best tuning parameter wrt SVM to classify the dataset h3 3 I will also apply different classification techniques and compare the results obtained from these with result obtained from SVM classifier h1 Steps involved in this case study h3 1 Data Manipulation h3 2 Setting a benchmark accuracy for classifiers using Raw Data Naive Bayes h3 3 Exploratory Data Analysis h3 4 Data Munging and Partition h3 5 Validating the cleaned dataset with benchmark accuracy obtained h3 6 Core Model Building Applying Different Kernals for SVM h4 6 1 Linear Kernal SVM h4 6 2 RBF Kernal SVM h4 6 3 Polynomial Kernal SVM h4 6 4 Sigmoidal Kernal SVM h3 7 Perfomance Evaluation on Different Kernals for SVM with 10 fold cross validation h4 7 1 Evaluation on Linear Kernal SVM h4 7 2 Evaluation on RBF Kernal SVM h4 7 3 Evaluation on Polynomial Kernal SVM h4 7 4 Evaluation on Sigmoidal Kernal SVM h3 8 Parameter tuning on Different Kernals for SVM with 10 fold cross validation h4 8 1 Tuning on Linear Kernal SVM h4 8 2 Tuning on RBF Kernal SVM h4 8 3 Tuning on Polynomial Kernal SVM h3 9 Choosing best Kernals Parameters with grid search h3 10 Visualization of kernal Margin and boundries considereing on two columns meanfun sp ent h3 11 Building a Decision Tree h3 12 Building a KNN model h3 13 Comparing individual classifier results h3 14 Ensemble Learning h3 15 Reporting and Discussing final results h3 16 Final Model h1 Dataset URL h1 Importing Packages h1 Step 1 Data Manipulation h3 Reading Data h3 Data Types of Features h3 Checking for Missing Values h3 Seperating Independent and Target Variables h3 Target Variable Encoding h3 Inference h4 1 All independent variables are continuous in nature h4 2 While the target variables seems binary in nature of typr str h4 3 There are totally 3168 rows with 21 columns h4 4 There are no missing values in any of the record h1 Step 2 Setting a benchmark accuracy for classifiers using Raw Data Naive Bayes h3 Inference h4 1 Naive Bayes is a naive method which uses the probablistic theory to classify a target table h4 2 Since it has a fast computation power in training a data and testing it we can use it as a base method to validate our dataset h4 3 Accuracy obtained from this can be set as a bench mark for any classifier that we will start to work going forward h4 4 Using the raw data and classifying the dataset with Naive implementation with cross validation i obtained an accuracy of 0 85671 h4 5 Thus any data clean up we do further or any classifier model we build should not decrease the accuracy that we obtained here and it must always yeald a high or atleast an accuracy equal to 0 85671 else we will discard the data cleaning done or classifier built to classify the target variable h1 Step 3 Exploratory Data Analysis EDA h4 1 Variables meanfreq sd median Q25 are normally distributed h4 1 From above visualization and summary stats we can say Q75 is normally distributed h4 2 While IQR skew and kurt are skewed to right h4 1 sp ent s fm centroid are normally distributed h4 2 While mode is skewed h4 1 Variables meanfun is normally distributed h4 2 While variables minfun maxfun meandom are skewed h4 1 Variables modindx is normally distributed h4 2 While variables mindom maxdom and dfrange are skewed h3 Inference h4 1 Lets explain the skeweness in data from above visualization and summary stats h4 2 Irrespectve to viz of histogram we can also infer those attributes with mean and median values almost equal have gaussian distribution h4 3 Thus variables meanfreq sd median Q25 Q75 sp ent sfm centroid meanfun are Normally distributed h4 4 Variables skew kurt minfun maxfun meandom mindom maxdom dfrange midindex IQR mode are skewed h4 5 Exceptable range of voice freq for a human as per wiki is between 0 085 and 0 255KHz and hence we will remove any values from the dataset below 0 085 and above 0 255 assuming it to be a outlier based on domain knowledge h4 6 Our target variables 1 Male and 0 Female are symmetrical in nature with equal count of 1584 records for both Male and Female h1 Step 4 Data Munging and Partition h3 Data Cleaning h4 1 Exceptable range of voice freq for a human as per wiki is between 0 085 and 0 255KHz and hence we will identify the variable which has this frequncy information and remove them assuming it to be a outlier based on domain knowledge h4 2 In our data set meanfun is the variable which have the value of Fundamental frequency h4 3 As per the sitation given in wiki we can say that typical adult male will have a fundamental frequency from 85 to 180 xa0Hz and typical adult female from 165 to 255 xa0Hz h4 4 Thus from given dataset we will filter values based on meanfun whose values less than 0 085 and greater than 0 18 for male and values less than 0 165 and greater than 0 255 for female and consider them as outliers and remove them h3 Normalization h4 1 In this dataset meanfreq median Q25 Q75 IQR are the only variables associated with unit kHz h4 2 let us normalize these variables to make them unit free h4 3 we will apply the z score normalization for meanfreq median Q25 Q75 h4 4 we will apply min max normalization for IQR h3 Creating Partially Normalized Data h3 Handling Multicollinearity h3 Creating Completely Normalized Dataset All columns are normalized h3 Data Partition h3 Inference h4 1 I treated the variables with units making them unit free by standardizing them h4 2 z score normalization for meanfreq median Q25 Q75 was done h4 3 min max normalization was done for IQR variable h4 4 correlation between independent variables was checked to handle the multicollinearity issues h4 5 correlation between two variables greater than 0 9 are considered to be heavily coreelated and with respective VIF factor h4 6 Variables kurt Centroid dfrange z meanfreq was removed from dataset and this was maintained as a whole new dataset h4 7 Target variable was converted to numeric male as 1 and female as 0 using sklearn preprocessing pack n labelencoder object h4 8 Data partition was done based on sklearns model selection package using train test split object h4 9 Thus I have 4 dataset treated from raw data h4 10 I have 4 dataset treated from raw data and dimentionality reduced h4 11 I have 4 dataset treated from raw data with all independent variables normalized h1 Step 5 Validating the cleaned dataset with benchmark accuracy obtained h4 1 NB Cross Validation on Treated raw dataset h4 2 NB Cross Validation on Treated partially normalized and dimension reduced dataset This can at times help in building best SVM h4 3 NB Cross Validation on Treated and Completely Normalized dataset h3 Inference h4 1 Naive bayes classifier after data tretment produce an avg accuracy of 0 95 being the data is normalized or not normalized h4 2 we see a significant increase in accuracy from 0 85671 to 0 952 after we clean the data h4 3 We see the data with dimention reduced and data which are completely normalized works better than raw treated dataset h4 4 However this can be considered as a base classifier at this point and above result makes sure that our data clean up holds good and we havent removed any influential datas from dataset h4 5 This also set a new benchmark for any complex classifier that will be built further h4 6 Thus accuracy of 0 95 can be set as a bench mark accuracy value for this dataset which is cleaned and processed h4 7 Any model which produce accuracy less than 0 95 can be consodired as a non efficient model for this dataset from now on h1 Step 6 Core Model Building Applying Different Kernals for SVM h3 6 1 Linear Kernal SVM h3 Inference h4 1 I subjected 3 different dataset as explained above to a linear SVM model and I can observe that dataset which is completely normalize is performing well h4 2 As part of this kernal trick we have our hyperplane to be linear in a 20 dimentional space h4 3 This model exhibit a classification accuracy of 0 993902 h4 4 Since the data is 20 dimentional we cannot visualize if the data pocesses a linear or curved relation in feature space we can take a domain level expertise here h4 5 However since we have none for individual analysis purpose we will try to build a model with other kernal tricks types too and see how the model behaves in classifying the gender h3 6 2 RBF Kernal SVM h3 Inference h4 1 RBF or Gaussian is the default kernal which SVM uses in sklearn h4 2 Performance of RBF kernal trick is also same as linear kernal SVM h4 3 I obtained a accuracy of 0 993902 for RBF Kernal using SVM for normalized dataset h4 4 This shows that our voice dataset are both linearly and gaussian seperable h3 6 3 Polynomial Kernal SVM h3 Inference h4 1 To acheive much more high accuracy i tried using polynomial kernal too h4 2 I obtained an accuracy of 0 985 for polynomial kernal on normalized dataset h4 3 This is comparitively much less than the linear and rbf kernals h4 4 However we cannot conclude this result at this stage as our training dataset is just one single sample on which we obtained this result h3 6 4 Sigmoidal Kernal SVM h3 Inference h4 1 When a dataset is behaving well linearly it is explicitly known that it doesn t work well in a sigmoidal space h4 2 Above result obtained is the evident for this h4 3 I obtained accuracy of just 0 831 with sigmoidal kernal h3 4 5 Consolidated model accuracy h3 Inference h4 1 From above table it is clear that a completely normalized dataset behaves well compare to un normalized dataset h4 2 I obtain a maximum accuracy due to the data treatment done that is treating the meanfun attribute based on biological fact h4 3 Maximum accuracy i could acheive is 0 9939 whcih is from Linear and Gaussian Kernal using SVM h4 4 While the polinomial and Sigmoidal kernal doesn t seems to classify the target variable accurately and giving a low accuracy of 0 95 and 0 83 for Polynomial and Sigmoidal keransl respectively h4 5 However I cannot blindly accept this accuracy result because this is derived from one sample of training set and validated with a sample test set In order to evaluate this model to be more robust and to ensure data doesnt overfit I wanted to subject these model and dataset to a 10 fold cross validation and observe its result as part of next session h1 Step 7 Perfomance Evaluation on Different Kernals for SVM with 10 fold cross validation h3 7 1 Evaluation on Linear Kernal SVM h3 Inference h4 1 I see even with 10 fold cross validation our linear kernal SVM is providing a high accuracy of 0 9939 h4 2 Thus I can consider linear Kernal SVM as one of the serious model to subject for further tuning and see if it increases the accuracy h4 3 From abov table it is still evident that the completely normalized dataset behaves well comparitively h3 7 2 Evaluation on RBF Kernal SVM h3 Inference h4 1 From above table I see a slight decrease in accuracy when I subject Gaussian kernal to 10 fold cross validation h4 2 With out 80 20 split test set we saw an accuracy of 0 9939 however with 10 fold CV we obtain accuracy of 0 986 h4 3 Thus so far we see linear kernal is behaving well consistently and there is a slight decrese with gaussian kernal h3 7 4 Evaluation on Sigmoidal Kernal SVM h2 Inference h4 1 Like Gaussian kernal even polynomial and sigmoidal kernals yeald less accuracy with 10 fold CV h4 2 I did not include the results of polynomial kernal subjected to 10 fold CV because it was consuming more time to compute h4 3 However results of sigmoidal kernal is shown above and we see accuracy is dropped from 0 81 to 0 79 h3 7 5 Consolidated SVM Kernal Model s Evaluation Result h3 Inference h4 1 From above table it is clearly evident that Linear SVM Kernal on a completely normalized datset behaves really well h4 2 Even with 10 fold cross validation I obtaned an accuracy of 0 9933927 which seems consistent when compare to other kernals h4 3 After linear kernal it is the Gaussian and Polynomial kernal which gives high accuracy h4 4 So as part of next session we will drop Sigmoidal kernal from our further analyis as it doens t even satisfy the bench mark accuracy h4 5 I will take up other 3 SVM models for performance tuning and see how the accuracychanges when we tradeoff between kernal parameters like penalty C and gamma in order to obtain a soft margin h1 Step 8 Parameter tuning on Different Kernals for SVM with 5 fold cross validation experimenting with margins h4 From above experimentation we see dataset which was normalized yeald a good result h4 Thus for further experimentation we will use the dataset whose independent variables are normalized i e h4 data x3 and data y3 h3 8 1 Tuning on Linear Kernal SVM h3 Inference h4 1 Ultimate aim in building a kernal is to find an optimum hyper plane in feature space which has maximum margin in classifying our target variable h4 2 Kernal which I have built above so far in order to check the performance are those with hard margins this is not good to be generalized as it may cause overfitting h4 3 So in this session we will trade off between margin and Support vectors to choose an optimum boundry which will not overfit the model and at the same time deliver a high accuracy in classifying the target variable h4 4 With linear kernal it is the penalty measure through which we can do some trade off h4 5 Above table shows the accuracy model performance for different values of C h4 6 Both from graph and above table we see 0 6 and 1 1 to be the optimum penalty measure or C value which we can treade off with in classifying the target variable h4 7 Even with such trade off we obtain almost 0 9939 accuracy for linear kernal h3 8 2 Tuning on RBF Kernal SVM h3 Inference h4 1 In Gaussian kernal tradeoff is done with penalty C along with gamma parameter h4 2 I first experimented with wider Gamma values ranging between 1 and 10 and obsevred Kernal started to behave bad with gamma greater than 1 h4 3 So I tried to find the most optimum value with in 0 and 1 and as show in above table i obtained a maximum accuracy of 0 991 when gammal was equal to 0 03 and 0 05 h4 4 However when compare to Linear kernal we see rbf produce an accuracy of 0 002 times less h4 5 Thus it is quite evident again that linear kernal acts well on this dataset in classification of target variable h3 8 3 Tuning on Polynomial Kernal SVM h3 Inference h4 1 Along with penalty and gamma parameter with polynomial kernal we can trade off with degree h4 2 I experimented with various degree as shown above and obtained degree 1 1 produce a high accuracy h4 3 Accuracy obtained by polynomial is almost same as Linear which is 0 993 h4 4 So to produce a final inference in choosing the best kernal we will apply a grid search in our next session and see which model and which parameter produce a high accuracy h1 Step 9 Choosing best Kernals Parameters with grid search h3 9 1 Choosing the best parameter h3 Inference h4 1 I did a grid search whcih is a structure way to obtain an optimized kernal and its parameter measures h4 2 From above result I see it is the polynomial kernal with penalty measure of C 1 6 and gamma 0 005 and with degree 1 produce a high accuracy of 0 9939 in classifying the target variable h4 3 In this next session i have tried to visualize my margin and kernal behaviour by subjecting only 2 columns for analysis as it becomes a 2 dimentional space for visualization h1 Step 10 Visualization of kernal Margin and boundries considereing only two columns meanfun sp ent to represent a 2D space h3 10 1 Choosing the best attribute to represent dataset in 2D space h3 Inference h4 1 After doing necessary data cleanup and model building I was able to infer that a polinomial kernal SVM with parameters C 1 6 gamma 0 005 and degree 1 plots a perfect margin in a high dimentional space to classify gender label which is our target variable h4 2 However vizualizing more than two dimention is complex to represnt h4 3 So I would like to choose any 2 variables from dataset through which i can represnt my margin and kernal boundries in a 2 dimentional space h4 4 For this i used the correlation matrix and above scatter plot obtained above and choose two variable which is moderately correlated As neither the strong nor the weak correlation variables might not be well represented in ourder to show the decision boundries h4 5 meanfun being the most important variable for the dataset I decided to choose it and match it with another variable which has moderate correlation with it with 0 52 as correlation value between i choose sp ent and meanfun to be my choise of 2 dimentional feature space h3 10 2 Visualizing the margin modeled h3 Inference h4 1 meanfun being the most important variable for the dataset I decided to choose it and match it with another variable which has moderate correlation with it with 0 52 as correlation value between i choose sp ent and meanfun to be my choise of 2 dimentional feature space h4 2 I modeled polynomial kernal with penalty measure of C 1 6 gamma 0 05 and degree 1 to obtain the above scatter plot h4 3 When did SVM projected my data in a 2 dimentional space and obtained an optimal margin that classifies my gender being male and female h4 4 From the above figure we can infer h4 1 Orage points Instance which are Male h4 2 Blue Points Instance which are Female h4 3 Circled Points Support Vectors used to obtain margin h4 4 Straingh Line Hard Margin h4 5 Dotted Lines Soft Margin with trade off being C 1 6 gamma 0 05 and degree 1 h4 5 With respective to only these two variables meanfun and sp ent It is so evident that our model is not being overfit as it gives a clear distinction between two classes Male and Female with no complications in margins Thus accuracy of 0 99 can be considered to be valid enough at this point However this is just the visualization about margins we will not visualize how the SVM boundy is placed in a for all our parameters in a 2D space h3 10 3 Visualizing the Kernal boundaries h3 Inference h4 1 I still consider meanfun and sp ent to be my favorite variables to visualize my kernal boundries in a 2D space h4 2 I modeled polynomial kernal with same parameters penalty measure of C 1 6 gamma 0 05 and degree 1 to obtain the above scatter plot h4 3 When did SVM projected my data in a 2 dimentional space and obtained above feature space with boundries that classifies gender being male and female h4 4 From the above figure we can infer h4 1 Linear kernal with c 1 6 have a strict boundry h4 2 While in RBF kernal the boundry is strict and also have some points misclassified h4 3 Polynomial kernal have a lineant boundry which are discriminative h4 4 From above figure we dont see any complex boundries for polynomial and hence we need not worry about the model being over fitting h4 5 With respective to only these two variables meanfun and sp ent It is so evident that our model is not being overfit as it gives a clear distinction between two classes Male and Female with no complications in margins in a feature space h4 6 Thus accuracy of 0 993 produced by Polynomial kernal can be considered to be valid enough this means 7 out of 1000 times ther could be a misclassification Let is see if we can minimize this error occurence by increasing the accuracy further using few ensemble learnings h1 Step 11 Building a Decision Tree Classifier with grid search h3 Inference h4 1 I see accuracy yealded by decision tree is 0 9894 which is less when compare to SVM classifier which was 0 993 h4 2 We can say compare to decision tree SVM model seems more efficient h4 3 So if scrutability is the requirement based on which a model needs to be built we can go ahead with decision tree model h1 Step 12 Building a KNN with 5 nearest neighbors h3 Inference h4 1 KNN yealds an accuracy of 0 977 which is comparitive less to SVM h4 2 However its accuracy touches the benchmark of 0 95 which we decided based on Naive Bayes we can have this model for any ensemble building etc and it not advisable to just discard it h4 2 Though KNN perform better than Naive Bayes its accuracy is less compare to SVM h1 13 Comparing individual classifier results h3 Inference h4 1 From above table and graph it seems very clear that SVM with polynomial kernal behaves best h4 2 Accuracy produces by Polynomial kernal equal to 0 993 is the highest of all cross validation results obtained from other classifiers h4 3 Thus with individual classifiers we can infer that as a individual classifier SVM with Polynomial Kernal does a best classification wrt his voice dataset in classifying an instance as Male or Female h4 4 This SVM polynomial kernal tend to miss classify only 7 out of 1000 times when subjected to such dataset which is pretty good h4 5 However we will yet try to improve the accuracy further using some ensemble techniques h1 14 Ensemble Learning h3 14 1 Bagging with Random Forest h3 14 2 Boosting with Random Forest h1 15 Reporting and Discussing the final results h1 16 Final Model h3 Inference h4 1 From above table and graph it seems very clear that SVM with polynomial kernal behaves best h4 2 Accuracy produces by Polynomial kernal equal to 0 993 is the highest of all cross validation results obtained from other classifiers h4 3 Thus with individual classifiers we can infer that as a individual classifier SVM with Polynomial Kernal does a best classification wrt his voice dataset in classifying an instance as Male or Female h4 4 This SVM polynomial kernal tend to miss classify only 7 out of 1000 times when subjected to such dataset which is pretty good h4 5 However we will yet try to improve the accuracy further using some ensemble techniques h2 End of the Book ", "stargazers_count": 0, "forks_count": 0, "description": "To acheive much more high accuracy i tried using polynomial kernal too 2. From above table it is clearly evident that Linear SVM Kernal on a completely normalized datset behaves really well 2. Accuracy obtained from this can be set as a bench mark for any classifier that we will start to work going forward 4. Using the raw data and classifying the dataset with Naive implementation with cross validation i obtained an accuracy of 0. min max normalization was done for IQR variable 4. However when compare to Linear kernal we see rbf produce an accuracy of 0. 9939 however with 10 fold CV we obtain accuracy of 0. I still consider meanfun and sp. From above figure we dont see any complex boundries for polynomial and hence we need not worry about the model being over fitting 5. Validating the cleaned dataset with benchmark accuracy obtained 6. However we will yet try to improve the accuracy further using some ensemble techniques. data_y3_train d. Orage points Instance which are Male 2. This is comparitively much less than the linear and rbf kernals 4. I will take up other 3 SVM models for performance tuning and see how the accuracychanges when we tradeoff between kernal parameters like penalty C and gamma in order to obtain a soft margin. Evaluation on RBF Kernal SVM 7. Blue Points Instance which are Female 3. 05 and degree 1 to obtain the above scatter plot. Linear Kernal SVM 6. Accuracy produces by Polynomial kernal equal to 0. Performance of RBF kernal trick is also same as linear kernal SVM 3. In this next session i have tried to visualize my margin and kernal behaviour by subjecting only 2 columns for analysis as it becomes a 2 dimentional space for visualization. With respective to only these two variables meanfun and sp. Thus from given dataset we will filter values based on meanfun whose values less than 0. Exceptable range of voice freq for a human as per wiki is between 0. With out 80 20 split test set we saw an accuracy of 0. So to produce a final inference in choosing the best kernal we will apply a grid search in our next session and see which model and which parameter produce a high accuracy. 99 can be considered to be valid enough at this point. min_samples_leaf np. linspace 1 30 15 min_samples_split np. However vizualizing more than two dimention is complex to represnt 3. Polynomial Kernal SVM Inference 1. Like Gaussian kernal even polynomial and sigmoidal kernals yeald less accuracy with 10 fold CV 2. 831 with sigmoidal kernal 4. Normalization 1. Tuning on RBF Kernal SVM 8. 9939 accuracy for linear kernal 8. Tuning on Polynomial Kernal SVM Inference 1. 18 for male and values less than 0. From above visualization and summary stats we can say Q75 is normally distributed 2. 95 can be set as a bench mark accuracy value for this dataset which is cleaned and processed. Data Manipulation 2. 95 can be consodired as a non efficient model for this dataset from now on Step 6 Core Model Building Applying Different Kernals for SVM 6. Step 12 Building a KNN with 5 nearest neighbors Inference 1. Boosting with Random Forest 15. I have 4 dataset treated from raw data with all independent variables normalized a. I first experimented with wider Gamma values ranging between 1 and 10 and obsevred Kernal started to behave bad with gamma greater than 1 3. 05 and degree 1 5. let us normalize these variables to make them unit free 3. There are no missing values in any of the record. Sigmoidal Kernal SVM Inference 1. So I would like to choose any 2 variables from dataset through which i can represnt my margin and kernal boundries in a 2 dimentional space 4. 6 have a strict boundry 2. Exploratory Data Analysis 4. I did not include the results of polynomial kernal subjected to 10 fold CV because it was consuming more time to compute 3. 993 is the highest of all cross validation results obtained from other classifiers. Final Model Dataset URL http www. From the above figure we can infer 1. From abov table it is still evident that the completely normalized dataset behaves well comparitively 7. Even with such trade off we obtain almost 0. ent to represent a 2D space 10. ent sfm centroid meanfun are Normally distributed 4. data_x3_train b. NB Cross Validation on Treated raw dataset 2. From above table I see a slight decrease in accuracy when I subject Gaussian kernal to 10 fold cross validation 2. RBF or Gaussian is the default kernal which SVM uses in sklearn 2. With linear kernal it is the penalty measure through which we can do some trade off 5. In our data set meanfun is the variable which have the value of Fundamental frequency 3. Consolidated model accuracy Inference 1. 9933927 which seems consistent when compare to other kernals. 255 assuming it to be a outlier based on domain knowledge 6. So if scrutability is the requirement based on which a model needs to be built we can go ahead with decision tree model. 985 for polynomial kernal on normalized dataset 3. and it not advisable to just discard it. 1 and kernel as linear With rbf gamma value 0. Thus I can consider linear Kernal SVM as one of the serious model to subject for further tuning and see if it increases the accuracy 3. In order to evaluate this model to be more robust and to ensure data doesnt overfit I wanted to subject these model and dataset to a 10 fold cross validation and observe its result as part of next session Step 7 Perfomance Evaluation on Different Kernals for SVM with 10 fold cross validation 7. Thus in this notebook I will be using the voice dataset obtained from URL sighted below to classify if the parameters for a particular instances is a male or a female Objective of case study 1. we will apply min max normalization for IQR Creating Partially Normalized Data Handling Multicollinearity Creating Completely Normalized Dataset All columns are normalized Data Partition Inference 1. data_y3_test Step 5 Validating the cleaned dataset with benchmark accuracy obtained 1. Polynomial Kernal SVM 6. Variables meanfreq sd median Q25 are normally distributed 1. I treated the variables with units making them unit free by standardizing them 2. meanfun being the most important variable for the dataset I decided to choose it and match it with another variable which has moderate correlation with it. Choosing best Kernals Parameters with grid search 10. As part of this case study I am going to apply SVM for a supervised learning as I am aware of the class labels to be classified. Let is see if we can minimize this error occurence by increasing the accuracy further using few ensemble learnings. ent It is so evident that our model is not being overfit as it gives a clear distinction between two classes Male and Female with no complications in margins in a feature space. Choosing the best parameter Inference 1. Maximum accuracy i could acheive is 0. Author Pradeep Sathyamurthy Date Started Oct 15 2017 Last Modified Date Oct 28 2017 Topic Focussed SVM Kernals Dataset Voice Dataset for Gender Regonition from Kaggle Introduction 1. Comparing individual classifier results 14. I obtained a accuracy of 0. data_x2_train b. fm centroid are normally distributed 2. 255 for female and consider them as outliers and remove them. Choosing the best attribute to represent dataset in 2D space Inference 1. Naive bayes classifier after data tretment produce an avg accuracy of 0. Thus any data clean up we do further or any classifier model we build should not decrease the accuracy that we obtained here and it must always yeald a high or atleast an accuracy equal to 0. This shows that our voice dataset are both linearly and gaussian seperable 6. 0 by default in sklearn I would like to experiment it with multiple margins in range of c from 1 to 10 Completely normlized dataset plot the value of C for SVM x axis versus the cross validated accuracy y axis Completely normlized dataset plot the value of C for SVM x axis versus the cross validated accuracy y axis Doing further tradeoff plot the value of C for SVM x axis versus the cross validated accuracy y axis Completely normlized dataset plot the value of C for SVM x axis versus the cross validated accuracy y axis Now performing SVM by taking hyperparameter C 0. Since it has a fast computation power in training a data and testing it we can use it as a base method to validate our dataset 3. RBF Kernal SVM 6. 977 which is comparitive less to SVM 2. I see even with 10 fold cross validation our linear kernal SVM is providing a high accuracy of 0. ent It is so evident that our model is not being overfit as it gives a clear distinction between two classes Male and Female with no complications in margins. com 2016 06 22 identifying the gender of a voice using machine learning Importing Packages Step 1 Data Manipulation Reading Data Data Types of Features Checking for Missing Values Seperating Independent and Target Variables Target Variable Encoding Inference 1. This also set a new benchmark for any complex classifier that will be built further 6. 993902 for RBF Kernal using SVM for normalized dataset 4. From above result I see it is the polynomial kernal with penalty measure of C 1. Sigmoidal Kernal SVM 7. correlation between two variables greater than 0. 005 and degree 1 plots a perfect margin in a high dimentional space to classify gender label which is our target variable 2. As neither the strong nor the weak correlation variables might not be well represented in ourder to show the decision boundries. Bagging with Random Forest 14. Data partition was done based on sklearns model_selection package using train_test_split object 9. Thus with individual classifiers we can infer that as a individual classifier SVM with Polynomial Kernal does a best classification wrt his voice dataset in classifying an instance as Male or Female 4. 165 and greater than 0. we see a significant increase in accuracy from 0. Polynomial kernal have a lineant boundry which are discriminative 4. Data Munging and Partition 5. 9939 whcih is from Linear and Gaussian Kernal using SVM 4. Evaluation on Linear Kernal SVM Inference 1. 9939 in classifying the target variable. Thus I have 4 dataset treated from raw data a. For this i used the correlation matrix and above scatter plot obtained above and choose two variable which is moderately correlated. My main objective is to apply SVM and its different kernals and observe how the margin defined helps in improving the classification accuracy 2. Step 9 Choosing best Kernals Parameters with grid search 9. Visualizing the margin modeled Inference 1. Evaluation on Sigmoidal Kernal SVM 8. When a dataset is behaving well linearly it is explicitly known that it doesn t work well in a sigmoidal space 2. After doing necessary data cleanup and model building I was able to infer that a polinomial kernal SVM with parameters C 1. Reporting and Discussing the final results 16. 9 are considered to be heavily coreelated and with respective VIF factor 6. Building a Decision Tree 12. Above table shows the accuracy model performance for different values of C 6. Along with penalty and gamma parameter with polynomial kernal we can trade off with degree 2. When did SVM projected my data in a 2 dimentional space and obtained above feature space with boundries that classifies gender being male and female. However its accuracy touches the benchmark of 0. 95 which we decided based on Naive Bayes we can have this model for any ensemble building etc. I did a grid search whcih is a structure way to obtain an optimized kernal and its parameter measures 2. Evaluation on Polynomial Kernal SVM 7. Variables kurt Centroid dfrange z_meanfreq was removed from dataset and this was maintained as a whole new dataset 7. Setting a benchmark accuracy for classifiers using Raw Data Naive Bayes 3. From above table and graph it seems very clear that SVM with polynomial kernal behaves best. SVM is a kernal trick which can be used for both supervised and unsupervised learning. Naive Bayes is a naive method which uses the probablistic theory to classify a target table 2. Tuning on Linear Kernal SVM 8. Ensemble Learning 14. In this dataset meanfreq median Q25 Q75 IQR are the only variables associated with unit kHz 2. So as part of next session we will drop Sigmoidal kernal from our further analyis as it doens t even satisfy the bench mark accuracy. 52 as correlation value between i choose sp. Parameter tuning on Different Kernals for SVM with 10 fold cross validation 8. End of the Book for data handling for data manipulation for plotting For encoding class variables for train and test split to built svm model inherits other SVM objects to calculate classifiers accuracy to perform cross validation to perform standardization to perform grid search for all classifiers to perform decision tree classification to perform knn to perform Naive Bayes produce classifier reports to perform ensemble bagging random forest to perform ensemble boosting to plot ROC Curve Reding the data as pandas dataframe Verifying if all records are read having the headers handy Data type Checking for any missing values in data and other junk values if any let us seperate the independent and dependent variables seperately encoding the target variable from categorical values to binary form Let us do a 80 20 split let us do a descriptive statistics Distribution of target variables Actual Raw Data size Filtering ouliers from male category Filtering ouliers from female category Thus we need to remove 710 rows from both data_x and data_y using the index obtained from above filters Preparing final dataset for model building Target dataset Distribution of target variables after cleanup Z score Normalization Lets now drop the original column from data_x as we have these as backup in data_raw dataframe Plotting the normalized columns we could see that z score norm variables have mean 0 and standard deviation 1 And the min max norm varibales value are confined between 0 1 and stays positive let us see the correlation in data Thus we see high correlation exist between above variables thus let us create a dataset by removing variables that create high Variance Inflation Factor Thus removing kurt Centroid dfrange z_meanfreq let me not do any dimentionality reduction and do z score normalization on all independent variables Let us do a 80 20 split on raw dataset let us do a 80 20 split on dimention reduced dataset too let us do a 80 20 split on raw dataset which was only normalized let us check the size let is cross check the size of dimention reduced data set too let is cross check the size of normalized raw data set too defining the Naive Bayes object lets do a 10 fold Cross validation to make sure the accuracy obtained above lets do a 10 fold Cross validation to make sure the accuracy obtained above lets do a 10 fold Cross validation to make sure the accuracy obtained above Partially normlized dataset Dimention reduced dataset Completely normalized dataset Partially normlized dataset Dimention reduced dataset Completely normalized dataset Partially normlized dataset Dimentione reduced dataset Completely normalized dataset Partially normlized dataset Dimentione reduced dataset Completely normalized dataset Partially normlized dataset Dimentione reduced dataset Completely normalized dataset Partially normlized dataset Dimentione reduced dataset Completely normalized dataset Partially normlized dataset Dimentione reduced dataset Completely normalized dataset penality parameter C is 1. I will try to tune different parameters in Kernal and choose the best tuning parameter wrt SVM to classify the dataset 3. Even with 10 fold cross validation I obtaned an accuracy of 0. I will also apply different classification techniques and compare the results obtained from these with result obtained from SVM classifier Steps involved in this case study 1. There are totally 3168 rows with 21 columns 4. Linear kernal with c 1. Irrespectve to viz of histogram we can also infer those attributes with mean and median values almost equal have gaussian distribution. 005 and with degree 1 produce a high accuracy of 0. 993 produced by Polynomial kernal can be considered to be valid enough this means 7 out of 1000 times ther could be a misclassification. I have 4 dataset treated from raw data and dimentionality reduced a. correlation between independent variables was checked to handle the multicollinearity issues 5. Tuning on RBF Kernal SVM Inference 1. Visualizing the Kernal boundaries Inference 1. Step 11 Building a Decision Tree Classifier with grid search Inference 1. However since we have none for individual analysis purpose we will try to build a model with other kernal tricks types too and see how the model behaves in classifying the gender. Thus it is quite evident again that linear kernal acts well on this dataset in classification of target variable. As per the sitation given in wiki we can say that typical adult male will have a fundamental frequency from 85 to 180 Hz and typical adult female from 165 to 255 Hz 4. Evaluation on RBF Kernal SVM Inference 1. 9894 which is less when compare to SVM classifier which was 0. Step 10 Visualization of kernal Margin and boundries considereing only two columns meanfun sp. Ensemble Learning 15. Kernal which I have built above so far in order to check the performance are those with hard margins this is not good to be generalized as it may cause overfitting. Lets explain the skeweness in data from above visualization and summary stats 2. 991 when gammal was equal to 0. 255KHz and hence we will identify the variable which has this frequncy information and remove them assuming it to be a outlier based on domain knowledge 2. Thus so far we see linear kernal is behaving well consistently and there is a slight decrese with gaussian kernal 7. Our target variables 1 Male and 0 Female are symmetrical in nature with equal count of 1584 records for both Male and Female Step 4 Data Munging and Partition Data Cleaning 1. 1 produce a high accuracy 3. I obtain a maximum accuracy due to the data treatment done that is treating the meanfun attribute based on biological fact 3. 95 being the data is normalized or not normalized 2. This model exhibit a classification accuracy of 0. Evaluation on Sigmoidal Kernal SVM Inference 1. Comparing individual classifier results Inference 1. data_y2_train d. I modeled polynomial kernal with penalty measure of C 1. Final Model Inference 1. Linear Kernal SVM Inference 1. Since the data is 20 dimentional we cannot visualize if the data pocesses a linear or curved relation in feature space we can take a domain level expertise here. While variables minfun maxfun meandom are skewed 1. Dotted Lines Soft Margin with trade off being C 1. Target variable was converted to numeric male as 1 and female as 0 using sklearn preprocessing pack n labelencoder object 8. 255KHz and hence we will remove any values from the dataset below 0. I subjected 3 different dataset as explained above to a linear SVM model and I can observe that dataset which is completely normalize is performing well. Any model which produce accuracy less than 0. 83 for Polynomial and Sigmoidal keransl respectively. 085 and greater than 0. 952 after we clean the data 3. I obtained accuracy of just 0. However I cannot blindly accept this accuracy result because this is derived from one sample of training set and validated with a sample test set. 01 performing grid search with different tuning parameters Scatter plot with strong correlation not useful much to represnt the distribution wrt kernal boundries Scatter plot with weak correlation not useful much to represnt the distribution wrt kernal boundries Scatter plot with moderate correlation useful much to represnt the distribution wrt kernal boundries Scatter plot with moderate negative correlation useful much to represnt the distribution wrt kernal boundries import some data to play with fit the model don t regularize for illustration purposes title for the plots plot the decision function create grid to evaluate model plot decision boundary and margins plot support vectors import some data to play with SVM regularization parameter title for the plots Set up 2x2 grid for plotting. NB Cross Validation on Treated partially normalized and dimension reduced dataset This can at times help in building best SVM 3. linspace 2 20 10 lets do a 10 fold Cross validation to make sure the accuracy obtained above lets do a 10 fold Cross validation to make sure the accuracy obtained above Applying Random forest to improve the decision tree model lets do a 10 fold Cross validation to make sure the accuracy obtained above adaboost lets do a 10 fold Cross validation to make sure the accuracy obtained above Building the ROC Curve for the final SVM Kernal model CV Accuracy ROC measure. Exceptable range of voice freq for a human as per wiki is between 0. Consolidated SVM Kernal Model s Evaluation Result Inference 1. RBF Kernal SVM Inference 1. 85671 else we will discard the data cleaning done or classifier built to classify the target variable. Circled Points Support Vectors used to obtain margin 4. Perfomance Evaluation on Different Kernals for SVM with 10 fold cross validation 7. While the target variables seems binary in nature of typr str 3. All independent variables are continuous in nature 2. While the polinomial and Sigmoidal kernal doesn t seems to classify the target variable accurately and giving a low accuracy of 0. Core Model Building Applying Different Kernals for SVM 6. When did SVM projected my data in a 2 dimentional space and obtained an optimal margin that classifies my gender being male and female. Accuracy obtained by polynomial is almost same as Linear which is 0. NB Cross Validation on Treated and Completely Normalized dataset Inference 1. KNN yealds an accuracy of 0. While IQR skew and kurt are skewed to right 1. Step 2 Setting a benchmark accuracy for classifiers using Raw Data Naive Bayes Inference 1. Variables skew kurt minfun maxfun meandom mindom maxdom dfrange midindex IQR mode are skewed 5. We can say compare to decision tree SVM model seems more efficient 3. Step 3 Exploratory Data Analysis EDA 1. While variables mindom maxdom and dfrange are skewed Inference 1. Step 8 Parameter tuning on Different Kernals for SVM with 5 fold cross validation experimenting with margins From above experimentation we see dataset which was normalized yeald a good result Thus for further experimentation we will use the dataset whose independent variables are normalized i. While mode is skewed 1. I see accuracy yealded by decision tree is 0. Straingh Line Hard Margin 5. I obtained an accuracy of 0. Though KNN perform better than Naive Bayes its accuracy is less compare to SVM 13. However we cannot conclude this result at this stage as our training dataset is just one single sample on which we obtained this result. So I tried to find the most optimum value with in 0 and 1 and as show in above table i obtained a maximum accuracy of 0. However this is just the visualization about margins we will not visualize how the SVM boundy is placed in a for all our parameters in a 2D space. Thus variables meanfreq sd median Q25 Q75 sp. Visualization of kernal Margin and boundries considereing on two columns meanfun sp. Variables modindx is normally distributed 2. we will apply the z score normalization for meanfreq median Q25 Q75 4. 1 to be the optimum penalty measure or C value which we can treade off with in classifying the target variable. So in this session we will trade off between margin and Support vectors to choose an optimum boundry which will not overfit the model and at the same time deliver a high accuracy in classifying the target variable. In Gaussian kernal tradeoff is done with penalty C along with gamma parameter 2. data_x3 and data_y3 8. ent and meanfun to be my choise of 2 dimentional feature space. I modeled polynomial kernal with same parameters penalty measure of C 1. Both from graph and above table we see 0. As part of this kernal trick we have our hyperplane to be linear in a 20 dimentional space 3. However results of sigmoidal kernal is shown above and we see accuracy is dropped from 0. Building a KNN model 13. Variables meanfun is normally distributed 2. Tuning on Polynomial Kernal SVM 9. While in RBF kernal the boundry is strict and also have some points misclassified 3. Tuning on Linear Kernal SVM Inference 1. Evaluation on Linear Kernal SVM 7. Reporting and Discussing final results 16. However this can be considered as a base classifier at this point and above result makes sure that our data clean up holds good and we havent removed any influential datas from dataset. After linear kernal it is the Gaussian and Polynomial kernal which gives high accuracy 4. z score normalization for meanfreq median Q25 Q75 was done 3. We see the data with dimention reduced and data which are completely normalized works better than raw treated dataset. Ultimate aim in building a kernal is to find an optimum hyper plane in feature space which has maximum margin in classifying our target variable. From above table it is clear that a completely normalized dataset behaves well compare to un normalized dataset 2. I experimented with various degree as shown above and obtained degree 1. This SVM polynomial kernal tend to miss classify only 7 out of 1000 times when subjected to such dataset which is pretty good. Above result obtained is the evident for this 3. ent to be my favorite variables to visualize my kernal boundries in a 2D space. ", "id": "pradeepsathyamurthy/kernel-tricks-to-build-efficient-classifier-model", "size": "27466", "language": "python", "html_url": "https://www.kaggle.com/code/pradeepsathyamurthy/kernel-tricks-to-build-efficient-classifier-model", "git_url": "https://www.kaggle.com/code/pradeepsathyamurthy/kernel-tricks-to-build-efficient-classifier-model", "script": "svm # inherits other SVM objects funct_svm SVC # to built svm model cross_val_score # to perform cross validation pyplot StandardScaler # to perform standardization naive_bayes # to perform Naive Bayes classification_report # produce classifier reports neighbors # to perform knn sklearn.svm numpy funct_tune_svm AdaBoostClassifier # to perform ensemble boosting LabelEncoder # For encoding class variables sklearn make_meshgrid funct_svm_cv sklearn.model_selection pandas tree # to perform decision tree classification pyplot as plt # for plotting measure_performance roc_curve RandomForestClassifier GridSearchCV # to perform grid search for all classifiers metrics # to calculate classifiers accuracy matplotlib train_test_split # for train and test split plot_contours auc # to plot ROC Curve RandomForestClassifier # to perform ensemble bagging - random forest sklearn.metrics sklearn.ensemble sklearn.preprocessing ", "entities": "(('Accuracy', 'equal 0'), 'produce') (('it', 'feature space'), 'ent') (('variables', 'mindom maxdom'), 'be') (('Exceptable range', '0'), 'be') (('I', '0'), 'with') (('dataset penality parameter Completely normalized C', 'dataset'), 'inherit') (('bayes data Naive tretment', '0'), 'classifier') (('below parameters', 'female case'), 'use') (('grid search whcih', 'structure optimized kernal'), 'do') (('which', 'SVM when classifier'), '9894') (('hyperplane', '20 dimentional space'), 'have') (('which', 'target variable'), 'trade') (('we', 'dataset'), 'consider') (('variables', 'Male Step'), 'be') (('that', 'gender'), 'project') (('i', 'sp'), '52') (('independent variables', 'nature'), 'be') (('dataset completely normalized behaves', 'un well normalized dataset'), 'be') (('we', 'almost 0'), 'obtain') (('This', 'times'), 'normalize') (('Target variable', 'pack n labelencoder object'), 'convert') (('which', 'lineant boundry'), 'have') (('model', '0'), 'exhibit') (('hence we', '0'), '255khz') (('I', 'class labels'), 'go') (('Polynomial which', 'linear kernal'), 'be') (('min max normalization', 'IQR variable'), 'do') (('it', '3'), 'include') (('ent sfm centroid meanfun', 'Normally 4'), 'distribute') (('which', 'maximum margin'), 'be') (('doesn polinomial kernal t', '0'), 'seem') (('when I', 'cross validation'), 'see') (('Step', 'grid search Inference'), 'build') (('I', 'dataset'), 'try') (('Comparing', 'classifier individual Inference'), 'result') (('I', 'above degree'), 'experiment') (('here it', 'equal 0'), 'clean') (('I', 'C'), 'model') (('I', 'fold cross 10 validation'), 'in') (('linear kernal', 'well consistently slight gaussian kernal'), 'see') (('we', 'classifier'), 'set') (('different how margin', 'classification accuracy'), 'be') (('However we', 'further ensemble techniques'), 'try') (('I', 'gamma'), 'experiment') (('it', 'overfitting'), 'be') (('SVM', 'sklearn'), 'be') (('95', 'SVM'), 'consodire') (('model', 'fitting 5'), 'see') (('Q75', 'normally 2'), 'say') (('Q25 Q75 IQR', 'unit only kHz'), 'be') (('table', 'C'), 'show') (('993', 'other classifiers'), 'be') (('margins plot support vectors', '2x2 grid'), 'useful') (('we', 'decision tree ahead model'), 'go') (('linear kernal SVM', '0'), 'see') (('accuracy', '0'), 'show') (('we', '5'), 'be') (('Data partition', 'train_test_split object'), 'do') (('which', 'accuracy'), 'model') (('also points', 'RBF kernal'), 'be') (('Performance', 'also linear kernal SVM'), 'be') (('Points Support Circled Vectors', 'margin'), 'use') (('which', 'gender label'), 'plot') (('which', 'above two variable'), 'use') (('we', 'dataset'), 'use') (('Step', 'Raw Data Naive Bayes Inference'), 'set') (('which', 'SVM above linear model'), 'subject') (('values', 'meanfun'), 'filter') (('how when we', 'soft margin'), 'take') (('it', 'accuracy'), 'consider') (('accuracy', 'decision tree'), 'see') (('model I', 'parameters C'), 'be') (('i', 'polynomial kernal'), 'acheive') (('which', 'when other kernals'), '9933927') (('columns', 'Data Partition Inference'), 'apply') (('target variables', 'typr str'), 'seem') (('i', '0'), 'try') (('which', 'completely normalized better raw treated dataset'), 'see') (('However accuracy', '0'), 'touch') (('IQR skew', 'right 1'), 'be') (('strong', 'decision boundries'), 'represent') (('i', '2 dimentional space'), 'like') (('variables', 'maxfun meandom'), 'skewed') (('evident', '3'), 'be') (('9939 whcih', 'Gaussian SVM'), 'be') (('correlation', 'multicollinearity issues'), 'check') (('t', 'bench mark even accuracy'), 'drop') (('However vizualizing', 'more than two dimention'), 'be') (('this', 'sample test set'), 'accept') (('mean values', 'almost gaussian distribution'), 'have') (('5 Validating', '1'), 'data_y3_t') (('them', '2'), 'treat') (('we', 'further few ensemble learnings'), 'let') (('that', 'biological fact'), 'obtain') (('which', 'it'), 'meanfun') (('Modified 15 Last Oct 28 2017 Topic', 'Kaggle Introduction'), 'Started') (('times ther', '1000'), 'mean') (('which', 'almost Linear'), 'be') (('9', 'VIF heavily respective factor'), 'consider') (('rbf', '0'), 'see') (('too how model', 'gender'), 'try') (('99', 'enough point'), 'consider') (('which', 'dataset'), 'set') (('we', 'degree'), 'trade') (('this', 'whole new dataset'), 'dfrange') (('we', 'building ensemble etc'), '95') (('Gaussian kernal tradeoff', 'gamma parameter'), 'do') (('accuracy', 'less SVM'), 'be') (('that', 'complex classifier'), 'set') (('kernal which', 'supervised learning'), 'be') (('classifier', 'data cleaning'), 'discard') (('We', 'tree SVM model'), 'say') (('I', '1'), 'apply') (('I', 'reduced a.'), 'have') (('it', '2 dimentional visualization'), 'try') (('SVM how boundy', '2D space'), 'be') (('This', 'comparitively much linear'), 'be') (('unit', '3'), 'let') (('adult typical male', '165'), 'say') (('naive which', 'target table'), 'be') (('Linear SVM clearly Kernal', 'datset completely normalized behaves'), 'be') (('Lets', 'visualization'), 'explain') (('we', 'meanfreq median'), 'apply') (('accuracy', 'Kernal model CV Accuracy ROC final SVM measure'), 'linspace') (('it', 'margins'), 'ent') (('quite again that', 'target variable'), 'be') (('it', 'domain knowledge'), '255khz') (('Lines Soft Dotted Margin', 'trade'), 'be') (('Thus I', 'data raw a.'), 'have') (('very SVM', 'polynomial kernal behaves'), 'seem') (('it', 'abov table'), 'be') (('we', 'domain level expertise'), 'visualize') (('Step', '5 nearest neighbors'), 'build') (('independent variables', 'dataset'), 'see') (('score normalization', 'meanfreq median Q25 Q75'), 'do') (('parameter', 'high accuracy'), 'produce') (('best classification', 'Male'), 'infer') (('explicitly it', 't well sigmoidal space'), 'know') (('we', 'target variable'), '1') (('it', 'C'), 'see') (('I', 'hyperparameter'), 'like') (('we', '0'), 'see') (('Step', 'columns meanfun only two sp'), 'consideree') (('even polynomial kernals', '10 fold'), 'yeald') (('we', 'result'), 'conclude') (('I', 'independent variables'), 'have') (('Variables sd median Q25', 'normally 1'), 'meanfreq') (('which', 'Fundamental frequency'), 'be') (('i', '0'), 'obtain') (('we', '0'), 'obtain') (('which', 'only 7 out of times when such dataset'), 'tend') (('it', 'domain knowledge'), 'assume') "}