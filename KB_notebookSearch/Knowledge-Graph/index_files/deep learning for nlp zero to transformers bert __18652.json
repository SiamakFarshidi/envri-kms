{"name": "deep learning for nlp zero to transformers bert ", "full_name": " h1 About this Notebook h1 Contents h1 Configuring TPU s h3 Data Preparation h1 Before We Begin h1 Simple RNN h2 Basic Overview h2 In Depth Understanding h2 Code Implementation h2 Code Explanantion h1 Word Embeddings h1 LSTM s h2 Basic Overview h2 In Depth Understanding h1 Code Implementation h2 Code Explanation h1 GRU s h2 Basic Overview h2 In Depth Explanation h2 Code Implementation h1 Bi Directional RNN s h2 In Depth Explanation h2 Code Implementation h2 Code Explanation h1 Seq2Seq Model Architecture h2 Overview h2 In Depth Understanding h1 Attention Models h2 Code Implementation h1 Transformers Attention is all you need h2 Code Implementation h1 BERT and Its Implementation on this Competition h2 Tokenization h2 Starting Training h1 End Notes h3 I am attaching more resources if you want NLP end to end ", "stargazers_count": 0, "forks_count": 0, "description": "com arthurtok spooky nlp and topic modelling tutorial https www. com abhishek approaching almost any nlp problem on kaggleIf you want a more basic dataset to practice with here is another kernel which I wrote https www. Rest of the model is same as before except we have replaced the SimpleRNN By LSTM Units Comments on the ModelWe now see that the model is not overfitting and achieves an auc score of 0. As this community has been very kind to me and helped me in learning all of this I want to take this forward. org learn nlp sequence models lecture HyEui basic models A basic idea of different Seq2Seq Models https blog. ai chapter_recurrent modern bi rnn. com questions 44579161 why do we do padding in nlp tasksThe code token. com how to implement seq2seq lstm model in keras shortcutnlp 6f355f3e5639 A More advanced Seq2seq Model and its explanation https d2l. pub 2019 memorization in rnns https towardsdatascience. The point is we got an AUC score of 0. html This presents the code implementation of the architecture presented in the paper by Google BERT and Its Implementation on this CompetitionAs Promised I am back with Resiurces to understand about BERT architecture please follow the contents in the given order http jalammar. Works on CPU and single GPU. Why RNN s https www. Embedding layer is also a layer of neurons which takes in as input the nth dimensional one hot vector of every word and converts it into 300 dimensional vector it gives us word embeddings similar to word2vec. in 2014 GRU Gated Recurrent Unit aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. using keras tokenizer here zero pad the sequences A simpleRNN without any pretrained embeddings and one dense layer Multiplying by Strategy to run on TPU s load the GloVe vectors in a dictionary create an embedding matrix for the words we have in the dataset A simple LSTM with glove embeddings and one dense layer GRU with glove embeddings and two dense layers A simple bidirectional LSTM with glove embeddings and one dense layer Visualization of Results obtained from various Deep learning models Loading Dependencies LOADING THE DATA IMP DATA FOR CONFIG Configuration First load the real tokenizer Save the loaded tokenizer locally Reload it with the huggingface tokenizers library. org learn nlp sequence models lecture RDXpX attention model intuition Only watch this video and not the next one https towardsdatascience. Next we add an 100 LSTM units without any dropout or regularizationAt last we add a single neuron with sigmoid function which takes output from 100 LSTM cells Please note we have 100 LSTM cells not layers to predict the results and then we compile the model using adam optimizer Comments on the modelWe can see our model achieves an accuracy of 1 which is just insane we are clearly overfitting I know but this was the simplest model of all we can tune a lot of hyperparameters like RNN units we can do batch normalization dropouts etc to get better result. com define encoder decoder sequence sequence model neural machine translation keras Basic Encoder Decoder Model and its explanation respectively https towardsdatascience. pub http jalammar. Seq2Seq is a many to many RNN architecture where the input is a sequence and the output is also a sequence where input and output sequences can be or cannot be of different lengths. Default distribution strategy in Tensorflow. In traditional neural networks all the inputs and outputs are independent of each other but in cases like when it is required to predict the next word of a sentence the previous words are required and hence there is a need to remember the previous words. com blog 2017 12 fundamentals of deep learning introduction to lstm What are LSTM s https www. com shivajbd understanding input and output shape in lstm keras c501ee95c65eThe first line model. zip or you can search for GloVe in datasets on Kaggle and add the file LSTM s Basic OverviewSimple RNN s were certainly better than classical ML algorithms and gave state of the art results but it failed to capture long term dependencies that is present in sentences. com what are word embeddings The latest approach to getting word Embeddings is using pretained GLoVe or using Fasttext. Also after discussing all of these ideas I will present a starter solution for this competiton This kernel has been a work of more than 10 days If you find my kernel useful and my efforts appreciable Please Upvote it it motivates me to write more Quality content Configuring TPU sFor this version of Notebook we will be using TPU s as we have to built a BERT ModelWe will drop the other columns and approach this problem as a Binary Classification Problem and also we will have our exercise done on a smaller subsection of the dataset only 12000 data points to make it easier to train the modelsWe will check the maximum number of words that can be present in a comment this will help us in padding laterWriting a function for getting auc score for validation Data Preparation Before We BeginBefore we Begin If you are a complete starter with NLP and never worked with text data I am attaching a few kernels that will serve as a starting point of your journey https www. org learn nlp sequence models lecture agZiL gated recurrent unit gru https www. Without going into too much details I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it and since I am a fan of GloVe vectors word2vec and fasttext. About this NotebookNLP is a very hot topic right now and as belived by many experts 2020 is going to be NLP s Year with its ever changing dynamics it is experiencing a boom same as computer vision once did. html Implementation of Encoder Decoder Model from scratch https www. Transformers were introduced in the paper Attention is all you need by Google. org tutorials intermediate seq2seq_translation_tutorial. co transformers main_classes tokenizer. What keras Tokenizer does is it takes all the unique words in the corpus forms a dictionary with words as keys and their number of occurences as values it then sorts the dictionary in descending order of counts. 96 which is quite commendable also we close in on the gap between accuracy and auc. I am attaching more resources if you want NLP end to end 1 Books https d2l. So let s suppose word the occured the most in the corpus then it will assigned index 1 and vector representing the would be a one hot vector with value 1 at position 1 and rest zereos. This is the part where I spent the most time on and I suggest you do the same. I wrote a kernel on the Tweet Sentiment Extraction competition that has now got a gold medal it can be viewed here https www. Now these architectures can be used in two ways 1 We can use the model for prediction on our problems using the pretrained weights without fine tuning or training the model for our sepcific tasks EG http jalammar. html here TokenizationFor understanding please refer to hugging face documentation again Starting TrainingIf you want to use any another model just replace the model name in transformers. com watch v Ilg3gGewQ5U list PL_h2yd2CGtBHEKwEH5iqTZH85wLS eUzv index 3 https www. word_index simply gives the dictionary of vocab that keras created for us Building the Neural NetworkTo understand the Dimensions of input and output given to RNN in keras her is a beautiful article https medium. I invite you all to come and learn alongside with me and take a step closer towards becoming an NLP expert ContentsIn this Notebook I will start with the very Basics of RNN s and Build all the way to latest deep learning architectures to solve NLP problems. Sequential tells keras that we will be building our network sequentially. Deep learning is really revolutionary Word EmbeddingsWhile building our simple RNN models we talked about using word embeddings So what is word embeddings and how do we get word embeddings Here is the answer https www. GRU s are a variation on the LSTM because both are designed similarly and in some cases produce equally excellent results. Join me and make these NLP competitions your first without being overwhelmed by the shear number of techniques used. No parameters necessary if TPU_NAME environment variable is set this is always the case on Kaggle. Owing to its popularity Kaggle launched two NLP competitions recently and me being a lover of this Hot topic prepared myself to join in my first Kaggle Competition. Thus RNN came into existence which solved this issue with the help of a Hidden Layer. ai Attention ModelsThis is the toughest and most tricky part. org learn nlp sequence models lecture 6Oq70 word representation https machinelearningmastery. edu 2018 04 03 attention. com mindorks understanding the recurrent neural network 44d593f112a2 https www. com tanulsingh077 what s cookingBelow are some Resources to get started with basic level Neural Networks It will help us to easily understand the upcoming parts https www. In this Notebook I ll be using the GloVe vectors. com tanulsingh077 twitter sentiment extaction analysis eda and model After 10 days of extensive learning finishing all the latest NLP approaches I am back here to share my leaning by writing a kernel that starts from the very Basic RNN s to built over all the way to BERT. com attention and its different forms 7fc3674d14dc https distill. Here is the reason why it s done https stackoverflow. ai chapter_recurrent neural networks rnn. com watch v 2E65LDnM2cA list PL1F3ABbhcqa3BBWo170U4Ev2wfsF7FN8l https www. ai chapter_recurrent modern encoder decoder. In Depth UnderstandingWhy LSTM s https www. 82 without much efforts and we know have learnt about RNN s. As I joined the competitions and since I was a complete beginner with Deep Learning Techniques for NLP all my enthusiasm took a beating when I saw everyone Using all kinds of BERT everything just went over my head I thought to quit but there is a special thing about Kaggle it just hooks you. It will cover the Following Simple RNN s Word Embeddings Definition and How to get them LSTM s GRU s BI Directional RNN s Encoder Decoder Models Seq2Seq Models Attention Models Transformers Attention is all you need BERTI will divide every Topic into four subsections Basic Overview In Depth Understanding In this I will attach links of articles and videos to learn about the topic in depth Code Implementation Code ExplanationThis is a comprehensive kernel and if you follow along till the end I promise you would learn all the techniques completelyNote that the aim of this notebook is not to have a High LB score but to present a beginner guide to understand Deep Learning techniques used for NLP. com understanding gru networks 2ef37df6c9be https www. This architecture is used in a lot of applications like Machine Translation text summarization question answering etc In Depth UnderstandingI will not write the code implementation for this but rather I will provide the resources where code has already been implemented and explained in a much better way than I could have ever explained. com sequence 2 sequence model with attention mechanism 9e9ca2a613a https towardsdatascience. pub 2016 augmented rnns Code Implementation https www. We have achieve similar accuracy and auc score as before and now we have learned all the types of typical RNN architectures We are now at the end of part 1 of this notebook and things are about to go wild now as we Enter more complex and State of the art models. Please read and view the following resources in the order I am providing to ignore getting confused also at the end of this try to write and draw an attention block in your own way https www. Try printing first 2 elements of xtrain_seq you will see every word is represented as a digit nowNow you might be wondering What is padding Why its doneHere is the answer https www. If you are able to understand the intiuition and working of attention block understanding transformers and transformer based architectures like BERT will be a piece of cake. com blog 2019 11 comprehensive guide attention mechanism deep learning Basic Level https pytorch. I thought I have to learn someday why not now so I braced myself and sat on the learning curve. GRU s were designed to be simpler and faster than LSTM s and in most cases produce equally good results and thus there is no clear winner. com Which effect does sequence padding have on the training of a neural network https machinelearningmastery. com xhlulu jigsaw tpu distilbert with huggingface and kerasSteps Involved Data Preparation Tokenization and encoding of data Configuring TPU s Building a Function for Model Training and adding an output layer for classification Train the model and get the resultsEncoder FOr DATA for understanding waht encode batch does read documentation of hugging face tokenizer https huggingface. io illustrated bert In Depth Understanding of BERTAfter going through the post Above I guess you must have understood how transformer architecture have been utilized by the current SOTA models. We represent every word as one hot vectors of dimensions Numbers of words in Vocab 1. com Why do we use an RNN instead of a simple neural network In Depth Understanding https medium. io This is subtle effort of contributing towards the community if it helped you in any way please show a token of love by upvoting linear algebra data processing CSV file I O e. Then while building the embedding layer we pass Embedding Matrix as weights to the layer instead of training it over Vocabulary and thus we pass trainable False. If you have understood the Attention models this will be very easy Here is transformers fully explained http jalammar. html https machinelearningmastery. You can download the GloVe vectors from here http www nlp. In Depth Explanation https towardsdatascience. _____ and use accordingly End NotesThis was my effort to share my learnings so that everyone can benifit from it. Then we first add the Embedding layer. ai chapter_recurrent modern machine translation and dataset. org learn nlp sequence models home welcome Fast. org lecture natural language processing tensorflow padding 2CyzsAlso sometimes people might use special tokens while tokenizing like EOS end of string and BOS Begining of string. com illustrated guide to lstms and gru s a step by step explanation 44e9eb85bf21 Code ImplementationWe have already tokenized and paded our text for input to LSTM s Code ExplanationAs a first step we calculate embedding matrix for our vocabulary from the pretrained GLoVe vectors. html Code Implementation Code ExplanationCode is same as before only we have added bidirectional nature to the LSTM cells we used before and is self explanatory. org learn nlp sequence models lecture BO8PS different types of rnns. We see that in this case we used dropout and prevented overfitting the data GRU s Basic OverviewIntroduced by Cho et al. io illustrated transformer Code Implementation http nlp. I have shared all the resources I used to learn all the stuff. html Code ImplementationSo first I will implement the and then I will explain the code step by step Code Explanantion Tokenization So if you have watched the videos and referred to the links you would know that in an RNN we input a sentence word by word. com data preparation variable length input sequences sequence prediction https www. com watch v IHZwWFHWa w list PL_h2yd2CGtBHEKwEH5iqTZH85wLS eUzv index 2 https www. com watch v IfsjMg4fLWQ list PLtmWHNX gukKocXQOkQjuVxglSDYWsSh9 index 8 t 0s Introduction to Seq2seq By fast. I recommend Finishing Part 1 before continuing as the upcoming techniques can be quite overwhelming Seq2Seq Model Architecture OverviewRNN s are of many types and different architectures are used for different purposes. org learn nlp sequence models lecture PKMRR vanishing gradients with rnns https www. html Implementation from Scratch in Pytorch Transformers Attention is all you needSo finally we have reached the end of the learning curve and are about to start learning the technology that changed NLP completely and are the reasons for the state of the art NLP techniques. ai Jason Brownlee s Books2 Courses https www. com watch v tIeHLnjs5U8 list PL_h2yd2CGtBHEKwEH5iqTZH85wLS eUzv index 4For Learning how to visualize test data and what to use view https www. com understanding bidirectional rnn in pytorch 5bd25a5dd66 https d2l. com watch v hinZO TEk4 t 2933s Tuning BERT For your TASKWe will be using the first example as a base for our implementation of BERT model using Hugging Face and KERAS but contrary to first example we will also Fine Tune our model for our taskAcknowledgements https www. org gated recurrent unit networks Code Implementation Bi Directional RNN s In Depth Explanation https www. com jagangupta stop the s toxic comments eda Simple RNN Basic OverviewWhat is a RNN Recurrent Neural Network RNN are a type of Neural Network where the output from previous step are fed as input to the current step. So in 1998 99 LSTM s were introduced to counter to these drawbacks. If you have followed along from the starting and read all the articles and understood everything these complex models would be fairly easy to understand. io a visual guide to using bert for the first time Using Pre trained BERT without Tuning2 We can fine tune or train these transformer models for our task by tweaking the already pre trained weights and training on a much smaller dataset EG https www. org learn nlp sequence models lecture fyXnn bidirectional rnn https towardsdatascience. com watch v aircAruvnKk list PL_h2yd2CGtBHEKwEH5iqTZH85wLS eUzv https www. org learn nlp sequence models lecture KXoay long short term memory lstm https distill. We could have used word2vec but the embeddings layer learns during training to enhance the embeddings. Here is a nice video explanining different types of model architectures https www. It took me 10 days to learn all of this you can learn it at your pace and dont give in at the end of all this you will be a different person and it will all be worth it. com tanulsingh077 twitter sentiment extaction analysis eda and model https www. io a ten minute introduction to sequence to sequence learning in keras. ai NLP Course3 Blogs and websites Machine Learning Mastery https distill. It then assigns the first value 1 second value 2 and so on. read_csv Detect hardware return appropriate distribution strategy TPU detection. ", "id": "tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert", "size": "18652", "language": "python", "html_url": "https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert", "git_url": "https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert", "script": "Flatten Bidirectional keras.layers tensorflow.keras.optimizers keras.callbacks tensorflow.keras.layers train_test_split keras.layers.core tensorflow.keras.callbacks Embedding build_model np_utils Dropout EarlyStopping Sequential LSTM Adam tensorflow.keras.models decomposition SimpleRNN seaborn numpy graph_objs GRU MaxPooling1D Input GlobalMaxPooling1D plotly fast_encode plotly.express keras.layers.embeddings ModelCheckpoint sklearn.model_selection sklearn metrics matplotlib.pyplot text Activation Dense tensorflow keras.utils pandas SpatialDropout1D kaggle_datasets BertWordPieceTokenizer tqdm BatchNormalization pipeline roc_auc keras.layers.recurrent Conv1D keras.preprocessing sequence Model model_selection tokenizers KaggleDatasets graph_objs as go keras.models plotly.figure_factory keras.layers.normalization preprocessing ", "entities": "(('We', 'Vocab'), 'represent') (('nlp sequence models', 'unit gru https agZiL gated recurrent www'), 'learn') (('tasks sepcific EG', 'jalammar'), 'use') (('previous words', 'hence previous words'), 'be') (('both', 'equally excellent results'), 'be') (('read_csv Detect hardware', 'distribution strategy TPU appropriate detection'), 'return') (('GRU s', 'equally good results'), 'design') (('it', 'similar word2vec'), 'be') (('transformer how architecture', 'SOTA current models'), 'illustrate') (('PLtmWHNX', 't 0s 8 Seq2seq'), 'gukKocXQOkQjuVxglSDYWsSh9') (('why we', 'nlp'), 'com') (('NLP', 'Books https 1 d2l'), 'attach') (('It', 'then first value'), 'assign') (('Why we', 'Depth Understanding https instead simple neural medium'), 'com') (('that', 'way'), 'tanulsingh077') (('nlp sequence models lecture PKMRR', 'rnns https www'), 'learn') (('now we', 'art models'), 'achieve') (('her', 'keras'), 'give') (('now model', '0'), 'be') (('TPU_NAME environment necessary variable', 'always Kaggle'), 'be') (('Why doneHere', 'digit nowNow'), 'try') (('Loading Dependencies', 'huggingface tokenizers library'), 'use') (('CompetitionAs I', 'jalammar'), 'html') (('nlp sequence models', 'rnns'), 'learn') (('I', 'already much better way'), 'use') (('that', 'journey https www'), 'present') (('everyone', 'it'), '_') (('we', 'network'), 'tell') (('NLP', 'techniques'), 'join') (('You', 'www nlp'), 'download') (('nlp sequence models', 'word representation https 6Oq70 machinelearningmastery'), 'learn') (('where output', 'current step'), 'stop') (('Here nice video', 'model architectures https www'), 'be') (('we', 'RNN s.'), '82') (('that', 'art NLP techniques'), 'be') (('recently me', 'Kaggle first Competition'), 'launch') (('we', 'better result'), 'add') (('It', 'parts https easily upcoming www'), 'com') (('com watch', 'view https www'), 'PL_h2yd2CGtBHEKwEH5iqTZH85wLS') (('LSTM 99 s', 'drawbacks'), 'introduce') (('embeddings layer', 'embeddings'), 'use') (('com watch Ilg3gGewQ5U v list', 'https eUzv index 3 www'), 'PL_h2yd2CGtBHEKwEH5iqTZH85wLS') (('which', 'neural standard recurrent network'), 'aim') (('answer https Here www', 'word embeddings'), 'be') (('Building', 'hugging face tokenizer https huggingface'), 'tpu') (('NLP step closer I', 'NLP problems'), 'invite') (('sequence padding', 'network https neural machinelearningmastery'), 'com') (('we', 'LSTM cells'), 'be') (('Embeddings', 'Fasttext'), 'com') (('we', 'word'), 'implement') (('I', 'GloVe vectors'), 'use') (('attention model intuition', 'Only video'), 'learn') (('why it', 'https stackoverflow'), 'be') (('you', 'same'), 'be') (('very Here transformers', 'fully http jalammar'), 'understand') (('thus we', 'trainable False'), 'pass') (('it', 'counts'), 'be') (('org', 'nlp sequence models'), 'learn') (('we', 'taskAcknowledgements https www'), 'watch') (('different it', 'this'), 'take') (('I', 'stuff'), 'share') (('that', 'sentences'), 'search') (('also where input sequences', 'different lengths'), 'be') (('We', 'dataset EG https much smaller www'), 'io') (('which', 'Hidden Layer'), 'come') (('com watch', 'eUzv https www'), 'PL_h2yd2CGtBHEKwEH5iqTZH85wLS') (('it', 'CSV file'), 'be') (('I', 'GloVe vectors'), 'explain') (('it', 'gold now medal'), 'write') (('someday why now so I', 'learning curve'), 'think') (('it', 'just you'), 'join') (('I', 'this'), 'be') (('org', 'nlp sequence models rnn https bidirectional towardsdatascience'), 'learn') (('What', 'com learning 2017 12 deep introduction'), 'blog') (('aim', 'NLP'), 'cover') (('I', 'https www'), 'approach') (('complex models', 'everything'), 'follow') (('we', 'pretrained GLoVe vectors'), 's') (('then it', '1 position'), 'let') (('org', 'nlp sequence models Seq2Seq Models https HyEui basic basic different blog'), 'learn') (('you', 'Google'), 'introduce') (('quite also we', 'accuracy'), 'close') (('I', 'own way'), 'read') (('we', 'Cho et al'), 'see') (('nlp sequence models', 'term memory lstm https KXoay long short distill'), 'learn') (('you', 'transformers'), 'html') (('you', 'cake'), 'be') (('com define encoder', 'sequence sequence model machine translation keras Basic Encoder Decoder neural Model'), 'decoder') (('sometimes people', 'BOS string'), 'padding') (('computer same vision', 'boom'), 'be') (('io', 'transformer Code Implementation nlp'), 'illustrate') (('are', 'different different purposes'), 'recommend') ", "extra": "['test']"}