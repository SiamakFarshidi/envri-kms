{"name": "handling multimodal distributions fe techniques ", "full_name": " h1 Tabular Playground 1 Bimodal Regression h1 Problem Definition h1 Modality h1 Data Overview h2 Target h2 cont Features h1 Model Baseline h1 Feature Engineering Techniques h2 Gaussian Mixture Modelling GMM h2 Binning h2 Statistical Features h2 Deep Feature Synthesis h2 Summary h1 EDA h1 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "com ttahara tps jan 2021 gbdts baseline instantiate a second axes that shares the same x axis we already handled the x label with ax1 otherwise the right y label is slightly clipped Initialize variables Prepare training and validation data Define model Calculate evaluation metric Root Mean Squared Error RMSE Make predictions Calculate evaluation metric for out of fold validation set Average predictions over all folds instantiate a second axes that shares the same x axis otherwise the right y label is slightly clipped ncols ncols. The most commonly known distribution is unimodal with only one peak. ModalityIn this challenge we will learn about the modality of a distribution. You can find the modality of a distribution by counting the number of its peaks. Therefore we do not necessarily need to transform the target to fit a normal distribution. com ttahara s notebook TPS Jan 2021 GBDTs Baseline https www. However we can also see that this highly depends on the quality of our features. We should probably drop it since it is only one single data point. Hopefully you now have a few ideas on how to investigate the features. For simplicity reasons the target equals feature1 if feature3 0 and else the targets equals feature2. If we look at the distributions of our predictions versus the ground truth we can see that our model is doing quite poorly. From the below results we can see that the LightGBM model is able to handle the bimodal distribution. Data Overview Target It looks like an overlay of two different distributions when the data distribution has two peaks it is called bimodal distribution. com c santander customer transaction prediction discussion 86951 Since the features have multimodal distributions it would be worthwhile checking what happens if we add a GMM feature for each cont feature. cont FeaturesLet s look at the cont features in bulk. You can list out all possibilities with the function list_primitives. How about dividing each feature by another feature using the primitive divide_numeric. This is called multimodal distribution. 000094 improvement to our baseline and our distribution is still not close to the distribution of the ground truth. If you need more inspiration I recommend the discussion section of the BNP Paribas Cardif Claims Management Competition https www. In the following minimal example we have three features and a target that has a bimodal distribution as shown in the plot. distplot train_df var ax ax i 0 sns. You can see that we end up with 196 features. 703148 n a GMM class 0. However in this challenge we do not have any information about the features. From a top level point of view we can see that none of the features seem to be correlated to the target and cont14. Let s see if we can get any interesting insights if we difference the features with np. they are highly correlated to each other e. what is going on We definitely need to dig deeper here before we can start building our model. read_csv Configurations Unimodal Bimodal instantiate a second axes that shares the same x axis otherwise the right y label is slightly clipped Multimodal instantiate a second axes that shares the same x axis otherwise the right y label is slightly clipped Drop one outlier instantiate a second axes that shares the same x axis otherwise the right y label is slightly clipped for i var in enumerate train_df. startswith cont sns. There is no missing data but instead we have a different obstacle that we have to overcome bimodal distribution of the target variable and multimodal distributions of the features. Without further context we are given some features with continuous values to predict a continuous target. This is probably due to the fact that although we have more features now a. Happy Kaggling Submission linear algebra data processing CSV file I O e. If we were given the names of each feature we could start by creating new features based on our intuition. If you have two peaks it is called bimodal and if you have three or more peaks then it is called multimodal. There are unimodal bimodal and multimodal distributions. com c bnp paribas cardif claims management discussion. 00004 In contrast to the other feature engineering techniques with the binning features we get a worse score than with the other techniques. However for 14 features that will take a lot of time. Therefore we cannot create new features by intuition. EDAFrom above section we understand the urgency of gaining some insights from the data. 00003 DFS multiply_numeric 0. Can tree based models handle bimodal distributions TL DR Yes tree based models in general should be able to handle bimodal distributions. 703148 this is our benchmark for the next steps. 00081 DFS divide_numeric 0. boxplot train_df var ax ax i 1 stats. Model BaselineAlothough we did not do any feature engineering so far let s create a simple LightGBM baseline to get a benchmark first. An idea would be to randomly combine features and hoping to see a correlation to our target. we lose some information when binning features. There is exactly one data point with a target value of 0 this looks very much like an outlier. Log transformFrom the above experiments we can see that cont2 cont3 cont4 and cont13 seem to be the most important features for our model. Additionally there seems to be a highly correlated cluster consisting of cont1 and cont6 through cont13. Feature Engineering TechniquesWe have seen that our baseline model in theory is able to model the bimodal distribution of our target. The baselibe in Tawara https www. Let s try a couple of primitives. probplot train_df var plot ax i 2 Baseline model parameters copied from https www. com willkoehrsen automated feature engineering tutorial. This is probably also the most comfortable distribution to work with. It is an unsupervised learning algorithm. With the newly added features we get a RMSE of 0. We already saw that our target variable has a bimodal distribution. Deep Feature SynthesisAnother common approach is creating new features by combining features with basic mathematical operations addition subtraction multiplication division. Let s try another idea What happens if we use the classes we got from the GMM to separate each feature BinningWork in progress Statistical FeaturesAnother common approach is to create new features from the data s statistics mean sum std etc. 000089 Statistical Features 0. Now we have some feature distributions with multiple peaks. If you wanto read more about this topic in depth I highly recommend this kernel Automated Feature Engineering Tutorial https www. We can somewhat automate this with Deep Feature Synthesis. Discussion When you have bimodal distribution https www. In this section we will be exploring different feature engineering techniques. Therefore we would expect tree based models to be able to handle bimodal distributions without any transformations as well. Differencing means that we take the difference between two consecutive datapoints of a column. collinearity between cont1 and cont1_bin10 and b. Gaussian Mixture Modelling GMM We can use Gaussian Mixture Modelling to separate the two distributions. 000115 Binning 0. Tabular Playground 1 Bimodal Regression Problem Definition Problem Definition Modality Modality Data Overview Data Overview Target Target cont Features cont Features Model Baseline Model Baseline Feature Engineering Techniques Feature Engineering Techniques Gaussian Mixture Modelling Gaussian Mixture Modelling Binning Binning Statistical Features Statistical Features Deep Feature Synthesis Deep Feature Synthesis Summary Summary EDA EDA Problem DefinitionIn this challenge we are asked to build a regression model. Let s use our baseline model as is and use it for modelling the data. 000094 GMM separated 0. Decision trees are insensitive to the targets distribution. In the Data Overview Data Overview we saw that the features have a low absolute correlation to the target. For example if you have two features hours_spent_writing_kernels and number_of_kernels you could combine them by dividion to get a new feature average_time_to_write_a_kernel hours_spent_writing_kernels number_of_kernels. Also don t get it confused with multimodal learning which describes problems with mixed feature modalities such as pictures and text. For below plots we can see some odd distributions all cont features show multiple peaks with no sign of a normal distribution. This score is quite bad since RMSE of 0 would be ideal. Let s explore them a little more. The out of fold OOF RMSE score is 0. This way we can practice on focussing on the data without requiring any specific domain knowledge because the column names cont do not indicate any further information. The data does not seem to be skewed and therefore does not necessarily need to be transformed if non tree based models are used for tree based models this would not matter anyways. For this purpose we will concatenate the training and the testing data and sort them by the column id. Let s also try it for multiplication SummarySo far the feature engineering techniques only gave us minor improvements OOF RMSE Delta to Benchmark Benchmark 0. Let s see if this is true. com ttahara tps jan 2021 gbdts baseline will be the base for our experiments. ", "id": "iamleonie/handling-multimodal-distributions-fe-techniques", "size": "8923", "language": "python", "html_url": "https://www.kaggle.com/code/iamleonie/handling-multimodal-distributions-fe-techniques", "git_url": "https://www.kaggle.com/code/iamleonie/handling-multimodal-distributions-fe-techniques", "script": "sklearn.metrics run_model featuretools scipy sklearn.model_selection GaussianMixture KFold mean_squared_error seaborn numpy matplotlib.pyplot lightgbm stats sklearn.mixture pandas get_gmm_class_feature visualize_results ", "entities": "(('this', 'next steps'), '703148') (('also this', 'features'), 'see') (('most commonly known distribution', 'only one peak'), 'be') (('tree', 'transformations'), 'expect') (('feature engineering far techniques', 'OOF RMSE Benchmark Benchmark'), 'let') (('so far s', 'benchmark'), 'do') (('Statistical Deep Feature Synthesis Deep Feature Synthesis Summary EDA Problem Summary we', 'regression model'), 'Model') (('Therefore we', 'intuition'), 'create') (('quite RMSE', '0'), 'be') (('it', 'two peaks'), 'Target') (('that', 'time'), 'however') (('feature3 0 else targets', 'feature2'), 'equal') (('idea', 'target'), 'be') (('You', 'peaks'), 'find') (('we', 'column'), 'concatenate') (('this', 'very much outlier'), 'be') (('we', 'more features'), 'be') (('then it', 'three peaks'), 'call') (('cont features', 'normal distribution'), 'see') (('we', '196 features'), 'see') (('we', 'distribution'), 'modalityin') (('You', 'list_primitives'), 'list') (('that', 'plot'), 'have') (('we', 'feature engineering different techniques'), 'explore') (('When you', 'bimodal distribution https www'), 'discussion') (('we', 'features'), 'have') (('Hopefully you', 'how features'), 'have') (('we', 'np'), 'let') (('we', 'intuition'), 'start') (('I', 'BNP Paribas Cardif Claims Management Competition https www'), 'recommend') (('TL DR Yes based models', 'bimodal general distributions'), 'handle') (('I', 'highly kernel'), 'recommend') (('cont3 cont2 cont4', 'most important model'), 'log') (('y same axis otherwise right label', 'ncols slightly clipped ncols'), 'instantiate') (('Decision trees', 'targets distribution'), 'be') (('com ttahara tps gbdts jan 2021 baseline', 'experiments'), 'be') (('this', 'tree based models'), 'seem') (('we', 'data'), 'understand') (('We', 'Deep Feature Synthesis'), 'automate') (('you', 'new feature'), 'combine') (('Therefore we', 'normal distribution'), 'need') (('var plot Baseline model probplot train_df i 2 parameters', 'https www'), 'ax') (('they', 'highly other e.'), 'correlate') (('target already variable', 'bimodal distribution'), 'see') (('we', 'continuous target'), 'give') (('distribution', 'ground truth'), 'improvement') (('we', 'other techniques'), '00004') (('we', 'features'), 'be') (('we', '0'), 'get') (('LightGBM model', 'bimodal distribution'), 'see') (('it', 'probably it'), 'drop') (('Gaussian Modelling We', 'two distributions'), 'mixture') (('features', 'target'), 'see') (('Now we', 'multiple peaks'), 'have') (('none', 'target'), 'see') (('column names cont', 'further information'), 'practice') (('model', 'ground truth'), 'see') (('Happy Kaggling Submission', 'linear algebra data CSV file'), 'process') (('which', 'such pictures'), 'get') (('baseline model', 'target'), 'see') (('we', 'cont feature'), 'com') (('s', 'data'), 'let') (('slightly i', 'enumerate'), 'instantiate') (('we', 'sum std statistics mean etc'), 'let') (('definitely deeper here we', 'model'), 'need') (('we', 'when features'), 'lose') (('Deep Feature SynthesisAnother common approach', 'operations addition subtraction multiplication basic mathematical division'), 'create') (('we', 'column'), 'mean') ", "extra": "['test']"}