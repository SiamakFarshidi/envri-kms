{"name": "convolutional neural nets cnns explained ", "full_name": " h1 1 Introduction h1 2 Before we start h1 3 Convolutional Neural Networks h2 3 1 Why FNNs might not be the best approach h2 3 2 Youtube Videos to save you time h2 3 3 Convolutions h2 3 4 Computing a convolutional kernel h1 4 Understanding Convolutions h2 4 1 Imports and Data Preparation h3 Libraries h3 Seed h2 4 2 Create the Convolutions h2 4 3 Parameters weights of the Convolutional Layer h2 4 4 Visualize Convolutions h2 4 5 Another Example Increasing Padding and Stride h1 5 AlexNet h1 6 MNIST Classification using CNNs h2 6 1 CNN MNISTClassifier neural network h2 6 2 Understanding how the Network Works h2 6 3 Training on all Images h3 6 3 1 Accuracy Function h3 6 3 2 Training Function h3 6 3 3 Training h1 Bonuses h2 1 Confusion Matrix h2 2 2D Visualization of Convolutional Neural Nets h1 Other How I taught myself Deep Learning Notebooks h1 References ", "stargazers_count": 0, "forks_count": 0, "description": "I am by no means a teacher but in this notebook I will 1. For more information head here https towardsdatascience. AlexNet AlexNet is a very popular CNN architecture that is capable of achieving high accuracies in classifying 1000 different groups animals breeds onjects etc. 2 Understanding how the Network Works How the schema of this example looks 6. 2 Youtube Videos to save you time Watch these 2 Youtube videos to grasp a better understanding of CNNs. So if you are frustrated just like I was when I started this stuff I hope the following guidelines will help you. How I taught myself Deep Learning Vanilla NNs https www. Convolutional Neural Networks Pro Tip Use this tool http alexlenail. Filters like neurons in human visual cortex in smaller layers detect edges lines and as the layers increase they start detecting shapes patterns and even faces objects. html to create your own convolutional neural nets. Recurrent Neural Networks and LSTMs Explained https www. Convolutional Neural Networks explainedSiraj Raval CNNs The Math of Intelligence 3. 1 CNN_MNISTClassifier neural networkThe Architecture will contain 2 main parts the features part which will take the images and put them through 2 convolutional layers that will learn features from the data the classifier a feed forward neural net containing 2 hidden layers and 1 output layer that will classify based on the learned features the image. There are 1000 different groups in which an image can be classified. 1 Imports and Data Preparation Libraries Seed 4. 4 Computing a convolutional kernel Convolutional Filter Convolutional Layer Kernel FilterFeature Map Activation Map 4. 2D Visualization of Convolutional Neural Nets Here is a very good resource to better visualize and understand MNIST Classification using CNN. Understanding Convolutions Let s create some convolutions on an image sample. 2 Training Function We ll use the same train function used in the How I taught myself Deep Learning Vanilla NNs https www. There is one bias for each output channel. com andradaolteanu how i taught myself deep learning 1 pytorch fnn notebook. If you liked this upvote Cheers References CNNs Explained https www. FNNs are prone to overfitting. So please be patient with yourself and if you don t understand something right away continue reading coding and it will all make sense in the end. They do not take the 2D geometry of the image into account. com alexnet the architecture that challenged cnns e406d5297951 Build you own neural network http alexlenail. This means there is no notion of proximity. ReLU It simply takes all the negative numbers in filter and turns them into 0 So a natural techinque during convolutions is Conv2d ReLU MaxPool 6. classifier it is the classification part which takes the last output last activation maps of the features part and takes them through a FNN to output a prediction. This means they learn some features very well instead of learning to generalize. 2 Create the Convolutions To create Convolutions you need to have in_channels the number of input channels in this case 3 for MNIST data 1 out_channels the number of output channels any number this is a hyperparameter that can be tuned kernel_size the size of the filter this one is a matrix of 5 by 5 25 weights in total padding the amount of pixels outside the image used to keep the size of the image intact stride the amount of pixels the filter is jumping this one shrinks the image in size Example original image 6x6 pixels kernel_size 3x3 padding 1 stride 2 Let s first visualize the way this convolution works Note You can see that the Convolution alters a bit the size of the image from 320x320 to 316x316. Confusion Matrix 2. com andradaolteanu how i taught myself deep learning 1 pytorch fnn Recurrent Neural Networks and LSTMs Explained https www. For somebody that starts in this area with no background whatsoever it can be very confusing especially because I seem to be unable to find code with many explanations and comments. ca aharley vis conv flat. This notebook is made to bring more clear understanding of concepts and coding so this would also help me add modify and improve it. com alexnet the architecture that challenged cnns e406d5297951 6. This method also introduces a degree of spatial invariance. Instead 1 neuron in the Hidden layer connects with ALL pixels from the image NO matter their position in that image. com andradaolteanu recurrent neural networks and lstms explained 1 Layer RNNs Multiple Neurons RNN Vanilla RNN for MNIST Classification Multilayer RNNs Tanh Activation Function Multilayer RNN for MNIST LSTMs and Vanishing Gradient Problem Bidirectional LSTMs LSTM for MNIST Classification 3. com andradaolteanu how i taught myself deep learning 1 pytorch fnn notebook only change is that we ll create an Adam optimizer instead of SGD. Notice there is a much higher Test Accuracy for the CNN model vs the plain Vanilla FNN in the last notebook. 4 Visualize Convolutions 4. To compute the new image shape after each convolution use the following formula Output W K 2P S 1 x W K 2P S 1 4. So to reduce computational load after each Convolution we can call MaxPool2d to reduce the activation map size to half. The human eye detects features in an image locally meaning that it looks at portions of a picture and recognizes patterns whereas FNNs don t. Share articles videos I watched that TRULY helped2. However removing any layer or changing any of it s parameters could drastically degrade it s performance. 3 Convolutions Convolutions solve the issues encoutered by FNNs they take spatial structure into account they are made up by weights which learn to identify patterns and fire when they find one same weights are used for the entire image weight sharing you can specify multiple features in one convolutional layer So convolutional layers have multiple filters composed by weights that learn patterns in the data by sliding over the entire image. Each bias is added to every element in that output channel. Introduction This notebook is just me being frustrated on deep learning and trying to understand in baby steps what is going on here. com watch v YRhxdVk_sIs t 1s CNNs The math of intelligence https www. Convert to Tensor 2. 1 Accuracy Function We ll use the same accuracy function used in the How I taught myself Deep Learning Vanilla NNs https www. com watch v FTr3n7uBIuE t 2201s AlexNet https towardsdatascience. html Other How I taught myself Deep Learning Notebooks How I taught myself Deep Learning Vanilla NNs https www. features it is the convolutional part of the network that identifies patterns classifier can be called by alexNet. In addition it usually works with images and text while ML usually works with tabular data. 3 Parameters weights of the Convolutional Layer FNN the trainable parameters include the network weights and biases one weight for each connection one bias for each output unit CNN the trainable parameters include the convolutional kernels filters and also a set of biases. Explain code along the way to the best of my ability Note Deep learning coding is VERY different in structure than the usual sklearn for machine learning. 1 Why FNNs might not be the best approach For image classification Feed Forward Neural Nets are not the best approach for a number of reasons 1. 3 Training on all Images 6. com andradaolteanu recurrent neural networks and lstms explained If you have any questions please do not hesitate to ask. com andradaolteanu how i taught myself deep learning 1 pytorch fnn PyTorch and Tensors Neural Network Basics Perceptrons and a Plain Vanilla Neural Net model MNIST Classification using FNN Activation Functions Forward Pass Backpropagation Loss and Optimizer Functions Batching Iterations and Epochs Computing Classification Accuracy Overfitting Data Augmentation Weight Decay Learning Rate Dropout and Layer Optimization 2. html Visualize Convolutions in 2D MNIST Dataset https www. First we make sure we disable Gradient Computing Model in Evaluation Mode Append 1 more dimension for batching Prediction. For example if we have a 100x100 pixel image 10 000 neurons in the first layer. It is composed of features and classifier features can be called by alexNet. Module here we define the structure of the network Convolutional Layers that learn patterns in the data output size 28 3 0 1 1 26 activation function 26 2 13 output size 13 3 0 1 1 11 activation function 11 2 5 FNN which classifies the data 10 channels 5 by 5 pixel output 10 possible predictions here we take the images through the network Take the image through the convolutions Reshape the output to vectorize it Log Probabilities output Apply softmax Create Model Instance Importing the MNIST data Select only 1 image index 13 Add 1 more dimension for batching 1 image in the batch Print Information Create Log Probabilities Create criterion and optimizer Compute loss Label Example has been transformed to tensor and reshaped so it suits the requirements of function Backpropagation Clears all gradients Compute gradients with respect to the loss Update parameters Import the train and test data Transforms data to Tensors using transforms Select only first 500 instances from each to make training fast Sets the model in evaluation mode Creates the dataloader Is formed by 20 images by default with 10 probabilities each Choose maximum probability and then select only the label not the prob number First check how many are correct in the batch then we sum then convert to integer not tensor Total cases Create dataloader for training dataset so we can train on multiple batches Shuffle after every epoch Create criterion and optimizer Losses Iterations to keep all losses during training for plotting Train and test accuracies to keep their values also for plotting Train the data multiple times Set model in training mode Create log probabilities Clears the gradients from previous iteration Computes loss how far is the prediction from the actual Computes gradients for neurons Updates the weights Save information after this iteration Compute accuracy after this epoch and save Show Accuracies Show the last accuracy registered Create plots Create Model Instance Train. FNNs require many connections therefore they have many weights parameters to compute. If the second layer has 500 neurons then we end up with 10 000x500 5 000 000 weights. MNIST Classification using CNNs Finally let s put everything into practice. MaxPool Data is spatially autocorrelated if a given pixel is green an adjacent pixel is more likely to be a different tone of green than bright pink. 5 Another Example Increasing Padding and Stride Let s visualise what has happened new activation map size remember activation maps are the result of the filters applied to the image or another activation map 316 10 2x2 2 1 156 5. html Original Image is formed by 3 channels Original pixels are the sum of R G B Filters of size 3 x 3 containing trainable weights that identify patterns Imports for alexnet model When running on the CuDNN backend two further options must be set Set a fixed value for the hash seed Import an image example See Image and Shape Before aplying any convolutions we need to change the structure of the image 1. Bring the channel in front Add 1 more dimension for batching 1 because we have only 1 image Create 1 convolutional layer Padding and Stride are at default values Apply convolution to the image Select Convolution Parameters 2 sets of parameters in total there are 5 filters for 3 channels with size 5 by 5 5 biases for each filter Convert result to numpy Remove the dim 1 batch Normalize to 0 1 for plotting Plotting the channels We ll use the same image of Suki to continue with this example Create another convolutional layer a second one Apply convolution to the LAST convolution created Convert result to numpy Remove the dim 1 batch Normalize to 0 1 for plotting Plotting the channels Import alexNet AlexNet Structure Creating the Architecture the class inherits from nn. We ll also introduce the notions of kernel_size stride padding 4. Before we start This is my second notebook in the series How I taught myself Deep Learning. Hence there is large computing cost. ", "id": "andradaolteanu/convolutional-neural-nets-cnns-explained", "size": "11251", "language": "python", "html_url": "https://www.kaggle.com/code/andradaolteanu/convolutional-neural-nets-cnns-explained", "git_url": "https://www.kaggle.com/code/andradaolteanu/convolutional-neural-nets-cnns-explained", "script": "torch.nn.functional torch.optim __init__ train_network forward torch.nn transforms get_accuracy CNN_MNISTClassifier(nn.Module) seaborn numpy matplotlib.pyplot torchvision.datasets set_seed get_confusion_matrix torchvision MNIST ", "entities": "(('we', 'first layer'), 'have') (('following guidelines', 'you'), 'be') (('So natural techinque', 'convolutions'), 'ReLU') (('we', 'instead SGD'), 'andradaolteanu') (('image classification Feed Forward Neural best Nets', 'reasons'), 'be') (('Youtube 2 Videos', 'CNNs'), 'watch') (('com andradaolteanu recurrent neural networks', 'MNIST Classification'), 'explain') (('also me', 'it'), 'make') (('CNN very popular that', 'onjects'), 'be') (('that', 'entire image'), 'solve') (('we', 'image'), 'form') (('You', '316x316'), 'create') (('therefore they', 'weights many parameters'), 'require') (('then we', '000 10 5 000 weights'), 'end') (('com how i', 'Tensors Neural Network Basics Plain Vanilla Neural Net model MNIST FNN Activation Functions'), 'andradaolteanu') (('you', 'questions'), 'explain') (('trainable parameters', 'also biases'), 'FNN') (('drastically it', 'it'), 's') (('Finally s', 'practice'), 'let') (('adjacent pixel', 'bright pink'), 'autocorrelate') (('you', 'https www'), 'explain') (('com how i', 'pytorch fnn deep 1 notebook'), 'andradaolteanu') (('Instead 1 neuron', 'image'), 'connect') (('W 1 K', 'S'), '2p') (('s', 'image sample'), 'let') (('it', 'end'), 'be') (('com architecture that', 'cnns'), 'alexnet') (('They', 'account'), 'take') (('How I', 'Deep Vanilla https Learning NNs www'), 'Function') (('bias', 'output channel'), 'add') (('that', 'alexNet'), 'feature') (('I', 'notebook'), 'be') (('locally it', 'FNNs don t.'), 'feature') (('com how i', 'Recurrent Neural Networks'), 'andradaolteanu') (('they', 'even objects'), 'detect') (('what', 'baby steps'), 'introduction') (('Convolutional Neural Networks', 'Intelligence'), 'explainedSiraj') (('they', 'features'), 'mean') (('we', 'half'), 'call') (('Deep learning coding', 'machine learning'), 'explain') (('composed', 'classifier alexNet'), 'call') (('How I', 'Deep Vanilla https Learning NNs www'), 'html') (('We', 'kernel_size stride padding'), 'introduce') (('remember', 'image'), '5') (('MNIST better Classification', 'CNN'), 'Visualization') (('image', '1000 different which'), 'be') (('output 1 that', 'image'), 'cnn_mnistclassifi') (('This', 'proximity'), 'mean') (('classification which', 'prediction'), 'classifier') (('we', 'Prediction'), 'make') (('Show Accuracies', 'Create plots Create Model Instance last accuracy registered Train'), 'define') (('We', 'nn'), 'bring') (('method', 'spatial invariance'), 'introduce') (('very especially I', 'many explanations'), 'be') (('ML', 'usually tabular data'), 'work') (('How I', 'Deep Vanilla https Learning NNs www'), 'teach') (('tool', 'alexlenail'), 'Networks') (('How I', 'Deep Learning'), 'be') (('How schema', 'example'), 'understand') ", "extra": "['patient', 'test']"}