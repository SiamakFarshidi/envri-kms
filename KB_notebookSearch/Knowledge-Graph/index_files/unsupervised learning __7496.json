{"name": "unsupervised learning ", "full_name": " h1 Clustering for dataset exploration h1 Visualization with hierarchical clustering and t SNE h1 Decorrelating your data and dimension reduction h1 Discovering interpretable features h2 Non negative matrix factorization h2 Using scikit learn NMF h2 NMF components h2 NMF features h2 Sample reconstruction h2 NMF learns interpretable parts h3 NMF learns topics of documents h2 Which articles are similar to Cristiano Ronaldo ", "stargazers_count": 0, "forks_count": 0, "description": "Learning rate should be carefully chosen wrong choice can bunch points together values between 50 and 200 can be chosen. Cant extend the map to new data. After you are done take a moment to recognise the topic that the articles about Anne Hathaway and Denzel Washington have in common Which articles are similar to Cristiano Ronaldo You learned how to use NMF features and the cosine similarity to find similar articles. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. Principal Components Principal components is the direction of variance PCA Aligns principal componenets with the axesAvailable as components_ attribute of PCA objectEach row defines displacement from mean Decorrelating the grain measurements with PCA You observed in the previous exercise that the width and length measurements of the grain are correlated. predict articles df pd. plotting sepal length and petal length Preparing seeds for clustering by dropping the target column Inertia decreases from 3 to 4 very slowly so 3 can be a good choice A vs LKG P vs C WK vs A_Coeff Variance comparison between Proline and OD280 After scaling we get tight clusters TSNE on Seed Data Create a TSNE instance model Apply fit_transform to samples tsne_features Select the 0th feature xs Select the 1st feature ys Scatter plot coloring by variety_numbers the t SNE visualization manages to separate the 3 varieties of grain samples. note that This will be a row corresponding to these topics from the NMF Features matrix which is of topic component dimension Print the row for Anne Hathaway Print the row for Denzel Washington Compute the dot products similarities dot product is cosine hence cosine simialrity. New samples can be assigned to existing cluster K means remembers the mean of each cluster Cluster Centroids Finds the nearest centroid for each new sample Evaluating a Cluster Inertia measures cluster quality Measures how spread the clusters are the lower the betterDistance from each sample to centroid of its clusterAfter fit available as attribute inertia_k means attemps to minimize the inertia when choosing the clusters The number of clusters More clusters means lower inertia A good clustering has tight clusters so low inertiabut not too many clustersChoose an elbow in the inertia plot where inertia begins to decrease more slowly Analysing seeds data from UCI Tranforming Features for Better Clustering Piedmont wine Dataset Feature Variances The wine features have very different variances spread of its values Hence the cross tabulation results has loose clusters too many values overlapping. Decorrelating your data and dimension reductionDimension reduction summarizes a dataset using its common occuring patterns. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. PCA is often used before supervised learning to improve model performance and generalization. Ex Clustering Customers by their purchases Compressing the data using purchase pattern dimentionality reduction Difference between Supervised and Unsupervised Learning Supervised learning finds the pattern for a prediction task unsupervised learning find patterns but without specific prediction in mind. For example it expresses documents as combinations of topics and images in terms of commonly occurring visual patterns. t sne features are different every time running again gives a different scatter plot although the relative position of clusters remain the same. decomposition import TruncatedSVD svd TruncatedSVD n_components 50 kmeans KMeans n_clusters 6 pipeline make_pipeline svd kmeans import pandas as pd pipeline. annotate seedlabel x y fontsize 5 alpha 0. fit articles labels pipeline. Now you ll use PCA to decorrelate these measurements then plot the decorrelated points and measure their Pearson correlation. It can also be useful for unsupervised learning. Verify this for yourself for the NMF model that you built earlier using the Wikipedia articles. For example you ll employ a variant of PCA will allow you to cluster Wikipedia articles by their content Visualizing the PCA Transformation Dimension Reduction Removes less informative noise features Principal Component Analysis Fundamental Dimension Reduction TechniqueFirst step is decorrealtion second is reducing dimension PCA aligns data with axes Rotates data samples to be aligned with axesShifts data samples so they have mean 0No information is lostPCA follows the fit transform pattern PCA Features Rows of tranformed corresponds to samplescolumns are PCA featuresFeatures of the data are correlated but but PCA Features are not linearly correlated. sort_values label nmf_features is a numpy array with shape Topics Components Lets consider the Topic Anne Hathaway and Denzel Washington and see what their NMF Features has to say. You ll also learn to use NMF to build recommender systems that can find you similar articles to read or musical artists that match your listening history Non negative matrix factorizationNMF is non negative matrix factorization Dimension reduction technique NMF models are interpretable unlike PCA All features must be non negative Using scikit learn NMFFollows fit transfrom pattern must specify number of components works with numpy array and with csr_matrix NMF componentsNMF has components just like PCA has principal componentsDimension of components dimension of samples Entries are non negative NMF featuresNMF Feature values are non negative Can be used to reconstruct the samples combine feature values with components Sample reconstructionMultiply feature values with components and add upSample can be expressed as the product of matrices hence the term Matrix Factorization NMF learns interpretable parts NMF learns topics of documentsWhen NMF is applied to documents the components correspond to topics of documents and the NMF features reconstruct the documents from the topics. Datasets used Iris K Means Clustering Finds clusters of samples Number of clusters must be specified in advance. Example Cluster labeled 2 has all the three class of Wines in it. read_csv Input data files are available in the. Previously you saw that the 3rd NMF feature value was high for the articles about actors Anne Hathaway and Denzel Washington. Applying Standard Scaler then KMeans in sklearn Pipeline Scaling without pipeline Visualization with hierarchical clustering and t SNE Hierarchical Clustering Cluster Labels in Hierarchical Clustering Cluster labels at any intermediate stage can be recoveredThen they can be used in cross tabulationHeight can be used to extract intermediate clustersHeight on the dendrogram is the distance between merging clusters Distance Between Clusters Defined by linkage methodSpecified via method parameter complete means distance between clusters is maximum distance between their samples Extraxcting Cluster Labels Use the fcluster methodReturns numpy array of cluster labels t SNE for 2 dimentional maps t distributed stochastic neighbor embeddingMaps samples to 2D or 3D spaceMap approximately preserves nearness of samplesGreat for inspecting datasets t SNE on iris data Iris samples are 4Dt SNE maps samples to 2D space Interpreting t SNE scatter plots versicolor and virginica harder to distinguishConsistent with intertia plot from KMeans both 2 and 3 clusters can be viewed t SNE in sklearn It has a fit_transform method no separate fit and transform methods exist. Now use PCA for dimensionality reduction of the fish measurements retaining only the 2 most important components. In this section you ll learn about the most fundamental of dimension reduction techniques Principal Component Analysis PCA. DataFrame label labels article titles print df. Clustering for dataset exploration Definition Unsupervised learning is all about finding patterns in the data. 75 WINE DATA Samples contain two wine features Seeds width length Data Scatter plot width vs length Calculate the Pearson correlation Display the correlation Apply the fit_transform method of model to grains pca_features Assign 0th column of pca_features xs Assign 1st column of pca_features ys Scatter plot xs vs ys Calculate the Pearson correlation of xs and ys Display the correlation Make a scatter plot of the untransformed points the coordinates of the mean of the data Get the first principal component first_pc Data varies the most along this direction Plot first_pc as an arrow starting at mean Keep axes on same scale FISH DATA Plot the explained variances from sklearn. Apply this to your NMF model for popular Wikipedia articles by finding the articles most similar to the article about the footballer Cristiano Ronaldo. Discovering interpretable featuresIn this chapter you ll learn about a dimension reduction technique called Non negative matrix factorization NMF that expresses samples as combinations of interpretable parts. Annotate the points for x y seedlabel in zip xs ys seed_labels plt. In this exercise identify the topic of the corresponding NMF component. Intrinsic dimensions Intrinsic dimension is the number of features needed to approximate the datasetIt is the essential idea behind dimension reductionIntrinsic dimention number of PCA Features with significant variance Dimension Reduction with PCA Dimension reduction of the fish measurementsIn a previous exercise you saw that 2 was a reasonable choice for the intrinsic dimension of the fish measurements. For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. ", "id": "impratiksingh/unsupervised-learning", "size": "7496", "language": "python", "html_url": "https://www.kaggle.com/code/impratiksingh/unsupervised-learning", "git_url": "https://www.kaggle.com/code/impratiksingh/unsupervised-learning", "script": "dendrogram PCA load_iris sklearn.cluster KMeans NMF scipy.sparse fcluster TSNE seaborn numpy sklearn.pipeline make_pipeline scipy.stats sklearn.decomposition scipy.cluster.hierarchy matplotlib.pyplot linkage sklearn.manifold pandas StandardScaler pearsonr csr_matrix sklearn.datasets TruncatedSVD normalize sklearn.preprocessing ", "entities": "(('dot products similarities', 'Denzel Washington Compute'), 'note') (('it', 'commonly occurring visual patterns'), 'express') (('features', 'topics'), 'learn') (('NMF feature 3rd value', 'actors Anne Hathaway'), 'see') (('Example Cluster', 'it'), 'label') (('PCA Features', 'data'), 'employ') (('width measurements', 'grain'), 'be') (('It', 'also unsupervised learning'), 'be') (('you', 'Principal Component Analysis PCA'), 'learn') (('NMF Features', 'Denzel what'), 'label') (('relative position', 'clusters'), 'be') (('You', 'cosine similar articles'), 'take') (('together values', '50'), 'choose') (('too many values', 'loose clusters'), 'mean') (('you', 'output'), 'list') (('you', 'Wikipedia earlier articles'), 'verify') (('2', 'fish measurements'), 'be') (('Clustering', 'data'), 'be') (('separate fit methods', 'fit_transform method'), 'apply') (('PCA', 'model performance'), 'use') (('Finds clusters', 'advance'), 'use') (('decomposition TruncatedSVD pipeline make_pipeline svd import 50 KMeans 6 kmeans', 'pd pipeline'), 'svd') (('Now you', 'Pearson correlation'), 'use') (('read_csv Input data files', 'the'), 'be') (('Data', 'sklearn'), 'contain') (('It', 'python docker image https kaggle github'), 'come') (('reductionDimension data reduction', 'common occuring patterns'), 'summarize') (('pattern', 'mind'), 'find') (('SNE t visualization', 'grain samples'), 'be') (('matrix factorization Non negative that', 'interpretable parts'), 'learn') ", "extra": "['biopsy of the greater curvature']"}