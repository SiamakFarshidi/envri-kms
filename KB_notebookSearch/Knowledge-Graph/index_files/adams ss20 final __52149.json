{"name": "adams ss20 final ", "full_name": " h1 ADAMS Assignment SS20 Prediction of Claps h1 Introduction h3 Description of Imports h1 Data exploration h2 Data import h2 Duplicate removal h2 Analysis of TotalClapCount h2 Language analysis h1 Data Cleaning h2 Publication Details in test set h1 Natural language processing and predictions h2 Tf Idf Vectorizer with Support Vector Regression h2 Test set predictions h2 BERT Multi classification h3 Predictions on validation set h3 Predictions on provided test set h1 Regression Analysis on non text features h2 Ridge Regression h3 Ridge Test set predictions h2 Light Gradient Boosting Machine h3 LGB Test set predictions h1 Ensembling h1 Conclusion h1 Bibliography ", "stargazers_count": 0, "forks_count": 0, "description": "com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. Lemmatization must be performed on the test set too. This can be understood better by looking at a fractioned totalClapCount in a value range between 1 and 200 as plooted in the second plot below. This is in line with the assumptions from above. This code cell allows for the VRAM CUDA Cache to be flushed opening it up for additional calculation. First the sentence_extract function uses the sentence tokenizer function in the nltk package to extract sentences. This is not guaranteed. walk kaggle input for filename in filenames print os. For analysis the Length equivalent in the train set wordCount is plotted against the totalClapCount below. Test train split along the totalClapLog is performed for the regression trainset with the number of claps dropped from the train and validation set afterwards. Stopwords are automatically removed as part of the lemmatization on the basis of the NLTK stopwords 11. The duplicate_remover function then takes the tokenized sentences and for each text checks whether the same sentence already exists in its cache. Results again revealed an overwhelming majority of English texts with a number of Spanish and Portuguese texts. The NLTK package provides a Lemmatizer function which is used here to Lemmatize each word. read_csv Input data files are available in the. Natural language processing prediction approaches are made using two separate approaches with the first one being a Tf Idf Support Vector Regression SVR followed by a BERT based multiclass classification approach. The imports here enable preprocessing prediction and error calculation with a random seed ensuring replicability. For the most parts the words contained in the title are present in the text column too and will be picked up by BERT automatically. The LGBM already achieves much better results than the Ridge regression. Calculating the number of times a single author has published something did not lead to significant insight as it is Netflix Technology and TE FOOD who make up the most often occurences with 175 and 34 times respectively. This allows words to be split into more frequent subwords and reduces the vocabulary size immensely. com max 968 1 F6SrJR7_s95r6oCF3ugMZw. com what is gradient accumulation in deep learning ec034122cfa 19 Apex. BERT Multi classification This implementation of a Bidirectional Encoder Representations from Transformers short BERT is inspired by publicly available work from Kaggle users yuval reina and Abhishek Thakur who both developed implementations for BERT in the KAGGLE environment providing valuable help with the use of GPU acceleration. Regression Analysis on non text featuresThe non text features present are deserving of further investigation too. Version 2 Bug fix thanks to chinhuic This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. Retrieved August 30 2020 from https scikit learn. long as the Crossentropyloss used in later execution expects. The first imports are for the data exploration and analysis part with adequate visiualization packages such as seaborn being imported too. BERT Multiclassification BERT Multiclassification 5. This format is necessary for the execution in the pyTorch environment. With no totalClapCount available the MAE and MSE cannot be calculated. Again the number of claps are not normal distributed with the log1p transformation helping but still hightlighting this fact. io illustrated bert 4 Kirk D. CrossEntropyLoss as implemented in pyTorch combines a LogSoftmax with a NLLLoss. Different calculation methods were tried with the ones commented out discarded for lack of improvement on the test set. Natural language processing and predictions The most important part of this predictive analysis is the Natural language processing part. Both algorithmic approaches aim to solve a numeric prediction task which means that human language level text must be transformed into numeric statistical representations to extract the necessary information. INSIDE MEDIUM S MELTDOWN How an idealistic Silicon Valley founder raised 134 million to change journalism then crashed into reality. To perform an adequate regression analysis some preparatory steps are necessary. The following graphic by Adrien Sieg shows how BERT fits into the general scheme of Natural Language Processing algorithms. This indicates that text cleaning is necessary which will be performed in the next step. A first rough combination of the predictions in one dataframe reveals how different the predictions value achieved by each one are. com httpwwwfszyc bert in keras taming create tensor dataset Y_train tensor type changed to long for Bert Multiclass as Crossentropyloss expects tensor. A combination of both text and non text data in one model would have been possible but this notebook provides the ability to compare them all in the same predictive task. html 20 Lasso vs ridge vs elastic net ml. For example running this by clicking run or pressing Shift Enter will list the files in the input directory for dirname _ filenames in os. With most of the columns being the object type the describe function highlights the numeric columns by giving their mean median and more information. To make restarting easier the adams 2020 folder contains train and test files where the text has already been cleaned using the cleaning functions implemented at the beginning of this notebook. The final ensemble output for the test set predictions. However bumps are visible at 50 and 100 claps. Both tensor types in the train set have to be of type torch. Seeing that they make up less than 0 4 of the test dataset these languages can be overlooked in NLP based analysis. Depending on the type of task predicting articles with a very high number of claps might be more preferable as they could for example be more actively recommended by the Medium. Tokenize package Nltk 3. The epsilon value specifies the epsilon tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value 12. The learning rate is the step size made at each iteration and needs to be set at a low enough rate to detect minima without being too low so that it gets stuck or overfits. The BERT model used here is one of these pre trained models. In their paper A tutorial on support vector regression Alex Smola and Bernhard Sch\u00f6lkopf describe the goal of a Support Vector regression to find a function f x that has at most \u03b5 deviation from the actually obtained targets for all the training data and at the same time is as flat as possible. Another advantage aside from the unmatched NLP prowess is its lack of requirement for extensive preprocessing. The apex install takes up a few minutes and cannot be skipped as Nvidia Apex is a CUDA requirement and not yet part of the Kaggle preinstalled packages. At this point it becomes important to take a closer look again at the test set. The optimum parameters are printed out after execution. It remains to be seen whether the BERT based predictions can improve upon the Tf Idf SVR predictions. Self attention allows the Transformer to determine encodings for each word with the help of other words in the input sequence and connecting them on a logical level thus using other relevant words in a sequence to understand the one currently being processed. Again features used here can only be created from information available in the test set. In Advances in neural information processing systems pp. com articles with a lot of claps are themselves being recommended to users through the use of recommender algorithms at the hand of Medium. Ridge RegressionRidge regression is a regression with a built in penalty term which is equal to the square of the coefficient 20. The average accuracy is calculated as described above. What is LightGBM How to implement it How to fine tune the parameters Medium. to device command which directly sends the underlying vector calculations to be performed on the device in this case defined above as the Cuda device. png A Support Vector Machine SVM is a supervised machine learning approach originally developed for classification tasks that can be successfully used for regression tasks. The margins include the errors neglected in the support vector machine while the support vector around the hyperplane try to separate the data into two separate spaces. float32 label Y_valid_regr lgb_pred np. The representation and meaning involves the processing and determination of sentence level meaning. Predictions on the validation data set look okay however the model does not seem to pick up on outlier cases. Support vector regression allows for relatively fast estimation while having adequate accuracy. correlation between responses and totalclapcount visible def sentence_extract text lang en return sent_tokenize text LANGUAGES. Tf Idf is widely being used in recommender algorithm for its accuracy and useability being of high relative efficiency in comparison to other word vectorization methods. The tensorflow BERT model gets converted to a pyTorch compatible BERT model. The following Natural language processing analyses try to extract information from text on different levels as can be compared to the process described above. The number of labels given over to the model is twenty for the number of categories to detect. Visualization of a Support Vector Machine Credit Javatpoint SVR https static. Natural language processing. However it presented itself as quite difficult to predict the claps for articles which have received thousands in some cases hundreds of thousands of likes. As Chowdhary in Fundamentals of Artificial Intelligence describes At the core of any NLP task there is the important issue of natural language understanding 7. The creation of bins is of much importance in this case of a multiclassification approximation for a regression task. The Dataloader object is iterated through in batches with a progressbar itarator visible for the duration of training in order to visualize progress. This way comparisons between the real value and predictive values can be made. A mean absolute error of 108 seems quite good in comparison to the 175 achieved through the use of the Ridge regression. These values are tensor values which need to be converted into legible values. It might be that using only two input variables is not enough. Designed and distributed by Microsoft it is has won many competitions aimed at predicting numeric variables 22. In the case of NLP subword pairs are merged together instead of being replaced by another byte. png attachment image. Predictions on validation setIn this first prediction part with the now trained BERT model predictions are made on the test set created from the train set above. The GPU is heavily used for training as it happens in parallel on the thousands of cores available on the GPU. Publication Details in test setThe publication details in the test set contain more information than is useful for a single column so an extraction of the date seems useful. Link to the Notebok on Kaggle https www. com uc export download id 1NHFBLsBD4bysTyANgAXhk79zoFrkA2Qy Actual training The actual training happens iteratively over many loops with the number of epochs as defined above being the number of times the whole training data will be run through in the course of training. predict X_valid_ridge. Unicode Error in the train set image. md machine learning challenge winning solutions 23 Mandot P. In gradient accumulation a configured number of steps are run with update of the model variables. With a mean wordCount of 901 and a median word count of 703 even the maximum sequence length of 512 for BERT does not suffice to capture all words for each article every time. Claps are a form of appreciation users can give an article on Medium. The train test split is based on the tokenized array and the target column with 20 of the data held back for test purposes. In the end all predictions were combined into final predictions that hopefully capture some of the information given to predict the claps. The most common values present in the dataset are very small which is why more bins were created in the lower range. long type instead of float Bug fix thanks to chinhuic optimizer. The distances between each bin increase in order to have a workable number of samples for each bin. Converting tokens to IDsCreate a smaller sample as the whole train_df is too large for efficient handling. The CrossEntropyLoss takes untransformed values as inputs. Data exploration Data exploration 3. The criterion needs to be defined before execution and initialized as an instance of CrossEntropyLoss. Regression Analysis on non text features Regression Analysis on non text features 1. Further improvements on the mean absolute error could not be made however the robustness behind the model as a result of the parameter tuning should have increased. org lasso vs ridge vs elastic net ml 21 Top medium stories. To help with the reversal a function that reassigns the original bin names was written too. This helps in achieving greater validity through the use of multiple differently drawn samples for the predictive models. In the case of this analysis performed here LGBM can serve as a good improvement upon the Ridge regression with much more refinement options and a good framework for big datasets 24. However runtimes can be very long even with significant computational backing. Predictions on provided test setThe predictions made here are for the provided test set. The LGBM seems promising for the Ensembling with the results of the text based predictions. best_iteration Just to be run if the code above has not been run in full. org stable modules generated sklearn. com louispk adams ss20 final As the notebook has to be set to public in order for the link to work it will only be public after the final submission deadline starting from the 1st of September 2020 guaranteeing no other person can access it before. Polyglot self describes as a natural language pipeline that supports massive multilingual applications 6 and is licensed as free software under a GPLv3 license. To do so the predictions are detached from the CPU further dimensions are removed through. This mechanism at the base of BERT helps it be successful at various tasks from question answering to text generation to text classification. To correct for negative numbers and the single extreme outlier value these are normalized to 0 and 50000 respectively. Machine learning has reached a point where the understanding of complex and context dependent texts is possible with high accuracy and allows for the capture of meaning and sentiments in language level tasks 3. Furthermore he posits NLP to be a multiple step process with the three main parts being 1. However many like codeBlockCount imageCount socialRecommendsCount and others are not present in the test set and cannot be realistically replicated with the information provided in the test set. A small improvement on the mean_absolute_error was achieved for some samples however results were not stable enough to consider standard scaling a definite improvement. It also seems to pick up quite well on the articles containing many more claps than the average. nbsp The BERT pre trained model from the GoogleAI has been described as mind blowing by machine learning developers involved with developing state of the art algorithms aiming for new highscores in comparison metrics such as GLUE and SQuAD 13. It is not task specific as each prediction approach used in this notebook requires its own preprocessing and data cleaning appropriate to the underlying algorithms. NAs found in this column are replaced with a 0 as no response information for these cells can be established. This might be a result of the articles in the test set containing more responses on average. This involves the order of words their part in a given sentence and the sentence itself. Ensembling is only possible when all prediction approaches have been run. While the Header Text Length Responses and Author are present in almost the same way in the train file the PublicationDetails column contains the publicationdomain the publishing date and the author again. Seeing that one variable might be correlated with the target variable totalClapCount it only makes sense to take a deeper look at the number of claps itself. The number of accumulation steps was left at two as this mostly relates to the ability of parallel GPU based computing through Gradient accumulation. For the parameters used in the first SVR approach with the train and test split as above the mean absolute error can be seen below. Kaggle offers certain advantages here by offering 16GB of RAM and access to high end GPUs Graphical Processing Units by nVidia which can significantly speed up computation 4. Afterwards the column type is changed to an integer. With few articles having over 10000 claps all having received more are put into one bin. The uniqueSlug attribute is especially fitting for a duplicate removal as A slug is a human readable unique identifier used to identify a resource instead of a less human readable identifier like an id 5. To guarantee that the parameters used for the predictions are optimal a GridSearch Crossvalidation is implemented testing different combinations of parameters to find the optimum ones. The plot suggests a positive correlation between the two however it is weak. Importing the Bert tokenizer. The following code cell produces a CSV file which contains the indices and the predictions based on the svr_out dataframe. A learning rate of 2e 5 was chosen after experimenting with other learning rates in the range of 3e 6 and 2e 3. Language detection on the test set revealed that only three texts were not in English. This saves valuable time. Overall Ridge regression on the self created test set did not provide satisfactory results. The primary advantage of BERT and its iterations is the pre training done on them. Using tf idf to determine word relevance in document queries. In the end the outputs of all models are combined in an ensemble model. The numerical features that can be directly replicated are the Length and Responses. The optimizer_grouped_parameters define the optimizer parameters and set the decay thereby defining how much the learning rate gets adjusted per iteration. The following implementation of a Tf Idf based SVR makes use of a train test split first and calculates predictions for a test set first to check the model s validity. Retrieved August 30 2020 from http ai. Launched in 2012 more than 60 million users read articles on Medium in one 2017 1. The dataframe containing the true clap count and the predictions. Can be omitted for a final runthrough more than doubling the runtime. Analysis of TotalClapCount As described above the totalClapCount variable has its mean at around 163. To give the predicted values into a visually more appealing look they are put into a dataframe and retransformed from class bins into whole numer Claps. Padding of sequences if they are shorter than max_seq_length 512 4. This code cell allows for the outputs of each predictive approach as saved in the subfolder and computed during earlier runs to be imported in order to faster create the ensemble model again. The Support Vector regression can also be performed with standardized values as performed below. The text preparation steps in BERT are as follows 1. Visual inspection of a few heads and URLs suggests some authors to post a multitude of articles. In Fundamentals of Artificial Intelligence pp. Simple guide to text classification Nlp using svm and naive bayes with python. com tutorial machine learning images support vector machine algorithm. A Tf Idf vectorization might therefore replicate the recommendation at the base of a higher amount of claps for an article. The BERT model used here consists of twelve encoder layers 768 hidden units and 12 attention heads. Add positional encoding Indication of the order of words in the sequence for the transformer blocks3. The maximum number of rounds of 200 is set in accordance with personal experience and the fact that throughout all runs the validation score always stopped improving before reaching round 200. valid_preds contains the predictions for the validation dataset. zero_grad Louis Stuff Louis Stuff lgb_x_train_regr lgb. This suggests that many articles in the dataset are on scientfic and computer science related topics with high informational content. BERT was pretrained on an immensely big dataset removing the need to train a machine learning model for NLP from nothing and starting off with a model already full of language knowledge. It is an example of social journalism giving professional journalists and inexperienced private parties the chance to publish on the same platform 2. This way the clean_text column does not get modified by the following preprocessing steps allowing for repeated execution. It can be mathematically posited as follows CrossEntropyLoss https drive. com and in form quite similar to Likes on platforms such as Facebook and Instagram. A tutorial on support vector regression. The test set does not contain as much information as the train set and omisses many of the columns present in the train set. This makes handling the data more difficult as computational power and storage needs to be capable of handling all data at once. The Responses column is modified in such a way that only the numeric response values are left with no textual string content in the cells left. Tf Idf values are especially fitting to the task at hand of numerical prediction as the values can be directly fed to a Support Vector Machine. com uc export download id 1fx2ZEoLMCvRhORAWiXEJ3z1MHZpjXwiL One specialty of BERT is its use of a CLS and SEP token. This suggests a highly imbalanced distribution which is plotted below. This is further limited by the fact that the full sequence length does not fit into the VRAM provided by the Nvidia K80 or P100 GPUs on Kaggle. Representation and meaning3. It could be assumed that longer articles attract more claps. Unfortunately this command can only be run after succesful runs unsuccessful runs or errors demand a notebook restart. The train_dataset contains a lot more useful variables which could help in obtaining better prediction results. Train test split based on totalClapCount with 20 of the train_small sample reserved for the test set. While the compressed file uploaded to Kaggle was less than 300MB in size uncompressed it turns into a multiple gigabyte big comma separated value file. The Encoding in BERT is different from the one used by many other algorithms as it makes use of Byte Pair Encoding. The SEP token marks the end of a sequence while the CLS classification token tries to capture a tokenized sequence s meaning. The implementation used here is based on the pyTorch library an open source machine learning library capable of almost all state of the art NLP algorithms 17. With the predictions being vastly different it makes sense to consider an ensemble of all predictions. com 2014 03 29 the new rules of social journalism a proposal 3 Alammar J. Finally the text_cleaner function removes The same text cleaning is applied to the test set. squeeze and numpy values created which are saved in a valid_preds array. com 22 Microsoft lightgbm. This is then compared to the correct class as given in y_test which contains the actual values. The Length variable contained in the test set might be worth a closer look. Running BERT requires the use of a GPU accelerator and the VRAM to be completely free at the beginning. png Data CleaningThe data cleaning performed here is focused on overall quality improvement of the dataset. To circumvent this the author name was extracted from the PublicationDetails column. Accordingly it consists of two parts which both get combined for the final value 1. Of particular value here is the totalClapCount column as this is the target variable which will be predicted in the test set. One must thus ask themselves what a model should be more focused on capturing then A realistic number of claps for most of the articles or finding just those articles that might go viral. com articles based on their textual content and other information given. BERT s vocabulary consists of approximately 30000 subwords allowing it to represent a vast majority of words with a fixed vocabulary saving much valuable RAM by using a fixed size. The clapcat column is defined as the target column intended for training later. Average loss and accuracy are set to 0 for each epoch so that following each run through the dataset the loss can be compared hoping for an improvement. Two of these were in a language not using the Latin Alphabet namely Chinese and Japanese. In the beginning a Dataloader object is created based on the train set with batch size 32 as above and no shuffling performed. The totalClapCount is once again log1p transformed to make the target data look normally distributed while taking into account the potential of 0 claps for an article which log alone would not do. Retrieved August 15 2020 from http jalammar. They will be taken care of in further calculations. It extracts the date from any text and save it as a list element. float32 label Y_train_regr lgb_x_valid_regr lgb. io whats a slug f7e74b6c23e0 6 Welcome to polyglot s documentation Polyglot 16. The batch size was chosen as 32 with batch sizes mostly limited by available VRAM. Light Gradient Boosting MachineLight GBM short for Light Gradient Boosting Machine is a tree based gradient boosting implementation. For some reason the Ridge model predicts one single absurdly high value for one of the articles. The Support Vector Regression is performed below with different versions and Kernels being commented out but left there for interesting comparisons. The text column is replaced by the clean_text column. The last step is the context dependent understanding seeing how words and sentences carry information and meaning in a given domain thus understanding their meaning at a higher level. LGB Test set predictionsPredicting on the test set with the LGBM the average number of claps predicted seems quite high with more than 1720 claps predicted. In this case the metric used is the mean absolute error. An appropriate number of of training steps per epoch is dependent on the size of the train set batch size and accumulation steps and should be of a size that the whole training set can be iterated through during training. As Tf Idf is case sensitive this is necessary. This pretraining is done on a large amount of text in a semi supervised way with the help of enormous amounts of computational power leaving the user with a pre trained model that only requires some finetuning for the specific task at hand. From there it is visible that the model still has difficulties picking up on articles with more than just a few claps. The first one tries to extract information from each word in a text by using its frequency of occurrence thus focusing on the first step of the process while the pre trained embedding based NLP approach tries to extract sentence and context dependent meaning from a text thus incorporating the second and third steps. Accuracy function giving back the accuracy for training in a multiclass setting. By default it detects Nouns only. com uc export download id 1vCWy7MXl_dQmIXI52CNKZa9QFvz4tM 2 The Self attention mechanism allows the Transformer to take contextual clues into account when assigning encodings for the word it relating it to a robot at the beginning of the sentence 16. In the Medium articles given the content ranges widely in topics however it is likely that certain keywords attract special attention and thereby Claps as Medium. The dataset contains NAs in multiple columns however not all of them. Numeric features especially can carry strong correlations with the target variable. The following code cell executes multiple steps necessary for Tf Idf word vectorization. The non text features are analysed using a Ridge Regression and LGB Machine. Bibliography Bibliography IntroductionThis notebook here is concerned with predicting claps for a selection of Medium. join dirname filename Any results you write to the current directory are saved as output. In the same way many articles with a low number of responses still receive thousands of Claps. 2019 The Illustrated BERT ELMo and co. Natural language processing and predictions Natural language processing and predictions 1. Changes in parameters especially in the BERT implementation can break the notebook and require a restart. The field of research in computer assisted natural language processing has been experiencing immense changes over the last years with big steps forward still being made on a regular basis. In other words we do not care about errors as long as they are less than \u03b5 but will not accept any deviation larger than this 9. These values are therefore obviously false and need to be corrected. In Proceedings of the first instructional conference on machine learning Vol. Afterwards the gradients get added up to calculate the model variable updates 18. The focus here lies on the text itself as it contains much more information than the title only. In the use case here it presents a basic first baseline for the clap predictions made by using only non text variables. Thus we can use the Slug as a legible substitute for the URL which is used as a column identifier in the dropping of duplicates below. Claps are being predicted in the million and billion Claps range which is impossible as the most clapped article as of August 2020 has no more than 281000 claps 21. get lang english Installing Nvidia Apex Converting the lines to BERT format Thanks to https www. It also outperforms a GPT 2 classification model. Considering these articles to be exceptional in their claps their content usually is not particularly different from other articles that might receive less than 5 of the claps. The text gets tokenized each sentence gets broken up into separate words or more specifically strings get divided into substrings 10. Bibliography 1 Bort J. Retrieved August 30 2020 from https nvidia. Seeing just how overly high some of the values in the Ridge predictions are it makes sense to assign them less weight in the combination of all predictions. A combination of both would be perfect but is as of now still elusive. Conclusion Conclusion 8. Language analysisWith a dataset this varied in content it is of question which languages are present in the texts. SVMs can be used in conjunction with polynomial radial and other Kernel functions in order to perform non linear classification. Byte pair encoding breaks down words into segments and uses the subwords in the embedding. com inside the meltdown of evan williams startup medium 2017 2 2 The new rules of social journalism A proposal. Test set predictionsFollowing are the prediction calculations for the test set provided as part of this task. The multiclassification has a built in error as the bins do not always exactly match the totalClapCount allowing for approximation with the loss as part of the algorithmic approach. ADAMS Assignment SS20 Prediction of ClapsCredit Medium. Combined with a 50 value of 5 this shows how unequal the distribution of claps is throughout the dataset with a high number of articles having less than 5 claps while some contain up to 291000 claps accounting for the significant standard deviation. html 13 Albert A lite bert for self supervised learning of language representations. This is adequately depicted in the graphic above taken from Javatpoint. To start off the text in both train and test set will be cleaned in a way that makes it more understandable legible and better suited to further processing steps. The original train dataset is sampled to create a smaller more computable sample. The test set contains 514 links to Medium articles for each of which a prediction of claps is to be made. The mean_absolute_error and mean_squared_error are calculated below with the mean_absolute error being acceptable although not exceptionally high. A first preliminary analysis shows the train dataset to contain over 270000 lines of information each concerning a Medium article. Nonetheless the BERT model in particular proves just how far pre trained embeddings have come in their understanding of human language with the use case presented here being just one of many in which they can be taken and used. Plotted the totalClapCount shows that most articles receive 0 claps. y_pred is a tensor vector containing predictions for each batch as determined by the model receiving the x_batch tensor values with an attention_mask binary tensor indicating which tensors to focus on. Following is an examination of the data given after which preprocessing steps such as feature creation and text cleaning are performed. Improvements will likely come from longer training on more GPUs providing more VRAM to load more data at once. Personal experience has shown GPT 2 to be a very useable zero shot classifier its successor GPT 3 is likely to improve upon this immensely however its size prohibits any realistic handling on single GPU machines. The BERT model chosen was BertForSequenceClassification as this Classification head is the one best suited for Multiclass classification tasks. Description of ImportsThe following imports are necessary for many computations in this notebook and are therefore at the beginning and not called at later points in the notebook. Ridge Test set predictionsThe Ridge predictions for the test set show the same outliers as present in the predictions for the validation set. Data cleaning Data cleaning 4. This is partially caused by the way a regression works and partially by the content used for predicting this phenomenon. Defining the device as a CUDA capable pytorch device. io illustrated gpt2 16 Pytorch. In case it still does not suffice additional sequences will be used for a text. Like other tree based algorithms it makes use of a leaf wise tree growth however it uses histogram based algorithms instead of pre sort based algorithms such as the one used by the XGBoosting framework. While BERT does not necessarily need cleaned text it makes sense to use the cleaned text from above for calculation as it will likely at least help in speeding up the process. The test file also contains duplicates however they cannot be dropped since predictions need to be made for all of them. LightGBM as a Gradient Boosting implementation iterates so far over the training until validation scores as defined by the metric specified do not improve anymore. The boxplot in particular shows how a number of articles receive an abnormally high amount of claps. com microsoft LightGBM blob master examples README. In the end the Lemmatized words are added back into the text_final column which is then used for further analysis. NVIDIA CUDA software and GPU parallel computing architecture. Retrieved August 30 2020 from https github. The Kernels used are linear and radial basis function with the latter being a common choice for the hyperplane decision boundary. After duplicate removal the train file contains 72337 lines. This problem would not occur could direct hardware access be provided but the layer of abstraction provided through the Kaggle virtual environment prevent directs hardware access to the GPU and therefore the empyting of the Cache. This has multiple advantages It drastically reduces time complexity and reduces memory usage. First a Dataloader object has to be created from the validation dataset. BERT handles most of the preprocessing itself taking its pre trained knowledge and applying it to the new texts it shall handle. io illustrated bert 18 Haleva R. Personal experience has shown a Tf Idf based SVR to be close to a fully trained BERT in a binary and multi class text classification setting while keeping computational cost at a fraction of the deep neural net. Improvements on BERT such as the recently released ALBERT still dominate Natural Language Processing benchmarks proving the supreme ability of BERT in handling understanding and solving human level text tasks 14. The following code cell extracts the publication platform from the PublicationDetails. The Author column in the test dataset contains the authors names only in the first 39 lines after which the author is gradually replaced by the date in this column. Good results on the test set were reached with a Linear Kernel and a C value of 50 which ensures higher weighting of the outliers and enables greater deviation from the mean of the regression. It has not been tuned and was chosen mostly for its efficiency and ease of use. float32 num_iteration lgb_opt. This tag_map is then handed to the Lemmatizer function which then lemmatizes each word in the corpus and appends it to a list of lemmatized words. This limits us to an analysis of the categorical and numerical features that can be replicated in both the train and the test set. Without actual knowledge it can be guessed that Length is a numerical representation of the number of characters present in the Text. The following code cell converts the text into token an ID array with the NAs replaced with a dummy value to avoid errors in tokenization. Can be weighted or adjusted for special usecases2. Duplicate removalIn datasets of this size especially ones containing data scraped from websites the question of potential duplicates must be posed early as the removal of duplicates can significantly alter the size of the dataset and accelerate further analysis. To get a further feel for the number of claps the log1p transformed Claps are visualized here. Retrieved August 30 2020 from https topmediumstories. MODEL CREATION To create and define the model used for training many parameters and steps have to be defined starting with the learning rate. The ensemble created from all predictions here tries to find the middle ground for the problem established above its overall MAE and MSE might not be award winning but it might likely still pick up on much clapped articles. com bedigunjit simple guide to text classification nlp using svm and naive bayes with python 421db3a72d34 12 Sklearn. The describe function above suggests that the recommends column might be correlated with the number of claps. Tf Idf Vectorizer with Support Vector Regression Tf Idf Vectorizer with Support Vector Regression 2. This is done here through the use of the dateparser package. What is gradient accumulation in deep learning Medium. The size of these models is often a result of them being pre trained on huge datasets to give them an innate understanding of text. Retrieved August 30 2020 from https www. com followed by hackernoon. The responses are an obvious feature as they directly measure public interest in the article at hand just differently. Data exploration Data importWhen importing the data into the Kaggle Notebook it becomes immediately clear that the train dataset is quite large. scale_loss is specifically called to deal with the loss as part of Automated Mixed Precision execution on the GPU 19. Statistics and computing 14 3 199 222. Lightgbm A highly efficient gradient boosting decision tree. The describe and dtypes functions show the extent of information contained in the train dataset. Twenty bins have been chosen in order to gain more separation in categories as the number of claps increases. The loss in the Multiclassification case is a CrossentropyLoss as it combines a negative log likelihood function with a logarithmic softmax function to determine the optimal item out of the 20 potential classes for each tensor item. A dataframe is created for the prediction with a truth column containing the actual number of claps and the pred column containing the predicted number of claps. Further data exploration in this direction would necessitate either the prediction of variables or the scraping of further information from the Internet. Byte pair encoding Credit Jay Alammar Bytepairencoding https drive. As a first and rough measure their value could just be set to 100000 with further corrections being made through the combination of the Ridge predictions with the predictions from the other approaches in this notebook. Tf Idf Vectorizer with Support Vector RegressionThe Tf Idf Vectorizer is the short form of a term frequency inverse document frequency Vectorizer. Predictions have to be fed to a sigmoid function to determine positive comparable value from which the maximum is drawn by a softmax which itself returns the number in the array this maximum occurs at. The code cell below highlights this with the actual article behind this having over 50000 Claps and the predictions being in the range of billiards. Polyglot gives a prediction for each language it knows and uses the highest prediction as the most likely language. Personal experience has shown that BERT and GPT 2 do better with classes recategorized on a scale from 1 to x with x being the maximum number of classes. An important step in the BERT process is the creation of a TensorDataset. This removes outliers lowering the Mean absolute error but does not help in detecting articles with a very high number of claps anymore. Context dependent environmentThe first part relates to the understanding of words themselves extracting its structure and meaning as well as a breakdown of its informational content. The Nvidia apex repository needs to be present in order for it to be installed for this session runtime. Both feats are achieved through the use of bins 23 Another advantage is its ability to handle categorical data by one hot encoding it without the need for further preprocessing. Its mean is approximately 163 with a standard deviation of 1813. Its prowess can be directly compared to that of the Tf Idf SVR which although not pre trained and much smaller can often still perform competitively in language level tasks. The Tf Idf Vectorizer takes the cleaned and lemmatized texts and calculates the values for each of them then transforms the Train and Test set into vectorized form. 2018 The Illustrated BERT ELMo and co. Running the Tf Idf with a sample size significantly bigger than 5000 such as 30000 led to crashes in the Kaggle environment due to RAM constraints. This takes multiple hours. Predictions are made based on the GridSearch determined values. Overview of Embeddings and models Credit Adrien Sieg embedding https drive. If it doesn t it adds the sentence to the text that is now cleaned from duplicates. Afterwards predictions are made on the provided test set. It highlights clearly how different algorithms put weights on different aspects of variables used for prediction and how pre trained embeddings can be used as a multi classifier in a regression context. com uc export download id 1bOHPwkRoOLO7wk8sDKbmE 2bEcMeqxhT Functionally BERT makes use of the Transformer architecture which itself makes use of the concept of self attention. This can be plotted in a scatter plot as seen below. To do so a first look at duplicates in the row uniqueSlug reveals a lot of duplicates. Running the same language detection on the train file led to a problem The input file contains Unicode characters the Polyglot function cannot handle as can be seen below. The Lemmatizer only focuses on alphabetic english words in accordance with the language analysis above. Overall the English language texts still make up over 93 of the texts. The BERT model seems to consistently predict the lowest values while the non text based ones predict very high values. The version of BERT used here is case insensitive as the text content is scraped HTML and likely not formatted correctly in the beginning. The following code cell produces two separate plots showing the number of articles released by each publication domain. Term frequency In most cases the number of times a term appears in a given text. Retransformation of the log1p transformed totalClapCount. All text gets transformed to lower case. The GPU is specifically targeted in the loop through the. The csv containing the BERT predictionsPotential improvements might come from the use of ROBERTA or ALBERT. The task at hand here is more suited to a regression analysis however BERT s last layer cannot be easily modified for a regression function. The y_pred_list object and its calculations exist as a visualization of the predictive process going on in the loop. Most common in the train dataset is a missing publisher while the most common publication domain in the test set is Netflix. A sample of the whole dataset is used here to ensure that the other predictions can also be run without having to restart the notebook thereby loosing the output of the SVR. Lemmatization involves the reduction of a word to its base form while taking into account contextual meaning thus reducing errors prone to happen when using a Stemming function only The tag_map is necessary to provide the Wordnetlemmatizer with tags indicating whether a word is a noun verb adjective or adverb. This process of self attention is visualized succinctly by Jay Alammar in this graphic 15 attention https drive. com 2019 12 albert lite bert for self supervised. A higher batch size would have allowed for better approximation but sizes starting at 32 can be considered for adequate results. Some imports here are already NLTK and BERT related although more imports for both are done later as they require package installs that take up time therefore only being done at the point of execution. One way to do so is by using the polyglot package which guesses the languages for each text inserted into it by comparing it to its own vocabulary. Length as used here is the strings in the text field divided into substrings by whitespace and then counted. Introduction Introduction 2. Preparatory analysis is not limited to the train dataset but also extends to the test dataset. Ridge regression can therefore be considered an extremely rudimentary baseline not worthy of further optimization in this use case. Inverse Document Frequency Logarithmically scaled inverse fraction of texts with the term in themTf Idf calculates numerical values for each word in a given text based on the number of times it appears in a text and the relative occurence of the word in all texts given to the Tf Idf algorithm 8. Predicting which articles might go viral based on language level content poses to be extraordinarily difficult with many big companies aiming to achieve just that. The number of maximum leaves was set at 255 after which the tree will grow laterally. Steps involved are nearly identical with the production of a dataframe containing index and predicted Claps at the end. This ensemble can be created in many different ways. This is then used in the prediction loop where the model predicts classes for each batch object. The indices are dropped then to make further merges easier. While it is possible to use BERT in a regression context by using the sentence embeddings of all sentences generated by BERT as inputs for a logistic regression model in the case of our already performed logistic regression it is more intuitive to convert the dataset at hand into a multiclass classification task suitable dataset and predicting classes which can then be used in an ensemble with the predictions generated above. Retrieved August 30 2020 from https polyglot. Thus claps are a whole number integer representative of how many people like a certain Medium post. However the max and min number of predictions seem to quite in line with a realistic number of claps expectable. A high number of responses is very likely to be correlated with a high number of claps as people tend to leave a comment response only under articles they are already interested in. com pushkarmandot https medium com pushkarmandot what is lightgbm how to implement it how to fine tune the parameters 60347819b7fc 24 Ke G. A short plot of the log1p transformed totalClapCount showing the distribution afterwards. This makes these languages difficult to use in NLP tasks as existing vocabulary cannot be easily expanded for these languages. Its median however is at 5. Other authors do not occur more often than 4 times. Byte pair encoding represents a well working and efficient compromise between word level and character level encodings 17. Ridge Regression Ridge Regression 2. This notebook can be run chronologically in order with the available resources given by Kaggle. These two tokens are not present in other pre trained models such as GPT 2. Bert Pre training of deep bidirectional transformers for language understanding. How NLP Cracked Transfer Learning. The optimizer performs an optimization step every two accumulation steps and can take a step backwards if necessary. This problem might be helped in part by the removal of stopwords and the performed cleaning of the text above which reduces the text length quite a bit. This has been aided by big advances in computing power helping with the possibility of ever greater models being created. The most common publisher in the train set is towardsdatascience. Output files from each predictive apporach are contained in the adams2020 folder for easier replicability. png Table of Contents 1. Look up the embedding of each input word in the embedding matrix which is made up of the vocabulary times the embedding size which depends on the exact version of BERT used 2. Light Gradient Boosting Machine Light Gradient Boosting Machine 6. To do so multiple text preprocessing functions are used 1. Here too the Length is calculated by splitting the strings into substrings anc counting them. Language detection on the train set using the cleaned text. https towardsdatascience. strip function removes whitespaces and linebreaks to only have the sentence left. ConclusionThe analyses performed above showed multiple approaches to the same problem The prediction of claps for a Medium article. Ensembling All predictions performed above need to be combined in some way to form a final prediction. Following is an overview of the totalClapCount column. Medium itself is a digital publishing platform allowing people to share their self created articles and other written works with people all around the world. Ensembling Ensembling 7. This furthers the time consuming aspect of BERT. ", "id": "louispk/adams-ss20-final", "size": "52149", "language": "python", "html_url": "https://www.kaggle.com/code/louispk/adams-ss20-final", "git_url": "https://www.kaggle.com/code/louispk/adams-ss20-final", "script": "sklearn.metrics matplotlib.gridspec duplicate_remover BertForSequenceClassification Ridge tqdm_notebook division logger naive_bayes torch.nn collections dateparser.search sklearn.model_selection convert_lines Image wordcloud probplot digits __future__ InteractiveShell sklearn.svm train_test_split text_cleaner WordNetLemmatizer SVR search_dates TfidfVectorizer mean_squared_error seaborn numpy sklearn.pipeline scipy.stats metrics CountVectorizer LabelEncoder stats WordCloud pandas word_tokenize norm nltk.stem nltk.corpus mean_absolute_error GridSearchCV sklearn.linear_model ImageColorGenerator string preprocessing sentence_extract stopwords pytorch_pretrained_bert Detector BertConfig amp IPython.core.interactiveshell defaultdict apex sklearn.feature_extraction.text make_pipeline PIL print_function accuracy_score pos_tag scipy roc_auc_score absolute_import BertTokenizer datetime nltk lightgbm wordnet BertAdam multi_acc plot PorterStemmer polyglot.detect date pyLDAvis wordnet as wn sklearn = {v matplotlib.pyplot convert_tf_checkpoint_to_pytorch StandardScaler sent_tokenize tqdm polyglot.detect.base torch.nn.functional svm nltk.tokenize model_selection SVC sklearn.preprocessing LinearSVR STOPWORDS ", "entities": "(('that', 'hand'), 'do') (('values', 'Support Vector directly Machine'), 'be') (('which', 'coefficient'), 'be') (('sizes', 'adequate results'), 'allow') (('so first look', 'duplicates'), 'reveal') (('they', 'numer whole Claps'), 'put') (('early removal', 'further analysis'), 'one') (('code following cell', 'tokenization'), 'convert') (('It', 'memory usage'), 'have') (('Lemmatizer', 'language analysis'), 'focus') (('extraction', 'date'), 'Details') (('using', 'input only two variables'), 'be') (('that', 'now duplicates'), 'add') (('which', 'then further analysis'), 'add') (('what', '6 Welcome documentation'), 'io') (('Kernels', 'hyperplane decision common boundary'), 'use') (('BERT tensorflow model', 'BERT pyTorch compatible model'), 'convert') (('BERT', 'maximum classes'), 'show') (('authors', 'articles'), 'suggest') (('Length', 'present Text'), 'guess') (('it', 'at least process'), 'clean') (('maximum', 'array'), 'have') (('too it', 'minima'), 'be') (('sentence_extract First function', 'sentences'), 'use') (('they', 'max_seq_length'), 'padding') (('learning rate', '3e'), 'choose') (('format', 'pyTorch environment'), 'be') (('validation scores', 'metric specified'), 'LightGBM') (('that', 'execution'), 'be') (('png Data CleaningThe data cleaning', 'dataset'), 'focus') (('only sentence', 'whitespaces'), 'remove') (('many articles', 'high informational content'), 'suggest') (('It', 'CrossEntropyLoss https mathematically drive'), 'posit') (('it', 'numeric variables'), 'design') (('which', 'self attention'), 'com') (('Ridge regression', 'use case'), 'consider') (('Improvements', 'level text human tasks'), 'dominate') (('here other predictions', 'SVR'), 'use') (('Kernels', 'there interesting comparisons'), 'perform') (('you', 'output'), 'join') (('correlation', 'text LANGUAGES'), 'sent_tokenize') (('Again features', 'test available set'), 'create') (('This', 'predictive models'), 'help') (('BERT', 'language already full knowledge'), 'pretraine') (('what', 'ec034122cfa 19 Apex'), 'com') (('prevent', 'therefore Cache'), 'occur') (('which', 'regression'), 'reach') (('This', 'BERT'), 'further') (('lang english Installing Nvidia Apex', 'https Thanks www'), 'get') (('Test train', 'train'), 'perform') (('criterion', 'CrossEntropyLoss'), 'need') (('it', 'session'), 'need') (('GridSearch Crossvalidation', 'optimum ones'), 'guarantee') (('tree', 'which'), 'set') (('loss', 'improvement'), 'set') (('it', 'Tf Idf algorithm'), 'scale') (('test where text', 'notebook'), 'easy') (('Dataloader object', 'progress'), 'iterate') (('predictions', 'final prediction'), 'ensemble') (('text non features', 'Ridge Regression'), 'analyse') (('where model', 'batch object'), 'use') (('Thus claps', 'certain Medium post'), 'be') (('y_pred_list object', 'loop'), 'exist') (('penalty', 'actual value'), 'specify') (('which', 'dataframe'), 'produce') (('BERT model', 'encoder here twelve layers'), 'consist') (('Bibliography Bibliography IntroductionThis notebook', 'Medium'), 'concern') (('preparatory steps', 'regression adequate analysis'), 'be') (('it', 'Byte Pair Encoding'), 'be') (('average number', 'quite more than 1720 claps'), 'set') (('which', 'highly imbalanced distribution'), 'suggest') (('codeBlockCount imageCount However many socialRecommendsCount', 'test set'), 'be') (('Tf Idf Vectorizer', 'vectorized form'), 'take') (('two tokens', 'such GPT'), 'be') (('where understanding', 'language level tasks'), 'reach') (('which', 'actual values'), 'compare') (('Lemmatization', 'test'), 'perform') (('important step', 'TensorDataset'), 'be') (('csv', 'ROBERTA'), 'come') (('which', 'lemmatized words'), 'hand') (('training whole set', 'training'), 'be') (('prediction', 'claps'), 'contain') (('read_csv Input data files', 'the'), 'be') (('pair encoding', 'embedding'), 'byte') (('text features featuresThe non present', 'further investigation'), 'Analysis') (('they', 'which'), 'prove') (('code following cell', 'PublicationDetails'), 'extract') (('extraordinarily many big companies', 'just that'), 'pose') (('word', 'tags'), 'involve') (('tensor which', 'legible values'), 'be') (('response information', 'cells'), 'replace') (('NLP trained based approach', 'thus second steps'), 'try') (('less than 0 4', 'NLP based analysis'), 'overlook') (('Support vector regression', 'adequate accuracy'), 'allow') (('size', 'text'), 'be') (('which', 'BERT'), 'look') (('training whole data', 'training'), 'com') (('Regression Analysis', 'text non features'), 'feature') (('only three texts', 'English'), 'reveal') (('PublicationDetails column', 'publishing date'), 'contain') (('describe', 'train dataset'), 'show') (('which', 'predictions'), 'be') (('dataset', 'them'), 'contain') (('code following cell', 'Tf Idf word necessary vectorization'), 'execute') (('above recommends column', 'claps'), 'suggest') (('non text based ones', 'very high values'), 'seem') (('they', 'only articles'), 'be') (('Output files', 'easier replicability'), 'contain') (('process', 'attention https graphic 15 drive'), 'visualize') (('vastly it', 'predictions'), 'make') (('notebook', 'same predictive task'), 'be') (('predictions how value', 'one'), 'reveal') (('train immediately dataset', 'Kaggle Notebook'), 'import') (('it', 'text'), 'use') (('This', 'scatter plot'), 'plot') (('which', 'likes'), 'present') (('text same cleaning', 'test set'), 'remove') (('that', 'GPLv3 license'), 'describe') (('support vector', 'two separate spaces'), 'include') (('which', 'language level much often still competitively tasks'), 'compare') (('themselves', 'Medium'), 'com') (('This', 'more responses'), 'be') (('many articles', 'Claps'), 'receive') (('Tf Idf Vectorizer', 'inverse document frequency short term frequency Vectorizer'), 'be') (('Length', 'whitespace'), 'be') (('LightGBM', 'parameters'), 'be') (('author', 'column'), 'contain') (('language Overall English texts', 'texts'), 'make') (('which', 'own vocabulary'), 'be') (('only succesful runs unsuccessful runs', 'notebook restart'), 'run') (('actual article', 'billiards'), 'highlight') (('some', 'significant standard deviation'), 'show') (('BERT model', 'here pre trained models'), 'be') (('ever greater models', 'possibility'), 'aid') (('model', 'just a few claps'), 'be') (('This', 'part given sentence'), 'involve') (('Running BERT', 'completely beginning'), 'require') (('that', 'just articles'), 'ask') (('configured number', 'model variables'), 'run') (('Abhishek who', 'GPU acceleration'), 'classification') (('This', 'dateparser package'), 'do') (('model variable', '18'), 'add') (('Furthermore he', 'step multiple three main parts'), 'posit') (('it', 'title'), 'lie') (('combination', 'now still elusive'), 'be') (('imports', 'replicability'), 'enable') (('that', 'claps'), 'be') (('it', 'sentence'), 'com') (('which', 'article'), 'transform') (('GPU', 'the'), 'target') (('languages', 'texts'), 'analysisWith') (('however it', 'XGBoosting framework'), 'make') (('transformed Claps', 'claps'), 'get') (('that', 'claps'), 'combine') (('LGBM', 'Ridge regression'), 'achieve') (('advantage', 'extensive preprocessing'), 'be') (('this', 'Gradient based accumulation'), 'leave') (('text preparation steps', '1'), 'be') (('Gradient Boosting MachineLight Light GBM', 'Light Gradient Boosting Machine'), 'short') (('Description', 'notebook'), 'be') (('which', 'prediction better results'), 'contain') (('long instead float Bug', 'chinhuic optimizer'), 'type') (('ConclusionThe analyses', 'Medium article'), 'perform') (('most articles', '0 claps'), 'show') (('languages', 'easily languages'), 'make') (('however last layer', 'regression easily function'), 'be') (('mean absolute error', 'Ridge regression'), 'seem') (('so predictions', 'CPU further dimensions'), 'detach') (('it', 'fixed size'), 'consist') (('which', 'significantly computation'), 'offer') (('code following cell', 'publication domain'), 'produce') (('ensemble', 'many different ways'), 'create') (('which', 'here Lemmatize'), 'provide') (('it', 'available GPU'), 'use') (('publishing digital people', 'all world'), 'be') (('it', 'tensor item'), 'be') (('outputs', 'faster ensemble model'), 'allow') (('CrossEntropyLoss', 'inputs'), 'take') (('at most deviation', 'same time'), 'in') (('as mean absolute error', 'train'), 'see') (('dataframe', 'claps'), 'create') (('It', 'average'), 'seem') (('it', 'most likely language'), 'give') (('which', 'duplicates'), 'use') (('gradient accumulation', 'deep learning Medium'), 'be') (('same sentence', 'already cache'), 'take') (('column Afterwards type', 'integer'), 'change') (('which', 'final value'), 'consist') (('Steps', 'end'), 'be') (('it', 'text classification'), 'help') (('Improvements', 'more data'), 'come') (('Classification head', 'Multiclass classification best tasks'), 'be') (('Twenty bins', 'claps increases'), 'choose') (('following implementation', 'first validity'), 'make') (('metric', 'case'), 'be') (('However bumps', '50 claps'), 'be') (('Overall Ridge regression', 'satisfactory results'), 'provide') (('further corrections', 'notebook'), 'set') (('batch size', 'mostly available VRAM'), 'choose') (('most clapped article', '2020 no more than 281000 claps'), 'predict') (('why more bins', 'lower range'), 'be') (('1fx2ZEoLMCvRhORAWiXEJ3z1MHZpjXwiL One specialty', 'CLS token'), 'com') (('outputs', 'ensemble model'), 'combine') (('slug', 'instead less human readable i 5'), 'be') (('more they', 'more actively Medium'), 'be') (('scale_loss', 'GPU'), 'call') (('long Crossentropyloss', 'later execution'), 'expect') (('2020 other person', 'it'), 'adams') (('Changes', 'restart'), 'break') (('Transformer', 'one'), 'allow') (('Two', 'namely Chinese'), 'be') (('number', 'categories'), 'be') (('It', 'classification also GPT 2 model'), 'outperform') (('bin original names', 'reversal'), 'write') (('60 more than million users', 'one 2017 1'), 'read') (('single extreme outlier these', '0'), 'normalized') (('it', 'processing more legible further steps'), 'clean') (('Tf Idf vectorization', 'article'), 'replicate') (('as long they', 'larger 9'), 'care') (('words', 'too BERT'), 'be') (('here it', 'text only non variables'), 'present') (('valid_preds', 'validation dataset'), 'contain') (('train file', '72337 lines'), 'contain') (('it', 'test again set'), 'become') (('language processing following Natural analyses', 'process'), 'try') (('MODEL create', 'learning rate'), 'CREATION') (('Following', 'totalClapCount column'), 'be') (('It', 'use'), 'tune') (('pair encoding', 'word level'), 'byte') (('numpy which', 'valid_preds array'), 'squeeze') (('setThe predictions', 'test here provided set'), 'prediction') (('Personal experience', 'deep neural net'), 'show') (('calculation Different methods', 'test set'), 'try') (('enough standard', 'definite improvement'), 'achieve') (('train original dataset', 'smaller more computable sample'), 'sample') (('language level human text', 'necessary information'), 'aim') (('CrossEntropyLoss', 'NLLLoss'), 'combine') (('words', 'vocabulary size'), 'allow') (('CLS classification token', 'tokenized meaning'), 'mark') (('train test split', 'test back purposes'), 'base') (('optimizer', 'step'), 'perform') (('certain keywords', 'thereby Medium'), 'range') (('users', 'Medium'), 'be') (('predictionsThe Ridge predictions', 'validation set'), 'set') (('language processing prediction Natural approaches', 'Tf Idf Support Vector Regression multiclass classification BERT based approach'), 'make') (('themselves', 'as well informational content'), 'relate') (('Running', 'RAM constraints'), 'lead') (('This', 'between 1 second plot'), 'understand') (('that', 'train'), 'limit') (('It', 'python docker image https kaggle github'), 'version') (('totalClapCount', 'MAE'), 'calculate') (('Results', 'Spanish texts'), 'reveal') (('tensor types', 'type torch'), 'have') (('Test', 'task'), 'be') (('it', 'predictions'), 'make') (('data exploration part', 'such seaborn'), 'be') (('optimum parameters', 'execution'), 'print') (('trained model', 'such GLUE'), 'nbsp') (('that', 'regression successfully tasks'), 'be') (('notebook', 'Kaggle'), 'run') (('train', 'train present set'), 'contain') (('LGBM', 'text based predictions'), 'seem') (('language Natural most important part', 'predictive analysis'), 'processing') (('Length variable', 'test set'), 'be') (('how trained embeddings', 'regression context'), 'highlight') (('Dataloader First object', 'validation dataset'), 'have') (('y_pred', 'tensors'), 'be') (('Afterwards predictions', 'test provided set'), 'make') (('mean wordCount', 'article'), 'suffice') (('Polyglot function', 'Unicode characters'), 'run') (('However max number', 'claps'), 'seem') (('it', 'likely still much clapped articles'), 'try') (('it', 'new texts'), 'handle') (('data Further exploration', 'Internet'), 'necessitate') (('each', 'Medium article'), 'show') (('number', 'given text'), 'frequency') (('Ridge model', 'articles'), 'predict') (('validation throughout score', 'always round 200'), 'set') (('Stopwords', 'NLTK stopwords'), 'remove') (('field', 'forward still regular basis'), 'assist') (('primary advantage', 'pre them'), 'be') (('VRAM CUDA Cache', 'additional calculation'), 'allow') (('prediction specific approach', 'underlying algorithms'), 'be') (('SVMs', 'linear non classification'), 'use') (('It', 'same platform'), 'be') (('Numeric', 'target variable'), 'feature') (('BERT based predictions', 'Tf Idf SVR predictions'), 'remain') (('here LGBM', 'good big datasets'), 'serve') (('CUDA yet part', 'packages'), 'take') (('it', 'value multiple gigabyte big comma separated file'), 'turn') (('obvious they', 'hand'), 'be') (('Further improvements', 'parameter tuning'), 'increase') (('Albert 13 lite bert', 'language representations'), 'html') (('dependent how words', 'higher level'), 'be') (('INSIDE MEDIUM Silicon Valley How idealistic founder', 'then reality'), 'S') (('train_df', 'too efficient handling'), 'be') (('Predictions', 'GridSearch determined values'), 'make') (('more computational power', 'data'), 'make') (('which', 'next step'), 'indicate') (('more specifically strings', 'substrings'), 'tokenize') (('clapcat column', 'training'), 'define') (('how BERT', 'Natural Language Processing algorithms'), 'show') (('mean', '1813'), 'be') (('clean_text column', 'repeated execution'), 'way') (('response only numeric values', 'cells'), 'modify') (('most common publisher', 'train set'), 'be') (('prediction first part', 'train'), 'setIn') (('totalClapCount it', 'claps'), 'see') (('author name', 'PublicationDetails column'), 'extract') (('predictions', 'them'), 'contain') (('which', 'Cuda above device'), 'to') (('sequence full length', 'P100 Kaggle'), 'limit') (('This', 'assumptions'), 'be') (('Retransformation', 'log1p'), 'transform') (('wordCount', 'totalClapCount'), 'plot') (('Again number', 'still fact'), 'be') (('object describe function', 'mean median'), 'with') (('representation', 'sentence level meaning'), 'involve') (('Chowdhary', 'language important natural understanding'), 'be') (('received', 'more one bin'), 'put') (('regression', 'phenomenon'), 'cause') (('Overview', 'models Credit Adrien https drive'), 'embed') (('text here case content', 'likely correctly beginning'), 'be') (('however model', 'outlier cases'), 'look') (('mean_absolute_error', 'mean_absolute below error'), 'calculate') (('bins', 'algorithmic approach'), 'have') (('publication missing most common domain', 'test set'), 'be') (('However runtimes', 'very even significant computational backing'), 'be') (('It', 'list element'), 'extract') (('implementation', 'art'), 'base') (('longer articles', 'more claps'), 'assume') (('32 as above shuffling', 'batch size'), 'create') (('com tutorial machine learning images', 'vector machine algorithm'), 'support') (('particular how number', 'claps'), 'show') (('23 advantage', 'further preprocessing'), 'achieve') (('This', 'above Javatpoint'), 'depict') (('Preparatory analysis', 'test also dataset'), 'be') (('performed cleaning', 'text length'), 'help') (('They', 'further calculations'), 'take') (('text column', 'clean_text column'), 'replace') (('however it', 'two'), 'suggest') (('target which', 'test set'), 'be') (('Crossentropyloss', 'tensor'), 'change') (('short plot', 'totalClapCount distribution'), 'transform') (('immensely however size', 'GPU single machines'), 'show') (('Retrieved August', 'https 30 2020 scikit'), 'learn') (('Support Vector regression', 'also standardized values'), 'perform') (('learning thereby how much rate', 'iteration'), 'define') (('examination', 'feature such creation'), 'perform') (('way comparisons', 'real value'), 'make') (('This', 'claps'), 'remove') (('Tf Idf', 'word vectorization other methods'), 'use') (('20', 'test set'), 'split') (('creation', 'regression task'), 'be') (('Here too Length', 'them'), 'calculate') (('how fine tune', 'parameters'), 'pushkarmandot') (('Netflix TE who', '175 times respectively'), 'lead') ", "extra": "['biopsy of the greater curvature', 'test']"}