{"name": "house price model draft ", "full_name": " h3 EDA h3 Preprocessing h4 Transform to normal distribution h4 np exp to tranform back to real price h4 Transform some features to normal distribution in df train only h4 Number of null in each col in df h4 Features above can be removed from training because almost all are missing h2 To do h3 1 categorical data numerical h3 2 null na value handling h3 3 feature selection h3 Data preprocessing h4 1 Convert the attributes that can be numeric straightly h4 pre dealling NA values for above features h4 Now deal with other NA h4 Fill null value for numerical features with its mode median mean which depends on distribution h4 df train h4 df test h4 df train and df test h4 Replace NA with median in numerical features h4 new features are added h4 All NA in numerical features are filled with their median h4 Now handle categorical features with NA h4 Fill nan with 0 h3 Now transforming categorical to numerical by frequency label encoding h4 Now all categorical data has transformed to numerical h3 XGBoost h4 provides a XGBRegressor and permutation importance that can find important feature h4 Import libraries for model and eval h3 Now train the model with the extracted features h4 Include the features with score 0 h4 XGB model h4 r2 score 0 9728663100221 if new cols added OverallQual and Cond multi by SalePrice try double regression failed h4 Other models h4 Random forest regressor h3 Apply the model to test csv h3 Random forest regressor h4 Load models back ", "stargazers_count": 0, "forks_count": 0, "description": "read_csv You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session 1460 rows with 80 attributes for categorical features only Change below for different encoding Using logged y get features 0 features by XGB df_test maximum depth of each tree try 2 to 10 effect of each tree try 0. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. null na value handling filled with its mean median AND OR add a indicating coloumn for that 3. 0 fraction of instances rows per tree try 0. exp to tranform back to real price Transform some features to normal distribution in df_train only Number of null in each col in df Features above can be removed from training because almost all are missingGet the columns with missing value To do 1. csv Random forest regressor Load models back This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. Convert the attributes that can be numeric straightly0 for No and 1 for YesConvert quality except NA pre dealling NA values for above features Now deal with other NA Fill null value for numerical features with its mode median mean which depends on distribution df_train df_test df_train and df_test Replace NA with median in numerical features new features are added All NA in numerical features are filled with their median Now handle categorical features with NA Fill nan with 0 Now transforming categorical to numerical by frequency label encoding Now all categorical data has transformed to numerical XGBoost provides a XGBRegressor and permutation_importance that can find important featurehttps mljar. com blog feature importance xgboost Import libraries for model and eval Now train the model with the extracted features Include the features with score 0 XGB model r2_score 0. 8862649501692593 Other models Random forest regressor Apply the model to test. 0 L1 regularization like LASSO try 0. 9728663100221 if new cols added OverallQual and Cond multi by SalePrice try double regression failedBy LabelEncoder 0. 0 set 1 for boosted random forests Run this if y is logged Run this if y is logged Run this if y is logged By notebook in OneDrive QualxCond multiplying grading QualxCond multiplying grading factorize QualxCond multiplying grading frequency threshold 0. feature selection Data preprocessing 1. categorical data numerical target encoding index each category or frequency encoding mean median or one hot too many features 2. 1 number of trees that is boosting rounds try 1000 to 8000 minimum number of houses in a leaf try 1 to 10 fraction of features columns per tree try 0. EDA Preprocessing Transform to normal distribution np. ", "id": "samwong127/house-price-model-draft", "size": "1873", "language": "python", "html_url": "https://www.kaggle.com/code/samwong127/house-price-model-draft", "git_url": "https://www.kaggle.com/code/samwong127/house-price-model-draft", "script": "sklearn.metrics checkINFNULL permutation_importance pyplot as plt sklearn.kernel_ridge transformCatToNum_LabelEncoding transformCatToNum_FrequencyEncoding r2_score mean_squared_error seaborn numpy replaceNA_with_0 pyplot gradingconverter getNumCatFeatures PredictionPipeline f_regression multiplier2 sklearn.ensemble sklearn sklearn.model_selection XGBRegressor mutual_info_regression pandas replaceNA_with_Median transformCatToNum_Factorizing getColsWithMissingValue sklearn.inspection sklearn.feature_selection sklearn.linear_model matplotlib multiplier transformCatToNum_OneHot xgboost train_test_split extractFeatures pre_replaceNA_with_0 preprocessing ", "entities": "(('It', 'kaggle python Docker image https github'), 'model') (('2 to 10 effect', '0'), 'write') (('OverallQual multi', 'regression failedBy double LabelEncoder'), 'try') (('that', 'featurehttps important mljar'), 'convert') (('Random forest 8862649501692593 Other regressor', 'model'), 'model') (('multiplying grading', 'QualxCond'), 'set') (('0 fraction', '0'), 'try') (('almost all', '1'), 'remove') (('that', '0'), 'try') (('target encoding categorical data numerical index', 'category'), 'feature') ", "extra": "['test']"}