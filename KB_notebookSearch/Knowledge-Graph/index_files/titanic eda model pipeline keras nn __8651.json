{"name": "titanic eda model pipeline keras nn ", "full_name": " h1 Welcome to my Titanic Kernel h2 This kernel will provide a analysis through the Titanic Disaster to understand the Survivors patterns h1 Table of Contents h1 1 Introduction h3 The data have 891 entries on train dataset and 418 on test dataset h2 Competition Description h3 Data Dictionary h3 Variable Notes h1 2 Librarys h1 3 First look at the data h1 4 Exploring the data h2 To try a new approach in the data I will start the data analysis by the Name column n Grouping some titles and ploting the results n Title grouped nIt s interesting Children s and ladys first huh n Now I will handle the Age variable that has a high number of NaN s using some columns to correctly input he missing Age s nThis might show us a better way to input the NAN s n nFor example an male in 2 class that is a Officer the median Age is 42 nAnd we will use that to complete the missing data n nNow let s categorize them nNow it look s better and clearly n Let s cross our Pclass with the Age cat nWe will aggregate than to get the mean of Fare by each category pair n n Very interesting We can see that babies has the highest mean value n Looking the Fare distribuition to survivors and not survivors n n nDescription of Fare variable n Min 0 n Median 14 45 n Mean 32 20 n Max 512 32 n Std 49 69 nI will create a categorical variable to treat the Fare expend nI will use the same technique used in Age but now I will use the quantiles to binning n n nTo complete this part I will now work on Names nIt s looking ok nNow lets start explore the data nWe can look that dies to mens are much higher than female nNow lets do some exploration in Pclass and Embarked to see if might have some information to build the model nLooking the graphs is clear that 3st class and Embarked at Southampton have a high probabilities to not survive nTo finish the analysis I let s look the Sibsp and Parch variables nInteresting With 1 or 2 siblings spouses have more chance to survived the disaster nWe can see a high standard deviation in the survival with 3 parents children person s nAlso that small families 1 2 have more chance to survival than single or big families nSo to Finish our exploration I will create a new column to with familiees size nOK its might be enough to start with the preprocess and builting the model n n n 5 Preprocessing nNow we might have information enough to think about the model structure n n 6 Modelling nTitanic survivors prediction na binary classification example nTwo class classification or binary classification may be the most widely applied kind of machine learning problem nAnatomy of a neural network n nAs you saw in the previous chapters training a neural network revolves around the following nobjects n Layers which are combined into a network or model n The input data and corresponding targets n The loss function which defines the feedback signal used for learning n The optimizer which determines how learning proceeds n n n n n Layers the building blocks of deep learning nfrom keras import layers nlayer layers Dense 32 input dim data dimension n n We can think of layers as the LEGO bricks of deep learning a metaphor that is nmade explicit by frameworks like Keras Building deep learning models in Keras is ndone by clipping together compatible layers to form useful data transformation pipelines n n nWhat are activation functions and why are they necessary nWithout an activation function like relu also called a non linearity the Dense layer would consist of two linear operations a dot product and an addition noutput dot W input b n nSo the layer could only learn linear transformations affine transformations of the ninput data the hypothesis space of the layer would be the set of all possible linear ntransformations of the input data into a 16 dimensional space n n nLoss functions and optimizers nkeys to configuring the learning process nOnce the network architecture is defined you still have to choose two more things n Loss function objective function The quantity that will be minimized during ntraining It represents a measure of success for the task at hand n Optimizer Determines how the network will be updated based on the loss function nIt implements a specific variant of stochastic gradient descent SGD nStacks of Dense layers with relu activations can solve a wide range of problems including sentiment classification and you ll likely use them frequently nFinally we need to choose a loss function and an optimizer nBecause you re facing a binary classification problem and the output of your network is a probability you end your network with a single unit layer with a sigmoid activation it s best to use the binary crossentropy loss n Predicting X test nEvaluating the model nNot bad result to a simple model Let s now verify the validation of our model to see and understand the learning curve n n 7 Validation nLet s look this keys values further nWhy this occurs and how to solve this problem in graph it s a overffiting nIt s my first Deep Learning implementation I am studying about this and I will continue editing this Kernel to improve the results nGive me your feedback how can I increase this model n Stay tuned and don t forget to votesup this kernel h1 Now I will handle the Age variable that has a high number of NaN s using some columns to correctly input he missing Age s nThis might show us a better way to input the NAN s n nFor example an male in 2 class that is a Officer the median Age is 42 nAnd we will use that to complete the missing data n nNow let s categorize them nNow it look s better and clearly n Let s cross our Pclass with the Age cat nWe will aggregate than to get the mean of Fare by each category pair n n Very interesting We can see that babies has the highest mean value n Looking the Fare distribuition to survivors and not survivors n n nDescription of Fare variable n Min 0 n Median 14 45 n Mean 32 20 n Max 512 32 n Std 49 69 nI will create a categorical variable to treat the Fare expend nI will use the same technique used in Age but now I will use the quantiles to binning n n nTo complete this part I will now work on Names nIt s looking ok nNow lets start explore the data nWe can look that dies to mens are much higher than female nNow lets do some exploration in Pclass and Embarked to see if might have some information to build the model nLooking the graphs is clear that 3st class and Embarked at Southampton have a high probabilities to not survive nTo finish the analysis I let s look the Sibsp and Parch variables nInteresting With 1 or 2 siblings spouses have more chance to survived the disaster nWe can see a high standard deviation in the survival with 3 parents children person s nAlso that small families 1 2 have more chance to survival than single or big families nSo to Finish our exploration I will create a new column to with familiees size nOK its might be enough to start with the preprocess and builting the model n n n 5 Preprocessing nNow we might have information enough to think about the model structure n n 6 Modelling nTitanic survivors prediction na binary classification example nTwo class classification or binary classification may be the most widely applied kind of machine learning problem nAnatomy of a neural network n nAs you saw in the previous chapters training a neural network revolves around the following nobjects n Layers which are combined into a network or model n The input data and corresponding targets n The loss function which defines the feedback signal used for learning n The optimizer which determines how learning proceeds n n n n n Layers the building blocks of deep learning nfrom keras import layers nlayer layers Dense 32 input dim data dimension n n We can think of layers as the LEGO bricks of deep learning a metaphor that is nmade explicit by frameworks like Keras Building deep learning models in Keras is ndone by clipping together compatible layers to form useful data transformation pipelines n n nWhat are activation functions and why are they necessary nWithout an activation function like relu also called a non linearity the Dense layer would consist of two linear operations a dot product and an addition noutput dot W input b n nSo the layer could only learn linear transformations affine transformations of the ninput data the hypothesis space of the layer would be the set of all possible linear ntransformations of the input data into a 16 dimensional space n n nLoss functions and optimizers nkeys to configuring the learning process nOnce the network architecture is defined you still have to choose two more things n Loss function objective function The quantity that will be minimized during ntraining It represents a measure of success for the task at hand n Optimizer Determines how the network will be updated based on the loss function nIt implements a specific variant of stochastic gradient descent SGD nStacks of Dense layers with relu activations can solve a wide range of problems including sentiment classification and you ll likely use them frequently nFinally we need to choose a loss function and an optimizer nBecause you re facing a binary classification problem and the output of your network is a probability you end your network with a single unit layer with a sigmoid activation it s best to use the binary crossentropy loss n Predicting X test nEvaluating the model nNot bad result to a simple model Let s now verify the validation of our model to see and understand the learning curve n n 7 Validation nLet s look this keys values further nWhy this occurs and how to solve this problem in graph it s a overffiting nIt s my first Deep Learning implementation I am studying about this and I will continue editing this Kernel to improve the results nGive me your feedback how can I increase this model n Stay tuned and don t forget to votesup this kernel h3 I will create a categorical variable to treat the Fare expend h2 To complete this part I will now work on Names nIt s looking ok nNow lets start explore the data nWe can look that dies to mens are much higher than female nNow lets do some exploration in Pclass and Embarked to see if might have some information to build the model nLooking the graphs is clear that 3st class and Embarked at Southampton have a high probabilities to not survive nTo finish the analysis I let s look the Sibsp and Parch variables nInteresting With 1 or 2 siblings spouses have more chance to survived the disaster nWe can see a high standard deviation in the survival with 3 parents children person s nAlso that small families 1 2 have more chance to survival than single or big families nSo to Finish our exploration I will create a new column to with familiees size nOK its might be enough to start with the preprocess and builting the model n n n 5 Preprocessing nNow we might have information enough to think about the model structure n n 6 Modelling nTitanic survivors prediction na binary classification example nTwo class classification or binary classification may be the most widely applied kind of machine learning problem nAnatomy of a neural network n nAs you saw in the previous chapters training a neural network revolves around the following nobjects n Layers which are combined into a network or model n The input data and corresponding targets n The loss function which defines the feedback signal used for learning n The optimizer which determines how learning proceeds n n n n n Layers the building blocks of deep learning nfrom keras import layers nlayer layers Dense 32 input dim data dimension n n We can think of layers as the LEGO bricks of deep learning a metaphor that is nmade explicit by frameworks like Keras Building deep learning models in Keras is ndone by clipping together compatible layers to form useful data transformation pipelines n n nWhat are activation functions and why are they necessary nWithout an activation function like relu also called a non linearity the Dense layer would consist of two linear operations a dot product and an addition noutput dot W input b n nSo the layer could only learn linear transformations affine transformations of the ninput data the hypothesis space of the layer would be the set of all possible linear ntransformations of the input data into a 16 dimensional space n n nLoss functions and optimizers nkeys to configuring the learning process nOnce the network architecture is defined you still have to choose two more things n Loss function objective function The quantity that will be minimized during ntraining It represents a measure of success for the task at hand n Optimizer Determines how the network will be updated based on the loss function nIt implements a specific variant of stochastic gradient descent SGD nStacks of Dense layers with relu activations can solve a wide range of problems including sentiment classification and you ll likely use them frequently nFinally we need to choose a loss function and an optimizer nBecause you re facing a binary classification problem and the output of your network is a probability you end your network with a single unit layer with a sigmoid activation it s best to use the binary crossentropy loss n Predicting X test nEvaluating the model nNot bad result to a simple model Let s now verify the validation of our model to see and understand the learning curve n n 7 Validation nLet s look this keys values further nWhy this occurs and how to solve this problem in graph it s a overffiting nIt s my first Deep Learning implementation I am studying about this and I will continue editing this Kernel to improve the results nGive me your feedback how can I increase this model n Stay tuned and don t forget to votesup this kernel h1 It s looking ok nNow lets start explore the data nWe can look that dies to mens are much higher than female nNow lets do some exploration in Pclass and Embarked to see if might have some information to build the model nLooking the graphs is clear that 3st class and Embarked at Southampton have a high probabilities to not survive nTo finish the analysis I let s look the Sibsp and Parch variables nInteresting With 1 or 2 siblings spouses have more chance to survived the disaster nWe can see a high standard deviation in the survival with 3 parents children person s nAlso that small families 1 2 have more chance to survival than single or big families nSo to Finish our exploration I will create a new column to with familiees size nOK its might be enough to start with the preprocess and builting the model n n n 5 Preprocessing nNow we might have information enough to think about the model structure n n 6 Modelling nTitanic survivors prediction na binary classification example nTwo class classification or binary classification may be the most widely applied kind of machine learning problem nAnatomy of a neural network n nAs you saw in the previous chapters training a neural network revolves around the following nobjects n Layers which are combined into a network or model n The input data and corresponding targets n The loss function which defines the feedback signal used for learning n The optimizer which determines how learning proceeds n n n n n Layers the building blocks of deep learning nfrom keras import layers nlayer layers Dense 32 input dim data dimension n n We can think of layers as the LEGO bricks of deep learning a metaphor that is nmade explicit by frameworks like Keras Building deep learning models in Keras is ndone by clipping together compatible layers to form useful data transformation pipelines n n nWhat are activation functions and why are they necessary nWithout an activation function like relu also called a non linearity the Dense layer would consist of two linear operations a dot product and an addition noutput dot W input b n nSo the layer could only learn linear transformations affine transformations of the ninput data the hypothesis space of the layer would be the set of all possible linear ntransformations of the input data into a 16 dimensional space n n nLoss functions and optimizers nkeys to configuring the learning process nOnce the network architecture is defined you still have to choose two more things n Loss function objective function The quantity that will be minimized during ntraining It represents a measure of success for the task at hand n Optimizer Determines how the network will be updated based on the loss function nIt implements a specific variant of stochastic gradient descent SGD nStacks of Dense layers with relu activations can solve a wide range of problems including sentiment classification and you ll likely use them frequently nFinally we need to choose a loss function and an optimizer nBecause you re facing a binary classification problem and the output of your network is a probability you end your network with a single unit layer with a sigmoid activation it s best to use the binary crossentropy loss n Predicting X test nEvaluating the model nNot bad result to a simple model Let s now verify the validation of our model to see and understand the learning curve n n 7 Validation nLet s look this keys values further nWhy this occurs and how to solve this problem in graph it s a overffiting nIt s my first Deep Learning implementation I am studying about this and I will continue editing this Kernel to improve the results nGive me your feedback how can I increase this model n Stay tuned and don t forget to votesup this kernel h2 We can look that dies to mens are much higher than female nNow lets do some exploration in Pclass and Embarked to see if might have some information to build the model nLooking the graphs is clear that 3st class and Embarked at Southampton have a high probabilities to not survive nTo finish the analysis I let s look the Sibsp and Parch variables nInteresting With 1 or 2 siblings spouses have more chance to survived the disaster nWe can see a high standard deviation in the survival with 3 parents children person s nAlso that small families 1 2 have more chance to survival than single or big families nSo to Finish our exploration I will create a new column to with familiees size nOK its might be enough to start with the preprocess and builting the model n n n 5 Preprocessing nNow we might have information enough to think about the model structure n n 6 Modelling nTitanic survivors prediction na binary classification example nTwo class classification or binary classification may be the most widely applied kind of machine learning problem nAnatomy of a neural network n nAs you saw in the previous chapters training a neural network revolves around the following nobjects n Layers which are combined into a network or model n The input data and corresponding targets n The loss function which defines the feedback signal used for learning n The optimizer which determines how learning proceeds n n n n n Layers the building blocks of deep learning nfrom keras import layers nlayer layers Dense 32 input dim data dimension n n We can think of layers as the LEGO bricks of deep learning a metaphor that is nmade explicit by frameworks like Keras Building deep learning models in Keras is ndone by clipping together compatible layers to form useful data transformation pipelines n n nWhat are activation functions and why are they necessary nWithout an activation function like relu also called a non linearity the Dense layer would consist of two linear operations a dot product and an addition noutput dot W input b n nSo the layer could only learn linear transformations affine transformations of the ninput data the hypothesis space of the layer would be the set of all possible linear ntransformations of the input data into a 16 dimensional space n n nLoss functions and optimizers nkeys to configuring the learning process nOnce the network architecture is defined you still have to choose two more things n Loss function objective function The quantity that will be minimized during ntraining It represents a measure of success for the task at hand n Optimizer Determines how the network will be updated based on the loss function nIt implements a specific variant of stochastic gradient descent SGD nStacks of Dense layers with relu activations can solve a wide range of problems including sentiment classification and you ll likely use them frequently nFinally we need to choose a loss function and an optimizer nBecause you re facing a binary classification problem and the output of your network is a probability you end your network with a single unit layer with a sigmoid activation it s best to use the binary crossentropy loss n Predicting X test nEvaluating the model nNot bad result to a simple model Let s now verify the validation of our model to see and understand the learning curve n n 7 Validation nLet s look this keys values further nWhy this occurs and how to solve this problem in graph it s a overffiting nIt s my first Deep Learning implementation I am studying about this and I will continue editing this Kernel to improve the results nGive me your feedback how can I increase this model n Stay tuned and don t forget to votesup this kernel h1 Now lets do some exploration in Pclass and Embarked to see if might have some information to build the model nLooking the graphs is clear that 3st class and Embarked at Southampton have a high probabilities to not survive nTo finish the analysis I let s look the Sibsp and Parch variables nInteresting With 1 or 2 siblings spouses have more chance to survived the disaster nWe can see a high standard deviation in the survival with 3 parents children person s nAlso that small families 1 2 have more chance to survival than single or big families nSo to Finish our exploration I will create a new column to with familiees size nOK its might be enough to start with the preprocess and builting the model n n n 5 Preprocessing nNow we might have information enough to think about the model structure n n 6 Modelling nTitanic survivors prediction na binary classification example nTwo class classification or binary classification may be the most widely applied kind of machine learning problem nAnatomy of a neural network n nAs you saw in the previous chapters training a neural network revolves around the following nobjects n Layers which are combined into a network or model n The input data and corresponding targets n The loss function which defines the feedback signal used for learning n The optimizer which determines how learning proceeds n n n n n Layers the building blocks of deep learning nfrom keras import layers nlayer layers Dense 32 input dim data dimension n n We can think of layers as the LEGO bricks of deep learning a metaphor that is nmade explicit by frameworks like Keras Building deep learning models in Keras is ndone by clipping together compatible layers to form useful data transformation pipelines n n nWhat are activation functions and why are they necessary nWithout an activation function like relu also called a non linearity the Dense layer would consist of two linear operations a dot product and an addition noutput dot W input b n nSo the layer could only learn linear transformations affine transformations of the ninput data the hypothesis space of the layer would be the set of all possible linear ntransformations of the input data into a 16 dimensional space n n nLoss functions and optimizers nkeys to configuring the learning process nOnce the network architecture is defined you still have to choose two more things n Loss function objective function The quantity that will be minimized during ntraining It represents a measure of success for the task at hand n Optimizer Determines how the network will be updated based on the loss function nIt implements a specific variant of stochastic gradient descent SGD nStacks of Dense layers with relu activations can solve a wide range of problems including sentiment classification and you ll likely use them frequently nFinally we need to choose a loss function and an optimizer nBecause you re facing a binary classification problem and the output of your network is a probability you end your network with a single unit layer with a sigmoid activation it s best to use the binary crossentropy loss n Predicting X test nEvaluating the model nNot bad result to a simple model Let s now verify the validation of our model to see and understand the learning curve n n 7 Validation nLet s look this keys values further nWhy this occurs and how to solve this problem in graph it s a overffiting nIt s my first Deep Learning implementation I am studying about this and I will continue editing this Kernel to improve the results nGive me your feedback how can I increase this model n Stay tuned and don t forget to votesup this kernel h3 Titanic survivors prediction na binary classification example h1 Anatomy of a neural network h2 Layers the building blocks of deep learning h2 What are activation functions and why are they necessary h2 Loss functions and optimizers nkeys to configuring the learning process h1 Evaluating the model h1 It s my first Deep Learning implementation I am studying about this and I will continue editing this Kernel to improve the results ", "stargazers_count": 0, "forks_count": 0, "description": "Modelling Titanic survivors prediction a binary classification exampleTwo class classification or binary classification may be the most widely applied kind of machine learning problem. And we will use that to complete the missing dataNow let s categorize them Now it look s better and clearly Let s cross our Pclass with the Age_cat We will aggregate than to get the mean of Fare by each category pair Very interesting. Adding an Dropout layer to previne from overfitting adding second hidden layer Adding another Dropout layer adding the output layer that is binary 0 1 With such a scalar sigmoid output on a binary classification problem the loss function you should use is binary_crossentropy Visualizing the model Creating an Stochastic Gradient Descent Compiling our model optimizers list Fitting the ANN to the Training set Fit the model list all data in history summarizing historical accuracy summarize history for loss Trying to implementing the TensorBoard to evaluate the model Importing the auxiliar and preprocessing librarys boxplot algorithm comparison. First look at the data I will start looking the type and informations of the datasets 4. Predicting X_testEvaluating the modelNot bad result to a simple model Let s now verify the validation of our model to see and understand the learning curve 7. Children s and ladys first huh Now I will handle the Age variable that has a high number of NaN s using some columns to correctly input he missing Age sThis might show us a better way to input the NAN s For example an male in 2 class that is a Officer the median Age is 42. Modelling Model 7. If the age is estimated is it in the form of xx. Dense 32 input_dim data_dimension We can think of layers as the LEGO bricks of deep learning a metaphor that ismade explicit by frameworks like Keras. cut and using the parameters that we created Printing the new Category Do the same to test dataset same as the other df train same that we used above in df train Describe of categorical Age Using pd. A hidden unit is a dimension in the representation space of the layer. We can see that babies has the highest mean value. Looking the Fare distribuition to survivors and not survivorsDescription of Fare variable Min 0 Median 14. It represents a measure of success for the task at hand. 5 intervals to categorize Labels without input values doing the cut in fare and puting in a new column Description of transformation Plotting the new feature Replicate the same to df_test Now lets drop the variable Fare Age and ticket that is irrelevant now Looking the result of transformations Let see how many people die or survived Distribuition by class lets input the NA s with the highest frequency Exploring Survivors vs Embarked Exploring Survivors vs Pclass Explore Parch feature vs Survived Create a new column and sum the Parch SibSp 1 that refers the people self Finallt lets look the correlation of df_train Feature Scaling Creating the model Inputing the first layer with input dimensions The argument being passed to each Dense layer 18 is the number of hidden units of the layer. Welcome to my Titanic Kernel This kernel will provide a analysis through the Titanic Disaster to understand the Survivors patternsI will handle with data transform missings manipulation explore the data descritive and visual and also create a Deep Learning modelAre you looking for another interesting Kernels CLICK HERE Give me your feedback and if yo like this kernel votes up I m from Brazil so english is not my first language sorry about some mistakes Table of Contents 1. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. On April 15 1912 during her maiden voyage the Titanic sank after colliding with an iceberg killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. Optimizer Determines how the network will be updated based on the loss function. Exploring some Variables Explorations 5. Stacks of Dense layers with relu activations can solve a wide range of problems including sentiment classification and you ll likely use them frequently. 5sibsp The dataset defines family relations in this way. Building deep learning models in Keras isdone by clipping together compatible layers to form useful data transformation pipelines. Finally we need to choose a loss function and an optimizer. Validation Let s look this keys values furtherWhy this occurs and how to solve this problem in graph it s a overffiting It s my first Deep Learning implementation. With 1 or 2 siblings spouses have more chance to survived the disasterWe can see a high standard deviation in the survival with 3 parents children person s Also that small families 1 2 have more chance to survival than single or big familiesSo to Finish our exploration I will create a new column to with familiees sizeOK its might be enough to start with the preprocess and builting the model 5. What are activation functions and why are they necessary Without an activation function like relu also called a non linearity the Dense layer would consist of two linear operations a dot product and an addition output dot W input bSo the layer could only learn linear transformations affine transformations of theinput data the hypothesis space of the layer would be the set of all possible lineartransformations of the input data into a 16 dimensional space. Data DictionaryVariable Definition Keysurvival Survival 0 No 1 Yespclass Ticket class 1 1st 2 2nd 3 3rdsex Sex Age Age in years sibsp of siblings spouses aboard the Titanic parch of parents children aboard the Titanic ticket Ticket number fare Passenger fare cabin Cabin number embarked Port of Embarkation C Cherbourg Q Queenstown S SouthamptonVariable Notespclass A proxy for socio economic status SES 1st Upper2nd Middle3rd Lowerage Age is fractional if less than 1. Sibling brother sister stepbrother stepsister Spouse husband wife mistresses and fianc\u00e9s were ignored parch The dataset defines family relations in this way. Loss functions and optimizers keys to configuring the learning processOnce the network architecture is defined you still have to choose two more things Loss function objective function The quantity that will be minimized duringtraining. Exploring the data To try a new approach in the data I will start the data analysis by the Name column Grouping some titles and ploting the results Title groupedIt s interesting. Anatomy of a neural network As you saw in the previous chapters training a neural network revolves around the followingobjects Layers which are combined into a network or model The input data and corresponding targets The loss function which defines the feedback signal used for learning The optimizer which determines how learning proceeds Layers the building blocks of deep learningfrom keras import layerslayer layers. Introduction Introduction 2. Preprocessing Prepocess 6. Stacks of Dense layers with relu activations can solve a wide range of problems including sentiment classification and you ll likely use them frequently. In this challenge we ask you to complete the analysis of what sorts of people were likely to survive. Parent mother father Child daughter son stepdaughter stepsonSome children travelled only with a nanny therefore parch 0 for them. Because you re facing a binary classification problem and the output of your network is a probability you end your network with a single unit layer with a sigmoid activation it s best to use the binary_crossentropy loss. crosstab to understand the Survived rate by Age Category s Seting the figure size Plotting the result seting the desired Seting the figure size Understanding the Fare Distribuition Filling the NA s with 0. Validation Validation 1. Introduction The data have 891 entries on train dataset and 418 on test dataset 10 columns in train_csv and 9 columns in train_testCompetition Description The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. 69I will create a categorical variable to treat the Fare expendI will use the same technique used in Age but now I will use the quantiles to binningTo complete this part I will now work on Names It s looking okNow lets start explore the dataWe can look that dies to mens are much higher than femaleNow lets do some exploration in Pclass and Embarked to see if might have some information to build the modelLooking the graphs is clear that 3st class and Embarked at Southampton have a high probabilities to not surviveTo finish the analysis I let s look the Sibsp and Parch variablesInteresting. I am studying about this and I will continue editing this Kernel to improve the resultsGive me your feedback how can I increase this model Stay tuned and don t forget to votesup this kernel This librarys is to work with matrices This librarys is to work with vectors This library is to create some graphics algorithmn to render the graphs import module to set some ploting parameters Library to work with Regular Expressions This function makes the plot directly on browser Seting a universal figure size Importing train dataset Importing test dataset Looking data format and types printing test info Some Statistics Take a look at the data Looking how the data is and searching for a re patterns GettingLooking the prefix of all Passengers defining the figure size of our graphic Plotting the result seting the xtitle and size Seting the ytitle and size Doing the same on df_test with regular expressions Now I will identify the social status of each title we map each title to correct category printing the chance to survive by each title figure size Plotting the count of title by Survived or not category First I will look my distribuition without NaN s I will create a df to look distribuition figure size Ploting the 2 variables that we create and compare the two Let s group the median age by sex pclass and title to have any idea and maybe input in Age NAN s printing the variabe that we created by median inputing the values on Age Na s using the groupby to transform this variables printing the total of nulls in Age Feature Let s see the result of the inputation seting the figure size ploting again the Age Distribuition after the transformation in our dataset separate by survivors or not figure size using facetgrid that is a great way to get information of our dataset creating the intervals that we need to cut each range of ages Seting the names that we want use to the categorys Applying the pd. Knowning the data Known 4. Preprocessing Now we might have information enough to think about the model structure 6. In particular we ask you to apply the tools of machine learning to predict which passengers survived the tragedy. It implements a specific variant of stochastic gradient descent SGD. Although there was some element of luck involved in surviving the sinking some groups of people were more likely to survive than others such as women children and the upper class. I am using the beapproachs as possible but if you think I can do anything another best way please let me know. Librarys Librarys 3. ", "id": "kabure/titanic-eda-model-pipeline-keras-nn", "size": "8651", "language": "python", "html_url": "https://www.kaggle.com/code/kabure/titanic-eda-model-pipeline-keras-nn", "git_url": "https://www.kaggle.com/code/kabure/titanic-eda-model-pipeline-keras-nn", "script": "sklearn.metrics RidgeClassifier cross_val_score PCA sklearn.naive_bayes keras.layers sklearn.tree sklearn.cluster train_test_split KMeans BaggingClassifier rcParams Dropout RandomTreesEmbedding Sequential KNeighborsClassifier SGD DecisionTreeClassifier cross_validate seaborn numpy sklearn.pipeline LinearSVC SGDClassifier MultinomialNB XGBClassifier ExtraTreesClassifier sklearn.decomposition GradientBoostingClassifier VotingClassifier sklearn.ensemble confusion_matrix sklearn.model_selection LabelEncoder KFold RandomForestClassifier matplotlib.pyplot Activation Dense pandas classification_report Pipeline keras.optimizers StandardScaler LogisticRegression accuracy_score sklearn.feature_selection sklearn.neighbors SVC sklearn.linear_model matplotlib sklearn.preprocessing sklearn.svm xgboost SelectFromModel keras.models ", "entities": "(('Age', '2 class'), 's') (('It', 'graph'), 'let') (('s', 'Sibsp variablesInteresting'), 'create') (('so english', 'Contents'), 'welcome') (('s', 'learning curve'), 'predict') (('Building', 'data transformation useful pipelines'), 'isdone') (('how network', 'loss function'), 'determine') (('hypothesis space', '16 dimensional space'), 'be') (('sensational tragedy', 'ships'), 'shock') (('Finally we', 'loss function'), 'need') (('it', 'binary_crossentropy loss'), 'be') (('how learning', 'learningfrom keras import layerslayer deep layers'), 'revolve') (('it', 'xx'), 'be') (('Now we', 'model enough structure'), 'have') (('I', 'datasets'), 'start') (('we', 'Age categorical pd'), 'do') (('that', 'still two more things'), 'function') (('best way me', 'anything'), 'use') (('data', 'algorithm comparison'), 'add') (('parch dataset', 'way'), 'ignore') (('classification exampleTwo class binary classification', 'machine learning binary most widely applied problem'), 'prediction') (('shipwreck', 'enough passengers'), 'be') (('ismade', 'Keras'), 'dense') (('hidden unit', 'layer'), 'be') (('SES Middle3rd Lowerage Age', 'less than 1'), 'Survival') (('sinking', 'history'), 'introduction') (('you', 'likely them'), 'solve') (('sizeOK', 'model'), 'have') (('I', 'results'), 'start') (('sorts', 'people'), 'ask') (('argument', 'layer'), 'interval') (('5sibsp dataset', 'way'), 'define') (('we', 'pd'), 'study') (('passengers', 'tragedy'), 'ask') (('It', 'hand'), 'represent') (('mean', 'category pair'), 'use') (('babies', 'highest mean value'), 'see') (('It', 'descent stochastic gradient SGD'), 'implement') ", "extra": "['test']"}