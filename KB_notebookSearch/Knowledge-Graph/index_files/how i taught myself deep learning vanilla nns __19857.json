{"name": "how i taught myself deep learning vanilla nns ", "full_name": " h1 1 Introduction h1 2 Before we start h1 3 What are Neural Networks h2 3 1 Youtube Videos that will save you time h2 3 2 The Perceptron h2 3 3 How does a plain Vanilla Neural Network look h2 3 4 Why so deep Deep vs Shallow Networks h1 4 The Data MNIST h1 5 Vanilla FNN Neural Network h2 5 1 Activation Functions h3 Rectifier Linear Unit ReLu h3 Sigmoid h3 Tanh h2 5 2 Making a Forward Pass h2 5 3 Backpropagation h3 5 3 1 Loss and Optimizer Functions h3 5 3 2 MNISTClassifier trainable parameters h3 5 3 3 Do 1 backpropagation Compute the LOSS and OPTIMIZE for our images example h1 6 Training the Neural Network h2 6 1 Batches h3 6 1 1 Training the Example Network on a batch instead of image by image h2 6 2 Accuracy of the Classifier h2 6 3 Iterations vs Epochs h2 6 4 Predefined Functions Accuracy and Training Loop h3 6 4 1 Predefined Accuracy Function h3 6 4 2 Predefined Training Function h1 7 Model Evaluation h1 8 Overfitting h2 8 1 Data Augmentation h3 8 1 1 Training on Augmented Data h2 8 2 Weight Decay and Learning Rate h2 8 3 Dropout and Layer Optimization h3 8 3 1 Dropout Function h3 8 3 2 Layer Optimization h3 8 3 3 Changing the Structure of our MNISTClassifier h4 Improved Model Structure h1 9 Bonuses h2 9 1 Confusion Matrix h1 Other How I taught myself Deep Learning Notebooks h1 References ", "stargazers_count": 0, "forks_count": 0, "description": "I am by no means a teacher but in this notebook I will 1. By calling this function we make this possible. The next chapters will be dedicated to training the network and improving it. For example above we did 3 iterations. Cuz this is just it numbers. It is known that Deep Neural Nets thin and tall are better than Shallow ones fat and short. So if you are frustrated just like I was when I started this stuff I hope the following guidelines will help you. That way we can compute the average loss across a mini batch of multiple images and take a step to optimize the average loss. Please bear with me. TanhA variation of the Sigmoid but it outputs values between 1 and 1. Let s start programming. optim updates the weights and biases to REDUCE the loss Examples Stochastic Gradient Descent SGD Adam Adam Adagrad Adagrad Different neural networks and purposes can require different loss and optimizer functions. Look at the prediction vs actual and computed the loss4. 3 Dropout and Layer Optimization 8. Let s look at 1 example The prediction is wrong Keep in mind the model is NOT trained yet so of course the prediction is not accurate. 1 Confusion Matrix Other How I taught myself Deep Learning Notebooks How I taught myself Deep Learning ConvNet CNNs https www. com watch v IHZwWFHWa w All torch. Size 10 value of the final neurons the log probabilities 5. Size 10 20 10 weights or parameters for each 20 neurons 10x20 weights in total 6 torch. Structure of the FNN Dropout for first layer From 784 neurons to layer1_size Activation Function Dropout for second layer From layer1_size neurons to layer2_size Activation Function Dropout for last layer Output layer Taking the image through the NN Flatten the matrix to a vector Log Probabilities output Training on the newly network Create Model Instance Train. com andradaolteanu how i taught myself deep learning convnet cnns Why ConvNets Convolutions Explained Computing Activation Maps Kernels Padding Stride AlexNet MNIST Classification using Convolutions2. When you evaluate the model call your_model. Size 50 50 biases 3 torch. Predefined Function that shows 20 images Create original and rotated set Show images Creating a personalized transform First Rotates then transforms to tensor then normalizes the images Import the MNIST data aplying the transformations We select first 500 images as our training Training the model Create Model Instance Train. Normalize means to scale the input features of a neural network so that all features are scaled similarly. Clears the gradients of all optimized Instantiate 2 variables for total cases and correct cases Sets the module in evaluation mode VERY IMPORTANT Just show first 3 batches accuracy Choose maximum probability and then select only the label not the prob number Number of correct cases we first see how many are correct in the batch then we sum then convert to integer not tensor Total cases Sets the model in evaluation mode Creates the dataloader Is formed by 20 images by default with 10 probabilities each Choose maximum probability and then select only the label not the prob number First check how many are correct in the batch then we sum then convert to integer not tensor Total cases Create dataloader for training dataset so we can train on multiple batches Shuffle after every epoch Create criterion and optimizer Losses Iterations to keep all losses during training for plotting Train and test accuracies to keep their values also for plotting Train the data multiple times Set model in training mode Create log probabilities Clears the gradients from previous iteration Computes loss how far is the prediction from the actual Computes gradients for neurons Updates the weights Save information after this iteration Compute accuracy after this epoch and save Show Accuracies Show the last accuracy registered Create plots Select images Training and Testing selection 500 training images 500 test images Create Model Instance Train. They are interpreted as probabilities probability of input to be digit 1 probability of input to be digit 2 etc. OverfittingAs any other Machine Learning Model Neural Nets can suffer from overfitting. Batches can have different sizes one extreme is batch_size 1 meaning that we compute the loss and update after EACH image so we have 60 000 batches of size 1 a batch_size 60 means that for 60 000 training images we ll have 1000 batches of size 60 the other extreme is batch_size 60 000 when we input ALL images and do 1 backpropagation we have 1 batch of size 60 000 images The actual batch size that we choose depends on many things. com watch v Ilg3gGewQ5U list PLZHQObOWTQDNU6R1_67000Dx_ZCJB 3pi index 3 Gradient Descent how Neural Networks learn https www. This way we prevent the weight from being overly dependent on eachother for example for one weight to be unnecessarily large to compensate for another unnecessarily large weight with the opposite sign. If you liked this upvote Cheers References Create your own FNN http alexlenail. This disables the gradients Dropout function etc and sets the model in evaluation mode. Size 20 20 biases 5 torch. So please be patient with yourself and if you don t understand something right away continue reading coding and it will all make sense in the end. What are Neural Networks How do Neural Networks learn 3. functions including loss optimizer functions https pytorch. 2 Predefined Training Function 7. So we penalize them by adding and extra term to the criterion function. CosineEmbeddingLoss etc. For somebody that starts in this area with no background whatsoever it can be very confusing especially because I seem to be unable to find code with many explanations and comments. 1 Training the Example Network on a batch instead of image by image 6. For Neural Networks is different they can be so volatile depending on the structure of your input eg. The learning rate controls how much to change the model in response to the estimated error each time the model weights are updated https machinelearningmastery. 4 each neuron has 40 chance of being dropped layer1_size size of the first hidden layer layer2_size size of the second hidden layer Improved Model Structure 9. html Impact of Learning Rate in NNs https machinelearningmastery. In the above example instead of having 70 noisy losses we ll have just 7 averaged losses. These Activation Functions squish the neuron s output between the 2 values preventing big numbers becoming much bigger and small numbers becoming much smaller. Until now in this notebook we haven t completed yet a full epoch. a portion of neurons from each training iteration. com 2018 05 wtf tensor. This notebook is made to bring more clear understanding of concepts and coding so this would also help me add modify and improve it. Function where you take the image though the FNN Flatten image from 1 28 28 to 784 Create Log Probabilities set the random seed set random seed in numpy Selecting 1 image with its label Creating an instance of the model Creating the log probabilities Choose maximum probability and then select only the label not the prob number Creating LOSS and Optimizer instances Loss is the function that calculates how far is the prediction from the true value Using this loss the Optimizer computes the gradients of each neuron and updates the weights Let s also look at how many parameters weights and biases are updating during 1 single backpropagation Parameter Understanding set the random seed set random seed in numpy Clear gradients always needs to be called before backpropagation Compute loss Compute Gradients Update weights After this 1 iteration the weights have updated once Create trainloaders for train and test data We put shuffle True so the images shuffle after every epoch Inspect Trainloader Select First Batch 60 1 28 28 60 images of size 1 28 28 actual labels for the 60 images 60 labels in total set the random seed set random seed in numpy Stop after 3 iterations Prediction Update weights or parameters Computes the gradient of current tensor Performs a single optimization step. CrossEntropyLoss Binary Cross Entropy Loss torch. 1 Youtube Videos that will save you time There are 2 super informative videos on Youtube from 3Blue1Brown that explain very well what neural networks are and what is an FNN Feed Foward Neural Network. We can change that by making it changable during training so eventually we can apply Grid Search and find the best combination possible. Optimizer Function torch. eval when we want to evaluate. Training the Neural NetworkOur purpose now that we have the structure in place and the data is to make the Vanilla FNN perform well. How I taught myself Deep Learning ConvNet CNNs https www. 2 Making a Forward PassA forward pass is when you take the images one by one or batch by batch we ll come back to this and we put them through the neural network which outputs for each a log probability 10 in out case. This is used because in general we would want to train the network for longer. For images is not really necessary as they all have the same structure but I threw it here just for reference. For example when you re building a RandomForestClassifier sklearn already has that made for you. L1Loss MSE torch. Learning Rate This one is probably not new. We want our batch size to be large enough to not be too noisy but not so large as to make each iteration too expensive to run. In Feed Foward Neural Nets the hidden layers gradually increase decrease in hidden size number of neurons so more and more details of the input images text etc. I highly suggest reading this blog post https towardsdatascience. Overfitting is when a neural network model learns about the quirks of the training data rather than information that is generalizable to the task at hand. 2 MNISTClassifier trainable parameters 1 torch. IntroductionThis notebook is just me being frustrated on deep learning and trying to understand in baby steps what is going on here. 2 The Perceptron A Perceptron is a single layer neural network while a Multi Layer Perceptron is called a Neural Network. Vanilla FNN Neural NetworkWhen working with neural networks you actually NEED to define a class for yourself. I highly recommend taking 20 minutes and watching them before going any further it is always better to visualise than rather read to understand abstract concepts. 1 Data AugmentationWhy try to collect more data when you can create some on your own Data Augmentation generates more data points from our existing data set by Flipping each image horizontally or vertically won t work for digit recognition but might for other tasks Shifting each pixel a little to the left or right Rotating the images a little Adding noise to the imageFor our example we ll rotate the images randomly up to 35 degrees. Share articles videos I watched that TRULY helped2. com andradaolteanu how i taught myself deep learning recurrent nns If you have any questions please do not hesitate to ask. 1 BatchesWith an artificial neural network we may want to use more than one image at one time. But first to make the training faster we will select 500 training images and 500 testing images batch_size will be by default set to 20 images batch we ll iterate through the data 200 times num_epochs 200 8. For each model we dropout drop out zero out remove etc. Large weights mean that the prediction relies heavily on the content of one or multiple pixels. html to construct your own FNN it can be dreadful to draw lines by yourself. com understand the dynamics of learning rate on deep learning neural networks Import torch Create some tensors Imports to access build in functions to build the NN to access activation functions to access the MNIST dataset to build out optimizer for plotting Load in the data from torchvision datasets train True to access training images and train False to access test images We also transform to Tensors the images How the object looks Check a sample of the images We need to import again because our train and test data are Tensors already Creating the Network nn. The Data MNISTWe ll be working in MNIST Dataset which is usually the go to dataset when starting in Neural Networks. com andradaolteanu how i taught myself deep learning convnet cnns How I taught myself Deep Learning Recurrent NNs https www. Pytorch has a different structure than the normal machine learning code in sklearn. Pro Tip Use this site http alexlenail. Embedding Loss functions whether 2 inputs are similar or not Hinge Loss torch. Took 1 image through the network and create prediction3. Hence in different iterations of training we will drop out a different set of neurons. Output It is composed of 1 single neuron which fires the value 1 if the image is a cat or 0 otherwise. There are maaany types of activation functions but some of them are Rectifier Linear Unit ReLu The function is linear when the activation is above zero and is equal to zero otherwise. Using the loss we updated the weights and biasesThis is called training. 2 Weight Decay and Learning Rate Weight Decay The idea of weight decay is to penalize large weights. Nevertheless you can apply the following principles on any datasets images text tabular data audio data as all data can be represented in numbers. You can also build multiple neural networks and then combine them in another one for example in a Sequence2Sequence RNN. In FNNs we train using gradient descent to update the weights. So you just call the object from the library and afterwards just fine tune the hyperparameters. Note During training it is highly important to set the model into training mode by calling your_model. 3 Changing the Structure of our MNISTClassifier Now let s change our Neural Net a bit nn. This enables gradients training the Dropout function etc. The image composed by numbers as you saw in the above videos is therefore a matrix of shape 1 10 20. com andradaolteanu how i taught myself deep learning recurrent nns 1 Layer RNNs Multiple Neurons RNN Vanilla RNN for MNIST Classification Multilayer RNNs Tanh Activation Function Multilayer RNN for MNIST LSTMs and Vanishing Gradient Problem Bidirectional LSTMs LSTM for MNIST Classification Pytorch This is the library we will be using it is allegedly much easier than Keras and it s starting to make a breakthrough nowadays. html What the hell is a Perceptron https towardsdatascience. 3 BackpropagationSo the purpose is to UPDATE the weights and biases in the neural network so it learns to recognize the digits and accurately classify them. an image of shape 3 500 250 number of hidden layers number of neurons in each hidden layer weather or not you want to call the Dropout functions etc. com watch v aircAruvnKk t 1007s What is Backpropagation really doing https www. Model EvaluationNow that we have our functions ready we can start training on the ENTIRE dataset. In addition it usually works with images and text while ML usually works with tabular data. Explain code along the way to the best of my ability Note Deep learning coding is VERY different in structure than the usual sklearn for machine learning. 2 Layer OptimizationOur MNISTClassifier had until now 3 layers with a fixed number on neurons in each layer. 3 Iterations vs Epochs Iterations number of iterations is the number of times we update the weights parameters of the FNN. com what the hell is perceptron 626217814f53 3Blue1Brown videos But what is a Neural Network https www. Layer2 the first layer is multiplied by the weights and formes the second layer composed by 300 neurons5. SigmoidThe sigmoid function has a tilted S shape and its output is always between 0 and 1. Layer1 Because the input in the FNN must be linear the matrix is vectorized by reshaping it into a vector of size 200. html to check all of them. Epoch number of times all training data was used once to update the parameters. Module is a subclass from which we inherit Here you define the structure Create first layer from 784 neurons to 50 Call activation function Second layer from 50 neurons to 20 Call Activation function Last layer from 20 neurons to 10 10 because we have 10 categories of numbers from which we need to pick 1 If we would have wanted to classify images labeled dog cat crocodile the final layer would have had 3 neurons. What is Backpropagation really doing Cheers again to 3Blue1Brown for his amazing structured videos. First we make sure we disable Gradient Computing Model in Evaluation Mode Add 1 more dimension for batching. 4 Why so deep Deep vs Shallow NetworksFirst of all the FNN isn t really doing anything special that a simple ML can t achieve. HingeEmbeddingLoss Cosine Loss torch. Also the number of parameters is smaller so the training is faster. Removing it would lead to an error. com understand the dynamics of learning rate on deep learning neural networks. Create Model Instance Train. How I taught myself Deep Learning Recurrent NNs https www. So to not complicate ourselves tensors are very similar with numpy arrays but they offer much better GPU support so they are faster. This is done during backpropagation when the model literally goes back and updates the parameters weights a little bit. 1 Activation FunctionsAn activation function is a fancy way of saying that we are making the output of each neuron nonlinear because we WANT to learn non linear relationships between the input and the output. Let s take a look 3. 1 Loss and Optimizer Functions These 2 are like brother and sister work hand in hand during the neural network training. Size 20 50 20 weights or parameters for each 50 neurons 50x20 weights in total 4 torch. 2 Accuracy of the ClassifierDuring Training we would usually want to check for the accuracy of the model to see how good or how bad is performing. 4 Predefined Functions Accuracy and Training LoopNow let s create some functions so our trainin process will become easier 6. This happens because the deep ones can learn more and more abstract representations the deeper you go. A tensor is a container which can house data in N dimensions https www. So be aware Tensors Instead of working with tabular data or numpy arrays we ll be working with tensors. 3 Do 1 backpropagation Compute the LOSS and OPTIMIZE for our images_example Until now we 1. So here we will create our own neural net Note you can ignore the prints I usually put them to understand what the network does when you click run Note2 super function is there because the MNISTClassifier class inherits attributes from it s parent class nn. html WTF is a Tensor https www. They change by the case but their main purpose is the same Loss Function criterion given an output and an actual it computes the difference between them Regressive loss functions MAE torch. We have 1 example of an FNN our network is already trained to receive an B W image and create the output 1 if the image is a cat and 0 otherwise. Click here https pytorch. In other words weights are encouraged to be strong and independent. com what the hell is perceptron 626217814f53 for some very good explanations. Input An image of a cat with height 10 pixels length 20 pixels and channels 1 because is B W for RGB images the number of channels is 3. Before going any further I highly recommend watching the following video which explains the concept of Backpropagation. If we choose a lr too large we might overshoot the local minima while using a lr too small we might wait longer for the model to train as the steps are tinier. Therefore the FNN is a Classifier. Size 50 784 50 weights or parameters for each 28x28 neurons 28x28x50 weights in total 2 torch. The beauty in Deep Learning is actually the amount of data and computations that can be handled much better than in an usual ML algorithm. 1 Predefined Accuracy Function 6. Classification loss functions Cross Entropy Loss torch. Layer3 The last hidden layer is composed of 400 neurons6. Created a Vanilla FNN2. 3 How does a plain Vanilla Neural Network look Plain vanilla Neural Networks or Feed Forward Neural Networks FNNs for the lazy people have the most simple architecture in the Neural Networks realm but their basics will help you understand much more complicated dinosaurs moving on. 1 Training on Augmented Data transforms. 1 Dropout FunctionThis technique builds many models and then averages their prediction at test time this is why it is very important to call model. The average loss across multiple training inputs is going to be less noisy than the loss for a single input and is less likely to provide bad information because of a bad input. Let s break down the steps 1. What are Neural Networks 3. Before we start This is my first notebook in the series How I taught myself Deep Learning. ", "id": "andradaolteanu/how-i-taught-myself-deep-learning-vanilla-nns", "size": "19857", "language": "python", "html_url": "https://www.kaggle.com/code/andradaolteanu/how-i-taught-myself-deep-learning-vanilla-nns", "git_url": "https://www.kaggle.com/code/andradaolteanu/how-i-taught-myself-deep-learning-vanilla-nns", "script": "torch.nn.functional torch.optim __init__ train_network forward MNISTClassifier_improved(nn.Module) torch.nn get_accuracy MNISTClassifier(nn.Module) transforms        # to access the MNIST dataset numpy matplotlib.pyplot seaborn show20 datasets get_confusion_matrix torchvision ", "entities": "(('Tensors', 'Network already nn'), 'understand') (('training test Training selection 500 images 500 images', 'Model Instance Train'), 'clear') (('How I', 'Deep Learning NNs https Recurrent www'), 'andradaolteanu') (('average loss', 'bad input'), 'go') (('So you', 'library'), 'call') (('we', 'input'), 'be') (('How I', 'Deep Learning NNs https Recurrent www'), 'teach') (('we', 'weights'), 'train') (('we', 'data'), 'make') (('trainin process', 'functions'), '4') (('first layer', '300'), 'Layer2') (('Dropout', 'evaluation mode'), 'disable') (('Learning Weight 2 Weight Decay idea', 'large weights'), 'decay') (('When you', 'your_model'), 'evaluate') (('why it', 'very model'), 'build') (('number', 'channels'), 'input') (('deeper you', 'more abstract representations'), 'happen') (('steps', 'longer model'), 'overshoot') (('next chapters', 'it'), 'dedicate') (('Neural how Networks', 'https www'), 'list') (('prediction', 'one pixels'), 'mean') (('image', '1'), 'output') (('any further it', 'rather abstract concepts'), 'recommend') (('rather that', 'hand'), 'be') (('I', 'here just reference'), 'be') (('SGD Adam Adam Adagrad Adagrad Different', 'neural different loss functions'), 'update') (('matrix', 'size'), 'Layer1') (('enables', 'etc'), 'gradient') (('it', '1'), 'variation') (('How I', 'Deep Learning'), 'be') (('data', 'numbers'), 'apply') (('Backpropagation', 'amazing structured videos'), 'do') (('we', 'neurons'), 'drop') (('literally back parameters', 'backpropagation'), 'do') (('following guidelines', 'you'), 'be') (('which', 'N dimensions https www'), 'be') (('already that', 'you'), 'make') (('way we', 'opposite sign'), 'prevent') (('Inspect Trainloader Select First 28 60 1 28 60 images', 'optimization single step'), 'set') (('How I', 'Deep Learning ConvNet CNNs https www'), 'teach') (('dropout', 'etc'), 'drop') (('model time weights', 'https updated machinelearningmastery'), 'control') (('special simple ML', 'really anything'), '4') (('which', 'Backpropagation'), 'recommend') (('we', 'batching'), 'make') (('it', 'breakthrough'), 'andradaolteanu') (('Vanilla FNN', 'place'), 'train') (('much more complicated dinosaurs', 'Neural Networks realm'), '3') (('Do 1 backpropagation', 'we'), 'compute') (('that', 'ML much better usual algorithm'), 'be') (('ConvNets Why Convolutions', 'Convolutions2'), 'andradaolteanu') (('general we', 'network'), 'use') (('we', 'ENTIRE dataset'), 'EvaluationNow') (('it', 'yourself'), 'be') (('this', 'function'), 'make') (('we', 'randomly up to 35 degrees'), '1') (('FNN Feed Foward Neural Network', '3Blue1Brown'), 'be') (('1 artificial neural we', 'one time'), 'BatchesWith') (('final layer', '3 neurons'), 'be') (('I', 'blog post https highly towardsdatascience'), 'suggest') (('numpy we', 'tensors'), 'be') (('it', 'accurately them'), 'be') (('they', 'input eg'), 'be') (('also me', 'it'), 'make') (('we', 'just 7 averaged losses'), 'in') (('hidden layers', 'input images'), 'increase') (('you', 'therefore shape'), 'be') (('They', 'digit 1 input'), 'interpret') (('class there MNISTClassifier inherits', 'parent class nn'), 'create') (('which', 'Neural usually when Networks'), 'work') (('we', 'many things'), 'have') (('last hidden layer', '400 neurons6'), 'Layer3') (('prediction', 'yet course'), 'let') (('Now s', 'bit nn'), 'let') (('Pytorch', 'sklearn'), 'have') (('training data', 'once parameters'), 'use') (('hell', 'very good explanations'), 'com') (('How I', 'Deep Learning ConvNet CNNs https www'), '1') (('Neural How Networks', '3'), 'be') (('I', 'notebook'), 'be') (('you', 'Dropout functions'), 'image') (('features', 'neural network'), 'mean') (('it', 'loss functions MAE Regressive torch'), 'change') (('very especially I', 'many explanations'), 'be') (('Machine Learning Model Neural OverfittingAs other Nets', 'overfitting'), 'suffer') (('what', 'baby steps'), 'be') (('training', 'Create Model Instance Train'), 'transform') (('You', 'Sequence2Sequence RNN'), 'build') (('Neural Deep Nets', 'ones Shallow fat'), 'know') (('So we', 'criterion extra function'), 'penalize') (('2 MNISTClassifier trainable', '1 torch'), 'parameter') (('we', 'FNN'), 'be') (('when activation', 'zero'), 'be') (('output', 'always 0'), 'have') (('one', 'Rate'), 'learn') (('it', 'your_model'), 'note') (('you', 'questions'), 'andradaolteanu') (('it', 'end'), 'be') (('biasesThis', 'weights'), 'update') (('Backpropagation', 'https really www'), 'watch') (('Layer OptimizationOur 2 MNISTClassifier', 'layer'), 'have') (('best combination', 'Grid Search'), 'change') (('haven t', 'yet full epoch'), 'complete') (('you', 'yourself'), 'FNN') (('Deep learning coding', 'machine learning'), 'explain') (('how how bad', 'model'), 'want') (('which', 'out case'), 'be') (('much bigger numbers', 'big numbers'), 'squish') (('com', 'deep learning neural networks'), 'understand') (('4 neuron', 'second hidden layer'), 'have') (('training', 'parameters'), 'be') (('1 image', 'output'), 'have') (('ML', 'usually tabular data'), 'work') (('1 Loss 2', 'network neural training'), 'function') (('they', 'GPU much better support'), 'be') (('way we', 'average loss'), 'compute') ", "extra": "['patient', 'test']"}