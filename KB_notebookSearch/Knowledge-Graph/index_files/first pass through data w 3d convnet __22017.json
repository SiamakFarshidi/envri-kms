{"name": "first pass through data w 3d convnet ", "full_name": " h2 Applying a 3D convolutional neural network to the data h2 Quick introduction to Kaggle h2 About this specific competition h2 Requirements and suggestions for following along h2 Alright let s get started h1 Section 1 Handling Data h1 Section 2 Processing and viewing our Data h1 Section 3 Preprocessing our Data h1 Section 4 3D Convolutional Neural Network h2 Moment o truth h1 Section 5 Concluding Remarks h2 Now what ", "stargazers_count": 0, "forks_count": 0, "description": "When you create an account head to competitions in the nav bar choose the Data Science Bowl then head to the data tab. For example you can grab data from the LUNA2016 challenge https luna16. Bottom line There are tons of options here. Neural networks are capable of amazing things only when we have the datasets to support it. My goal here is that anyone even people new to kaggle can follow along. Example output that I got Epoch 1 completed out of 10 loss 195148607547. It might be the case our testing set has more cancerous examples or maybe less we really don t know. Bring in more outside data. We have a few options at this point we could take the code that we have already and do the processing online. com questions 312443 how do you split a list into evenly sized chunks some patients don t have labels so we ll just pass on this for now again some patients are not labeled but JIC we still want the error if something else is wrong with our code Credit Ned Batchelder Link http stackoverflow. We might just simply not have enough data. net data analysis python pandas tutorial introduction 3 https pythonprogramming. com questions 312443 how do you split a list into evenly sized chunks stage 1 for real. Thus we have to begin by simply trying things and seeing what happens I have a few theories about what might work but my first interest was to try a 3D Convolutional Neural Network. Just uncomment it locally and it will run. Before just adding any noise it would be wise to research what exactly doctors do when analyzing CT scans for cancerous tumors. com c data science bowl 2017 discussion 27922 1 for how an actual doctor reads a CT scan. the list goes on and on. Do note here that the actual scan when loaded by dicom is clearly not JUST some sort of array of values instead it s got attributes. Awesome Thanks Ned Okay once we ve got these chunks of these scans what are we going to do Well we can just average them together. To be honest I don t know of any super smooth way of doing this but that s fine. Finishing accuracy Accuracy 0. Your job is to predict chance of cancer it s not so binary. net cnn tensorflow convolutional nerual network machine learning tutorial Now we re ready for the network itself Why 54080 magic number To get this I simply run the script once and see what the error yells at me for the expected size multiple. We will also be making use of Pandas for some data analysis 2 Matplotlib for data visualization 3 You do not need to go through all of those tutorials to follow here but if you are confused it might be useful to poke around those. For example you can check out this tutorial https www. 6Epoch 3 completed out of 10 loss 5744945978. My thought is that what we have is really a big list of slices. Even if it was what was the number to beat Was it 50 since it s either cancer or not Not quite. Okay so now what I think we need to address the whole non uniformity of depth next. Your convolutional window padding strides need to change. Now let s see what an actual slice looks like. It s going to take a while. I am by no means an expert data analyst statistician and certainly not a doctor. This is a raw look into the actual code I used on my first pass there s a ton of room for improvment. All of our images are the same size but the slices arent. Really any of this code. Do note that if you do wish to compete you can only use free datasets that are available to anyone who bothers to look. Awesome That s actually a decently large hurdle. for you To install the CPU version of TensorFlow just do pip install tensorflow To install the GPU version of TensorFlow you need to get alllll the dependencies and such. This current competition is a 2 stage competition where you have to participate in both stages to win. There are a few attributes here of arrays but not all of them. com gzuidhof data science bowl 2017 full preprocessing tutorial a couple great 1 liners from https www. There are numerous ways that we could go about creating a classifier. If you see something that you could improve share it with me Quick introduction to Kaggle If you are new to kaggle create an account and start downloading the data. Regardless this much data wont be an issue to keep in memory or do whatever the heck we want. you don t have enough data to really do this I am passing for the sake of notebook space but we are getting 1 shaping issue from one input tensor. You wouldn t want to add noise that conflicts here most likely. net tensorflow introduction machine learning tutorial 5 https pythonprogramming. Being 512 x 512 I am already expecting all this data to be the same size but let s see what we have from other patients too 1 https www. We know the scans are in this dicom format but what is that If you re like me you have no idea what that is or how it will look in Python You can learn more about DICOM from Wikipedia 1 if you like but our main focus is what this will actually be in Python terms. Thus we can hopefully just average this slice together and maybe we re now working with a centimeter or so. Okay the Python gods are really not happy with me for that hacky solution. No cancer always would give us 72 accuracy. If you are completely new to data science I will do my best to link to tutorials and provide information on everything you need to take part. net how to cuda gpu tensorflow deep learning tutorial 2 https www. We have about 200 slices though I d feel more comfortable if I saw a few more. This initial pass is not going to win the competition but hopefully it can serve as a starting point or at the very least you can learn something new along with me. At the release of stage 2 the validation set answers are released and then you make predictions on a new test set that comes out at the release of this second stage. If there s a growth there it should still show up on scan. holy moly 512 x 512 This means our 3D rendering is a 195 x 512 x 512 right now. Now what Well likely the largest issue here is our size of data. Then there will be actual blind or out of sample testing data that you will actually use your model on which will spit out an output CSV file with your predictions based on the input data. That s fine we can play with that constant more later we just want to know how to do it. Consider other models. just how much data do we have here Oh. Do note that now to have a bigger window your processing penalty increases significantly as we increase in size obviously much more than with 2D windows. If you can preprocess all of the data into one file and that one file doesn t exceed your available memory then training should likely be faster so you can more easily tweak your neural network and not be processing your data the same way over and over. If you do not have matplotlib do pip install matplotlib Want to learn more about Matplotlib Check out the Data Visualization with Python and Matplotlib tutorial 1. In effort to not turn this notebook into an actual book however we re going to move forward We can now see our new data by doing Section 3 Preprocessing our Data Okay so we know what we ve got and what we need to do with it. It s still possible to cheat. 1 http 20http stackoverflow. Someone feel free to enlighten me how one could actually calculate this number beforehand. 6Epoch 6 completed out of 10 loss 1014763813. AFAIK it s the padding that causes this to not be EXACTLY 50 000 50 x 50 x 20 is the size of our actual input data which is 50 000 total. net matplotlib intro tutorial Now I am not a doctor but I m going to claim a mini victory and say that s our first CT scan slice. Also there s no good reason to maintain a network in GPU memory while we re wasting time processing the data which can be easily done on a CPU. You can submit up to 3 entries a day so you want to be very happy with your model and you are at least slightly disincentivised from trying to simply fit the answer key over time. Applying a 3D convolutional neural network to the data Welcome everyone to my coverage of the Kaggle Data Science Bowl 2017. If we re going to be successful with a neural network we need more data. 57Epoch 10 completed out of 10 loss 96427552. size of window movement of window as you slide about 5 x 5 x 5 patches 32 channels 64 features to compute. You will also need numpy here. Let s see what the best score our classifer could get is if it just always picked the most common class So actually our dataset has 1035 non cancer examples and 362 cancerous examples. We re sorting by the actual image position in the scan. But hey we did it We figured out a way to make sure our 3 dimensional data can be at any resolution we want or need. Our dataset is only 1500 even less if you are following in the Kaggle kernel patients and will be for example 20 slices of 150x150 image data if we went off the numbers we have now but this will need to be even smaller for a typical computer most likely. 62Epoch 9 completed out of 10 loss 122785997. We ve got CT scans of about 1500 patients and then we ve got another file that contains the labels for this data. 64 features image X image Y image Z If you are working with the basic sample data use maybe 2 instead of 100 here. Your training file should be 700mb with 1400 total labeled samples. This isn t quite ideal and will cause a problem later. Consider neural network variables like layers learning rate activation functions optimizer. net matplotlib intro tutorial Alright let s get started Section 1 Handling Data Assuming you ve downloaded the data what exactly are we working with here The data consists of many 2D slices which when combined produce a 3 dimensional rendering of whatever was scanned. When running locally make sure your training data is NOT the sample images it should be the stage1 images. As we continue through this however you re hopefully going to see just how many theories we come up with and how many variables we can tweak and change to possibly get better results. com c data science bowl 2017 discussion 27922 2 https www. com gzuidhof data science bowl 2017 full preprocessing tutorial 1 One immediate thing to note here is those rows and columns. I ve never had data to try one on before so I was excited to try my hand at it Before we can feed the data through any model however we need to at least understand the data we re working with. We d definitely want to confirm our testing set actually has this ratio before assuming anything. org wiki DICOM 2 https pypi. About this specific competition At its core the aim here is to take the sample data consisting of low dose CT scan information and predict what the liklihood of a patient having lung cancer is. Create new data from the existing data by adding noise. If you do not have opencv do a pip install cv2 Want to learn more about what you can do with Open CV Check out the Image analysis and manipulation with OpenCV and Python tutorial 1. Later we could actually put these together to get a full 3D rendering of the scan. Being a realistic data science problem we actually don t really know what the best path is going to be. One major issue is these colors and ranges of data. com c data science bowl 2017 discussion 27666 for reading dicom files for doing directory operations for some simple data analysis right now just to load in the labels data and quickly reference it Change this to wherever you are storing your data IF YOU ARE FOLLOWING ON KAGGLE YOU CAN ONLY PLAY WITH THE SAMPLE DATA WHICH IS MUCH SMALLER a couple great 1 liners from https www. We ve got to actually figure out a way to solve that uniformity problem but also. Guessing it s one of the depths that doesn t come to 20. All this said this specific competition isn t actually just right or wrong. net neural networks machine learning tutorial 4 https pythonprogramming. Installation tutorials Installing the GPU version of TensorFlow in Ubuntu 1 Installing the GPU version of TensorFlow on a Windows machine 2 Using TensorFlow and concept tutorials Introduction to deep learning with neural networks 3 Introduction to TensorFlow 4 Intro to Convolutional Neural Networks 5 Convolutional Neural Network in TensorFlow tutorial 6 Now the data we have is actually 3D data not 2D data that s covered in most convnet tutorials including mine above. If at all possible I prefer to separate out steps in any big process like this so I am going to go ahead and pre process the data so our neural network code is much simpler. The real number we need to beat is if our network was to always predict a single class. I am going to do my best to make this tutorial one that anyone can follow within the built in Kaggle kernels Requirements and suggestions for following along I will be using Python 3 and you should at least know the basics 1 of Python 3. That s why this is a competition. com gzuidhof data science bowl 2017 full preprocessing tutorial the first slice Credit Ned Batchelder Link http stackoverflow. I found the torrent to download the fastest so I d suggest you go that route. Now we can begin to iterate through the patients and gather their respective data. As per Ned Batchelder via Link http stackoverflow. 1595 in real data 20 if you re in the Kaggle sample dataset Well that s also going to be a challenge for the convnet to figure out but we re going to try Also there are outside datasources for more lung scans. Let s look at the first 12 and resize them with opencv. Let s say we want to have 20 scans instead. For the actual dependency installs and such I will link to them as we go. Okay what you re about to see you shouldn t attempt if anyone else is watching like if you re going to show your code to the public. 1 https pythonprogramming. Your submission is scored based on the log loss of your predictions. I expect that with a large enough dataset this wouldn t be an actual issue but with this size of data it might be of huge importance. Run this locally train_neural_network x. In our test that s the real number to beat. That s huge Alright so we already know that we re going to absolutely need to resize this data. org data for another 888 scans. I am not going to ask the Kaggle online kernel to even bother building this computation graph so I will comment out the line to actually run this. 46Epoch 7 completed out of 10 loss 680146186. 54Epoch 8 completed out of 10 loss 289082075. 505a20caf6ab6df4643644c923f06a5eb 0. We can either hunt for more outside datasources or we can engage in adding some noise to the data. com questions 312443 how do you split a list into evenly sized chunks 1 we ve got ourselves a nice chunker generator. Do a pip install pydicom and pip install pandas and let s see what we ve got 1 https en. How can we do this Well first we need something that will take our current list of scans and chunk it into a list of lists of scans. That s not in my plans here since that s already been something covered very well see this kernel https www. Are we totally done. org pypi pydicomAt this point we ve got the list of patients by their IDs and their associated labels stored in a dataframe. 63Epoch 2 completed out of 10 loss 14236109414. In this case the submission file should have two columns one for the patient s id and another for the prediction of the liklihood that this patient has cancer like id cancer01e349d34c02410e1da273add27be25c 0. I couldn t think of anything off the top of my head for this so I Googled how to chunk a list into a list of lists. This is what you will upload to kaggle and your score here is what you compete with. This is just a theory it has to be tested. If you re already familiar with neural networks and TensorFlow great If not as you might guess I have a tutorial. these images are just WAY too big for a convolutional neural network to handle without some serious computing power. We can though In this case it s pretty close. It s unclear to me whether or not a model would appreciate that. net introduction to python programming 2 https pythonprogramming. This is certainly not the right way to go about it but that s my 100 honest method and my first time working in a 3D convnet. At the end you can submit 2 final submissions allowing you to compete with 2 models if you like. My theory is that a scan is a few millimeters of actual tissue at most. So what changes EVERYTHING OMG IT S THE END OF THE WORLD AS WE KNOW IT It s not really all too bad. 6Epoch 5 completed out of 10 loss 1916325681. 9992289899768697 Section 5 Concluding Remarks So how did we do Well we overfit almost certainly. In terms of a 3D rendering these actually are not the same size. What we need is to be able to just take any list of images whether it s got 200 scans 150 scans or 300 scans and set it to be some fixed number. net loading images python opencv tutorial Section 2 Processing and viewing our Data Alright so we re resizing our images from 512x512 to 150x150. net convolutional neural network cnn machine learning tutorial 6 https pythonprogramming. There s always a sample submission file in the dataset so you can see how to exactly format your output predictions. If you do cheat you wont win anything since you will have to disclose your model for any prizes. This notebook is my actual personal initial run through this data and my notes along the way. The dataset is pretty large at 140GB just in initial training data so this can be somewhat restrictive right out of the gate. If any of you would like to improve this chunking averaging code feel free. see what happens Now I am not about to stuff a neural networks tutorial into this one. Not sure why will have to look into it. In this case that s the chest cavity of the patient. You probably already have numpy if you installed pandas but just in case numpy is pip install numpy 1 https pythonprogramming. I d start here https www. com watch v r7 WPbx8VuY 3 https pythonprogramming. Even if we do a grayscale colormap in the imshow you ll see that some scans are just darker overall than others. We actually don t have to have all of the data prepared before we go through the network. 50d12f1c627df49eb223771c28548350e 0. 7Epoch 4 completed out of 10 loss 3268944715. com c data science bowl 2017 discussion 27666 2 official external data thread. Thus an algorithm that always predicted no cancer with our model would be 74 accurate 1035 1397. Stage one has you competing based on a validation dataset. Now we re set to train the network. Overlaying other pictures on top rotating messing with edges adding various transforms. It might also just simply be the case that a neural network isn t the best model of choice here. This is how real programming is happens. We re almost certainly going to need to do some preprocessing of this data but we ll see. By this I mean while training the network we can actually just loop over our patients resize the data then feed it through our neural network. Now I will just make a slight modification to all of the code up to this point and add some new final lines to preprocess this data and save the array of arrays to a file Section 4 3D Convolutional Neural Network Moment o truth Okay we ve got preprocessed normalized data. How about our accuracy Due to the lower amount of data on Kaggle I have no idea what number you re seeing just know it s probably not all that great. There are many ways to add noise to data. Welcome to datascience Okay next question is. I ll have us stick to just the base dataset again mainly so anyone can poke around this code in the kernel environment. Above we iterate through each patient we grab their label we get the full path to that specific patient inside THAT path contains 200ish scans which we also iterate over BUT also want to sort since they wont necessarily be in proper order. In many more realistic examples in the world however your dataset will be so large that you wouldn t be able to read it all into memory at once anyway but you could still maintain one big database or something. Just in case you are new how does all this work In general Kaggle competitions will come with training and testing data for you to build a model on where both the training and testing data comes with labels so you can fit a model. 150 is still going to wind up likely being waaaaaaay to big. Now we re ready to feed it through our 3D convnet and. Keep hacking away 1 https www. Thus we already know out of the gate that we re going to need to downsample this data quite a bit AND somehow make the depth uniform. if you have improvements share them This is going to stay pretty messy. This might be problematic and we might need to actually normalize this dataset. With pictures we can do all sorts of things. I can at least think of A way and that s all we need. Luckily for us there already exists a Python package for reading dicom files Pydicom 2. com questions 312443 how do you split a list into evenly sized chunksThe struggle is real. You will need to accept the terms of the competition to proceed with downloading the data. com gzuidhof data science bowl 2017 full preprocessing tutorialAlright so above we just went ahead and grabbed the pixel_array attribute which is what I assume to be the scan slice itself we will confirm this soon but immediately I am surprised by this non uniformity of slices. ", "id": "sentdex/first-pass-through-data-w-3d-convnet", "size": "22017", "language": "python", "html_url": "https://www.kaggle.com/code/sentdex/first-pass-through-data-w-3d-convnet", "git_url": "https://www.kaggle.com/code/sentdex/first-pass-through-data-w-3d-convnet", "script": "convolutional_neural_network train_neural_network numpy matplotlib.pyplot tensorflow pandas chunks maxpool3d process_data mean conv3d ", "entities": "(('Ned Batchelder', 'stackoverflow'), 'http') (('also also they', 'necessarily proper order'), 'grab') (('It', 'network isn also just simply neural best choice'), 'be') (('we', 'at least way'), 'think') (('cancer', 'always 72 accuracy'), 'give') (('pip', 'Python tutorial'), 'install') (('3 you', '1 Python'), 'go') (('We', 'uniformity problem'), 'get') (('this', 'somewhat right gate'), 'be') (('we', 'actually dataset'), 'be') (('that', 'mini victory'), 'tutorial') (('we', 'what'), 'install') (('just it', 'idea'), 'about') (('Now we', 'respective data'), 'begin') (('already we', 'absolutely data'), 's') (('we', 'network'), 'have') (('we', 'input one tensor'), 'have') (('you', 'https tutorial www'), 'check') (('net', 'https deep learning tutorial 2 www'), 'tensorflow') (('so I', 'actually this'), 'go') (('6Epoch 6', '10 loss'), 'complete') (('we', 'more data'), 'need') (('which', 'easily CPU'), 's') (('we', 'data'), 'go') (('actual slice', 'what'), 'let') (('these', '3D rendering'), 'be') (('this', 'Python actually terms'), 'know') (('I', 'improvment'), 'be') (('instead it', 'attributes'), 'note') (('how one', 'actually number'), 'feel') (('so I', 'lists'), 'think') (('This', 'them'), 'have') (('we', 'lung more scans'), '1595') (('So actually dataset', 'non cancer 1035 examples'), 'let') (('more later we', 'just how it'), 's') (('there it', 'still scan'), 'show') (('You', 'data'), 'need') (('that', 'test'), 's') (('notebook', 'way'), 'be') (('we', '20 scans'), 'let') (('exactly doctors', 'cancerous tumors'), 'be') (('we', 'it'), 'go') (('network', 'always single class'), 'be') (('you', 'about 5 5 5 patches 32 64 features'), 'size') (('anyway you', 'still one big database'), 'be') (('62Epoch 9', '10 loss'), 'complete') (('Later we', 'scan'), 'put') (('don really t', 'more cancerous examples'), 'be') (('Credit Ned Batchelder Link', 'stackoverflow'), '312443') (('any', 'averaging chunking code'), 'feel') (('we', 'possibly better results'), 'go') (('you', 'dependencies'), 'install') (('scan', 'few actual tissue'), 'be') (('you', 'data'), 'share') (('we', 'file 3D Convolutional Neural Network Moment o Section 4 truth'), 'make') (('together maybe we', 'now centimeter'), 'average') (('We', 'scan'), 'sort') (('conflicts', 'noise'), 'wouldn') (('which', 'input actual data'), 's') (('it', '200 150 300 it'), 'be') (('training file', '700 1400 total labeled samples'), 'be') (('submission', 'predictions'), 'score') (('we', 'data'), 'hunt') (('you', 'prizes'), 'win') (('case just numpy', 'https 1 pythonprogramming'), 'numpy') (('57Epoch 10', '10 loss'), 'complete') (('net data analysis', 'python tutorial https pandas 3 pythonprogramming'), 'introduction') (('you', 'model'), 'be') (('first interest', 'Convolutional Neural 3D Network'), 'have') (('we', 'resolution'), 'do') (('list', 'associated dataframe'), 'pypi') (('that', 'chest patient'), 's') (('we', 'at least data'), 'have') (('one you', 'validation dataset'), 'have') (('2017 27922 1 how actual doctor', 'CT scan'), 'c') (('we', 'depth bit somehow uniform'), 'know') (('Now likely largest issue', 'here data'), 'be') (('I', 'means'), 'be') (('who', 'anyone'), 'note') (('Python gods', 'hacky solution'), 'be') (('you', 'part'), 'do') (('already something', 'kernel https very well www'), 's') (('Now I', 'one'), 'see') (('have', 'really big slices'), 'be') (('you', 'new me'), 'go') (('how you', '1 real'), '312443') (('lung cancer', 'patient'), 'be') (('Credit Ned Batchelder Link', 'stackoverflow'), 'bowl') (('you', 'challenge LUNA2016 https'), 'grab') (('doesn t', '20'), 'guess') (('46Epoch 7', '10 loss'), 'complete') (('Data Science Bowl', 'data then tab'), 'choose') (('we', 'just them'), 'thank') (('locally it', 'Just it'), 'run') (('loading images python opencv Data net tutorial Section 2 so we', '150x150'), 'Processing') (('scans', 'just darker others'), 'see') (('We', 'anything'), 'want') (('WHICH', 'https MUCH couple great 1 www'), 'change') (('that', 'mine'), 'tutorial') (('that', 'this'), 'know') (('6Epoch 3', '10 loss'), 'complete') (('we', 'https too 1 www'), 'expect') (('I', '1 10 loss'), 'complete') (('same slices', 'images'), 'be') (('We', 'just simply enough data'), 'have') (('best path', 'really what'), 'be') (('One major issue', 'data'), 'be') (('again mainly anyone', 'kernel environment'), 'have') (('it', 'case'), 'can') (('stage 2 where you', 'stages'), 'be') (('you', 'time'), 'submit') (('now this', 'even typical computer'), 'be') (('else you', 'public'), 'about') (('likely you', 'data'), 'be') (('you', 'here what'), 'be') (('that', 'data'), 'get') (('com gzuidhof data science full preprocessing', '1 One immediate thing'), 'bowl') (('features X Y you', 'sample data basic use'), 'image') (('Now we', '3D convnet'), 're') (('63Epoch 2', '10 loss'), 'complete') (('you', '2 models'), 'submit') (('it', 'huge importance'), 'expect') (('we', 'things'), 'do') (('you', 'OpenCV tutorial'), 'do') (('we', 'heck'), 'be') (('54Epoch 8', '10 loss'), 'complete') (('data', 'neural network'), 'loop') (('that', '100 honest first 3D convnet'), 'be') (('we', 'already processing'), 'have') (('network neural code', 'ahead data'), 'prefer') (('soon immediately I', 'slices'), 'bowl') (('I so you', 'route'), 'find') (('how you', 'chunksThe evenly sized struggle'), '312443') (('s', 'opencv'), 'let') (('It', 'IT'), 's') (('only when we', 'it'), 'be') (('I', 'tutorial'), 're') (('it', 'cancer'), 'be') (('it', 'those'), 'make') (('6Epoch 5', '10 loss'), 'complete') (('7Epoch 4', '10 loss'), 'complete') (('you', 'input data'), 'be') (('we', 'depth'), 'okay') (('error', 'size expected multiple'), 'tutorial') (('you', 'output how exactly predictions'), 's') (('that', 'second stage'), 'release') (('whatever', 'when 3 dimensional rendering'), 'tutorial') (('more I', 'a few more'), 'have') (('neural just WAY too convolutional network', 'computing serious power'), 'be') (('even people', 'new kaggle'), 'be') (('significantly we', '2D obviously much windows'), 'note') (('that', 'scans'), 'do') (('we', 'classifier'), 'be') (('i', '0'), 'have') (('we', 'them'), 'link') (('that', 'model'), 'be') (('ourselves', 'evenly sized chunks'), '312443') (('model', 'that'), 's') ", "extra": "['patient', 'test', 'lung', 'lung cancer']"}