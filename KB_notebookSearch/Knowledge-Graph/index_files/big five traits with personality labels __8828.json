{"name": "big five traits with personality labels ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "Each personality label has very similar distributions of openness besides lively which has a distribution which seems a bit more concentrated at above average values. Perhaps if you limit yourself to the following labels you can achieve some sort of predictive capability by giving candidates Big 5 personality tests either a candidate is responsible or dependable OR he is not either of these. Still such an approach would not solve the problem of differently distributed training and test data so I have chosen not to show it here. This is likely due to an imbalance in the labels of both the training and the test set. Whereas our training set is a bit imbalanced with regards to its personality labels our test set is even more imbalanced e. I will draw a box and whiskers plot for each Big Five trait to roughly see the distribution of each trait within each of the personality label categories. First I ll give Gender and Personality numerical values Now I ll check for rows with null values Only one row with a null value in Gender. But before we jump to showing the predictive capabilites of a stacking metamodel let s get acquainted with the data. Let s relabel Y and Y_test accordingly and fit some new models These results are better than flipping a coin but still quite mediocre. As we already understand the relationship between the Big Five scores and the accompanying labels is very weak. train_metamodel uses get_oof_predictions to get all of the out of fold oof predictions for each model and then chooses the best hyperparameters for the metamodel that uses all of the oof predictions to predict the labels. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. Can we try to make some lemonade from this quite sour lemon Maybe. Let s take a look Indeed we can see an imbalance in both sets. get_oof_predictions trains a model on all of a training set that is split into some number of cross validation sets each time making predictions on the part of the training set that was not trained on. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. Alas it seems that our data is simply not terribly suitable for good predictions as we were afraid from our preliminary exploration of the data especially if it is divided into differently distributed training and test sets. Whereas the metamodel shows a slight improvement in prediction accuracy over even the best submodel when it comes to the training set which is to be expected it is surprisingly accurate when it comes to the test set this is even more surprising due to the fact that the training and the test sets are differently distributed. Still some limited predictions can ultimately be made. conscientiousness is distributed quite similarly within each personality label although the extraverted and responsible labels have more individuals with a below average rating. It is unclear how these labels were created to begin with and as we shall soon see these labels are at most weakly related to the Big Five ratings. csv together so that the data can be first preprocessed and explored as a whole although I m not going to assume that these two datasets are similarly distributed they aren t which will result in some strange outcomes later on. Still I ll define a few functions that will allow me to build a stacking metamodel and then see how well such a metamodel does on our test set a test set that has comes from a different distribution than our training set. I ll quickly impute it in a somewhat ad hoc manner. I ll start by defining the models and the parameter grids I want GridSearchCV to search through Now I ll define a function that will be able to return the best models and then use that function to get these best models and their scores What we have here is a pretty bad situation the best models we could find each have an accuracy of about 25 percent. I ll now go back to do some more exploration let s take a look at the correlation matrix of our data. The best that the metamodel learned to do was to predict 0 for almost every datapoint of the training set and the same behavior applied to the test set ends up with an even higher accuracy So while the test accuracy is high were we to measure the F score of our predictions which also takes into account precision and recall it wouldn t be terribly good at all. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session. Unlike agreeableness it seems that extraversion could be somewhat useful with regards to prediction. The given data is composed of features that represent age in years and the Big Five personality traits each datapoint has ratings for openness conscientiousness extroversion agreeableness and neuroticism and is labeled by overall personality labels that don t seem to be directly related to Big Five psychometrics each datapoint is labeled as either dependable extraverted lively responsible or serious. I ll use a square root transformation over a reflection of these columns At this point I will split the unified database back into the training and the test set and then scale them separately When we write machine learning models we very often start with the i. neuroticism looks a bit more useful for us in as much as it has somewhat different distributions within each personality label. Let s see if this assumption holds in our case by checking the distributions of personality labels in both our datasets As we can see the training and test sets given to us seem to come from very different distributions. This isn t a good situation it means that we shouldn t expect any model trained on our training set to do very well on our test set. 5 so it s probably a good idea to unskew these columns. agreeableness has basically identical distributions for each personality label and is thus unlikely to be very useful when making predictions. Before I do a bit more data exploration some preprocessing is required. Still let us press on For our models to work well we should make sure that our data isn t too skewed openness and agreeableness have a negative skew below 0. closer to the male numerical value. First I want to find the best hyperparameters for a number of different models. That is we might as well just choose our labels randomly and get results that are about as accurate So as our correlation heatmap has foretold it seems that our datapoints don t seem to have any clear relationship with their accompanying labels. read_csv Input data files are available in the read only. I ll start by concatenating train. label 4 serious makes up almost half of the labels in the test set while labels 0 and 2 each make up less than ten percent. This does not discount the possibility of non linear relationships that could allow a model to make good predictions but it s not an encouraging start. I ll therefore impute the missing value with 0. Still let s continue and see what we can do. Imagine you re an employer and want to hire people who are specifically responsible or dependable more than anything else. So from first glance some Big Five traits seem potentially useful for prediction on their own neuroticism extraversion some traits seem potentially quite useless on their own openness agreeableness while conscientiousness looks to be somewhere in between. I ll group our dataframe by Personality and Age and see what the mean age value is for serious 21 year olds The mean is closer to 0 i. This will allow us to see if there are any clear linear relationships between any of our values and more importantly between any of our features and the label. We can see that extraversion has only somewhat different distributions across the personality labels and a rough divide can be seen between some of the labels. Before I preprocess anything I ll do some exploration. Another possible approach to the data that I have not shown in this notebook would be to balance the given training data using an algorithm such as Synthetic Minority Oversampling Technique before fitting the models and metamodel. We already have a rough idea from our box and whiskers plots that no Big Five trait seems to have a strong relationship with any of the personality labels but looking at a correlation matrix may still be useful. Finally I will use predict_with_models to prepare all of the predictions that the metamodel requires to make a final prediction on the test set. Alas it s now clear that there aren t any strong linear relationships at play the strongest correlations with regards to our label are with Gender and neuroticism and even those are quite weak. In this notebook I will explore the data and see how well it can be used for prediction while relying on a stacked metamodel. d assumption that is we assume that our datapoints come from the same data generating process which also means that our training and test sets are assumed to be identically distributed. ", "id": "yonatanilan/big-five-traits-with-personality-labels", "size": "8828", "language": "python", "html_url": "https://www.kaggle.com/code/yonatanilan/big-five-traits-with-personality-labels", "git_url": "https://www.kaggle.com/code/yonatanilan/big-five-traits-with-personality-labels", "script": "choose_hyperparameters AdaBoostClassifier train_metamodel KNeighborsClassifier seaborn numpy relabel_personality get_oof_predictions sklearn.ensemble sklearn.model_selection KFold RandomForestClassifier label)' pandas LogisticRegression RobustScaler predict_with_models GridSearchCV sklearn.neighbors SVC sklearn.linear_model sklearn.preprocessing sklearn.svm ", "entities": "(('label', 'less than ten percent'), 'make') (('I', 'personality label categories'), 'draw') (('I', 'exploration'), 'preprocess') (('it', 'personality label'), 'look') (('s', 'data'), 'go') (('I', 'models'), 'be') (('model', 'test very well set'), 't') (('we', 'what'), 'let') (('data', '0'), 'let') (('it', 'good predictions'), 'discount') (('t', 'outside current session'), 'list') (('us', 'features'), 'allow') (('which', 'bit more average values'), 'have') (('that', 'training set'), 'train') (('we', 'quite sour lemon'), 'try') (('agreeableness', 'thus very when predictions'), 'have') (('First I', 'different models'), 'want') (('datapoints', 'accompanying labels'), 'is') (('mean', '0 i.'), 'will') (('test set', 'personality'), 'be') (('test so I', 'it'), 'solve') (('extraversion', 'prediction'), 'seem') (('each', 'about 25 percent'), 'start') (('t which', 'strange outcomes'), 'csv') (('results', 'coin'), 'let') (('that', 'training set'), 'define') (('rough divide', 'labels'), 'see') (('especially it', 'differently distributed training'), 'seem') (('training sets', 'very different distributions'), 'let') (('Indeed we', 'sets'), 'let') (('training', 'even more fact'), 'be') (('read_csv Input data files', 'read'), 'be') (('so it', 'probably good columns'), '5') (('Now I', 'Gender'), 'give') (('metamodel', 'test set'), 'use') (('already relationship', 'Big Five scores'), 'be') (('I', 'hoc somewhat ad manner'), 'impute') (('datapoint', 'Big directly Five psychometrics'), 'compose') (('that', 'labels'), 'use') (('This', 'training'), 'be') (('how well it', 'stacked metamodel'), 'explore') (('It', 'kaggle python Docker image https github'), 'come') (('extraverted labels', 'below average rating'), 'distribute') (('I', '0'), 'impute') (('preprocessing', 'bit more'), 'require') (('Big Five trait', 'correlation matrix'), 'have') (('also training sets', 'data generating same process'), 'assumption') (('who', 'specifically more anything'), 're') (('he', 'either these'), 'be') (('we', 'very often i.'), 'use') (('even those', 'Gender'), 's') (('conscientiousness', 'openness potentially quite own agreeableness'), 'seem') (('soon labels', 'Big Five ratings'), 'be') (('s', 'data'), 'let') (('which', 'it'), 'be') ", "extra": "['gender', 'outcome', 'test']"}