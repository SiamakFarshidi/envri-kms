{"name": "yaeda yet another eda ", "full_name": " h2 Overview h2 Packages h3 To do h2 To be continued ", "stargazers_count": 0, "forks_count": 0, "description": "So it seems that the vast majority of columns have 95 percent of zeros in them. This submission scored 1. This is IMHO overall a good thing although a lot of times there is some signal in the missing values that s valuable and worth exploring. We ll take a look at naturally the target variable. The memory size of the train dataset is fairly large 170 MB which is to be expected. So let s get started PackagesFirst let s load a few useful Python packages. We ll have to investigate this further. This is probably one of the main reasons why the metric that we are trying to optimize for this competition is RMSLE root mean square logarithmic error. This is going to be a very very interesting competition. That looks pretty sparse to me but let s see how much variation is there between different columns. We also see that the number of rows in the test set far surpasses the number of rows in the train set. It seems that for this competition we don t have to do any complicated combination and mergers of files. So we have the total of 4735 columns to work with. There are a LOT of features Almost 5000 And they outnumber the number of rows in the training set 4. As expected this distribution looks much more ahem normal. However as mentioned earlier most of these columns seem to be filled predominatly with zeros. Now let s see some basic descriptive statistics for the train and test dataframes. First let s make a histogram of its raw value. Now let s look at the test set. We ll start with a simple LighGBM regression and see if that yields any results. This section will keep growing in subsequent versions of this EDA. Now let s take a look at columns with constant value. Here we find all the relevant files for this competition. These constant columns are thus most likely an artifact of the way that the train and test sets were constructed and not necessarily irrelevant in their own right. Let s see how would that look on a plot. Well that s great we made a prediction on the test set and saved it to a file which we were able to submit to the competition. Now I m really curious about those. So let s subset the colums that we d use to just those that are not constant. Wow not very diverse at all Most of the values are heavily concentrated around 0. csv and sample_submission. Yes a very interesting competition indeed. 0 for 75 another indication that we are probably dealing with sparse data. The standard deviation for most features seems larger than the feature mean. Now let s take a closer look at the shape and content of the train data. If we treat all the train matrix values as if they belonged to a single row vector we see a huge amount of varience far exceeding the similar variance for the target variable. There doesn t appear to be any missing values in the train set. In fact there are fewer rows than columns which means we ll have to invest a lot of effort into feature selection feature engineering. Most features seem to have similarly wide spread of values as the target variable. We see a similar distribution of various statistical aggregates but by no means the same seems like there soem substantial distribution shifts between the train and test sets. Again we see that these distributions look similar but they are definitely not the same. And it looks like a fairly nice distribution albeit still fairly asymetrical. According to Pandas there are no int values in the test set. There are a few features such as d5308d8bc c330f1a67 that seem to be filled with zeros. Features seem sparse. Another way of looking at the same distribution is with the help of violinplot. This will probably be another major concern when it comes to feature selection engineering. First let s set our target variable to be the log of 1 target. This is yet another byproduct of having a very small dataset. Can the violin plot help Not really the plot looks nicer but the overall shape is pretty much the same. Let s do the same thing with the test data. We want to get a better numerical grasp of the true extent of zeros. Now let s do some plotting. Now let s import and take a glimpse at these files. We ll have to investigate this later and possibly do some reverse engineering. We see that the input folder only contains three files train. 53 on Public Leaderboard. So this is interesting there are 256 constant columns in the train set but none in the test set. Now let us look at the input folder. This is a highly skewed distribution so let s try to re plot it with with log transform of the target. Pandas is treating 1845 features as float and 3147 as integer. For most problems it would be useful to take a look at the description of these columns but in this competition they are anonymized and thus would not yield any useful information. Let s take a look at the statistics of the Log 1 target We see that the statistical properties of teh Log 1 Target distribution are much more amenable. Only marginal improvement there is a verly small bump close to 15. A few things to notice 1. As advertised features are numeric and anonymized. So as we suspected almost 97 of all values in the train dataframe are zeros. Unfortunately there was no way to tell how this model would perform on the unseen data. Maybe if we used the log plot things would be better. There are less than 5000 training rows. OverviewThe purpose of this kernel is to take a look at the data come up with some insights and attempt to create a predictive model or two. It is possible that some of those int features are one hot encoded or label encoded categorical variables. Sanity check is always a good thing and at leas at this level hte Kaggle people did not mess things up. Now let s plot it to see how diverse the numerical values are. OK that s much more interesting. Now let s take a look at the test dataset. Here we see that the number of features in the test set 4992 matches the number in the train set. A few things immediately stand out 1. Most features have 0. OK let s take a look at the distribution of non zero values. OK let s try to do some modeling. Let s try to get a better sense of this data. These will need to be eliminated. Do feature importance analysis To be continued. There also doesn t seem to be any missing values in the test set. Target variable ranges over 4 orders of magnitude. ", "id": "tunguz/yaeda-yet-another-eda", "size": "6658", "language": "python", "html_url": "https://www.kaggle.com/code/tunguz/yaeda-yet-another-eda", "git_url": "https://www.kaggle.com/code/tunguz/yaeda-yet-another-eda", "script": "sklearn.metrics sklearn.model_selection KFold mean_squared_error seaborn numpy matplotlib.pyplot describe lightgbm pandas scipy.stats xgboost ", "entities": "(('root', 'square logarithmic error'), 'mean') (('probably major when it', 'feature selection engineering'), 'be') (('vast majority', 'them'), 'seem') (('Unfortunately how model', 'unseen data'), 'be') (('Now s', 'test dataset'), 'let') (('how much variation', 'there different columns'), 'look') (('target', 'values'), 'seem') (('So we', '4735 columns'), 'have') (('we', 'target variable'), 'see') (('s', 'non zero values'), 'let') (('way', 'violinplot'), 'be') (('things', 'log plot'), 'be') (('they', 'thus useful information'), 'be') (('that', 'missing values'), 'be') (('t', 'test missing set'), 'seem') (('that', 'zeros'), 'be') (('s', 'Python a few useful packages'), 'let') (('we', 'files'), 'seem') (('almost 97', 'train dataframe'), 'be') (('purpose', 'predictive model'), 'be') (('We', 'target naturally variable'), 'take') (('s', 'test data'), 'let') (('that', 'results'), 'start') (('it', 'fairly nice distribution'), 'look') (('However earlier most', 'predominatly zeros'), 'seem') (('how numerical values', 'it'), 'let') (('Now s', 'files'), 'let') (('We', 'later possibly reverse engineering'), 'have') (('So this', 'test set'), 'be') (('we', 'probably sparse data'), '0') (('Here we', 'competition'), 'find') (('feature', 'most features'), 'seem') (('also number', 'train set'), 'see') (('statistical properties', 'Target 1 distribution'), 'let') (('Target variable', 'magnitude'), 'range') (('First s', '1 target'), 'let') (('that train sets', 'necessarily own right'), 'be') (('Now s', 'train dataframes'), 'let') (('doesn There t', 'train missing set'), 'appear') (('We', 'train sets'), 'see') (('we', 'competition'), 's') (('input folder', 'files only three train'), 'see') (('fairly large 170 which', 'train dataset'), 'be') (('Now s', 'train data'), 'let') (('we', 'feature selection feature engineering'), 'be') (('highly skewed so s', 'target'), 'let') (('We', 'zeros'), 'want') (('number', 'train set'), 'see') (('This', 'yet very small dataset'), 'be') (('First s', 'raw value'), 'let') (('that', 'just those'), 'let') (('Almost they', 'training'), 'be') (('hte Kaggle people', 'things'), 'be') (('s', 'data'), 'let') (('section', 'EDA'), 'keep') (('some', 'one hot categorical variables'), 'be') (('Now s', 'constant value'), 'let') (('Pandas', 'integer'), 'treat') ", "extra": "['test']"}