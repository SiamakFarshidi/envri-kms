{"name": "quantile reg lr schedulers checkpoints ", "full_name": " h1 Quick intro h2 Competition description h3 What should I expect the data format to be what am I predicting h1 Update history h2 New Notebook for CV backed Random Grid Hyperparameter Search click here h2 Domain knowledge h3 Some domain knowledge can be gained from watching the following video and from reading here h2 Load all dependencies you need h1 First glimpse at the data h1 Data Wrangling h2 Getting the format right h2 Preparing the data for the Neural Network h3 The first apporach is using sklearn as it is super famous and used frequently h3 The 2nd approach is doing all the legwork ourselfs h1 Model Loss h1 CONFIG Section h3 Loss Function h2 Neural Network Model h3 Normalization Regularization h3 Activation function h3 What about those mysterious quantiles q1 and quantiles adjustment q adjust layers h1 Training h1 Evaluation submission h3 Check loss and accuracy h3 OOF Evaluation h2 Thanks a lot for reading I hope you could gain as much insights from reading this as I got from writing it ", "stargazers_count": 0, "forks_count": 0, "description": "Here is an interesting post about it for those who want to learn more Apply vs transform. Current methods make fibrotic lung diseases difficult to treat even with access to a chest CT scan. Learning Rate start endpoint Custom Logging Callback Checkpoint Saving CallbackThe Learning Rate scheduler below is inspired by Chris great Melanoma detection notebook https www. Not the data has the format we need to work with. Then our output preds are q1 tf. Ever wondered why lr_max is scaled by BATCH_SIZE and therefore bigger for larger batches The reason for this is the following the larger the BATCH_SIZE the more averaged smoothened a step of gradient decent is and the bigger our confidence in the direction of the step is. com c osic pulmonary fibrosis progression discussion 168469. com rftexas osic eda leak free kfold cv lgb baseline kln 440 Please note that we still don t use propoer stratification based on Age Sex SmokingStatus. If successful patients and their families would better understand their prognosis when they are first diagnosed with this incurable lung disease. But we used OneHot Encoding. Okay wow that was not as easy as I expected. If you don t do that you need to be careful with some steps e. Does anybody have an idea on HOW to improve this The 2nd approach is doing all the legwork ourselfs. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. Let s consider two arrays vectors q1 a b c q_adjust e f g. Preparing the data for the Neural Network The first apporach is using sklearn as it is super famous and used frequently. MinMax Scaling in test_df will not have the same range of values e. com questions 27517425 apply vs transform on a group object It still wasn t perfect and I found a third super clean approach here https www. Data Wrangling Getting the format rightIn this section we are going to do all the Data Wrangling and pre processing. In this competition you ll predict a patient s severity of decline in lung function based on a CT scan of their lungs. So I looked up how other people solved it and I found a rough equivalent to the following function The second apporach is using transform which is not as known as apply but faster for basic operations not involving multiple columns of a dataframe. Then we merge our info from the test data to the submission_df that s the fastest way of getting the correct format for predictions and submissions later on. Okay now we encoded all the data and want to have a look at it. Model LossIn this section we are going to define the loss a first model. Additionally we gain more robustness for the choosing of the hyperparameter learning rate. com c osic pulmonary fibrosis progression discussion 165727 Load all dependencies you need from coffee import Let s start seeding everything to make results somewhat reproducible. But now check our model s result and evaluate it OOF EvaluationIn the next section we are going to use the train_preds to calculate the optimized sigma which is a measure for certainty or rather uncertainty. For this we are going to define some functions and transformations which then are applied to the data. com cdeotte triple stratified kfold with tfrecords. mean val_score limit y values for beter zoom scale get rid of unused data and show some non empty data. Theoretically we could also use only q1 and add tf. Note that the BASELINE FVC it not the minimum FVC but the first measurement meaning the measurement taken in the min_week or baselined_week 0. As we can clearly see some patients took the first measure of FVC before and some after their baseline CT images. This class needs a fit and a transform method so that the ColumnTransformer itself can use fit_transform like for the numerical and categorical attributes. That s where a troubling disease becomes frightening for the patient outcomes can range from long term stability to rapid deterioration but doctors aren t easily able to tell where an individual may fall on that spectrum. For getting the baselined FVC I first wrote the following straightforward function This apporach works fine but as it contains a lot of look ups its slow and didn t feel right. Feel free to experiment with the scheduler and it s max min and decay values. The idea is to avoid having the same patient PatientID in training and in validation Data as this might lead to evaluate a higher CV score for a model which is luckily learning memorizing the data for a particular patientID which is also frequently occuring in the validation data. com chrisden 6 82x cv backed hyperp gridsearch quantile reg V39 Optimized Wording Hyperparameters reduced NFOLDs V33 Improved loggingV30 31 enhanced model explanations q1 and q_adjust layers and outputs. V23 Minor optimizations wording hyperparametersV19 Introduced plotting evaluation of results V18 Introduced usage of tensorflow. We subtract the lower quartile from the upper quartile defined in the loss function and average it. png Long before we hit the 10th epoch the validation loss is increasing again while the training loss keeps decreasing. V24 29 Minor adoptions testing multiple hyperparameters correcting MONITORING during training now correctly monitoring val_score instead of score which improves OOF score. 5 is the best possible score plt. test_df only contains male Ex smokers. Cited from page 3 Empirically we find that the ability to grow the norm v makes optimization of neural networks with weight normalization very robust to the value of the learning rate If the learning rate is too large the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached. The good thing is we dont need a NoTransformer here as we simply can work in the DataFrame itself and not change any data which we want to preserve. For the architecture It s good practice to use numbers of units following the schema 2 x with x element of N resulting in 1 2 4 8 16 32 64 128. We need to get this into a format which we can easily use for making our predictions so let s split up the Patient_Week column into a Patient and a Weeks column to align it with the train test data format. The categorical features might have different categories in test_df than in train_df e. First glimpse at the dataTo get an idea of the meaning of the weeks column let s check some of their values. The first big challenge is data wrangling We could see that some patients take FVE measurements only after their baseline CT Images and some took measurements before that. 07868 from tensorflow_addons to support faster convergence. Produces negative outputs which helps the network nudge weights and biases in the right directions for negative inputs too. It s good practice to concatinate all tabular data train test submission to ensure all data get s the same correct treatment. DataFrame data columns column_names function for which we need the column names. Standardization or Normalization e. Downside if you want to replace the MinMaxScaler with another scaling method RobustScaler StdScaler you need to implement it first. com ulrich07 great notebook https www. com ulrich07 osic multiple quantile regression starter which also inspired me to change my loss to the above coded version. https mlfromscratch. As there is less randomness in a huge averaged batch compared with for example Stochastic Gradient Decent SGD with batch size 1 and our confidence in the direction is higher the learning rate can be bigger to advance fast to the optimum. To get a ColumnTransformer which outputs the whole DataFrame compatible format we can create a class that takes attributes that we don t want to change and simply passes them through. Those plots can tell us whether our model training is working as expected or if it strongly overfits. Finally patients suffer extreme anxiety in addition to fibrosis related symptoms from the disease s opaque path of progression. Loss Function Neural Network ModelIn this section we build an initial neural Network. Quick intro Competition descriptionImagine one day your breathing became consistently labored and shallow. Ignore future warnings and Data Frame Slicing warnings. We can also clearly ovserve that there is no improvement in our accuracy anymore. As you can see in the loss function and in the neural network output we expect 3 values one value for each of the defined quantiles. And we also should not get things wrong and mix the columns up. Make sure to not call it MinMaxScaler and shadow the already important MinMaxScaler from Sklearn Okay so the second apporach using our own implementation was more straightforward and less code. png The global minimum of this function is achieved for delta 0 and sigma 70 which results in a loss of roughly 4. The code of this section is derived from Ulrich s https www. The following code is getting our data back to a Dataframe and preserving the correct order. We are trying to minimize the following image. cumsum q_adjust a e b e f c e f g Second We can see q_adjust as a baseline which is added to the q1 layers. What can we do against strongly overfitting models We could do the following Collect more training data or use augmentation to generate new data Sadly I have no brilliant idea on how to do this for this specific Kaggle competition. Then the patient took the next FVC measurement 9 weeks later. What should I expect the data format to be what am I predicting Each row in the dataset contains a Patiend_ID and a week you must predict the FVC and a confidence. The authors describe the method like this Weight normalization a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. Thanks a lot for reading I hope you could gain as much insights from reading this as I got from writing it. get column names for non categorical data extract possible values from the fitted transformer create Dataframe based on the extracted Column Names get back original data split be careful the resulsts are very SEED DEPENDEND you can exclude and include features by extending this feature list 0. In the next section we go trough two possibilities on how to normalize standardize and prepare the data for the neural Network. So let s concatinate all our data first and then start with the transformations. In the next step we need to baseline the FVC values. The downside is it s not intuitive at least not for me and takes quite some lines of code. The final submission file should contain a header and have the following format Patient_Week FVC ConfidenceID00002637202176704235138_1 2000 100ID00002637202176704235138_2 2000 100ID00002637202176704235138_3 2000 100 Update history New Notebook for CV backed Random Grid Hyperparameter Search click here https www. But wait we only get back a series element How to put that in a Dataframe again Sadly not as easy as I had hoped. Months later you were finally diagnosed with pulmonary fibrosis a disorder with no known cause and no known cure created by scarring of the lungs. For more info you can read here. Normalization RegularizationWe are going to use weight normalization link to the paper click here https arxiv. Currently the way to go seems to be pinball loss. the relative number of weeks pre post the baseline CT may be negative We can eighter drop all except the first or last duplicate or average them. Cons Introduces longer computation time because of the exponential operation included. iterrows is roughly 8 times slower than using for idx in df. Activation functionAs the given task without the usage of images is not very compute intensive you don t need a GPU CPU will do we will change the activation function from relu to elu. com activation functions explained elu below you can find a short summary Pros Avoids the dead ReLu problem ReLus provides activation values gradients of 0 for negative input values Produces activations for negative inputs instead of letting them be zero when calculating the gradient. CHECK FOR DUPLICATES DEAL WITH THEM keep False All duplicates will be shown split Patient_Week Column and re arrage columns introduce a column to indicate the source train test for the data make a copy to not change original df ensure all Weeks values are INT and not accidentaly saved as string as test data is containing all weeks copy the DF to not in place change the original one get only the rows containing the baseline min_weeks and therefore the baseline FVC fill the df with the baseline FVC values same as above add a row which contains the cumulated sum of rows for each patient drop all except the first row for each patient unique rows containing the min_week merge the rows containing the base_FVC on the original _df import the necessary Encoders Transformers define which attributes shall not be transformed are numeric or categorical create an instance of the ColumnTransformer the No Transformer does not change the data and is applied to all no_transform_attribs Apply MinMax to the numerical attributes here you can change to e. Does not avoid the exploding gradient problem. com c osic pulmonary fibrosis progression discussion 179033 I wanted to know how much this speeds up the processing you can find the results in the following Clearly the third approach works best and I have learned a very clean way of processing data. Downside is we need to implement the MinMaxScaler by hand. CONFIG Section In this section you can configure the following Features used for training Basic training setup BATCH_SIZE and EPOCHS Configuration for the loss function Optimizers Learning Rate Schedulers incl. First we are taking care of the loss. There are 2 reasons for the q_adjust layer First With adding the cumulative sum on top of q1 we ensure or at least very strongly support that the output preds are in increasing order. If that happened to you you would want to know your prognosis. It adds additional degrees of freedom more neurons more trainable weights which provides better results. What about those mysterious quantiles q1 and quantiles_adjustment q_adjust layers The idea of the q1 layer is to actually predict the 3 quantiles. Remember that roughly 4. So let s first find out what the actual baseline week and baseline FVC for each Patient is. StdScaler OneHotEncoder all categorical attributes. Open Source Imaging Consortium OSIC is a not for profit co operative effort between academia industry and philanthropy. min max values and therefore scaling than in train_df. Btw there is an even worse approach Using for row in df. The group enables rapid advances in the fight against Idiopathic Pulmonary Fibrosis IPF fibrosing interstitial lung diseases ILDs and other respiratory diseases including emphysematous conditions. Getting our model to predict the Confidence and FVC values which is what we need is not working fine so far as you can read here https www. cumsum q1 then we would guarantee that we have an increasing order but we have less degrees of freedom and the OOF Score and LB score is worse. com c osic pulmonary fibrosis progression discussion 167764. Please support the original Notebook creators The chosen quartiles are simply derived by testing using 0. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. 8 default choose ADAM or SGD higher batch size higher lr 30 of all epochs are used for ramping up the LR and then declining starts how many epochs shall L_RMAX be sustained rate of decay get the Keras required callback with our LR for training plot check the LR Scheulder for sanity check logging saving defining custom callbacks defining a class variable which is used in the following to ensure LogPrinting evaluation saving is consitent choose between val_score and val_loss get index of best epoch get score in best epoch create constants for the loss function define competition metric Python is automatically broadcasting y_true with shape 1 0 to shape 3 0 in order to make this subtraction work define pinball loss IMPORTANT define quartiles feel free to change here combine competition metric and pinball loss to a joint loss function instantiate optimizer create model predicting the 3 quantiles generating another output for quantile adjusting the quantile predictions adding the tf. 75 leads to worse results. We can do that as we have both the model s estimate and the real data. Okay we made it Let s finally check our stats and submit it In the following last step we overwrite our predictions with the known data from the orginal submission file to not waste known data. https stackoverflow. As there are only a few duplicates we can drop them without a bad conciousness for loosing to much data for our first apporach. To avoid potential leakage in the timing of follow up visits you are asked to predict every patient s FVC measurement for every possible week. Improved severity detection would also positively impact treatment trial design and accelerate the clinical development of novel treatments. You ll determine lung function based on output from a spirometer which measures the volume of air inhaled and exhaled. We are going to use dropout for regularization and not a too broad and deep network as the training data is very limited. The idea on how to do that is coming from PAB97 Pierre s great notebook CHECK IT OUT https www. Your help and data science may be able to aid in this prediction which would dramatically help both patients and clinicians. In addition the wide range of varied prognoses create issues organizing clinical trials. The upside is it s easy to add some additional features and pump them through the Pipeline or to change the Pipeline itself e. Its mission is to bring together radiologists clinicians and computational scientists from around the world to improve imaging based treatments. We start with the baseline week What we can see here is that the Patient with ID ending on 430 had his first FVC measure 4 weeks before the first baseline CT images Weeks column 4 were taken. As I learned in a comment from frederiklaubisch sklearns ColumnTransformer has a remainder parameter that can be set to passtrough which would eliminate the need for the NoTranformer. exchanging MiNmaxScaler with StdScaler RobustScaler etc. png attachment image. We need to use pd. TrainingIn the following we want to create leak free folds to get a robust cross validation strategy in order to evaluate all our models our training. The challenge is to use machine learning techniques to make a prediction with the image metadata and baseline FVC as input. Those weeks which are not in the final three visits are ignored in scoring. cumsum of q_adjust to the output q1 to ensure increasing values a a a b a b c create neural Network get target value get training test data instantiate target arrays extract Patient IDs for ensuring callbacks logging model saving with checkpoints each fold callbacks get_lr_callback BATCH_SIZE un comment for using LRScheduler build and train model evaluate append OOF evaluation to calculate OFF_Score predict on test set and average the predictions over all folds fetch results from history create subplots limit y values for better zoom scale. addons tfa WeightNormalization V16 Code optimizations https www. The below EXAMPLE IMAGE is an example of a strongly overfitting model image. Anyway in keras it is quite hard to get 100 reproducible results. com c osic pulmonary fibrosis progression discussion 179033 see third approach in the Data Wrangling section V13 14 small corrections edited some links V12 Corrected bug for saving models correctly see comment section enhanced readabilty V11 Introduced model checkpoints saving V10 Introduced configurable Learning Rate Schedulers V8 9 Introduced GroupKFolds to get leak free cross validation strategy to evaluate models training Domain knowledge Some domain knowledge can be gained from watching the following video and from reading here. If for the column SmokingStatus we only have the values Smoker and Never Smoked we have two resulting columns if there are additional possible values like Ex_Smoker we get more columns. Reduce the network s size width andor dept by removing layers or reducing the number of neurons in the hidden layers Use regularization like LASSO Least Absolute Shrinkage and Selection Operator aka L1 regularization or Ridge aka L2 regularization which results in adding a cost term to the loss function Use higher dropout rate in the Dropout Layers which will randomly remove more connections by setting them to zero and forcing the network to generalize better avoid relying on a limitied number of strong influence neurons. Evaluation submission Check loss and accuracyOkay we made it Let s evaluate our model check our stats Out Of Fold log loss and submit it Let s start with some plots. So the number of columns now depends on how many different values categories a categorical value has because for each unique value we get a separate column e. ", "id": "ChristianDenich/quantile-reg-lr-schedulers-checkpoints", "size": "19221", "language": "python", "html_url": "https://www.kaggle.com/code/ChristianDenich/quantile-reg-lr-schedulers-checkpoints", "git_url": "https://www.kaggle.com/code/ChristianDenich/quantile-reg-lr-schedulers-checkpoints", "script": "sklearn.metrics get_model TransformerMixin qloss tensorflow.keras.layers NoTransformer(BaseEstimator MinMaxScaler on_epoch_end mloss TransformerMixin) numpy get_baseline_FVC_old own_OneHotColumnCreator Back get_checkpoint_saver_callback GroupKFold get_baseline_FVC_new sklearn.base PIL loss get_baseline_week sklearn.model_selection sklearn.compose seed_everything timeit KFold lr_scheduler tensorflow_addons Image matplotlib.pyplot tensorflow on_train_end pandas tensorflow.keras.backend OneHotEncoder get_lr_callback StandardScaler fit tqdm RobustScaler baseline_FVC_new score Style baseline_FVC on_train_begin mean_absolute_error colorama own_MinMaxColumnScaler ColumnTransformer StratifiedKFold get_baseline_FVC sklearn.preprocessing BaseEstimator old_baseline_FVC LogPrintingCallback(tf.keras.callbacks.Callback) Fore transform IPython.display HTML ", "entities": "(('training again loss', 'decreasing'), 'png') (('challenge', 'input'), 'be') (('which', 'scoring'), 'ignore') (('EXAMPLE below IMAGE', 'model strongly overfitting image'), 'be') (('results', 'everything'), 'Load') (('We', 'it'), 'subtract') (('learning quickly appropriate effective rate', 'too large unnormalized weights'), 'find') (('fine so far you', 'what'), 'work') (('4', 'Weeks column'), 'be') (('we', 'defined quantiles'), 'expect') (('best I', 'data'), 'discussion') (('which', 'NoTranformer'), 'have') (('BASELINE FVC', 'min_week'), 'note') (('arrays vectors two q1', 'e f'), 'let') (('we', 'FVC values'), 'need') (('chosen quartiles', '0'), 'support') (('which', 'influence strong neurons'), 'avoid') (('We', 'also clearly accuracy'), 'ovserve') (('we', 'freedom'), 'guarantee') (('we', 'training'), 'trainingin') (('when they', 'lung first incurable disease'), 'understand') (('Loss Function Neural Network we', 'initial neural Network'), 'ModelIn') (('440 we', 'Age Sex SmokingStatus'), 'cv') (('so second apporach', 'own implementation'), 'make') (('we', 'estimate'), 'do') (('Learning Rate', 'Custom Logging Callback Checkpoint Saving CallbackThe Learning Rate Melanoma detection notebook https scheduler below Chris great www'), 'start') (('it', 'sklearn'), 'prepare') (('Additionally we', 'hyperparameter learning rate'), 'gain') (('you', 'lungs'), 'predict') (('we', 'column names'), 'function') (('which', 'then data'), 'go') (('Normalization RegularizationWe', 'paper click'), 'go') (('here you', 'e.'), 'CHECK') (('we', 'neural Network'), 'go') (('that', 'predictions'), 'merge') (('lung fibrotic diseases', 'chest CT scan'), 'make') (('again Sadly as I', 'Dataframe'), 'wait') (('We', 'following image'), 'try') (('which', 'validation also frequently data'), 'be') (('which', 'q1 layers'), 'cumsum') (('more neurons more trainable which', 'better results'), 'add') (('easily where individual', 'spectrum'), 's') (('we', 'don simply them'), 'pass') (('which', 'air'), 'determine') (('the bigger confidence', 'step'), 'wonder') (('s', 'values'), 'get') (('categorical features', 'e.'), 'have') (('Scaling', 'values e.'), 'minmax') (('which', 'dataframe'), 'look') (('Optimized Wording V39 Hyperparameters', 'model explanations 31 enhanced q1'), 'chrisden') (('0 70 which', 'roughly 4'), 'png') (('output at least very strongly preds', 'increasing order'), 'be') (('it', 'code'), 'be') (('instead them', 'when gradient'), 'explain') (('So s', 'first then transformations'), 'let') (('also things', 'columns'), 'get') (('which', 'above coded version'), 'com') (('Months later you', 'lungs'), 'diagnose') (('CT some', 'that'), 'be') (('reparameterization', 'minibatch'), 'inspire') (('mission', 'based treatments'), 'be') (('so s', 'train test data format'), 'need') (('group', 'other respiratory emphysematous conditions'), 'enable') (('that', 'direction'), 'describe') (('we', 'gradient stochastic descent'), 'improve') (('data', 'data train test good tabular submission'), 's') (('you', 'FVC'), 'expect') (('It', '2 16'), 's') (('FVC Update history New 2000 2000 100 Notebook', 'Random Grid Hyperparameter Search click'), 'contain') (('profit', 'academia industry'), 'be') (('RobustScaler you', 'it'), 'downside') (('Model section we', 'first model'), 'LossIn') (('we', 'known data'), 'make') (('slow t', 'look'), 'write') (('clearly patients', 'baseline CT before images'), 'see') (('code', 'https www'), 'derive') (('you', 'steps e.'), 'do') (('Finally patients', 'progression'), 'suffer') (('now we', 'it'), 'encode') (('you', 'possible week'), 'ask') (('wide range', 'clinical trials'), 'create') (('it', 'quite 100 reproducible results'), 'be') (('iterrows', 'df'), 'be') (('So s', 'Patient'), 'let') (('SEED very you', 'feature list'), 'get') (('Optimizers Learning Rate Schedulers', 'EPOCHS loss function'), 'Section') (('we', 'hand'), 'be') (('we', 'format'), 'have') (('which', 'dramatically patients'), 'be') (('q_adjust idea', 'actually 3 quantiles'), 'about') (('following code', 'correct order'), 'get') (('which', 'OOF score'), 'V24') (('you', 'prognosis'), 'want') (('I', 'third super clean approach'), 'apply') (('which', 'certainty uncertainty'), 'check') (('learning rate', 'fast optimum'), 'be') (('it', 'Pipeline'), 'be') (('y values', 'non empty data'), 'mean') (('it', 'us'), 'tell') (('Then patient', 'FVC next measurement'), 'take') (('We', 'them'), 'post') (('training too broad data', 'regularization'), 'go') (('2nd approach', 'legwork ourselfs'), 'have') (('fetch results', 'zoom better scale'), 'cumsum') (('we', 'column separate e.'), 'depend') (('domain knowledge', 'reading'), 'see') (('we', 'Data Wrangling'), 'get') (('we', 'more columns'), 'have') (('severity Improved detection', 'novel treatments'), 'impact') (('pinball loss IMPORTANT define quartiles', 'tf'), 'choose') (('we', 'which'), 'be') (('how that', 'PAB97 great notebook'), 'idea') (('only a few we', 'first apporach'), 'be') (('which', 'negative inputs'), 'produce') (('it', 'scheduler'), 'feel') (('Theoretically we', 'tf'), 'use') (('who', 'transform'), 'be') (('we', 'elu'), 'be') (('s', 'plots'), 'loss') (('transform ColumnTransformer', 'numerical attributes'), 'need') (('Sadly I', 'Kaggle specific competition'), 'do') (('I', 'it'), 'thank') ", "extra": "['disease', 'outcome', 'patient', 'test', 'lung']"}