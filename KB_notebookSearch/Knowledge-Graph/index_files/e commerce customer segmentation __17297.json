{"name": "e commerce customer segmentation ", "full_name": " h1 Understanding the content of Data h3 This is a transnational data set which contains all the transactions occurring between 01 12 2010 and 09 12 2011 for a UK based and registered non store online retail The company mainly sells unique all occasion gifts Many customers of the company are wholesalers h1 1 Data Preparation h1 Null Values h1 Duplicate Entries h1 2 Exploring the Contents of the Variables h1 Visualizing the Countries from where the orders are getting placed and cancelled h1 Observation h3 We have a lot of variety to cover h3 From the above table there are some observations which we can figure out h1 3 Intuitions about the categories of the product the customer buys h4 Product Description h4 Data Encoding h1 Creating the clusters of product h1 WordCLoud foe each cluster h1 4 Categories of the Customer that buys the product h1 5 Classifying customers h3 Support Vector Machine h1 Confusion Matrix h1 Learning Curves h1 Logistic Regression h1 k Nearest neighbors h1 Random Forest h1 Adaboost Classifier h1 Gradient Boosting ", "stargazers_count": 0, "forks_count": 0, "description": "Therefore I discard these words from the analysis that follows and also I decide to consider only the words that appear more than 13 times. It remains to understand the habits of the customers in each cluster. In practice I decide to delete all of these entries which count respectively for 1. Categorical the name of the country where each customer resides. As for now let us see the first 5 columns of the dataframe. However while examinating the content of the list I noticed that some names are useless. Duplicate Entries Let us check for the duplicate entries of the DataFrame 2. Exploring the Contents of the Variables InvoiceNo Invoice number. In order to ensure a good classification at every run of the notebook I iterate untill we obtain the best possible silhouette score which is in the present case around 0. Null Values From the first table we can say that in the CustomerId column about 25 of the entire column are of Null values. WordCLoud foe each clusterPrinciple Component AnalysisWe see that the number of components required to explain the data is extremely important we need more than 100 components to explain 90 of the variance of the data. In order to use it I create an instance of the Class_Fit class and then callgrid_search. Later one of the objectives will be to develop a model capable of characterizing and anticipating the habits of the customers visiting the site and this from their first visit. More generally we see that there is always a representation in which two clusters will appear to be distinct. To do this I define a class to create Radar Charts This allows to have a global view of the content of each cluster It can be seen for example that the first 5 clusters correspond to a strong preponderance of purchases in a particular category of products. I decide to collect the information related to a particular order and put in in a single entry. 2 of the dataframe entries. InvoiceDate Invice Date and time. I choose the number of clusters based on the silhouette score and I find that the best score is obtained with 11 clusters At first I look at the number of customers in each cluster There is a certain disparity in the sizes of different groups that have been created. Each entry of the dataframe indicates prices for a single kind of product. First in order to simplify their use I define a class that allows to interface several of the functionalities common to these different classifiers Since the goal is to define the class to which a client belongs and this as soon as its first visit I only keep the variables that describe the content of the basket and do not take into account the variables related to the frequency of visits or variations of the basket price over time Finally I split the dataset in train and test sets Support Vector MachineThe first classifier I use is the SVC classifier. This function takes as input the dataframe and analyzes the content of the Description column by performing the following operations extract the names proper common appearing in the products description for each name I extract the root of the word and aggregate the set of names associated with this particular root count the number of times each root appears in the dataframe when several words are listed for the same root I consider that the keyword associated with this root is the shortest name this systematically selects the singular when there are singular plural variants The first step of the analysis is to retrieve the list of products Once this list is created I use the function which was previously defined in order to analyze the description of the various products The execution of this function returns three variables keywords the list of extracted keywords keywords_roots a dictionary where the keys are the keywords roots and the values are the lists of words associated with those roots count_keywords dictionary listing the number of times every word is usedAt this point I convert the count_keywords dictionary into a list to sort the keywords according to their occurences Now we will create a representation of the most frequently repeating wordsThe list that was obtained contains more than 1400 keywords and the most frequent ones appear in more than 200 products. In practice before creating these clusters it is interesting to define a base of smaller dimension allowing to describe the scaled_matrix matrix. If various counter parts exist we remove the last counter part. To do so I start by adding to the selected_customers dataframe a variable that defines the cluster to which each client belongs Then I average the contents of this dataframe by first selecting the different groups of clients. The quantity is indicated as the negative of the quantites that were placed earlier making the net summation to zero. So let us move forward. One of the objectives may be for example to target these customers in order to retain them. Else if if there existys atleast one counter part add it to the doubtful_entry. UnitPrice Unit price. I therefore perform a PCA beforehand and then I represent the amount of variance explained by each of the componentsAt this point I define clusters of clients from the standardized matrix that was defined earlier and using the k means algorithm fromscikit learn. Visualizing the Countries from where the orders are getting placed and cancelled. The objective is to make this classification possible at the first visit. Analyzing the content of the E Commerce database that contains the purchases made by approximate 4000 customers over a period of one year2. e the 2nd entry consists fo the InvoiceNo. Let us see what will happen if we remove the Description of Category Again the hypothesis is not fulfilled. To fulfill this objective I will test several classifiers implemented in scikit learn. Categorical a 5 digit integral number uniquely assigned to each customer. In part I find that this type of customer represents 1 3 of the customers listed The dataframe transactions_per_user contains a summary of all the commands that were made. Categories of the Customer that buys the productIn the previous section the different products were grouped in five clusters. Now let us see that the orders which are getting placed or cancelled belongs to which countries. Quantity The quantities of each product item per transaction. In practice I decide to keep only a limited number of components since this decomposition is only performed to visualize the data 4. To develop a model that allows to anticipate or predict the purchases that will be made by a new customer during the following this year based on the data available from the previous purchases. In this section I intend to use the content of this latter variable in order to group the products into different categories. I also determine the number of clients in each group variable size Finally I re organize the content of the dataframe by ordering the different clusters first in relation to the amount wpsent in each product category and then according to the total amount spent Customers morphologyFinally I created a representation of the different morphotypes. Hence I add 6 extra columns to this matrix where I indicate the price range of the products And to choose the appropriate ranges I check the number of products in the different groups Creating the clusters of product In order to define approximately the number of clusters that best represents the data I use the silhouette score In practice the scores obtained above can be considered equivalent since depending on the run scores of 0. In this case I will use this base in order to create a representation of the different clusters and thus verify the quality of the separation of the different groups. Data PreparationIn the first step we will load the Data and the main objective would be to find the null values related to the different coumns and then deciding whether to replace those None values or to drop those null values depending upon the percentage proportion of those null values with respect to the entire column. Intuitions about the categories of the product the customer buys4. This would be of a great deal to the E Commerce retailer. In order to be able to test the model in a realistic way I split the data set by retaining the first 10 months to develop the model and the following two months for tesring and predicting In a second step I group together the different entries that correspond to the same user. If the entry gets cancelled with its counter part add the index of the element to te entry_to_remove list. Now if we see the Dataframe it consists of large number of enteries so what does thses entereies correspond to Let us find out. If this code starts with letter c C it indicates the order is cancelled. I therefore create a new dataframe that contains for each order the amount of the basket as well as the way it is distributed over the 5 categories of products The dataframe basket_price contains information for a period of 12 months. The dataframe has also included the customers who cancelled the purchases i. There are also customers who purchases a single item and then went away. Categories of the Customer that buys the product5. The company mainly sells unique all occasion gifts. Maybe this is quite a considerable amount of number 16. Hence I will now try to understand the content of these clusters in order to validate or not this particular separation. Corresponding to the cancelled orders there is a next line just below the line where ths summary of the order has been listed. When calling this method I provide as parameters The hyperparameters for which I will seek an optimal value The number of folds to be used for cross validation Confusion Matrix Learning Curves Logistic Regression k Nearest neighbors Random Forest Adaboost Classifier Gradient Boosting Importing all the required libraries Ignoring the warnings Setting the default style of Matplotlib Loading the Data getting the information about the column type and the null values show first 5 lines gives some information on columns types and numer of null values in each column nb or no both are same Cancelation WITHOUT counterpart Cancelation WITH a counterpart Various counterparts exist in orders we delete the last one sum of purchases user order date of the order selection of significant entries Purchase statement Representation of the number of purchases amount collect the words root association root keyword Aggregate the silhouette scores for samples belonging to cluster i and sort them Label the silhouette plots with their cluster numbers at the middle Compute the new y_lower for next plot define individual silouhette scores and do the graph define the color of the words I set the legend abreviation airline name sum of purchases user order percentage of order price product category date of the order selection of significant entries number of visits and stats on the amount of the cart users I set the legend abreviation airline name define individual silouhette scores and do the graph Correcting time range. Classifying the Customers based on some parameters6. Why not apply some statistics and python and check out if our hypothesis would prove out to be true or False So the hypothesis is not fulfilled as this time the Description of Discount has been taken into consideration. The total number of transactions carried out is of the order of 22 000. Others are do not carry information like colors. It can be seen that the data consists of 4372 users and that they bought 3684 different products. Each entry in this dataframe corresponds to a particular client. A list that contains the entries in which the orders gets cancelled with its counterpart. To do this I create the categorical variable categ_product where I indicate the cluster of each product In a second step I decide to create the categ_N variables with N 0 4 that contains the amount spent in each product category Up to now the information related to a single order was split over several lines of the dataframe one line per product. Now in order to get the total price of each of the customer let us create a new column that would contain the total price of individual customer. I collect all the purchases made during a single order to recover the total order price Now with the help of PIE CHART let us see what are the prices proportion of which the purchases are made. Testing the Predictions Understanding the content of Data This is a transnational data set which contains all the transactions occurring between 01 12 2010 and 09 12 2011 for a UK based and registered non store online retail. I use this information to characterize the different types of customers and only keep a subset of variables In practice the different variables I selected have quite different ranges of variation and before continuing the analysis I create a matrix where these data are standardized In the following I will create clusters of customers. I thus determine the number of purchases made by the user as well as the minimum maximum average amounts and the total amount spent during all the visits Finally I define two additional variables that give the number of days elapsed since the first purchase FirstPurchase and the number of days since the last purchase LastPurchase A customer category of particular interest is that of customers who make only one purchase. Hence orders are split on several lines. 15 Checking the content of Clusters In order to have an insight on the quality of the classification we can represent the silhouette scores of each element of the different clusters. Exploring the content of variables in short Exploratory Data Analysis not inlcuding the product category 3. A short description of the products is given in the Description variable. Other clusters will differ from basket averages mean the total sum spent by the clients sum or the total number of visits made count. CustomerID Customer number. From the above table there are some observations which we can figure out. Country Country name. 5 so we would be also going to analyse the portion of cancelled orders. Score of the silhouette intra clusterAs with product categories another way to look at the quality of the separation is to look at silouhette scores within different clusters Customers morphotypeAt this stage I have verified that the different clusters are indeed disjoint at least in a global way. The other list would contain the entries in which there exists atleast one counterpartThe list will have the name as entry_to_remove and doubtful_entry respectively. Now I check the number of entries that correspond to cancellations and that have not been deleted with the previous filter Now in the StockCode there are some Capital letters which have some description. StockCode Product item code. So we should leave our hypothesis and move to the next step. So it would be a better choice to remove those null values as if we replace those null values by some central measures we would be introducing bias to it. Also the customer s purchases are not bound by a specific number it can also go high as 47 maybe some wholesale customer as well 3. Numerical the day and time when each transaction was generated. Classifying customersIn this part the objective will be to adjust a classifier that will classify consumers in the different client categories that were established in the previous section. Now in order to deal with the negative numbers present in the Quantites column adn the cancelled order the thing that we can do is Create two list 1. Numerical Product price per unit in sterling. Now let us see that the customer who purchases the item purchases how much quantity of the items. Intuitions about the categories of the product the customer buysIn the dataframe products are uniquely identified through the StockCode variable. At first I use the result of the PCA in order to create a representation of the various clusters From this representation it can be seen for example that the first principal component allow to separate the tiniest clusters from the rest. Product Description Since product description would turn out to be a key in the product analyses if would be important to extract the description. Just an intuiton Among these entries the lines listed in the doubtful_entry list correspond to the entries indicating a cancellation but for which there is no command beforehand. 05 will be obtained for all clusters with n_clusters 3 we obtain slightly lower scores for the first cluster. We have a lot of variety to cover Now let us see how many of the purchases made by the customer got cancelled as it would be having a huge impact on our analysis in the future. So what the above function tells is 1. This gives access to for example the average baskets price the number of visits or the total sums spent by the clients of the different clusters. Whola now there are no null values present in any of the column and the Data Types of each of the column as observed from the above table is appropriate. Categorical a 6 digit integral number which is uniquely assigned to each transaction carried out. In practice I have found that introducing the price range results in more balanced groups in terms of element numbers. starting with C which means that the3 customer has cancelled the order. Observation From the table there are few things that are noticeable1. In order to prepare the rest of the analysis a first step consists in introducing this information into the dataframe. SO a question comes in mind if this would be true for all the entries. Many customers of the company are wholesalers. Data EncodingThe X matrix indicates the words contained in the description of the products using the one hot encoding principle. Categorical a 5 digit integral number uniquely assigned to each distinct product. Description Product item name. ", "id": "akarshupadhyay/e-commerce-customer-segmentation", "size": "17297", "language": "python", "html_url": "https://www.kaggle.com/code/akarshupadhyay/e-commerce-customer-segmentation", "git_url": "https://www.kaggle.com/code/akarshupadhyay/e-commerce-customer-segmentation", "script": "matplotlib.cm sklearn.metrics __init__ PCA display legend plotly.offline sklearn.cluster KMeans _scale_data predict AdaBoostClassifier plot train Path plot_learning_curve matplotlib.patches keywords_inventory feature_selection iplot seaborn numpy plotly.graph_objs pathlib learning_curve fill title plot_confusion_matrix sklearn.decomposition linear_model HTML sklearn.ensemble sklearn metrics sklearn.model_selection confusion_matrix Class_Fit(object) grid_search neighbors matplotlib.pyplot WordCloud pandas silhouette_samples wordcloud StandardScaler random_color_func make_wordcloud RadarChart() svm grid_predict GridSearchCV init_notebook_mode model_selection SVC matplotlib sklearn.preprocessing sklearn.svm graph_component_silhouette tree grid_fit silhouette_score IPython.display ensemble STOPWORDS preprocessing ", "entities": "(('Many customers', 'company'), 'be') (('I', 'customers'), 'use') (('legend abreviation airline name', 'time range'), 'call') (('first principal component', 'rest'), 'use') (('I', 'scikit'), 'test') (('Whola', 'above table'), 'be') (('information', 'product'), 'create') (('Product product description', 'description'), 'description') (('I', 'element numbers'), 'find') (('also who', 'single item'), 'be') (('entry', 'te entry_to_remove list'), 'add') (('classification', 'first visit'), 'be') (('the3 customer', 'order'), 'start') (('Others', 'colors'), 'carry') (('I', 'train'), 'define') (('two clusters', 'always which'), 'see') (('that', 'only words'), 'discard') (('that', 'same user'), 'in') (('data transnational which', '2010 12 UK'), 'test') (('lines', 'which'), 'correspond') (('names', 'list'), 'notice') (('that', 'zero'), 'indicate') (('about 25', 'Null values'), 'value') (('I', 'single entry'), 'decide') (('Description', 'consideration'), 'apply') (('purchases', 'which'), 'collect') (('us', 'enteries'), 'consist') (('so we', 'cancelled orders'), '5') (('first step', 'dataframe'), 'consist') (('This', 'E Commerce retailer'), 'be') (('scores', '0'), 'add') (('order', 'letter c C'), 'start') (('2nd entry', 'InvoiceNo'), 'consist') (('3 we', 'first cluster'), 'obtain') (('company', 'occasion gifts'), 'sell') (('it', 'also high 47'), 'bind') (('it', 'future'), 'have') (('which', 'uniquely transaction'), 'carry') (('different products', 'five clusters'), 'category') (('which', '0'), 'in') (('Exploring', 'product category'), 'inlcude') (('that', 'commands'), 'find') (('who', 'purchases'), 'include') (('more than 1400 keywords', 'most frequent more than 200 products'), 'take') (('Later one', 'first visit'), 'be') (('decomposition', 'only data'), 'decide') (('that', 'previous section'), 'be') (('entry', 'particular client'), 'correspond') (('which', 'countries'), 'let') (('we', 'two list'), 'create') (('that', 'product5'), 'category') (('I', 'different categories'), 'intend') (('dataframe products', 'StockCode uniquely variable'), 'identify') (('entry', 'product'), 'indicate') (('One', 'them'), 'be') (('we', 'last counter part'), 'remove') (('average baskets', 'different clusters'), 'give') (('Again hypothesis', 'Category'), 'fulfil') (('Hence I', 'order'), 'try') (('who', 'items'), 'let') (('who', 'only one purchase'), 'determine') (('I', 'Class_Fit class'), 'create') (('Then I', 'clients'), 'start') (('total number', '22 000'), 'be') (('that', 'one year2'), 'analyze') (('So we', 'next step'), 'leave') (('It', 'cluster'), 'remain') (('that', 'available previous purchases'), 'develop') (('Data EncodingThe X matrix', 'encoding one hot principle'), 'indicate') (('that', 'different groups'), 'choose') (('we', 'entire column'), 'PreparationIn') (('customer', 'product'), 'intuition') (('I', 'different groups'), 'use') (('they', '3684 different products'), 'see') (('short description', 'Description variable'), 'give') (('we', 'different clusters'), '15') (('different clusters', 'indeed at least global way'), 'be') (('morphologyFinally I', 'different morphotypes'), 'determine') (('Maybe this', 'considerable number'), 'be') (('where customer', 'country'), 'categorical') (('orders', 'counterpart'), 'list') (('we', 'which'), 'be') (('it', 'scaled_matrix matrix'), 'be') (('Other clusters', 'total visits'), 'differ') (('dataframe basket_price', '12 months'), 'create') (('few that', 'table'), 'be') (('that', 'individual customer'), 'in') (('Capital which', 'description'), 'check') (('extremely we', 'data'), 'foe') (('us', 'dataframe'), 'let') (('us', 'DataFrame'), 'let') (('algorithm fromscikit', 'earlier k'), 'perform') (('we', 'it'), 'be') (('this', 'entries'), 'come') (('one counter part', 'doubtful_entry'), 'else') (('counterpartThe one list', 'entry_to_remove'), 'contain') (('first 5 clusters', 'products'), 'define') (('where orders', 'Countries'), 'visualize') (('which', 'respectively 1'), 'decide') (('ths where summary', 'order'), 'be') ", "extra": "['test']"}