{"name": "practical time series pt 1 the basics ", "full_name": " h1 Groundwork h1 Patterns h2 Examples h3 Passengers dataset h3 Changes in level of savings in the US h3 Annual averages of the daily sunspot areas h1 Dependence h1 Stationarity h2 A little bit of theory h3 Differencing h3 Tests for stationarity ", "stargazers_count": 0, "forks_count": 0, "description": "concept drift between our training and test sets. Interpretation of p value above alpha Accepts the Null Hypothesis H0 the data has a unit root and is non stationary. GroundworkTL DR Time series is any sequence you record over time and applications are everywhere. It is a product of impressive work by Seabold and Perktold http conference. Applying a logarithm does not remove the trend but it does seem to stabilize the amplitude periodic variations have comparable magnitude now. high quantiles of a distribution is only possible with simulation because there is not enough historical dataSome useful references include Brockwell and Davies Time Series Theory and Methods Shumway and Stoffer Time Series Analysis and Its Applications Durbin and Koopman Time Series Analysis by State Space Methods Just about anything written by Rob Hyndman https robjhyndman. StationarityThe reason why bother with the concept in stationarity can be is best summarized with a bad joke I heard many years ago as a math student say a mathematician is given a piece of wood with two nails one hammered all the way in the other half there and told to get remove them. We will use this dataset to demonstrate in practice what kind of information can be obtained using seasonal decomposition. As we can see the hypothesis of non stationarity is non rejected for the trend component but not for seasonal. below alpha Rejects the Null Hypothesis H0 the data is stationary. If you are likely to use statistics in your work and you are a Pythonista familiarizing yourself with this library is a very good idea https www. The proper mathematical treatment of this theory is way beyond the scope of this notebook so a mathematically inclinded reader is advised to look up those terms and then follow the references. To define things more formally a stationary time series is one whose unconditional joint probability distribution does not change when shifted in time. It determines how strongly a time series is defined by a trend. Alternative Hypothesis H1 Alternative Hypothesis of the test is that the time series is stationary. Each observation in the series can be expressed as either a sum or a product of the components. com konradb practical time series part 1 the basics this notebook Part 2 smoothing methods https www. How does he solve it 1. There is more content where that came from based on a course I used to teach so further installments will involve ARIMA state space methods and eventually sequence models because DeepLearnEverything Groundwork section one Patterns section two Dependence section three Stationarity section four We start by importing the necessary libraries most of them are familiar to anybody working with the data science with the exception of statsmodels. We may have different combinations of trends and seasonality. Depending on the nature of the trends and seasonality a time series can be modeled as an additive or multiplicative time series. Changes in level of savings in the USLet s check how does seasonal decomposition work with some other popular datasets Annual averages of the daily sunspot areasAs we can see changing the decomposition type to multiplicative alleviates the problem with residual behavior but only to a certain degree the pattern is still not consistent with increase in the amplitude in the middle of the sample. It is implemented in the statsmodel package. com Ross Ihaka Time Series Analysis https www. pdf two people who set out to bring statistical functionality in Python into the 21st century. Since stationarity is an assumption underlying many statistical procedures used in time series analysis non stationary data are often transformed to become stationary. A fast and therefore very popular manner of examining this dependence are the autocorrelation and partial autocorrelation functions which are defined below Mean function of time series begin equation mu_t E X_t end equation Autocovariance function of a time series begin equation gamma s t Cov X_s X_t E X_s X_t E X_s E X_t end equation which leads to the following definitions of ACF PACF Autocorrelation begin equation rho u t u Cor X_ u X_ t u frac Cov X_t X_ t u Var X_t Var X_ t u end equation Partial autocorrelation begin equation phi u Cor X_t X_ t u X_ t 1 ldots X_ t u 1 end equation An intuitive way to think about it is that ACF at lag k measures a linear dependence between X_t and X_ t k while PACF captures the dependence between those values correcting for all the intermediate effects. once done hammer the other all the way to reduce to an already solved caseI warned you it was bad but there is an analogy to how time series theory handles the problem of nonstationarity. Differencing at lag 1 is best thought of as discreet counterpart to differentiation first derivative of a linear function is flat first derivative of a quadratic function is linear etc so if we want to get rid of a trend behaving like polynomial of degree n we need to apply the differencing operator n times. plot skip the start of the series adfuller does not handle missing values which appear for values within the first full period. Below we plot a few examples of stationary and non stationary series starting with the simplest non trivial stationary series Gaussian white noise. pdf PatternsThe first we can do to identify patterns in a time series is separate it into components with easily understandable characteristics begin equation X_t T_t S_t C_t I_t quad end equation where T_t the trend shows a general direction of the time series data over a long period of time. Next we can try differentiating to get rid of the trendAs expected differentiation removes the trend oscillations happen around a fixed level but variations amplitude is magnified. I_t the irregular component residuals consists of the fluctuations in the time series that are observed after removing trend and seasonal cyclical variations. Let s go back to our hammer and nail approach and try some transformations to make the series stationary. com konradb practical time series pt 4 prophet This notebook summarizes some elementary methods for time series analysis sometimes you don t have the time hardware data to go for a Transformer and vintage methods can be your friend. Obvious examples include daily power consumption patterns or annual sales of seasonal goods. The most popular tests dealing with stationarity are Augmented Dickey Fuller ADF Kwiatkowski Phillips Schmidt Shin KPSS Philips Perron PP ADF test is a unit root test. How does that translate into ADF results The null is still not rejected but p value has dropped which indicates the transformations are the right way to go. com konradb practical time series part 2 vintage methods Part 3 ARMA https www. The Null hypothesis is that the series in the second column does not Granger cause the series in the first. C_t optional cyclical component is a repetitive pattern which does not occur at fixed intervals usually observed in an economic context like business cycles. 05 then we reject the null hypothesis and conclude that the said lag of X is indeed useful. This is a first part of a series of notebooks about practical time series methods Part 1 the basics https www. However visual inspection is not what one would could a rigorous criterion so let s define things in a formal manner. This demonstrates that while seasonal decomposition is a fast tool it has severe limitations when dealing with more sophisticated data generating processes. If the stats are quite different then the series is not likely to be stationary. Let s start with a basic additive decomposition Trend and seasonality are behaving more or less in line with expectations but the behavior of the residuals is clearly not consistent over time average level of oscillations in the middle of the sample is very different than on either end. When a log transformation has been used this is equivalent to using a multiplicative decomposition because begin equation X_t T_t S_t I_t end equation is equivalent to begin equation log X_t log T_t log S_t log I_t end equation A popular implementation for calculating the fundamental decomposition can be used via the statsmodels package Examples Passengers datasetEvery field of knowledge has the dataset that is used for teaching purposes machine learning has Iris and CIFAR differential equations Canadian lynx data and statistics has the airline passengers dataset between 1949 and 1960 first compiled by Box and Jenkins you will be hearing those two names again in the next module in 1976. If we are dealing with a process that does not adhere to those characteristics we can either try and capture them directly or transform it in such a manner that it can be considered stationary. com konradb practical time series pt 3 arma and friends Part 4 Prophet https www. Null Hypothesis H0 Null hypothesis of the test is that the time series can be represented by a unit root that is not stationary. It is important to understand the difference between iterating diffencing operator n times and differencing once at lag n which is best demonstrated in an example. Instead it is generally used on exogenous not Y lag variables only. For that reason a less strict variant has been introduced weak a. It accepts a 2D array with 2 columns as the main argument. For a slightly more interesting example of non stationary behavior we can examine the passengers dataset The stationarity of a series can be checked by examining the distribution of the series we split the series into 2 contiguous parts and compute the summary statistics like the mean variance and the autocorrelation. Algorithms are likely to yield better predictions if we apply them to stationary processes because we do not need to worry about e. While there are many possible reasons one quick explanation is the additive vs multiplicative relationship between the series components which is something we can examine quickly Not much of a qualitative change in trend and seasonality components but the residuals looks much more stable around a constant level such phenomenon does not of course imply stationarity by itself but at least a clear signal in the opposite direction is not there anymore. This implies that parameters such as mean and variance also do not change over time. The core idea is that it s much easier to model dynamic behavior over time if the statistical properties do not change oscillations happen around the same level the amplitude does not change too much etc in other words the probability distribution of X_t is the same as the distribution of X_ t h such models are well understood. More formally time series data is a sequence of data points or observations recorded at different time intervals those intervals are frequently but not always regular hourly daily weekly monthly quarterly etc begin equation X_t quad t 1 ldots T end equation A strict formulation would be that a time series discrete realization of a continuous stochastic process generating the data and the underlying reason why we can infer from the former about the latter is the Kolmogorov extension theorem. a import matplotlib as mpl general settings adjust the parameters for displayed figures decomposition decomposition Non stationary example series. If the P Values are less than a significance level 0. It is based on the idea that if X causes Y then the forecast of Y based on previous values of Y AND the previous values of X should outperform the forecast of Y based on previous values of Y alone. It represents a long term progression of the series secular variation S_t the seasonal component with fixed and known period. Phenomena measured over time are everywhere so a natural question is what can we do with time series Some of the more popular applications reasons to bother are interpretation we want to be able to make sense of diverse phenomena and capture the nature of the underlying dynamics modelling understanding inherent aspects of the time series data so that we can create meaningful and accurate forecasts. forecasting prediction we want to know something about the future filtering smoothing we want to get a better understanding of the process based on partially fully observed sample simulation in certain applications calculating e. DependenceThe only way to succesful prediction is if past values of a series carry some information about the future behavior in other words if the present values are dependent on the past. We continue moving in the right direction what happens if we combine the two transformations So after applying logarithm to stabilize the variance and differentiation to remove the trend we have transformed our series to one that can be plausibly treated as stationary. Compute the summary statistics The values are clearly very different across the two data subsets which strongly suggests non stationarity. We can verify that intuition by examining ACF and PACF TODO lag plots moar explanations CausalityMOAR Granger causality test is used to determine if one time series will be useful to forecast another. The reason for that is that ADF test check for a very specific form of non stationarity namely variation in the presence of a linear trend existence of a single unit root while the seasonal component is clearly not stationary see graph above it is a qualitatively different kind of behavior. htmlIn this module we are merely scratching the surface of statsmodel functionality with seasonal decomposition as our primary tool. A trend stationary process is not strictly stationary but can easily be transformed into a stationary process by removing the underlying trend which is solely a function of time the same holds true for a stationary process with an added cyclical component. org proceedings scipy2010 pdfs seabold. The second argument maxlag says till how many lags of Y should be included in the test. ACF PACFWe can decompose the series to check the components one by one which parts are responsible for the non stationary behavior An important caveat it is useful to remember that statistical tests do not accept a hypothesis we can only fail to reject it. A little bit of theoryGiven a stochastic process X_t and cdf F_X a process is strictly stationary iff begin equation F_X X_ t_1 ldots X_ t_n F_X X_ t_1 tau ldots X_ t_n tau end equation The definition is very powerful if we know a cdf of a distribution we can infer everything however it is not possible to verify in practice. So Granger causality test should not be used to test if a lag of Y causes Y. The values are in the first column and the predictor X is in the second column. Tests for stationarityWhile inspecting plots before after transformation can be useful to assess presence of trends or seasonalities as we did above with the passengers dataset in practice we need a more formal approach like testing a hypothesis introduction to statistical tests is beyond the scope of this notebook so if you feel like you need a refresher please consult other sources. second order stationarity a process X_t is weakly stationary if it satisfies the following conditions mu_t mu_ t tau gamma s t gamma s t Var X_t infty Implication constant mean covariance only depends on distance in time between variables autocorrelation begin equation rho u frac gamma u gamma 0 end equation So how can we turn a non stationary series into a stationary one Popular transformations include but are not limited to differencing the series taking the log of the series power transforms Differencing Lag operator of order d begin equation nabla_d X_t X_t X_ t d end equation Differencing can help stabilize the mean of a time series by removing changes in the level of a time series and so eliminating trend and seasonality. It is observed when there is a distinct repeated pattern observed between regular intervals due to seasonal factors annual monthly or weekly. It is worth pointing out that an alternative to using a multiplicative decomposition is to first transform the data until the variation in the series appears to be stable over time then use an additive decomposition. start with the nail that s completetely in that s the more interesting case2. ", "id": "konradb/practical-time-series-pt-1-the-basics", "size": "17177", "language": "python", "html_url": "https://www.kaggle.com/code/konradb/practical-time-series-pt-1-the-basics", "git_url": "https://www.kaggle.com/code/konradb/practical-time-series-pt-1-the-basics", "script": "pandas.plotting adfuller CFG plot_pacf gauss statsmodels.formula.api random plot_acf AR numpy seaborn statsmodels.tsa.ar_model statsmodels.tsa.stattools seasonal_decompose Image matplotlib.pyplot pandas statsmodels.graphics.tsaplots statsmodels.tsa.seasonal matplotlib autocorrelation_plot IPython.display statsmodels.api ", "entities": "(('time series', 'additive'), 'depend') (('parameters', 'also time'), 'imply') (('you', 'library'), 'be') (('we', 'e.'), 'be') (('Augmented Dickey Fuller ADF Kwiatkowski Phillips Schmidt Shin KPSS Perron ADF Philips test', 'stationarity'), 'be') (('that', 'completetely that'), 'start') (('lag', 'Y.'), 'use') (('that', 'unit root'), 'be') (('It', 'seasonal factors'), 'observe') (('time one series', 'another'), 'verify') (('that', 'one'), 'continue') (('it', 'manner'), 'deal') (('time how strongly series', 'trend'), 'determine') (('which', 'added cyclical component'), 'be') (('so s', 'formal manner'), 'be') (('It', 'S_t seasonal fixed period'), 'represent') (('P Values', 'significance level'), 'be') (('hypothesis', 'trend non component'), 'reject') (('applications', 'time'), 'be') (('amplitude periodic variations', 'comparable magnitude'), 'remove') (('Instead it', 'lag generally exogenous Y variables'), 'use') (('that', 'trend'), 'consist') (('who', '21st century'), 'pdf') (('data', 'Null Hypothesis H0'), 'reject') (('This', 'notebooks'), 'be') (('one', 'there them'), 'summarize') (('fast it', 'data generating when more sophisticated processes'), 'demonstrate') (('variations amplitude', 'fixed level'), 'try') (('you', '1976'), 'be') (('said lag', 'X'), '05') (('high quantiles', 'Rob Hyndman https robjhyndman'), 'be') (('present values', 'past'), 'be') (('data', 'unit root'), 'accept') (('kind', 'seasonal decomposition'), 'use') (('It', 'statsmodel package'), 'implement') (('clearly graph', 'qualitatively different behavior'), 'be') (('mathematically inclinded reader', 'then references'), 'be') (('series', 'first'), 'be') (('such models', 'X _ t h'), 'be') (('we', 'mean variance'), 'examine') (('sometimes you', 'hardware Transformer methods'), 'series') (('most', 'statsmodels'), 'be') (('observation', 'components'), 'express') (('probability unconditional joint distribution', 'when time'), 'be') (('this', '1 basics'), 'part') (('import mpl general settings', 'figures decomposition example displayed decomposition Non stationary series'), 'matplotlib') (('however it', 'practice'), 'bit') (('_ t end equation nabla_d d Differencing', 'so trend'), 'be') (('which', 'first full period'), 'skip') (('average level', 'very end'), 'let') (('we', 'differencing operator'), 'be') (('at least clear signal', 'opposite direction'), 'be') (('you', 'other sources'), 'test') (('how many lags', 'test'), 'say') (('future filtering we', 'e.'), 'prediction') (('Obvious examples', 'annual seasonal goods'), 'include') (('transformations', 'ADF results'), 'translate') (('module we', 'primary tool'), 'htmlIn') (('Below we', 'simplest non trivial stationary series'), 'plot') (('we', 'only it'), 'decompose') (('we', 'meaningful forecasts'), 'be') (('pattern', 'sample'), 'change') (('then forecast', 'Y'), 'base') (('time series', 'test'), 'be') (('X _ t PACF', 'intermediate effects'), 'be') (('less strict variant', 'reason'), 'introduce') (('We', 'trends'), 'have') (('which', 'non strongly stationarity'), 'compute') (('which', 'best example'), 'be') (('variation', 'additive then decomposition'), 'be') (('It', 'main argument'), 'accept') (('assumption', 'time series analysis non stationary data'), 'transform') (('series', 'transformations'), 'let') (('predictor X', 'second column'), 'be') (('It', 'http Seabold conference'), 'be') (('end quad where trend', 'time'), 'pdf') (('underlying why we', 'latter'), 'be') (('time series how theory', 'nonstationarity'), 'warn') (('repetitive which', 'business cycles'), 'be') ", "extra": "['biopsy of the greater curvature', 'test', 'procedure']"}