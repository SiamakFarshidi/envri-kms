{"name": "whale recognition model with score 0 78563 ", "full_name": " h1 Whale Classification Model h1 Abstract h1 Overview h2 Content h1 Duplicate image identification h1 Image preprocessing h2 Image rotation h2 Convert to black and white h2 Affine tranformation h1 Siamese Neural Network architecture h2 Head model h2 Branch model h2 Code h1 Training data construction h2 Image selection h2 Matching whale examples h2 Different whales examples h2 Code h1 Training procedure h1 Generating the submission file h1 Bootstrapping and ensemble h2 Bootstrapping h2 Ensemble h1 Visualization h2 Feature weights h2 Pseudo distance function h2 Feature activation h1 Off topic h2 Training Scalability h2 Interesting results and scores h2 Validation set ", "stargazers_count": 0, "forks_count": 0, "description": "The left side image is the original image fluke pointing down. 1556 like network for example. To speed things up the results of some slow computations are included as a dataset instead of being recomputed here. 752 VGG like CNN trained like standard model0. Training lasts 400 epochs with the following quantities changing as the training progresses Learning rate L2 regularization Constant K measuring the scale of the random component of the score matrix used to match similar images to construct tough training cases. Implementing this strategy while training a Siamese Neural Network is what makes the largest contribution to the model accuracy. Weighted sum implemented as a Dense layer. com martinpiotte bounding box model as computed in the model defined in a separate kernel The idea is that clipping the edges of the fluke is more harmful than the gain obtained by fitting it exactly thus a margin is preferred. Also a distance measure does not provide for features to be negatively correlated. With Windows 10 and a GTX 1080 card there is about 6. What we expect to see is that equal zero feature should produce a smaller output than similar large values. Head modelThe head model compares the feature vector from the branch model to decide if the pictures show the same or different whales. The same neural net with the same weights is used for each feature. Reasons could be the underside of the fluke is not visible or we see only dead fluke fragments on the a beach there are two whales in the picture etc. The list is not exhausitve there are probably more case that I have not noticed. Another approach to training scalability would be to partition the training data into different subset each subset being processed separately to match image pairs. This would be effective not only for the Linear Assignment Problem part but also when computing the cost matrix which still has complexity O n 2. Off topic why add this unless it is interesting Duplicate image identificationThis section describes the heuristic used to identify duplicate images. 78563 using a threshold of 0. However this does not find the real optimum just an approximation. 8GB VRAM available and this limit has constrained the model choice. I encourage everyone to skip ahead directly to whatever you are most interested in without necessarily going through everything. Feature activationThis section attempts to reconstruct image that maximally activate a feature. However the code is still provided in the notebook as reference even if it is not executed by default. Other details contribute somewhat to the accuracy but have a much smaller impact. Use the original order as picture A and the derangment as picture B. OverviewThis notebook describes all the different elements of the submission. 00 0151 200 16. Siamese Neural Network architecture some interesting thoughts 1. Whenever I encountered such instance in the training set not in the test set I would add it to a list. Branch modelThe branch model is a regular CNN model. This is inspired from adversarial training find pairs of images that are from different whales but that are still very similar from the model perspective. Note that after this the tensor has dimension 96x96x64 the same volume as the initial 384x384x1 image thus we can assume no significant information has been lost. If a picture is used more often in negative examples the model risks simply learning to guess a mismatch whenever this picture is present without learning how to compare the whales correctly. Some images are perfect binary copies while other have been altered somewhat contrast and brightness size masking the legend etc. It takes about 15 minutes on setup with a GTX 1080. Find elements from training sets not new_whale Use only identified whales Dictionary of picture indices Evaluate the model. I use 4 such subblocks by block plus a single 1x1 convolution to increase the feature count after each pooling layer. pdf with a few modifications that will be covered in details later. Training procedure zzzzz. 25 0241 250 10 5 0. The p2h dictionary associate a unique image id phash for each picture. The simplest solution was to convert all images to black and white which did not reduce the accuracy even when comparing originally colored images. We can also check how per feature network behave for different feature values. A ResNet like architecture is more economical than a VGG https arxiv. At the time I am writing this kaggle kernel with custom package fail to commit. Image preprocessingTraining is performed on images subjected to the following operations 1. model also included in the dataset. Given the small number of pictures in the training dataset it is not irrealistic to imagine the model recognizing a specific picture using the shape of a wave or a bird flying by. The list is in no way exhaustive. Both images have 1. For each feature I compute the sum the product the absolute difference and the difference squared x y x y x y x y 2. Matching zeros get llower value. After 250 epochs trainings accuracy is fantastic but it also grossly overfits. I used a custom model mostly inspired from ResNet https www. Because of the high resolution it uses a lot of memory so a minimum of work is done here to save memory for subsequent blocks. SIAMESE NEURAL NETWORK Complete model is constructed by calling the branch model on each input image and then the head model on the resulting 512 vectors. This model as a slightly better score of 0. Feature weightsAs was discussed in the model description the head model makes a weighted sum of the features allowing for negative weights. To randomize the selection and control the matching difficulty we add a random matrix to the cost matrix from step 1. VisualizationThis section explores the model through some visualizations. Pseudo distance functionNot very easy to see but still the largest output best whale match occurs for matching features with large values. The smaller K the more difficult the pairing is for the model. model and mpiotte bootstrap. The subset can be reconstructed randomly each time the cost matrix is computed. This could be that we match uniform unicolor flukes which is less likely to be correct that mathing flukes with multiple caracteristic markings. To force all the other whales to a low probability the training algorithm presents pairs of pictures with increasing difficulty as evaluated by the model at any given time. When y is None score is a packed upper triangular matrix. Obviously to cover everything it has to be fairly long. Consider a case where if both images have feature X they must be the same whale unless they also both have feature Y in which case X is not as clear. Using slow scipy. org wiki Derangement of its pictures. The following table shows the exact schedule used for the learning rate LR L2 regularization L2 and randomized score matrix K. By testing each image from the test set against every image from the training set the most likely whales can be identified by sorting the pictures in likelihood for a match. At this point L2 regularization is applied learning rate is reset to a large value and training continues for an additional 150 epochs. Just the colormap is probably easier to see. The following algorith is used to generate the pairs 1. 999999 score is just a number use to rank whales it is not a probability. 10 4 351 390 4. The code to generate images is modified from examples found in Deep Learning with Python https www. 50 0200 240 4. Still I feel there is more postive signal in the active features than in the negative ones especially with ReLU Rectified Linear Unit activation concept that is lost by the distance measure. Construct the head model with linear activation Evaluate the model for constant vectors. The image shape used by the model The horizontal compression ratio The margin added around the bounding box to compensate for bounding box inaccuracy If an image id was given convert to filename Determine the region of the original image we want to capture based on the bounding box. For this experiment I selected all images from the test set for which the mpiotte standard model provides a single whale prediction with a score 0. Essentially we forcus the training on pairs that he model is getting wrong as a form of adversarial training. Siamese Neural Network architectureThe Siamese Neural Network compares two images and decides if the two images are taken from the same whale or different whales. The same CNN with the same weights is used for both images. 78563 submission to the Humpack Whale identification Challenge. First pair is for matching whale Second pair is for different whales A Keras generator to evaluate only the BRANCH MODEL A Keras generator to evaluate on the HEAD MODEL on features already pre computed. Block 2 has two 3x3 convolutions similar to VGG. Sort the whales in decreasing score. Duplicate image identification not much to see here keep moving 1. Similarly training starts with no L2 regularization. CodeThe following is the Keras code for the model. This creates a random number of matching image pairs with each image taken exactly two times. BootstrappingSubmitting these 1885 pictures as a submission show that this set if over 93 accurate. 10 5 infty 011 15 64. However to scale this approach a less costly randomized matching heuristic would be more effective. It should be studied in conjunction with the Bounding Box Model http www. The procedure itself has evolved from many previous experiments trial and errors on earlier versions of the model. Training data construction most of the secret sauce is here 1. The position in train of each training image id First try to use lapjv Linear Assignment Problem solver as it is much faster. I suggest reading the original paper but the idea is to form a subblock with a 1x1 convolution reducing the number of features a 3x3 convolution and another 1x1 convolution to restore the number of features to the original. Generating the submission file re zzzzz. Image preprocessing just the regular stuff 1. Affine tranformationThe affine transformation https en. org openaccess content_cvpr_2016 papers He_Deep_Residual_Learning_CVPR_2016_paper. Pushed to the extreme it it possible to construct a training dataset for which pairs of pictures of different whales appear to the model more similar than pairs of pictures from the same whale making the model learn to classify similar images as different whales and dissimilar images as same whales To prevent this early training is executed with a large value of K making the negative examples essentially random pairs of pictures of different whales. ca gkoch files msc thesis. For each picture from the test set 1. For each whale of the training set compute a derangement https en. The bootstrap model is trained on more data however the tagging accuracy is lower since the bootstrap data is only 93 accurate. Adding these files to the training set and re running the training from scratch generates the mpiotte. While scoring the correct whale high it must simultaneously score all other whales lower. com books deep learning with python by Francois Chollet. This notebook essentially describes this final retraining and thus there is no validation set involved. Find all images associated with a given phash value. These convolutions are less memory intensive then the subsequent ResNet blocks and are used to save memory. However comparing a colored image with a black and white image resulted in much lower accuracy. 786 best score obtained by a linear combination of the standard model with the bootstrap model0. Generate the transformation matrix Read the image transform to black and white and comvert to numpy array Apply affine transformation Normalize to zero mean and unit variance Reduce the number of features to filter Extend the feature field no activation Restore the number of original features Add the bypass connection BRANCH MODEL 384x384x1 96x96x64 48x48x64 48x48x128 24x24x128 24x24x256 12x12x256 12x12x384 6x6x384 6x6x512 512 HEAD MODEL Per feature NN with shared weight is implemented using CONV2D with appropriate stride. During development I used a validation set composed of 570 images from the training set to test ideas and to tune the training procedure. Whale Classification ModelThis notebook describes the strategy behind the 0. EnsembleThe best score is obtained by an ensemble of the mpiotte standard. The right side image is rotated 180 degrees. Off topic Training ScalabilityAs described training the base model takes about 2 days and the bootstrap version a little under 3 days using a i7 8700 CPU and GTX 1080 GPU. Both of these models make different errors because of their nature which make them good candidate for ensembling The standard model is trained on the smallest training set and thus has more potential for overfitting. Each training epoch is composed of a sequence of image pairs A B such that Exactly 50 of the pairs are for matching whales and 50 for different whales Each image from the training set is used exactly 4 times per epoch A and B images of matching whales A and B images of different whale pairs Pairs of images of different whales are selected to be difficult for the network to distinguish at a given stage of the training. 15 close to the average picture aspect ratio. org wiki Affine_transformation maps a rectangular area of the original image to a square image with resolution 384x384x1 only one channel for black and white. The typical approach is to use a distance measure e. Solve the linear assignment problem Compute a derangement for matching whales Construct unmatched whale pairs from the LAP solution. To address these concerns i proceed as follow 1. The final step of the branch model is a global max pooling which makes the model robust to fluke not being always well centered. Bootstrapping and ensemble classic but short 1. Both images have the same Perceptual Hash http www. shuffle the training pictures Map whale id to the list of associated training picture hash value Map training picture hash value to index in train array Compute the match score for each picture pair Train the model for step epochs Collect history data epoch 10 epoch 150 epoch 200 epoch 240 epoch 250 epoch 300 epoch 350 epoch 390 epoch 400 Not computing the submission in this notebook because it is a little slow. Generate the subsmission file. We want the Siamese Neural Network to pick the one correct whale from all the possible whales from the training set. The rectangular area has a width over height aspect ratio of 2. A Siamese Neural Network is composed of two parts. For each whale from the training set compute the whale score as the maximum image score for this whale. phash that differ by at most 6 bits and 1. The assembly strategy consist in compute a score matrix or dimension test by train that is a linear combination of the standard and bootstrap model. This provides some insight on the filtering process. Interesting results and scoresScore Description 0. The fact that the training and test set have duplicate images has already been well documented. 766 standard model0. However the score matrix is randomized so investing a lot of time to compute an exact solution to a randomized input is wasteful. The element that generated the largest accuracy improvement is the procedure used to generate image pairs during training. Block 1 384x384 Block 2 96x96 Block 3 48x48 Block 4 24x24 Block 5 12x12 Block 6 6x6Block 1 has a single convolution layer with stride 2 followed by 2x2 max pooling. The rectangle is taken to be slightly larger than the computed bounding box https www. model is good for a 0. It can be ambiguous as duplicate images may have different whale ids. Unpack and transpose to form the symmetrical lower triangular matrix. 774 bootstrapped model0. Linear sum assignment https en. This confirms are initial assumptions. Because the training data set is small and the test set is larger bootstrapping is a good candidate to improve the score. For new_whale the algorithm will chose high confidence prediction first then insert new_whale then low confidence predictions. threshold is selected through trial and error although most model variants perform best with a value of threshold that results in 7100 test images with new_whale as first choice something that can be measured without submitting a prediction to Kaggle. The left image is the original picture. A Convolutation Neural Network CNN transforms an input image into a vector of features describing the whale. This has complexity frac n n 1 2 where n is the size of the training set. Training data constructionAs highlighted in the abstract section this is the part that makes a big difference in the model accuracy. Generation of the submission using the score matrix is unchanged. Finally the image is normalized to zero mean and unit variance. 55 for the bootstrap model. The output is a weighted sum of the converted features with a sigmoid activation. The resulting ensemble as an accuracy of 0. The larger K the more random the matching. Set a large value for matching whales eliminates this potential pairing This is a match Different whales Skip this on the last epoch. Entry that correpond to pair of images from the same whale are set with similarity infty. no manual judgement on the training set 0. 45 for the standard model and 0. The random transform is skipped when testing. the pixelwise mean square error between the normalized images is below a given threshold. I call the CNN the __branch model__. What I call the __head model__ is used to compare the feature vectors from the CNN and decide if the whales match or not. In fact if the model is initially fed examples that are too hard it does not converge at all. As the model ability to distinguish between whales increases K is gradually reduced presenting harder training cases. Because algorithm is O n 3 small partitions are much faster but not what produced the submitted solution Maximizing the score is the same as minimuzing score. Different whales examplesDifferent whales examples are generated by computing a derangement of all pictures from the training set subject to The image pairs must belong to different whales The pairs formed must be difficult for the model to distinguish. By presenting each image an equal number of times with 50 positive and 50 negative examples the model has no incentive in learning to recognize specific pictures and thus focuses on recognizing whales as desired. It is not enough to have a random whale score low. A list of training image ids Associate the image ids from train to each whale id. 107 duplicate images only Validation setI have not discussed validation data until now. 728 standard model without L2 regularization result after 250 epochs 0. Oops this is HUGE Find all the whales associated with an image id. By fixing the subset size to a reasonable value the complexity then grows linearly with the number of subsets allowing larger training datasets. AbstractThe approach used for this submission is essentially a Siamese Neural Network http www. The center image does the test transformation. Fortunately only the head model must be computed for all pairs and it is very fast. Apply an affine transformation. CodeThe described logic is essentially implemented by the TrainingData class that performs just in time data augmentation as well as computing the matching. scipy can be used as a fallback but it is too slow to run this kernel under the time limit As a workaround use scipy with data partitioning. The problem turned out to be memory bound with most memory being taken to store activations from the feedforward pass used to compute the gradient during backpropagation. At the same time we want the model to recognize whales and not pictures. At the same time there is this implicit assumption that swapping the two images must produce the same result if A is the same whale as B B must be the same whale as A. The right image adds a random data augmentation transformation. However greater accuracy can be achieved by retraining the model using all the data repeating he procedure that was successful on the validation set. Trial and error suggest a weight of 0. match len train len self. Training the large model from random weights is difficult. 0 the mean square error does not exceed 0. Training procedureThis scection describes the procedure used to train the model. Also note that the score matrix for the Linear Assignment Problem is computed at every 5 epochs starting with epoch 10. Two images are considered duplicate if they meet the following criteria 1. Duplicate image are free answers assuming there is no tagging error. At the same time very dissimilar values must be penalized. Because algorithm is O n 3 small batches are much faster. Rotate the image if it is in the rotate set 1. In the context here hard examples are similar images belonging to different whales. Bootstrapping and ensembleThe mpiotte standard. The similarity between each pair of image is computed using the current model state. More than 50 of the time is spent solving the Linear Assignment Problem because the algorithm used has complexity O n 3 and provides an exact solution. Use only identified whales For each whale find the unambiguous images ids. Visualization everyone s favorite 1. org wiki Assignment_problem algorithm is used to find the most difficult matching. The output of these convolutions is then added to the original tensor bypass connection. The prefered image is the one with the highest resolution or any one if they have the same resolution. The random matrix has values uniformly distributed between 0 and K. The blacklist was constructed manually by spotting images unhelpful to training. Read the dataset description Determise the size of each image Read or generate p2h a dictionary of image name to image id picture to hash Two phash values are considered duplicate if for all associated image pairs 1 They have the same mode and size 2 After normalizing the pixel to zero mean and variance 1. Image selectionTo begin we reduce the number of images from the training set Images from the blacklist are removed Duplicate images are removed All new_whale images are removed All whales with a single image are removed. Find all distinct phash values If the images are close enough associate the two phash values this is the slow part n 2 algorithm Group together images with equivalent phash and replace by string format of phash faster and more readable For each image id determine the list of pictures Notice how 25460 images use only 20913 distinct image ids. The value of the weight is redundant because the weight is just a scaling factor of the feature which could be learned by another layer however it allows for negative weights which cannot be produced otherwise when using ReLU activation. Matching whale examplesHalf the examples used during training are for pair of images. Transform to black and white 1. For each image not new_whale from the training set compute the image score which is the model score for the image pair. In the context of this competition with no constraint on runtime and a small dataset it was a pragmatic choice. 25 0251 300 64. com martinpiotte bounding box model notebook which describes separately the strategy for image cropping. If the image duplicates one or more training set images add the whales possibly more than one from the training image as the top candidates. unmatch len train Test on a batch of 32 with random costs. Image rotationI noticed that some pictures have the whale fluke pointing down instead of up as usual. 1 Compute phash for each image in the training and test set. In this context bootstrapping means using the model to automatically generate additional training example and retrained the model over this larger dataset. Blocks 3 to 6 perform ResNet like convolution. The four values are passed through a small neural network which can learn how to weigh between matching zeros and close non zero values. It computes only the upper triangular matrix of the cost matrix if y is None. 58 061 150 64. We can verify that we see a combination of positive and negative weights that confirm that some features when matched reduce the probability of matching whales. Force a different choice for an eventual next epoch. have the same size and 1. Initial noise Gradient step Construct the function Stimulate the 4 central cells Use gradient descent to form image Blurring a little creates nicer images by reducing reconstruction noise Clip the image to improve contrast Show the first 25 features of 512. It is interesting to note how the threshold value for the ensemble is much lower which is consistent with the fact that both models make different errors and thus the ensemble scores are typically lower than the individual models which are grossly optimistic about they guesses. 423 duplicate images and new_whale submission0. During training these images are rotated 180 degrees to normalize them with the fluke pointing up. The branch model is composed of 6 blocks each block processing maps with smaller and smaller resolution with intermediate pooling layers. 325 new_whale submission0. The 512 feature vectors can be pre computed once for each image i. Add new_whale with a fixed new whale score of threshold. Epochs LR K L2 1 10 64. Use only unambiguous pictures Skip excluded images Find the list of training images keep only whales with at least two images. 10 4 Generating the submission fileThe basic strategy is this. During training data augmentation is performed by adding a random transformation that composes zoom shift rotation and shear. Here are the key elements of its design Because the training dataset is small I tried to keep the number of learned parameter relatively small while keeping the model expressive enough. 10 4 301 350 16. L_1 with a contrastive loss function but there are a few reasons to try something different A distance measure will consider two features with value zero as a perfect match while two features with large but slightly different values will be seen as good but not quite as good since they are not exactly equal. To produce different matching for successive epochs the selected entries in the matrix are overwritten with infty to force an alternate selection for the next matching. Convert to black and whiteIn my early experiments I noticed that my models achieved approximately the same accuracy when comparing two colored images or two black and white images. 744 with a threshold 0. To prevent this the data presented to the model must be unbiased. 714 standard model without exclusion list rotation list and bounding box model i. The h2p dictionary associate each unique image id to the prefered image to be used for this hash. Show an example of a duplicate image from training of test set For each images id select the prefered image Select the image with highest resolution Read the bounding box data from the bounding box kernel see reference above Suppress annoying stderr output when importing keras. 10 4 391 400 10 5 0. ", "id": "martinpiotte/whale-recognition-model-with-score-0-78563", "size": "23828", "language": "others", "html_url": "https://www.kaggle.com/code/martinpiotte/whale-recognition-model-with-score-0-78563", "git_url": "https://www.kaggle.com/code/martinpiotte/whale-recognition-model-with-score-0-78563", "script": "keras.engine.topology Flatten Concatenate tqdm_notebook expand_path linear_sum_assignment gaussian_filter os.path Sequence set_lr Image Activation mpl_toolkits.mplot3d __len__ backend as K keras.preprocessing.image BoundaryNorm matplotlib.ticker on_epoch_end numpy sqrt Input img_to_array read_cropped_image match keras.utils pandas keras_tqdm keras.optimizers lap matplotlib.colors BatchNormalization read_for_training Model Add matplotlib show_whale get_lr lapjv keras.models prefer Reshape build_model Adam read_csv Conv2D scipy.ndimage TrainingData(Sequence) score_reshape read_for_validation array_to_img affine_transform PIL subblock Axes3D compute_score GlobalMaxPooling2D imagehash MaxPooling2D backend __init__ keras.layers TQDMNotebookCallback phash show_filter __getitem__ MaxNLocator make_steps isfile matplotlib.pyplot prepare_submission Dense FeatureGen(Sequence) Lambda cm tqdm plot_model regularizers read_raw_image ScoreGen(Sequence) keras Image as pil_image scipy.optimize math build_transform ", "entities": "(('output', 'tensor bypass then original connection'), 'add') (('procedure', 'model'), 'evolve') (('problem', 'backpropagation'), 'turn') (('org wiki Affine_transformation', 'resolution'), 'map') (('network', 'training'), 'compose') (('com books', 'Francois Chollet'), 'learning') (('Training', 'random weights'), 'be') (('selected entries', 'next matching'), 'overwrite') (('following table', 'score matrix K.'), 'show') (('Validation only setI', 'validation data'), 'discuss') (('Pseudo distance whale very easy still largest output best match', 'large values'), 'functionNot') (('presented', 'model'), 'be') (('minimum', 'subsequent blocks'), 'use') (('score Also matrix', 'epoch'), 'note') (('similarity', 'model current state'), 'compute') (('ResNet', 'VGG https more arxiv'), 'be') (('fluke', 'them'), 'rotate') (('notebook', 'essentially final retraining'), 'describe') (('Siamese Neural Network', 'two parts'), 'compose') (('I', 'list'), 'add') (('that', 'shift rotation'), 'perform') (('Keras generator', 'features'), 'be') (('that', 'maximally feature'), 'attempt') (('image Duplicate identification', 'here 1'), 'much') (('it', 'lapjv Linear Assignment Problem First solver'), 'position') (('which', 'guesses'), 'be') (('pictures', 'same whales'), 'compare') (('I', 'pooling layer'), 'use') (('feature 512 vectors', 'image once i.'), 'compute') (('K', 'training gradually harder cases'), 'as') (('I', 'training procedure'), 'use') (('which', 'ReLU otherwise when activation'), 'be') (('it', 'wave'), 'be') (('which', 'even when originally colored images'), 'be') (('square error', 'given threshold'), 'mean') (('blacklist', 'unhelpful training'), 'construct') (('score However matrix', 'randomized input'), 'randomize') (('image which', 'model image pair'), 'for') (('branch model', 'pooling intermediate layers'), 'compose') (('Perceptual same Hash', 'www'), 'have') (('which', 'image cropping'), 'martinpiotte') (('Image preprocessingTraining', 'following operations'), 'perform') (('BRANCH HEAD 384x384x1 96x96x64 24x24x128 12x12x256 6x6x384 6x6x512 512 MODEL', 'appropriate stride'), 'generate') (('It', 'www'), 'study') (('most', 'secret sauce'), 'be') (('i', 'picture'), 'associate') (('that', 'model accuracy'), 'highlight') (('I', 'ResNet https mostly www'), 'use') (('whenever picture', 'how whales'), 'learn') (('subset', 'image separately pairs'), 'be') (('Different whales', 'last epoch'), 'eliminate') (('manual judgement', 'training'), 'set') (('which', 'how matching zeros'), 'pass') (('speed', 'dataset'), 'include') (('it', 'data partitioning'), 'use') (('notebook', 'submission'), 'describe') (('produced', 'minimuzing score'), 'be') (('which', 'still complexity'), 'be') (('algorithm', 'confidence then low predictions'), 'insert') (('exactly thus margin', 'it'), 'martinpiotte') (('model', 'adversarial training'), 'forcus') (('over 93', 'submission'), 'show') (('2 5 1 96x96 Block 3 Block 4 24x24 Block 12x12 Block', 'max 2 2x2 pooling'), 'block') (('bootstrap data', 'more data'), 'train') (('that', 'Kaggle'), 'select') (('Blocks', 'convolution'), 'perform') (('significant information', 'initial 384x384x1 image'), 'note') (('Entry', 'similarity infty'), 'set') (('B same B', 'same A.'), 'be') (('4 central cells', '512'), 'construct') (('head model', 'negative weights'), 'discuss') (('two images', 'same whale'), 'compare') (('org wiki Assignment_problem algorithm', 'most difficult matching'), 'use') (('1 They', 'zero mean'), 'determise') (('training set one images', 'top candidates'), 'add') (('small it', 'runtime'), 'be') (('here hard examples', 'similar different whales'), 'be') (('high it', 'simultaneously other whales'), 'score') (('larger K', 'the more matching'), 'random') (('good standard model', 'overfitting'), 'train') (('absolute difference', 'y'), 'compute') (('Use', 'images unambiguous ids'), 'identify') (('single image', 'whales'), 'begin') (('It', 'GTX'), 'take') (('This', 'filtering process'), 'provide') (('HUGE', 'image'), 'be') (('mean square error', '0'), 'exceed') (('Duplicate interesting section', 'duplicate images'), 'describe') (('algorithm', 'n 3 exact solution'), 'spend') (('that', 'details'), 'pdf') (('Siamese Neural essentially Network', 'www'), 'be') (('they', 'same resolution'), 'be') (('limit', 'model choice'), 'available') (('what', 'model accuracy'), 'be') (('it', 'number just whales'), 'be') (('Other details', 'much smaller impact'), 'contribute') (('model', 'thus whales'), 'by') (('Whale Classification ModelThis notebook', '0'), 'describe') (('that', 'as well matching'), 'implement') (('smaller the more difficult pairing', 'model'), 'k') (('Matching examplesHalf examples', 'images'), 'whale') (('models', 'when two colored images'), 'convert') (('equal zero feature', 'similar large values'), 'be') (('same neural net', 'feature'), 'use') (('Skip', 'at least two images'), 'use') (('fileThe basic strategy', '10 4 submission'), 'generate') (('rectangle', 'bounding box https slightly computed www'), 'take') (('i', 'follow'), 'address') (('it', 'pairs'), 'compute') (('I', 'relatively small model'), 'be') (('center image', 'test transformation'), 'do') (('how 25460 images', 'image only 20913 distinct ids'), 'find') (('following algorith', 'pairs'), 'use') (('However this', 'real optimum'), 'find') (('model', 'whales'), 'want') (('idea', 'original'), 'suggest') (('scection', 'model'), 'describe') (('we', 'step'), 'randomize') (('that', 'distance measure'), 'feel') (('that', 'training'), 'be') (('features', 'matching whales'), 'verify') (('even it', 'default'), 'provide') (('it', 'rotate'), 'rotate') (('that', 'model still very perspective'), 'inspire') (('values', 'uniformly 0'), 'have') (('side right image', '180 degrees'), 'rotate') (('i', 'stderr annoying when keras'), 'show') (('we', 'two picture'), 'be') (('EnsembleThe best score', 'mpiotte standard'), 'obtain') (('I', 'custom package'), 'write') (('complexity', 'training larger datasets'), 'grow') (('CodeThe following', 'Keras model'), 'be') (('somewhat contrast size', 'legend'), 'be') (('1 2 where n', 'training set'), 'have') (('duplicate images', 'whale different ids'), 'be') (('learning rate', 'additional 150 epochs'), 'apply') (('we', 'bounding box'), 'shape') (('SIAMESE NEURAL NETWORK Complete model', 'head then resulting 512 vectors'), 'construct') (('convolutions', 'ResNet less memory then subsequent memory'), 'be') (('training algorithm', 'given time'), 'force') (('they', 'following criteria'), 'consider') (('it', 'notebook'), 'shuffle') (('Siamese Neural Network', 'training set'), 'want') (('negative examples', 'different whales'), 'appear') (('model', 'branch model'), 'be') (('VisualizationThis section', 'visualizations'), 'explore') (('X', 'case'), 'consider') (('that', 'linear standard model'), 'consist') (('code', 'Python https www'), 'modify') (('Convolutation Neural Network CNN', 'whale'), 'transform') (('too it', 'initially examples'), 'feed') (('It', 'whale random score'), 'be') (('less mathing', 'multiple caracteristic markings'), 'be') (('Block', 'similar VGG'), 'have') (('right image', 'data augmentation random transformation'), 'add') (('However scale', 'matching less costly randomized heuristic'), 'be') (('training', 'training tough cases'), 'last') (('same CNN', 'images'), 'use') (('Generation', 'score matrix'), 'be') (('Finally image', 'unit zero mean variance'), 'normalized') (('Adding', 'mpiotte'), 'generate') (('752 VGG', 'standard model0'), 'train') (('mpiotte standard model', 'score'), 'select') (('Similarly training', 'L2 regularization'), 'start') (('most likely whales', 'match'), 'identify') (('very dissimilar values', 'same time'), 'penalize') (('it', 'Obviously everything'), 'cover') (('Dictionary', 'model'), 'find') (('h2p dictionary unique image i', 'hash'), 'associate') (('test set', 'larger good score'), 'be') (('We', 'feature different values'), 'check') (('y', 'cost matrix'), 'compute') (('base model', 'bootstrap little under 3 days i7 8700 CPU'), 'describe') (('that', 'validation set'), 'achieve') (('you', 'most necessarily everything'), 'encourage') (('that', 'most 6 bits'), 'phash') (('typical approach', 'distance measure e.'), 'be') (('good quite as they', 'large slightly values'), 'L_1') (('whales', 'CNN'), 'use') (('This', 'image'), 'create') (('output', 'sigmoid activation'), 'be') (('training', 'duplicate images'), 'document') (('rectangular area', '2'), 'have') (('ids', 'whale'), 'list') (('model', 'different whales'), 'generate') ", "extra": "['biopsy of the greater curvature', 'test', 'procedure']"}