{"name": "eda toxicity of identities updated 29 4 ", "full_name": " h1 Toxicity EDA work in progress h2 Content h2 References h1 Target Distribution h1 Comment Length h1 No of Toxicity Annotators vs Comment Length h1 Annotators Distribution h1 Identity Distribution h1 Toxic vs Non toxic Words by Identity h3 To be continued let me know your thoughts and give me an upvote if you find the EDA useful interesting ", "stargazers_count": 0, "forks_count": 0, "description": "Content Target Distribution Comment Length No. of Toxicity Annotators vs Comment LengthThe main motivation is to check out how diligent the annotators are reliable or should I say less biased the labels of the comments are. Note that there are still some comments with length 1000. Perhaps we should take special note of the truncated comments The toxic may not occure before the truncation. Toxicity annotator distribution peaks near 0 which means a lot of the comments have very little annotators and hence less reliable Identity Distribution Key Findings Female Male and Christian are the most common identities For samples with identity_annotator_count 0 60 of them are not identified with the identities used for metrics Of the remaining 40 about 28 have a single identity 9 have double identities From the pair wise venn diagram it is observed that Male and Female tend to occur together Black and White as well. Just wondering if how diligent our annotators are lol Check out how the selected identities are related to each other Set the criteria for dataframe query get the concatenated string for both positive toxic and negative non toxic samples for an identity as negative samples are much larger in population there is a need to normalize them to the positive sample size Word Cloud plot Line plot of the term frequencies. To check if the non toxic word frequency distributions for the identity is similar the normalized frequency is plotted out of vocab word frequency are set to 0 Key Findings 30 of the samples identified with Homosexual_gay_or_lesbian Black White are toxic which is much higher than the remaining identities Toxic and Non toxic words frequency distribution for Male Female and Christian have more variations which means traditional bags of words approach Tf Idf Count Vector can probably distinguish between the toxic and non toxic samplesFor the remaining identities with similar words frequency distirbution for toxic and non toxic samples other strategies will likely be needed if you were to work on the sub group AUC. I m guessing the bin 0. Toxic vs Non toxic Words by IdentityFor each of the selected identities the word frequency distributions of the adjectives nouns proper nouns and verbs which are chosen as they tend to convey more semantics and sentiments are obtained and plotted. Key Findings Similar distribution for word and character level just the way English is A peak at character length 1000 and minimal data with length 1000. com theoviel improve your score with some text preprocessing notebook currently not used Target DistributionLooking at the raw target output with bins 10 distribution a few things can be observed Majority of the comments are non toxic target 0. If you are going to work on BPSN and BNSP metrics on individual identities it is wise to spend more time on the related pairs Black White Male Female as well as the most common identities. 5 will be a critical area to look at the comments in this bin are likely prone to misclassifications by the machine learning models. 5 where some annotators find the comments toxic we aren t sure how toxic they find these comments though. Top 10 words that are associated with the toxic samples are plotted. keep only alphabets and whitespaces. And after binarization our dataset becomes a highly imbalaned Comment LengthWe will analyse the comment length at word and character level. Annotators Distribution Key Findings Identity annotator distribution peaks at around 1200 1500 1700 probably an artifact of how the annotations were conducted. Key Findings As the comment gets longer the number of annotators get lesserBe wary of the correctness of long comments as these tend to have lesser annotators and thus a less reliable score. 02 cents on the data exploratory analysis any comments feedbacks are welcome. of Toxicity Annotators vs Comment Length Annotators Distribution New added on 29 Apr Identity Distribution New added on 29 Apr Toxic vs Non toxic Words by Identity New added on 29 Apr References Preprocessing https www. 1 Interesting bin 0. I shall work on a more in depth analysis on this bin in subsequent versions. Toxicity EDA work in progressBeginner here trying to give my 0. This is probably due to different character truncation selection during the data collection. Off my mind I can think of the following methods that could help Negation tagging Sentence structure analysis Dependency parsing To be continued let me know your thoughts and give me an upvote if you find the EDA useful interesting Note I m using the custom train test file I created which contains the original cols additional columns of POS tags Referene benchmark kernel for the competition remove other non alphabets symbols with space i. I would think that if a comment is labelled by more annotators its target score is more reliable. ", "id": "chewzy/eda-toxicity-of-identities-updated-29-4", "size": "4658", "language": "python", "html_url": "https://www.kaggle.com/code/chewzy/eda-toxicity-of-identities-updated-29-4", "git_url": "https://www.kaggle.com/code/chewzy/eda-toxicity-of-identities-updated-29-4", "script": "multiprocessing stopwords parallelize_dataframe matplotlib_venn clean_text convert_to_bool correct_spelling clean_special_chars seaborn numpy convert_dataframe_to_bool get_word_freq_and_plot matplotlib.pyplot WordCloud pandas FreqDist wordcloud venn_diagram_subplot tqdm Pool clean_contractions nltk.corpus text_processing get_pos_neg_string venn2 nltk preprocessing ", "entities": "(('which', 'space i.'), 'think') (('toxic', 'truncation'), 'take') (('negative samples', 'Word Cloud plot Line term frequencies'), 'lol') (('that', 'toxic samples'), 'Top') (('I', 'subsequent versions'), 'work') (('it', 'related pairs'), 'be') (('t how toxic they', 'comments'), '5') (('Male', 'together Black'), 'be') (('they', 'more semantics'), 'obtain') (('these', 'lesser annotators'), 'Findings') (('target score', 'more annotators'), 'think') (('Majority', 'comments'), 'improve') (('comments feedbacks', 'data exploratory analysis'), 'be') (('character just way English', '1000 minimal length'), 'distribution') (('5', 'machine learning models'), 'be') (('1500 1700 probably how annotations', 'Annotators Distribution Key Findings Identity annotator distribution around 1200'), 'peak') (('less labels', 'comments'), 'of') (('This', 'data collection'), 'be') (('dataset', 'word'), 'analyse') (('likely you', 'sub group'), 'plot') ", "extra": "['annotation', 'test', 'bag']"}