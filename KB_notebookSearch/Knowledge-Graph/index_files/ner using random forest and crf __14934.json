{"name": "ner using random forest and crf ", "full_name": " h1 Named Entity Recognition using GMB Groningen Meaning Bank corpus h2 Objective h2 Background h3 Understanding Named Entity Recognition h2 Data source h2 Analysis pipeline the OSEMN approach h3 Environment set up and loading dependencies h2 Obtain the data h2 Scrubbing Cleaning the data h2 Exploring Visualizing our data h2 Modeling the data h3 Performance metrics h3 Random Forest classifier h3 Conditional Random Fields classifier h3 Hyperparameter tuning using Randomized CV Search h2 Interpreting the results h2 References ", "stargazers_count": 0, "forks_count": 0, "description": "Quite surprising most of the words are tagged as outside of any chunk. Some models like decision trees and neural networks are often be able to get 100 accuracy on the training data but perform much worse on new data. It is a supervised learning method which has been proven to be better than the tree based models when it comes to NER. Transition features make sense at least model learned that I ENITITY must follow B ENTITY. Now we have a good model fit using the CRF classifier. remove the O tag from the list. Scrubbing Cleaning the data Initial data exploration and preparation for analysis Exploring Visualizing our data Basic EDA to understand the data Modeling the data Create classification models for NER Simple tree based model RandomForest State of the art CRF model Hyperparameter tuning Interpreting the results Analysing the performance and further steps to improve Environment set up and loading dependenciesAnaconda is used to do the analysis which is an easy to install free enterprise ready Python distribution for data analytics processing and scientific computing. Lets check for any missing values in the dataset A class to retrieve the sentences from the dataset This is how a sentence will look like. it is not common in this dataset to have a location right after an organization name i. True Positives TP These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. CRFsuite CRF models use two kinds of features state features and transition features. We divide the dataset into train and test sets Lets see how the input array looks like Random Forest classifier Lets check the performance Feature set Creating the train and test set Creating the CRF model We predcit using the same 5 fold cross validation Lets evaluate the mode Tuning the parameters manually setting c1 10 First we select all the tags that are relevant for us i. MONEY 175 million Canadian Dollars GBP 10. pdf NLTK book chapter 7 http www. The model is trying to understand the context as well and not just remember words. Now we will create the Randomized CV search model wherein we will use a modified F1 scorer model considering only the relevant labels define fixed parameters and parameters to search use the same metric for evaluation search Lets check the best estimated parameters and CV score We sort the tags a bit so that they appear in an orderly fashion in the classification report Now we create the model again using the best estimators. Maybe the model is again remembering words and not taking into the context information completely. html ELI5 report documentation https media. We can either work on this model alone by improving the features or ensembling it with a more contextual model or use a different model altogether. Secondly RF is considered one of the most accurate classifier available. We will try tuning the model manually to see if we can improve it. It is important that the classifier has proper features fed in to improve the performance. Now that we know the words and sentences lets try to understand what sort of words each tag contains. Now since we have good model we can take sneak peek into the model and see how it is classifying and what all weights are assigned. Gradient Descent will be used as an optimization function. The precision and recall values of most of the classes were 0. Using different algorithms in the CRF model. It is essential that the model is evaluated by these metrics per class to make sure we have a good model. Conditional Random Fields classifierA Conditional Random Field CRF is a standard model for predicting the most likely sequence of labels that correspond to a sequence of inputs. So our dataset mostly contains words related to geographical locations geopolitical entities and person names. Named Entity Recognition using GMB Groningen Meaning Bank corpus ObjectiveNamed Entity Recognition for annotated corpus using GMB Groningen Meaning Bank corpus for entity classification with enhanced and popular features by Natural Language Processing applied to the data set. Since NER usually deals with a huge corpus neural networks are very efficient in identifying patterns in the data provide a better model. It seems the features which require the model to take proper decisions are missing. In short we try to provide a sequence of features to the model for each word the sequence containing POS tags capitalisations type of word title etc. This can be broken down into two sub tasks identifying the boundaries of the NE and identifying its type. Random Forest classifier We will use 5 fold cross validation as an input parameter to the classifier i. Exploring Visualizing our data Before going further we will try to understand what the dataset is all about and what all the features mean. However the precision and recall metrics of the classes individually have not improved. we will divide the dataset into 5 subsets and train test on them. We will understand them in detail in the exploration step. False Positives FP When actual class is no and predicted class is yes. The model is basically memorizing words and tags which will not suffice. Current implementation considers only 2 hyper parameters in CV search however the CRF model offers more parameters which can be further tuned to improve on the performance. It requires huge computing resources to achieve this performance boost unfortunately. html Annotated GMB corpus dataset https www. edu sekine papers li07. com all accuracy precision recall f1 score interpretation of performance measures Data analysis Data visualisation Let us take a sneak peak into the dataset first The dataset does not have any header currently. We can use the first row as a header as it has the relevant headings. Currently it uses Gradient Descent but Stochastic Gradient Descent can be used to create a faster model. de fileadmin user_upload fachgebiete plattner teaching NaturalLanguageProcessing NLP2015 NLP04_POS_NER. Recall TP TP FN F1 score F1 Score is the weighted average of Precision and Recall. Lets find the number of words in the dataset Lets visualize how the sentences are distributed by their length Lets find out the longest sentence length in the dataset Words tagged as B org Words tagged as I org Words tagged as B per Words tagged as I per Words tagged as B geo Words tagged as I geo Words distribution across Tags Words distribution across Tags without O tag Words distribution across POS Simple feature map to feed arrays into the classifier. Whereas a discrete classifier predicts a label for a single sample without considering neighboring samples a CRF can take context into account e. Random Forest being one of the most popular tree based models can learn the underlying rules according to which terms are tagged. pdf POS tagging and NER https hpi. It is the harmonic mean of the both Precision and Recall begin equation F1 Score 2 frac Recall Precision Recall Precision end equation For a decent classifier we would prefer high precision and recall values. False Negatives FN When actual class is yes but predicted class in no. This will help us in understanding what each tag type and sub type represents. In particular a tagger can be built that labels each word in a sentence using the IOB format where chunks are labelled by their appropriate type. It also learned that some transitions are unlikely e. We will make the first row as the heading remove the first row and re index the dataset We have 66161 samples and 5 features. The model is not perfect and it can be further improved by various ways Adding more features to the model e. The dataset has the following columns or features Index Index numbers for each word Numeric type Sentence The number of sentences in the dataset We will find the number of sentences below Numeric type Word The words in the sentence Character type POS Parts Of Speech tags these are tags given to the type of words as per the Penn TreeBank Tagset Categorical type Tag The tags given to each word based on the IOB tagging system described above Target variable Categorical type Since the dataset is annotated with POS and Tags we will build a simple class to combine the words into a sentence. It is a popular algorithm for parameter estimation in machine learning. Classification reports are used to obtain the values of these metrics in a text format per class. Since we are dealing with Information Extraction we will use the following metrics to evaluate the models Precision Recall F1 score The metrics mentioned above are calculated using True False positives and True False negatives respectively. Interpreting the results Let s try to understand the report in brief. The context information behind each word needs to be fed to the model as well so that the predictions are more accurate. Compared to the Random Forest classifier the CRF classifier did better as the scores have improved. This will help us understand the extract of the dataset. Lets check the dataset again without the O tags. In order to use CRF we will enhance the feature set and create more features which can be used by the model to predict the tags correctly. Also we add new features such as upper lower digit title etc. President Obama Park Street etc. Background Understanding Named Entity RecognitionNamed Entity Recognition or Named Entity Recognition and Classification NERC is probably the first step towards information extraction that seeks to locate and classify named entities in text into pre defined categories. It seems most of the sentences are 20 30 words long and the distribution is normal. We can also see that B ORG I ORG has positive weights which makes sense as the First names are always followed by the inner names the same applies for organisations. Analysis pipeline the OSEMN approach Obtain the data The dataset is an extract of the GMB corpus which is tagged annotated. Although we have a good average score the model performed quite badly. Now we shall start with the modeling part where we create new features create the model and evaluate it on the metrics stated above. combination of words with gives a proper meaning e. 40 PERCENT twenty pct 18. Dependencies used are below Obtain the data Scrubbing Cleaning the data Seems the dataset does not contain any missing values and we are good to proceed to the exploration step where we will try to understand the data more. We chose Random Forest because of the following reasons Firstly RF is able to automatically construct correlation paths from the feature space i. Since we need to take into account the context as well we create features which will provide consecutive POS tags for each word. WHO PERSON Eddy Bonte President Obama LOCATION Murray River Mount Everest DATE June 2008 06 29 TIME two fifty a m 1 30 p. GMB is a fairly large corpus with a lot of annotations. for each word and also consider the consecutive words in the list. Maybe we can do it computationally and get a better model. 75 FACILITY Washington Monument Stonehenge GPE South East Asia Midlothian The goal of a named entity recognition NER system is to identify all textual mentions of the named entities. Named entities are definite noun phrases that refer to specific types of individuals such as organizations persons dates and so on. We can expect that O I ENTITY transitions to have large negative weights because they are impossible. References SKLearn CRF Documentation https sklearn crfsuite. I ORG B LOC has a negative weight. Using state of the art deep learning models using Keras or Tensor Flow deep learning models are known to outperform every other model when it comes to huge datasets therefore they can be used to create a superior NER classifier. Modeling the dataWith the basic EDA done and understanding the dataset we can move to the modeling stage. Since the problem statement is a simple classification problem we will start with a simple tree based model Random Forest using a simple feature map. The average score has dropped but the individual precision and recall scores have improved. We can see that there is a scope of improving the model. It is in tab separated text file. It is not a gold standard corpus meaning that it s not completely human annotated and it s not considered 100 correct. Hyperparameter tuning using Randomized CV Search Now we have a good model with decent precision and recall scores for each class. These words can be considered as fillers and their presence might impact the classifier performance as well. True Negatives TN These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. It is possible to improve the model performance from a current 95 F1 score to about 98 using the above techniques. We can fit this model and use it to predict tags given a sentence. Using a larger corpus from the GMB dataset. decision rules that correspond to the translation rules that we intend to capture. Therefore we will train on one subset and test on the other and repeat for every subset so that the classifier classifies correctly on average and the performance estimate is not overly optimistic. Commonly Used Types of Named Entity NE Type Examples ORGANIZATION Georgia Pacific Corp. ELI5 report has built in support for several ML frameworks and provides a way to explain black box models. This is important in order to understand how the classifiers will perform and help us interpret the results. Therefore this score takes both false positives and false negatives into account. Performance metricsBefore we move to the modeling part it is important to understand the performance metrics on the basis of which the models will be evaluated. The corpus is created by using already existed annotators and then corrected by humans where needed. Unfortunately GMB is not perfect. Simple tree based models have been proven to provide decent performance in building NERC systems. pdf Survey of different NER techniques https nlp. com abhinavwalia95 entity annotated corpus home Understanding Model performance https blog. Using a LSTM neural network and creating a ensemble model with CRF. the linear chain CRF which is popular in natural language processing predicts sequences of labels for sequences of input samples. Precision TP TP FP Recall Sensitivity Recall is the ratio of correctly predicted positive observations to the all observations in actual class yes. Precision Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. Named entity recognition is a task that is well suited to the type of classifier based approach. We will be using the LGBFS algorithm Gradient descent using the L BFGS method and it works best using a limited amount of computer memory. The IOB Tagging system contains tags of the form B CHUNK_TYPE for the word in the Beginning chunk I CHUNK_TYPE for words Inside the chunk O Outside any chunkThe IOB tags are further classified into the following classes geo Geographical Entity org Organization per Person gpe Geopolitical Entity tim Time indicator art Artifact eve Event nat Natural Phenomenon Data sourceThe dataset an extract from GMB corpus which is tagged annotated and built specifically to train the classifier to predict named entities such as name location etc. ", "id": "shoumikgoswami/ner-using-random-forest-and-crf", "size": "14934", "language": "python", "html_url": "https://www.kaggle.com/code/shoumikgoswami/ner-using-random-forest-and-crf", "git_url": "https://www.kaggle.com/code/shoumikgoswami/ner-using-random-forest-and-crf", "script": "sklearn.metrics __init__ cross_val_score scorers seaborn numpy sent2labels flat_classification_report sklearn.grid_search cross_val_predict sklearn.ensemble metrics RandomForestClassifier feature_map matplotlib.pyplot pandas classification_report getsentence(object) word2features sklearn_crfsuite.metrics sklearn.cross_validation CRF RandomizedSearchCV sent2features sklearn_crfsuite make_scorer ", "entities": "(('which', 'basically words'), 'memorize') (('We', 'different model'), 'work') (('where we', 'data'), 'be') (('that', 'inputs'), 'be') (('which', 'name such location'), 'contain') (('Now we', 'CRF classifier'), 'have') (('I', 'B ENTITY'), 'make') (('Maybe we', 'computationally better model'), 'do') (('we', 'high precision values'), 'be') (('GMB', 'annotations'), 'be') (('that', 'i.'), 'divide') (('It', 'boost'), 'require') (('ELI5 report', 'box black models'), 'build') (('This', 'type'), 'break') (('Quite surprising most', 'as outside chunk'), 'tag') (('20 30 words distribution', 'sentences'), 'seem') (('we', 'word title etc'), 'try') (('So dataset', 'geographical locations geopolitical entities'), 'contain') (('Random Forest We', 'classifier i.'), 'classifier') (('Secondly RF', 'most accurate classifier'), 'consider') (('Also we', 'digit title such upper lower etc'), 'add') (('We', 'exploration step'), 'understand') (('which', 'proper decisions'), 'seem') (('models', 'which'), 'move') (('we', 'translation rules'), 'rule') (('FACILITY Washington Monument Stonehenge GPE South East Asia 75 goal', 'named entities'), 'Midlothian') (('We', 'sentence'), 'fit') (('Lets', 'O again tags'), 'check') (('Stochastic Gradient Descent', 'faster model'), 'use') (('us', 'results'), 'be') (('as well predictions', 'model'), 'need') (('that', 'classifier based approach'), 'be') (('where chunks', 'appropriate type'), 'build') (('Negatives False When actual class', 'no'), 'fn') (('noun definite that', 'organizations persons such dates'), 'be') (('Simple tree based models', 'NERC systems'), 'prove') (('it', 'computer memory'), 'use') (('precision values', 'classes'), 'be') (('We', '66161 samples'), 'make') (('tag type', 'what'), 'help') (('CRF Documentation https', 'sklearn crfsuite'), 'sklearn') (('therefore they', 'NER superior classifier'), 'use') (('which', 'further performance'), 'consider') (('WHO', 'Eddy Bonte President Obama LOCATION Murray River Mount Everest DATE'), 'person') (('Gradient Descent', 'optimization function'), 'use') (('It', 'above techniques'), 'be') (('which', 'GMB corpus'), 'pipeline') (('fillers', 'classifier performance'), 'consider') (('Firstly RF', 'feature space i.'), 'choose') (('Recall TP TP FN F1 score F1 Score', 'weighted Precision'), 'be') (('Precision Recall F1 metrics', 'True False negatives'), 'use') (('model', 'good average score'), 'have') (('better scores', 'Random Forest classifier'), 'do') (('First names', 'same organisations'), 'see') (('that', 'defined categories'), 'be') (('they', 'large negative weights'), 'expect') (('CRF', 'account e.'), 'predict') (('Now we', 'recall class'), 'tuning') (('when it', 'NER'), 'be') (('tag', 'words'), 'now') (('corpus', 'then humans'), 'create') (('Precision TP TP FP Recall Sensitivity Recall', 'actual class'), 'be') (('which', 'data analytics processing'), 'scrub') (('html ELI5', 'documentation https media'), 'report') (('features', 'all what'), 'try') (('However precision metrics', 'classes'), 'improve') (('models', 'new data'), 'be') (('model', 'as well just words'), 'try') (('classifier', 'performance'), 'be') (('Classification reports', 'class'), 'use') (('sentence', 'how'), 'check') (('first dataset', 'header'), 'com') (('Named', 'data set'), 'apply') (('com abhinavwalia95', 'corpus home Understanding Model performance https entity blog'), 'annotate') (('classifier classifies', 'subset'), 'train') (('where we', 'metrics'), 'start') (('weights', 'model'), 'take') (('We', 'model'), 'see') (('terms', 'which'), 'learn') (('which', 'word'), 'create') (('which', 'tags'), 'enhance') (('we', 'modeling stage'), 'model') (('s', 'brief'), 'let') (('we', 'train them'), 'divide') (('we', 'good model'), 'be') (('value', 'predicted class'), 'TN') (('Precision Precision', 'positive observations'), 'be') (('It', 'machine learning'), 'be') (('I', 'classifier'), 'find') (('manually we', 'it'), 'try') (('Maybe model', 'context information'), 'remember') (('us', 'dataset'), 'help') (('Now we', 'again best estimators'), 'create') (('we', 'sentence'), 'have') (('CRFsuite CRF models', 'features state features'), 'use') (('it', 'model e.'), 'be') (('it', 'relevant headings'), 'use') (('it', 'organization right name'), 'be') (('classification simple we', 'Random feature simple map'), 'be') (('Therefore score', 'false account'), 'take') (('NER', 'better model'), 'be') (('value', 'predicted class'), 'tp') (('which', 'input samples'), 'predict') ", "extra": "['annotation', 'organization', 'test']"}