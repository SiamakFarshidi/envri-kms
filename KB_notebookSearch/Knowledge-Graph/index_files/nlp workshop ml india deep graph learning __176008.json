{"name": "nlp workshop ml india deep graph learning ", "full_name": " h2 Deep Graph Embeddings h2 SDNE Structural Deep Network Embeddings h3 SDNE algorithm principle h4 Similarity definition h4 2nd order similarity optimization goal h2 LINE Large Scale Information Network Embedding h3 first order proximity h3 second order proximity h2 Graph Neural Networks h2 Vanilla GNN VGCN h2 Laplacian GNN LAPGCN h2 Spline GNN SGCN h2 ChebNets GNN ChebGCN h2 Conclusion of Spectral GCN h2 GCN using Torch Geometric h2 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "A descriptive view of the same is provided here We define that the nodes that don t have predecessors are in layer 0. Vanilla GNN VGCN The most important part of spectral convolution arises from Vanilla GNNs. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. shape 1 X_n for i in range feature_dim X_n. number_of_nodes A preprocess_adj A X np. Notice two important thing here first the weights for all the neighboring nodes is the same they don t have individual weights. com drive 14OvFnAXggxB8vM4e8vSURUp1TaKnovzX usp sharing scrollTo imGrKO5YH11 Torch Geometric https pytorch geometric. shape 1 print X X print X type X print A type A print feature_dim feature_dim print type model_input 0 type model_input 1 ReLU AXW X_emb LSTM 3235 return_sequences True X_emb print Xemb X_emb H Reshape X_emb. io pytorch Deep Learning en week13 13 2 GCN using Torch GeometricTorch Geometric is a library which has different modules for creating GCNs and variations of Graph networks and can be found in the links provided below. The second order similarity measures the similarity of the neighbor sets of two vertices. This kind of neighborhood aggregation is called Graph Convolutional Networks GCN look here for a good introduction. laplacian_matrix L inputs A L A nx. Deep Graph Embeddings This is a notebook which walks through 2 of the most popular deep learning based graph embeddings SDNE and LINE. This can actually indicate that 5 and 6 are similar and 2 Order similarity is used to describe this relationship. com method line These are very high order embeddings and used for capturing representations on exclusively large networks. SDNE Structural Deep Network Embeddings SDNE algorithm principle Similarity definitionThe definition of similarity in SDNE is the same as LINE. As shown in the figure above although there are no straight edges between 5 and 6 they have many similar neighbor vertices 1 2 3 4. This represents the eigenvectors which can be added independently instead of taking the entire laplacian at one time. number_of_nodes A preprocess_adj A print X X print X type X print A type A print feature_dim feature_dim print type model_input 0 type model_input 1 ReLU AXW X_emb LSTM 3235 return_sequences True X_emb print Xemb X_emb H Reshape X_emb. One problem here is that due to the sparseness of the graph the number of non zero elements in the adjacency matrix S is far fewer than zero elements so for the neural network as long as all output 0 can achieve a good effect this is not ours want. is formally defined asOrder p_u w_ u 1. Information is also encoded in the structure of the graph. Some points Since no Laplacian eigen decomposition is used all operations are in the spatial not spectral domain Another drawback of LapGCNs is that convolutional layers involve sparse linear operations which GPU s are not fully optimized for. html ConclusionYou have seen how to apply GNNs to real world problems and in particular how they can effectively be used for boosting a model s performance. To calculate the embeddings of layer k we weight the average embeddings of layer k 1 and put it into an activation function. shape 1 X_emb print type Adj H X_in y_train list set list labels print len y_train A nx. Standard ConvNets produce anisotropic filters because Euclidean grids have direction while Spectral GCNs compute isotropic filters since graphs have no notion of direction up down left right. Second when we calculate the embedding for a node v we also want to include the features of this node so we add self loops to every node. shape 1 dropout_ratecreate_spline2_reg 2. from_scipy_sparse_matrix L L nx. By looking at friends of a person it is often possible to get some insight into this person. One method given in the article is to use a weighted loss function which has a higher penalty coefficient for non zero elements. Then we can compile information about the friends of a person. The normalization factor is not the same for all nodes but depends on their individual number of neighbors. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. The two distributions are the adjacency matrix and the dot product of node embedding. Laplacian GNN LAPGCN This is another variation of GCNs where instead of a laplacian we take the normalized laplacian matrix as the adjacency matrix and the feature matrix as inputs. coo_matrix degree_vals idx idx shape n n L D A L nx. train 1 0 1 def train self epochs 10 initial_epoch 0 verbose 1 return tf. io graph convolutional networks blog https atcold. This may be useful for example in recommender systems where we have movie graphs and user graphs. 2nd order similarity optimization goal L2nd i 1n x i xi 22 Here we use the adjacency matrix of the graph for input. This work helped inspire interest in Graph Learning as a niche in Machine Learning and eventually Deep Learning in specific. LINE Large Scale Information Network Embedding first order proximityThe first order similarity is used to describe the local similarity between pairs of vertices in the graph and the formal description is if u u v vThere are straight edges between them then the edges are right w_ uv wuv It is the similarity between two vertices. Simply put the first order similarity measures the similarity between two adjacent vertex pairs. shape 1 feature_dim 32 y_train. This also makes sense intuitively when characterizing a node the neighbors do play an important role but the node itself is also important. The connection between smoothness in frequency domain and localization in space is based on Parseval s Identity also Heisenberg uncertainty principle smaller derivative of spectral filter smoother function smaller variance of spatial filter localization In this case we wrap the vanilla GCN with an additional spline functionality by decomposing the laplacian to its diagonals 1 spline. We can first of all look just at one person itself. w_ u V pu wu 1. read_csv Input data files are available in the read only. number_of_nodes D scipy. Then information about friends of friends and so on. Let s look at a simple example to make things clearer. For example when we look at a social network every node can be a person with a certain age gender interests political views etc. These architectures are different from the previous notebooks owing to the fact that these use first and second order proximity to determine the node representations. The features of the node itself are therefore multiplied by the same weights as all of its neighbors. If u uversus v vIf there are no identical neighbor vertices the second order similarity is 0. So how does all of this come together as a neural network Let s continue with our example of a social circle. number_of_nodes degree_vals np. wu V Represents vertices u u1st order similarity with all other vertices then u uversus v vThe second order similarity can be passed p_u pu with p_v pv The similarity is expressed. If there are no straight edges the first order similarity is 0. KL Divergence is an important similarity metric in information theory and entropy. Nevertheless the simplicity and effectiveness of LINE are just a couple reasons why it was the most cited paper on WWW of 2015. Resource blog https towardsdatascience. The embeddings of these nodes are just their features. shape 1 A create_spline A print X X print X type X print A type A print feature_dim feature_dim print type model_input 0 type model_input 1 Add aggregation Step 1 Add self loops Step 2 Multiply with weights Step 3 Calculate the normalization Step 4 Propagate the embeddings to the next layer Normalize node features. The original repository is here https github. Here the spectral filter weights are initialized using keras tf. com overview of deep learning on graph embeddings 4305c10ad4a4 Graph Neural NetworksThe major difference between graph data and normal data we encounter in other machine learning tasks is that we can derive knowledge from two sources Just like in other machine learning applications every node has a set of features. For the i th vertex we have x_i s_i xi si Every s_i si Both contain the neighbor structure information of vertex i so this reconstruction process can make vertices with similar structures have similar embedding representation vectors. Before we come to the implementation I want to introduce a slight modification that has shown to regularly outperform normal graph nets. Blog https atcold. The labels have to be one hot encoded to maintain the dimensions of the inputs. com paper chebnet efficient and stable constructions of Conclusion of Spectral GCNSpectral GCN is the core part of GCN where we analysed laplacian transforms in the light of convolutions. Papers with Code https paperswithcode. io pytorch Deep Learning en week13 13 2 The steps to produce this include creating the adjacency matrix representation along with the node features from the inputs. The nodes are not ordered in any way. shape 1 dropout_rate 0. second order proximityIs only 1st order similarity enough Obviously not enough. nodes n graph. The classification uses the spectral graph convolution aspect where the formula is as follows A Vanilla GCN GNN utilizes the graph laplacian not normalized laplacian along with a spectral filter and recursively augments the weights of the next layer based on the previous layer. The rest of the part involves multiplying the Laplacian tuple node_features adjacency matrix with the spectral filter kernel and applying an activation over the result. The algorithm is used in probabilistic generative models like Variational Autoencoders which embed inputs of an autoencoder into a latent space which becomes the distribution. degree node for node in list graph. For implementing the Chebnets an additional function has been added which recursively fills the Laplacian with the cheb polynomials upto k degree. The graph below shows a small friend group where an edge between two nodes means that these two people are friends with each other. Both the papers associated with the embeddings are present here SDNE https paperswithcode. This matrix is then processed and additional layers such as Embedding Layer LSTM can be added to perform node classification. shape 1 X_emb print type Adj H X_in y_train list set list labels print len y_train inputs A L A nx. The goal of LINE is to minimize the difference between the input and embedding distributions. fit model_input y_train X. KLDivergence y_pred y_true self. adjacency_matrix graph nodelist range graph. com rusty1s pytorch_geometric Colab https colab. There are two major differences image 6 We have just one set of weights in each layer so no different weights W and B anymore. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session self. The model inputs are in the form of node features adjacency matrix representation and the outputs are one hot encoded node labels. io en latest notes colabs. So far we learned to know how vanilla graph nets work. Generally a softmax activation is applied for classifying the outputs according to the labels. The following are the resources which are helpful blog https tkipf. Implementation Details For creating Line embeddings we use the default SDNE script and convert the reconstruction loss autoencoder loss for SDNE to a KL divergence based loss. train 1 0 1 def train self epochs 10 initial_epoch 0 verbose 1 pip install tensorflow ReLU AXW X_emb LSTM 3235 return_sequences True X_emb print Xemb X_emb H Reshape X_emb. html Torch Geometric Examples https pytorch geometric. number_of_nodes idx np. Where L is the Laplacian matrix corresponding to the graph L D S L D S D is the degree matrix of the vertices in the graph and S is the adjacency matrix. shape 1 print X feature_dim A. asarray X_n model_input A A print X type X print A type A model GCN A. Since the algorithm has to define new functions for each increasing order of proximity LINE doesn t perform very well if the application needs an understanding of node community structure. com method sdne LINE https paperswithcode. As shown in the figure above there is a straight edge between 6 and 7 and the edge weight is larger it is considered that the two are similar and the first order similarity is higher but there is no between 5 and 6 If the edges are directly connected the first order similarity between the two is 0. We can extend ChebNets to multiple graphs using a 2D spectral filter. io en latest notes introduction. The following image depicts how a particular node learns from the neighbours when passed through h number of deep learning layers. The rest of the part is similar to the previous case. 1st order similarity optimization goalCapture Reconstruction loss of the predicted and true values2nd order similarity optimization goalUse the trace of the laplacian matrix and normalize the resultsThe loss function can make the embedding vectors corresponding to two adjacent vertices in the graph close in the hidden space. The rest of the code segment remains the same. This is achieved using KL divergence LINE defines two joint probability distributions for each pair of nodes then minimizes the KL divergence of the distributions. io pytorch Deep Learning en week13 13 2 ChebNets GNN ChebGCN This is one of the most important parts of spectral GCN where Chebyshev polynomials are used instead of the laplacian. Here for simplicity k has been chosen as 2. This is basically the idea of a graph net we aggregate information of neighbors and neighbors of neighbors etc. These are scalable representations which are based on regressed adjacency properties and laplacian maps. The first order proximity suggests that nodes can be related based on adjacency and the second order suggests that nodes are characterised based on the neighbourhood of the nodes. Overall optimization goalThe loss function of joint optimization is Lmix L2nd \u03b1L1st \u03bdLreg L_ reg Lreg Is the regularization term alpha \u03b1To control the parameters of the first order lossThe second order proximity is preserved by passing the adjacency matrix of te graph through an unsupervised autoencoder which has a built in reconstruction loss function it must minimize. ChebNets are GCNs that can be used for any arbitrary graph domain but the limitation is that they are isotropic. compile optimizer adam loss categorical_crossentropy weighted_metrics categorical_crossentropy acc print type model_input 0 type model_input 1 model. Spline GNN SGCN Spline GCN involve computing smooth spectral filters to get localized spatial filters. ", "id": "abhilash1910/nlp-workshop-ml-india-deep-graph-learning", "size": "176008", "language": "python", "html_url": "https://www.kaggle.com/code/abhilash1910/nlp-workshop-ml-india-deep-graph-learning", "git_url": "https://www.kaggle.com/code/abhilash1910/nlp-workshop-ml-india-deep-graph-learning", "script": "MessagePassing __init__ tensorflow.keras.initializers torch_geometric.utils Reshape Net(torch.nn.Module) plot_dataset chebyshev_polynomial reconstruction_loss plotly.offline tensorflow.keras.layers GraphConvolution(tf.keras.layers.Layer) tensorflow.keras.regularizers Embedding train model Dropout LSTM tensorflow.keras.models forward TSNE Layer iplot build numpy download_plotlyjs recursion plotly.graph_objs create_model Zeros Input chebyshev_distance LINE() get_config create_spline networkx Identity cal test LabelEncoder matplotlib.pyplot l2 tensorflow sklearn.manifold pandas call get_embeddings torch_geometric.datasets loss_laplace message torch.nn.functional plotter GCNConv(MessagePassing) encode_onehot normalize_adj init_notebook_mode Model Loss() GCN kldivergence_loss sklearn.preprocessing preprocess_adj degree Planetoid SDNE() torch_geometric.nn glorot_uniform node_level_embedding add_self_loops ", "entities": "(('L D S L D S D', 'graph'), 'be') (('order directly first similarity', 'two'), 'be') (('KL Divergence', 'similarity information important theory'), 'be') (('nodes', 'nodes'), 'suggest') (('we', 'diagonals 1 spline'), 'base') (('it', 'reconstruction loss function'), 'be') (('which', 'latent space'), 'use') (('where we', 'convolutions'), 'be') (('rest', 'previous case'), 'be') (('which', 'graph embeddings most popular deep learning SDNE'), 'embedding') (('where we', 'movie graphs'), 'be') (('they', 'graph arbitrary domain'), 'be') (('two distributions', 'adjacency dot node embedding'), 'be') (('Here we', 'input'), 'goal') (('scalable which', 'adjacency regressed properties'), 'be') (('train', 'train self 1 1 def 10 0 1 return'), 'verbose') (('probability two joint distributions', 'distributions'), 'achieve') (('Chebyshev where polynomials', 'instead laplacian'), 'pytorch') (('Spline GNN SGCN Spline GCN', 'localized spatial filters'), 'involve') (('GPU', 'which'), 'be') (('first order', 'node representations'), 'be') (('this', '0 good effect'), 'be') (('Graph Convolutional Networks GCN', 'here good introduction'), 'call') (('we', 'feature inputs'), 'LAPGCN') (('filter Here spectral weights', 'keras'), 'initialize') (('which', 'links'), 'pytorch') (('number_of_nodes preprocess_adj A', 'feature_dim print 1 AXW True X_emb feature_dim type model_input 0 model_input 3235 print'), 'a') (('embeddings', 'nodes'), 'be') (('com method These', 'exclusively large networks'), 'line') (('features', 'neighbors'), 'multiply') (('node', 'political views'), 'be') (('that', 'graph regularly normal nets'), 'come') (('t', 'session outside current self'), 'list') (('things', 'simple example'), 'let') (('We', '2D spectral filter'), 'extend') (('rest', 'code segment'), 'remain') (('Information', 'graph'), 'encode') (('softmax Generally activation', 'labels'), 'apply') (('Order 2 similarity', 'relationship'), 'indicate') (('asarray model_input A print X type X', 'type model'), 'X_n') (('is', 'LSTM node classification'), 'add') (('vertices', 'embedding representation similar vectors'), 'have') (('SDNE Structural Deep Network Embeddings SDNE algorithm principle Similarity definitionThe definition', 'LINE'), 'be') (('node', 'important role'), 'make') (('we', 'KL divergence based loss'), 'use') (('following image', 'learning deep layers'), 'depict') (('s', 'social circle'), 'come') (('we', 'neighbors'), 'be') (('list labels', 'print len'), 'type') (('which', 'zero elements'), 'be') (('Then we', 'person'), 'compile') (('rest', 'result'), 'involve') (('outputs', 'adjacency matrix representation'), 'be') (('which', 'one time'), 'represent') (('particular how they', 'effectively performance'), 'see') (('we', 'activation function'), 'calculate') (('It', 'two vertices'), 'order') (('X print X X', 'feature_dim print 1 AXW True X_emb feature_dim type model_input 0 model_input 3235 print'), 'shape') (('normalization factor', 'neighbors'), 'be') (('two people', 'other'), 'show') (('it', 'person'), 'by') (('neighbor order identical second similarity', 'u vIf'), 'be') (('order second similarity', 'two vertices'), 'measure') (('work', 'Machine Learning'), 'help') (('labels', 'inputs'), 'have') (('embedding vectors', 'close hidden space'), 'optimization') (('13 week13 2 steps', 'inputs'), 'pytorch') (('Vanilla GCN GNN', 'previous layer'), 'use') (('k', '2'), 'choose') (('most important part', 'Vanilla GNNs'), 'VGCN') (('similarity', 'p_v'), 'vertice') (('node', 'features'), 'embedding') (('We', 'just one person'), 'look') (('they', 'don individual weights'), 'notice') (('don t', 'layer'), 'provide') (('list labels', 'print len y_train L A nx'), 'type') (('so we', 'node'), 'want') (('papers', 'embeddings'), 'be') (('normalization', 'next layer'), 'shape') (('very well application', 'node community structure'), 'perform') (('read_csv Input data files', 'read'), 'be') (('which', 'k degree'), 'add') (('differences two major 6 We', 'layer'), 'be') (('It', 'kaggle python Docker image https github'), 'come') (('just a couple why it', '2015'), 'be') (('goal', 'input distributions'), 'be') (('they', '1 2 3 4'), 'have') (('graphs', 'direction'), 'produce') ", "extra": "['biopsy of the greater curvature', 'gender', 'test']"}