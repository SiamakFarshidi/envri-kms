{"name": "tps 08 21 catboost ", "full_name": " h2 Data import h1 EDA h1 Data preparation h1 Hyperparameters optimization h1 Model training h2 Feature importances h2 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "578784014838337 bagging_temperature 0. to_datetime train date_time format Y m d H M S test date_time pd. sum axis 1 df custom_feat_1 df_copy. mean axis 1 df custom_feat_2 df_copy. max axis 1 df custom_feat_4 df_copy. params print Best score study. trials print Best trial parameters study. As you can see the correlation is between 0. 606620127979116 bagging_temperature 7. 010526847803225213 l2_leaf_reg 7. copy X_test add_new_features X_test. loc valid_idx study optuna. create_study direction minimize study. drop id axis 1 columns test. 5813008056988401 random_strength 1. 518949583881732 random_strength 1. Data import EDA There are no missing value in the both datasets. copy df custom_feat_0 df_copy. copy A set of hyperparameters to optimize by optuna Exact time split StratifiedShuffleSplit n_splits 1 test_size 0. 060436568484918 depth 2 grow_policy Depthwise leaf_estimation_method Gradient fold_rmse np. array oof_preds valid_idx. As you can see f1 feature has the smallest amount of unique values 289. There are some features with relatively low correlation with target value even comparing with other features Lets visualize each feature vs loss. Model training Feature importances Submission linear algebra data processing CSV file I O e. split X target_bins X_train X_valid X. columns X_test pd. best_value Hyperparameters optimized by Optuna cb_params iterations 10000 learning_rate 0. to_datetime test date_time format Y m d H M S Colors to be used for plots Plot dataframe Mask to hide upper right part of plot as it is a duplicate Making a plot Calculating edges of target bins to be used for stratified split x_scaler MinMaxScaler X pd. 7705601193056997 depth 7 grow_policy Depthwise leaf_estimation_method Gradient cb_params iterations 10000 learning_rate 0. reshape 1 1 print f Trees model. drop id loss axis 1. Lets check target distribution. columns y_scaler MinMaxScaler y pd. sqrt mean_squared_error y_scaler. 030108080370377578 l2_leaf_reg 4. drop id loss axis 1 columns train. read_csv Pandas setting to display more dataset rows and columns Input data files are available in the read only. 03 which is pretty small. min axis 1 df custom_feat_3 df_copy. optimize lambda trial train_model_optuna trial X_train X_valid y_train y_valid n_trials 100 print Number of finished trials len study. reshape 1 1 y_scaler. Lets look at feature correlation. The datasets are pretty well balanced. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session nrows 10000 train date_time pd. median axis 1 return df X add_new_features X. So I don t think any feature should be treated as categorical. flatten Scaling data def add_new_features df Adds custom features to a given dataframe df_copy df. So the features are weakly correlated. loc valid_idx y_train y_valid y. Data preparation Hyperparameters optimization The code below is commented in order to save runtime. Lets check feature values distribution in the both datasets. 2 random_state 42 for train_idx valid_idx in split. ", "id": "maximkazantsev/tps-08-21-catboost", "size": "818", "language": "python", "html_url": "https://www.kaggle.com/code/maximkazantsev/tps-08-21-catboost", "git_url": "https://www.kaggle.com/code/maximkazantsev/tps-08-21-catboost", "script": "sklearn.metrics StandardScaler sklearn.model_selection CatBoostRegressor mean_squared_error seaborn numpy matplotlib.pyplot train_model_optuna StratifiedKFold catboost sklearn.preprocessing pandas add_new_features StratifiedShuffleSplit MinMaxScaler ", "entities": "(('correlation', '0'), 'be') (('Lets', 'datasets'), 'check') (('read_csv Pandas', 'columns Input data read'), 'be') (('Lets', 'loss'), 'be') (('Scaling data flatten df', 'given df'), 'def') (('Submission', 'linear algebra data CSV file'), 'importance') (('t', 'train date_time 10000 pd'), 'list') (('code', 'runtime'), 'optimization') (('f1 feature', 'unique values'), 'have') (('it', 'stratified split x_scaler MinMaxScaler X pd'), 'format') (('best_value Hyperparameters', 'Optuna'), 'optimize') ", "extra": "['test', 'bag']"}