{"name": "wheat detection finalnb ", "full_name": " h1 Check data anamolies h1 Validating images in the directory h1 Make a label to classify data based on availability h1 Applying Faster RCNN Credit to the training code goes to RockyXu66 Git Repository h1 Model Inspection Graphs h1 Submission Code h2 All are JPGs h1 Couldn t complete from here ", "stargazers_count": 0, "forks_count": 0, "description": "randint 0 6 0 tall_imgs filename imageset trainval else tall_imgs filename imageset test make sure the bg class is last in the list Block 1 Block 2 Block 3 Block 4 Block 5 x MaxPooling2D 2 2 strides 2 2 name block5_pool x out_roi_pool. Don t need rpn probs in the later process img cv2. savefig main_title Print the process or not Name of base network Setting for data augmentation Anchor box scales Note that if im_size is smaller anchor_box_scales should be scaled Original anchor_box_scales in the paper is 128 256 512 Anchor box ratios Size to resize the smallest side of the image Original setting in paper is 600. Config setting Model Inspection Graphs Submission Code All are JPGs Couldn t complete from here. arange 0 r_epochs record_df curr_loss r plt. arange 0 r_epochs record_df loss_class_cls r plt. arange 0 r_epochs record_df loss_class_regr c plt. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. flush Augment with horizontal flips in training. Set to 300 in here to save training time image channel wise mean to subtract number of ROIs at once stride at the RPN this depends on the network configuration scaling the stdev overlaps for RPN overlaps for classifier ROIs placeholder for the class mapping automatically generated by the parser x 0 is image with shape rows cols channels x 1 is roi with shape num_rois 4 with ordering x y w h Resized roi of the image to pooling size 7x7 Reshape to 1 num_rois pool_size pool_size nb_channels Might be 1 4 7 7 3 permute_dimensions is similar to transpose print Parsing annotation files Print process Make sure the info saved in annotation file matching the format path_filename x1 y1 x2 y2 class_name Note tOne path_filename might has several classes class_name tx1 y1 x2 y2 are the pixel value of the origial image not the ratio value t x1 y1 top left coordinates x2 y2 bottom right coordinates x1 y1 t t t t t t t t t t t t t t t t t t t t t t t t t t t t t x2 y2 if np. Applying Faster RCNN Credit to the training code goes to RockyXu66 Git Repository. npz allow_pickle True arr_0. classes_count Car 2383 Mobile phone 1108 Person 3745 bg 0 class_mapping Person 0 Car 1 Mobile phone 2 bg 3 Save the configuration Shuffle the images with seed print y_rpn_cls for possible pos anchor. title total_loss plt. arange 0 r_epochs record_df elapsed_time r plt. title elapsed_time plt. arange 0 r_epochs record_df loss_rpn_cls b plt. write str idx r sys. classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes one hot code for bboxes from above x_roi X corresponding labels and corresponding gt bboxes 3 in here 3 in here A. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. format regr pos_regr 0 0 pos_regr 1 0 cv2. 5 ya downscale iy 0. width num_anchors Might be 4 18 25 18 if resized image is 400 width and 300 A is the coordinates for 9 anchors for every point in the feature map all 18x25x9 4050 anchors cooridnates anchor_x 128 1 16 8 width of current anchor anchor_y 128 2 16 16 height of current anchor curr_layer 0 8 9 anchors the Kth anchor of all position in the feature map 9th in total shape 18 25 4 shape 4 18 25 Create 18x25 mesh grid For every point in x there are all the y points and vice versa X. format mean_overlapping_bboxes epoch_length Generate X x_img and label Y y_rpn_cls y_rpn_regr Train rpn model and get loss value _ loss_rpn_cls loss_rpn_regr Get predicted rpn from rpn model rpn_cls rpn_regr R bboxes shape 300 4 Convert rpn layer to roi bboxes note calc_iou converts from x1 y1 x2 y2 to x y w h format X2 bboxes that iou C. to_csv modified_train. format cls pos_cls 0 0 pos_cls 1 0 print y_rpn_regr for positive anchor. imread row image_id sys. reset_index drop True. putText img gt bbox gt_x1 gt_y1 5 cv2. shape 18 25 Y. subplot 1 2 2 plt. title loss plt. csv file to record losses acc and mAP If this is a continued training load the trained model from before Load the records Training setting Just of sharing the kernel running with 2 epoch you try with min 20 epochs print Average number of overlapping bounding boxes from RPN for previous iterations. putText img pos anchor bbox str i 1 center 0 int anc_w 2 center 1 int anc_h 2 5 cv2. subplot 1 2 1 plt. com 9 this is a model that holds both the RPN and the classifier used to load save weights for the models Because the google colab can only run the session several hours one time then you need to connect again we need to save the model and load the model to continue training If this is the begin of the training load the pre traind base network such as vgg 16 Create the record. classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes Y1 one hot code for bboxes from above x_roi X Y2 corresponding labels and corresponding gt bboxes If X2 is None means there are no matching bboxes Find out the positive anchors and negative anchors If number of positive anchors is larger than 4 2 2 randomly choose 2 pos samples Randomly choose num_rois num_pos neg samples Save all the pos and neg samples in sel_samples in the extreme case where num_rois 1 we pick a random pos or neg sample training_data X X2 sel_samples labels Y1 sel_samples Y2 sel_samples X img_data resized image X2 sel_samples num_rois 4 in here bboxes which contains selected neg and pos Y1 sel_samples one hot encode for num_rois bboxes which contains selected neg and pos Y2 sel_samples labels and gt bboxes for num_rois bboxes which contains selected neg and pos plt. flush img cv2. 5 w h are the width and height of ground truth bbox wa ha are the width and height of anchor bboxe tx x xa wa ty y ya ha tw log w wa th log h ha all GT boxes should be mapped to an anchor box so we keep track of which anchor box was best we set the anchor to positive if the IOU is 0. read_csv Input data files are available in the read only. rename columns width count. shape 18 25 Calculate anchor position and size for each feature map point Top left x coordinate Top left y coordinate width of current anchor height of current anchor Apply regression to x y w and h if there is rpn regression layer Avoid width and height exceeding 1 Convert x y w h to x1 y1 x2 y2 x1 y1 is top left coordinate x2 y2 is bottom right coordinate Avoid bboxes drawn outside the feature map shape 4050 4 shape 4050 Find out the bboxes which is illegal and delete them from bboxes list Apply non_max_suppression Only extract the bboxes. tolist classes_count np. FONT_HERSHEY_DUPLEX 0. Check data anamolies Validating images in the directory Make a label to classify data based on availability. csv index False plt. 3 and RGB x is the difference between true value and predicted vaue absolute value of x If x_abs C. sort_values by count ascending False. 7 it does not matter if there was another better box it just indicates overlap we update the regression layer target if this IOU is the best for the current x y and anchor position if the IOU is 0. Create the config This step will spend some time to load the data train_imgs np. 7 color 1 Add text Draw positive anchors according to the y_rpn_regr print Center position of positive anchor center cv2. figure figsize 15 5 plt. arange 0 r_epochs record_df loss_rpn_regr g plt. show a and b should be x1 y1 x2 y2. Augment with 90 degree rotations in training. shape 1 num_rois channels pool_size pool_size num_rois 4 7x7 roi pooling Flatten the convlutional layer and connected to 2 FC and 2 dropout There are two output layer out_class softmax acivation function for classify the class name of the object out_regr linear activation function for bboxes coordinates regression note no regression target for bg class a and b should be x1 y1 x2 y2 128 256 512 1 1 1 2 sqrt 2 2 sqrt 2 1 3x3 9 calculate the output map size based on the network architecture 3 initialise empty output objectives get the GT box coordinates and resize to account for image resizing get the GT box coordinates and resize to account for image resizing rpn ground truth x coordinates of the current anchor box t ignore boxes that go across image boundaries t t t t t y coordinates of the current anchor box ignore boxes that go across image boundaries bbox_type indicates whether an anchor should be a target Initialize with negative this is the best IOU for the x y coord and the current anchor note that this is different from the best IOU for a GT bbox get IOU of the current GT box and the current anchor box calculate the regression targets if they will be needed x y are the center point of ground truth bbox xa ya are the center point of anchor bbox xa downscale ix 0. 5 color 1 define the base network VGG here can be Resnet50 Inception etc rm r github. Augment with vertical flips in training. shape 4 feature_map. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session pip install upgrade tensorflow df_train. groupby image_id width. npz allow_pickle True arr_0 class_mapping np. npz allow_pickle True arr_0 e. ", "id": "hpant438/wheat-detection-finalnb", "size": "344", "language": "python", "html_url": "https://www.kaggle.com/code/hpant438/wheat-detection-finalnb", "git_url": "https://www.kaggle.com/code/hpant438/wheat-detection-finalnb", "script": "Flatten sklearn.metrics keras.engine.topology nn_base Dropout data_preperation get_file collections pyplot build non_max_suppression_fast intersection pprint ceil layer_utils InteractiveShell classifier_layer backend as K Config rpn_loss_cls_fixed_num keras.objectives = model_classifier.train_on_batch([X get_new_img_size optparse pyplot as plt union calc_iou get_source_inputs average_precision_score plot_img Layer seaborn numpy scipy.stats check_image_size Input get_config iou tqdm.notebook tensorflow keras.utils pandas RoiPoolingConv(Layer) keras.optimizers categorical_crossentropy Counter * K.sum(y_true[ keras.engine class_loss_regr_fixed_num OptionParser pandas.util.testing Model get_output_length matplotlib rpn_to_roi keras.models calc_rpn class_loss_regr IPython.core.interactiveshell compute_output_shape SGD InputSpec Adam Conv2D X2[ rpn_loss_regr_fixed_num get_data RMSprop apply_regr GlobalMaxPooling2D generic_utils draw_rect keras.utils.data_utils rpn_loss_regr augment MaxPooling2D backend apply_regr_np __init__ keras.layers * K.mean(categorical_crossentropy(y_true[0 GlobalAveragePooling2D class_loss_cls rpn_layer draw_random_image_and_box TimeDistributed initializers get_anchor_gt matplotlib.pyplot Dense call get_img_output_length tqdm regularizers rpn_loss_cls keras math ", "entities": "(('step', 'data train_imgs np'), 'spend') (('Original setting', 'paper'), 'Print') (('Check data', 'availability'), 'anamolie') (('3', 'x'), 'be') (('Don t', 'later process'), 'need') (('that', 'C.'), 'Generate') (('t', 'pip df_train'), 'list') (('ratio t x1 y1 top', 'x2 y2 x1 y1 t t t t t t t t t t t t t t t t t t t t t t t t t t t t t x2 left bottom right y2'), 'Set') (('5 color 1 define', 'Inception rm r here github'), 'be') (('Config', 'Model Inspection Graphs Submission Code'), 'be') (('base pre network', '16 record'), 'com') (('IOU', 'anchor'), 'be') (('read_csv Input data files', 'read'), 'be') (('1 16 8 width', 'x'), 'be') (('bg class', 'list'), 'imageset') (('classes_count Mobile 2383 phone', 'pos possible anchor'), 'Car') (('which', 'selected neg'), 'y1') (('It', 'kaggle python Docker image https github'), 'come') (('Applying', 'RockyXu66 Git Repository'), 'go') (('which', 'Only bboxes'), 'shape') (('you', 'previous iterations'), 'file') (('IOU', 'current x y position'), '7') (('y', 'anchor bbox center xa'), 'channel') ", "extra": "['annotation', 'test']"}