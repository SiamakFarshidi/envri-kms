{"name": "epv peq aula 3 classifica o ", "full_name": " h2 Escola Piloto Virtual PEQ COPPE UFRJ h2 Data Science e Machine Learning na Pr\u00e1tica Introdu\u00e7\u00e3o e Aplica\u00e7\u00f5es na Ind\u00fastria de Processos h1 Videoaulas h1 O conjunto de dados MNIST h2 Importando dados h2 Separando treino valida\u00e7\u00e3o h1 Rede MLP no scikit learn h1 O que s\u00e3o redes neurais h2 Perc\u00e9ptron h2 Treinamento das redes h3 Detalhes sobre a minimiza\u00e7\u00e3o h4 V\u00e1rios algoritmos h4 Minilotes e \u00e9pocas h4 Regulariza\u00e7\u00e3o h2 TensorFlow Playground h1 Rede MLP no Keras h1 Aprendizado Profundo h3 Problema 1 treinamentos excessivamente lentos h3 Problema 2 desaparecimento explos\u00e3o de gradientes h4 Novas fun\u00e7\u00f5es de ativa\u00e7\u00e3o h4 Novas estrat\u00e9gias de inicializa\u00e7\u00e3o dos pesos h4 Normaliza\u00e7\u00e3o por lote h3 Problema 3 escassez de dados para treinamento h3 Problema 4 risco de sobreajuste por conta do excesso de par\u00e2metros h1 Redes convolucionais h2 Funcionamento da rede convolucional h1 Rede convolucional no Keras h1 Detec\u00e7\u00e3o de falhas em um processo de fundi\u00e7\u00e3o h2 O processo h2 Importando os dados h2 Gerando dados com Keras h2 Modelagem h2 An\u00e1lise dos resultados h1 Conclus\u00e3o ", "stargazers_count": 0, "forks_count": 0, "description": "Se voc\u00ea executar este notebook provavelmente vai obter resultados diferentes dos apresentados aqui principalmente quanto ao n\u00famero de classifica\u00e7\u00f5es incorretas no conjunto de teste que varia de 0 a 4 em geral. org wiki Universal_approximation_theorem cujo enunciado afirma que redes com uma camada oculta conseguem aproximar qualquer fun\u00e7\u00e3o cont\u00ednua redes com mais de uma camada oculta conseguem aproximar qualquer fun\u00e7\u00e3o. Poder\u00edamos tentar corrigir a preval\u00eancia de imagens de pe\u00e7as defeituosas em rela\u00e7\u00e3o \u00e0s normais mas como a diferen\u00e7a n\u00e3o \u00e9 t\u00e3o grande isso n\u00e3o ser\u00e1 feito. Uma m\u00e9trica tamb\u00e9m muito usada \u00e9 a F_1 https en. org wiki Transfer_learning em que modelos treinados em uma situa\u00e7\u00e3o com muitos dados podem ser aplicados a cen\u00e1rios similares com menor disponibilidade de dados. io api preprocessing image imagedatagenerator class dispon\u00edvel no m\u00f3dulo keras. O objetivo \u00e9 atualizar mais os par\u00e2metros que ainda n\u00e3o foram muito atualizados. html Parece um bom desempenho Com um pequeno grau de sobreajuste mas nada grave. pdf propuseram uma nova inicializa\u00e7\u00e3o conhecida como inicializa\u00e7\u00e3o de Glorot ou de Xavier usada como default no keras https keras. ModelagemA estrat\u00e9gia aqui ser\u00e1 criar m\u00faltiplas redes e juntar os v\u00e1rios resultados em uma \u00fanica predi\u00e7\u00e3o. O aprendizado da rede \u00e9 efetuado por meio do ajuste desses pesos. O que s\u00e3o redes neurais Redes neurais artificiais https en. A estrat\u00e9gia de revis\u00e3o \u00e9 particularmente importante nesse processo porque mesmo baixas taxas de erro de menos de 1 podem resultar em consider\u00e1veis perdas econ\u00f4micas j\u00e1 que o custo de um \u00fanico falso positivo ou negativo pode ser alto como discutido aqui https www. Na estrat\u00e9gia a seguir conhecida como zero padding ou same padding adicionam se zeros \u00e0s bordas do mapa de entrada de modo que o mapa de sa\u00edda possua o mesmo formato da entrada. org wiki Multilayer_perceptron e convolucionais https en. A segunda parte consiste de uma camada densa totalmente conectada como as da rede MLP e \u00e9 respons\u00e1vel por efetuar a classifica\u00e7\u00e3o. A solu\u00e7\u00e3o para esse problema veio de um resultado muito importante chamado de Teorema da Aproxima\u00e7\u00e3o Universal https en. Cada elemento de um mapa de caracter\u00edsticas corresponde a um neur\u00f4nio. org wiki Quasi Newton_method ou meta heur\u00edsticas https en. Os m\u00faltiplos mapas de caracter\u00edsticas podem ser alimentados a uma nova camada convolucional resultando em novos mapas como ilustrado Cada neur\u00f4nio de um mapa de caracter\u00edsticas da sa\u00edda conecta se a apenas uma por\u00e7\u00e3o dos neur\u00f4nios dos mapas da entrada. \u00c9 uma estrat\u00e9gia para impedir a morte de neur\u00f4nios ou seja quando a sa\u00edda do neur\u00f4nio \u00e9 nula para todas as amostras de treinamento situa\u00e7\u00e3o comum em redes com ativa\u00e7\u00e3o ReLU. Cada neur\u00f4nio individual apresenta um funcionamento bem simples. Este trabalho reviveu o interesse da comunidade cient\u00edfica e tecnol\u00f3gica nas redes neurais. Tipicamente utilizam se v\u00e1rios pares de camadas convolu\u00e7\u00e3o pooling o que define a profundidade da rede. Nas redes neurais os neur\u00f4nios s\u00e3o dispostos em camadas organizadas de acordo com a arquitetura da rede. O conjunto de dados MNISTO conjunto de dados MNIST https en. Flatten https keras. pdf foi constru\u00edda para implementar o modelo img src https upload. 09820 por exemplo em que voc\u00ea pode conferir dicas e boas pr\u00e1ticas para essa tarefa. Desde ent\u00e3o v\u00e1rias boas pr\u00e1ticas v\u00eam sendo propostas para garantir que haja uma efetiva propaga\u00e7\u00e3o dos sinais pela rede tanto as predi\u00e7\u00f5es no sentido direto quanto os gradientes no sentido reverso dentre as quais citamos novas fun\u00e7\u00f5es de ativa\u00e7\u00e3o novas estrat\u00e9gias de inicializa\u00e7\u00e3o dos pesos normaliza\u00e7\u00e3o por lote. org wiki Gradient_descent para a minimiza\u00e7\u00e3o do erro. De modo a fazer com que o formato seja o mesmo pode se utilizar estrat\u00e9gias de preenchimento padding. com activation functions explained. 2012 https arxiv. 03167 e desde ent\u00e3o aparece com frequ\u00eancia nas arquiteturas das redes neurais modernas. com ravirajsinh45 real life industrial dataset of casting product. com ravirajsinh45 real life industrial dataset of casting product proveniente da companhia indiana Pilot Technocast http www. Em redes recorrentes \u00e9 comum o problema inverso a explos\u00e3o dos gradientes. Quando a se torna um par\u00e2metro a ser aprendido pela rede como sugerido por HE et al. Para discuss\u00f5es interessantes a respeito consulte SANTUNKAR et al. Podemos encarar a fun\u00e7\u00e3o log\u00edstica como uma esp\u00e9cie de suaviza\u00e7\u00e3o da fun\u00e7\u00e3o de Heaviside Detalhes sobre a minimiza\u00e7\u00e3o V\u00e1rios algoritmosExistem v\u00e1rias t\u00e9cnicas pertencentes \u00e0 fam\u00edlia de algoritmos de gradiente descendente. Como temos certeza sobre a classe verdadeira nas amostras de treinamento na verdade os vetores verdadeiros de probabilidades mathbf y tem a forma 0 0. org wiki Enrico_Fermi em 1953 ao criticar a complexidade de um modelo em espec\u00edfico cunhou uma frase cl\u00e1ssica sobre o assunto With four parameters I can fit an elephant and with five I can make him wiggle his trunk. Obs h\u00e1 algumas refer\u00eancias SMITH 2018 https arxiv. Em outras palavras \u00e9 preciso escolher o valor de corte que separa cada uma das duas classes. Problema 2 desaparecimento explos\u00e3o de gradientesMesmo com o grande ganho de efici\u00eancia introduzido pelas GPU s s\u00e9rias dificuldades num\u00e9ricas relativas ao treinamento de redes profundas persistiram. org wiki Data_augmentation que consiste em alimentar aos modelos n\u00e3o apenas as imagens originais e sim um conjunto potencialmente muito maior composto por imagens com pequenas varia\u00e7\u00f5es das originais. 02 0 0 0 0 o que significa dizer que a rede prev\u00ea que 98 de certeza que o valor \u00e9 3. org wiki Precision_and_recall. Na pr\u00f3xima c\u00e9lula aplicamos a fun\u00e7\u00e3o para calcular as probabilidades relativas ao conjunto de teste Para calcular as m\u00e9tricas de classifica\u00e7\u00e3o \u00e9 necess\u00e1rio definir qual faixa de probabilidades est\u00e1 associada a cada classe. O elemento da diagonal principal C_ ii corresponde \u00e0 quantidade de predi\u00e7\u00f5es corretas da classe i. O objetivo \u00e9 que voc\u00ea ganhe familiaridade com a biblioteca e autonomia para navegar na API e implementar os recursos em seus modelos. Usando o matplotlib para dar uma olhada nas imagens Como os dados s\u00e3o provenientes da competi\u00e7\u00e3o Digit Recognizer https www. io api layers convolution_layers convolution2d implementa uma camada convolucional. Obs pode se ir al\u00e9m do gradiente descendente e utilizar outras t\u00e9cnicas como as de segunda ordem que utilizam informa\u00e7\u00f5es de derivada segunda exemplos s\u00e3o os m\u00e9todos de Newton https en. org wiki Entropia_cruzada vem da teoria da informa\u00e7\u00e3o https pt. Ser\u00e3o dois estudos de caso ambos constituindo aplica\u00e7\u00f5es do tipo vis\u00e3o computacional https pt. org wiki MNIST_database \u00e9 composto de 70000 imagens de d\u00edgitos escritos por funcion\u00e1rios do United States Census Bureau https en. org wiki Metaheuristic. A normaliza\u00e7\u00e3o por lote possibilita que se utilize maiores taxas de aprendizado diminui a sensibilidade da rede em rela\u00e7\u00e3o \u00e0 inicializa\u00e7\u00e3o e atua como um regularizador aumentando a capacidade de generaliza\u00e7\u00e3o do modelo. A figura a seguir ilustra a aplica\u00e7\u00e3o do algoritmo a uma superf\u00edcie partindo de tr\u00eas pontos distintos Um dos pontos iniciais levou a um m\u00ednimo local enquanto os outros dois chegaram ao que parece ser o m\u00ednimo global no dom\u00ednio apresentado. Mas particularmente verifiquei dois pontos consistentes ao longo de todas as execu\u00e7\u00f5es que efetuei a precis\u00e3o \u00e9 mais alta que a revoca\u00e7\u00e3o na maior parte da faixa de probabilidades de corte as probabilidades das amostras classificadas incorretamente encontram se perto de 0 5 em geral abaixo devido \u00e0 revoca\u00e7\u00e3o ser menor que precis\u00e3o. O processoO conjunto de dados https www. Boa jornada Conclus\u00e3oNesta aula aprendemos o que h\u00e1 de mais moderno e vibrante no campo de aprendizado de m\u00e1quina os modelos de redes neurais https en. No segundo gr\u00e1fico temos a fun\u00e7\u00e3o ReLU a mais popular hoje em dia por dois motivos n\u00e3o satura no sentido positivo e \u00e9 r\u00e1pida de calcular. org wiki Learning_rate e constitui talvez o hiperpar\u00e2metro mais importante do modelo. Agora vamos dar uma olhada em um estudo de caso de natureza industrial. Ao longo da estrutura da rede conforme se avan\u00e7a nas camadas mais profundas os padr\u00f5es locais simples v\u00e3o sendo agregados e tornando se cada vez mais complexos. O sucesso da proposta veio da introdu\u00e7\u00e3o dos par\u00e2metros gamma e beta que possibilitam que as sa\u00eddas y_i tenham qualquer m\u00e9dia e vari\u00e2ncia na verdade esses valores s\u00e3o determinados de maneira \u00f3tima pela rede. Como veremos a seguir existem v\u00e1rias possibilidades para a escolha da fun\u00e7\u00e3o de ativa\u00e7\u00e3o. No mundo acad\u00eamico construiu se um certo consenso de que modelos com muitos par\u00e2metros s\u00e3o inerentemente ruins por conta da complexidade matem\u00e1tica e da excessiva capacidade de adapta\u00e7\u00e3o resultantes. org wiki Ensemble_learning. Na segunda o passe reverso calcula se o quanto cada camada contribui para o erro da camada anterior. 01852 a fun\u00e7\u00e3o recebe o nome de PReLU Parametric ReLU. io api layers initializers glorotuniform class e ideal para fun\u00e7\u00f5es sigm\u00f3ides https en. Para criar uma rede neural no keras podemos inicializar o modelo utilizando a classe keras. S\u00e3o 10 neur\u00f4nios na camada de sa\u00edda correspondentes \u00e0s 10 classes aos 10 poss\u00edveis d\u00edgitos. org wiki Artificial_neural_network em particular de aprendizado profundo https en. press v9 glorot10a glorot10a. O termo momento vem da analogia com a F\u00edsica. org wiki Heaviside_step_function. Quanto maior a vari\u00e2ncia dos gradientes passados de um dado par\u00e2metro mais o m\u00e9todo diminui a taxa de aprendizado correspondente. S\u00e3o 200 neur\u00f4nios na camada oculta. Ap\u00f3s o treinamento na etapa de predi\u00e7\u00e3o novos dados de entrada s\u00e3o alimentados e caso o aprendizado tenha sido efetivo sa\u00eddas com baixo erro ou seja pr\u00f3ximas das corretas ser\u00e3o geradas pela rede. H\u00e1 margem para melhorias nos resultados e nas efici\u00eancias computacionais. De maneira geral o procedimento de criar um modelo composto por m\u00faltiplos submodelos \u00e9 conhecido como ensembling https en. Com a propaga\u00e7\u00e3o de um sinal praticamente constante ao longo da rede o gradiente na etapa de retropropaga\u00e7\u00e3o o sinal reverso resulta praticamente nulo. pdf propuseram o revolucion\u00e1rio algoritmo da retropropaga\u00e7\u00e3o do erro https en. A precis\u00e3o \u00e9 uma medida de exatid\u00e3o das detec\u00e7\u00f5es e tem a ver com qualidade a revoca\u00e7\u00e3o \u00e9 uma medida de completude das detec\u00e7\u00f5es e tem a ver com quantidade. Um exerc\u00edcio interessante \u00e9 predizer voc\u00ea mesma os d\u00edgitos de cada uma dessas imagens e calcular sua acur\u00e1cia humana. Esse fato pode ser usado para montar uma estrat\u00e9gia de revis\u00e3o dos resultados do modelo. Fazendo analogia com o modelo linear o vi\u00e9s \u00e9 como se fosse o intercepto da reta. A maneira mais intuitiva de efetuar esse procedimento \u00e9 Classe 0 p leq0 5 Classe 1 p 0 5. Para uma leitura sobre aprendizado profundo complementar a este notebook recomendo o artigo de LECUN et al. Na c\u00e9lula abaixo selecionamos as amostras que resultam em probabilidades distantes de 0 ou 1 o qu\u00e3o distantes podem ser par\u00e2metros ajust\u00e1veis Percebe se que apenas uma pequena fra\u00e7\u00e3o das amostras encontra se na faixa considerada suspeita no caso acima 0 2 p 0 8. io api e aprenda como utilizar os recursos explicados aqui fun\u00e7\u00f5es de ativa\u00e7\u00e3o inicializa\u00e7\u00e3o de pesos normaliza\u00e7\u00e3o por lote e dropout. org wiki Logistic_function. com en us research blog turing nlg a 17 billion parameter language model by microsoft de par\u00e2metros. O m\u00e9todo Adam KINGMA e BA 2014 https arxiv. Para tal utilizaremos a fun\u00e7\u00e3o visualizeImageBatch Visualizando um minilote Agora vamos criar o gerador das imagens de teste definido de maneira um pouco diferente Especificamos o tamanho do lote como o tamanho total do conjunto de teste j\u00e1 que a predi\u00e7\u00e3o de teste ser\u00e1 efetuada com todos os dados de uma vez ap\u00f3s o t\u00e9rmino do treinamento. Calculando as m\u00e9tricas Duas outras m\u00e9tricas adequadas para esse problema s\u00e3o a precis\u00e3o P e a revoca\u00e7\u00e3o R https en. Predi\u00e7\u00f5es cujas probabilidades estejam muito distantes de 0 ou 1 ou seja em que o modelo n\u00e3o est\u00e1 muito confiante quanto ao pertencimento a alguma das classes podem ser encaminhadas para verifica\u00e7\u00e3o humana. escassez de dados para treinamento 4. Outra vantagem quando a 1 a fun\u00e7\u00e3o \u00e9 suave https pt. org wiki Teoria_da_informa C3 A7 C3 A3o e mede a dist\u00e2ncia entre duas distribui\u00e7\u00f5es de probabilidade. pdf em um trabalho muito influente identificaram os principais culpados para o fen\u00f4meno do desaparecimento dos gradientes a at\u00e9 ent\u00e3o popular fun\u00e7\u00e3o de ativa\u00e7\u00e3o log\u00edstica a at\u00e9 ent\u00e3o popular estrat\u00e9gia de inicializa\u00e7\u00e3o dos pesos com uma distribui\u00e7\u00e3o gaussiana de m\u00e9dia 0 e desvio padr\u00e3o 1. Funcionamento da rede convolucional Para alimentar uma imagem \u00e0 rede \u00e9 necess\u00e1rio represent\u00e1 la como um tensor. Como fizemos no caso MNIST \u00e9 \u00fatil verificar as propor\u00e7\u00f5es das classes nos diferentes conjuntos As propor\u00e7\u00f5es parecem consistentes entre os conjuntos. io api models sequential mas ao inv\u00e9s de usar a fun\u00e7\u00e3o add dessa vez forneceremos as camadas como uma lista \u00e0 inicializa\u00e7\u00e3o do modelo A classe keras. Quanto mais fundo se avan\u00e7a nas camadas do modelo mais os padr\u00f5es v\u00e3o se tornando complexos e abstratos. A regulariza\u00e7\u00e3o L_1 \u00e9 an\u00e1loga \u00e0 L_2 no entanto usando valores absolutos no lugar dos quadrados. Por isso as redes convolucionais s\u00e3o t\u00e3o utilizadas para processar imagens. Normaliza\u00e7\u00e3o por loteA normaliza\u00e7\u00e3o por lote https en. Esses ajustes s\u00e3o o mecanismo por meio do qual ocorre o processo de aprendizado em seres vivos. No terceiro gr\u00e1fico temos a fun\u00e7\u00e3o Leaky ReLU que introduz uma inclina\u00e7\u00e3o na regi\u00e3o negativa da ReLU. No caso abaixo o callback early_stop definido a partir da classe EarlyStopping https keras. gradientes que desaparecem ou explodem ao serem retropropagados pela rede 3. A explica\u00e7\u00e3o mais fact\u00edvel parece ser uma diminui\u00e7\u00e3o das influ\u00eancias dos pesos uns sobre os outros ao longo das v\u00e1rias camadas o que suaviza a fun\u00e7\u00e3o objetivo e facilita a tarefa dos m\u00e9todos de minimiza\u00e7\u00e3o. edu hinton absps pdp8. Quanto mais perto de 0 est\u00e1 a probabilidade mais a rede acredita que a imagem pertence \u00e0 classe 0 quanto mais perto de 1 mais ela acredita que a classe \u00e9 a 1. Podemos proceder para a etapa de modelagem. O atributo history do objeto H possibilita que analisemos a evolu\u00e7\u00e3o das m\u00e9tricas e valores da fun\u00e7\u00e3o perda para os dados de treino e valida\u00e7\u00e3o Os resultados j\u00e1 s\u00e3o bons mesmo na primeira \u00e9poca e v\u00e3o melhorando lenta e continuamente conforme o treinamento evolui. A estrat\u00e9gia de momentos https www. Seu peso \u00e9 denotado por w_0 b_k e chamado de vi\u00e9s bias do neur\u00f4nio k. com a visual explanation of gradient descent methods momentum adagrad rmsprop adam f898b102325c p\u00e1ginas. Para que o algoritmo da retropropaga\u00e7\u00e3o do erro funcionasse os autores precisaram propor uma nova fun\u00e7\u00e3o de ativa\u00e7\u00e3o varphi u frac 1 1 exp u a fun\u00e7\u00e3o de ativa\u00e7\u00e3o log\u00edstica https en. Para duas distribui\u00e7\u00f5es y e p a entropia cruzada \u00e9 escrita como sum_ j y_ j log p_ j Nas aplica\u00e7\u00f5es de redes neurais para classifica\u00e7\u00e3o as distribui\u00e7\u00f5es y e p s\u00e3o substitu\u00eddas pelos vetores mathbf y o vetor verdadeiro de probabilidades de cada classe e mathbf p o vetor de probabilidades de cada classe predito pela rede. Para melhorar o resultado utilizaremos redes com m\u00faltiplas camadas que constituem o principal objeto de estudo do aprendizado profundo deep learning https pt. org wiki Softmax_function a \u00faltima etapa da rede \u00e9 a respons\u00e1vel por transformar a sa\u00edda em um vetor de probabilidades um vetor de soma 1 composto por n\u00fameros entre 0 e 1 sigma mathbf u _i frac e u_i sum_ j 1 K e u_j Dessa maneira a entrada da rede \u00e9 uma imagem de 784 pixels e a sa\u00edda \u00e9 um vetor de 10 elementos cada um deles descrevendo a probabilidade da imagem corresponder a um dos d\u00edgitos entre 0 e 9. Uma camada densa \u00e9 aquela em que os neur\u00f4nios est\u00e3o conectados a todos os neur\u00f4nios da camada anterior. Separando treino valida\u00e7\u00e3oPara separar treino valida\u00e7\u00e3o usaremos a fun\u00e7\u00e3o train_test_split \u00c9 \u00fatil verificar se as propor\u00e7\u00f5es de d\u00edgitos no treino e na valida\u00e7\u00e3o s\u00e3o parecidas As propor\u00e7\u00f5es entre classes parecem bem balanceadas em ambos os conjuntos. Essa gera\u00e7\u00e3o acontece em tempo real durante o treinamento a cada itera\u00e7\u00e3o um minilote de tensores \u00e9 fornecido ao modelo para que um passo da minimiza\u00e7\u00e3o seja efetuado. org wiki Activation_function e \u00e9 respons\u00e1vel por introduzir n\u00e3o linearidade nas redes. Problema 1 treinamentos excessivamente lentos RAINA et al. io api callbacks para parada antecipada. io api layers regularization_layers dropout implementa uma camada de dropout que no caso desliga 20 dos neur\u00f4nios antes de fornec\u00ea los \u00e0 camada de sa\u00edda. Os argumentos do m\u00e9todo flow_from_directory especificam que ser\u00e3o gerados minilotes contendo 32 imagens de tamanho 300x300 na escala de cinza ou seja com apenas 1 canal de cores. No treinamento das redes os pesos w das v\u00e1rias conex\u00f5es s\u00e3o ajustados de modo que para uma dada entrada uma sa\u00edda especificada seja gerada lembre se de que no aprendizado supervisionado tanto entrada quanto sa\u00edda est\u00e3o dispon\u00edveis na etapa de treinamento. A fun\u00e7\u00e3o objetivo a ser minimizada \u00e9 chamada de perda loss https en. Para mais detalhes sobre algoritmos de gradiente descendente em redes neurais confira esta https ruder. O ax\u00f4nio https pt. Moral da hist\u00f3ria mesmo que o modelo tenha muitos par\u00e2metros nem todos s\u00e3o utilizados efetivamente no aprendizado j\u00e1 que a regulariza\u00e7\u00e3o far\u00e1 com que uma parte deles tenda a zero. Em seguida a imagem passa por uma camada convolucional. O n\u00famero de filtros de dada camada \u00e9 um hiperpar\u00e2metro deve ser determinado a priori. Al\u00e9m do mais como h\u00e1 agrega\u00e7\u00e3o da informa\u00e7\u00e3o de janelas maiores em janelas menores os campos receptivos das camadas mais profundas da rede mesmo sendo pequenos possuem informa\u00e7\u00f5es relativas \u00e0 toda a imagem original. end cases varphi u \u00e9 chamada de fun\u00e7\u00e3o degrau de Heaviside https en. com c digit recognizer do Kaggle Inspecionando os dados Cada linha do DataFrame corresponde a uma imagem e cada coluna a um pixel da imagem com exce\u00e7\u00e3o da primeira coluna label que identifica o d\u00edgito. As redes densamente conectadas s\u00f3 podem aprender padr\u00f5es globais ou seja padr\u00f5es que envolvem todos os pontos presentes em dada amostra ou observa\u00e7\u00e3o. Ela est\u00e1 dispon\u00edvel em sklearn. Problema 3 escassez de dados para treinamentoEste problema vem sendo aliviado principalmente por conta do cen\u00e1rio big data com a cada vez maior quantidade de dados sendo gerada a partir das mais diversas fontes. A entrada 0 n\u00e3o \u00e9 uma conex\u00e3o com um neur\u00f4nio anterior j\u00e1 que fornece sempre o mesmo valor x_0 1. \u00c9 poss\u00edvel verificar a qualquer momento a estrutura do modelo com a fun\u00e7\u00e3o summary https keras. confusion_matrix https scikit learn. O tamanho do passo que o algoritmo de minimiza\u00e7\u00e3o efetua na descida a cada itera\u00e7\u00e3o \u00e9 chamado de taxa de aprendizado https en. org stable modules generated sklearn. Agora vamos implementar uma rede convolucional no Keras. MaxPooling2D https keras. Levar em conta essa fra\u00e7\u00e3o do passo anterior tamb\u00e9m no c\u00e1lculo do gradiente resulta na estrat\u00e9gia de momentos de Nesterov http mpawankumar. Esta \u00e9 a estrutura de um neur\u00f4nio biol\u00f3gico https pt. org wiki Submersible_pump cujo funcionamento \u00e9 ilustrado nos seguintes v\u00eddeos Importando os dadosOs dados est\u00e3o organizados em duas pastas de acordo com a estrutura casting_data test def_front ok_front train def_front ok_frontEssa estrutura \u00e9 conveniente porque no momento da leitura pelo keras h\u00e1 o reconhecimento das classes def_front e ok_front de acordo com os nomes das pastas. Nesse processo o material met\u00e1lico fundido \u00e9 alimentado a um molde e durante o resfriamento solidifica se na forma desejada como ilustrado a seguir Os defeitos https en. Fica evidente o alto grau de sobreajuste o que indica margem para melhoria do desempenho. Talvez outra estrat\u00e9gia ou arquitetura leve a resultados mais est\u00e1veis. possuem milhares ou milh\u00f5es ou mesmo bilh\u00f5es https www. org wiki Convolutional_neural_network s\u00e3o redes esparsamente conectadas em que cada neur\u00f4nio n\u00e3o se conecta a todos os neur\u00f4nios da camada anterior. org wiki Recurrent_neural_network e b de alimenta\u00e7\u00e3o direta https en. A entrada reflete a dimensionalidade das imagens 784 pixels. Assim como as camadas convolucionais os neur\u00f4nios das camadas de pooling tamb\u00e9m possuem campos receptivos mas n\u00e3o h\u00e1 pesos e a opera\u00e7\u00e3o efetuada n\u00e3o \u00e9 a convolu\u00e7\u00e3o e sim alguma opera\u00e7\u00e3o de agrega\u00e7\u00e3o como a m\u00e9dia ou a escolha do valor m\u00e1ximo sendo essa \u00faltima a mais usada img src https www. io api models model summary method Perceba a grande quantidade de par\u00e2metros mesmo em uma rede com apenas uma camada oculta Antes de treinar o modelo \u00e9 preciso compil\u00e1 lo definindo a fun\u00e7\u00e3o perda o otimizador e as m\u00e9tricas de avalia\u00e7\u00e3o A fun\u00e7\u00e3o entropia cruzada https pt. O termo dentro do par\u00eanteses \u00e9 a simples normaliza\u00e7\u00e3o do sinal utilizando as estat\u00edsticas do minilote. Teste v\u00e1rias estruturas para os ensembles n\u00famero de modelos arquitetura de cada modelo individual maneira de agregar os resultados etc. io que oferece uma interface intuitiva para implementa\u00e7\u00e3o de modelos de redes neurais com a possibilidade de utiliza\u00e7\u00e3o de diversos motores de c\u00e1lculo o motor utilizado aqui ser\u00e1 o TensorFlow https www. com playlist list PLvr45Arc0UpzsRhzq3q4_KmZcm0utwvvB. 0 representa\u00e7\u00e3o one hot encoding. Durante seus testes \u00e9 instrutivo verificar tamb\u00e9m quais modifica\u00e7\u00f5es n\u00e3o ajudam ou mesmo pioram o desempenho. GLOROT e BENGIO 2010 http proceedings. com c digit recognizer Fashion MNIST https www. 2006 https www. io api layers normalization_layers batch_normalization aceita um vetor de entradas mathbf x e retorna uma sa\u00edda mathbf y de mesmo comprimento cujos elementos resultam da seguinte opera\u00e7\u00e3o y_i left frac x_i mu_B sqrt sigma_B 2 epsilon right gamma beta em que mu_B e sigma_B 2 s\u00e3o a m\u00e9dia e a vari\u00e2ncia do minilote utilizado no momento gamma e beta s\u00e3o par\u00e2metros a serem aprendidos pela rede epsilon \u00e9 um n\u00famero fixo pequeno que ajuda a evitar problemas num\u00e9ricos. Nessa camada aplica se a opera\u00e7\u00e3o de convolu\u00e7\u00e3o Na opera\u00e7\u00e3o de convolu\u00e7\u00e3o um filtro tamb\u00e9m chamado de kernel em geral de dimens\u00e3o 3x3 a mais comum 5x5 ou 7x7 percorre os pixels de uma imagem calculando o produto escalar a cada passo e armazenando os resultados em uma matriz chamada de mapa de caracter\u00edsticas. Perc\u00e9ptronO perc\u00e9ptron https en. confusion_matrix Na matriz de confus\u00e3o o elemento C_ ij corresponde \u00e0 quantidade de observa\u00e7\u00f5es da classe i que foi confundida com a classe j. org wiki MNIST_database talvez o exemplo mais famoso da \u00e1rea de aprendizado de m\u00e1quina um conjunto de dados industrial https www. com science article abs pii S0893608098001166 via 3Dihub consiste em adicionar a cada passo da itera\u00e7\u00e3o uma fra\u00e7\u00e3o do passo anterior o que pode amortecer oscila\u00e7\u00f5es e acelerar a descida em dire\u00e7\u00f5es promissoras. Ou seja em um modelo de aprendizado profundo os padr\u00f5es reconhecidos a partir dos dados s\u00e3o organizados em m\u00faltiplas camadas. A por\u00e7\u00e3o da camada anterior que se conecta a um neur\u00f4nio \u00e9 chamada de seu campo receptivo. Na pr\u00f3xima c\u00e9lula \u00e9 efetuado o treino da rede Observe a necessidade de reformata\u00e7\u00e3o dos dados de entrada para o formato largura x altura x canais de cores. A informa\u00e7\u00e3o \u00e9 processada nas redes por meio de sinais el\u00e9tricos que cada neur\u00f4nio recebe e transmite atrav\u00e9s das sinapses nervosas. org wiki Convolutional_neural_network e entendemos porque este \u00faltimo \u00e9 adequado para aplica\u00e7\u00f5es do tipo vis\u00e3o computacional https pt. N\u00e3o h\u00e1 garantia de que o algoritmo convirja para o m\u00ednimo global. Denotando a classe negativa como sendo das amostras normais sem defeitos e a classe positiva como sendo das amostras com defeitos definimos P frac VP VP FP R frac VP VP FN em que VP \u00e9 o n\u00famero de verdadeiros positivos FP \u00e9 o n\u00famero de falsos positivos e FN \u00e9 o n\u00famero de falsos negativos. As caracter\u00edsticas complexas dos sistemas nervosos biol\u00f3gicos emergem da auto organiza\u00e7\u00e3o dos neur\u00f4nios em vastas e complexas redes contendo bilh\u00f5es de c\u00e9lulas cada uma podendo efetuar milhares de conex\u00f5es. org wiki Casting_defect podem ser de diferentes naturezas http www. Se n\u00e3o houvesse uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o em cada neur\u00f4nio as redes s\u00f3 seriam capazes de aprender padr\u00f5es lineares j\u00e1 que o termo v_k sum_ i 0 m w_ ki x_i \u00e9 uma combina\u00e7\u00e3o linear dos sinais de entrada cujos coeficientes s\u00e3o os pesos. Sua maior vantagem \u00e9 tornar a m\u00e9dia das ativa\u00e7\u00f5es mais pr\u00f3xima de 0 o que acelera o aprendizado por proporcionar maior flexibilidade \u00e0 minimiza\u00e7\u00e3o por gradiente descendente detalhes aqui https stats. Nesta se\u00e7\u00e3o retomaremos o tema abordando o no contexto da vis\u00e3o computacional como um problema de classifica\u00e7\u00e3o bin\u00e1ria utilizando modelos supervisionados de redes convolucionais. \u00c9 uma estrutura de aprendizado an\u00e1loga a que se observa no c\u00f3rtex visual https en. com questions 237169. pdf rebarbas porosidades trincas defeitos na superf\u00edcie dimens\u00f5es ou forma incorretas etc. Treine a rede nos conjuntos MNIST e Fashion MNIST mais vezes variando alguns hiperpar\u00e2metros e observando os efeitos nos resultados. Algumas sugest\u00f5es Varie as arquiteturas das redes e os demais hiperpar\u00e2metros n\u00fameros de filtros taxas de dropout etc. \u00c9 o contr\u00e1rio das redes densamente conectadas como a MLP que estudamos at\u00e9 agora. Muito provavelmente nesse conjunto eles n\u00e3o refletem o ambiente de produ\u00e7\u00e3o em que espera se a propor\u00e7\u00e3o de pe\u00e7as defeituosas seja bem menor que as normais. 01852 propuseram uma modifica\u00e7\u00e3o na inicializa\u00e7\u00e3o de Glorot para que funcionasse melhor com fun\u00e7\u00f5es de ativa\u00e7\u00e3o baseadas em retificadores lineares ReLU etc. org wikipedia commons b b9 Convolution_arithmetic_ _No_padding_strides. Em particular aqui utilizaremos a biblioteca keras https keras. org wiki Vanishing_gradient_problem impedindo a converg\u00eancia do treinamento. org wiki Backpropagation. Em ess\u00eancia A precis\u00e3o diz respeito \u00e0 propor\u00e7\u00e3o de pe\u00e7as detectadas como defeituosas que est\u00e1 correta. Treinamento das redesPor muito tempo lutou se para encontrar um algoritmo de treinamento eficaz para redes com v\u00e1rias camadas. Isso n\u00e3o \u00e9 poss\u00edvel quando a camada apresenta um n\u00famero diferente de entradas em rela\u00e7\u00e3o ao de neur\u00f4nios mas pode se chegar pr\u00f3ximo o suficiente dessa situa\u00e7\u00e3o ideal por meio de estrat\u00e9gias inteligentes de inicializa\u00e7\u00e3o de pesos. Detec\u00e7\u00e3o de falhas em um processo de fundi\u00e7\u00e3oNa aula passada estudamos o problema de detec\u00e7\u00e3o de falhas no contexto do controle estat\u00edstico multivariado de processos utilizando o modelo n\u00e3o supervisionado PCA. Utilizamos tr\u00eas conjuntos de dados MNIST https www. O resultado da segunda etapa \u00e9 uma medida eficiente do gradiente do erro ao longo de toda a rede o que torna poss\u00edvel a aplica\u00e7\u00e3o do algoritmo do gradiente descendente https en. Lembre se de que quanto maior for a independ\u00eancia entre os modelos maior ser\u00e1 a redu\u00e7\u00e3o do erro. org wiki Testes_de_hip C3 B3teses a precis\u00e3o est\u00e1 associada a erros do tipo I https pt. \u00c9 um cen\u00e1rio favor\u00e1vel para a implementa\u00e7\u00e3o de uma estrat\u00e9gia de revis\u00e3o das predi\u00e7\u00f5es resultando em um sistema de detec\u00e7\u00e3o que pode atingir acur\u00e1cia de 100 com baixo custo adicional de HH humano hora https en. A estrat\u00e9gia proposta foi muito popular mas caiu em desuso na d\u00e9cada de 2010 quando novas solu\u00e7\u00f5es para os problemas foram surgindo. Callbacks s\u00e3o objetos que efetuam a\u00e7\u00f5es durante o treinamento. Novas fun\u00e7\u00f5es de ativa\u00e7\u00e3oAs fun\u00e7\u00f5es de ativa\u00e7\u00e3o mais usadas atualmente s\u00e3o da classe de retificadores lineares https en. O neur\u00f4nio matem\u00e1tico https en. Geralmente algumas \u00e9pocas s\u00e3o necess\u00e1rias para que se atinja um erro baixo. Abaixo definimos um callback https keras. io api models sequential e ir adicionando as camadas sequencialmente com o m\u00e9todo add Adicionamos na rede acima duas camadas al\u00e9m da camada de entrada que n\u00e3o precisa ser explicitamente adicionada ambas definidas com a classe keras. Rede convolucional no KerasComo no exemplo anterior utilizaremos a classe keras. html enquanto a regulariza\u00e7\u00e3o L_2 d\u00e1 origem \u00e0 regress\u00e3o ridge https scikit learn. Em particular durante a etapa de retropropaga\u00e7\u00e3o do erro era inevit\u00e1vel que os gradientes zerassem https en. No quarto gr\u00e1fico temos a fun\u00e7\u00e3o ELU proposta por CLEVERT et al. com c digit recognizer apenas 42000 imagens est\u00e3o dispon\u00edveis no conjunto as outras 28000 s\u00e3o reservadas para teste. Pronto J\u00e1 implementamos o modelo e entendemos os detalhes matem\u00e1ticos. Quando a fun\u00e7\u00e3o perda possui um termo de regulariza\u00e7\u00e3o o algoritmo tentar\u00e1 minimizar esse termo se poss\u00edvel levando o a zero. Ap\u00f3s isso utilizamos o m\u00e9todo flow_from_directory para criar efetivamente os objetos que gerar\u00e3o os minilotes de treino e de valida\u00e7\u00e3o. com articles nature14539. Neste notebook s\u00f3 ser\u00e3o estudadas redes de alimenta\u00e7\u00e3o direta. Uma camada de normaliza\u00e7\u00e3o por lote https keras. io api preprocessing image para gerar os tensores de entrada do modelo a partir dos arquivos dispon\u00edveis nas pastas technocast_train_path e technocast_test_path. A revoca\u00e7\u00e3o diz respeito \u00e0 propor\u00e7\u00e3o de pe\u00e7as defeituosas que s\u00e3o corretamente detectadas. Na figura a seguir o mapa \u00e9 calculado com um stride de 2 img src https upload. M\u00e3o na massa 3 Explore a API do Keras https keras. Se deseja se um bom compromisso entre os dois cen\u00e1rios descritos pode se selecionar um corte que resulte em m\u00e1ximo F_1. A seguir abordaremos as mais influentes e utilizadas hoje em dia. jpeg width 250 height 250 Ap\u00f3s a empolga\u00e7\u00e3o inicial logo percebeu se que os perc\u00e9ptrons tinham muitas limita\u00e7\u00f5es uma delas de natureza te\u00f3rica n\u00e3o era poss\u00edvel fazer com que o modelo representasse algumas fun\u00e7\u00f5es simples. Antes de entrarmos nos detalhes do aprendizado profundo \u00e9 interessante implementarmos no keras um modelo MLP semelhante ao que usamos no scikit learn. org wiki Rectifier_ neural_networks hoje mais prestigiada do que a velha log\u00edstica \u00e9 usada tamb\u00e9m no default do MLP do scikit learn e ser\u00e1 explicada mais adiante. 0580 que consiste na desativa\u00e7\u00e3o de uma certa parcela aleat\u00f3ria dos neur\u00f4nios a cada passo do treinamento. \u00c9 conveniente ter um n\u00famero maior de filtros nas camadas mais profundas porque em geral h\u00e1 muito mais possibilidades de padr\u00f5es complexos do que padr\u00f5es simples lembre se de que a identifica\u00e7\u00e3o de padr\u00f5es complexos se d\u00e1 por meio da combina\u00e7\u00e3o dos padr\u00f5es simples. Um dos motivos para tal aplica\u00e7\u00e3o ter sido t\u00e3o bem sucedida \u00e9 que j\u00e1 havia press\u00e3o econ\u00f4mica para desenvolvimento de GPU s antes do uso pela comunidade cient\u00edfica vinda principalmente da ind\u00fastria de videogames. Visualizando a curva de aprendizado Conferindo a acur\u00e1cia e as matrizes de confus\u00e3o O desempenho \u00e9 melhor em rela\u00e7\u00e3o \u00e0 rede MLP Que tal darmos uma olhada nas imagens que a rede classificou incorretamente Percebe se que alguns d\u00edgitos foram escritos de maneira bem inusual. A motiva\u00e7\u00e3o para usar ensembling \u00e9 a mesma da aula de florestas a redu\u00e7\u00e3o do erro de generaliza\u00e7\u00e3o. Nota se que as maiores fontes de confus\u00e3o s\u00e3o 9 sendo classificado como 4 oito vezes 3 sendo classificado como 8 sete vezes 8 sendo classificado como 6 cinco vezes 9 sendo classificado como 7 cinco vezes. org wiki Convolution explicada a seguir. org wiki Erro_do_tipo_II. A desvantagem \u00e9 que ela \u00e9 mais lenta para calcular compensada \u00e0s vezes pela maior velocidade de converg\u00eancia do treinamento. Use a estrat\u00e9gia de aumento de dados https en. Para uma discuss\u00e3o mais profunda recomendo esta leitura https mlfromscratch. No caso espec\u00edfico do aprendizado de m\u00e1quina a regulariza\u00e7\u00e3o \u00e9 qualquer procedimento que tem como objetivo reduzir o erro de predi\u00e7\u00e3o sem reduzir o erro do treinamento. Nesse momento \u00e9 conveniente dar uma olhada em um exemplo de minilote. Percebe se que a quase totalidade das predi\u00e7\u00f5es est\u00e1 pr\u00f3xima de 0 ou 1 o que indica grande certeza do modelo quanto \u00e0s classes da maioria das imagens. A arquitetura \u00e9 t\u00edpica de uma rede convolucional pares consecutivos de camadas convolucionais pooling com os tamanhos dos mapas de caracter\u00edsticas diminuindo e o n\u00famero de filtros aumentando conforme se avan\u00e7a na profundidade. 11604 ou esta https blog. edu ang papers icml09 LargeScaleUnsupervisedDeepLearningGPU. Com os conceitos apresentados acima voc\u00ea deve ser capaz de entender a natureza da maior parte dos hiperpar\u00e2metros. Isso \u00e9 \u00fatil porque n\u00e3o necessariamente o resultado da \u00faltima \u00e9poca \u00e9 o melhor. org wiki Erro_do_tipo_I e do tipo II https pt. Voc\u00ea pode tentar Para concluir \u00e9 importante fazer uma ressalva quanto \u00e0 natureza dos dados de teste. Pronto J\u00e1 temos os objetos respons\u00e1veis por gerar nossos dados de treino valida\u00e7\u00e3o e teste. As camadas de pooling diminuem a quantidade de par\u00e2metros da rede atenuando o sobreajuste e o custo computacional. Abaixo apresentamos alguns exemplos junto \u00e0 fun\u00e7\u00e3o log\u00edstica No primeiro gr\u00e1fico da fun\u00e7\u00e3o log\u00edstica fica evidente como para valores de u afastados de 0 os valores de f u saturam. A regress\u00e3o linear utilizando regulariza\u00e7\u00e3o L_1 \u00e9 chamada de regress\u00e3o lasso https scikit learn. M\u00e3o na massa 1 Adicione ao kernel o conjunto de dados Fashion MNIST https github. io optimizing gradient descent ou esta https towardsdatascience. As curvas de treino e valida\u00e7\u00e3o est\u00e3o pr\u00f3ximas um indicativo de que n\u00e3o h\u00e1 muito sobreajuste. ca aharley vis conv e esta https poloclub. Cada itera\u00e7\u00e3o desse algoritmo \u00e9 dividida em duas etapas Na primeira etapa chamada de passe direto calcula se a sa\u00edda da rede. Outro fator que vem contribuindo para a melhoria de desempenho dos treinamentos \u00e9 o surgimento de novas metodologias de minimiza\u00e7\u00e3o por gradiente descendente como a Adam https arxiv. org wiki Dilution_ neural_networks proposta por HINTON et al. html conhecido como MLP Multilayer perceptron https en. Ou seja como treinar redes com m\u00faltiplas camadas Obs para uma visualiza\u00e7\u00e3o bem interessante do teorema da aproxima\u00e7\u00e3o universal recomendo esta leitura http neuralnetworksanddeeplearning. Essa \u00e9 uma linha de pesquisa ainda pouco explorada e potencialmente muito frut\u00edfera. Algumas excelentes p\u00e1ginas para visualiza\u00e7\u00f5es interativas de redes convolucionais s\u00e3o esta https www. Com essa combina\u00e7\u00e3o a cada camada a vari\u00e2ncia da sa\u00edda resulta maior que da entrada fazendo os sinais crescerem positiva ou negativamente at\u00e9 saturarem nas extremidades da fun\u00e7\u00e3o de ativa\u00e7\u00e3o log\u00edstica. As redes neurais justamente por conta de sua organiza\u00e7\u00e3o natural em camadas s\u00e3o o tipo de modelo de escolha para tais aplica\u00e7\u00f5es. Uma itera\u00e7\u00e3o do algoritmo de minimiza\u00e7\u00e3o \u00e9 efetuada para cada minilote ou seja os erros s\u00e3o calculados e os pesos s\u00e3o atualizados a cada minilote processado. org wiki Frank_Rosenblatt em 1957 foi uma das primeiras redes neurais propostas. io api layers core_layers dense. Isso possibilita a cria\u00e7\u00e3o de padr\u00f5es globais e mais complexos abstratos nas camadas profundas. conv2D https keras. Se deseja se evitar ao m\u00e1ximo o envio de pe\u00e7as defeituosas ao cliente ou seja diminuir o risco de falsos negativos deve se selecionar um corte que resulte em m\u00e1xima revoca\u00e7\u00e3o. com zalando research fashionmnist e Pilot Technocast https www. Importando dadosOs dados MNIST podem ser importados facilmente do scikit learn ou do keras. VideoaulasEste notebook \u00e9 explicado em detalhes ao longo das seguintes videoaulas Para a playlist do curso completo clique aqui https www. Em compensa\u00e7\u00e3o o desempenho computacional foi bem melhor. org wiki Newton 27s_method_in_optimization e Quase Newton https en. Rede MLP no scikit learnAntes de entrarmos nos belos detalhes matem\u00e1ticos dos modelos de redes neurais vamos utilizar o modelo dispon\u00edvel no scikit learn https scikit learn. A fun\u00e7\u00e3o a seguir retorna uma rede convolucional compilada Visualizando a estrutura da rede Como de costume os tamanhos dos mapas de caracter\u00edsticas diminuem e o n\u00famero de filtros aumenta conforme se avan\u00e7a na profundidade. org wiki Aprendizagem_profunda. O algoritmo do gradiente descendente \u00e9 um m\u00e9todo de minimiza\u00e7\u00e3o de fun\u00e7\u00f5es https en. dando origem \u00e0 inicializa\u00e7\u00e3o de He. Uma tend\u00eancia recente \u00e9 a utiliza\u00e7\u00e3o de taxas de aprendizado adaptativas ou seja atualizadas conforme a minimiza\u00e7\u00e3o evolui. N\u00e3o fique preso ao que foi mencionado na aula pesquise sobre e utilize outras fun\u00e7\u00f5es de ativa\u00e7\u00e3o inicializa\u00e7\u00f5es etc. Visualizando a estrutura do modelo S\u00e3o bem mais camadas do que o modelo MLP mas a quantidade de par\u00e2metros \u00e9 menor. net publication 338672883 figure fig1 AS 864764884422656 1583187423806 The comparison between Recurrent Neural Network RNN and Feed Forward Neural Network. A fun\u00e7\u00e3o softmax https en. org wiki Regularization_ mathematics. com r MachineLearning comments 67mjuw d_alternative_interpretation_of p\u00e1ginas. Em geral a matriz de n\u00fameros que constitui cada filtro de dada camada \u00e9 um par\u00e2metro da rede ou seja \u00e9 ajustada junto com os pesos de modo a resultar no aprendizado \u00f3timo. Armazenando os caminhos Usando o matplotlib para dar uma olhada em duas pe\u00e7as aleat\u00f3rias normal e defeituosa S\u00e3o esses dois tipos de imagens que nossos modelos precisar\u00e3o distinguir Gerando dados com Keras Aqui utilizaremos a classe ImageDataGenerator https keras. Na Internet em particular no Kaggle e principalmente para o MNIST voc\u00ea pode encontrar implementa\u00e7\u00f5es das ideias citadas acima ou de outras ideias. Uma \u00e9poca epoch \u00e9 marcada quando todos os minilotes foram processados quando todo o conjunto de dados passou pela rede. 2019 https arxiv. M\u00e3o na massa 2 Analise a API da fun\u00e7\u00e3o sklearn. Uma das estrat\u00e9gias \u00e9 a regulariza\u00e7\u00e3o https en. S\u00e3o 784 pixels j\u00e1 que as imagens t\u00eam tamanho 28x28. Isso foi necess\u00e1rio porque a fun\u00e7\u00e3o de Heaviside muito utilizada at\u00e9 ent\u00e3o \u00e9 composta de segmentos planos n\u00e3o apresentando gradientes. O porqu\u00ea de ela funcionar t\u00e3o bem ainda \u00e9 uma quest\u00e3o em aberto. info teaching cdt big data nesterov83. A natureza do padr\u00e3o reconhecido depende da estrutura do filtro. org wiki Batch_normalization foi proposta por IOFFE e SZEGEDY 2015 https arxiv. As diferen\u00e7as entre as redes neste caso surgem das aleatoriedades da i separa\u00e7\u00e3o entre treino valida\u00e7\u00e3o e ii inicializa\u00e7\u00e3o dos pesos. Uma variante muito usada nas redes neurais \u00e9 o gradiente descendente estoc\u00e1stico SGD https en. org wiki Feedforward_neural_network img src https www. Observa\u00e7\u00f5es lambda \u00e9 um hiperpar\u00e2metro que controla a intensidade da regulariza\u00e7\u00e3o. Voc\u00ea pode inclusive criar o seu pr\u00f3prio conjunto baixando imagens do Google para tentar diferenciar gatos de cachorros etc. O uso dessa ativa\u00e7\u00e3o resulta em um comportamento an\u00e1logo ao do neur\u00f4nio biol\u00f3gico que dispara ou n\u00e3o um sinal de sa\u00edda para os pr\u00f3ximos neur\u00f4nios a depender das intensidades dos sinais de entrada. org wiki Vis C3 A3o_computacional o conjunto de dados MNIST https en. Escola Piloto Virtual PEQ COPPE UFRJ Data Science e Machine Learning na Pr\u00e1tica Introdu\u00e7\u00e3o e Aplica\u00e7\u00f5es na Ind\u00fastria de ProcessosEste notebook \u00e9 referente \u00e0 Aula 3 do curso que trata do problema de classifica\u00e7\u00e3o utilizando modelos de redes neurais https en. \u00c9 uma maneira de preservar as informa\u00e7\u00f5es das bordas ao longo da rede. io api preprocessing image imagedatagenerator class para mais detalhes. Novas estrat\u00e9gias de inicializa\u00e7\u00e3o dos pesosLembre se de que a maior dificuldade para o bom funcionamento de uma rede profunda \u00e9 garantir a efetiva propaga\u00e7\u00e3o dos sinais pela rede. Quais classes est\u00e3o presentes Repita todo o procedimento que efetuamos acima para o conjunto MNIST separe em X e y separe em treino e valida\u00e7\u00e3o visualize algumas amostras aplique uma rede MLP e me\u00e7a o desempenho com a acur\u00e1cia simples e a matriz de confus\u00e3o. org wiki Dendrito s\u00e3o pequenas e m\u00faltiplas ramifica\u00e7\u00f5es do corpo celular. Recomendo que voc\u00ea brinque um pouquinho com os diversos recursos e possibilidades oferecidas em especial antes de fazer a atividade que vem a seguir. x_m e fornece uma sa\u00edda y_k. Quanto \u00e0 arquitetura as duas grandes classes de redes s\u00e3o as redes a recorrentes https en. No entanto tais aplica\u00e7\u00f5es s\u00e3o bem menos comuns. A entropia cruzada entre mathbf y e mathbf p seria 1 cdot log 0. Os dendritos https pt. 6980 um dos mais usados atualmente estende a estrat\u00e9gia de momentos descrita no par\u00e1grafo anterior adicionando um segundo momento respons\u00e1vel por controlar a taxa de aprendizado. 00877 A fun\u00e7\u00e3o perda \u00e9 definida como a m\u00e9dia das v\u00e1rias entropias cruzadas das M classes para todas as amostras N do conjunto ou do minilote frac 1 N sum_ i 1 N sum_ j 1 M y_ ij log p_ ij \u00c9 a fun\u00e7\u00e3o acima que a rede tenta minimizar durante o treinamento manipulando os pesos das conex\u00f5es. Em outras palavras o n\u00famero de \u00e9pocas \u00e9 a quantidade de vezes em que cada dado \u00e9 processado pela rede durante o treinamento. Valores t\u00edpicos v\u00e3o de 10 6 a 10 4. Nos casos em que h\u00e1 poucos dados para um problema em espec\u00edfico podem ser utilizadas t\u00e9cnicas de transfer\u00eancia de aprendizado https en. com zalandoresearch fashion mnist no menu superior clique em File e depois Add or Upload Data. J\u00e1 as redes convolucionais como veremos s\u00e3o capazes de aprender padr\u00f5es locais e reconhecer esses padr\u00f5es em qualquer posi\u00e7\u00e3o de alguma outra amostra. ai ai notes initialization. Cada conex\u00e3o i tem um peso associado denotado por w_ ki. org wiki Graphics_processing_unit para efetuar o treinamento das redes profundas de forma massivamente paralela o que se mostrou essencial para aplica\u00e7\u00f5es em larga escala. Caso deseje se gerar mapas de sa\u00edda menores do que os de entrada podem ser utilizados passos largos strides. io api layers reshaping_layers flatten. Mas na verdade existem bibliotecas muito mais poderosas para utiliza\u00e7\u00e3o de redes neurais que possibilitam a cria\u00e7\u00e3o de modelos com diferentes arquiteturas e m\u00faltiplas camadas. Repare na presen\u00e7a de outro callback o checkpoint instanciado da classe ModelCheckpoint https keras. A sa\u00edda y_k do neur\u00f4nio \u00e9 expressa por y_k varphi v_k varphi left sum_ i 0 m w_ ki x_i right A fun\u00e7\u00e3o n\u00e3o linear varphi \u00e9 chamada de fun\u00e7\u00e3o de ativa\u00e7\u00e3o https en. Al\u00e9m da camada convolucional \u00e9 comum adicionar \u00e0s redes camadas de pooling. org wiki Convolutional_neural_network. Antes de submeter n\u00e3o esque\u00e7a de retreinar os modelos no conjunto de treino completo sem a separa\u00e7\u00e3o treino valida\u00e7\u00e3o. Al\u00e9m dessas h\u00e1 v\u00e1rias outras fun\u00e7\u00f5es de ativa\u00e7\u00e3o SELU https arxiv. A partir de agora utilizaremos a fun\u00e7\u00e3o display_score para verificar o desempenho dos modelos Utilizando a fun\u00e7\u00e3o Nas figuras acimas para melhor visualiza\u00e7\u00e3o dos erros as cores da diagonal foram zeradas e as cores de fora da diagonal foram normalizadas. Voc\u00ea pode implementar o aumento de dados facilmente fornecendo hiperpar\u00e2metros descrevendo as varia\u00e7\u00f5es das imagens \u00e0 classe ImageDataGenerator cheque a refer\u00eancia https keras. 2016 https www. Redes convolucionais Redes convolucionais https en. org wiki United_States_Census_Bureau e estudantes do ensino m\u00e9dio estadunidenses. Por exemplo em certa aplica\u00e7\u00e3o o vetor mathbf y poderia ser mathbf y 0 0 0 1 0 0 0 0 0 0 o que significa dizer que temos certeza de que o valor verdadeiro \u00e9 3. MLPClassifier https scikit learn. pdf propuseram o uso de GPU s Unidades Gr\u00e1ficas de Processamento https en. mil dtic tr fulltext u2 236965. S\u00e3o as chamadas bibliotecas de aprendizado profundo deep learning https pt. A partir de agora o c\u00e9u \u00e9 o limite para voc\u00ea At\u00e9 a pr\u00f3xima comandos m\u00e1gicos que n\u00e3o se comunicam com a linguagem Python e sim diretamente com o kernel do Jupyter come\u00e7am com importando os principais m\u00f3dulos que usaremos ao longo da aula localiza\u00e7\u00e3o dos exemplos na matriz de dados selecionando os d\u00edgitos j\u00e1 no formato de matriz criando figura do matplotlib plotando desligando os eixos de todos os d\u00edgitos dividindo cada valor da matriz pelo n\u00famero total de imagens em cada classe sumindo com a diagonal pra podermos analizar s\u00f3 os erros plotando a matriz plotando os valores num\u00e9ricos para exibir todos os n\u00fameros t\u00edtulo localiza\u00e7\u00e3o dos exemplos na matriz de dados selecionando os d\u00edgitos j\u00e1 no formato de matriz criando figura do matplotlib plotando desligando os eixos de todos os d\u00edgitos n\u00famero de redes no ensemble listas que armazenar\u00e3o os modelos e os hist\u00f3ricos de resultados gerando conjuntos de treino valida\u00e7ao diferentes a cada treinamento gerando o modelo checkpoint para salvar o melhor modelo para a rede em quest\u00e3o treinando a rede em quest\u00e3o imprimindo resultados ap\u00f3s o t\u00e9rmino do treino da rede em quest\u00e3o selecionando as imagens com predi\u00e7\u00f5es erradas probabilidades para as imagens com predi\u00e7\u00f5es erradas transformando objetos ax y_probs_wrong e wrong_positions em containers caso nao sejam para podermos entrar no loop abaixo mesmo quando h\u00e1 apenas uma posi\u00e7\u00e3o errada plotando selecionando as imagens com predi\u00e7\u00f5es longe de 0 e 1 predi\u00e7\u00f5es propriamente ditas gerando a janela do gr\u00e1fico zerando todos os eixos antes de entrarmos no loop plotando. org wiki Multilayer_perceptron para implementar uma solu\u00e7\u00e3o inicial utilizando a API que j\u00e1 nos \u00e9 familiar Efetuando a predi\u00e7\u00e3o nos dados de valida\u00e7\u00e3o Comparando valores verdadeiros e predi\u00e7\u00f5es Dos seis valores mostrados percebemos que o modelo tomou um 3 por um 8 deve ser um erro recorrente dada a semelhan\u00e7a entre os d\u00edgitos. org wiki Artificial_neuron mimetiza o funcionamento do neur\u00f4nio biol\u00f3gico Cada neur\u00f4nio k em uma rede artificial \u00e9 uma unidade de processamento local que aceita m conex\u00f5es por meio das entradas x_1 x_2. Em sua forma original consiste de uma camada de neur\u00f4nios ou mesmo apenas um neur\u00f4nio com a seguinte fun\u00e7\u00e3o de ativa\u00e7\u00e3o varphi u begin cases 0 text se u leq 0 1 text se u 0. Incentivo que voc\u00ea se inscreva na competi\u00e7\u00e3o e submeta os resultados dos v\u00e1rios modelos que criaremos ao longo da aula de modo a verificar os respectivos desempenhos no conjunto de teste. O mapa de caracter\u00edsticas recebe esse nome porque \u00e9 a estrutura respons\u00e1vel por reconhecer os v\u00e1rios padr\u00f5es caracter\u00edsticas relativos \u00e0 imagem. Pronto J\u00e1 podemos treinar. org wiki Erro_do_tipo_II como discutido mais adiante. jpg width 400 height 400 Nas redes recorrentes o sinal pode ser propagado para tr\u00e1s enquanto nas redes de alimenta\u00e7\u00e3o direta o sinal s\u00f3 se propaga em um sentido. Tenta a\u00ed duvido acertar todas xD\u00c9 poss\u00edvel melhorar ainda mais os resultados no conjunto MNIST esse ser\u00e1 o tema do m\u00e3o na massa do final da aula. org wiki Stochastic_gradient_descent em que as derivadas s\u00e3o estimadas a partir de por\u00e7\u00f5es aleat\u00f3rias dos dados o que reduz problemas como m\u00ednimos locais e pontos de sela. org wiki Erro_do_tipo_I e a revoca\u00e7\u00e3o est\u00e1 associada a erros do tipo II https pt. org wiki Loss_function. \u00c9 incr\u00edvel como essa fun\u00e7\u00e3o bem mais simples do que a log\u00edstica funciona t\u00e3o melhor nas redes neurais e precisou esperar tantas d\u00e9cadas at\u00e9 ter seu uso disseminado O prov\u00e1vel motivo \u00e9 a analogia que se pode fazer entre a fun\u00e7\u00e3o log\u00edstica e os mecanismos de disparo do neur\u00f4nio biol\u00f3gico analogias s\u00e3o legais mas n\u00e3o ajudam em nada os algoritmos matem\u00e1ticos. Historicamente os grandes problemas foram 1. pdf em um artigo muito divertido foram os primeiros a implementar de fato o elefante com quatro par\u00e2metros e a fazer a tromba balan\u00e7ar com um par\u00e2metro adicional. org wiki Mathematical_optimization que a cada passo itera\u00e7\u00e3o busca o m\u00ednimo na dire\u00e7\u00e3o de maior inclina\u00e7\u00e3o da fun\u00e7\u00e3o. A vantagem da dilata\u00e7\u00e3o em rela\u00e7\u00e3o \u00e0 estrat\u00e9gia de passos largos \u00e9 a abrang\u00eancia maior do campo receptivo de cada neur\u00f4nio. Os dados ser\u00e3o embaralhados e a semente da aleatoriedade foi especificada como 0. An\u00e1lise dos resultadosPlotando as curvas de aprendizado Carregando as melhores configura\u00e7\u00f5es de cada um dos modelos salvas pelo callback model_checkpoint Definindo uma fun\u00e7\u00e3o para retornar a predi\u00e7\u00e3o do ensemble Lembre se de que a rede fornece como sa\u00edda para cada imagem uma probabilidade ou seja um n\u00famero entre 0 e 1. Crie ensembles nos casos em que usamos apenas um modelo MNIST e Fashion MNIST. No caso de predi\u00e7\u00f5es equivocadas os t\u00edtulos est\u00e3o em vermelho. Cada por\u00e7\u00e3o \u00e9 chamada de minilote minibatch. Do ponto de vista da teoria estat\u00edstica de testes de hip\u00f3teses https pt. Repare que a entrada da primeira camada \u00e9 um tensor de formato largura x altura x canais ao contr\u00e1rio da rede MLP em que a entrada era um vetor. org wiki Ax C3 B3nio \u00e9 uma longa extens\u00e3o pode ser algumas ou milhares de vezes maior do que o corpo celular em cujos terminais encontram se as sinapses https pt. org wiki Rectifier_ neural_networks. O problema \u00e9 bin\u00e1rio e \u00e0s classes normal ok_front e defeituosa def_front foram atribu\u00eddos os r\u00f3tulos 0 e 1 respectivamente. Incentivo voc\u00ea a testar suas novas habilidades com outros dados. Minilotes e \u00e9pocasComo vimos nas variantes SGD do m\u00e9todo de gradiente descendente os dados s\u00e3o divididos aleatoriamente em pequenas por\u00e7\u00f5es. Rede MLP no KerasNosso primeiro contato com as redes foi por meio do scikit learn uma abordagem conveniente pois j\u00e1 t\u00ednhamos familiaridade com sua interface. Separando os conjuntos X e y A divis\u00e3o por 255 \u00e9 uma normaliza\u00e7\u00e3o que converte os pixels da faixa 0 255 para a faixa 0 1 o que melhora o desempenho das redes neurais. org wiki Fun C3 A7 C3 A3o_suave uma conveni\u00eancia num\u00e9rica tamb\u00e9m \u00fatil na minimiza\u00e7\u00e3o. org wiki Neur C3 B3nio O corpo celular \u00e9 o local dos componentes mais complexos da c\u00e9lula n\u00facleo etc. Aprendizado Profundo GOODFELLOW et al. O uso de um ensemble de redes aumenta a acur\u00e1cia geral reduzindo as amostras nessa faixa e ajuda tamb\u00e9m a trazer as m\u00e9dias das predi\u00e7\u00f5es incertas para perto de 0 5 o que tende a trazer as imagens com classifica\u00e7\u00f5es incorretas para dentro dessa faixa. com 2018 01 10 an intuitive explanation of why batch normalization really works normalization in deep learning part 1 e esta https www. Na fun\u00e7\u00e3o acima a probabilidade do ensemble \u00e9 calculada como a m\u00e9dia das probabilidades fornecidas por cada rede. br upload files defeitos em pecas fabricadas pelo processo de fundicao. Na c\u00e9lula abaixo as v\u00e1rias redes s\u00e3o treinadas em sequ\u00eancia. No caso de imagens esses padr\u00f5es locais podem corresponder a bordas curvas etc. Pesquise e entenda a natureza do conjunto. Nas redes neurais uma regulariza\u00e7\u00e3o muito usada \u00e9 a regulariza\u00e7\u00e3o L_2 que adiciona \u00e0 fun\u00e7\u00e3o perda um termo de penaliza\u00e7\u00e3o dos pesos w na forma lambda sum_i w_i 2 Lembre se de que o treinamento consiste na minimiza\u00e7\u00e3o da fun\u00e7\u00e3o perda por meio do ajuste dos pesos w. Nas pr\u00f3ximas c\u00e9lulas essa import\u00e2ncia ficar\u00e1 ainda mais clara por meio de uma an\u00e1lise detalhada do conjunto de teste. Podemos enfim treinar Ufa Deu bem mais trabalho do que o modelo do scikit learn pois foi preciso definir muito mais detalhes. org wiki Artificial_neural_network s\u00e3o uma classe de modelos de aprendizado inspirados vagamente no funcionamento do c\u00e9rebro humano. treinamentos excessivamente lentos 2. org wikipedia en 5 52 Mark_I_perceptron. Dropout https keras. Acabamos de entender a import\u00e2ncia da probabilidade de sa\u00edda da rede para o processo de classifica\u00e7\u00e3o. 2009 https publications. com ravirajsinh45 real life industrial dataset of casting product discussion 129717. As redes recorrentes s\u00e3o \u00fateis para modelar dados em que h\u00e1 depend\u00eancia entre as observa\u00e7\u00f5es como processos din\u00e2micos textos ou sequ\u00eancias. Uau Est\u00e1 garantido que em tese o uso de mais camadas nas redes neurais possibilita a resolu\u00e7\u00e3o de uma enorme classe de problemas No entanto apesar de encorajador o teorema n\u00e3o diz nada sobre os procedimentos necess\u00e1rios para fazer isso acontecer. Para esse objetivo o ideal seria que a vari\u00e2ncia de um sinal direto ou reverso se mantivesse constante antes e depois de passar por uma camada. org wiki Regra_da_cadeia isso mesmo aquela do C\u00e1lculo 1. Ambas as estrat\u00e9gias envolvem amostragens aleat\u00f3rias a partir de distribui\u00e7\u00f5es estat\u00edsticas com caracter\u00edsticas espec\u00edficas. io api callbacks model_checkpoint e respons\u00e1vel por salvar o modelo com melhor desempenho medido em rela\u00e7\u00e3o \u00e0 perda da valida\u00e7\u00e3o. gif width 200 height 200 Outra estrat\u00e9gia para gerar mapas de sa\u00edda menores que os da entrada \u00e9 a dilata\u00e7\u00e3o proposta por YU e KOLTUN 2015 https arxiv. 2015 https www. A fun\u00e7\u00e3o de ativa\u00e7\u00e3o relu https en. io guides transfer_learning. org wiki Confusion_matrix dispon\u00edvel em sklearn. risco de sobreajuste por conta do excesso de par\u00e2metros. A regulariza\u00e7\u00e3o \u00e9 utilizada em muitos modelos n\u00e3o apenas em redes neurais. Apesar da grande simplicidade a empolga\u00e7\u00e3o com o perc\u00e9ptron foi grande j\u00e1 que era a \u00e9poca do surgimento da Intelig\u00eancia Artificial A m\u00e1quina Mark I Perceptron https apps. Regulariza\u00e7\u00e3oUma cr\u00edtica recorrente a modelos de redes neurais e a modelos de aprendizado de forma geral \u00e9 a grande quantidade de par\u00e2metros de sua estrutura matem\u00e1tica. Apesar de a ideia ser antiga o treinamento de redes com muitas camadas sempre se mostrou bem dif\u00edcil mesmo com o advento do algoritmo da retropropaga\u00e7\u00e3o do erro em 1986. O v\u00eddeo a seguir por exemplo fornece dicas para que se evite o problema da porosidade As pe\u00e7as em quest\u00e3o s\u00e3o impulsores de bombas submers\u00edveis https en. org wiki Sinapse que por sua vez se conectam aos dendritos ou corpos celulares de outros neur\u00f4nios. Observando apenas as imagens classificadas incorretamente As amostras com classifica\u00e7\u00f5es incorretas apresentam majoritariamente predi\u00e7\u00f5es pr\u00f3ximas de 0 5. Nossa miss\u00e3o aqui ser\u00e1 criar modelos capazes de identificar o d\u00edgito de cada imagem. com ravirajsinh45 real life industrial dataset of casting product com imagens de pe\u00e7as regulares e defeituosas resultantes de um processo de fundi\u00e7\u00e3o https pt. A figura a seguir ilustra a arquitetura t\u00edpica de uma rede convolucional A primeira parte da rede efetua a extra\u00e7\u00e3o de caracter\u00edsticas ou seja identifica os diversos padr\u00f5es presentes na imagem. org \u00e9 uma p\u00e1gina muito interessante para ganhar sentimento em rela\u00e7\u00e3o ao funcionamento das redes neurais. Por exemplo a seguir \u00e9 ilustrada a representa\u00e7\u00e3o da imagem de um c\u00e3ozinho como um tensor tridimensional de pixels No caso o tensor resultante possui formato 15x20x3 ou seja s\u00e3o 15 linhas 20 colunas e 3 canais de cores. A entrada de uma camada densa deve ser um vetor portanto antes dessa camada \u00e9 preciso achatar o tensor de entrada para apenas uma dimens\u00e3o o que \u00e9 efetuado com a classe keras. A camada convolucional pode ter v\u00e1rios filtros cada um dando origem a um mapa de caracter\u00edsticas. Para detalhes consulte esta p\u00e1gina https www. Para maior detalhamento do resultado podemos usar a matriz de confus\u00e3o https en. Estudamos dois tipos de redes MLP https en. Do ponto de vista matem\u00e1tico a regulariza\u00e7\u00e3o \u00e9 qualquer procedimento que adiciona informa\u00e7\u00e3o a um problema com muitos graus de liberdade. Voc\u00ea pode verificar a relev\u00e2ncia da metodologia rodando os pr\u00f3ximos c\u00f3digos deste notebook com GPU s ativadas e depois somente com CPU s e comparando os desempenhos. io api callbacks early_stopping ir\u00e1 monitorar a perda da valida\u00e7\u00e3o a cada \u00e9poca e caso a diminui\u00e7\u00e3o nas \u00faltimas 5 \u00e9pocas n\u00e3o seja maior que 10 6 o treinamento ser\u00e1 finalizado. net publication 333593451 figure fig2 AS 765890261966848 1559613876098 Illustration of Max Pooling and Average Pooling Figure 2 above shows an example of max. org propuseram uma excelente defini\u00e7\u00e3o de aprendizado profundo O aprendizado profundo \u00e9 um tipo espec\u00edfico de aprendizado de m\u00e1quina que alcan\u00e7a grande poder e flexibilidade ao representar o mundo como uma hierarquia aninhada de conceitos sendo i cada conceito definido por meio de sua rela\u00e7\u00e3o com conceitos mais simples e ii representa\u00e7\u00f5es mais abstratas calculadas em termos de outras menos abstratas. Um exemplo de rede com tr\u00eas camadas uma camada de entrada uma camada oculta e uma camada de sa\u00edda A camada de entrada n\u00e3o efetua nenhum c\u00e1lculo e \u00e9 respons\u00e1vel apenas por distribuir os sinais de entrada para as camadas internas. Modelos de aprendizado profundo que s\u00e3o incontestavelmente \u00fateis reconhecem rostos em imagens traduzem textos etc. \u00c9 uma abordagem semelhante \u00e0 que utilizamos na Aula 1 com modelos de florestas aleat\u00f3rias. Apesar de 0 5 ser o corte mais comum podemos variar esse valor de modo a manipular as propor\u00e7\u00f5es entre erros do tipo I https pt. No caso de ajuste desse par\u00e2metro \u00e9 bom que a escolha seja feita com base nos dados de valida\u00e7\u00e3o e n\u00e3o de teste de modo que possamos usar os dados de teste para verifica\u00e7\u00e3o da adequa\u00e7\u00e3o da escolha. Quando a imagem possui v\u00e1rios canais o mapa de caracter\u00edsticas resultante do filtro \u00e9 a soma dos mapas de caracter\u00edsticas correspondentes a cada canal. Em particular utilizaremos uma arquitetura especialmente apropriada para aplica\u00e7\u00f5es de vis\u00e3o computacional as chamadas redes convolucionais https en. As for\u00e7as entre as conex\u00f5es s\u00e3o ajustadas de acordo com est\u00edmulos externos. Uma m\u00e9trica de desempenho bastante usada em problemas de classifica\u00e7\u00e3o \u00e9 a acur\u00e1cia ou seja a simples propor\u00e7\u00e3o de predi\u00e7\u00f5es verdadeiras. Matematicamente a rede efetua o aprendizado desses padr\u00f5es locais por meio da opera\u00e7\u00e3o de convolu\u00e7\u00e3o https en. Problema 4 risco de sobreajuste por conta do excesso de par\u00e2metrosPara esse problema as velhas regulariza\u00e7\u00f5es L_1 e L_2 podem ser boas solu\u00e7\u00f5es. Sequential https keras. H\u00e1 material de sobra para estudo e explora\u00e7\u00e3o. Portanto na pr\u00e1tica todos os resultados e conclus\u00f5es deveriam ser reobtidos com dados que reflitam a opera\u00e7\u00e3o da planta industrial que infelizmente n\u00e3o est\u00e3o dispon\u00edveis para este estudo de caso. org wiki Sigmoid_function como a log\u00edstica. Para um par\u00e2metro w_i n\u00e3o resultar em zero ele precisa ser muito importante ou seja seu efeito na diminui\u00e7\u00e3o do erro tem de ser mais importante do que o efeito no aumento do erro causado pelo termo de regulariza\u00e7\u00e3o. png width 350 height 350 Em geral os campos receptivos de pooling t\u00eam tamanho 2x2 e stride 2 como mostrado acima. 02515 GELU https arxiv. org wiki Vis C3 A3o_computacional. 2015 https arxiv. Na figura a seguir visualizamos todas as 715 amostras do conjunto de teste junto com respectivamente a classe verdadeira e a probabilidade predita pelo modelo. No entanto os tempos mudam. No entanto utilizaremos os dados dispon\u00edveis na competi\u00e7\u00e3o Digit Recognizer https www. com busting the myths about batch normalization esta https mlexplained. \u00c9 uma ideia simples mas surpreendentemente efetiva para melhorar a capacidade de generaliza\u00e7\u00e3o das redes. No aprendizado profundo por exemplo podemos usar camadas gen\u00e9ricas pr\u00e9 treinadas e treinar apenas as \u00faltimas camadas para uma situa\u00e7\u00e3o em espec\u00edfico usando poucos dados como demonstrado aqui https keras. org wiki Visual_cortex estrutura do c\u00e9rebro respons\u00e1vel pelo processamento da informa\u00e7\u00e3o visual. Al\u00e9m delas uma estrat\u00e9gia de regulariza\u00e7\u00e3o muito usada \u00e9 a dropout https en. Podemos verificar esse balan\u00e7o plotando as curvas de precis\u00e3o e revoca\u00e7\u00e3o em fun\u00e7\u00e3o da probabilidade de corte da sa\u00edda do modelo A probabilidade de corte portanto pode ser um par\u00e2metro definido de acordo com o interesse da companhia Se deseja se diminuir ao m\u00e1ximo o preju\u00edzo causado pelo desperd\u00edcio de pe\u00e7as boas ou seja diminuir o risco de falsos positivos deve se selecionar um corte que resulte em m\u00e1xima precis\u00e3o. Enrico Fermi https en. Isso \u00e9 efetuado de tr\u00e1s para frente ou seja da sa\u00edda para a entrada por meio da aplica\u00e7\u00e3o da regra da cadeia https pt. Os neur\u00f4nios em um mesmo mapa de caracter\u00edsticas compartilham os mesmos par\u00e2metros pesos. M\u00e3o na massa 4 Tente aprimorar os desempenhos dos modelos deste notebook. 07122 e ilustrada na figura a seguir. org wiki Deep_learning. 1527 foram os primeiros a demonstrar a possibilidade de atenua\u00e7\u00e3o dos problemas 1 e 3 para treinamento de redes profundas utilizando uma metodologia de pr\u00e9 treinamento n\u00e3o supervisionado. At\u00e9 que RUMERHALT et al. TensorFlow PlaygroundO TensorFlow Playground https playground. Esse segundo momento rastreia as vari\u00e2ncias dos gradientes de cada par\u00e2metro ao longo dos passos anteriores. Aplique em alguns modelos MLP para os conjuntos MNIST e Fashion MNIST. org wiki F1_score a m\u00e9dia harm\u00f4nica entre precis\u00e3o e revoca\u00e7\u00e3o F_1 frac 2 1 P 1 R Na pr\u00f3xima c\u00e9lula calculamos a precis\u00e3o a revoca\u00e7\u00e3o e o F_1 de nosso modelo Existe um balan\u00e7o entre precis\u00e3o e revoca\u00e7\u00e3o quando aumentamos uma a outra tende a diminuir. Podemos usar o m\u00e9todo evaluate para avaliar rapidamente o desempenho final do modelo Analisando as matrizes de confus\u00e3o utilizando nossa fun\u00e7\u00e3o display_score O desempenho \u00e9 parecido com o que obtivemos com a rede MLP do scikit learn com a diferen\u00e7a de que parece haver menos sobreajuste. Use modelos pr\u00e9 treinados para introduzir par\u00e2metros n\u00e3o trein\u00e1veis nas primeiras camadas transfer\u00eancia de aprendizado. org wiki Artificial_neural_network. A figura a seguir ilustra bem o conceito de campo receptivo de um neur\u00f4nio A opera\u00e7\u00e3o de convolu\u00e7\u00e3o faz com que o mapa de sa\u00edda tenha formato menor do que o mapa de entrada. O vetor mathbf p poderia ser mathbf p 0 0 0 0. org wiki Perceptron inventado por Frank Rosenblatt https en. Perceba que o primeiro campo receptivo \u00e9 maior que os demais 7x7 o que pode ser conveniente para reduzir o tamanho da imagem sem perder muita informa\u00e7\u00e3o e como s\u00f3 h\u00e1 em geral no m\u00e1ximo 3 canais na terceira dimens\u00e3o da entrada a opera\u00e7\u00e3o n\u00e3o \u00e9 muito custosa. Na fun\u00e7\u00e3o abaixo definimos um objeto de nome datagen a partir da classe ImageDataGenerator especificando que os pixels ser\u00e3o normalizados para a faixa 0 1 por meio da divis\u00e3o por 255 e 10 dos dados ser\u00e3o reservados para valida\u00e7\u00e3o. accuracy_score https scikit learn. com \u00e9 composto por imagens de pe\u00e7as regulares e defeituosas resultantes de um processo de fundi\u00e7\u00e3o https pt. org wiki Fundi C3 A7 C3 A3o. Isso \u00e9 poss\u00edvel porque existem estrat\u00e9gias desenvolvidas justamente para lidar com essa quest\u00e3o. io api layers pooling_layers max_pooling2d implementa uma camada de pooling. ", "id": "afrniomelo/epv-peq-aula-3-classifica-o", "size": "65999", "language": "python", "html_url": "https://www.kaggle.com/code/afrniomelo/epv-peq-aula-3-classifica-o", "git_url": "https://www.kaggle.com/code/afrniomelo/epv-peq-aula-3-classifica-o", "script": "= {0 visualizeImageBatch ensemble_prediction seaborn numpy matplotlib.pyplot make_generators display_score tensorflow logistic pandas elu keras YouTubeVideo leaky_relu relu make_cnn IPython.display ", "entities": "(('motiva\u00e7\u00e3o para usar', 'de redu\u00e7\u00e3o erro generaliza\u00e7\u00e3o'), 'ensemble') (('profundo por exemplo podemos usar camadas gen\u00e9ricas pr\u00e9 treinadas e treinar \u00faltimas camadas para uma aprendizado situa\u00e7\u00e3o', 'usando poucos dados como demonstrado aqui https keras'), 'apenas') (('problema de classifica\u00e7\u00e3o', 'Escola Piloto PEQ UFRJ Data Science e Machine Pr\u00e1tica Introdu\u00e7\u00e3o e Virtual Ind\u00fastria de ProcessosEste'), 'COPPE') (('ou seja', 'treinamento situa\u00e7\u00e3o amostras de comum redes'), 'impedir') (('hiperpar\u00e2metros e efeitos nos resultados', 'nos conjuntos MNIST e Fashion MNIST variando alguns'), 'veze') (('ao alguma das classes', 'de'), 'distantes') (('quarto', 'ELU por CLEVERT et al'), 'gr\u00e1fico') (('io api layers initializers', 'class e para fun\u00e7\u00f5es sigm\u00f3ides ideal https'), 'glorotuniform') (('resulta maior que da entrada os sa\u00edda fazendo sinais', 'da ativa\u00e7\u00e3o saturarem nas extremidades fun\u00e7\u00e3o de log\u00edstica'), 'combina\u00e7\u00e3o') (('333593451 figure', 'max'), 'show') (('pr\u00e9 modelos treinados', 'introduzir par\u00e2metros'), 'use') (('de aprendizado adaptativas \u00e9 utiliza\u00e7\u00e3o de taxas ou', 'minimiza\u00e7\u00e3o evolui'), 'recente') (('dos entrada sinais de cujos', 'os s\u00e3o pesos'), 'ativa\u00e7\u00e3o') (('ajuste', 'pesos'), 'desse') (('Repare', 'callback checkpoint instanciado da classe ModelCheckpoint https de outro o keras'), 'presen\u00e7a') (('propor\u00e7\u00f5es parecem', 'entre os conjuntos'), 'fizemos') (('Glorot para que funcionasse melhor com ativa\u00e7\u00e3o de fun\u00e7\u00f5es de baseadas', 'ReLU'), 'modifica\u00e7\u00e3o') (('Outra estrat\u00e9gia para gerar menores os 200 height 200 mapas de sa\u00edda da', '\u00e9'), 'width') (('imagens com classifica\u00e7\u00f5es trazer incorretas', 'dentro dessa faixa'), 'aumenta') (('abrang\u00eancia maior', 'receptivo de cada neur\u00f4nio'), 'rela\u00e7\u00e3o') (('os resultados e conclus\u00f5es deveriam ser reobtidos', 'n\u00e3o est\u00e3o dispon\u00edveis para estudo de opera\u00e7\u00e3o da planta industrial que infelizmente caso'), 'com') (('org wiki Sinapse por sua se conectam aos dendritos ou que vez corpos', 'outros de neur\u00f4nios'), 'celulare') (('bom compromisso entre os dois cen\u00e1rios se descritos', 'selecionar corte que se resulte'), 'pode') (('relativas ao teste Para \u00e9 necess\u00e1rio probabilidades probabilidades conjunto de m\u00e9tricas de definir qual faixa de est\u00e1', 'cada classe'), 'para') (('de vista', 'problema com muitos graus de liberdade'), 'do') (('e diminui\u00e7\u00e3o nas', 'ser\u00e1 6 o finalizado'), 'callback') (('propor\u00e7\u00e3o de pe\u00e7as', 'defeituosas que est\u00e1 como correta'), 'ess\u00eancia') (('respons\u00e1vel por estrutura reconhecer', 'nome esse porque'), 'recebe') (('imagem caracter\u00edsticas o mapa de resultante', 'cada canal'), 'Quando') (('ou', 'aprendizado \u00f3timo'), 'constitui') (('diz nada sobre', 'fazer isso acontecer'), 'que') (('conex\u00f5es', 'de acordo com est\u00edmulos ajustadas externos'), 'entre') (('scikit', 'ao que usamos'), 'aprendizado') (('justamente sua por conta de organiza\u00e7\u00e3o', 'o de escolha para tais tipo de modelo aplica\u00e7\u00f5es'), 'rede') (('e ser\u00e1 explicada mais', 'default'), 'que') (('e chamado de vi\u00e9s bias', 'neur\u00f4nio k.'), 'denotado') (('org wiki Convolutional_neural_network e entendemos porque \u00faltimo adequado para \u00e9 aplica\u00e7\u00f5es', 'https tipo vis\u00e3o computacional pt'), 'este') (('Nossa miss\u00e3o', 'capazes de identificar'), 'aqui') (('Quanto mais fundo se avan\u00e7a nas camadas', 'padr\u00f5es se tornando complexos e v\u00e3o abstratos'), 'do') (('propor\u00e7\u00e3o pe\u00e7as defeituosas de seja', 'normais'), 'provavelmente') (('fator que vem Outro contribuindo', '\u00e9'), 'para') (('regulariza\u00e7\u00e3o L_1 \u00e9', 'usando valores lugar dos L_2 entanto quadrados'), 'an\u00e1loga') (('da', 'Digit Recognizer https www'), 'o') (('o suficiente dessa', 'de de inicializa\u00e7\u00e3o pesos'), 'entradas') (('y', 'forma'), 'sobre') (('ou', 'keras'), 'learn') (('uma vez o dados de t\u00e9rmino', 'treinamento'), 'utilizaremo') (('modelo', 'da maioria das imagens'), 'que') (('0 encontram de 5 geral abaixo', 'ser menor que revoca\u00e7\u00e3o precis\u00e3o'), 'consistente') (('minimiza\u00e7\u00e3o por gradiente descendente detalhes', 'https stats'), 'tornar') (('m\u00e3o na massa', 'da final aula'), 'Tenta') (('scikit', 'pois foi preciso definir muito mais detalhes'), 'do') (('Observa\u00e7\u00f5es', 'da intensidade regulariza\u00e7\u00e3o'), 'lambda') (('caracter\u00edsticas de mapa de da', 'conecta se'), 'm\u00faltiplos') (('Callbacks', 'objetos que efetuam a\u00e7\u00f5es durante'), 's\u00e3o') (('de caracter\u00edsticas mesmo mapa compartilham', 'neur\u00f4nios'), 'Os') (('em', 'da perda valida\u00e7\u00e3o'), 'salvar') (('Cada i', 'peso associado denotado por w _ ki'), 'conex\u00e3o') (('Em', 'passa por uma camada convolucional'), 'seguida') (('exce\u00e7\u00e3o da', 'coluna label que identifica o d\u00edgito'), 'recognizer') (('varia\u00e7\u00f5es das', 'classe ImageDataGenerator cheque'), 'pode') (('das corretas', 'geradas pela rede'), 's\u00e3o') (('recorrentes https', 'redes'), 'Quanto') (('Por exemplo', 'vetor mathbf y poderia ser 0 0 0'), 'mathbf') (('de o problema de detec\u00e7\u00e3o contexto', 'n\u00e3o supervisionado PCA'), 'falha') (('redes', 'que h\u00e1 depend\u00eancia observa\u00e7\u00f5es como processos din\u00e2micos textos ou sequ\u00eancias'), 'recorrente') (('Como', 'para'), 'veremo') (('tamb\u00e9m Uma m\u00e9trica muito', 'F_1 https'), 'usada') (('gradientesMesmo grande de efici\u00eancia introduzido 2 explos\u00e3o de o ganho s\u00e9rias', 'de redes profundas treinamento persistiram'), 'desaparecimento') (('def_front acordo os de nomes', 'das classes'), 'submersible_pump') (('Al\u00e9m dessas h\u00e1 v\u00e1rias', 'ativa\u00e7\u00e3o SELU https fun\u00e7\u00f5es de arxiv'), 'outras') (('sem o o erro de erro', 'treinamento'), 'aprendizado') (('de treino valida\u00e7\u00e3o est\u00e3o h\u00e1 curvas pr\u00f3ximas indicativo de que n\u00e3o muito', 'sobreajuste'), 'e') (('depender das', 'dos sinais de entrada'), 'dessa') (('seguir ilustra receptivo faz com formato conceito de campo de neur\u00f4nio opera\u00e7\u00e3o de o mapa de sa\u00edda tenha menor', 'mapa de o entrada'), 'figura') (('Na c\u00e9lula abaixo', 'sequ\u00eancia'), 's\u00e3o') (('de ajuste par\u00e2metro \u00e9 bom caso desse que', 'feita com base modo que possamos teste para verifica\u00e7\u00e3o da adequa\u00e7\u00e3o escolha seja nos dados de e n\u00e3o de teste de dados de da'), 'os') (('tais aplica\u00e7\u00f5es', 'bem menos comuns'), 's\u00e3o') (('Para resultado utilizaremos redes com m\u00faltiplas camadas que constituem o o principal objeto de estudo', 'learning https deep pt'), 'melhorar') (('Crie', 'que usamos apenas modelo MNIST e Fashion MNIST'), 'ensemble') (('Nas pr\u00f3ximas c\u00e9lulas essa import\u00e2ncia ficar\u00e1 ainda mais clara por uma an\u00e1lise meio de detalhada', 'de teste'), 'conjunto') (('algumas \u00e9pocas', 'atinja erro baixo'), 's\u00e3o') (('at\u00e9 ent\u00e3o popular fun\u00e7\u00e3o de ativa\u00e7\u00e3o', 'at\u00e9 dos pesos com uma ent\u00e3o popular estrat\u00e9gia de distribui\u00e7\u00e3o'), 'os') (('de ativa\u00e7\u00e3o varphi u', 'cases'), 'neur\u00f4nios') (('Para esse objetivo o ideal seria', 'direto ou mantivesse constante antes e passar por uma de sinal se depois de camada'), 'que') (('J\u00e1', 'de alguma outra qualquer posi\u00e7\u00e3o amostra'), 'esse') (('sinal estat\u00edsticas', '\u00e9'), 'par\u00eantese') (('uso', 'cient\u00edfica vinda principalmente da ind\u00fastria de comunidade videogames'), 'desenvolvimento') (('100 atingir acur\u00e1cia de com baixo custo adicional de HH', 'para'), 'favor\u00e1vel') (('duas camadas al\u00e9m entrada que n\u00e3o precisa acima da camada de ser', 'ambas definidas adicionada classe keras'), 'adicionando') (('Outra vantagem', 'suave https \u00e9 pt'), 'quando') (('que criaremos ao', 'respectivos desempenhos'), 'inscreva') (('De', 'fazer com que o formato seja'), 'modo') (('o solidifica forma desejada como', 'seguir Os defeitos https'), 'o') (('edu ang', 'icml09 LargeScaleUnsupervisedDeepLearningGPU'), 'papers') (('batch 2018 01 10 intuitive why normalization', 'really normalization'), 'com') (('\u00e9 atualizar objetivo mais', 'que ainda n\u00e3o foram muito par\u00e2metros atualizados'), 'os') (('dos apresentados', 'classifica\u00e7\u00f5es ao n\u00famero de conjunto'), 'este') (('neurais que', 'arquiteturas e m\u00faltiplas camadas'), 'verdade') (('das primeiras', 'neurais propostas'), 'wiki') (('nenhum c\u00e1lculo e apenas por \u00e9 respons\u00e1vel distribuir', 'camadas internas'), 'sa\u00edda') (('se', 'derivada segunda os m\u00e9todos de Newton informa\u00e7\u00f5es de s\u00e3o https'), 'gradiente') (('Minilotes vimos nas e SGD', 'divididos pequenas dados s\u00e3o por\u00e7\u00f5es'), 'm\u00e9todo') (('Casting_defect podem de naturezas', 'www'), 'diferente') (('ativadas e depois somente', 'e comparando os desempenhos'), 'verificar') (('8 sendo classificado como 6 cinco', 'cinco classificado como 7 vezes'), 'que') (('Nos casos', 'podem utilizadas transfer\u00eancia de aprendizado ser t\u00e9cnicas de https'), 'poucos') (('aprendizado', 'padr\u00f5es locais por meio da opera\u00e7\u00e3o de convolu\u00e7\u00e3o https'), 'efetua') (('dados os dispon\u00edveis', 'Digit Recognizer https www'), 'competi\u00e7\u00e3o') (('ModelagemA estrat\u00e9gia', 'ser\u00e1 criar m\u00faltiplas e juntar'), 'aqui') (('01852 fun\u00e7\u00e3o', 'nome de PReLU Parametric o ReLU'), 'recebe') (('camadas como uma inicializa\u00e7\u00e3o', 'classe keras'), 'add') (('api', 'image imagedatagenerator class m\u00f3dulo keras'), 'dispon\u00edvel') (('api e aprenda como utilizar os recursos io explicados', 'fun\u00e7\u00f5es de ativa\u00e7\u00e3o'), 'aqui') (('dos 10 dados', 'por 0 1 meio da divis\u00e3o por'), 'fun\u00e7\u00e3o') (('especialmente apropriada para aplica\u00e7\u00f5es de vis\u00e3o chamadas', 'convolucionais https'), 'em') (('Em', 'keras https particular biblioteca keras'), 'aqui') (('\u00c9 conveniente ter maior de filtros nas camadas mais profundas h\u00e1 muito n\u00famero geral mais', 'por meio da dos padr\u00f5es identifica\u00e7\u00e3o de se simples'), 'porque') (('\u00e9', 'classe keras'), 'deve') (('net publication', 'Recurrent Neural Network RNN'), 'figure') (('Algumas sugest\u00f5es arquiteturas das', 'e os filtros taxas de dropout demais hiperpar\u00e2metros n\u00fameros de etc'), 'Varie') (('que', 'entrada podem ser utilizados passos largos de strides'), 'os') (('o n\u00famero de \u00e9 quantidade de vezes', 'rede durante o treinamento'), 'palavras') (('org \u00e9 uma p\u00e1gina interessante muito para', 'das redes neurais'), 'ganhar') (('mas surpreendentemente efetiva para melhorar', 'generaliza\u00e7\u00e3o das capacidade de redes'), 'simples') (('ajudam ou', 'mesmo o desempenho'), 'testes') (('algoritmo', 'minimiza\u00e7\u00e3o de fun\u00e7\u00f5es descendente de https'), 'gradiente') (('que de uma certa parcela aleat\u00f3ria dos cada 0580 desativa\u00e7\u00e3o passo', 'treinamento'), 'consiste') (('minilotes foram processados', 'dados passou pela de rede'), 'quando') (('seguir', 'uma superf\u00edcie partindo de tr\u00eas pontos distintos'), 'algoritmo') (('o problema', 'dos explos\u00e3o gradientes'), 'recorrente') (('MLP e \u00e9 respons\u00e1vel por', 'classifica\u00e7\u00e3o'), 'efetuar') (('Funcionamento da rede convolucional Para alimentar uma', 'rede \u00e9 necess\u00e1rio represent\u00e1 la como tensor'), 'imagem') (('aprendizado se de supervisionado', 'est\u00e3o sa\u00edda dispon\u00edveis'), 'os') (('tamanhos de caracter\u00edsticas dos mapas diminuem', 'Como de costume'), 'rede') (('Para', 'consulte esta p\u00e1gina https www'), 'detalhe') (('de maior velocidade converg\u00eancia', 'treinamento'), 'pela') (('risco sobreajuste por de conta', 'de par\u00e2metros'), 'excesso') (('em', 'dire\u00e7\u00f5es descida promissoras'), 'S0893608098001166') (('Pesquise e natureza', 'conjunto'), 'entenda') (('pixels da o que melhora o desempenho 255 faixa 1 das', 'neurais'), 'os') (('das', 'MLP que estudamos at\u00e9 agora'), 'rede') (('quando novas solu\u00e7\u00f5es para', 'problemas foram surgindo'), 'foi') (('dos passos', 'momento rastreia'), 'segundo') (('same padding', 'de modo que sa\u00edda possua o da mapa de entrada o mapa de mesmo formato entrada'), 'estrat\u00e9gia') (('rede n\u00famero fixo pequeno \u00e9 que', 'evitar problemas num\u00e9ricos'), 'normalization_layers') (('estrat\u00e9gias envolvem', 'distribui\u00e7\u00f5es estat\u00edsticas com caracter\u00edsticas partir de espec\u00edficas'), 'aleat\u00f3rias') (('him', 'trunk'), 'wiki') (('300x300 na escala de ou', 'seja canal de apenas 1 cores'), 'm\u00e9todo') (('i', 'e dos ii pesos'), 'aleatoriedade') (('algumas amostras aplique uma MLP e me\u00e7a o desempenho', 'acur\u00e1cia simples'), 'class') (('de c\u00e1lculo o motor utilizado possibilidade de utiliza\u00e7\u00e3o de diversos motores aqui', 'o TensorFlow https www'), 'para') (('preju\u00edzo pelo pe\u00e7as boas ou causado desperd\u00edcio de seja', 'deve corte que m\u00e1xima o risco de falsos positivos se precis\u00e3o'), 'balan\u00e7o') (('I', 'tipo'), 'c3') (('com menor', 'disponibilidade de dados'), 'wiki') (('scikit', 'de que parece haver menos diferen\u00e7a sobreajuste'), 'usar') (('segundo', 'dia por dois mais popular motivos'), 'gr\u00e1fico') (('pdf foi', 'o modelo img src https upload'), 'constru\u00edda') (('redu\u00e7\u00e3o', 'erro'), 'quanto') (('de o algoritmo minimiza\u00e7\u00e3o', 'chamado taxa de aprendizado \u00e9 de https'), 'do') (('cada classe e cada classe o vetor verdadeiro de probabilidades de mathbf p o vetor de probabilidades de predito', 'rede'), 'duas') (('\u00c9 uma abordagem', 'que utilizamos'), 'semelhante') (('Poder\u00edamos tentar', 'diferen\u00e7a'), 'corrigir') (('dos desliga 20 neur\u00f4nios', 'camada de sa\u00edda'), 'layers') (('Google', 'o seu baixando pr\u00f3prio imagens'), 'pode') (('seguir abordaremos', 'dia'), 'influente') (('desde aparece com frequ\u00eancia nas das', 'neurais modernas'), '03167') (('ridge https regress\u00e3o scikit', 'html regulariza\u00e7\u00e3o'), 'origem') (('Quando', 'zero'), 'regulariza\u00e7\u00e3o') (('1527 foram', 'de atenua\u00e7\u00e3o'), 'os') (('715 amostras', 'junto com classe verdadeira'), 'figura') (('Aplique', 'alguns modelos'), 'MNIST') (('perc\u00e9ptrons', 'inicial empolga\u00e7\u00e3o se que'), 'percebeu') (('por 09820 exemplo', 'conferir dicas e boas pr\u00e1ticas para essa tarefa'), 'pode') (('org wiki d\u00edgitos escritos por composto de imagens de funcion\u00e1rios', 'United States Census Bureau https'), 'MNIST_database') (('api', 'image imagedatagenerator class para mais detalhes'), 'io') (('da imagem', 'dos d\u00edgitos entre'), 'softmax_function') (('dos partir dados', 'm\u00faltiplas camadas'), 'seja') (('e', 'precis\u00e3o P'), 'Calculando') (('o que margem para indica melhoria', 'desempenho'), 'evidente') (('solu\u00e7\u00e3o', 'chamado de Teorema da Aproxima\u00e7\u00e3o Universal de resultado muito importante https'), 'para') (('das probabilidades', 'por cada rede'), 'do') (('ajudam', 'nada'), 'incr\u00edvel') (('Talvez outra estrat\u00e9gia ou arquitetura', 'resultados mais est\u00e1veis'), 'leve') (('se n\u00e3o todos', 's\u00e3o esparsamente que cada neur\u00f4nio'), 'rede') (('treino conjunto de completo', 'separa\u00e7\u00e3o treino valida\u00e7\u00e3o'), 'submeter') (('para montar uma de revis\u00e3o dos usado estrat\u00e9gia resultados', 'modelo'), 'do') (('implementa\u00e7\u00f5es das ideias', 'ou outras de ideias'), 'internet') (('pdf rebarbas porosidades', 'trincas superf\u00edcie'), 'etc') (('que voc\u00ea objetivo \u00e9 ganhe', 'API e implementar'), 'familiaridade') (('Para discuss\u00f5es', 'respeito consulte SANTUNKAR'), 'interessante') (('e nosso modelo F_1 de Existe', 'outra tende'), 'wiki') (('ser\u00e3o', 'de alimenta\u00e7\u00e3o direta'), 'notebook') (('das m\u00e9tricas e valores da perda treino e Os resultados dados de j\u00e1', 'evolui'), 'do') (('M\u00e3o', 'desempenhos deste dos modelos notebook'), 'massa') (('imagem', 'quanto classe mais perto de 1 mais'), 'mais') (('org wiki Sigmoid_function', 'log\u00edstica'), 'como') (('cada passo itera\u00e7\u00e3o', 'inclina\u00e7\u00e3o da fun\u00e7\u00e3o'), 'que') (('\u00c9 uma estrutura de aprendizado', 'que c\u00f3rtex visual https'), 'an\u00e1loga') (('dois que nossos modelos precisar\u00e3o distinguir Gerando S\u00e3o tipos de imagens dados', 'ImageDataGenerator https classe keras'), 'os') (('ou 15x20x3 seja', 'de 15 linhas 20 colunas e 3 canais cores'), 'exemplo') (('acur\u00e1cia ou', 'predi\u00e7\u00f5es simples propor\u00e7\u00e3o de verdadeiras'), 'classifica\u00e7\u00e3o') (('estrat\u00e9gia de momentos de Nesterov', 'mpawankumar'), 'passo') (('usadas atualmente da classe de retificadores', 'https'), 'ativa\u00e7\u00e3o') (('Cada', 'bem simples'), 'neur\u00f4nio') (('dos propaga\u00e7\u00e3o sinais', 'rede'), 'garantir') (('API da 2 fun\u00e7\u00e3o', 'massa'), 'sklearn') (('em', 'entrada era vetor'), 'que') (('\u00e9poca', 'da Intelig\u00eancia Artificial m\u00e1quina Mark I'), 'simplicidade') (('tentar Para concluir \u00e9 importante fazer uma ressalva', 'natureza dos dados de teste'), 'pode') (('Cada', 'de minilote \u00e9 chamada minibatch'), 'por\u00e7\u00e3o') (('diversos padr\u00f5es', 'imagem'), 'ilustra') (('com', 'batch normalization esta https'), 'mlexplaine') (('desempenho dos o modelos', 'cores fora da foram de diagonal normalizadas'), 'verificar') (('seguir por exemplo fornece dicas para que se', 'pe\u00e7as'), 'v\u00eddeo') (('fun\u00e7\u00e3o', 'regi\u00e3o negativa da ReLU'), 'gr\u00e1fico') (('Para mais', 'sobre gradiente algoritmos de descendente'), 'detalhe') (('da minimiza\u00e7\u00e3o seja', '\u00e9 ao modelo para que um minilote de fornecido passo'), 'acontece') (('maneira mais', '\u00e9'), 'intuitiva') (('Uma variante muito usada nas', 'estoc\u00e1stico SGD \u00e9 gradiente descendente https'), 'rede') (('modelos de aprendizado de geral \u00e9', 'Regulariza\u00e7\u00e3oUma cr\u00edtica de redes modelos neurais'), 'recorrente') (('propaga\u00e7\u00e3o', 'reverso quais citamos novas sentido fun\u00e7\u00f5es de ativa\u00e7\u00e3o'), 'ent\u00e3o') (('da recomendo esta aproxima\u00e7\u00e3o universal leitura', 'neuralnetworksanddeeplearning'), 'seja') (('primeiro', 'da fun\u00e7\u00e3o log\u00edstica evidente como para de 0 valores f u fica valores de u afastados os de saturam'), 'exemplos') (('momento', 'F\u00edsica'), 'com') (('Quando', 'ser aprendido como sugerido por HE et se torna par\u00e2metro al'), 'rede') (('Nesse', 'conveniente'), 'minilote') (('Recomendo que voc\u00ea pouquinho os diversos recursos e brinque possibilidades', 'seguir'), 'com') (('padr\u00f5es locais podem', 'bordas curvas etc'), 'esse') (('por\u00e7\u00e3o da camada anterior que se conecta', 'de \u00e9 chamada campo receptivo'), 'seu') (('da', 'para'), 'efetuado') (('I', 'tipo'), 'de') (('filtros cada dando', 'mapa de caracter\u00edsticas'), 'origem') (('MLP', 'par\u00e2metros \u00e9 quantidade de menor'), 'do') (('neur\u00f4nio \u00e9 expressa por y_k varphi varphi', '_ i'), 'leave') (('Nas 400 height 400 redes', 'pode ser para nas alimenta\u00e7\u00e3o sinal redes de direta'), 'width') (('da', 'o gradiente'), 'com') (('Tipicamente se v\u00e1rios', 'da rede'), 'utilizam') (('margem para melhorias nos resultados e nas', 'computacionais'), 'efici\u00eancia') (('Cada', 'caracter\u00edsticas de mapa de corresponde'), 'elemento') (('Algumas', 'esta https www'), 'excelente') (('Heaviside muito utilizada fun\u00e7\u00e3o de at\u00e9', '\u00e9 composta segmentos de planos'), 'foi') (('que foi i confundida', 'classe j.'), 'confus\u00e3o') (('com \u00e9 composto por imagens de pe\u00e7as', 'resultantes de fundi\u00e7\u00e3o https e defeituosas processo de pt'), 'regulare') (('com muitas se mostrou bem mesmo o advento', '1986'), 'de') (('Pronto J\u00e1 temos', 'objetos respons\u00e1veis por gerar nossos dados treino de teste'), 'os') (('de processo aprendizado', 'seres'), 'ajuste') (('zerando todos', 'quando h\u00e1 apenas uma posi\u00e7\u00e3o errada plotando selecionando'), 'c\u00e9u') (('dimens\u00e3o m\u00e1ximo 3 terceira da', 'opera\u00e7\u00e3o'), 'receptivo') (('padr\u00f5es que envolvem todos os pontos', 'dada amostra ou observa\u00e7\u00e3o'), 'aprender') (('valor m\u00e1ximo sendo escolha essa', 'mais usada img src https www'), 'os') (('https scikit', 'vamos utilizar o modelo dispon\u00edvel'), 'MLP') (('em probabilidades', 'das amostras encontra considerada se suspeita'), 'distante') (('end cases', 'degrau de Heaviside u \u00e9 chamada de fun\u00e7\u00e3o https'), 'varphi') (('sendo quantidade de dados gerada', 'das mais diversas partir fontes'), 'do') (('i', 'com conceitos mais e ii representa\u00e7\u00f5es mais abstratas de outras menos termos abstratas'), 'excelente') (('videoaulas Para', 'https www'), 'notebook') (('ela funcionar t\u00e3o porqu\u00ea de bem', 'ainda \u00e9 uma aberto'), 'quest\u00e3o') (('um erro recorrente', 'semelhan\u00e7a entre'), 'utilizando') (('grande quantidade rede com apenas uma camada de o \u00e9 preciso compil\u00e1 Perceba de em Antes lo', 'de m\u00e9tricas avalia\u00e7\u00e3o'), 'io') (('outras', 'de corte que separa cada uma das duas classes'), 'palavras') (('org wiki United_States_Census_Bureau e estudantes', 'm\u00e9dio ensino estadunidenses'), 'do') (('aumento', 'termo de regulariza\u00e7\u00e3o'), 'resultar') (('org wiki Learning_rate e constitui talvez hiperpar\u00e2metro mais o importante', 'modelo'), 'do') (('o que', 'objetivo e dos m\u00e9todos de facilita tarefa minimiza\u00e7\u00e3o'), 'diminui\u00e7\u00e3o') (('contexto da vis\u00e3o computacional como problema de classifica\u00e7\u00e3o', 'utilizando modelos supervisionados redes de convolucionais'), 'retomaremo') (('os neur\u00f4nios', 'todos'), 'densa') (('V\u00e1rios algoritmosExistem v\u00e1rias minimiza\u00e7\u00e3o t\u00e9cnicas', 'gradiente fam\u00edlia de algoritmos de descendente'), 'encarar') (('capacidade de generaliza\u00e7\u00e3o', 'modelo'), 'lote') (('Podemos proceder', 'de etapa modelagem'), 'para') (('caracter\u00edsticas complexas dos sistemas nervosos biol\u00f3gicos', 'vastas e complexas redes contendo bilh\u00f5es de c\u00e9lulas cada uma podendo efetuar milhares de conex\u00f5es'), 'emergem') (('j\u00e1 que custo um \u00fanico falso ou de negativo', 'pode ser alto como discutido https www'), 'o') (('s\u00e3o inerentemente', 'por conta da complexidade matem\u00e1tica e da adapta\u00e7\u00e3o excessiva capacidade de resultantes'), 'ruins') (('VP VP FP R', 'que'), 'frac') (('terminais', 'sinapses https se pt'), 'wiki') (('sobreajuste por Problema 4 risco de conta', 'velhas regulariza\u00e7\u00f5es L_1 e L_2 podem ser boas solu\u00e7\u00f5es'), 'excesso') (('Incentivo', 'testar suas novas habilidades com outros dados'), 'voc\u00ea') (('natureza', 'filtro'), 'padr\u00e3o') (('ou seja', 's\u00e3o calculados e erros pesos s\u00e3o atualizados'), 'algoritmo') (('org wiki e \u00e9 respons\u00e1vel', 'nas n\u00e3o redes'), 'activation_function') (('dos dados de entrada', 'largura canais de o formato altura cores'), 'efetuado') (('com que uma parte', 'zero'), 'o') (('Pronto J\u00e1 implementamos o modelo e entendemos', 'detalhes matem\u00e1ticos'), 'os') (('\u00c9 uma informa\u00e7\u00f5es das bordas maneira de ao', 'da rede'), 'preservar') (('org pequenas e m\u00faltiplas Dendrito s\u00e3o ramifica\u00e7\u00f5es', 'corpo'), 'wiki') (('Para maior detalhamento', 'confus\u00e3o matriz de https'), 'resultado') (('cada passo e produto armazenando', 'o'), 'se') (('regulariza\u00e7\u00e3o muito', 'dropout https'), 'usada') (('Cada', 'artificial \u00e9 uma unidade que aceita m conex\u00f5es por meio de local das'), 'mimetiza') (('treinamento eficaz algoritmo de para', 'com v\u00e1rias camadas'), 'lutou') (('converg\u00eancia', 'treinamento'), 'impedindo') (('fornece sa\u00edda para cada uma probabilidade como ou', 'n\u00famero entre'), 'resultadosPlotando') (('s\u00e3o determinados maneira de \u00f3tima', 'rede'), 'veio') (('derivadas', 'de por\u00e7\u00f5es aleat\u00f3rias dos dados o que problemas como m\u00ednimos locais'), 'reduz') (('MLP tal darmos uma olhada nas imagens Que que', 'rede'), 'rela\u00e7\u00e3o') (('rede keras podemos', 'classe keras'), 'neural') (('estrat\u00e9gia momentos de par\u00e1grafo', 'estende'), '6980') (('org wiki', 'erro'), 'para') (('api', 'dos arquivos dispon\u00edveis partir nas'), 'modelo') (('Ser\u00e3o dois caso ambos constituindo estudos de aplica\u00e7\u00f5es', 'https tipo vis\u00e3o computacional pt'), 'do') (('conveniente pois j\u00e1 t\u00ednhamos', 'com sua interface'), 'MLP') (('ver com', 'ver com quantidade'), 'exatid\u00e3o') (('corretamente', 'detectadas'), 'respeito') (('muitos n\u00e3o em', 'neurais'), 'regulariza\u00e7\u00e3o') (('M\u00e3o', 'Keras https keras'), 'Explore') (('da fun\u00e7\u00e3o perda por meio', 'dos pesos w.'), 'regulariza\u00e7\u00e3o') (('Nas', 'acordo arquitetura da de rede'), 'rede') (('isso mesmo aquela', 'C\u00e1lculo'), 'do') (('pdf o revolucion\u00e1rio algoritmo da retropropaga\u00e7\u00e3o', 'erro https'), 'propuseram') (('deve hiperpar\u00e2metro ser', 'priori'), 'determinado') (('org wiki Graphics_processing_unit para das', 'forma massivamente profundas de paralela'), 'efetuar') (('imagem', 'toda'), 'os') (('propor\u00e7\u00f5es entre classes', 'bem balanceadas'), 'separar') (('Modelos de aprendizado profundo que s\u00e3o incontestavelmente \u00fateis reconhecem rostos', 'textos etc'), 'imagen') (('bom pequeno', 'sobreajuste mas nada de grave'), 'desempenho') (('artigo muito divertido foram', 'com quatro implementar de o elefante par\u00e2metros'), 'pdf') (('Fazendo analogia', 'o modelo da o \u00e9 como se intercepto reta'), 'com') (('br upload', 'pecas fabricadas pelo processo de fundicao'), 'file') (('org wiki Visual_cortex estrutura', 'respons\u00e1vel pelo'), 'c\u00e9rebro') (('erros', 'tipo II https pt'), 'associada') (('do algoritmo', 'descendente https'), 'gradiente') (('Quanto dos passados dado par\u00e2metro o', 'de taxa aprendizado correspondente'), 'maior') (('Uma normaliza\u00e7\u00e3o camada de por', 'https keras'), 'lote') (('uma camada oculta conseguem aproximar qualquer fun\u00e7\u00e3o cont\u00ednua redes', 'de uma camada oculta conseguem aproximar qualquer mais fun\u00e7\u00e3o'), 'wiki') (('primeira etapa passe direto calcula chamada de se', 'em etapas'), 'algoritmo') (('das', '784 pixels'), 'reflete') (('rede tenta minimizar durante', 'manipulando'), '00877') (('Uma das', 'regulariza\u00e7\u00e3o https'), 'estrat\u00e9gias') (('padr\u00f5es locais', 'v\u00e3o sendo agregados e cada vez mais tornando se complexos'), 'simples') (('modelo', 'fun\u00e7\u00e3o summary https keras'), 'verificar') (('Isso \u00e9 poss\u00edvel porque existem estrat\u00e9gias', 'justamente para lidar com essa quest\u00e3o'), 'desenvolvidas') ", "extra": "['test']"}