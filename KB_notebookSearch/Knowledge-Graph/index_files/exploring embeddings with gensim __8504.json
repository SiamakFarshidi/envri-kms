{"name": "exploring embeddings with gensim ", "full_name": " h1 Looking up embeddings h2 Vector similarity h2 Cosine Distance h1 Exploring embeddings with Gensim h1 Semantic vector math h2 Analogy solving h1 Your turn h3 P S ", "stargazers_count": 0, "forks_count": 0, "description": "Fortunately there s already a library for exactly this sort of work Gensim. Let s look at the closest neighbours for a few more movies from a variety of genres Artsy erotic dramas raunchy sophomoric comedies old school musicals superhero movies. Embeddings can be thought of as a smart distance metric. If they point in opposite directions it s 1. Embeddings are powerful But how do they actually work Previously I claimed that embeddings capture the meaning of the objects they represent and discover useful latent structure. How does this compare to a pair of movies that we would think of as very different As expected much further apart. We ll grab the layer responsible for embedding movies and use the get_weights method to get its learned weights. Our weight matrix has 26 744 rows for that many movies. Your turn Head over to the Exercises notebook https www. Why would you want to do that It turns out that doing addition and subtraction of embedding vectors often gives surprisingly meaningful results. at right angles their cosine similarity is 0. most_similar a b then instead of finding the vector closest to a it will find the closest vector to a b. In terms of vector math we can frame this as. Of course it s Toy Story I should have recognized that vector anywhere. Is our model wrong or were we Another difference we failed to account for between Cars 2 and Brave is that the former is a sequel and the latter is not. While the library is most frequently used in the text domain we can use it to explore embeddings of any sort. com gensim models keyedvectors. Earlier we trained a model to predict the ratings users would give to movies using a network with embeddings learned for each movie and user. If two vectors point in the same direction their cosine similarity is 1. WordEmbeddingsKeyedVectors with our model s movie embeddings and the titles of the corresponding movies. So it s learned something about 3 d animated kids flick but maybe that was just a fluke. Limit to movies with at least this many ratings in the dataset Split long titles over multiple lines. Okay so which movies are most similar to Toy Story Wow these are pretty great It makes perfect sense that Toy Story 2 is the most similar movie to Toy Story. If you have a moment to fill out a super short survey about this lesson https form. So how do we check whether these representations are sane and coherent Vector similarityA simple way to test this is to look at how close or distant pairs of movies are in the embedding space. WordEmbeddingsKeyedVectors. If we wanted to assign a single number to their similarity we could calculate the euclidean distance between these two vectors. You can also leave public feedback in the comments below or on the Learn Forum https www. The embedding weights are part of the model s internals so we ll have to do a bit of digging around to access them. com kernels fork 1598893 to get some hands on practice working with exploring embeddings with gensim. But we can actually ask Gensim to fill in the blank for us via vector math after some rearranging ________ Scream PsychoIf you are familiar with these movies you ll see that the missing ingredient that takes us from Psycho to Scream is comedy and also late 90 s teen movie ness. We never directed the model about how to use any particular embedding dimension. Okay I m being facetious. spatial module https docs. Cosine DistanceIf you check out the docs for the scipy. Each row is 32 numbers the size of our movie embeddings. What s a milder form of stare A good answer here would be glance or look. Comparing dimension by dimension these look vaguely similar. Easy to grok in 1 2 or 3 dimensions. Let s calculate a couple cosine distances between movie vectors Aside Why is cosine distance commonly used when working with embeddings The short answer as with so many deep learning techniques is empirically it works well. This is our conventional as the crow flies notion of distance between two points. One interpretation would be that Brave is like Cars 2 except that the latter is aimed primarily at boys and the former might be more appealing to girls given its female protagonist. org wiki Cosine_similarity. So maybe the answer should be like Pocahontas a mid 90 s conventional animation kids movie but more of a boy movie. html you ll see there are actually a lot of different measures of distance that people use for different tasks. Sequelness is an important property to our model which suggests that some of the variance in our data is accounted for the fact that some people tend to like sequels more than others. If they re orthogonal i. com 82826473884269 I d greatly appreciate it. Aside You may notice that Gensim s docs and many of its class and method names refer to word embeddings. It s hard to make anything of these vectors at this point. When judging the similarity of embeddings it s more common to use cosine similarity https en. What about Brave Cars 2 Pocahontas _____ The answer is not clear. Exploring embeddings with GensimI ll instantiate an instance of WordEmbeddingsKeyedVectors https radimrehurek. Cars 2 Brave X _____ Pocahontas X Rearranging we get ____ Pocahontas Cars 2 Brave We can solve this by passing in two movies Pocahontas and Cars 2 for the positive argument to most_similar with Brave as the negative argument This weakly fits our prediction the 4 closest movies are indeed kids animated movies from the 90s. 7 10 of our results are also sequels. In the exercise coming up you ll get to do a little hands on investigation that digs into this question more deeply. It s kind of astounding that this works but people have found that these can often be effectively solved by simple vector math on word embeddings. If our embedding matrix is any good it should map similar movies like Toy Story and Shrek to similar vectors. our embeddings manage to nail a wide variety of cinematic niches Semantic vector mathThe most_similar https radimrehurek. A shower is a milder form of a deluge. Hercules The Lion King Let s ask our embeddings what they think. Can we solve movie analogies with our embeddings Let s try. Let s look at an example movie vector What movie is this the embedding of Let s load up our dataframe of movie metadata. For example how would you fill in the following equation Scream Psycho ________ Scream and Psycho are similar in that they re violent scary movies somewhere on the border between Horror and Thriller. And most of the rest are animated kids movies with a similar computer animated style. Analogy solvingThe SAT test which is used to get into American colleges and universities poses analogy questions like shower deluge _____ stare Read shower is to deluge as ___ is to stare To solve this we find the relationship between deluge and shower and apply it to stare. We left it alone to learn whatever representation it found useful. This course is still in beta so I d love to get your feedback. In brief the cosine similarity of two vectors ranges from 1 to 1 and is a function of the angle between the vectors. After that the results are a bit more perplexing. The biggest difference is that Scream has elements of comedy. Cosine distance is just defined as 1 minus the cosine similarity and therefore ranges from 0 to 2. So I d say Scream is what you d get if you combined Psycho with a comedy. Let s put that to the test Looking up embeddingsLet s load a model we trained earlier so we can investigate the embedding weights that it learned. Mathematically we can also extend it to 32 dimensions though good luck visualizing it. Which movies are most similar to Toy Story Which movies fall right between Psycho and Scream in the embedding space We could write a bunch of code to work out questions like this but it d be pretty tedious. most_similar method optionally takes a second argument negative. This tells us something interesting about our learned embeddings and ultimately about the problem of predicting movie preferences. ", "id": "colinmorris/exploring-embeddings-with-gensim", "size": "8504", "language": "python", "html_url": "https://www.kaggle.com/code/colinmorris/exploring-embeddings-with-gensim", "git_url": "https://www.kaggle.com/code/colinmorris/exploring-embeddings-with-gensim", "script": "WordEmbeddingsKeyedVectors gensim.models.keyedvectors pyplot numpy matplotlib tensorflow scipy.spatial pandas pyplot as plt keras plot_most_similar distance ", "entities": "(('it', 'similar vectors'), 'map') (('You', 'Learn Forum https www'), 'leave') (('cosine similarity', 'right angles'), 'be') (('Cosine distance', '2'), 'define') (('course', 'feedback'), 'be') (('we', 'this'), 'frame') (('people', 'more others'), 'be') (('cosine similarity', 'vectors'), 'range') (('they', 'what'), 'let') (('we', 'it'), 'pose') (('So maybe answer', 'animation kids boy mid 90 s conventional movie'), 'be') (('you', 'lesson https form'), 'have') (('s', 'movie metadata'), 'let') (('Toy Story', 'Toy 2 most similar Story'), 'be') (('former', 'female protagonist'), 'be') (('4 closest movies', '90s'), 'car') (('Scream', 'comedy'), 'be') (('Artsy erotic dramas', 'school musicals superhero old movies'), 'let') (('these', 'word embeddings'), 's') (('it', 'that'), 'let') (('It', 'point'), 's') (('they', 'useful latent structure'), 'be') (('we', 'around them'), 'be') (('we', 'two vectors'), 'calculate') (('you', 'comedy'), 'say') (('it', 'cosine similarity more https'), 's') (('we', 'sort'), 'use') (('doing', 'often surprisingly meaningful results'), 'want') (('docs', 'word embeddings'), 'notice') (('empirically it', 'short answer'), 'let') (('crow', 'two points'), 'be') (('they', 'Horror'), 'fill') (('row', 'movie 32 embeddings'), 'be') (('most', 'similar computer animated style'), 'animate') (('This', 'movie preferences'), 'tell') (('Embeddings', 'distance smart metric'), 'think') (('that', 'Scream'), 'ask') (('it', 'b.'), 'find') (('d', 'greatly it'), 'com') (('results', 'that'), 'be') (('We', 'learned weights'), 'grab') (('Cosine DistanceIf you', 'scipy'), 'check') (('that', 'question'), 'get') (('it', 'opposite directions'), 's') (('Mathematically we', 'it'), 'extend') (('weight matrix', 'many movies'), 'have') (('Toy I', 'vector'), 's') (('how close pairs', 'embedding space'), 'check') (('answer', 'Pocahontas _ _ _ _ Brave Cars 2 _'), 'about') (('embeddings', 'niches Semantic cinematic vector'), 'manage') (('s', 'milder stare good answer'), 'be') (('latter', 'Cars'), 'be') (('s', 'embeddings'), 'solve') (('about 3 d animated maybe that', 'something'), 'learn') (('people', 'different tasks'), 'see') (('we', 'that'), 'compare') (('ratings users', 'movie'), 'train') (('cosine similarity', 'same direction'), 'be') (('Exploring', 'https radimrehurek'), 'instantiate') (('it', 'representation'), 'leave') (('We', 'it'), 'be') (('most_similar method', 'optionally second argument'), 'take') (('We', 'embedding how particular dimension'), 'direct') (('these', 'dimension'), 'look') ", "extra": "['test']"}