{"name": "introduction to manual feature engineering ", "full_name": " h1 Introduction Manual Feature Engineering h2 Example Counts of a client s previous loans h2 Assessing Usefulness of New Variable with r value h3 Kernel Density Estimate Plots h2 Aggregating Numeric Columns h3 Correlations of Aggregated Values with Target h4 The Multiple Comparisons Problem h2 Function for Numeric Aggregations h3 Correlation Function h2 Categorical Variables h3 Function to Handle Categorical Variables h3 Applying Operations to another dataframe h1 Putting the Functions Together h3 Counts of Bureau Dataframe h3 Aggregated Stats of Bureau Dataframe h3 Value counts of Bureau Balance dataframe by loan h3 Aggregated stats of Bureau Balance dataframe by loan h3 Aggregated Stats of Bureau Balance by Client h2 Insert Computed Features into Training Data h1 Feature Engineering Outcomes h2 Missing Values h2 Calculate Information for Testing Data h2 Correlations h3 Collinear Variables h1 Modeling h3 Control h3 Test One h3 Test Two h1 Results h2 Next Steps ", "stargazers_count": 0, "forks_count": 0, "description": "753 Note that these scores may change from run to run of the notebook. Let s move on to make a few more variables from the bureau dataframe. This might not necessarily be that important as a numeric variable and in future work we might want to consider this as a time variable. This variable represents the average number of monthly records per loan for each client. I encourage others to use the results here to make their own improvements and I will continue to document the steps I take to help others. The best model we made from this data achieved a score on the leaderboard around 0. This demonstrate the benefit of using functions for repeatable workflows Counts of Bureau Dataframe Aggregated Stats of Bureau Dataframe Value counts of Bureau Balance dataframe by loan Aggregated stats of Bureau Balance dataframe by loan Aggregated Stats of Bureau Balance by Client Insert Computed Features into Training Data Feature Engineering OutcomesAfter all that work now we want to take a look at the variables we have created. In this notebook we will explore making features by hand for the Home Credit Default Risk competition. We will be able to reuse this function for calculating numeric stats for other dataframes. We can address this in later notebooks when we look at regularization we already perform some regularization in this model by using reg_lambda and reg_alpha as well as early stopping. Each month has its own row. The agg function will only calculate the values for the numeric columns where the operation is considered valid. These results are better than the control but slightly lower than the raw features. Again we can use these as an approximation of the variables which may be important for modeling. Categorical VariablesNow we move from the numeric columns to the categorical columns. 0 for each observation. We will rename the column with the level 0 name appended with the stat. We can look at the KDE plot of the highest correlated variable bureau_DAYS_CREDIT_mean with the target in in terms of absolute magnitude correlation. Then when our model trains it may overfit to these variables because it thinks they have a relationship with the target in the training set but this does not necessarily generalize to the test set. This measures the strength of a linear relationship between two variables and ranges from 1 perfectly negatively linear to 1 perfectly positively linear. We can also use the percentage of missing values to remove features with a substantial majority of values that are not present. The sum columns represent the count of that category for the associated client and the mean represents the normalized count. In the code below we sort the correlations by the magnitude absolute value using the sorted Python function. __ Test TwoThat was easy so let s do another run Same as before but with the highly collinear variables removed. Since I have limited domain knowledge of loans and what makes a person likely to default I will instead concentrate of getting as much info as possible into the final training dataframe. 014 ROC AUC over the original testing data. We can remove these columns from both the training and the testing datasets. We also make use of an anonymous lambda function another important Python operation that is good to know. This will take the same form as the agg_numeric function in that it accepts a dataframe and a grouping variable. The above dataframes have the calculations done on each _loan_. Now we simply merge with the training data as we did before. SK_ID_CURR credit count cash count home count total count 1 1 0 3 4 2 1 0 0 1 3 1 2 0 3 4 1 0 2 3 Then we can normalize these value counts by the total number of occurences of that categorical variable for that observation meaning that the normalized counts must sum to 1. These are already recorded for us in fi from the original data. Function to Handle Categorical VariablesTo make the code more efficient we can now write a function to handle the categorical variables for us. Viewing the correlations skeptically it does appear that several of the newly created variables may be useful. com the multiple comparisons problem e5573e8b9578. Modeling To actually test the performance of these new datasets we will try using them for machine learning Here we will use a function I developed in another notebook to compare the features the raw version with the highly correlated variables removed. Removing the highly collinear variables slightly decreases performance so we will want to consider a different method for feature selection. In this case we will group by the unique client the SK_ID_CURR column agg perform a calculation on the grouped data such as taking the mean of columns. There is no well established threshold for removing missing values and the best course of action depends on the problem. The idea is that the model will then pick up on which features are important rather than us having to decide that. However just because the variable is correlated does not mean that it will be useful and we have to remember that if we generate hundreds of new variables some are going to be correlated with the target simply because of random noise. First we can calculate the value counts of each status for each loan. We can run this kind of like an experiment and the control will be the performance of just the application data in this function when submitted to the competition. Putting the Functions TogetherWe now have all the pieces in place to take the information from the previous loans at other institutions and the monthly payments information about these loans and put them into the main training dataframe. We will just need to pass in the data to the function which does most of the work for us. At this point we will save both the training and testing data. ControlThe first step in any experiment is establishing a control. The larger the r value of a variable with respect to the target the more a change in this variable is likely to affect the value of the target. As an example if we have the following dataframe SK_ID_CURR Loan type 1 home 1 home 1 home 1 credit 2 credit 3 credit 3 cash 3 cash 4 credit 4 home 4 home we will use this information counting the number of loans in each category for each client. To recap for the bureau_balance dataframe we 1. However we will have to submit the predictions to the leaderboard before we can say if this better validation performance transfers to the testing data. We can look at the percentage of missing values the correlations of variables with the target and also the correlation of variables with the other variables. The resulting KDE will show any significant differences in the distribution of the variable between people who did not repay their loan TARGET 1 and the people who did TARGET 0. We need to merge the original training data with the calculated stats on the SK_ID_CURR column which will insert NaN in any cell for which the client does not have the corresponding statisticWe also use the rename function quite a bit specifying the columns to be renamed as a dictionary. We will stick to using mean max min sum but any function can be passed in here. In a competition such as this even an improvement of this size is enough to move us up 100s of spots on the leaderboard. The process of manual feature engineering will involve plenty of Pandas code a little patience and a lot of great practice manipulation data. The following code creates a set of variables to remove by only adding one of each pair. For each of these pairs of highly correlated variables we only want to remove one of the variables. For now we can just calculate the same aggregation statistics as previously. Based on the distribution clients with a greater number of average monthly records per loan were more likely to repay their loans with Home Credit. For curiousity s sake and because we already wrote the function we can make a kde plot of two of the newly created variables. For this we will use the function defined above that implements a Gradient Boosting Machine model and the single main data source application. Therefore we look for the variables with the greatest absolute value r value relative to the target. This means we can use them in a machine learning model which needs to see the same columns in both the training and testing dataframes. We might have to apply another feature selection method to reduce the dimensionality. This variable represents the number of previous loans with a CREDIT_ACTIVE value of Active divided by the total number of previous loans for a client. Assessing Usefulness of New Variable with r valueTo determine if the new variable is useful we can calculate the Pearson Correlation Coefficient r value between this variable and the target. This requires a number of Pandas operations we will make heavy use of throughout the notebook groupby group a dataframe by a column. I find these confusing and hard to work with so I try to reduce to a single level index as quickly as possible. Introduction Manual Feature EngineeringIf you are new to this competition I highly suggest checking out this notebook https www. The definitions of these data files are bureau information about client s previous loans with other financial institutions reported to Home Credit. Let s now look at the percentage of missing values in the testing data so we can figure out the columns that should be dropped. Let s find the percentage of the top 100 most important features that we made in this notebook. All of our hard work translates to a small improvement of 0. At the end we will wrap up all the code into one function to be re used for many dataframes. The Multiple Comparisons ProblemWhen we have lots of variables we expect some of them to be correlated just by pure chance a problem known as multiple comparisons https towardsdatascience. Calculate Information for Testing DataWe need to align the testing and training dataframes which means matching up the columns so they have the exact same columns. As the number of variables increases the number of datapoints needed to learn the relationship between these variables and the target value increases exponentially. We will have to compare performance after removing these variables with performance keeping these variables the raw csv files we saved earlier. If anyone has a better idea for this process please let me know in the comments We will now go through this process step by step. The highest correlated variable with the target other than the TARGET which of course has a correlation of 1 is a variable we created. __ control only the data in the application files. This dataframe has monthly information about each client s previous loan s with other financial institutions. Let s officially summarize the performances __Experiment__ __Train AUC__ __Validation AUC__ __Test AUC__ __Control__ 0. The definition of this column is How many days before current application did client apply for Credit Bureau credit. We already have calculated all the counts and aggregation statistics so we only need to merge the testing data with the appropriate data. Then we iterate stats we calculated for each client. com willkoehrsen start here a gentle introduction according to a Random Forest and Gradient Boosting Machine. The sum column records the counts and the mean column records the normalized count. I have not observed that the general ordering changes however. Based on these numbers the engineered features perform better than the control case. We see there are a number of columns with a high percentage of missing values. As an example the column with CREDIT_ACTIVE_Active as level 0 and sum as level 1 will become CREDIT_ACTIVE_Active_count. Some of these variables are a little confusing so let s try to explain a few client_bureau_balance_MONTHS_BALANCE_mean_mean For each loan calculate the mean value of MONTHS_BALANCE. For example we can show the kernel density estimate of the previous_loan_count colored by whether the TARGET 1 or 0. There are many considerations that we have to take into account when making features Function for Numeric AggregationsLet s encapsulate all of the previous work into a function. __ Test OneLet s conduct the first test. Let s look for any variables that have a greather than 0. __ ResultsAfter all that work we can say that including the extra information did improve performance The model is definitely not optimized to our data but we still had a noticeable improvement over the original dataset when using the calculated features. Then for each client calculate the mean of this value for all of their loans. Then we can group by the SK_ID_CURR and calculate the aggregations across the loans of each client. Made value counts of each categorical variable grouping by loan3. 745 __Test One__ 0. In an earlier notebook we used only the application data in order to build a model. Example Counts of a client s previous loansTo illustrate the general process of manual feature engineering we will first simply get the count of a client s previous loans at other financial institutions. We can test this function using the EXT_SOURCE_3 variable which we found to be one of the most important variables https www. We can even write our own function and use it in an agg call. bureau_balance monthly information about the previous loans. These are discrete string variables so we cannot just calculate statistics such as mean and max which only work with numeric variables. Columns with too many missing values might have to be dropped. We will hold off on calculating the correlations until we have all the variables together in one dataframe. The function above returns a submission dataframe we can upload to the competition a fi dataframe of feature importances and a metrics dataframe with validation and test performance. Here we have to deal with the fact that the dataframe has a multi level index. We can also visually inspect a relationship with the target using the Kernel Density Estimate KDE plot. Missing ValuesAn important consideration is the missing values in the dataframe. By making numerous small improvements such as in this notebook we can gradually achieve better and better performance. 759 when submitted to the competition. Scroll all the way to the right to see the new column. 759 __Test Two__ 0. The dataframes now have the same columns with the exception of the TARGET column in the training data. First we one hot encode a dataframe with only the categorical columns dtype object. There are a number of tools we can use for this process but in this notebook we will stick to removing columns with a high percentage of missing values and variables that have a high correlation with one another. For example if a client had three previous loans with 3 4 and 5 records in the monthly data the value of this variable for them would be 4. I encourage anyone to try different percentages for dropping the missing columns and compare the outcomes. Instead of grouping this dataframe by the SK_ID_CURR which is the client id we will first group the dataframe by the SK_ID_BUREAU which is the id of the previous loan. To make sure the function worked as intended we should compare with the aggregated dataframe we constructed by hand. CorrelationsFirst let s look at the correlations of the variables with the target. Then we can build the same model and run more experiments to determine the effect of our feature engineering. mean or use the agg function together with a list of transforms grouped_df. We will re use this function when we want to apply the same operations for other dataframes. Then it will calculate the counts and normalized counts of each category for all categorical variables in the dataframe. We can merge this dataframe into the training data. Over half of the top 100 features were made by us That should give us confidence that all the hard work we did was worthwhile. Fortunately we already have a function that does this for us Now we can handle the one numeric column. Again we have to deal with the multi level index for the columns. test one the data in the application files with all of the data recorded from the bureau and bureau_balance files test two the data in the application files with all of the data recorded from the bureau and bureau_balance files with highly correlated variables removed. To do so we groupby the client id agg the grouped dataframe and merge the result back into the training data. Later we can look at using the feature importances returned from models such as the Gradient Boosting Machine or Random Forest to perform feature selection. In order to better this score we will have to include more information from the other dataframes. One hot encoding makes the process of calculating these figures very easy We can use a similar function as before to rename the columns. With a correlation this weak though it is just as likely to be noise as a signal. 33 0 0. Feature selection is the process of removing variables to help our model to learn and generalize better to the testing set. We iterate through the first level level 0 which is the name of the categorical variable appended with the value of the category from one hot encoding. The feature importances may be useful when it s time for feature selection. Examining the feature improtances it looks as if a few of the feature we constructed are among the most important. This might seem like a lot which is why we ll eventually write a function to do this process for us. However rather than just compare to the original features we need to compare to the _one hot encoded_ original features. Let s take a look at implementing this by hand first. client_bureau_balance_STATUS_X_count_norm_sum For each loan calculate the number of occurences of STATUS X divided by the number of total STATUS values for the loan. __Feature selection__ will be an important focus going forward because reducing the number of features can help the model learn during training and also generalize better to the testing data. Correlations of Aggregated Values with TargetWe can calculate the correlation of all new values with the target. This shouldn t be an issue here but when we one hot encode variables we need to align the dataframes to make sure they have the same columns. Now for the new variable we just made the number of previous loans at other institutions. This is useful in order to keep track of the new variables we create. Kernel Density Estimate PlotsThe kernel density estimate plot shows the distribution of a single variable think of it as a smoothed histogram. 66 0 4 1 0 2 3 0. My interpretation is this is the number of days that the previous loan was applied for before the application for a loan at Home Credit. Moreover we can say that some of the features we built are among the most important as judged by the model. To see the different in distributions dependent on the value of a categorical variable we can color the distributions differently according to the category. 745 when submitted to the competition. Here we will look at using information from the bureau and bureau_balance data. This will allow us to see if there are highly collinear variables that should perhaps be removed from the data. com willkoehrsen start here a gentle introduction to get started. Merged the stats and the value counts on the loans4. __The control scores 0. Calculated numeric stats grouping by each loan2. I ve already recorded that performance so we can list out our control and our two test conditions __For all datasets use the model shown below with the exact hyperparameters. 8 correlation with other variables. Fortunately once we have taken the time to write a function using it is simple if there s a central theme in this notebook it s use functions to make things simpler and reproducible. Often we want to remove one in a pair of collinear variables because having both variables would be redundant. We can make hundreds of features and some will turn out to be corelated with the target simply because of random noise in the data. The control slightly overfits because the training score is higher than the validation score. There is plenty more work to be done in this competition and plenty more gains in performance to be had I ll see you in the next notebook. This will give us one row of the dataframe for each loan. If we go through and inspect the values we do find that they are equivalent. We will put this plotting functionality in a function to re use for any variable. 753 when submitted to the competition. From this it s difficult to tell if this variable will be important. We can do this by merging the dataframes together first and then since all the variables are numeric we just need to aggregate the statistics again this time grouping by the SK_ID_CURR. Manual feature engineering can be a tedious process which is why we use automated feature engineering with featuretools and often relies on domain expertise. Let s now read in the testing data perform the same operations and look at the missing values in the testing data. Well this distribution is all over the place. Let s do a reset of all the variables and then use the functions we built to do this from the ground up. The curse of dimensionality is the name given to the issues caused by having too many features too high of a dimension. Next StepsGoing forward we can now use the functions we developed in this notebook on the other datasets. There are still 4 other data files to use in our model In the next notebook we will incorporate the information from these other data files which contain information on previous loans at Home Credit into our training data. Let s not read too much into this value but it could indicate that clients who have had more previous credit history are generally more likely to repay a loan. Therefore a larger negative number indicates the loan was further before the current loan application. pandas and numpy for data manipulation matplotlib and seaborn for plotting Suppress warnings from pandas Read in bureau Groupby the client id SK_ID_CURR count the number of previous loans and rename the column Join to the training dataframe Fill the missing values with 0 Plots the disribution of a variable colored by value of the target Calculate the correlation coefficient between the new variable and the target Calculate medians for repaid vs not repaid Plot the distribution for target 0 and target 1 label the plot print out the correlation Print out average values Group by the client id calculate aggregation statistics List of column names Iterate through the variables names Skip the id name Iterate through the stat names Make a new column name for the variable and stat Assign the list of columns names as the dataframe column names Merge with the training data List of new correlations Iterate through the columns Calculate correlation with the target Append the list as a tuple Sort the correlations by the absolute value Make sure to reverse to put the largest values at the front of list Remove id variables other than grouping variable Group by the specified variable and calculate the statistics Need to create new column names Iterate through the variables names Skip the grouping variable Iterate through the stat names Make a new column name for the variable and stat Function to calculate correlations with the target for a dataframe List of correlations Iterate through the columns Skip the target column Calculate correlation with the target Append the list as a tuple Sort by absolute magnitude of correlations Need to create new column names Iterate through the variables names Skip the grouping variable Iterate through the stat names Make a new column name for the variable and stat Rename the columns Select the categorical columns Make sure to put the identifying id on the column Groupby the group var and calculate the sum and mean Iterate through the columns in level 0 Iterate through the stats in level 1 Make a new column name Read in bureau balance Counts of each type of status for each previous loan Calculate value count statistics for each SK_ID_CURR Dataframe grouped by the loan Merge to include the SK_ID_CURR Free up memory by deleting old objects Read in new copies of all the dataframes Dataframe grouped by the loan Merge to include the SK_ID_CURR Aggregate the stats for each client Merge with the value counts of bureau Merge with the stats of bureau Merge with the monthly information grouped by client Function to calculate missing values by column Funct Total missing values Percentage of missing values Make a table with the results Rename the columns Sort the table by percentage of missing descending Print some summary information Return the dataframe with missing information Read in the test dataframe Merge with the value counts of bureau Merge with the stats of bureau Merge with the value counts of bureau balance Align the dataframes this will remove the TARGET column Drop the missing columns Calculate all correlations in dataframe Ten most positive correlations Ten most negative correlations Set the threshold Empty dictionary to hold correlated variables For each column record the variables that are above the threshold Track columns to remove and columns already examined Iterate through columns and correlated columns Keep track of columns already examined Only want to remove one in a pair Extract the ids Extract the labels for training Remove the ids and target One Hot Encoding Align the dataframes by the columns No categorical indices to record Integer label encoding Create a label encoder List for storing categorical indices Iterate through each column Map the categorical features to integers Record the categorical indices Catch error if label encoding scheme is not valid Extract feature names Convert to np arrays Create the kfold object Empty array for feature importances Empty array for test predictions Empty array for out of fold validation predictions Lists for recording validation and training scores Iterate through each fold Training data for the fold Validation data for the fold Create the model Train the model Record the best iteration Record the feature importances Make predictions Record the out of fold predictions Record the best score Clean up memory Make the submission dataframe Make the feature importance dataframe Overall validation score Add the overall scores to the metrics Needed for creating dataframe of validation scores Dataframe of validation scores Sort features according to importance Normalize the feature importances to add up to one Make a horizontal bar chart of feature importances Need to reverse the index to plot most important on top Set the yticks and labels Plot labeling. The following code makes new names by appending the stat to the name. Here to reduce the number of features we will remove any columns in either the training or the testing data that have greater than 90 missing values. The final result will be a dataframe with one row for each client with stats calculated for their loans. We can either call the function directly grouped_df. Before we remove the missing values we will find the missing value percentages in the testing data. We ended up removing no columns in this round because there are no columns with more than 90 missing values. We need to create new names for each of these columns. We see an extremely weak positive relationship between the average of this variable and the target meaning that clients who applied for loans further in the past potentially are more likely to repay loans at Home Credit. Each previous loan has its own row. This can serve as an indicator of whether a variable will be relevant to a machine learning model. Then for each client add up the values for each loan. The r value is not best measure of the usefulness of a new variable but it can give a first approximation of whether a variable will be helpful to a machine learning model. We can see in any of the variables we created have a greater correlation than those already present in the training data from application. Aggregating Numeric ColumnsTo account for the numeric information in the bureau dataframe we can compute statistics for all the numeric columns. The MONTHS_BALANCE column has the months of balance relative to application date. Now we need to aggregate these for each _client_. Applying Operations to another dataframeWe will now turn to the bureau balance dataframe. The correlation here is so weak that I do not think we should draw any conclusions Collinear VariablesWe can calculate not only the correlations of the variables with the target but also the correlation of each variable with every other variable. 75 2 1 0 0 1 1. None of the new variables have a significant correlation with the TARGET. Basically our approach is to make as many features as possible and then give them all to the model to use Later we can perform feature reduction using the feature importances from the model or other techniques such as PCA. SK_ID_CURR credit count cash count home count total count credit count norm cash count norm home count norm 1 1 0 3 4 0. Even though automated feature engineering tools are starting to be made available feature engineering will still have to be done using plenty of data wrangling for a little while longer. This will allow us to compute aggregate stats for numeric columns across any dataframe. Using functions allows for consistent results and decreases the amount of work we have to do in the future Correlation FunctionBefore we move on we can also make the code to calculate correlations with the target into a function. agg mean max min sum merge match the aggregated statistics to the appropriate client. The objective is to remove useless redundant variables while preserving those that are useful. 66 Hopefully encoding the categorical variables this way will allow us to capture the information they contain. The correlation coefficient is extremely weak and there is almost no noticeable difference in the distributions. Calculated numeric stats for the resulting dataframe grouping by the client idThe final resulting dataframe has one row for each client with statistics calculated for all of their loans with monthly balance information. Instead we will rely on calculating value counts of each category within each categorical variable. We will take the mean min and max of every numeric column in the bureau dataframe. 25 0 0. To assess the usefulness of variables we will look at the feature importances returned by the model. We can visualize the feature importance with another function plot_feature_importances. 00 0 0 3 1 2 0 3 0. We ll then remove any columns with greater than 90 missing values in either the training or testing data. The correlations between variables can show if we have collinear varibles that is variables that are highly correlated with one another. ", "id": "willkoehrsen/introduction-to-manual-feature-engineering", "size": "28153", "language": "python", "html_url": "https://www.kaggle.com/code/willkoehrsen/introduction-to-manual-feature-engineering", "git_url": "https://www.kaggle.com/code/willkoehrsen/introduction-to-manual-feature-engineering", "script": "sklearn.metrics missing_values_table sklearn.model_selection agg_numeric kde_target KFold LabelEncoder seaborn numpy matplotlib.pyplot lightgbm roc_auc_score sklearn.preprocessing count_categorical pandas model plot_feature_importances target_corrs ", "entities": "(('also code', 'function'), 'allow') (('calculations', '_ loan _'), 'have') (('Now we', '_ client _'), 'need') (('some', 'simply random noise'), 'mean') (('final result', 'loans'), 'be') (('several', 'newly created variables'), 'appear') (('better validation', 'testing data'), 'have') (('process', 'practice manipulation great data'), 'involve') (('corresponding statisticWe', 'dictionary'), 'need') (('we', 'gradually better performance'), 'achieve') (('None', 'TARGET'), 'have') (('variable', 'this'), 's') (('they', 'information'), '66') (('following code', 'pair'), 'create') (('We', 'function plot_feature_importances'), 'visualize') (('Later we', 'Random feature selection'), 'look') (('s', 'testing data'), 'let') (('we', 'training data'), 'save') (('very easy We', 'columns'), 'make') (('we', 'model'), 'say') (('These', 'original data'), 'record') (('I', 'outcomes'), 'encourage') (('we', 'variables'), 'want') (('0 which', 'one hot encoding'), 'iterate') (('that', 'those'), 'be') (('that', '0'), 'let') (('we', 'model'), 'assess') (('We', 'missing values'), 'see') (('testing that', 'greater than 90 missing values'), 'remove') (('I', 'notebook https highly www'), 'engineeringif') (('we', 'variables'), 'demonstrate') (('we', 'testing data'), 'remove') (('com willkoehrsen', 'Random Forest'), 'start') (('some', 'data'), 'make') (('kernel density estimate', 'TARGET'), 'show') (('4 home 4 home we', 'client'), 'use') (('Then we', 'client'), 'group') (('definitions', 'Home Credit'), 'be') (('We', 'agg call'), 'write') (('that', 'columns'), 'let') (('0 level', 'level'), 'become') (('we', 'Python sorted function'), 'sort') (('we', 'model'), 'use') (('We', 'columns'), 'need') (('aggregation we', 'appropriate data'), 'calculate') (('Later we', 'other such PCA'), 'be') (('that', 'Gradient Boosting Machine model'), 'use') (('We', 'training data'), 'merge') (('We', 'more than 90 missing values'), 'end') (('that', 'highly one'), 'show') (('we', 'below exact hyperparameters'), 'record') (('that', 'Python important operation'), 'make') (('we', 'confidence'), 'make') (('we', 'function'), 'be') (('we', 'most important'), 'examine') (('who', 'generally more loan'), 'let') (('we', 'other datasets'), 'use') (('this', 'test necessarily set'), 'overfit') (('Feature selection', 'testing better set'), 'be') (('a few loan', 'MONTHS_BALANCE'), 'be') (('we', 'client'), 'iterate') (('weak it', 'just as signal'), 'with') (('I', 'highly correlated variables'), 'model') (('Missing', 'missing dataframe'), 'be') (('best course', 'problem'), 'depend') (('variable', 'client'), 'represent') (('I', 'training possible final dataframe'), 'limit') (('we', 'variable'), 'calculate') (('we', 'metrics validation'), 'return') (('first step', 'control'), 'establish') (('curse', 'too high dimension'), 'be') (('dataframes', 'training data'), 'have') (('number', 'variables'), 'increase') (('we', 'many dataframes'), 'wrap') (('when it', 'feature selection'), 'be') (('feature available engineering', 'little while'), 'start') (('I', 'others'), 'encourage') (('we', 'notebook'), 'let') (('who', 'Home Credit'), 'see') (('some', 'comparisons https multiple towardsdatascience'), 'have') (('s', 'bureau dataframe'), 'let') (('We', 'variable'), 'put') (('Kernel Density PlotsThe kernel density estimate plot', 'smoothed histogram'), 'Estimate') (('mean', 'normalized count'), 'represent') (('We', 'other dataframes'), 'be') (('that', 'values'), 'use') (('even improvement', 'leaderboard'), 'be') (('which', 'training data'), 'be') (('Then client', 'loans'), 'calculate') (('training slightly score', 'validation score'), 'overfit') (('We', 'Kernel Density Estimate KDE plot'), 'inspect') (('s', 'officially performances'), 'let') (('we', 'aggregation just same statistics'), 'calculate') (('we', 'leaderboard'), 'achieve') (('We', 'function'), 'call') (('com willkoehrsen', 'here gentle introduction'), 'start') (('why we', 'domain often expertise'), 'be') (('variable', 'machine learning model'), 'serve') (('loan', 'loan current application'), 'indicate') (('bar up to one horizontal chart', 'Plot labeling'), 'pandas') (('Applying', 'bureau balance now dataframe'), 'turn') (('where operation', 'numeric columns'), 'calculate') (('which', 'us'), 'need') (('Collinear VariablesWe', 'other variable'), 'be') (('which', 'only numeric variables'), 'be') (('MONTHS_BALANCE column', 'application relative date'), 'have') (('Instead we', 'categorical variable'), 'rely') (('We', 'training data'), 'remove') (('collinear highly that', 'perhaps data'), 'allow') (('First we', 'loan'), 'calculate') (('_ _', 'application files'), 'control') (('Then it', 'dataframe'), 'calculate') (('larger r value', 'target'), 'be') (('which', 'i previous loan'), 'of') (('that previous loan', 'Home Credit'), 'be') (('Correlations', 'target'), 'calculate') (('we', 'csv raw files'), 'have') (('we', 'other institutions'), 'make') (('they', 'same columns'), 'be') (('We', 'other variables'), 'look') (('i d', 'training back data'), 'groupby') (('distribution', 'all place'), 'be') (('model', 'testing also better data'), 'be') (('First we', 'columns dtype only categorical object'), 'encode') (('following code', 'name'), 'make') (('We', 'stat'), 'rename') (('Calculated', 'balance monthly information'), 'have') (('we', 'one hot encoded _ original features'), 'compare') (('Then client', 'loan'), 'add') (('SK_ID_CURR column agg', 'columns'), 'group') (('I', 'next notebook'), 'be') (('measures', 'perfectly negatively 1'), 'linear') (('more we', 'us'), 'make') (('we', 'numeric columns'), 'compute') (('We', 'dimensionality'), 'have') (('function', 'max min mean sum'), 'stick') (('We', 'bureau dataframe'), 'take') (('Therefore we', 'relative target'), 'look') (('mean column', 'normalized count'), 'record') (('they', 'exact same columns'), 'calculate') (('collinear highly variables', 'Same'), 'be') (('when we', 'other dataframes'), 'use') (('we', 'variables https most important www'), 'test') (('dataframe', 'level multi index'), 'have') (('value', 'loans4'), 'merge') (('it', 'dataframe'), 'take') (('us', 'dataframe'), 'allow') (('normalized counts', '1'), 'count') (('s', 'target'), 'let') (('so I', 'level single index'), 'find') (('client_bureau_balance_STATUS_X_count_norm_sum', 'loan'), 'calculate') (('Now we', 'one numeric column'), 'have') (('we', '1'), 'variable') (('753 scores', 'notebook'), 'note') (('control', 'when competition'), 'run') (('which', 'modeling'), 'use') (('variables', 'collinear variables'), 'want') (('Again we', 'columns'), 'have') (('we', 'new variables'), 'be') (('merge', 'appropriate client'), 'match') (('that', 'one'), 'be') (('Then we', 'feature engineering'), 'build') (('we', 'Home Credit Default Risk competition'), 'explore') (('we', 'again time SK_ID_CURR'), 'do') (('we', 'other financial institutions'), 'illustrate') (('we', 'time'), 'be') (('we', 'column'), 'require') (('We', 'training'), 'remove') (('dataframe', 'other financial institutions'), 'have') (('we', 'when calculated features'), 'ResultsAfter') (('This', 'loan'), 'give') (('we', 'application'), 'see') (('we', 'together one dataframe'), 'hold') (('which', 'training dataframes'), 'mean') (('correlation coefficient', 'extremely almost no noticeable distributions'), 'be') (('why we', 'us'), 'seem') (('test', 'bureau_balance highly correlated variables'), 'test') (('engineered features', 'control better case'), 'perform') (('We', 'magnitude absolute correlation'), 'look') (('value', 'them'), 'be') (('we', 'other dataframes'), 'have') (('Here we', 'bureau'), 'look') (('things', 'functions'), 'be') (('results', 'slightly raw features'), 'be') (('we', 'training simply data'), 'merge') (('we', 'ground'), 'let') (('s', 'hand'), 'let') (('we', 'newly created variables'), 'for') (('1 who', 'TARGET'), 'show') (('we', 'hand'), 'make') (('We', 'step'), 'let') (('we', 'feature selection'), 'decrease') (('Columns', 'too many missing values'), 'have') (('they', 'through values'), 'go') (('TogetherWe', 'training main dataframe'), 'have') (('variable', 'machine learning model'), 'be') (('Categorical we', 'categorical columns'), 'VariablesNow') (('client', 'Credit Bureau credit'), 'be') (('rather us', 'that'), 'be') (('we', 'reg_lambda'), 'address') (('we', 'differently category'), 'color') ", "extra": "['outcome', 'test']"}