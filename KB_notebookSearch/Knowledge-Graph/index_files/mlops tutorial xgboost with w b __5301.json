{"name": "mlops tutorial xgboost with w b ", "full_name": " h1 Setup and Imports h1 Load Dataset h4 Utils h1 Prepare Train Validation Split h1 Features h1 Train a naive XGBRegressor on one crypto data h1 So Far h2 Check out the W B Dashboard h1 XGBRegressor as Multi Output Regressor h1 WORK IN PROGRESS ", "stargazers_count": 0, "forks_count": 0, "description": "Let s first start by initializing a W B run and use the split artifact reference that we logged previously. The step below might take some time since a large csv file is being written on the disk. Train the regressor. This tutorial is based on Tutorial Time Series forecasting with XGBoost https www. com cstein06 tutorial to the g research crypto competition. png The image shown below is the logged metrics. com robikscube tutorial time series forecasting with xgboost by Rob Mulla https www. Open Opening price of the time interval in USD. If you have any questions feel free to comment or reach out. Upcoming Extend the regression for the entire dataset. Note the use of wandb_callback Get the booster Save the booster to disk Get the booster s config Notice the use of earlier artifact as reference for model artifact select training and test periods 86400 corresponds to one day 24 hrs in seconds. Use a quarter worth of data and forecast for one month ahead. Imagine you are training tons of models on different splits of the same dataset. Let s save the model along with model configuration. Let s prepare the features for just Bitcoin trading data. I will be using this valid_df for evaluating all my models. Let s see how things go. We will use Sklearn s MultiOutputRegressor. VWAP is an aggregated form of trade data. Plus if you think it can be improved further please let me know. Note however that this strategy doesn t use any dependence between different targets. Using wandb_callback is like using a single line of code to keep a tab of your experiments. Not sure how exactly Show how to use W B Sweeps for Hyperparameter Optimization. Volume Quantity of asset bought or sold displayed in base currency USD. This notebook can be considered a tutorial on how to use XGBoost for this competition and use Weights and Biases to make the most out of your XGBoost model. VWAP The average price of the asset over the time interval weighted by volume. We will use the data from WORK IN PROGRESSI hope you will find it useful. Check out the W B Dashboard https wandb. timestamp All timestamps are returned as second Unix timestamps the number of seconds elapsed since 1970 01 01 00 00 00. Asset_ID 1 for Bitcoin. Note This is a one time step. We will take the crypto data of Bitcoin fill the missing gaps in the series compute features for train and validation splits. First let s build our train and valid dataset for one quarter. Here we will log the raw dataset as W B Artifacts to build data lineage as we train models and validate on different split of the dataset. Later in this notebook we will try to find the best parameters. Features Fill NaN values Fill Inf values Build features Concat all the features into one dataframe Rename feature columns Fill NaN and Inf Fill NaN and Inf Initialize a W B run Get single crypto trading data Fill missing value Create features Initialize a W B run Initialize an XGBRegressor with some parameters. Utils Prepare Train Validation SplitNote that I have used the data that s used for LB score computation as valid_df. You might want to repeat this for different splits of the raw dataset. For a competition that runs for months a good data version control can make a huge difference. Target Residual log returns for the asset over a 15 minute horizon. The config below is for demonstration purposes. if you encounter a year is out of range error the timestamp may be in milliseconds try ts 1000 in that case Notice the use of raw_artifact. Again this is a one time process. Now finally let s train a simple regression and use W B s XGBoost Callback to log the metrics and configs. Get single crypto trading data Get the windowed data Fill missing value Get single crypto trading data Get the windowed data Fill missing value Create features define the direct multioutput model and fit it. Asset_ID The asset ID corresponding to one of the crytocurrencies e. This will act as reference for this split. The following are the columns available in the train. Setup and ImportsWeights and Biases comes preinstalled with Kaggle environment but it s recommended to get the latest version of the same. Here we will log the features dataframe for better sanity check in the future. Taking the extra effor to build an MLOps pipeline around the same can be really useful in the long run. Low Lowest price reached during time interval in USD. We will again save the splits as W B Artifact and use the reference to the previously logged raw data to build the data lineage. Close Closing price of the time interval in USD. Count Total number of trades in the time interval last minute. Once you have logged your raw_data you just need to log different splits of the same or preprocessed data. We will then initalize a W B run and train an XGBRegressor with default parameters. We will use the data lineage created so far and start building model lineage on top of it. So FarSo far we built a data and model lineage and used wandb_callback for XGBoost. Load DatasetIf you haven t already check out the Tutorial to the G Research Crypto Competition https www. Since we have used the training and validation split and are going to train the model on a subset of the data we should log the subset as W B artifacts. ai ayush thakur gresearch workspace user ayush thakur The image below shows the data and model lineage created so far. png XGBRegressor as Multi Output RegressorIn this section we will take in the trading data for one quarter 3 months and try to forecast the returns for the 4th month. High Highest price reached during time interval in USD. The mapping from Asset_ID to crypto asset is contained in asset_details. Features Train a naive XGBRegressor on one crypto dataIn this section we will train an XGBRegressor which is an implementation of the scikit learn API for XGBoost regression. This might be an additional step but can be really useful to have in your arsenal. Timestamps in this dataset are multiple of 60 indicating minute by minute data. ", "id": "ayuraj/mlops-tutorial-xgboost-with-w-b", "size": "5301", "language": "python", "html_url": "https://www.kaggle.com/code/ayuraj/mlops-tutorial-xgboost-with-w-b", "git_url": "https://www.kaggle.com/code/ayuraj/mlops-tutorial-xgboost-with-w-b", "script": "sklearn.metrics MultiOutputRegressor log_return utc_to_timestamp upper_shadow create_features mean_squared_error numpy plot_tree wandb_callback fill_nan_inf matplotlib.pyplot pandas lower_shadow tqdm wandb.xgboost plot_importance mean_absolute_error sklearn.multioutput timestamp_to_utc xgboost datetime ", "entities": "(('second Unix', '01 00 00 00'), 'timestamp') (('We', 'MultiOutputRegressor'), 'use') (('we', 'dataset'), 'log') (('Volume Quantity', 'base currency USD'), 'buy') (('Using', 'experiments'), 'be') (('data version good control', 'huge difference'), 'make') (('use', '24 seconds'), 'note') (('image', 'below data lineage'), 'ai') (('you', 'same data'), 'need') (('you', 'same dataset'), 'imagine') (('that', 'valid_df'), 'SplitNote') (('strategy however doesn', 'different targets'), 'note') (('You', 'raw dataset'), 'want') (('We', 'default parameters'), 'initalize') (('it', 'PROGRESSI'), 'use') (('This', 'additional really arsenal'), 'be') (('This', 'split'), 'act') (('section we', '4th month'), 'XGBRegressor') (('VWAP', 'trade aggregated data'), 'be') (('Timestamps', 'minute minute data'), 'be') (('We', 'train splits'), 'take') (('it', 'same'), 'preinstalle') (('So FarSo far we', 'XGBoost'), 'build') (('csv large file', 'disk'), 'take') (('mapping', 'asset_details'), 'contain') (('Low Lowest price', 'USD'), 'reach') (('we', 'W B artifacts'), 'use') (('we', 'that'), 'let') (('We', 'it'), 'use') (('s', 'Bitcoin trading just data'), 'let') (('Here we', 'future'), 'log') (('Fill Inf Fill Inf W B crypto trading Get single data', 'parameters'), 'value') (('following', 'available train'), 'be') (('notebook', 'XGBoost model'), 'consider') (('We', 'data lineage'), 'save') (('config', 'demonstration below purposes'), 'be') (('I', 'models'), 'use') (('tutorial', 'Tutorial Time Series XGBoost https www'), 'base') (('s', 'model configuration'), 'let') (('Target Residual', 'minute 15 horizon'), 'log') (('Create features', 'it'), 'get') (('t', 'G Research Crypto Competition https www'), 'DatasetIf') (('High Highest price', 'USD'), 'reach') (('Now finally s', 'metrics'), 'let') (('asset ID', 'e.'), 'asset_id') (('timestamp', 'raw_artifact'), 'be') (('Taking', 'really long run'), 'be') (('we', 'best parameters'), 'try') (('which', 'XGBoost regression'), 'train') (('First s', 'one quarter'), 'let') ", "extra": "['test']"}