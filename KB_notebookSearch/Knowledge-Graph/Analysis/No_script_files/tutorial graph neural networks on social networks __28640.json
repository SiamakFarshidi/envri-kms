{"name": "tutorial graph neural networks on social networks ", "full_name": " h1 Tutorial Graph Neural Networks on Social Networks Using PyTorch h2 1 Set your expectations of this tutorial h2 2 GNN Graph Convolution Neural Network h2 3 Adjacency matrix Degree matrix an example h2 4 Let us add some intelligence learning h2 5 Time to get hands dirty h3 5 1 Required packages h3 5 2 Dataset exploration h3 5 3 Features encoding h3 5 4 Graph construction and visualization h3 5 5 Construct and train our GNN model h3 7 Some resources that helped me to prepare this tutorial ", "stargazers_count": 0, "forks_count": 0, "description": "For our graph we come up with a symmetric matrix with zero diagonal as shown below. This leads to some issues but let us say for now it is not a severe problem. Mathematicians like to be concise and they represent the graph as the set G V E where G V Eare the graph vertices and edges sets respectively. 3 Features encodingThe nodes features tell us which feature is attached to each node. This is a way to represent the undirected graph if we are given the edge indecies. graph0 https dev to uploads. pyplot as pltimport torchimport torch. vertices which are connected by the second ingredient edges. nn import GCNConvimport networkx as nx 5. reshape 1 1 sparse_feat_matrix np. Here is the function. round val_accuracy 4 np. We will construct the model with 16 filters and will use nn. zeros for disconnected nodes and ones for the adjacent nodes. pythondef draw_graph data0 node_labels data0. The contrary is the undirected graph which has bidirectional edges then the relation could be friendship for example. com uploads articles 4bb0obvymmv1w1k2i8k1. png Since you are now reading this post about GNNs I can confidently assume that you already have knowledge about CNNs. This target feature was derived from the job title of each user. The tensor shape will be 2 2 number_of_original_edges. parameters lr lr 00001 best_accuracy 0. tensor node_features_list node_labels torch. pythondef masked_loss predictions labels mask mask mask. 6 g msk g print g print print training samples torch. transforms import AddTrainValTestMask as maskingfrom torch_geometric. You can conceptualize the nodes as the graph entities or objects and the edges are any kind of relation that those nodes may have. The particular problem we will be working on is a typical node classification problem. plot train_accuracies plt. com uploads articles k3r2tdjyexhogymoussd. load json_data edges pd. png Now if we take a critical look at the result we can observe two problems. pythonclass SocialGNN torch. draw data_nx pos cmap plt. The figure below represents a graph that consists of 4 nodes represented by those blue circles with indices 1 2 3 and 4 and connected by directed edges represented by those green arrows. plot val_accuracies plt. array 0 max feats 1 this_feat data_raw str i one_hot_feat this_feat 1 data_encoded str i list one_hot_feat if light True sparse_feat_matrix np. png Then the latent space feature representation can be calculated by the following note that you may see different ways to solve this problem in the literature but their common goal is to achieve some level of normalization by exploiting the graph structure eq4 https dev to uploads. Also you should know that both nodes and edges can have features a. csv print 5 top nodes labels print target_df. This statistics tell us that our data set has those many nodes and those many edges. com uploads articles atav9s2229mama09f4xa. T edge_index02 torch. Most of us if not all already know that images consist of pixels and the CNN leverages the convolution operation to calculate latent hidden features for different pixels based on their surrounding pixel values. y mask data. By plotting the histogram frequency distribution we see some imbalance as the machine learning label 1 are fewer than the other classes. mean mask loss criterion predictions labels loss loss mask loss torch. 4f Train_Accuracy. It does this by sliding a kernel a. datset https dev to uploads. T edge_index02 0 edge_index01 1 edge_index02 1 edge_index01 0 edge_index0 torch. html there are different options if you want to install using Pip Wheels please check your syntax to download the compatible version based on your machine and software. In our training we use 30 as a validation set and 60 as test set while we keep only 10 for the training. All this information can give us a feeling that our dataset is a right candidate for this tutorial in term of simplicity. I am pasting here a dataset statistics table from the same source so we can have a preliminary idea about what we are dealing with. test_mask test_accuracies test_accuracy if np. functional as Ffrom torch_geometric. I am aiming at the end of this step by step tutorial that you will be able to 1. Graph classification where we can answer questions like can we predict chemical compounds activity against lung cancer Remember that chemical compounds can be represented as graphs with compound atoms as its nodes and chemical bonds are its edges. argmax predictions axis 1 labels. GCNConv class however there are many other layers you can try on PyTorch Geometric documentation https pytorch geometric. We will run the training for some number of epochs and we keep track for the best validation accuracy. hist feat_counts bins 20 plt. pythonwith open data musae_git_features. GNN Graph Convolution Neural Network A __BIG__ __caveat__ here is to emphasize that I do not mean GNNs are just CNNs that operate on graphs but what I want to say is that I felt comfortable with GNNs when I linked it to my understanding of CNNs and learnt something about graphs. It also tells us that the edges has no features and no directions. conv1 x x edge_index edge_index x F. 1 Required packagesTo begin with we must download and import the required packages. pythong_sample construct_graph data_encoded data_encoded_vis light True draw_graph g_sample graphlish https dev to uploads. 02907 if you would like to have a look at how it was developed. Working with data a golden rule is to learn as we go about the dataset. Then we can pass it to draw_graph to show the following graph. html which was collected from the public API. I hope this works for you as well note that I put the sign to avoid causing some people to cringe. Particularly we can leverage our last equation to be multiplied by a learnable weight matrix W and apply a nonlinear function to the product as in the next equation. 5 Construct and train our GNN modelWe start by encoding the whole data by calling encode_data with light False and construct the full graph by calling construct_graph with light False. long accuracy mask accuracy accuracy torch. a linear layer of the product and \u03a6 is a non linear activation function. Module def __init__ self num_of_feat f super SocialGNN self. Our graph degree matrix looks like this degree https dev to uploads. We also plot the losses and accuracies across epochs for the training. 4f Test_Accuracy. json as json_data data_raw json. ml_target bins 4 plt. json which contains the nodes features. com uploads articles o74ml5tspubldlcttkkb. y will be assigned to the node labels its shape is number_of_nodes edge_index to represent an undirected graph we need to extend the original edge indices in a way that we can have two separate directed edges connecting the same two nodes but pointing opposite to each other. com uploads articles ntbtrlzhle5fwetir5rz. The first is that when calculating the entry of the features matrix H for any of the nodes we exclude that particular node features from the calculation as shown by red zeros. com uploads articles 7yxbpebsz6oqjf7fv3lo. Let us add some intelligence learning So far we discussed how we can calculate latent features of a graph data structure. user and ml_target which is 1 if the user is a machine learning community user and 0 otherwise. AddTrainValTestMask class can take our graph and let us set how we want our masks to formed and it will add a node level split via train_mask val_mask and test_mask attributes. png After downloading the dataset we can see 3 important files musae_git_edges. Construct graphs and visualize them using code. The degree matrix is a diagonal matrix that contains information about the number of edges attached to each vertex. You might have your own way to represent the image as a graph what matters is that you just have the image to graph concept in your mind. Features are normally vectors but we represent them by scalers here for simplicity. This dot product leads to aggregate the neighboring pixel values to one representative scaler. concatenate sparse_feat_matrix temp axis 0 sparse_feat_matrix sparse_feat_matrix 1 return data_encoded sparse_feat_matrix elif light False return data_encoded None We can plot the first 250 features columns the total is 4005 of the code for the first 60 users. For instance during training we want to calculate the training loss and accuracy only based on the training set and here where we should use our masks. So if we can learn node features in the latent space we can do many interesting things. values edges_list edges. The third histogram shows the most common features accross users we can see different peaks on the distribution. Those features can represent any attributes that describe a node blue boxes or an edge green boxes. We will define functions masked_loss and masked_accuracy for which we pass the respective masks and it returns the corresponding loss and accuracy. io en latest modules nn. read_csv data musae_git_target. Data which is a plain old python object to model a single graph with various optional attributes. backward optimizer. We will construct our graph object using this class and passing the following attributes noting that all arguments are torch tensors. We will not try to visualize this big graph as I am assuming that you are using your local computer with limited resources. title Classes distribution plt. Those things are under three main categories 1. Stacking those functions together promote this equation to be a deep learning model. to_markdown headandtail https dev to uploads. item print validation samples torch. float mask mask torch. connected by edges or not in the graph. mean loss return loss def masked_accuracy predictions labels mask mask mask. plot test_losses plt. The elements of A indicate whether pairs of nodes are adjacent i. I do not know if now you got the same intuition of mine which is true that we can stack as many functions of those as we like and learn the weight matrices W using backpropagation. Of course as this field is evolving you will find many interesting original GNN models and many bells and whistles for better suiting your problem. com watch v 8owQBFAHw7E by Petar Veli\u010dkovi\u0107 on YouTube. We may also see how nodes are different based on the number of features. In order to tell the training phase which nodes that should be included during training and tell the inference phase which are the test data we can use masks which are binary vectors that indicate using 0 and 1 which nodes belong to each particular mask. Time to get hands dirty Even though the above functions look natural and manageable to implement using many machine learning frameworks torch_geometric PyTorch Geometric https pytorch geometric. spring_layout data_nx scale 1 plt. val_mask val_accuracies val_accuracy test_accuracy masked_accuracy predictions out labels data. The node features are extracted based on the location repositories starred employer and e mail address. Now we are sure that our task is a binary classification problem as we only have 2 classes. float edge_index data. 4f. We also need json pandas and numpy and some other packages. png hdash1 https dev to uploads. 1 epochs https dev to uploads. 01 optimizer torch. com uploads articles w4admdyj8cvgr3191b5w. 4 Graph construction and visualizationTo construct our graph we will use torch_geometric. Even though we see 4 columns but only 2 columns concern us here id of the node i. pythonmsk masking split train_rest num_splits 1 num_val 0. You can take a few seconds to relate the numbers on the calculation to the graph. mean mask accuracy torch. com uploads articles jtbjvngengfh8s02mcq1. com uploads articles zoqh07c6rpbw0xg3hdsz. This graph is directed because the edges may represent asymmetric relationship for instance if the nodes represent social network users and node 1 follows node 2 it does not mean that node 2 follows node 1. As we agreed we will use torch_geometric. nodes pos nx. pythondef encode_data light False n 60 if light True nodes_included n elif light False nodes_included len data_raw data_encoded for i in range nodes_included one_hot_feat np. The first is called the adjacency matrix and mostly denoted by A which is a square matrix used to represent finite graphs like this one. html convolutional layers. Now we will build construct_graph function which does the following pythondef construct_graph data_encoded light False node_features_list list data_encoded. png We can express the above graph using two matrices which can capture its structure. graph1 https dev to uploads. We have a large social network of GitHub developers named musae github https snap. Node classification our tutorial problem falls under this category as we are given a graph of a social network where users represent its nodes and the relationships between them are its edges and our task is to predict each user group i. hist feats bins 50 plt. show hists https dev to uploads. We will stack two GCNConv layers the first has input features equal to the number of features in our graph and some arbitrary number of output features f. You may have different split ratios but in this way we may have a more realistic performance and we will not overfit easily I know you might disagree with me on this point We can also print the graph information and the number of nodes each set mask. only 60 nodes for the purpose of visualization. figure figsize 25 25 plt. edge_index x self. com uploads articles 3ys8qq5snrztj0hrw1zw. train_mask loss. array data_encoded str j. And here is where we can start doing interesting things. item print test samples torch. If you get this then it is easier for you to think of GNNs as a generic version of CNNs that can operate on graphs. And the graph is a data structure that has two main ingredients nodes a. The benefit of using GNNs is the provision of a generalized form that enables us to exploit non Euclidean space data. Then we apply a relu activation function and deliver the latent features to the second layer which has output nodes equal to the number our classes i. This product if we closely look at it is generating a new feature vector that captures the feature of the surrounding nodes for each node based on how those surrounding nodes are connected to it. We can learn the optimal W based on the task at hand. conv2 x edge_index return x As our model will predict the class of all nodes in the graph we however want to calculate the loss and accuracy for specific set based on the phase we are in. item split https dev to uploads. Thank you for following this tutorial I hope it was of help for you. Graph neural networks as their name tells are neural networks that work on graphs. You can see how the nodes are connected by edges and labeled by color. HandA https dev to uploads. 2 Dataset explorationI assume the dataset files are in a subfolder named data. plot val_losses plt. Below we can see how sparse is the matrix that we construct for nodes features. We will download and explore a social network dataset collected from GitHub. imshow sparse_feat_matrix_vis 250 cmap Greys sparse https dev to uploads. Intro to graph neural networks presentation https www. pythondata_encoded _ encode_data light False g construct_graph data_encoded data_encoded light False The usual immediate step after data preparation in the machine learning pipelines is to perform data segregation and split subsets of the dataset to train the model and further validate how it performs against new data and save a test segment to report the overall performance. But if we want to accomplish a particular task we can guide this calculation toward our goal. Also the data has no temporal property. csv which contains the targets i. We need to convert our homogeneous graph to a NetworkX graph then pot using NetworkX. png Another important property we need to look at the class balance this is because severe class imbalances may cause the classifier to simply guessing the majority class and not making any evaluation on the underrepresented class. You can think of the pixels of the image as the nodes of the graph and the edges are present only between the adjacent pixels. png At this point I believe you agree with me that we are ready to construct our GNN model class. data import Datafrom torch_geometric. From torch_geometric we need to import AddTrainValTestMask this will help us to segregate the training and test sets later. html a geometric deep learning extension library for PyTorch provides many variations of deep learning on graphs and other irregular structures. PyTorch Geometric Documentation https pytorch geometric. pythonnum_of_feat g. plot test_accuracies plt. Know how graph datasets which are expected by GNNs look like. __init__ self. For sure there are many other variations of GNNs but let us stick to this for those 10 minutes of reading. zeros 1 max feats 1 for j in range nodes_included temp np. conv2 GCNConv f 2 def forward self data x data. To download torch_geometric please follow strictly the installation guide here https pytorch geometric. It deserve mentioning here that this Data class is very abstract in a sense that you can add any attribute that you think it describes your graph. For instance we can add a metadata attribute g meta_data bla bla bla which make it flexible to encapsulate any information you would like. 4f Val_Accuracy. format ep 1 epochs loss. In the forward function GCNConv can accept many arguments x as the nodes features edge_index and edge_weight in our case we only use the first two arguments. val_mask val_losses val_loss val_accuracy masked_accuracy predictions out labels data. item train_accuracy val_accuracy test_accuracy best_accuracy val_accuracy plt. return else data_nx to_networkx data0 node_colors data0. plot train_losses plt. Those elements can be weighted e. Tutorial Graph Neural Networks on Social Networks Using PyTorch 1. CrossEntropyLoss train_social net g epochs 50 lr 0. python matplotlib inlineimport jsonimport collectionsimport numpy as npimport pandas as pdimport matplotlibimport matplotlib. gnneq1 https dev to uploads. Set your expectations of this tutorialYou can follow this tutorial if you would like to know about Graph Neural Networks GNNs through a practical example using PyTorch framework. Edge classification as an example we can predict the congestion in a particular street if we are given the city map. eq1 https dev to uploads. zero_grad out net data loss masked_loss predictions out labels data. attributes note the can as having features is not a must. Below we can see that our simple model reached a very decent accuracy on the test set more than 87. Then our graph has the streets as its edges and their intersections as its nodes. The second histogram tells that most of the users have between 15 and about 23 features and very few of them have more than 30 and less than 5 features. read_csv data musae_git_edges. Now let us twist our conceptualization of images a little bit and think of images as a graphs. For example we need to have 2 edges between node 100 and node 200 one edge points from 100 to 200 and the other points from 200 to 100. png The second problem is the risk that may result from aggregating the features as we might end up with extremely high values which can lead to computation instability. to_markdown print print 5 last nodes print target_df. io en latest notes installation. Our task is to predict whether the GitHub user is a web or a machine learning developer. Construct and train a simple GNN model for node classification task based on convolutional GNN using torch_geometric the geometric deep learning extension library for PyTorch. edu data github social. The idea is to calculate the loss and accuracy for all nodes and multiply it by the mask to zero out the not required nodes. Yes convolutional neural networks the deep neural network class that is most commonly applied to image analysis and computer vision applications. We can see already signs of that in our simple example on the latent feature matrix H which has entries at least 10 times more than the entries in the original H. csv which contains the edges indices. title Number of features per graph distribution plt. The other matrix is the features matrix H a. com uploads articles 1kmabqemuvw6asoou45d. x will be assigned to the encoded node features its shape is number_of_nodes number_of_features. filter over the input image and calculating the dot product between that small filter and the overlapping image area. tolist edge_index01 torch. In contrast to the pixels representation of images the graph can be visualized as a collection of nodes and edges without having any order. com uploads articles 3p9fzpzt7hd0xxm21p19. Semi supervised Classification with Graph Convolutional Networks Paper https arxiv. get_cmap Set1 node_color node_colors node_size 600 connectionstyle angle3 width 1 with_labels False edge_color k arrowstyle Now we can draw a sub graph by calling construct_graph with light True. Let us have an example of undirected graphs which has some numerical features attached to its nodes as well as its edges. png hdash2 https dev to uploads. org documentation stable install. com uploads articles rha625rk7xhh16jmu1cv. Our plan is to use this function to encode a light subset of the graph e. The splitting is not straight forward in our case as we have a single giant graph that should be taken all at once there are some other ways to deal with graph segments but let us assume this is the case for now. The numbers between the brackets is the shape of each attribute tensor. train_mask train_accuracies train_accuracy val_loss masked_loss predictions out labels data. num_node_featuresnet SocialGNN num_of_feat num_of_feat f 16 criterion nn. 0 train_losses train_accuracies val_losses val_accuracies test_losses test_accuracies for ep in range epochs 1 optimizer. We will also need NetworkX Python package we will use it to visualize the graph I installed with pip https networkx. But for our tutorial let us make some visually appealing illustrations. We can one hot encode those features by writing our function encode_data. com uploads articles 4auwrxa91qjlb4f37e0o. step train_losses loss train_accuracy masked_accuracy predictions out labels data. pythondef train_social net data epochs 10 lr 0. show At this point we constructed all the required functions and ready to instantiate our model and train it. 02907 by Kipf Thomas N. png Implementation wise W can be a linear transformation e. com uploads articles 1rp7gummqde0jbxnseal. Nodes represent developers platform users who have starred at least 10 repositories and edges are mutual follower relationships between them note that word mutual indicates undirected relationship. tensor target_df ml_target. num_nodes 100 print This is a big graph can not plot. by edge features as in our case or can be unweighted i. CrossEntropyLoss as our loss criterion. png An interesting operation that we may like to do is to calculate the dot product between those two matrices. graph signal X which holds the nodes features of the graph. conv1 GCNConv num_of_feat f self. Some resources that helped me to prepare this tutorial 1. Adjacency matrix Degree matrix an example Let us take a simple type of graphs the undirected graph this graph type is simple because the relationship between nodes is symmetric. So now we can define the modified adjacency matrix \u00c3 as follows abar https dev to uploads. The solution of this problem is to normalize the latent features by dividing by the graph degree matrix D specifically we multiply by its inverse. But for the sake of this tutorial we will keep it simple and use only one flavor of GNNs which is the graph convolutional operator class GCNConv which is implemented from the paper Semi supervised Classification with Graph Convolutional Networks https arxiv. convert import to_networkxfrom torch_geometric. Of course you are so welcome to post comments if you have any question or you feel that I deviated from what I promised you here 2. com uploads articles 8eiarf89ehm31tj71m7k. tensor edges_list dtype torch. figure figsize 12 8 nx. pythondata_encoded_vis sparse_feat_matrix_vis encode_data light True n 60 plt. round best_accuracy 4 print Epoch Train_Loss. float mask torch. com uploads articles ebkho4uzy0a30jzahmbp. cat edge_index01 edge_index02 axis 1 g Data x node_features y node_labels edge_index edge_index0 g_light Data x node_features 0 2 y node_labels edge_index edge_index0 55 if light return g_light else return g For drawing a graph we construct our draw_graph function. We read them and plot the top 5 rows and the last 5 rows from the labels file. title Features distribution plt. Gain insights about what graph neural networks GNNs are and what type of problems they may solve. The dataset referenced paper is Multi scale Attributed Node Embedding https arxiv. values node_features torch. mean accuracy return accuracy Now we can define our training function in which we will use torch. relu x x self. Which seems to be absurd in the sense that we are ignoring our central node To solve this we can add self connections to every node on the graph and this can be achieved by adding the identity matrix I to the graph adjacency matrix which will alter its zero valued diagonal to ones. ", "id": "awadelrahman/tutorial-graph-neural-networks-on-social-networks", "size": "28640", "language": "python", "html_url": "https://www.kaggle.com/code/awadelrahman/tutorial-graph-neural-networks-on-social-networks", "git_url": "https://www.kaggle.com/code/awadelrahman/tutorial-graph-neural-networks-on-social-networks", "script": "", "entities": "(('data set', 'many nodes'), 'tell') (('nodes', 'features'), 'see') (('data that', 'a.'), 'be') (('task', 'user group i.'), 'classification') (('most', 'more than 30 less than 5 features'), 'tell') (('We', 'then NetworkX.'), 'need') (('we', 'opposite other'), 'assign') (('nodes', 'graph'), 'signal') (('which', 'one'), 'call') (('I', 'you'), 'be') (('train_mask train_accuracies val_loss masked_loss', 'labels data'), 'train_accuracy') (('GCNConv however many other you', 'PyTorch Geometric documentation https pytorch geometric'), 'class') (('feature', 'node'), 'feature') (('5 last nodes', 'target_df'), 'to_markdown') (('how we', 'graph data structure'), 'discuss') (('we', 'draw_graph function'), 'edge_index01') (('we', 'zero diagonal'), 'come') (('We', 'json also pandas'), 'need') (('node features', 'starred employer'), 'extract') (('We', 'nodes'), 'have') (('which', 'ingredient second edges'), 'vertex') (('CNN', 'pixel surrounding values'), 'know') (('Even we', 'columns node only 2 here i.'), 'see') (('it', 'graph'), 'deserve') (('to_markdown', 'uploads'), 'headandtail') (('which', 'ones'), 'add') (('We', 'GitHub'), 'download') (('learning extension geometric deep library', 'graphs'), 'provide') (('word mutual', 'undirected relationship'), 'note') (('I', 'pip https networkx'), 'need') (('graph degree matrix', 'uploads'), 'look') (('it', 'train_mask val_mask'), 'take') (('we', '100'), 'need') (('people', 'sign'), 'hope') (('csv 5 top nodes', 'print'), 'print') (('We', 'training'), 'plot') (('We', 'nn'), 'construct') (('Semi', 'Graph Convolutional Networks Paper https arxiv'), 'supervise') (('specifically we', 'inverse'), 'be') (('you', 'mind'), 'have') (('we', 'GNN model class'), 'png') (('input', 'output features arbitrary f.'), 'have') (('item val_accuracy', 'best_accuracy val_accuracy plt'), 'train_accuracy') (('edges', 'features'), 'tell') (('So now we', 'uploads'), 'define') (('how it', 'look'), '02907') (('they', 'problems'), 'solve') (('Then graph', 'nodes'), 'have') (('that', 'graphs'), 'be') (('you', 'information'), 'add') (('you', '1'), 'aim') (('that', 'tutorial'), 'resource') (('which', 'various optional attributes'), 'datum') (('we', 'many interesting things'), 'do') (('which', 'original H.'), 'see') (('how surrounding nodes', 'it'), 'generate') (('We', 'GitHub developers'), 'have') (('total', 'first 60 users'), 'axi') (('we', 'torch'), 'mean') (('node', '2 node'), 'direct') (('together equation', 'functions'), 'promote') (('k Now we', 'light True'), 'node_size') (('simple model', 'more than 87'), 'see') (('val_mask masked_accuracy', 'labels data'), 'val_accuracies') (('Now us', 'graphs'), 'let') (('We', 'hand'), 'learn') (('where we', 'masks'), 'want') (('nodes_included len False data_raw', 'range nodes_included one_hot_feat np'), 'light') (('this', 'training sets'), 'need') (('nodes bonds', 'compound atoms'), 'classification') (('we', 'dataset'), 'be') (('having', 'features'), 'note') (('some', 'visually appealing illustrations'), 'let') (('that', 'green arrows'), 'represent') (('construct_graph', 'data_encoded light False node_features_list list'), 'build') (('also how nodes', 'features'), 'see') (('we', 'training'), 'use') (('weight matrices', 'backpropagation'), 'know') (('classifier', 'underrepresented class'), 'png') (('dataset', 'simplicity'), 'give') (('python matplotlib inlineimport jsonimport collectionsimport', 'matplotlibimport pdimport matplotlib'), 'numpy') (('relationship', 'nodes'), 'matrix') (('how nodes', 'color'), 'see') (('we', 'two matrices'), 'png') (('which', 'public API'), 'html') (('we', 'edge indecies'), 'be') (('we', 'city map'), 'predict') (('step train_losses train_accuracy masked_accuracy', 'labels data'), 'loss') (('we', 'phase'), 'conv2') (('\u03a6', 'linear product'), 'layer') (('We', 'labels last 5 file'), 'read') (('this', 'graph all once other segments'), 'be') (('when I', 'graphs'), 'Network') (('which', 'nodes'), 'let') (('mask loss criterion predictions', 'loss loss mask loss torch'), 'mean') (('diagonal that', 'vertex'), 'be') (('classification binary we', 'only 2 classes'), 'be') (('i', 'True sparse_feat_matrix np'), 'feat') (('dataset files', 'subfolder'), 'assume') (('G V graph where vertices', 'G V set E'), 'like') (('we', 'validation best accuracy'), 'run') (('graph', 'order'), 'visualize') (('We', 'function'), 'can') (('plan', 'graph e.'), 'be') (('pairs', 'nodes'), 'indicate') (('nodes', 'particular mask'), 'in') (('it', 'corresponding loss'), 'define') (('we', 'two problems'), 'png') (('html different you', 'machine'), 'be') (('neural that', 'graphs'), 'be') (('we', 'what'), 'paste') (('which', 'computation instability'), 'be') (('we', 'it'), 'show') (('that', 'node blue boxes'), 'represent') (('target feature', 'user'), 'derive') (('things', 'three main categories'), 'be') (('it', 'issues'), 'lead') (('further how it', 'overall performance'), 'pythondata_encoded') (('us', 'reading'), 'be') (('train_losses val_losses val_accuracies', 'range epochs 1 optimizer'), '0') (('Then we', 'following graph'), 'pass') (('you', 'limited resources'), 'try') (('5 Construct', 'light False'), 'start') (('we', 'torch_geometric'), 'construct') (('You', 'graph'), 'take') (('that', 'space non Euclidean data'), 'be') (('idea', 'required nodes'), 'be') (('GNN many interesting original models', 'many better problem'), 'evolve') (('we', 'goal'), 'want') (('we', 'required packages'), 'begin') (('you', 'PyTorch framework'), 'follow') (('numbers', 'attribute tensor'), 'be') (('we', 'distribution'), 'show') (('we', 'torch_geometric'), 'use') (('it', 'you'), 'thank') (('machine learning label', '1 other classes'), 'see') (('which', 'structure'), 'express') (('last equation', 'next equation'), 'leverage') (('arguments', 'following attributes'), 'construct') (('here where we', 'interesting things'), 'be') (('we', 'files 3 important musae_git_edges'), 'png') (('Even above functions', 'machine learning many frameworks'), 'time') (('Semi', 'Graph Convolutional Networks https arxiv'), 'keep') (('pythondef masked_loss predictions', 'mask mask mask'), 'label') (('normally we', 'here simplicity'), 'be') (('nodes', 'only adjacent pixels'), 'think') (('nodes', 'a.'), 'know') (('confidently you', 'CNNs'), 'assume') (('then relation', 'example'), 'be') (('which', 'equal number'), 'apply') (('dot product', 'one representative scaler'), 'lead') (('particular node', 'red zeros'), 'be') (('network convolutional neural deep neural that', 'image most commonly analysis'), 'network') (('which', 'GNNs'), 'know') (('nodes', 'that'), 'conceptualize') (('mean loss return loss masked_accuracy def predictions', 'mask mask mask'), 'label') (('we', 'only first two arguments'), 'accept') (('common goal', 'eq4 https uploads'), 'calculate') ", "extra": "['test', 'lung', 'lung cancer']"}