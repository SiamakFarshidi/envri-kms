{"name": "data science trends on kaggle ", "full_name": " h2 Historical Data Science Trends on Kaggle h2 1 Linear Vs Logistic Regression h2 2 The dominance of xgboost h2 3 Trends of Neural Networks and Deep Learning h2 4 ML Tools used on Kaggle h2 5 XgBoost vs Keras h2 6 What Kagglers are using for Data Visualizations h2 7 Important Data Science Techniques h2 8 Kaggle Components What people talks about the most ", "stargazers_count": 0, "forks_count": 0, "description": "Personally I am a big fan of plotly as well. Surprizing to see that discussions related to Feature Engineering and Model Tuninig are less than Ensembling. But when XgBoost was open sourced in 2014 it gained popularty quickly and dominated the kaggle competitions and kernels. Important Data Science Techniques Among the important data science steps kagglers focus alot on Model Ensembling since many winning solutions on kaggle competitions are ensemble models the blends and stacked models. Historical Data Science Trends on Kaggle A number of trends have changed over the years in the field of Data Science. There is a regression competition as well House Prices advanced regression but people more often start it after titanic only. Kaggle Components What people talks about the most Kaggle communitiy has shared a number of competition related discussions in fourms and are increasing in general. One of the reason for light gbm popularity is the faster implementation and simple interface as compared to xgboost. One indication is that there are more number of classification problems than regression problems on Kaggle including the most popular Titanic Survival Prediction competition. For example lightgbm was used in the winning solution of Porto Seguro s Safe Driver Prediction. Deep learning is also become populary every month because of different variants of models such as RNNs CNNs have shown great improvements in the kernels. Kaggle is the largest and the most popular data science community across the globe. Also many cloud instance providers such as Amazon AWS Google cloud etc showcases their capabilities of training very deep neural networks on clouds. With the launch of kernels in 2016 their useage increased to a great extent. get relevant text cleaning ignore tokenize. One of the reason is the the Toxic Comments Classification Competition in which a number of authors shared excellent information related to classification models including logistic regression. In almost every regression or classification kernels one can notice the ensemblling kernels. XgBoost vs Keras Among both the popular techniques on Kaggle xgboost and deeplearning xgboost has remained on top because it is faster and requires less computational infrastructure than very complex and deeper neural networks. What Kagglers are using for Data Visualizations Plotly has gained so much popularity since 2017 and is one of the most used data visualization library among the kernels. The deeplearning models also became popular because of a number of Image Classification competitions on Kaggle such as Data Science Bowl competitions from Google etc. For instance Catboost was recently released and is starting gaining popularity. The era of deep learning started in 2014 with the arrival of libraries such as theano tensorflow in 2015 and keras in 2016. Also transfer learning and pre trained models have shown great results in competitions. Data Exploration is the important technique and people have started stressing on the importance of exploration in the EDA kernels. The number of discussions related to deep learning is increasing regularly and are always more than neural networks. This is because its primarily audience is the novice and begineers but for sure in coming years and with the more addition of courses kaggle learn section will reach the similar levels as competitions and kernels. Just for an example in Toxic Comment Classification Competition massively large number of ensemling kernels were shared. This competition has most number of discussions and is one of the longest running compeition on Kaggle. Also deeplearning models became popular for text classification problems for example Quora Duplicate Questions Classification. Based on the recent increasing trend of lightgbm shown in red one can forecast that it will dominate next few years as well unless any other company opensources a better model. Some of the high quality visualization kernels by kaggle grandmasters such as SRK and Anistropic are created with plotly. People tend to forget that ensembling is only the last stage of any modelling process but a considerable amount of time should be given to feature engineering and model tuning tasks. Also a number of Data Science for Good Challenges and Kernels only competitions have been launched on kaggle which are one of the reason of kernels popularity. Some examples are Otto Group Classification Competition in which first place solution made use of xgboost. Kaggle can launch more competitions playgrounds related to Image Classification Modelling as people wants to learn from them alot. ML Tools used on Kaggle Scikit Learn was the only library used on kaggle for machine learning tasks but since 2015 tensorflow gained populartiy. In this kernel I am using Kaggle Meta Data to explore the Data Science trends over the years. Firstly kagglers shared kernels in competitions only but with a more focus on kaggle datasets kernel awards the number of discussions related to kernels started rising and have surpassed the discussions related to competitions. Linear Vs Logistic RegressionLets look at the comparison of linear regression and logistic regression discussions on forums kernels and replies on kaggle. Among the ML tools Keras is the most popular because of the simplistic deep learning implementation. These two tasks have the most important significance in the best and accurate models. Not to forget that Kaggle have added the GPU support in kernels which facilitates the Deep Learning useage on kaggle. Kaggle also launched the awesome Kaggle Learn section which is becoming popular and popular but still it is behind than the compeitions kernels and discussions. From the above graph we can observe that there were always been more discussions related to logistic regression than linear regression. The dominance of xgboost Before 2014 Linear Models Decision Trees and Random Forests were very popular. The second best is seaborn which is used extensively as well. Today xgboost is still used exhaustively in compeitions and is the part of the winning models of many competitions. However with the arrival of Lightgbm in 2016 the useage of xgboost dipped to some extent and popularity of lightgbm started rising very quickly. Trends of Neural Networks and Deep Learning Neural networks were present in the industry since the decades but in recent years trends changed because of the access to much larger data and computational power. The generel trend is that number of discussions are increasing every month. The number of logistic regression discussions on forums kernel comments and replies boomed to high numbers in October 2017 and March 2018. ", "id": "shivamb/data-science-trends-on-kaggle", "size": "6991", "language": "python", "html_url": "https://www.kaggle.com/code/shivamb/data-science-trends-on-kaggle", "git_url": "https://www.kaggle.com/code/shivamb/data-science-trends-on-kaggle", "script": "stopwords get_top_ngrams plotly.offline plotthem check_presence clntxt itertools collections iplot seaborn numpy plotly.graph_objs striphtml plotly tools zip_longest matplotlib.pyplot pandas ngrams Counter nltk.corpus init_notebook_mode plotly.plotly nltk.util ", "entities": "(('One', 'faster simple xgboost'), 'be') (('regression as well House Prices advanced people', 'titanic'), 'be') (('Also trained models', 'competitions'), 'transfer') (('which', 'kaggle'), 'forget') (('classification one', 'ensemblling kernels'), 'notice') (('important people', 'EDA kernels'), 'be') (('people', 'alot'), 'launch') (('competition', 'Kaggle'), 'have') (('Linear Vs Logistic RegressionLets', 'kaggle'), 'look') (('trends', 'much larger data'), 'be') (('Historical Data Science Kaggle number', 'Data Science'), 'Trends') (('lightgbm', 'Porto Safe Driver Prediction'), 'use') (('also month different variants', 'kernels'), 'become') (('number', 'competitions'), 'kaggler') (('two tasks', 'best models'), 'have') (('Today xgboost', 'many competitions'), 'use') (('it', 'quickly kaggle competitions'), 'gain') (('which', 'kernels popularity'), 'number') (('many winning solutions', 'ensemble blends'), 'Techniques') (('place first solution', 'xgboost'), 'be') (('talks', 'fourms'), 'share') (('discussions', 'Model Ensembling'), 'surprize') (('number', 'discussions'), 'be') (('Kaggle', 'data science most popular globe'), 'be') (('still it', 'compeitions behind kernels'), 'launch') (('we', 'linear regression'), 'observe') (('tensorflow', 'populartiy'), 'be') (('number', 'logistic regression'), 'be') (('it', 'very complex neural networks'), 'remain') (('Keras', 'learning most simplistic deep implementation'), 'be') (('as well other company', 'better model'), 'forecast') (('useage', 'lightgbm'), 'dip') (('considerable amount', 'tuning engineering tasks'), 'tend') (('One indication', 'Titanic Survival Prediction most popular competition'), 'be') (('massively large number', 'ensemling kernels'), 'for') (('cloud instance Also many providers', 'clouds'), 'showcase') (('using', 'kernels'), 'gain') (('number', 'regularly always neural networks'), 'increase') (('Also deeplearning', 'example'), 'become') (('Some', 'such SRK'), 'create') (('Catboost', 'recently popularity'), 'release') (('primarily audience', 'competitions'), 'be') (('dominance', '2014'), 'be') (('era', '2016'), 'start') (('deeplearning models', 'Google etc'), 'become') (('useage', 'great extent'), 'increase') (('I', 'years'), 'use') ", "extra": ""}