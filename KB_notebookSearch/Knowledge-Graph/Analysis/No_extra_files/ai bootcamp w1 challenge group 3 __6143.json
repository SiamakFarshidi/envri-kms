{"name": "ai bootcamp w1 challenge group 3 ", "full_name": " h1 Week 1 Challenge h2 The task h2 The data h3 Loading the data into Python h2 Logistic regression h2 2 Layer network h2 3 Layer network h2 Excel tips h3 Matrix multiplication h3 You cannot change part of an array h3 Transpose h3 Exponents h3 Softmax h3 Random initialization h2 Grading h2 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "com en us article TRANSPOSE function ed039415 ed8a 4a81 93e9 4b6dfac76027 function. SoftmaxYou will have to enter the formula for softmax element wise. Sometimes you might encounter a warning You cannot change part of an array. See this youtube tutorial https www. Loop over epochs Forward propagation Backpropagation Gradient descent parameter update Assign new parameters to the model Pring loss accuracy every 100 iterations empty array to store losses Gradient descent. The labeling machine has mixed up the labels of three wine cultivars. Now there are thousands of bottles of which nobody knows who made them. The activation function of the hidden layer is tanh the activation function for the output layer is softmax again. 2 Layer networkIn the next sheet you will implement a 2 layer neural network. To update parameters copy over the New W1 and New b1 over into W1 and b1 on the left of the sheet. The original data contains the 13 measurements for all 178 bottles. To calculate the exponent of multiple values element wise like you have to do for softmax you enter EXP and then the range of cells. So you need to make the reference to those fixed. The first week is also the most math heavy. Note that you have to copy over two sets of weights and biases this time. ExponentsYou can calculate e x using the EXP function. Make sure to use Paste values only when you copy over the weights Try to experiment with the learning rate while you train and get the loss as low as possible. In a famous wine region a disaster has happened. Your first offer to distinguish the three wine makers by taste and then spend a couple of months drinking wine and labeling bottles as been refused. The taskYou got a call from Italy. So scroll to the right to see the full sheet. You can train this network the same way as the logistic regression network. For the python notebook create a public kernel and share the link via slack. If you don t fully understand this function don t worry it just generates decision boundary plot Set min and max values and give it some padding Generate a grid of points with distance h between them Predict the function value for the whole gid Plot the contour and training examples Get labels Get inputs Print shapes just to check Calculate exponent term first Clipping calue Number of samples Loss formula note that np. The goal of the challenge is to deepen your understanding of how neural networks work. The Excel file contains everything you need to solve the challenge in Excel. com watch v 5bNooxRm960 if you have trouble. We need to learn these. So to create random numbers once and then freeze them use RAND for all cells first then copy it and paste the values using paste values only on the same cells. Note however that the rand function create a new random number every time excel refreshes. Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280 OD315 of diluted wines Proline The wine makers had 178 bottles left in their cellars. sum sums up the entire matrix and therefore does the job of two sums from the formula Load parameters from model Linear step First activation function Second linear step Second activation function Load parameters from model Load forward propagation results Get number of samples Backpropagation Calculate loss derivative with respect to output Calculate loss derivative with respect to second layer weights Calculate loss derivative with respect to second layer bias Calculate loss derivative with respect to first layer Calculate loss derivative with respect to first layer weights Calculate loss derivative with respect to first layer bias Store gradients First layer weights First layer bias Second layer weights Second layer bias Package and return model Load parameters Update parameters Store and return parameters Do forward pass get y_hat Gradient descent. But first let s solve this weeks challenge. This is what we return at the end Package imports Matplotlib is a matlab like plotting library Numpy handles matrix operations SciKitLearn is a useful machine learning utilities library The sklearn dataset module helps generating datasets Display plots inline and change default figure size Helper function to plot a decision boundary. To make the sheet easier to handle Forward pass Backward pass and weight update are ordered horizontally not vertically. To multiply two matrices select the output area where you want the output to be enter the formula and hit CONTROL SHIFT ENTER. Week 1 ChallengeCongratulations You made it through the first weeks content The first week is the hardest because you have to learn about many new concepts. Note that softmax needs all cells of the example. So we will use these bottles as our training data. The dataYou can find all data in the datasource connected to this notebook on kaggle. Not that if you use transpose you always need to use CONTROL SHIFT ENTER otherwise it will not do anything. You will learn about normalization next week but the basic goal is to ensure that all features of the data have the same mean and standard deviation. It uses a softmax activation for the output layer. Just make sure that the output has a size of 3. GradingThis weeks challenge is not a competition for the most accurate prediction. The network has an input size of 13 and an output of 3. In this case either hit CONTROL SHIFT ENTER to apply the formula for the entire area or exit with ESC. SubmissionFor the excel challenges submit your finished Excel notebook through slack. Instead you are to build a classifier which recognizes the wine maker from 13 attributes of the wine. For this you will have to hit CONTROL SHIFT ENTER again. If you don t fully understand this function don t worry it just generates decision boundary plot Set min and max values and give it some padding Generate a grid of points with distance h between them Predict the function value for the whole gid Plot the contour and training examples Calculate exponent term first Clipping calue Number of samples Loss formula note that np. This makes it easier to deal with the data. You cannot change part of an array. That is you enter the formula for one cell and can then expand it for the other cells. Get labels Get inputs Print shapes just to check Package imports Matplotlib is a matlab like plotting library Numpy handles matrix operations SciKitLearn is a useful machine learning utilities library The sklearn dataset module helps generating datasets Display plots inline and change default figure size Helper function to plot a decision boundary. Excel tips Matrix multiplicationThe excel function for matrix multiplication is called MMULT https support. You can load the data like this Logistic regressionYour first task it to implement logistic regression in Excel. Loading the data into PythonYou can fork this notebook or simply create a new Kernel connected to this Kernels datasource in Kaggle. Instead points are awarded to teams who submit the following implementations Excel Logistic Regression 2 points 2 Layer Neural Net 2 points 3 Layer Neural Net 4 pointsPython 2 Layer Neural Net 2 points 3 Layer Neural Net 4 pointsA total of 14 points can be won. The normalized data and one hot encodings have been copied over to the logistic regression and 2 Layer Wine net sheet. 3 Layer networkAfter you have implemented a 2 layer network open a new sheet and implement a 3 layer network. Because this is a multi class problem the output y has already been converted to one hot matrix. This is what we return at the end. Say you have a sample with 3 values you want to compute softmax for. 1 A_1 1 A_1 2 A_1 3 EXP A1 SUM EXP A1 C1 EXP B1 SUM EXP A1 C1 EXP C1 SUM EXP A1 C1 Random initializationYou can create random numbers in Excel using the RAND https support. 0 A B C 1 0. sum sums up the entire matrix and therefore does the job of two sums from the formula Load parameters from model Linear step First activation function Second linear step Second activation function Third linear step Third activation function Load parameters from model Load forward propagation results Get number of samples Backpropagation Calculate loss derivative with respect to output Calculate loss derivative with respect to second layer weights Calculate loss derivative with respect to second layer bias Calculate loss derivative with respect to first layer Calculate loss derivative with respect to first layer weights Calculate loss derivative with respect to first layer bias Store gradients First layer weights First layer bias Second layer weights Second layer bias Third layer weights Third layer bias Package and return model Load parameters Update parameters Store and return parameters Do forward pass get y_hat empty array to store losses Gradient descent. com en us article MMULT function 40593ed7 a3cd 4b6b b9a3 e4ad3c7245eb. TransposeTo transpose a matrix in excel you can use the TRANSPOSE https support. You deserve to kick your feed up and enjoy a glass of wine. You will implement a multi class classifier in Excel and Python. To make training easier the data has also been normalized already. Loop over epochs Forward propagation Backpropagation Gradient descent parameter update Assign new parameters to the model Pring loss accuracy every 100 iterations Hyper parameters I picked this value because it showed good results in my experiments Initialize the parameters to random values. com en us article RAND function 4cbfa695 8869 4788 8d90 021ea9f5be73 function. You may choose the size of the two hidden layers yourself. So it is all downhill from here. The hidden unit has a size of 5. ", "id": "odraode/ai-bootcamp-w1-challenge-group-3", "size": "6143", "language": "python", "html_url": "https://www.kaggle.com/code/odraode/ai-bootcamp-w1-challenge-group-3", "git_url": "https://www.kaggle.com/code/odraode/ai-bootcamp-w1-challenge-group-3", "script": "sklearn.metrics softmax_loss predict train update_parameters plot_decision_boundary numpy train_plot softmax matplotlib.pyplot pandas OneHotEncoder accuracy_score loss_derivative forward_prop tanh_derivative initialize_parameters sklearn.preprocessing backward_prop ", "entities": "(('You', 'Excel'), 'load') (('Excel tips Matrix multiplicationThe excel function', 'matrix multiplication'), 'call') (('EXP A1 SUM EXP A1 C1 EXP B1 SUM EXP A1 C1 EXP C1 SUM EXP A1 Random 1 2 3 initializationYou', 'RAND https support'), '1') (('you', 'loss'), 'make') (('it', 'data'), 'make') (('GradingThis weeks challenge', 'most accurate prediction'), 'be') (('which', 'wine'), 'be') (('You', 'Excel'), 'implement') (('you', 'softmax'), 'say') (('You', 'two hidden layers'), 'choose') (('activation function', 'output layer'), 'be') (('you', 'Excel'), 'contain') (('Backward pass update', 'Forward'), 'pass') (('who', 'them'), 'be') (('you', 'then cells'), 'enter') (('distance h', 'samples Loss formula'), 'understand') (('dataYou', 'kaggle'), 'find') (('So you', 'those'), 'need') (('You', 'array'), 'encounter') (('you', 'weights'), 'note') (('rand however function', 'new random number'), 'note') (('wine makers', 'cellars'), 'Alcalinity') (('forward pass', 'Gradient descent'), 'sum') (('Logistic Neural Neural Neural Excel Regression 2 2 Net 2 3 Neural Net 4 pointsPython 2 2 3 4 pointsA total', '14 points'), 'award') (('output', 'CONTROL SHIFT ENTER'), 'select') (('class multi output', 'already one hot matrix'), 'be') (('you', 'trouble'), 'watch') (('normalized data', 'one hot logistic regression'), 'copy') (('otherwise it', 'anything'), 'need') (('labeling machine', 'wine three cultivars'), 'mix') (('softmax', 'example'), 'note') (('features', 'same mean deviation'), 'learn') (('Just output', '3'), 'make') (('excel challenges', 'slack'), 'submit') (('it', 'values'), 'update') (('So we', 'training data'), 'use') (('You', 'wine'), 'deserve') (('SoftmaxYou', 'softmax element'), 'have') (('CONTROL', 'ESC'), 'SHIFT') (('model Load Update', 'Gradient descent'), 'sum') (('how neural networks', 'understanding'), 'be') (('you', 'TRANSPOSE https support'), 'transpose') (('Layer 2 next sheet you', 'layer neural 2 network'), 'networkin') (('layer 2 network', 'layer 3 network'), 'implement') (('you', 'many new concepts'), 'make') (('ExponentsYou', 'EXP function'), 'calculate') (('utilities sklearn dataset useful machine learning module', 'decision boundary'), 'be') (('disaster', 'wine famous region'), 'happen') (('CONTROL SHIFT', 'ENTER'), 'have') (('python notebook', 'slack'), 'create') (('original data', '178 bottles'), 'contain') (('It', 'output layer'), 'use') (('you', 'other cells'), 'be') (('You', 'logistic regression same way network'), 'train') (('first s', 'weeks challenge'), 'let') (('Pring loss accuracy', 'Gradient descent'), 'update') (('Loading', 'Kaggle'), 'fork') (('network', '3'), 'have') ", "extra": ""}