{"name": "fast ai machine learning lesson 1 ", "full_name": " h1 Intro to Random Forests h2 About this course h3 Teaching approach h3 Your practice h3 Books h3 Syllabus in brief h2 Imports h1 Introduction to Blue Book for Bulldozers h2 About h3 our teaching h3 our approach to machine learning h3 this dataset h3 Kaggle Competitions h2 The data h3 Look at the data h3 Initial processing h3 Pre processing h1 Random Forests h2 Base model h2 Speeding things up h2 Single tree h2 Bagging h3 Intro to bagging h3 Out of bag OOB score h2 Reducing over fitting h3 Subsampling h3 Tree building parameters ", "stargazers_count": 0, "forks_count": 0, "description": "Compare this to our original model on a sample Out of bag OOB scoreIs our validation set worse than our training set because we re over fitting or because the validation set is for a different time period or a bit of both With the existing information we ve shown we can t tell. To start focus on what things DO not what they ARE. A validation set helps diagnose this problem. However random forests have a very clever trick called out of bag OOB error which can handle this and more The idea is to calculate error on the training set but only include the trees in the calculation of a row s error where that row was not included in training that tree. ipynbBased on notebook version bbcd4e0Course page http course. explaining what they ve learned by writing or helping others Therefore we suggest that you practice these skills on Kaggle by 1. Since each additional tree allows the model to see more data this approach can make additional trees more useful. shallow and or deep learning mainly for unstructured data such as audio vision and natural language In this course we ll be doing a deep dive into random forests and simple models learnt with SGD. We do this by specifying with min_samples_leaf that we require some minimum number of rows in every leaf node. The data for this competition is split into three parts Train. csv saledate the date of the sale Question What stands out to you from the above description What needs to be true of our training and validation sets In any sort of data science work it s important to look at your data to make sure you understand the format how it s stored what type of values it holds etc. Without expanding your date time into these additional fields you can t capture any trend cyclical behavior as a function of time at any of these granularities. Even if you ve read descriptions about your data the actual data may not be what you expect. Random Forests Base modelLet s try our model again this time with separate training and validation sets. This has two benefits There are less decision rules for each leaf node simpler models should generalize better The predictions are made by averaging more rows in the leaf node resulting in less volatilityWe can also increase the amount of variation amongst the trees by not only use a sample of rows for each tree but to also using a sample of columns for each split. The training set result looks great But the validation set is worse than our original model. Typically in a bottom up approach you first learn all the separate components you will be using and then you gradually build them up into more complex structures. A leader board to see what s good what s possible and what s state of art. com questions 361 when is a model underfitted The error for the pictured data points is lowest for the model on the far right the blue curve passes through the red points almost perfectly yet it s not the best choice. Your practicePeople learn by 1. Speeding things up Single treeLet s see what happens if we create a bigger tree. com Hands Machine Learning Scikit Learn TensorFlow dp 1491962291 ref pd_lpo_sbs_14_t_0 _encoding UTF8 psc 1 refRID MBV2QMFH3EZ6B3YBY40K Syllabus in briefDepending on time and class interests we ll cover something like not necessarily in this order Train vs test Effective validation set construction Trees and ensembles Creating random forests Interpreting random forests What is ML Why do we use it What makes a good ML project Structured vs unstructured data Examples of failures mistakes Feature engineering Domain specific dates URLs text Embeddings latent factors Regularized models trained with SGD GLMs Elasticnet etc NB see what James covered Basic neural nets PyTorch Broadcasting Matrix Multiplication Training loop backpropagation KNN CV bootstrap Diabetes data set Ethical considerationsSkip Dimensionality reduction Interactions Monitoring training Collaborative filtering Momentum and LR annealing Imports Introduction to Blue Book for Bulldozers About. We ll grab the predictions for each individual tree and look at one example. The data Look at the dataKaggle provides info about some of the fields of our dataset on the Kaggle Data info https www. Kaggle CompetitionsKaggle is an awesome resource for aspiring data scientists or anyone looking to improve their machine learning skills. Interesting data sets2. We will start using some black boxes such as random forests that haven t yet been explained in detail and then we ll dig into the lower level details later. com articles 2017 deepcoder_and_ai_hype. Entering competitions doing 2. me 2014 10 07 moving beyond ctr better recommendations through human evaluation Slav Ivanov https blog. We ll replace categories with their numeric codes handle missing continuous values and split the dependent variable into a separate variable. However in this case Kaggle tells us what metric to use RMSLE root mean squared log error between the actual and predicted auction prices. There is nothing like being able to get hands on practice and receiving real time feedback to help you improve your skills. Jeremy has worked in a number of different areas feel free to ask about anything that he might be able to help you with at any time even if not directly related to the current topic Management consultant McKinsey AT Kearney Self funded startup entrepreneur Fastmail first consumer synchronized email Optimal Decisions first optimized insurance pricing VC funded startup entrepreneur Kaggle Enlitic first deep learning medical company I ll be using a top down teaching method which is different from how most math courses operate. Why is that If you were to gather some new data points they most likely would not be on that curve in the graph on the right but would be closer to the curve in the middle graph. csv is the validation set which contains data from January 1 2012 April 30 2012 You make predictions on this set throughout the majority of the competition. csv is the training set which contains data through the end of 2011. The sklearn docs show an example http scikit learn. Rachel has been dealing with a life threatening illness so will not be teaching as originally planned this year. The categorical variables are currently stored as strings which is inefficient and doesn t provide the numeric coding required for a random forest. com non artistic style transfer or how to draw kanye using captain picards face c4a50256b814 fast. It s important to note what metric is being used for a project. The good news is that modern machine learning can be distilled down to a couple of key techniques that are of very wide applicability. And suppose you have lots of parameters Underfitting and Overfitting https datascience. Reducing over fitting SubsamplingIt turns out that one of the easiest ways to avoid over fitting is also one of the best ways to speed up analysis subsampling. The problems with this are that students often lose motivation don t have a sense of the big picture and don t know what they ll need. com fastai fastai blob master courses ml1 lesson1 rf. com Making Learning Whole Principles Transform dp 0470633719 in which he uses baseball as an analogy. this datasetWe will be looking at the Blue Book for Bulldozers Kaggle Competition The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it s usage equipment type and configuaration. ipynb more here http norvig. ca blog 2014 08 12 what happens if you write a tcp stack in python Julia Ferraioli http blog. Therefore we call train_cats to convert strings to pandas categories. This is why we need to use bagging of multiple trees to get more generalizable results. html Julia Evans https codewords. This illustrates how using all our data can lead to overfitting. That way given enough trees the model can still see all the data but for each individual tree it ll be just as fast as if we had cut down our dataset as before. org stable auto_examples ensemble plot_ensemble_oob. We can specify the order to use for categorical variables if we wish Normally pandas will continue displaying the text categories while treating them as numerical data internally. ai deep learning course that is what we used. It was modified in the data input only so that i can run on Kaggle kernels. Generally selecting the metric s is an important part of the project setup. Initial processingThis dataset contains a mix of continuous and categorical variables. Optionally we can replace the text categories with numbers which will make this variable non categorical like so. Source https github. ai 2016 10 08 teaching philosophy or in this talk https vimeo. doing coding and building 2. To understand this issue let s simplify things down to a single small tree. com towards data science thoughts after taking the deeplearning ai courses 8568f132153 and children begin playing baseball before they learn all the formal rules. We don t require kids to memorize all the rules of baseball and understand all the technical details before we let them play the game. com ipython ProbabilityParadox. html of different max_features methods with increasing numbers of trees as you see using a subset of features on each split requires using more trees but results in better models sklearn max_features chart http scikit learn. This is a copy of lesson 1 notebook from fast. html Edwin Chen http blog. Another way to reduce over fitting is to grow our trees less deeply. Tree building parametersWe revert to using a full bootstrap sample in order to show the impact of other over fitting avoidance methods. Your score on this set is used to create the public leaderboard. For machine learning with Python we recommend Introduction to Machine Learning with Python https www. Blog posts by winning contestants share useful tips and techniques. A machine can be sold multiple times saleprice what the machine sold for at auction only provided in train. The following method extracts particular date fields from a complete datetime for the purpose of constructing categoricals. Let s return to using our full dataset so that we can demonstrate the impact of this technique. org stable _images sphx_glr_plot_ensemble_oob_001. com Python Data Analysis Wrangling IPython dp 1491957662 ref asap_bc ie UTF8. png The following code is supposed to fail due to string values in the input data same as Kaggle s test set size leaf. our approach to machine learningMost machine learning courses will throw at you dozens of different algorithms with a brief technical description of the math behind them and maybe a toy example. An r 2 in the high 80 s isn t bad at all and the RMSLE puts us around rank 100 of 470 on the Kaggle leaderboard but we can see from the validation set score that we re over fitting badly. com picking an optimizer for style transfer 86e7b8cba84b fast. As motivation suppose you don t divide up your data but instead use all of it. ai we have a distinctive teaching philosophy http www. The basic idea is this rather than limit the total amount of data that our model can access let s instead limit it to a different random subset per tree. You re left confused by the enormous range of techniques shown and have little practical understanding of how to apply them. A lot of the new material however covers deep learning in Tensorflow which isn t relevant to this course Hands On Machine Learning with Scikit Learn and TensorFlow https www. com Introduction Machine Learning Andreas Mueller dp 1449369413 From one of the scikit learn authors which is the main library we ll be using Python Machine Learning Machine Learning and Deep Learning with Python scikit learn and TensorFlow 2nd Edition https www. We now have something we can pass to a random forest In statistics the coefficient of determination denoted R2 or r2 and pronounced R squared is the proportion of the variance in the dependent variable that is predictable from the independent variable s. Therefore we take the log of the prices so that RMSE will give us what we need. This is a very common type of dataset and prediciton problem and similar to what you may see in your project or workplace. This is different from how most traditional math technical courses are taught where you have to learn all the individual elements before you can combine them Harvard professor David Perkins call this elementitis but it is similar to how topics like driving and baseball are taught. All that to say don t worry if you don t understand everything at first You re not supposed to. We re still not quite done for instance we have lots of missing values which we can t pass directly to a random forest. It contains data from May 1 2012 November 2012. com issues five why do neural networks think a panda is a vulture more here https jvns. ai and USF MSAN student BooksThe more familiarity you have with numeric programming in Python the better. You should always consider this feature extraction step when working with date time. csv is the test set which won t be released until the last week of the competition. ai 2016 10 08 teaching philosophy of the whole game https www. Let s get a baseline for this full set to compare to. com c bluebook for bulldozers data page they say the following For this competition you are predicting the sale price of bulldozers sold at auctions. The key fields are in train. This shows that our validation set time difference is making an impact as is model over fitting. com ipython Stephen Merity https smerity. If you re looking to improve in this area we strongly suggest Wes McKinney s Python for Data Analysis 2nd ed https www. This allows us to see whether the model is over fitting without needing a separate validation set. Random Forests and Gradient Boosting Machines mainly for structured data such as you might find in a database table at most companies Multi layered neural networks learnt with SGD i. html Intro to Random Forests About this course Teaching approachThis course is being taught by Jeremy Howard and was developed by Jeremy along with Rachel Thomas. To get better at technical writing study the top ranked Kaggle kernels from past competitions and read posts from well regarded technical bloggers. You can hear more about my teaching philosophy in this blog post http www. But let s save this file for now since it s already in format can we be stored and accessed efficiently. This is as simple as adding one more parameter to our model constructor. Rather they start playing with a just general sense of it and then gradually learn more rules details as time goes on. Bagging Intro to baggingTo learn about bagging in random forests let s start with our basic model again. ai student Brad Kenstler https hackernoon. com Python Machine Learning scikit learn TensorFlow dp 1787125939 ref dp_ob_title_bk New version of a very successful book. Creating Kaggle kernels explaining It s OK if you don t get good competition ranks or any kernel votes at first that s totally normal Just try to keep improving every day and you ll see the results over time. Feedback on how you re doing3. com Making Learning Whole Principles Transform dp 0470633719 ref sr_1_1 ie UTF8 qid 1505094653. com 2016 02 exploring world using vision twilio. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods Ensembles of decision trees i. csv are SalesID the uniue identifier of the sale MachineID the unique identifier of a machine. You ll be learning about gradient boosting and deep learning in part 2. That is you can start driving without knowing how an internal combustion engine works https medium. Pre processingIn the future we can simply read it from this fast format. Some good role models include Peter Norvig http nbviewer. 98 that s great right Well perhaps not. 5 sqrt 1 3 5 10 25 100We can t compare our results directly with the Kaggle competition since it used a different validation set and we can no longer to submit to this competition but we can at least see that we re getting similar results to the winners based on the dataset we have. Possibly the most important idea in machine learning is that of having separate training validation data sets. We print the OOB error last in our print_score function below. The shape of this curve suggests that adding more trees isn t going to help us much. We do this by specifying max_features which is the proportion of features to randomly select from at each split. This also has the benefit of allowing us to see whether our model generalizes even if we only have a small amount of data so want to avoid separating some out to create a validation set. Harvard Professor David Perkins has a book Making Learning Whole https www. The data is sourced from auction result postings and includes information on usage and equipment configurations. Your score on the test set determines your final rank for the competition. ai course Introduction to Machine Learning for Coders. org wiki Coefficient_of_determinationWow an r 2 of 0. ", "id": "miwojc/fast-ai-machine-learning-lesson-1", "size": "18000", "language": "python", "html_url": "https://www.kaggle.com/code/miwojc/fast-ai-machine-learning-lesson-1", "git_url": "https://www.kaggle.com/code/miwojc/fast-ai-machine-learning-lesson-1", "script": "sklearn.ensemble walk rmse dectree_max_depth display sklearn metrics fastai.imports RandomForestClassifier split_vals pandas_summary RandomForestRegressor display_all print_score DataFrameSummary fastai.structured IPython.display ", "entities": "(('machine', 'only train'), 'sell') (('variable', 'numbers'), 'replace') (('Ethical considerationsSkip Dimensionality reduction Interactions Collaborative filtering Momentum', 'Bulldozers'), 'com') (('goal', 'usage equipment type'), 'look') (('data', 'usage configurations'), 'source') (('they', 'what'), 'start') (('ai Introduction', 'Coders'), 'course') (('you', 'Overfitting https parameters Underfitting datascience'), 'suppose') (('you', 'skills'), 'be') (('math how most courses', 'teaching top method'), 'work') (('training which', '2011'), 'be') (('http scikit', 'example'), 'show') (('you', 'what'), 'be') (('better predictions', 'split'), 'have') (('future we', 'fast format'), 'pre') (('then we', 'level lower details'), 'start') (('you', '1'), 'explain') (('scikit', 'better models'), 'require') (('score', 'competition'), 'determine') (('we', 'already format'), 'let') (('them', 'game'), 'don') (('Harvard Professor David Perkins', 'book'), 'have') (('me', 'evaluation Slav Ivanov https human blog'), 'move') (('png following code', 'size same test set leaf'), 'suppose') (('com Python Machine Learning scikit', 'TensorFlow dp 1787125939 dp_ob_title_bk New very successful book'), 'learn') (('we', 'bigger tree'), 'see') (('s', 'basic model'), 'let') (('Multi', 'SGD i.'), 'layer') (('score', 'public leaderboard'), 'use') (('we', 'Python https www'), 'recommend') (('that', 'very wide applicability'), 'be') (('Teaching course', 'Rachel Thomas'), 'Intro') (('just as we', 'dataset'), 'see') (('they', 'formal rules'), 'com') (('You', 'www'), 'hear') (('Initial processingThis dataset', 'continuous variables'), 'contain') (('trees isn more t', 'us'), 'suggest') (('You', 'deep part'), 'learn') (('vast majority', 'decision trees i.'), 'show') (('totally Just day you', 'time'), 'create') (('You', 'first'), 'say') (('then you', 'more complex structures'), 'learn') (('almost perfectly yet it', 'red points'), 'be') (('how using', 'overfitting'), 'illustrate') (('additional trees', 'more data'), 'allow') (('how topics', 'driving'), 'be') (('he', 'analogy'), 'com') (('mean', 'auction actual prices'), 'tell') (('we', 'what'), 'ai') (('why we', 'more generalizable results'), 'be') (('only i', 'Kaggle kernels'), 'modify') (('even we', 'validation set'), 'have') (('Generally selecting', 'project important setup'), 'be') (('way', 'trees'), 'be') (('We', 'separate variable'), 'replace') (('s', 'single small tree'), 'let') (('we', 't directly random forest'), 'have') (('Therefore we', 'pandas categories'), 'call') (('they', 'middle graph'), 'be') (('we', 'what'), 'take') (('It', 'May'), 'contain') (('they', 'what'), 'be') (('data', 'parts three Train'), 'split') (('You', 'how them'), 'leave') (('you', 'project'), 'be') (('we', 'technique'), 'let') (('This', 'lesson 1 notebook'), 'be') (('following method', 'categoricals'), 'extract') (('that', 's.'), 'have') (('time', 'rules then gradually more details'), 'start') (('we', 'existing information'), 'compare') (('Rachel', 'life threatening illness'), 'deal') (('you', 'granularities'), 'without') (('don t', 'it'), 'suppose') (('We', 'one example'), 'grab') (('csv', 'unique machine'), 'salesid') (('it', 'etc'), 'saledate') (('30 2012 You', 'competition'), 'be') (('combustion how internal engine', 'https medium'), 'be') (('data', 'Kaggle Data info https www'), 'look') (('Random Forests Base modelLet', 'again time separate training sets'), 'try') (('s', 'full set'), 'let') (('which', 'competition'), 'be') (('we', 'fitting'), 'put') (('what', 'art'), 'board') (('we', 'www'), 'have') (('We', 'print_score last function'), 'print') (('validation set', 'problem'), 'help') (('which', 'random forest'), 'store') (('metric', 'project'), 's') (('USF MSAN BooksThe more you', 'Python'), 'ai') (('Possibly most important idea', 'training validation data separate sets'), 'be') (('Blog posts', 'useful tips'), 'share') (('approach', 'them'), 'throw') (('impact', 'fitting'), 'show') (('we', 'dataset'), 'compare') (('Normally pandas', 'numerical data'), 'specify') (('where row', 'tree'), 'have') (('we', 'leaf node'), 'do') (('You', 'date when time'), 'consider') (('you', 'auctions'), 'bluebook') (('validation', 'original model'), 'look') (('we', 'Data Analysis 2nd ed https www'), 'suggest') (('we', 'simple SGD'), 'learning') (('Kaggle CompetitionsKaggle', 'machine learning skills'), 'be') (('s', 'tree'), 'be') (('you', 'http blog'), 'ca') (('one', 'analysis subsampling'), 'turn') (('which', 'randomly split'), 'do') (('panda', 'com five'), 'issue') (('lot', 'Scikit Learn'), 'cover') (('This', 'model constructor'), 'be') (('model', 'validation separate set'), 'allow') (('scikit', 'Deep Python'), 'dp') ", "extra": "['test', 'bag']", "label": "Perfect_files", "potential_description_queries": ["apply", "approach", "area", "audio", "backpropagation", "bag", "bagging", "baseline", "basic", "behavior", "best", "bit", "blob", "blog", "board", "book", "boosting", "bottom", "build", "calculate", "calculation", "call", "case", "categorical", "chart", "children", "code", "coding", "coefficient", "combine", "company", "compare", "competition", "consider", "convert", "copy", "course", "create", "csv", "current", "curve", "cut", "data", "database", "dataset", "date", "datetime", "day", "decision", "dependent", "description", "detail", "difference", "directly", "draw", "end", "engineering", "ensemble", "error", "evaluation", "even", "every", "everything", "explained", "extraction", "face", "fail", "fastai", "feature", "feedback", "file", "final", "find", "fitting", "following", "forest", "format", "function", "future", "game", "general", "grab", "gradient", "graph", "grow", "handle", "help", "high", "http", "https github", "human", "idea", "improve", "include", "increase", "individual", "info", "input", "instance", "issue", "kernel", "key", "language", "lead", "leaderboard", "leaf", "learn", "learning", "least", "left", "lesson", "let", "level", "library", "life", "little", "log", "look", "looking", "loop", "lot", "lower", "main", "majority", "math", "max_features", "mean", "medical", "method", "metric", "middle", "might", "minimum", "missing", "model", "most", "motivation", "multiple", "my", "need", "neural", "new", "no", "node", "non", "normal", "not", "notebook", "number", "numeric", "numerical", "optimizer", "order", "out", "page", "parameter", "part", "past", "per", "period", "picture", "png", "post", "practice", "predict", "price", "print", "problem", "project", "provide", "public", "purpose", "python", "r2", "random", "range", "rank", "re", "read", "recommend", "reduce", "replace", "result", "return", "right", "role", "row", "run", "sample", "save", "science", "scikit", "score", "select", "sense", "separate", "set", "shape", "similar", "single", "size", "sklearn", "something", "sort", "speed", "split", "sqrt", "squared", "stack", "start", "state", "step", "string", "student", "style", "subset", "table", "test", "text", "think", "through", "time", "topic", "total", "training", "transfer", "tree", "trend", "try", "type", "understanding", "unique", "until", "up", "usage", "validation", "variable", "variance", "variation", "version", "vision", "week", "while", "work", "world", "write"], "potential_description_queries_len": 246, "potential_script_queries": ["display", "rmse", "walk"], "potential_script_queries_len": 3, "potential_entities_queries": ["day", "following", "info", "lower", "random", "set", "time", "validation"], "potential_entities_queries_len": 8, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 248}