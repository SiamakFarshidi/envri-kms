{"name": "python ml breast cancer diagnostic data set ", "full_name": " h1 A brief tutorial on using Python to make predictions Breast Cancer Wisconsin Diagnostic Data Set h3 de Freitas R C h1 1 Introduction h1 2 Preparing the data h1 3 Visualizing the data h1 4 Machine learning h2 4 1 Using all mean values features h3 4 1 1 Stochastic Gradient Descent h3 4 1 2 Support Vector Machines h3 4 1 3 Nearest Neighbors h3 4 1 3 Naive Bayes h3 4 1 4 Forest and tree methods h2 4 2 Using the selected features h3 4 2 1 Stochastic Gradient Descent h3 4 2 2 Support Vector Machines h3 4 2 3 Nearest Neighbors h3 4 2 4 Naive Bayes h3 4 2 5 Forest and tree methods h1 5 Improving the best model h2 5 1 Naive Bayes h2 5 2 Forest and tree methods ", "stargazers_count": 0, "forks_count": 0, "description": "Still another form of doing this could be using box plots which is done below. As we saw above some of the features can have most of the times values that will fall in some range depending on the diagnosis been malignant or benign. In all cases the procedure will be the following 1. We will use the code below to delete this entire column. We can also see how the malignant or benign tumors cells can have or not different values for the features plotting the distribution of each type of diagnosis for each of the mean features. org Below we will use Seaborn to create a heat map of the correlations between the features. Diagnosis M malignant B benign 3 32 Ten real valued features are computed for each cell nucleus a radius mean of distances from center to points on the perimeter b texture standard deviation of gray scale values c perimeter d area e smoothness local variation in radius lengths f compactness perimeter 2 area 1. For this we will use Scikit learn 1 package. org stable modules generated sklearn. 1 Stochastic Gradient Descent 4. Each estimator has a different set of hyper parameters which can be found in the corresponding documentation. 3 Visualizing the data In this section we will build visualizations of the data in order to decide how to proceed with the machine learning tools. org wiki OverfittingNext we will use nine different classifiers all with standard parameters. 4 Naive Bayes 4. edu ml datasets Breast Cancer Wisconsin 28Diagnostic 29 2 Preparing the data We will start loading some of the packages that will help us organize and visualize the data. This will be done with an exhaustive grid search 1 provided by the GridSearchCV function. 1 Stochastic Gradient Descent The first classifier is the Stochastic Gradient Descent 1. Now we can count how many diagnosis are malignant M and how many are benign B. 2 Support Vector Machines Now we will use three different Support Vector Machines 1 classifiers. To do that we will need to use the Seaborn 1 and the Matplotlib 2 packages. 2 Using the selected features In this section we will apply the same classifiers for the data with the features that were previously selected based on the analysis of section 3. A brief tutorial on using Python to make predictions Breast Cancer Wisconsin Diagnostic Data Set de Freitas R. 5 Improving the best model Not all parameters of a classifier is learned from the estimators. the classifier clf is fitted with the train data set X_train and y_train 3. 1 Naive Bayes 5. org 2 https matplotlib. In the end we will compare the accuracy the cross validation score for the selected set and the complete set of features. org stable modules model_evaluation. It is also possible to create a scatter matrix with the features. The grid search will be done only on the best models which are Naive Bayes Random Forest Extra Trees and Decision Trees. the predictions are found using X_test 4. org stable tutorial machine_learning_map index. html accuracy score 4. 3 Naive Bayes The Naive Bayes algorithm applies Bayes theorem with the assumption of independence between every pair of features. In order to avoid Overfitting 1 we will use the function train_test_split to split the data randomly random_state 42 into a train and a test set. cross_val_score 2 http scikit learn. 0 g concavity severity of concave portions of the contour h concave points number of concave portions of the contour i symmetry j fractal dimension coastline approximation 1 The mean standard error and worst or largest mean of the three largest values of these features were computed for each image resulting in 30 features. We are interested mainly in the mean values of the features so we will separate those features in the list below in order to make some work easier and the code more readably. The features from the data set describe characteristics of the cell nuclei and are computed from a digitized image of a fine needle aspirate FNA of a breast mass. At the end the results are presents in along with the total time needed to run all the process. 4 Machine learning In this section we will test and analyze machine learning algorithms for classification in order to identify if the tumor is malignant or benign based on the cell features. To remember those features are radius_mean perimeter_mean area_mean concavity_mean concave points_mean. 2 Support Vector Machines 4. the classifier clf is initialized 2. 1 IntroductionThe aim of this notebook is to me and others to understand the process of organizing and preparing the data selecting the features choosing and applying the machine learning tools comparing selecting and improving the best models. With help of Pandas 1 we will load the data set and print some basic informations. 4 Forest and tree methods 4. 1 https archive. Those parameters are called hyper parameters and are passed as arguments to the constructor of the classifier. htmlThe algorithms will process only numerical values. During the data set loading a extra column was created. The function f will be construct by the machine learning algorithm based on the ys and Xs that are already known. After running the piece of codes below it will be presented the accuracy the cross validation score and the best set of parameters. For this reason we will transform the categories M and B into values 1 and 0 respectively. the accuracy is estimated with help of cross validation 1 5. After training our machine learning algorithm we need to test its accuracy. 1 http scikit learn. The red dots correspond to malignant diagnosis and blue to benign. The necessary tools will be loaded as needed. In other cases only the accuracy or the cross validation score could be improved. As described in UCI Machine Learning Repository 1 the attribute informations are 1. org stable 2 http scikit learn. We can search for the best performance of the classifier sampling different hyper parameter combinations. To choose the right estimator algorithm we used the flowchart 2 found in the Scikit learn web page. 2 Forest and tree methods As can be seen in one case Extra Trees both accuracy and cross validations score were improved but only by some few percents and with the cost of more computational resources and time. The problem we are dealing with here is a classification problem. org As can bee seen above except for the diagnosis that is M malignant or B benign all other features are of type float64 and have 0 non null numbers. 5 Forest and tree methods As can be seen in the table above using only some of the mean features reduced in most of the cases both accuracy and cross validation scores. org stable modules grid_search. 3 Nearest Neighbors The nearest neighbors classifier finds predefined number of training samples closest in distance to the new point and predict the label from these. the accuracy 2 of the predictions is measured. 1 Using all mean values features Our aim is to construct a function y f X such that the value of y 1 or 0 will be determined once we input the values X into f. Other packages will be loaded as necessary. The test set will correspond to 20 of the total data test_size 0. html grid search 5. 3 Nearest Neighbors 4. For instance field 3 is Mean Radius field 13 is Radius SE field 23 is Worst Radius. org stable modules svm. We will select those features to use in the next section. 1 https seaborn. Look how in some cases reds and blues dots occupies different regions of the plots. ", "id": "rcfreitas/python-ml-breast-cancer-diagnostic-data-set", "size": "8298", "language": "python", "html_url": "https://www.kaggle.com/code/rcfreitas/python-ml-breast-cancer-diagnostic-data-set", "git_url": "https://www.kaggle.com/code/rcfreitas/python-ml-breast-cancer-diagnostic-data-set", "script": "sklearn.metrics cross_val_score sklearn.naive_bayes sklearn.tree KNeighborsClassifier DecisionTreeClassifier seaborn numpy LinearSVC SGDClassifier ExtraTreesClassifier sklearn.ensemble sklearn.model_selection RandomForestClassifier matplotlib.pyplot pandas NuSVC accuracy_score GridSearchCV sklearn.neighbors SVC sklearn.linear_model GaussianNB sklearn.svm train_test_split ", "entities": "(('we', '1 package'), 'use') (('radius_mean perimeter_mean area_mean concavity_mean', 'points_mean'), 'be') (('predictions', 'X_test'), 'find') (('IntroductionThe aim', 'best models'), '1') (('1 we', 'basic informations'), 'load') (('accuracy', 'cross validation'), 'estimate') (('that', 'ys'), 'construct') (('we', 'accuracy'), 'need') (('Naive Naive Bayes 3 algorithm', 'features'), 'Bayes') (('We', 'below entire column'), 'use') (('that', 'section'), 'apply') (('results', 'process'), 'be') (('code', 'below order'), 'be') (('that', 'diagnosis'), 'have') (('This', 'GridSearchCV 1 function'), 'do') (('1 mean standard error', '30 features'), 'severity') (('We', 'next section'), 'select') (('M benign', 'non 0 null numbers'), 'org') (('red dots', 'malignant diagnosis'), 'correspond') (('features', 'breast mass'), 'describe') (('we', 'features'), 'use') (('train data', 'X_train'), 'fit') (('tumor', 'cell features'), 'test') (('which', 'corresponding documentation'), 'have') (('we', 'complete features'), 'compare') (('blues dots', 'plots'), 'look') (('1 we', 'f.'), 'feature') (('we', 'Seaborn'), 'need') (('org wiki OverfittingNext we', 'all standard parameters'), 'use') (('which', 'only best models'), 'do') (('which', 'box plots'), 'use') (('parameters', 'classifier'), 'call') (('We', 'classifier hyper parameter different combinations'), 'search') (('Nearest neighbors 3 nearest classifier', 'these'), 'neighbor') (('2 Forest methods', 'more computational resources'), 'improve') (('extra column', 'data'), 'create') (('attribute 1 informations', 'UCI Machine Learning Repository'), 'be') (('1 we', '42 train'), 'use') (('accuracy', '2 predictions'), 'measure') (('cell nucleus', 'radius lengths'), 'compute') (('It', 'features'), 'be') (('procedure', 'cases'), 'be') (('tumors also how malignant cells', 'mean features'), 'see') (('we', 'web page'), 'use') (('parameters', 'estimators'), '5') (('htmlThe algorithms', 'only numerical values'), 'process') (('us', 'data'), 'dataset') (('test set', 'data total test_size'), 'correspond') (('only accuracy', 'other cases'), 'improve') (('we', 'values'), 'transform') (('we', 'machine learning how tools'), '3') (('Support Vector 2 Now we', 'Support three different Vector Machines 1 classifiers'), 'Machines') ", "extra": "['biopsy of the greater curvature', 'test', 'diagnosis', 'procedure']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "algorithm", "analyze", "apply", "area", "attribute", "basic", "best", "box", "breast", "build", "case", "cell", "center", "choose", "classification", "classifier", "clf", "code", "column", "compare", "contour", "correlations", "cost", "could", "count", "create", "data", "describe", "diagnosis", "dimension", "distance", "distribution", "end", "error", "estimator", "every", "field", "following", "form", "found", "function", "generated", "gray", "grid", "help", "http", "image", "input", "instance", "label", "largest", "learn", "learning", "list", "load", "local", "malignant", "map", "matrix", "mean", "ml", "model", "most", "nearest", "need", "new", "next", "non", "not", "notebook", "nuclei", "null", "number", "numerical", "order", "organize", "pair", "parameter", "performance", "plotting", "point", "predict", "print", "problem", "procedure", "range", "reason", "right", "run", "running", "sampling", "scale", "scatter", "scikit", "score", "search", "section", "select", "selected", "separate", "set", "split", "standard", "start", "table", "test", "those", "time", "total", "train", "training", "transform", "tree", "tumor", "tutorial", "type", "validation", "value", "variation", "visualize", "web", "work", "worst"], "potential_description_queries_len": 124, "potential_script_queries": ["numpy", "seaborn", "sklearn"], "potential_script_queries_len": 3, "potential_entities_queries": ["learning", "mean", "null", "parameter", "total"], "potential_entities_queries_len": 5, "potential_extra_queries": ["biopsy"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 128}