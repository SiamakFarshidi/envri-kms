{"name": "greatbarrierreef full guide to bboxaugmentation ", "full_name": " h1 Great Barrier Reef Image Bounding Box Augmentation h1 1 Introduction h3 Crown of Thorns Starfish h3 Libraries Below h3 Helper Functions Below h1 2 Dataset Understanding h2 2 1 train csv h3 I Length of Videos Sequences and Frames h3 II Target Variable annotations h2 2 2 train images h3 I Showing 1 Frame h3 II Show Multiple Consecutive Frames h3 III Comparison between No Annotated vs Annotated Images h1 3 Bounding Box Augmentation h2 3 1 Random Horizontal Flip h2 3 2 Random Scaling h2 3 3 Random Translate h2 3 4 Random Rotation h3 Function for Image Rotation h3 Functions for BBox Rotation h2 3 5 Random Shearing h3 Log Augmented Images to W B h1 4 Final Changes to Train datasets h2 What is the COCO format h3 W B Dashboard h3 My Specs ", "stargazers_count": 0, "forks_count": 0, "description": "This way we can help researchers scientists to control COTS outbreaks which are a threat to the Great Barrier Reef. For the frames that do have annotations most have between 1 and 3 annotations with a few outlier frames that have more than 10 unique coordinates bounding boxes identified within the image. Why is this a problem One or two Crown of Thorn starfish on a reef may be arguably beneficial for biological diversity as they keep down the growth of fast growing coral species and leave space for other slow growing corals. Some COTS can be seen with the naked eye however others are extremely hidden in the background. My question would be could we somehow distort enhance these images so we could better identify the presence of COTS within them We already know that all the images will have around the same tonal colors blue green yellow and around the same texture. It s like if you would look through a camera lence at a piece of paper on a table and then you would move it left right up or down leaving some parts of the table exposed and some areas or the paper not visible. x1 y1 x2 y2 the formated version we have created during the BBox Augmentation phase also called xmin ymin xmax ymax. org wiki Crown of thorns_starfish is a large starfish that preys upon hard or stony coral polyps. My Specs Z8 G4 Workstation 2 CPUs 96GB Memory NVIDIA Quadro RTX 8000 Zbook Studio G7 on the go Libraries Environment check Secrets Custom colors Set Style W B W B Experiment Read training dataset Plot 1 Plot 2 Log plots into W B Dashboard Calculate the number of total annotations within the frame annotations Plot Log info and plots into W B Dashboard List of unique sequence values Enumerate through all sequences W B Experiment Create a path column containing full path to the frames Show image and annotations if applicable This is in case we plot only 1 image W B Log Show only 1 image as example Log Image to W B Show only 1 image as example Log Image to W B Select image paths their annotations Plot Select image paths their annotations Plot No annotations 5 annotations 17 annotations Create a new column with the new formated annotations This is in case we plot only 1 image p probability of the image to be flipped set p 1 to always flip Convert bboxes If random number between 0 and 1 probability p Reverse image elements in the 1st dimension Convert the bounding boxes Take an example Horizontal Flip Show the Before and After Clips the bboxes scale must always be a positive number Maintain the aspect ratio scaling factor remains the same for width height Convert bboxes Chose a random digit to scale by Resize the image by scale factor The black image the remaining area after we have clipped the image Determine the size of the scaled image Adjust the bboxes remove all annotations that dissapeared after the scaling Scaling Show the Before and After Maintain the aspect ratio scaling factor remains the same for width height Convert bboxes Chose a random digit to scale by Percentage of the dimension of the image to translate Change the origin to the top left corner of the translated box Translate Show the Before and After Image Rotation Image dimensions Image Centre Rotation Matrix from cv2 Sine Cosine rotation components of the matrix NEW Bounding Dimensions of the image Adjust the rotation matrix to take into account translation Perform the Rotation Get Corners of Bounding Boxes Each bounding box is described by 8 coordinates x1 y1 x2 y2 x3 y3 x4 y4 Box Rotation corners x1 y1 x2 y2 x3 y3 x4 y4 Rotation Matrix from cv2 Sine Cosine rotation components of the matrix NEW Bounding Dimensions of the image Adjust the rotation matrix to take into account translation Prepare the vector to be transformed Get the Enclosing Box Notation where each bounding box is determined by 4 coordinates or two corners Convert bboxes Compute the random angle width height and center of the image Rotate the image Rotate the bounding boxes Get the 4 point corner coordinates Rotate the bounding box Get the enclosing new bboxes Get scaling factors to clip the image and bboxes Rescale the image to w h and not nW nH Clip boxes in case there are any outside of the rotated image Translate Show the Before and After Convert bboxes Get the shear factor and size of the image Flip the image and boxes horizontally Apply the shear transformation Transform using cv2 warpAffine like in rotation Flip the image back again Resize Translate Show the Before and After W B Log redone for formated annotations Log all augmented images to the Dashboard Create sepparate paths for images and their labels annotations these will come in handy later for the YOLO model Save the width and height of the images it is the same for the entire dataset Simplify the annotation format Data Sample Save dataset Save dataset Artifact. As in the case of scaling the bounding boxes which have an area of less than 25 in the remaining in the transformed image is dropped. However this bounding box can have multiple ways of being displayed as there is no wrong way to locate a rectangle within an image x y width height this is the case in our training dataset also called the COCO format. 1 video is split into 4 sequences while the other 2 videos are split into 8 sequences each. Crown of Thorns Starfish What is this creature The crown of thorns starfish https en. com data augmentation bounding boxes scaling translation Let s see an example of the original image and then the scaled one. Final Changes to Train datasetsThis is the part where we create the last helper features for our dataset. Comparison between No Annotated vs Annotated ImagesI wanted to look at multiple random images frames and see if they look significantly different. The image below is a case that has the most annotations a frame can have 18 bounding boxes in total. W B Dashboard My W B Dashboard is here https wandb. x_center y_center are the normalized coordinates of the center of the bounding box and width height are the normalized width and height of the image. Note Most of my inspiration and research is from here https blog. Target Variable annotations We can compute the total number of annotations per frame or. jpg images out of all. 1 Random Horizontal FlipCreates a class that randomly flips the image and the bounding box with it. 2 train_images The train_images folder is structured as follows I. jpg images within the train_images folder. org crown of thorns starfish. I also wanted to look at all sequences and see how the annotations distribute through time. In this case the bounding boxes which have an area of less than 25 in the remaining in the transformed image is dropped. com data augmentation for object detection rotation and shearing Let s see an example of the original image and then the sheared one. These sequences don t seem to have an apparition pattern either so I tend to believe that the COTS appear as sporradic as possible within the videos which is very good we want to mimic a natural setting as much as possible. Dataset Understanding 2. 3 Random TranslateWhen we translate the image we move it around on the canvas. com bounding box formats for models like yolo ssd rcnn fast rcnn faster rcnn 807be7721527. ai andrada GreatBarrierReef workspace workspace user andrada. com data augmentation for object detection rotation and shearing TODO needs work bboxes don t rotate with the image Function for Image Rotation Functions for BBox Rotation Let s see an example of the original image and then the rotated one. Introduction Competition Goal accurately identify starfish COTS coral eating Crown Of Thorns Starfish in real time by building an object detection model trained on underwater videos of coral reefs. Sequences 53708 8503 60754 22643 and 8399 these have lots of annotations throughout the entire sequence with no particular pattern of apparition what I mean by this is that the annotations don t seem to usually appear either at the beginning middle nor end of the sequence. The resolution is maintained and the remaining area if any is filled by black color. Log Augmented Images to W BLet s now log the a sample of each augmentation to the W B Dashboard. However as the starfish population multiplies or the starfish begin eating coral tissue faster than it can grow back a devastating Crown of Thorn COTS outbreak can occur. This format is used within the SSD RCNN Fast RCNN Faster RCNN models https lohithmunakala. jpg images numerotated in the order that they appear within the video from 1 to n. What is the COCO format As we have seen an Object Detection model locates an object within an image using a bounding box. Showing 1 FrameBefore doing anything let s explore the frames and how do they look like. Additionaly it has an annotations columns which can be empty or could contain 1 or multiple coordinates for the location or a bounding box of the COTS. Hence the top left corner of an image has the coordinates 0 0 while the bottom right corner has the coordinates width_max height_max. csv dataset contains 5 columns that help identify the position within the video and sequence of the. Libraries Below Helper Functions Below 2. x_center y_center width height this is the YOLO format or rather the format used when training using the YOLO model. Each sequence has an unique ID and has various numbers of frames raging from 71 frames per sequence all the way to 3 000 frames per sequence. This is an example of a naked image there are no annotations found meaning that there are no COTS present. All other sequences for the remainer of sequences most have a few or close to no annotation within them. com data augmentation for bounding boxes Let s see an example of the original image and then the flipped one. 5 Random ShearingFinally shearing is when the image is shifted like it is dragged from one corner and opposite to the other so the image ends up looking sort of like a parallelogram. 4 Random RotationRotation is when you guessed you rotate the image a random number degrees and it might be the hardest one to deal with when trying to accomodate bounding boxes with it. At the polar opposite these images show the presence of multiple COTS within them. The distribution of annotations is extremely skewed with most of the frames having no annotation at all. Show Multiple Consecutive FramesNow let s look at multiple consecutive frames within a few sequences. com data augmentation bounding boxes scaling translation Let s see an example of the original image and then the translated one. It is not known exactly what causes a COTS outbreaks however scientists agree it could have something to do with increased levels of nutrients in the water due to agriculture runoff or warming oceans leading to a plankton bloom which is a necessary food source for starfish larvae source here https oceangardener. Great Barrier Reef Image Bounding Box Augmentation 1. Notice that in the first 3 frames only 1 COTS is annotated however the second COTS one is also visible but NOT identified. jpg image a picture caught in time within the video. It is one of the largest starfish in the world. 2 Random ScalingWhen we scale the image we descrease it s original size. COCO comes from Common Objects in Context which is a database that aims to support and improve models for Object Detection Instance Segmentation and Image Captioning. These frames however have 1 and 2 COTS identified. However they are not extremely imbalanced with enough frame numbers fro each of the 3 videos. Sequences 44160 29424 37114 these don t have ANY annotation appear in any of the frame meaning that no COTS has been identified and tagged within these images. Hence we can visualize the number of annotations per frame through time to see if these have irregularities between sequences or if they have some kind of systematic appearance. jpg image by counting how many coordinates can be found within a frame. Crown of Thorns are among some of the larges starfish species generally 25 35cm 10 14inch in diameter and can grow to a size of 80cm 31inch this makes them easy to spot on a reef source here https oceangardener. Again a frame is actually a. Note Keep in mind that cv2 works with BGR images so in order to view the original image within RGB we need to convert using cv2. It receives its name from the venomous thorn like spines that cover its upper surface resembling the biblical crown of thorns. Each video is split into sequences. We know that each sequence has the frames. Can an AI spot them A COTS outbreak can have devastating impacts to an entire coral reef and depending on the event the ravenous starfish could wipe out nearly all living corals. Before we do that we will need to format the annotations we have now from this x 628 y 321 width 42 height 47 to this x1 628 y1 321 x2 670 y2 368 628 321 670 368 In order to do so we just need to compute as follows x1 x y1 y x2 x width y2 y height Note we are adding and not substracting to y2 because we aren t using a coordinate system although x and y are coordinates but an image so the coordinates are actually pixels on the surface. Example first bbox bigger one x 520 y 151 width 78 height 62 second bbox smaller one x 598 y 204 width 58 height 32 One last thing I would like to do is create a new function called show_image_bbox that receives the new formated annotations x1 y1 x2 y2 and displays the new augmented image. The frames below have no COTS identified within them. Bounding Box AugmentationIn this part I wanted to explore some ways to do Image Augmentation and adjust the annotations aka bounding boxes to match all sorts of augmentations applied on the image. Length of Videos Sequences and Frames There are 3 total videos with the last one having the most frames. This COTS is identified and annotated only starting the 4th frame onwards. ", "id": "andradaolteanu/greatbarrierreef-full-guide-to-bboxaugmentation", "size": "12421", "language": "python", "html_url": "https://www.kaggle.com/code/andradaolteanu/greatbarrierreef-full-guide-to-bboxaugmentation", "git_url": "https://www.kaggle.com/code/andradaolteanu/greatbarrierreef-full-guide-to-bboxaugmentation", "script": "show_values_on_bars __init__ RandomScale(object) plot_comparison create_wandb_hist show_image_bbox UserSecretsClient show_multiple_images display_html matplotlib.patches _show_on_single_plot seaborn numpy RandomRotate(object) RandomShear(object) rotate_im color rotate_box save_dataset_artifact IPython.display wandb_bboxes matplotlib.pyplot get_corners pandas RandomHorizontalFlip(object) tqdm format_annotations RandomTranslate(object) get_enclosing_box show_image matplotlib create_wandb_plot bbox_area __call__ clip_box kaggle_secrets wandb_annotation ", "entities": "(('train_images 2 folder', 'I.'), 'train_image') (('we', 'cv2'), 'keep') (('that', 'thorns'), 'receive') (('however others', 'extremely background'), 'see') (('already images', 'blue green same texture'), 'be') (('x1 y1 x2 formated we', 'BBox Augmentation phase'), 'y2') (('that', 'x1 y1 x2 new augmented image'), 'bbox') (('areas', 'table'), 's') (('how annotations', 'time'), 'want') (('image', 'sort of parallelogram'), 'be') (('they', 'systematic appearance'), 'visualize') (('COTS', 'only 4th frame'), 'identify') (('which', 'transformed image'), 'drop') (('other sequences', 'them'), 'have') (('sequence', 'frames'), 'know') (('they', 'n.'), 'image') (('This', 'naked image'), 'be') (('annotations don t', 'sequence'), 'sequence') (('jpg', 'video'), 'image') (('part I', 'image'), 'bound') (('Random Horizontal 1 that', 'bounding it'), 'FlipCreates') (('very we', 'natural setting'), 'seem') (('width this', 'YOLO YOLO rather when model'), 'x_center') (('that', 'image'), 'have') (('it', 'it'), 'be') (('frames', 'them'), 'have') (('COTS however second one', 'first 3 frames'), 'notice') (('it', 'image'), 's') (('height', 'normalized image'), 'x_center') (('faster it', 'Thorn COTS outbreak'), 'occur') (('However they', '3 videos'), 'imbalanced') (('It', 'world'), 'be') (('how they', 'frames'), 'show') (('Data Save Save', 'Artifact'), 'NVIDIA') (('frame', 'total'), 'be') (('Most', 'https blog'), 'note') (('we', 'around canvas'), 'TranslateWhen') (('0 0 bottom right corner', 'coordinates'), 'left') (('ravenous starfish', 'nearly all living corals'), 'spot') (('coordinates', 'actually surface'), 'do') (('they', 'other slow growing corals'), 'be') (('other 2 videos', '8 sequences'), 'split') (('Length', 'most frames'), 'be') (('they', 'images multiple random frames'), 'want') (('that', 'the'), 'contain') (('creature', 'thorns'), 'Starfish') (('s', 'few sequences'), 'show') (('jpg how many coordinates', 'frame'), 'image') (('s', 'original image'), 'need') (('images', 'them'), 'show') (('them', 'reef source'), 'be') (('Object Detection model', 'bounding box'), 'be') (('which', 'Great Barrier Reef'), 'help') (('which', 'bounding COTS'), 'have') (('Target Variable We', 'frame'), 'annotation') (('which', 'food starfish larvae necessary source'), 'know') (('COTS', 'images'), 'sequence') (('where we', 'helper last dataset'), 'Changes') (('remaining any', 'black color'), 'maintain') (('distribution', 'annotation'), 'be') (('large that', 'hard coral polyps'), 'be') (('format', 'SSD'), 'use') (('that', 'Object Detection Instance Segmentation'), 'come') (('Introduction Competition Goal', 'coral reefs'), 'identify') (('s', 'original image'), 'augmentation') (('y width this', 'training dataset'), 'have') (('s', 'original image'), 'let') (('sequence', 'sequence'), 'have') ", "extra": "['annotation', 'biopsy of the greater curvature']", "label": "Perfect_files", "potential_description_queries": ["account", "adjust", "annotation", "appear", "area", "augmentation", "bbox", "bottom", "bounding", "box", "camera", "case", "center", "check", "clip", "close", "column", "compute", "contain", "control", "convert", "could", "create", "csv", "cv2", "data", "database", "dataset", "detection", "digit", "dimension", "distribution", "diversity", "empty", "end", "enhance", "event", "explore", "exposed", "eye", "factor", "faster", "flip", "folder", "format", "found", "frame", "function", "green", "grow", "growth", "height", "help", "helper", "image", "improve", "info", "largest", "leave", "left", "let", "log", "look", "looking", "match", "matrix", "mean", "meaning", "middle", "might", "mind", "model", "most", "move", "multiple", "my", "name", "need", "new", "no", "normalized", "not", "number", "object", "order", "out", "outbreak", "outlier", "part", "path", "pattern", "per", "picture", "plot", "point", "polar", "population", "position", "positive", "probability", "problem", "question", "random", "ratio", "rcnn", "remove", "research", "resolution", "right", "rotate", "sample", "scale", "scaled", "scaling", "second", "sequence", "set", "shear", "size", "something", "sort", "source", "space", "split", "support", "surface", "system", "table", "through", "time", "tissue", "total", "train_images", "training", "transformation", "translate", "unique", "up", "upper", "user", "vector", "version", "video", "view", "visualize", "while", "width", "work", "yolo"], "potential_description_queries_len": 149, "potential_script_queries": ["color", "matplotlib", "numpy", "seaborn", "tqdm"], "potential_script_queries_len": 5, "potential_entities_queries": ["green", "new", "right", "width"], "potential_entities_queries_len": 4, "potential_extra_queries": ["biopsy of the greater curvature", "biopsy", "curvature"], "potential_extra_queries_len": 3, "all_components_potential_queries_len": 157}