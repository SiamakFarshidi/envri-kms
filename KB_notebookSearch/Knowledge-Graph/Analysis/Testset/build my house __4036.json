{"name": "build my house ", "full_name": " h3 Import Libraries h1 Data Visualization on training test set h1 Feature Engineering h2 Onehot encoding on categorical data h1 Linear Regression h3 Train Test Split h3 Creating and Training the Model h2 confusion Recall Precision matrix function for performance checking h3 Model Evaluation h3 Predictions from our Model h3 Regression Evaluation Metrics h1 Gradient Boosting Regression h1 Decision Tree Regression h1 Support Vector Machine Regression h1 Random Forest Regression h1 Model Comparison ", "stargazers_count": 0, "forks_count": 0, "description": "MSE is more popular than MAE because MSE punishes larger errors which tends to be useful in the real world. A logical understanding of loss function would depend on what we are trying to optimise. read_csv database connection Categorical boolean mask filter categorical columns using mask and turn it into alist i in range len categorical_cols data i data i. The topmost node in a tree is the root node. We will train out model on the training set and then use the test set to evaluate the model. We will toss out the Address column because it only has text info that the linear regression model can t use. sum axix 0 4 6 C C. Predictions from our Model Let s grab predictions off our test set and see how well it did Regression Evaluation Metrics Here are three common evaluation metrics for regression problems Mean Absolute Error MAE is the mean of the absolute value of the errors 1\ud835\udc5b \ud835\udc56 1\ud835\udc5b \ud835\udc66\ud835\udc56 \ud835\udc66 \ud835\udc56 Mean Squared Error MSE is the mean of the squared errors 1\ud835\udc5b \ud835\udc56 1\ud835\udc5b \ud835\udc66\ud835\udc56 \ud835\udc66 \ud835\udc56 2 Root Mean Squared Error RMSE is the square root of the mean of the squared errors 1\ud835\udc5b \ud835\udc56 1\ud835\udc5b \ud835\udc66\ud835\udc56 \ud835\udc66 \ud835\udc56 Comparing these metrics MAE is the easiest to understand because it s the average error. T 1 3 2 4 C. Decision Tree Regression The decision tree is a simple machine learning model for getting started with regression tasks. Not suited for large dataset because of it complexity Support Vector Machine Regression Random Forest Regression Model Comparison We can say the best working model by loking MSE rates The best working model is Support Vector Machine. Data Visualization on training test set Heatmap for train set Pair plot CDF And Pdf for yearbuilt feature LMPLOT for yearbuilt Box plot for GarageCars feature Feature EngineeringWe have to convert all columns into numeric or categorical data. Train Test Split Now let s split the data into a training set and a testing set. sum axix 1 3 7 C. The loss function is a measure indicating how good are model s coefficients are at fitting the underlying data. which one is better Now we will use test data. sum axis 0 1 4 2 6 3 4 4 6 representing A in heatmap format representing B in heatmap format print the intercept prepare models making dataframe. prediction with SVM Model linear algebra data processing CSV file I O e. RMSE is even more popular than MSE because RMSE is interpretable in the y units. sum axis 1 axis 0 corresonds to columns and axis 1 corresponds to rows in two diamensional array C. Creating and Training the Model confusion Recall Precision matrix function for performance checking Model Evaluation Let s evaluate the model by checking out it s coefficients and how we can interpret them. Filling NULL values Onehot encoding on categorical data Linear Regression Let s now begin to train out regression model We will need to first split up our data into an X array that contains the features to train on and a y array with the target variable in this case the Price column. Background A decision tree is a flow chart like structure where each internal non leaf node denotes a test on an attribute each branch represents the outcome of a test and each leaf or terminal node holds a class label. fillna we are going to scale to data This function plots the confusion matrices given y_i y_i_hat. sum axis 1 1 3 3 7 2 3 4 7 C. see here for more details. And We are going to drop SalePrice column for predict. Let s get started Building my first Kernel Hope you like it Import LibrariesWhat is the data trying to say to us We are going to convert train ad test data. While the AdaBoost model identifies the shortcomings by using high weight data points gradient boosting performs the same by using gradients in the loss function y ax b e e needs a special mention as it is the error term. C 9 9 matrix each cell i j represents number of points of class i are predicted class j divid each element of the confusion matrix with the sum of elements in that column C 1 2 3 4 C. We are going to see the error rate. We are trying to predict the sales prices by using a regression then the loss function would be based off the error between true and predicted house prices. sum axis 0 axis 0 corresonds to columns and axis 1 corresponds to rows in two diamensional array C. All of these are loss functions because we want to minimize them. Gradient Boosting Regression Gradient Boosting trains many models in a gradual additive and sequential manner. T 1 3 2 3 3 7 4 7 sum of row elements 1 divid each element of the confusion matrix with the sum of elements in that row C 1 2 3 4 C. The major difference between AdaBoost and Gradient Boosting Algorithm is how the two algorithms identify the shortcomings of weak learners eg. ", "id": "jacckashakash/build-my-house", "size": "4036", "language": "python", "html_url": "https://www.kaggle.com/code/jacckashakash/build-my-house", "git_url": "https://www.kaggle.com/code/jacckashakash/build-my-house", "script": "sklearn.metrics sklearn.utils cross_val_score sklearn.naive_bayes stopwords sklearn.tree DecisionTreeRegressor create_engine # database connection StackingClassifier SVR TfidfVectorizer KNeighborsClassifier scipy.sparse normalized_mutual_info_score defaultdict shuffle r2_score TSNE collections mean_squared_error seaborn numpy sklearn.feature_extraction.text SGDClassifier auc MultinomialNB plot_confusion_matrix RandomForestRegressor sklearn.decomposition OneVsRestClassifier sklearn.ensemble confusion_matrix CountVectorizer sklearn.model_selection sqlalchemy RandomForestClassifier sklearn.calibration sklearn metrics matplotlib.pyplot CalibratedClassifierCV sklearn.manifold pandas hstack StandardScaler LogisticRegression accuracy_score Counter mlxtend.classifier TruncatedSVD normalize log_loss precision_recall_curve nltk.corpus GridSearchCV sklearn.neighbors SVC sklearn.metrics.classification sklearn.preprocessing GaussianNB sklearn.svm sklearn.linear_model model_selection roc_curve sklearn.multiclass train_test_split LinearRegression ensemble datetime ", "entities": "(('Gradient Boosting Regression Gradient Boosting', 'gradual additive'), 'train') (('working best model', 'MSE rates'), 'suited') (('which', 'real world'), 'be') (('it', 'squared errors'), 'let') (('Now we', 'test data'), 'use') (('prediction', 'SVM Model linear algebra data CSV file'), 'process') (('sum', 'two diamensional array'), 'corresond') (('B', 'dataframe'), 'axis') (('that', 'Price column'), 'value') (('linear regression model', 'that'), 'toss') (('loss then function', 'house prices'), 'base') (('Data Visualization', 'numeric categorical data'), 'set') (('sum', 'two diamensional array'), 'axis') (('how we', 'them'), 'let') (('how s coefficients', 'underlying data'), 'be') (('class j', 'column'), 'matrix') (('We', 'predict'), 'go') (('topmost node', 'tree'), 'be') (('RMSE', 'y units'), 'be') (('function', 'y_i y_i_hat'), 'go') (('i', 'range len categorical_cols data'), 'connection') (('loss we', 'them'), 'be') (('branch', 'terminal class label'), 'be') (('Now s', 'training set'), 'let') (('we', 'what'), 'depend') (('Gradient Boosting how two algorithms', 'learners weak eg'), 'be') (('We', 'train ad test data'), 'let') (('it', 'special mention'), 'perform') (('We', 'test then model'), 'train') (('Decision Tree decision tree', 'machine learning regression simple tasks'), 'regression') ", "extra": "['outcome', 'test']", "label": "Perfect_files", "potential_description_queries": ["absolute", "array", "attribute", "average", "best", "boolean", "boosting", "branch", "case", "categorical", "cell", "chart", "checking", "column", "confusion", "connection", "convert", "data", "database", "dataset", "decision", "depend", "difference", "drop", "encoding", "error", "evaluate", "evaluation", "even", "feature", "file", "filter", "fitting", "flow", "format", "function", "grab", "gradient", "heatmap", "high", "house", "info", "leaf", "learning", "len", "let", "linear", "major", "mask", "matrix", "mean", "measure", "mention", "minimize", "model", "my", "need", "node", "non", "number", "numeric", "out", "outcome", "performance", "plot", "predict", "prediction", "prepare", "print", "processing", "range", "regression", "row", "scale", "set", "special", "split", "square", "squared", "structure", "sum", "target", "test", "testing", "text", "train", "training", "tree", "turn", "understanding", "up", "value", "variable", "weight"], "potential_description_queries_len": 94, "potential_script_queries": ["auc", "database", "datetime", "defaultdict", "ensemble", "normalize", "numpy", "seaborn", "shuffle", "sklearn"], "potential_script_queries_len": 10, "potential_entities_queries": ["categorical", "decision", "len", "regression"], "potential_entities_queries_len": 4, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 103}