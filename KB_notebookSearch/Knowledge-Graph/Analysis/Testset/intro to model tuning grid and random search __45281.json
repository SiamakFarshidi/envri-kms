{"name": "intro to model tuning grid and random search ", "full_name": " h1 Introduction Hyperparameter Tuning using Grid and Random Search h2 Model Gradient Boosting Machine h3 Getting Started h1 Cross Validation h2 Early Stopping h3 Example of Cross Validation and Early Stopping h2 Hyperparameter Tuning Implementation h1 Four parts of Hyperparameter tuning h2 Objective Function h1 Domain h2 Hyperparameters for GBM h3 Learning Rate Domain h1 Algorithm for selecting next values h1 Results History h1 Grid Search Implementation h4 Application h1 Random Search h3 Stacking Random and Grid Search h2 Next Steps h2 Writing to File to Monitor Progress h3 Extremely Important Note about Checking Files h1 Results on Limited Data h1 Visualizations h2 Distribution of Search Values h2 Sequence of Search Values h2 Score versus Hyperparameters h1 Testing Results on Full Data h2 Model Tuning Next Steps h1 Conclusions ", "stargazers_count": 0, "forks_count": 0, "description": "html rather than allocating a list of all possible combinations which would be far too large to hold in memory. These methods including Bayesian optimization https sigopt. Distribution of Search ValuesWe can show the distribution of search values for random search grid search is very uninteresting. Sequence of Search ValuesFinally we can plot the sequence of search values against the iteration for random search. As an example below we plot the distribution of learning rates from both the sampling distribution and the random search results. Think about going from 1 to 10 and then from 10 to 100. Then we can look at the file to track progress while the searching is running and eventually have the entire results saved when the search is complete. com willkoehrsen automated model tuning There are several approaches to hyperparameter tuning1. The code below shows the algorithm for grid search. com willkoehrsen introduction to manual feature engineering Manual Feature Engineering Part Two https www. com williamkoehrsen and can be reached on Twitter at https twitter. com koehrsen_willWill Data manipulation Modeling Splitting data Sample 16000 rows 10000 for training 6000 for testing Only numeric features Extract the labels Split into training and testing data Create a training and testing dataset Get default hyperparameters Remove the number of estimators because we set this to 10000 in the cv call Cross validation with early stopping Optimal number of esimators found in cv Train and make predicions with model Number of estimators will be found using early stopping Perform n_folds cross validation results to retun Create a default model Hyperparameter grid Randomly sample a boosting type Set subsample depending on boosting type Learning rate histogram Check number of values in each category Check values number of leaves domain Dataframes for random and grid search Dataframe to store results https codereview. Some of these we do not need to tune such as silent objective random_state and n_jobs and we will use early stopping to determine perhaps the most important hyperparameter the number of individual learners trained n_estimators also referred to as num_boost_rounds or the number of iterations. Later when we get to Bayesian Optimization we will have to use a value to minimize so we can take 1 text ROC AUC as the score. We can use this result as a baseline model to beat. com questions 171173 list all possible permutations from a python dictionary of lists Iterate through every possible combination of hyperparameters Select the hyperparameters Set the subsample ratio accounting for boosting type Evalute the hyperparameters open connection append option and write results make sure to close connection Normally would not limit iterations Sort with best score on top MAX_EVALS 1000 out_file grid_search_trials_1000. Use random search with a large hyperparameter grid 2. We can print information to the command prompt but this will grow cluttered after 1000 iterations and the results will be gone if we close the command prompt. org wiki Early_stopping. Even though we expect these to be _random_ it s always a good idea to check our code both quantitatively and visually. html that control both the overall ensemble such as the learning rate and the individual decision trees such as the number of leaves in the tree or the maximum depth of the tree. Using a logarithmic uniform distribution allows us to create a domain where there are as many values from 0. 792 so we can conclude that the results from random search on the smaller dataset to not translate to a full dataset. com willkoehrsen automated feature engineering basics Advanced Automated Feature Engineering https www. Thanks for reading and I ll see you in the next notebook As always I welcome feedback and constructive criticism. To actually test our methods from this notebook we would need to train the best model on all of the training data make predictions on the actual testing data and then submit our answers to the competition. Choosing a hyperparameter grid is probably the most difficult part of hyperparameter tuning it s nearly impossible ahead of time to say which values of hyperparameters will work well and the optimal settings will depend on the dataset. If we have a limited time to evaluate hyperparameters random search is a better option than grid search for exactly this reason. 0 which means use all the rows if boosting_type goss. com jsaguiar and olivier https www. Instead you can view the end of the file by typing tail out_file. This great paper explains why this is so http www. The weights learned during training of a linear regression model are parameters while the number of trees in a random forest is a model hyperparameter because this is set by the data scientist. The random search method does a very good job of exploring the search space as we will see when we look at the hyperparameter values searched. We can only use the testing data __a single time__ when we evaluate the final model that has been tuned on the validation data. If that s a little confusing perhaps the graph above makes it clearer. The grid search results are completely uninteresting. Results on Limited DataWe can examine 1000 search iterations of the above functions on the reduced dataset. Normally in grid search we do not limit the number of evaluations. pdf is fascinating so stay tuned for this upcoming notebook. linspace start stop num. com willkoehrsen introduction to manual feature engineering p2 Introduction to Automated Feature Engineering https www. Now let s make a function to parse the results from the hyperparameter searches. This indicates that whatever hyperparameters are changing in grid search are gradually increasing the score. Hence the need for hyperparameter tuning the only way to find the optimal hyperparameter values is to try many different combinations on a dataset We will use the implementation of the Gradient Boosting Machine in the LightGBM library http lightgbm. logspace to define the range of values for each hyperparameter. Again this is something we would not want to do on a real problem but for demonstration purposes it will allow us to see the concepts in practice rather than waiting days months for the search to finish. Hyperparameters can be thought of as model settings. com jsaguiar updated 0 792 lb lightgbm with simple features. In the case of grid search we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. If you re still confused this article https machinelearningmastery. com difference between a parameter and a hyperparameter may help you out __Additional Notebooks__ If you haven t checked out my other work on this problem here is a complete list of the notebooks I have completed so far A Gentle Introduction https www. ApplicationIf you want to run this on the entire dataset feel free to take these functions and put them in a script. The vertical dashed line indicates the optimal value found from random search. You can also refer to the LightGBM documentation http lightgbm. The public leaderboard score is only calculated on 10 of the test data so the cross validation score might actually give us a better idea of how the model will perform on the full test set. __Manual__ select hyperparameters based on intuition experience guessing train the model with the hyperparameters and score on the validation data. To test the tuning results we will save some of the training data 6000 rows as a separate testing set. As an example below if we randomly select a set of hyperparameters and the boosting type is goss then we set the subsample to 1. Below we read in the data and separate into a training set of 10000 observations and a testing set of 6000 observations. We do this by opening a connection this time using the a option for append the first time we used the w option for write and writing a line with the desired information which in this case is the cross validation score the hyperparameters and the number of the iteration. The key line is for v in itertools. com willkoehrsen introduction to feature selection Intro to Model Tuning Grid and Random Search https www. Algorithm method for selecting the next set of hyperparameters to evaluate in the objective function. Results history of hyperparameters and cross validation scoresThese four parts apply to grid and random search as well as to Bayesian optimization a form of automated hyperparameter tuning. The GBM is extremely effective on structured data where the information is in rows and columns and medium sized datasets where there are at most a few million observations. This will cause a permission error in Python and the search will be terminated. The results are likely to be different because we were only using a random subset of the training data. Use the results of random search to build a focused hyperparameter grid around the best performing hyperparameter values. Random and grid search are two decent methods to start tuning a model at least they are better than manual tuning and are important tools to have in the data science skillset. The cv_results is a dictionary with lists for the metric mean and the metric standard deviation. Next StepsWe can now take these random and grid search functions and use them on the complete dataset or any dataset of our choosing. io visualization library to make some plots First we need to put our data into a long format dataframe. However the same ideas that we will implement here can be applied to the full dataset and while this notebook is specifically aimed at the GBM the methods can be applied for any machine learning model. If we want to observe the effects of one hyperparameter on the cross validation score we could alter only that hyperparameter while holding all the others constant. However if we don t have much experience we can simply define a large search space and hope that the best values are in there somewhere. Learning Rate DomainThe learning rate domain is from 0. Results HistoryThe results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function. logspace start stop num 100 base 10. This returns a dataframe where each column is a hyperparameter and each row has one search result so taking the dictionary of hyperparameters and mapping it into a row in a dataframe. org doc numpy reference generated numpy. When we get to Bayesian Optimization the model actually _uses the past results to decide on the next hyperparmeters_ to evaluate. For this notebook we will work with a subset of the data consisting of 10000 rows. org packing and unpacking arguments in python in the hyperparameter grid which is a Python dictionary using the line keys values zip param_grid. In addition to preserving training data this should give us a better estimate of generalization performance on the test set than using a single validation set since then we are probably overfitting to that validation set. This is the baseline score _before hyperparameter tuning_. __Quick Note__ a lot of data scientists use the terms _parameters_ and _hyperparameters_ interchangeably to refer to the model settings. First we unpack the values https www. Early stopping is simple to implement with the LightGBM library in the cross validation function. The following code repeats this plot for all the of the numeric hyperparameters. Moreover the hyperparameters have complex interactions with each other which means that just tuning one at a time doesn t work because when we start changing other hyperparameters that will affect the one we just tuned If we have prior experience with a model we might know where the best values for the hyperparameters typically lie or what a good search space is. In this example we will use 5 fold cross validation which means training and testing the model with each set of hyperparameter values 5 times to assess performance. As this article https medium. The code below carries out both cross validation with 5 folds and early stopping with 100 early stopping rounds. In a future notebook we will implement automated hyperparameter tuning using Bayesian optimization specifically the Hyperopt library. com difference between a parameter and a hyperparameter to model __parameters__ which are learned during training model __hyperparameters__ are set by the data scientist ahead of training and control implementation aspects of the model. One aspect to note is that if boosting_type is goss then we cannot use subsample which refers to training on only a fraction of the rows in the training data a technique known as stochastic gradient boosting https en. According to the the docs https docs. Random search turns out to work pretty well in practice because it is good at exploring the search domain but it still is not a reasoning method because it does not use past evaluation results to choose the next hyperparameter values. Then if I see that some values of hyperparameters tend to work better I can concentrate the search around those values. __Grid Search__ set up a grid of hyperparameter values and for each combination train a model and score on the validation data. Results history data structure containing each set of hyperparameters and the resulting score from the objective function. com watch v TIgfjmp 4BA. close random_results random_search param_grid out_file Convert strings to dictionaries Sort with best values on top Print out cross validation high score Use best hyperparameters to create a model Train and make predictions Create dataframe of hyperparameters Iterate through each set of hyperparameters that were evaluated Put the iteration and score in the hyperparameter dataframe Combine results into one dataframe Plot of scores over the course of searching Create bar chart Add text for labels Display Bar plots of boosting type Density plots of the learning rate distributions Iterate through each hyperparameter Plot the random search distribution and the sampling distribution Plot of four hyperparameters Scatterplot Scatterplot of next four hyperparameters Plot of four hyperparameters Scatterplot Scatterplot of next four hyperparameters Read in full dataset Extract the test ids and train labels Cross validation with n_folds and early stopping Train the model with the optimal number of estimators from early stopping Predictions on the test data. The concept of early stopping is commonly applied to the GBM and to deep neural networks so it s a great technique to understand. 792 LB LightGBM with Simple Features https www. If we could plot this in higher dimensions it might be interesting to see if there are more promising regions of the search space but here we are limited to one dimension a single hyperparameter versus the score. com automated machine learning hyperparameter tuning in python dfda59b72f8a using Bayesian optimization. Four parts of Hyperparameter tuningIt s helpful to think of hyperparameter tuning as having four parts these four parts also will form the basis of Bayesian Optimization 1. While this is technically incorrect it s pretty common practice and it s usually possible to tell when they are referring to parameters learned during training versus hyperparameters. To implement KFold cross validation we will use the LightGBM cross validation function cv because this allows us to use a critical technique for training a GBM early stopping. I write for Towards Data Science at https medium. The performance of each set of hyperparameters is determined by Receiver Operating Characteristic Area Under the Curve ROC AUC from the cross validation. Model Gradient Boosting Machine The Gradient Boosting Machine GBM https machinelearningmastery. A better solution although not perfect is to write a line to a csv comma separated value file on each iteration. In a later notebook upcoming we will look at methods that use the past evaluation results to pick the next hyperparameter values to try in the objective function. The boosting type should be evenly distributed for random search. On a logarithmic scale these intervals are the same size but on a linear scale the latter is 10 times the size of the former. Domain of hyperparameters values over which we want to search3. I ll see you in the next notebook ConclusionsModel tuning is the process of finding the best machine learning model hyperparameters for a particular problem. The code below implements a simple objective function which we can use for both grid and random search. This is grid search trying every single value in the grid No matter how small the increment between subsequent values of a hyperparameter it will try them all. Random and grid search are _uninformed_ methods that do not use the past history but we still need the history so we can find out which hyperparameters worked the best A dataframe is a useful data structure to hold the results. I ll try to stick to using model hyperparameters or model settings and I ll point out when I m talking about a parameter that is learned during training. In other words a logarithmic uniform distribution lets us sample more evenly from a domain that varies over several orders of magnitude. Mostly these plots are for my own interest to see if there are any trends We want to avoid placing too much emphasis on any of these relationships because we were not changing one hyperparameter at a time although we could carry out experiments where we only change one hyperparameter and observes the effects on the score and so the trends are not due solely to the single hyperparameter we show. __Automated Hyperparameter Tuning__ use methods such as gradient descent Bayesian Optimization or evolutionary algorithms to conduct a guided search for the best hyperparameters. However I would advise against using grid search unless you have a very small hyperparameter grid because this is such as exhaustive method Later we will look at results from 1000 iterations of grid and random search run on the same small subset of data as we used above. We will implement automated optimization of machine learning hyperparameters step by step using the Hyperopt open source Python library. We will implement these algorithms very shortly as soon as we cover the final part of hyperparameter tuning. These results will let us go back and inspect what occurred during a search. Algorithm method for selecting the next hyperparameter values to evaluate in the objective function4. In this notebook we implemented both random and grid search on a reduced dataset inspected the results and tried to translate the optimal hyperparameters to a full dataset from this kernel https www. writerow headers of_connection. com automated machine learning hyperparameter tuning in python dfda59b72f8a. The basic strategy for both grid and random search is simple for each hyperparameter value combination evaluate the cross validation score and record the results along with the hyperparameters. Until Kaggle upgrades the kernels to quantum computers we are not going to be able to run evan a fraction of the combinations Let s assume 100 seconds per evaluation and see how many years this would take I think we re going to need a better approach Before we discuss alternatives let s walk through how we would actually use this grid and evaluate all the hyperparameters. This creates the csv file opens a connection writes the header column names and then closes the connection. Objective FunctionThe objective function takes in hyperparameters and outputs a value representing a score. com ogrellier lighgbm with selected features. Hyperparameter Tuning ImplementationNow we have the basic framework in place we will use cross validation to determine the performance of model hyperparameters and early stopping with the GBM so we do not have to tune the number of estimators. Usually the opposite happens higher on cross validation than on test because the model is tuned to the validation data. We can also do a sanity check to make sure the spacing is correct by counting the number of values in each interval. These search methods are very expensive so expect the hyperparameter tuning to take a while. Now since we have the best hyperparameters we can evaluate them on our test data remember not the real test data It s interesting that the model scores better on the test set than in cross validation. This can give us an idea of the generalization error on the test set. The random cross validation scores on the other hand are all over the place as expected. Grid Search ImplementationGrid search is best described as exhuastive guess and check. com jsaguiar updated 0 792 lb lightgbm with simple features and test on the testing features. Otherwise I have run these functions on the reduced dataset and attached the results to this kernel. What occurs in the middle of the objective function will vary according to the problem but for this problem we will use cross validation with the specified model hyperparameters to get the cross validation ROC AUC. The last entry index of 1 contains the best performing score. Algorithm for selecting next valuesAlthough we don t generally think of them as such both grid and random search are algorithms. Objective function a function that takes in hyperparameters and returns a score we are trying to minimize or maximize2. As an example of a simple domain the num_leaves is a uniform distribution. org wiki Gradient_boosting Stochastic_gradient_boosting. Repeat process until you run out of patience or are satisfied with the results. The only requirement of grid search is that it tries every combination in a grid once and only once. com willkoehrsen tuning automated feature engineering exploratory Feature Selection https www. The only difference we made from the default model was using early stopping to set the number of estimators which by default is 100. writer of_connection headers score hyperparameters iteration writer. The __variance__ of the model increases as we continue adding estimators because the model starts to rely too heavily on the training data high variance means overfitting. org wiki Hyperparameter_optimization provides a good high level overview of tuning options with links for more details In this notebook we will implement approaches 2 and 3 for a Gradient Boosting Machine Learning Model. For each combination of values we create a dictionary hyperparameters dict zip keys v and then pass these to the objective function defined earlier. A better approach would be to use the past results to reason about the best values to try next in the objective function especially because as we saw evaluating the objective function is time consuming In future work we will look at implementing automated hyperparameter tuning https towardsdatascience. When we save the results to a csv for some reason the dictionaries are saved as strings. Moreover we can take the functions developed here and apply them to any dataset or to any machine learning model not just the gradient boosting machine. Objective function takes in hyperparameters and returns the cross validation score we want to maximize or minimize2. However __do not use Excel to open a file that is being written to in Python__. I have not tried to run any form of grid search on the full data and probably will not try this method. The grid cross validation score increases over time. Clearly we are going to need a more efficient approach if we want to find better hyperparameters in a reasonable amount of time. html for the description of all the hyperparameters. This works the same way except the third argument is the number of values by default 100. com gentle introduction gradient boosting algorithm machine learning has recently emerged as one of the top machine learning models. A complete grid for the 10 hyperparameter is defined below. com an introductory example of bayesian optimization in python with hyperopt aae40fff4ff0 or this article on automated hyperparameter tuning https towardsdatascience. Here we will make a few simple plots using matplotlib seaborn and Altair __Unfortunately the Altair visualizations do not show up when the notebook is rendered. no xmlui bitstream handle 11250 2433761 16128_FULLTEXT. Part of the reason why hyperparameter tuning is so time consuming is because of the use of cross validation. org wiki Hyperparameter_optimization means finding the combination of hyperparameter values for a machine learning model that performs the best as measured on a validation dataset for a problem. The original score from the kernel where I got these features was 0. Then the number of estimators that yielded the best score on the validation data will be chosen as the number of estimators to use in the final model. The basics you need to know about the GBM are that it is an ensemble method that works by training many individual learners almost always decision trees. The code below uses the best random search hyperparameters to build a model train on the full features from Updated 0. Random search in contrast does a better job of exploring the search space and therefore can usually find a good combination of hyperparameters in far fewer iterations. These topics are pretty neat and it s incredible that they are available in an easy to use format for anyone to take advantage of. This will overwrite any information currently in the out_file so change to a new file name every time you want to start a new search. However if we are using such a small portion of the data that is not representative of the entire dataset then we should not expect the tuning to translate to the full dataset. cc gpmc17 slides LancasterMasterclass_1. Although the best hyperparameters from the smaller dataset did not work that well on the full dataset we were still able to see the ideas behind these two tuning methods. This is a mistake I ve made several times so you do not have to Below is the code we need to run before the search. This is a much faster and some say more accurate implementation than that available in Scikit Learn. com jsaguiar updated 0 792 lb lightgbm with simple features kernel I did not develop these features and want to give credit to the numerous people including Aguiar https www. Therefore we need to convert them back to dictionaries after reading in the results using the ast. org papers volume13 bergstra12a bergstra12a. The number of search iterations is set based on time resources. Hyperparameter tuning is extremely computationally expensive and working with the full dataset in a Kaggle Kernel would not be feasible for more than a few search iterations. close grid_results grid_search param_grid out_file out_file random_search_trials_1000. Run grid search on the reduced hyperparameter grid. Each of the values in the dicionary must be a list so we use list combined with range np. For now we will turn to implementing random and grid search on the reduced dataset for 1000 iterations just to compare the results I took the code below and already ran it because even with the small dataset it takes a very long time. VisualizationsVisualizations are both enjoyable to make and can give us an intuitive look into a technique. Early StoppingOne of the most important hyperparameters in a Gradient Boosting Machine is the number of estimators the number of decision trees trained sequentially. As a small note it s important to remember that we tune the hyperparameters to the training data using cross validation so the hyperparameter values we find are only optimal for the training data. literal_eval function. However we might be able to identify values of hyperparameters that seem more promising. This takes the same general structure as grid_search except for the method used to select the next hyperparameter values. The length of each list in the dictionary will be the optimal number of estimators to train. However the hyperparameters do not act by themselves and there are complex interactions between the model settings. This time we see hyperparameter values that are all over the place almost as if they had been selected at random Random search will do a much better job than grid search of exploring the search domain for the same number of iterations. This score will then be used to select the best model hyperparameter values. The overall objective of these _informed methods_ is to limit evaluations of the objective function by reasoning about the next values to try based on past evaluation results. com rants on machine learning smarter parameter sweeps or why grid search is plain stupid c17d97a0e881 lays out random search should probably be the first hyperparameter optimization method tried because of its effectiveness. Early stopping means training until the validation error does not decrease for a specified number of iterations. Repeat grid search on more focused grids until maximum computational time budget is exceeded. The test data is the actual competition data so we can then submit these and see how well the score translates to a full dataset First we will test the cross validation score using the best model hyperparameter values from random search. In addition to returning the value to maximize our objective function will return the hyperparameters and the iteration of the search. We simply need to pass in the number of early stopping rounds. Traditionally in optimization this is a score to minimize but here our score will be the ROC AUC which of course we want to maximize. However there are still many hyperparameters to optimize and we will choose 10 to tune. Hyperparameter tuning is a crucial part of the machine learning pipeline because the performance of a model can depend strongly on the choices of the hyperparameter values. As a reminder the metric we are using is Receiver Operating Characteristic Area Under the Curve ROC AUC. In the cv call the num_boost_round is set to 10 000 num_boost_round is the same as n_estimators but this number won t actually be reached because we are using early stopping. This Wikipedia Article https en. Moreover random search is always run with a limit on the number of search iterations. 0 returns values evenly spaced on a logarithmic scale. DomainThe domain or search space is all the possible values for all the hyperparameters that we want to search over. Switching from grid to random search to Bayesian optimization will only require making minor modifications to these four parts. The grid search method for finding the answer is to try all combinations of values in the domain and hope that the best combination is in the grid in reality we will never know if we found the best settings unless we have an infinite hyperparameter grid which would then require an infinite amount of time to run. product values where we iterate through all the possible combinations of values in the hyperparameter grid one at a time. com en latest generators. csv is the name of the file being written to. csv from Bash where out_file. Later we can try the hyperparameters that worked the best for the small versions of the data on a complete dataset to see if the best hyperparameters translate when increasing the size of the data 30 times The 1000 search iterations were not run in a kernel although they might be able to finish no guarantees in the 12 hour time limit. I currently am running random search with 500 iterations on the full dataset and will make those results publicly available when the search is complete Model Tuning Next StepsFrom here we might want to take the functions we wrote and apply them to a complete dataset. Although this might seem positive it means that the model will start to memorize the training data and then will not perform well on new testing data. __Random search__ set up a grid of hyperparameter values and select _random_ combinations to train the model and score. The GBM has many hyperparameters to tune http lightgbm. However unlike in a random forest where the trees are trained in __parallel__ in a GBM the trees are trained __sequentially__ with each tree learning from the mistakes of the previous ones. Next we will make predictions on the test data that can be submitted to the competition. Here we will run grid search for 5 iterations just as an example. In an upcoming notebook we will turn to automated hyperparameter tuning in particular Bayesian Optimization. pdf Getting StartedWith the necessary background out of the way let s get started. Again we have to remake this chart in seaborn to have the visualization appear in the rendered notebook if anyone knows how to address this issue please tell me in the comments Next for the numeric hyperparameters we will plot both the sampling distribution the hyperparameter grid and the results from random search in a kernel density estimate KDE plot. When we do hyperparameter tuning it s crucial to __not tune the hyperparameters on the testing data__. The boosting_type and is_unbalance domains are pretty simple because these are categorical variables. We need to keep in mind that the hyperparameters are not changed one at a time so if there are relationships between the values and the score they do not mean that particular hyperparameter is influencing the score. Extremely Important Note about Checking FilesWhen you want to check the csv file __do not open it in Excel while the search is ongoing__. com willkoehrsen intro to model tuning grid and random search Automated Model Tuning https www. This is one of many forms of regularization that aims to improve generalization performance on the testing set by not overfitting to the training data. Hyperparameters for GBMTo see which settings we can tune let s make a model and print it out. range always returns integers which means that if we want evenly spaced values that can be fractions we need to use np. io en latest Parameters. Typically when first using a method I define a wide search space centered around the default values. In a linear space there would be far more values from 0. Usually we expect the cross validation score to be higher than on the testing data but because of the small size of the testing data this might be reversed for this problem. Stacking Random and Grid SearchOne option for a smarter implementation of hyperparameter tuning is to combine random search and grid search 1. This process is repeated for each and every combination of hyperparameter values. Random search can also be thought of as an algorithm randomly select the next set of hyperparameters from the grid We can build a dictionary of hyperparameters by selecting one random value for each hyperparameter as follows again accounting for subsampling Next we define the random_search function. This means values are evenly spaced on a linear scale. After creating the testing set we cannot do any hyperparameter tuning with it We will also use only the numeric features to reduce the number of dimensions which will help speed up the hyperparameter search. This is a really cool topic and Bayesian optimization http gpss. In contrast https machinelearningmastery. com questions 171173 list all possible permutations from a python dictionary of lists Iterate through every possible combination of hyperparameters Create a hyperparameter dictionary Set the subsample ratio accounting for boosting type Evalute the hyperparameters Normally would not limit iterations Sort with best score on top Get the best parameters Create train test model Randomly sample from dictionary Deal with subsample ratio Dataframe for results Keep searching until reach max evaluations Choose random hyperparameters Evaluate randomly selected hyperparameters Sort with best score on top Get the best parameters Create train test model Create file and open connection Write column names Dataframe for results Choose random hyperparameters Evaluate randomly selected hyperparameters open connection append option and write results make sure to close connection Sort with best score on top Dataframe to store results https codereview. com questions 171173 list all possible permutations from a python dictionary of lists we create a generator http book. com 2017 01 23 a kaggle master explains gradient boosting or this in depth technical article. So the lines if i MAX_EVALS break would not be used in actual grid search. Therefore we will need a line of logic in our algorithm that sets the subsample to 1. The testing data is meant to serve as an estimate of the model performance when deployed on real data and therefore we do not want to optimize our model to the testing data because that will not give us a fair estimate of the actual performance. For random and grid search the domain is a hyperparameter grid and usually takes the form of a dictionary with the keys being the hyperparameters and the values lists of values for each hyperparameter. For random search we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. The results returned will show us the validation score ROC AUC the hyperparameters and the iteration sorted by best performing combination of hyperparameter values. pdf grid search spends too much time evaluating unpromising regions of the hyperparameter search space because it has to evaluate every single combination in the grid. There are no requirements for random search other than that the next values are selected at random. Here we will use the features from the Updated 0. Example of Cross Validation and Early Stopping To use the cv function we first need to make a LightGBM dataset. The process of hyperparameter tuning also called hyperparameter optimization https en. It is difficult to know which combination of hyperparameters will work best based only on theory because there are complex interactions between hyperparameters. Writing to File to Monitor ProgressWhen we run these searches for a long time it s natural to want to track the performance while the search is going on. org lecture deep neural network train dev test sets cxG1s but cross validation is a safer method to avoid overfitting. If we have a large enough training set we can probably get away with just using a single separate validation set https www. In fact the effect of changing these values is so small that validation scores literally did not change across runs indicating this small of a change has no effect on the model. Here we will use the Altair https altair viz. Even though it s an _uninformed_ method meaning it does not rely on past evaluation results random search can still usually find better values than the default and is simple to run. There are also some text editors such as notepad or Sublime Text where you can open the results safely while the search is occurring. However instead of splitting the valuable training data into a separate training and validation set we use KFold cross validation https www. The number of evaluations is set by the total combinations in the hyperparameter grid or the number of years we are willing to wait. Finally we can view the random search sequence of hyperparameters. In this approach every single combination of hyperparameters values is tried which can be very inefficient 3. 5 because this represents a larger distance in linear space but in logarithmic space each of these two intervals is the same width because they are multiples of 10 of each other. html In linear space the sequence starts at base start base to the power of start and ends with base stop This is useful for values that differ over several orders of magnitude such as the learning rate. Then at the end of searching choose the hyperparameters that yielded the highest cross validation score train the model on all the training data and make predictions on the test data. These settings need to be tuned for each problem because the best model hyperparameters for one particular dataset will __not be__ the best across all datasets. We could set this as another hyperparameter in our search but there s a better method early stopping https en. We have to pass in a set of hyperparameters to the cross validation so we will use the default hyperparameters in LightGBM. If you want to get an idea of how automated hyperparameter tuning is done check out this article https towardsdatascience. Random SearchRandom search is surprisingly efficient compared to grid search. As random search is just drawing random values we would expect the random search distribution to align with the sampling grid although it won t be perfectly aligned because of the limited number of searches. Cross ValidationTo evaluate each combination of hyperparameter values we need to score them on a validation set. Score versus HyperparametersAs a final plot we can show the score versus the value of each hyperparameter. We will delete the number of estimators found from the smaller dataset and use early stopping to find the best number of decision trees to train. In this case the better performance is probably due to small size of the test data and we get very lucky although this probably does not translate to the actual competition data. The score when submitting to the test competition is __0. Now we must slightly modify random_search and grid_search to write to this file every time. The four parts of hyperparameter tuning are 1. com willkoehrsen start here a gentle introduction Manual Feature Engineering Part One https www. For more details of the Gradient Boosting Machine GBM check out this high level blog post http blog. This grid search appears to be stuck in a relatively low performing region of the search space and because it is constrained to try all the values in the grid it is not able to try significantly different hyperparameter values that would perform better as occurs in random search. Testing Results on Full DataWe can take the best hyperparameters found from the 1000 iterations of random search on the reduced training data and try these on an entire training dataset. I ll provide the link here as soon as this notebook is finished but if you want to get an idea of Bayesian optimization you can check out this introductory article https towardsdatascience. To get a sense of how grid search works we can look at the progression of hyperparameters that were evaluated. Then we close the connection until the function is called again. Clearly there will not be any order but this can let us visualize what happens in a random search The star indicates the best value of the hyperparameter that was found. For other machine learning models where we do not need to use early stopping we can use the Scikit Learn functions RandomizedSearchCV or GridSearchCV. com ogrellier who have worked on these features. The results are available as part of the data in this kernel. First we can find out which method returned the best results. csv of_connection open out_file w writer csv. Look at the subsample and the is_unbalance because these are the only hyperparameters that change. pdf are essentially doing what we would do in the strategy outlined above adjust the next values tried in the search from the previous results. Random and grid search are two uniformed methods for hyperparameter tuning that search by selecting hyperparameter values from a grid domain. The hyperparameters __can not be tuned on the testing data__. Although grid search will find the optimal value of hyperparameters assuming they are in your grid eventually random search will usually find a close enough value in far fewer iterations. We can only use the testing data __once__ when we evaluate the final model. Introduction Hyperparameter Tuning using Grid and Random SearchIn this notebook we will explore two methods for hyperparameter tuning a machine learning model. In the case of the GBM this means training more decision trees and in this example we will use early stopping with 100 rounds meaning that the training will continue until validation error has not decreased for 100 rounds. To run these functions for 1000 iterations or however many you choose uncomment the cell below. Domain the set of hyperparameter values over which we want to search. The hundreds or thousands of weak learners are combined to make a single strong ensemble learner with the contributions of each individual learned during training using Gradient Descent the weights of the individual trees would therefore be a model _parameter_. These algorithms are therefore able to save time by evaluating more promising values of hyperparameters. Please check out their kernels https www. We can also evaluate the best random search model on the test data. Grid search suffers from one limiting problem it is extremely computationally expensive because we have to perform cross validation with every single combination of hyperparameters in the grid Let s see how many total hyperparameter settings there are in our simple little grid we developed. The objective function returns the cross validation score from the hyperparameters which we record in the dataframe. We have a problem find the hyperparameters that result in the best cross validation score and a set of values to try in the hyperparameter grid the domain. product from this Stack Overflow Question and Answer https codereview. I m currently running the random search on the full dataset from the Kernel referenced above and will see how the results turn out. Sampling some of the observations is not inherently negative and it can help us get reasonable answers in a much shorter time frame. For the hyperparameters that must be integers num_leaves min_child_samples we use range start stop step which returns a range of numbers from start to stop spaced by step or 1 if not specified. I am currently running this script on a full set of features for 500 iterations and will make the results public when they are available. If we keep adding estimators the training error will always decrease because the capacity of the model increases. We will focus on this model because it is currently the top performing method for most competitions on Kaggle and because the performance is highly dependent on the hyperparameter choices. However this will take much longer 300000 observations instead of 10000. The correct approach is therefore to use a validation set. com static pdf SigOpt_Bayesian_Optimization_Primer. To find out how well the model does on our test data we will retrain it on all the training data with the best number of estimators found during cross validation with early stopping. Some of the hyperparameters do not need to be tuned if others are for example min_child_samples and min_child_weight both limit the complexity of individual decision trees by adjusting the minimum leaf observation requirements and therefore we will only adjust one. To view the Altair figures you ll have to run the notebook yourself __First we can plot the validation scores versus the iteration. Below we make the same plot using seaborn because the Altair visualizations do not show up in the rendered notebook. ", "id": "willkoehrsen/intro-to-model-tuning-grid-and-random-search", "size": "45281", "language": "python", "html_url": "https://www.kaggle.com/code/willkoehrsen/intro-to-model-tuning-grid-and-random-search", "git_url": "https://www.kaggle.com/code/willkoehrsen/intro-to-model-tuning-grid-and-random-search", "script": "sklearn.metrics sklearn.model_selection random_search grid_search objective lightgbm numpy matplotlib.pyplot roc_auc_score seaborn pandas evaluate altair train_test_split ", "entities": "(('Repeat you', 'results'), 'process') (('so we', 'estimators'), 'Tuning') (('_ _', '_ best datasets'), 'need') (('methods', 'machine learning model'), 'apply') (('we', 'early stopping'), 'find') (('which', 'Python line keys values'), 'org') (('I', 'particular problem'), 'see') (('We', 'early stopping rounds'), 'need') (('hyperparameter why tuning', 'cross validation'), 'be') (('actually we', 'early stopping'), 'set') (('which', 'hyperparameter search'), 'do') (('that', 'maximum tree'), 'html') (('we', '10000 rows'), 'work') (('Hyperparameter tuning', 'search more than a few iterations'), 'be') (('you', 'script'), 'want') (('that', 'training data'), 'be') (('random search', 'kernel https www'), 'implement') (('Here we', 'just example'), 'run') (('we', 'that'), 'be') (('we', 'np'), 'return') (('where information', 'medium sized where most a few million observations'), 'be') (('better I', 'values'), 'concentrate') (('Hyperparameters', 'model settings'), 'think') (('score', 'test when competition'), 'be') (('code', 'early 100 early stopping rounds'), 'carry') (('algorithms', 'hyperparameters'), 'be') (('validation error', 'iterations'), 'mean') (('we', 'cross validation'), 'vary') (('we', 'testing separate set'), 'test') (('This', 'test set'), 'give') (('where we', 'time'), 'grid') (('We', 'decision trees'), 'delete') (('that', 'hyperparameters'), 'be') (('num_leaves', 'simple domain'), 'be') (('still reasoning it', 'hyperparameter next values'), 'turn') (('it', 'grid'), 'spend') (('only that', 'subsample'), 'look') (('we', 'which'), 'domain') (('we', 'hyperparameter how many total simple little grid'), 'suffer') (('they', 'hour time 12 limit'), 'try') (('that', 'objective function'), 'look') (('row', 'dataframe'), 'return') (('results', 'full dataset'), '792') (('always I', 'feedback criticism'), 'thank') (('at least they', 'data science important skillset'), 'be') (('boosting type', 'evenly random search'), 'distribute') (('we', 'course'), 'be') (('we', 'first LightGBM dataset'), 'need') (('we', 'previous results'), 'do') (('particular hyperparameter', 'score'), 'need') (('that', 'hyperparameters'), 'look') (('therefore we', 'only one'), 'need') (('usually when they', 'hyperparameters'), 'be') (('SearchRandom Random search', 'grid surprisingly search'), 'be') (('_ First we', 'iteration'), 'view') (('Here we', 'Updated'), 'use') (('objective function', 'search'), 'in') (('We', 'baseline model'), 'use') (('pdf', 'so upcoming notebook'), 'be') (('that', 'domain'), 'have') (('time algorithm', 'hyperparameter values'), 'input') (('us', 'GBM early stopping'), 'use') (('com willkoehrsen', 'feature engineering exploratory Feature Selection https automated www'), 'tuning') (('Otherwise I', 'kernel'), 'run') (('Now we', 'file'), 'modify') (('why this', 'so www'), 'explain') (('it', 'deep neural networks'), 'apply') (('dictionary hyperparameters', 'objective function'), 'create') (('that', 'problem'), 'mean') (('dataframe', 'data useful results'), 'be') (('we', 'kernel density estimate KDE plot'), 'have') (('First we', 'random search'), 'be') (('SearchOne option', 'random search search'), 'be') (('well optimal settings', 'dataset'), 'be') (('spacing', 'interval'), 'do') (('we', 'years'), 'set') (('that', 'validation data'), 'use') (('Below we', 'testing 6000 observations'), 'read') (('random search', 'default'), 'find') (('we', 'generator http book'), 'list') (('dictionaries', 'strings'), 'save') (('we', 'RandomizedSearchCV'), 'use') (('we', 'evaluations'), 'limit') (('search typically good space', 'hyperparameters'), 'have') (('we', 'competition'), 'need') (('when they', '500 iterations'), 'run') (('that', 'better random search'), 'appear') (('we', 'validation set https probably away just single separate www'), 'get') (('we', 'exactly reason'), 'be') (('search', 'Python'), 'cause') (('I', 'Gentle Introduction https so far www'), 'help') (('We', 'Hyperopt open source'), 'implement') (('cv_results', 'metric mean'), 'be') (('that', 'actual performance'), 'mean') (('However hyperparameters', 'model complex settings'), 'act') (('we', 'dataframe'), 'return') (('Now s', 'hyperparameter searches'), 'let') (('values', 'evenly linear scale'), 'mean') (('time maximum computational budget', 'more focused grids'), 'search') (('which', 'boosting stochastic gradient https'), 'be') (('lecture', 'safer overfitting'), 'be') (('I', 'Aguiar https www'), 'update') (('four parts', 'hyperparameter automated tuning'), 'scoresthese') (('we', 'score'), 'function') (('xmlui bitstream', '11250 2433761 16128_FULLTEXT'), 'handle') (('performance', 'hyperparameter values'), 'be') (('that', 'final model'), 'choose') (('we', 'complete dataset'), 'run') (('Results', 'reduced dataset'), 'examine') (('best values', 'search simply large space'), 'have') (('we', 'training data'), 'be') (('cross validation highest score', 'test data'), 'choose') (('Add text', 'test data'), 'random_results') (('we', 'Hyperopt specifically library'), 'implement') (('time you', 'new search'), 'overwrite') (('First we', 'format long dataframe'), 'library') (('Moreover we', 'machine learning model'), 'take') (('model this', 'data scientist'), 'learn') (('we', 'Gradient Boosting Machine Learning 2 Model'), 'provide') (('Random search', 'far fewer iterations'), 'do') (('hyperparameter how automated tuning', 'article https towardsdatascience'), 'want') (('_', '_ hyperparameters model interchangeably settings'), '_') (('entry last index', 'best performing score'), 'contain') (('Hyperparameter Randomly', 'results https codereview'), 'manipulation') (('we', 'ordered sequence'), 'input') (('how model', 'test full set'), 'calculate') (('which', '5 times performance'), 'use') (('process', 'hyperparameter values'), 'repeat') (('grid', 'time'), 'cross') (('Early stopping', 'cross validation function'), 'be') (('we', 'solely single hyperparameter'), 'be') (('we', 'cross validation score'), 'take') (('we', 'LightGBM'), 'have') (('which', 'step'), 'for') (('here we', 'single hyperparameter score'), 'be') (('you', 'article https still machinelearningmastery'), 'confuse') (('we', 'grid'), 'function') (('very shortly as soon we', 'hyperparameter tuning'), 'implement') (('it', 'grid'), 'be') (('we', 'others'), 'alter') (('we', 'range np'), 'be') (('back what', 'search'), 'let') (('_ _ Random search _ _', 'model'), 'set') (('Here we', 'Altair https altair viz'), 'use') (('com jsaguiar', 'simple features'), 'update') (('tuning', 'full dataset'), 'however') (('Objective FunctionThe objective function', 'score'), 'take') (('Moreover random search', 'search iterations'), 'run') (('However this', 'instead 10000'), 'take') (('com', 'machine learning top models'), 'emerge') (('we', 'https towardsdatascience'), 'be') (('Finally we', 'hyperparameters'), 'view') (('csv comma', 'iteration'), 'be') (('we', 'tuning two methods'), 'work') (('when notebook', '_ seaborn _'), 'make') (('break', 'grid actual search'), 'use') (('four parts', 'hyperparameter tuning'), 'be') (('that', 'competition'), 'make') (('This', 'default'), 'work') (('however you', 'uncomment cell'), 'choose') (('This', 'hyperparameter next values'), 'take') (('we', 'score'), 'have') (('randomly selected hyperparameters', 'results https codereview'), 'list') (('Therefore we', 'ast'), 'need') (('when search', 'eventually entire results'), 'look') (('function', 'connection'), 'close') (('keys', 'hyperparameter'), 'be') (('we', 'command prompt'), 'print') (('ensemble that', 'almost always trees'), 'be') (('which', 'hyperparameters values'), 'try') (('same they', 'other'), '5') (('when we', 'hyperparameter values'), 'do') (('Random notebook we', 'machine learning model'), 'Tuning') (('number', 'time resources'), 'set') (('s', 'way'), 'StartedWith') (('it', 'testing data'), 'tune') (('that', 'hyperparameter'), 'be') (('anyone', 'advantage'), 'be') (('validation so scores', 'model'), 'be') (('weights', 'individual trees'), 'combine') (('following code', 'numeric hyperparameters'), 'repeat') (('which', 'default'), 'use') (('hyperparameters', 'validation data'), 'select') (('that', '1'), 'need') (('combination', 'complex hyperparameters'), 'be') (('how we', 'hyperparameters'), 'go') (('latter', '10 times former'), 'be') (('hyperparameters', 'gradually score'), 'indicate') (('It', 'cross validation'), 'evaluate') (('Switching', 'four parts'), 'require') (('actually _', '_'), 'use') (('model', 'validation data'), 'happen') (('code', 'Updated'), 'use') (('length', 'optimal estimators'), 'be') (('However still many we', '10'), 'be') (('that', 'learning such rate'), 'start') (('above how results', 'Kernel'), 'run') (('Learning learning rate domain', '0'), 'rate') (('again Next we', 'random_search function'), 'think') (('Instead you', 'tail out_file'), 'view') (('cross validation random scores', 'all place'), 'be') (('we', 'sampling distribution'), 'plot') (('we', 'data'), 'advise') (('we', 'search3'), 'value') (('search', 'Excel'), 'note') (('GBM', 'http'), 'have') (('score', 'model hyperparameter then best values'), 'use') (('very this', 'competition probably actual data'), 'be') (('search', 'performance'), 'run') (('You', 'documentation also LightGBM http'), 'refer') (('Next StepsWe', 'choosing'), 'take') (('it', 'them'), 'be') (('method', 'best results'), 'find') (('performance', 'cross validation'), 'determine') (('Normally iterations', 'out_file 1000 grid_search_trials_1000'), 'list') (('model', 'testing then well new data'), 'mean') (('process', 'tuning'), 'call') (('Altair visualizations', 'rendered notebook'), 'make') (('which', 'rows'), 'use') (('lays', 'hyperparameter optimization probably first effectiveness'), 'rant') (('we', 'Receiver Operating Characteristic Curve ROC AUC'), 'be') (('this', 'problem'), 'expect') (('final we', 'hyperparameter'), 'score') (('VisualizationsVisualizations', 'technique'), 'be') (('correct approach', 'validation therefore set'), 'be') (('Grid Search ImplementationGrid search', 'best exhuastive guess'), 'describe') (('I', 'https medium'), 'write') (('Distribution', 'search grid random search'), 'show') (('performance', 'hyperparameter highly choices'), 'focus') (('Testing', 'training entire dataset'), 'take') (('connection', 'then connection'), 'create') (('This', 'really cool Bayesian gpss'), 'be') (('we', 'KFold cross validation https www'), 'use') (('number', 'decision trees'), 'be') (('vertical dashed line', 'random search'), 'indicate') (('_ _', 'model'), 'set') (('four parts', 'Bayesian Optimization'), 's') (('it', 'searches'), 'align') (('We', 'test data'), 'evaluate') (('training error', 'model increases'), 'decrease') (('high variance', 'overfitting'), 'mean') (('safely search', 'results'), 'be') (('we', 'training only data'), 's') (('very so hyperparameter', 'while'), 'be') (('where I', 'features'), 'be') (('returns values', 'evenly logarithmic scale'), 'space') (('it', 'very long time'), 'turn') (('com who', 'features'), 'ogrellier') (('they', 'far fewer iterations'), 'find') (('validation error', '100 rounds'), 'mean') (('s', 'it'), 'see') (('us', 'finish'), 'be') (('_', 'evaluation past results'), 'inform') (('much some', 'Scikit available Learn'), 'be') (('We', 'LightGBM library'), 'be') (('_ _ Grid Search _ _', 'validation data'), 'set') (('which', 'time'), 'be') (('complete grid', '10 hyperparameter'), 'define') (('that', 'magnitude'), 'let') (('us', 'time much shorter frame'), 'be') (('we', 'time'), 'go') (('basic strategy', 'hyperparameters'), 'be') (('which', 'far too memory'), 'html') (('I', 'probably method'), 'try') (('number', 'iterations'), 'need') (('I', 'default values'), 'define') (('hyperparameters', '_ testing data'), 'tune') (('then we', '1'), 'set') (('we', 'Bayesian particular Optimization'), 'turn') (('2017 01 23 kaggle master', 'depth technical article'), 'com') (('other next values', 'random'), 'be') (('such grid', 'generally them'), 'Algorithm') (('results', 'kernel'), 'be') (('baseline score hyperparameter', '_'), 'be') (('We', 'better early https'), 'set') (('us', 'where as many 0'), 'allow') (('we', 'random search'), 'plot') (('results data that', 'resulting objective function'), 'be') (('we', 'probably validation'), 'give') (('com jsaguiar', 'testing features'), 'update') (('code', 'grid search'), 'show') (('you', 'article https introductory towardsdatascience'), 'provide') (('cross validation', 'iteration'), 'do') (('that', 'training'), 'try') (('almost they', 'iterations'), 'see') (('Random search', 'grid domain'), 'be') (('when we', 'final model'), 'use') (('hyperparameters', 'hyperparameter values'), 'show') (('we', 'search'), 'be') (('we', 'validation set'), 'evaluate') (('_ sequentially tree', 'previous ones'), 'train') (('_ random it', 'always good code'), 'expect') (('that', 'Python _ _'), 'use') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["adjust", "advantage", "algorithm", "altair", "answer", "appear", "append", "apply", "approach", "argument", "article https medium", "article", "background", "baseline", "basic", "best", "blog", "boosting", "build", "call", "case", "categorical", "category", "cause", "cell", "chart", "check", "choose", "close", "code", "column", "combine", "combined", "command", "compare", "competition", "concept", "connection", "contrast", "control", "convert", "correct", "could", "course", "create", "credit", "csv", "cv", "data", "dataframe", "dataset", "decision", "default", "define", "demonstration", "depend", "dependent", "depth", "description", "develop", "dict", "dictionary", "difference", "dimension", "distance", "distributed", "distribution", "doc", "domain", "effect", "en", "end", "engineering", "ensemble", "error", "evaluate", "evaluation", "even", "every", "experience", "explore", "fact", "faster", "feature", "feedback", "file", "final", "find", "fold", "following", "forest", "form", "format", "found", "framework", "function", "future", "general", "generalization", "generated", "generator", "gradient", "graph", "grid", "grow", "hand", "handle", "header", "help", "high", "histogram", "history", "hope", "http", "hyperopt", "hyperparameter", "idea", "implement", "implementation", "improve", "including", "index", "individual", "input", "interest", "intuition", "io", "issue", "iteration", "job", "kaggle", "kernel", "key", "latter", "leaderboard", "leaf", "learner", "learning", "least", "lecture", "length", "let", "level", "library", "lightgbm", "line", "linear", "link", "list", "little", "look", "lot", "magnitude", "manual", "mapping", "matplotlib", "max", "maximum", "mean", "meaning", "method", "metric", "middle", "might", "mind", "minimize", "minimum", "mistake", "model", "most", "my", "name", "need", "negative", "network", "neural", "new", "next", "no", "not", "notebook", "num", "number", "numeric", "numpy", "objective", "observation", "open", "opening", "optimization", "optimize", "option", "order", "ordered", "out", "overall", "overfitting", "overview", "parameter", "parse", "part", "past", "pdf", "people", "per", "perform", "performance", "performing", "pipeline", "place", "plot", "point", "positive", "post", "power", "practice", "print", "problem", "product", "provide", "public", "python", "random", "range", "ratio", "re", "read", "reading", "reason", "record", "reduce", "reference", "region", "regression", "regularization", "result", "return", "row", "run", "running", "sample", "sampling", "save", "scale", "science", "score", "script", "seaborn", "search", "select", "selected", "selection", "sense", "separate", "sequence", "set", "several", "single", "size", "solution", "something", "source", "space", "speed", "splitting", "standard", "start", "step", "store", "strategy", "structure", "subset", "technique", "test", "testing", "text", "theory", "think", "those", "thought", "through", "time", "topic", "total", "track", "train", "training", "translate", "tree", "try", "tune", "tuning", "turn", "type", "uniform", "unpack", "until", "up", "validation", "value", "variance", "vertical", "view", "visualization", "visualize", "walk", "while", "who", "width", "work", "write", "zip"], "potential_description_queries_len": 312, "potential_script_queries": ["evaluate", "numpy", "seaborn", "sklearn"], "potential_script_queries_len": 4, "potential_entities_queries": ["best", "engineering", "grid", "hyperparameter", "learning", "line", "little", "model", "neural", "next", "objective", "optimization", "performing", "science", "seaborn", "selected", "set", "testing", "total"], "potential_entities_queries_len": 19, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 313}