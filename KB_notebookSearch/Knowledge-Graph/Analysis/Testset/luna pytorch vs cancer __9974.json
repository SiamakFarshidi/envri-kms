{"name": "luna pytorch vs cancer ", "full_name": " h2 Introduction h2 Load required libraries h2 Understand the Dataset h3 Annotations h3 Candidates h2 Missing data h2 Load the data h2 Train the Model h2 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "More information https luna16. There are two CSV files that we ll be working with annotations. com gzuidhof full preprocessing tutorial has a great walkthrough and visualization of this data format. png We will create a namedtuple to store the information that we combine from the candidates and annotations. Next we create DataLoader s out of our datasets. The code used to set up caching is from the Deep Learning with PyTorch book and can be found in the book s GitHub repository https github. Another thing to note is that there can be multiple annotations and candidates in a single CT scan. The SimpleITK package has a very simple API that we ll use to get details about the CT scan. com sentdex first pass through data w 3d convnet. Candidates that have a value of False for is_nodule will come last in the list with the same relative order of diameters as above. IntroductionThis notebook was created as a part of the Weights Biases PyTorch Book Reading Group https community. org articles hounsfield unit which goes from 1000 to 3000. We need to also convert the output we want from the model is_nodule into a PyTorch tensor. Now let s put it all together in a few utility functions and a Dataset class. com sentdex s TensorFlow model in this notebook https www. We read the CT scan and store it as a NumPy array. png attachment b9b5d21a 10c2 437e bc94 383e0182f2c5. Therefore when looking for a candidate s diameter from the diameters dict we ll assume that if the center coordinates of the candidate are less than half the radius of the annotated nodule away from the center coordinates of the annotated nodule then they are the same nodule. have the same seriesuid. We will walk through and understand all the steps using a single candidate before putting the code together into utility functions and a PyTorch Dataset. That path will be available at index 0 of the result. The next step would be to convert this chunk of CT scan to a PyTorch tensor. The numbers below the accuracy give us a clearer picture of what is going on. The huge difference between candidates and annotations tells us that we will have many candidates for which we won t have a diameter in the annotations file. The cell below finds all such seriesuid s. class is 0 if the mass isn t a nodule and 1 if it is a nodule both malignant and benign. That sentence was a mouthful Maybe this diagram will be easier to understand image. Most of the code used to read the data comes from the amazing book we re reading in the group Deep Learning with PyTorch https www. The idea was to find a notebook on Kaggle with a TensorFlow model trained on the Luna16 dataset and try to convert it to PyTorch. It would be unrealistic to expect the model to learn how to predict positive samples with just a couple of data points. For now we ll try not to worry about this and move on with the data that we have left. org SimpleITK Notebooks 01_Image_Basics. We can now train the model. We will be using the cross entropy loss https ravimashru. Similarly we group the candidates that are part of the same CT scan and then use the diameters dictionary we created above to fetch each candidate s diameter. com sentdex first pass through data w 3d convnet in PyTorch. There are over 750k candidates. Load required libraries Understand the DatasetThe Luna16 Lung Nodule Analysis 2016 dataset contains chest CT scans and annotations indicating where there are nodules in each CT scan and their diameters. ai c community events pytorch book 32 hosted by Sanyam Butani https www. Since the list contains tuples the order of fields will determine the way the list is sorted. The files could be in any one of the subset folders in the dataset. We lost almost 50 of the data PANIC Well we d panic if this wasn t something we re just playing with. raw files associated with the candidate. The glob package can return multiple files that match the pattern specified. com books deep learning with pytorch. There are 1 186 total annotations available. However if fine for now as we won t be using the diameter information when building this simple model. This will allow us to easily access the centers and respective diameters of all nodules in a particular CT scan. csv and candidates_V2. dev blog 2021 07 18 understanding cross entropy loss function to train our model so we need two columns for the output one hot encoded values of the boolean is_nodule value we re interested in. That s not the entire story. We also get the following information Center point of reference of the CT scan also known as the origin Size of each voxel short for volume pixel since each CT scan can have a different size of voxels Direction matrix that has a direction vector for of each axis in the CT scanWe convert the origin and voxel size to NumPy arrays so that they are easier to use in calculations. So this model is not very useful right now. AnnotationsThe annotations file contains the center and diameter of each mass in CT scans. All candidates with a diameter of zero recall this happens when the candidate coordinates are not close enough to any annotation coordinates or there is no corresponding annotation for the particular candidate will come after these. We can now load this file using the SimpleITK package. Load the dataWe ll now walk through how we want to convert the data we have into a format that we can consume with PyTorch. org The model we will build will try to predict whether a particular region of a CT scan has a nodule or not. html available if you want to read more. png Missing dataThere are some seriesuid s in the dataset that don t have corresponding CT scans in the dataset. We will extract a chunk by getting a list of three slices one for each direction and then using that to extract the actual values from the CT scan. This notebook is my attempt to train the model that Sentdex https www. However for simplicity we ll assume that the dataset contains only one. The actual values in the CT scan are in Hounsfield units HU https radiopaedia. That s equivalent to 41 RGB color images of resolution 512x512 for a single CT scan Since most of the CT scan doesn t contain any interesting to us we will extract 3 dimensional chunks of the CT scan that contain nodules as input for our model. There are 443 seriesuid s in the annotations and candidates CSV files that don t have corresponding. Let s say we want to extract a chunk of size 10 along the index column and 18 rows and columns. seriesuid is the unique identifer of the CT scan. Among these candidates those with the largest diameter will come before those with smaller diameters. Also because our dataset is highly imbalanced we need a better strategy to train our model and also a better indication of model performance instead of accuracy. coordX coordY and coordZ are coorindates of the center of the mass. com deep learning with pytorch dlwpt code blob master util disk. png attachment 93cf076d 063b 4b3f 90d4 1a1ab3f6fe04. Since it takes about 10 minutes to run I have made those seriesuid s available as a separate dataset https www. We then sort the list of candidates in reverse order. The first epoch takes a long time but once the data is cached the other epochs are super fast ConclusionWe ve got an accuracy of over 99 on the validation set Time to pop the champagne But hang on. First let us set up some utility functions for caching the dataset. We now remove from our list of candidates those that don t have a CT scan. We use a range of 1000 or 1000 to remove extremely dense materials from the CT scan. Train the ModelThis model is a PyTorch version of Sentdex https www. CandidatesThe candidates file contains a class flag for each mass in the CT scans. The glob module allows us to find the file by using patterns instead of manually looking inside each of the folders. We will first group all annotations that are part of the same CT scan i. I guess we ll have to keep that champagne on the ice for a little longer. We now convert the origin and voxel size from the patient coorindate system previously mentioned to coordinates that we can use to index into the NumPy array representing the CT scan. We ll load data from this dataset to save precious GPU time on Kaggle. We now create an instance of the model the loss function and an optimizer to train the model. We use a VALIDATION_STRIDE of 10 which means every 10th CT scan will be in the validation set. This notebook https www. In particular after sorting we ll have all candidates with the value of is_nodule as True at the beginning of the list. They have great documentation https simpleitk. The center of the nodule is at index irc in the complete scan ct_scan. The X Y and Z coordinates of the center can be slightly different in the annotations and candidate files. The entire CT scan is currently very large for us to work with. We now have a CT scan chunk ct_scan_chunk. mhd file for a given seriesuid. As we ll see later this will significantly speed up training after the first epoch. com sentdex built in his notebook First pass through Data w 3D ConvNet https www. With the utility function and caching set up we can now create a PyTorch datset. com mashruravi luna16missingcandidates. The model predicted all negatives correctly but didn t make any correct predictions for the positive data points. We use the glob module to find the. This probably happened because we don t have enough positive samples in the training and validation set. We now have three slices we can use in each direction to extract the chunk we need. ", "id": "shivanir23/luna-pytorch-vs-cancer", "size": "9974", "language": "python", "html_url": "https://www.kaggle.com/code/shivanir23/luna-pytorch-vs-cancer", "git_url": "https://www.kaggle.com/code/shivanir23/luna-pytorch-vs-cancer", "script": "torch.optim torch.utils.data __init__ BytesIO fetch GzipDisk(Disk) eval_loop MODE_BINARY diskcache.core DataLoader morphology getCache ThreadPoolExecutor find_missing_and_multiple forward torch.nn collections core numpy store __getitem__ concurrent.futures io LunaModel(nn.Module) LunaDataset(Dataset) skimage matplotlib.pyplot cassandra.cqltypes measure tqdm.notebook pandas Dataset __len__ BytesType tqdm Disk diskcache getCtScanChunk namedtuple SimpleITK FanoutCache mpl_toolkits.mplot3d.art3d Poly3DCollection train_loop ", "entities": "(('We', 'NumPy array'), 'read') (('CT 10th scan', 'validation set'), 'use') (('2016 dataset', 'CT where scan'), 'require') (('what', 'clearer picture'), 'give') (('center', 'complete scan'), 'be') (('notebook', 'Weights Biases'), 'create') (('code', 'GitHub repository https github'), 'use') (('idea', 'PyTorch'), 'be') (('us', 'folders'), 'allow') (('we', 'above diameter'), 'group') (('annotations file', 'CT scans'), 'contain') (('actual values', 'Hounsfield units HU https radiopaedia'), 'be') (('next step', 'PyTorch tensor'), 'be') (('didn correctly t', 'data positive points'), 'predict') (('highly we', 'instead accuracy'), 'also') (('we', 'candidates'), 'create') (('path', '0 result'), 'be') (('we', 'boolean value'), 'blog') (('we', 'PyTorch'), 'load') (('org articles hounsfield which', '3000'), 'unit') (('s', 'dataset https separate www'), 'make') (('dataset', 'only one'), 'assume') (('thing', 'CT multiple single scan'), 'be') (('We', 'model'), 'create') (('We', 'CT scan'), 'use') (('full preprocessing', 'data format'), 'have') (('we', 'CT scan'), 'convert') (('Now s', 'utility together a few functions'), 'let') (('X Y coordinates', 'slightly annotations'), 'be') (('model', 'data points'), 'be') (('They', 'documentation https great simpleitk'), 'have') (('they', 'calculations'), 'get') (('Maybe diagram', 'image'), 'be') (('particular we', 'list'), 'have') (('We', 'Kaggle'), 'load') (('particular region', 'nodule'), 'org') (('seriesuid', 'CT unique scan'), 'be') (('We', 'CT scan chunk now ct_scan_chunk'), 'have') (('We', 'CT scan'), 'extract') (('we', 'that'), 'try') (('we', 'PyTorch now datset'), 'create') (('we', 'PyTorch tensor'), 'need') (('However we', 'when simple model'), 'use') (('we', 'ice'), 'guess') (('we', 'Deep PyTorch https www'), 'use') (('DataLoader', 'datasets'), 'create') (('probably we', 'training set'), 'happen') (('First us', 'dataset'), 'let') (('com sentdex', 'Data w ConvNet https First 3D www'), 'pass') (('we', 'index 10 column'), 'let') (('don t', 'corresponding'), 'be') (('we', 'annotations file'), 'tell') (('corresponding annotation', 'these'), 'happen') (('those', 'smaller diameters'), 'come') (('Train', 'PyTorch Sentdex https www'), 'be') (('com sentdex', 'PyTorch'), 'pass') (('we', 'chunk'), 'have') (('we', 'CT scan'), 'have') (('don t', 'CT scan'), 'remove') (('later this', 'first epoch'), 'speed') (('then they', 'annotated nodule'), 'dict') (('files', 'dataset'), 'be') (('list', 'way'), 'tuple') (('We', 'utility together functions'), 'walk') (('us', 'CT particular scan'), 'allow') (('We', 'reverse order'), 'sort') (('We', 'SimpleITK package'), 'load') (('CandidatesThe candidates', 'CT scans'), 'file') (('that', 'diameters'), 'come') (('ConclusionWe super fast ve', 'champagne'), 'take') (('that', 'model'), 's') (('coordX coordY', 'mass'), 'be') (('We', 'loss https cross entropy ravimashru'), 'use') (('notebook', 'model'), 'be') (('don t', 'CT corresponding dataset'), 'be') (('com sentdex', 'data w 3d first convnet'), 'pass') (('that', 'same CT'), 'group') (('CSV two we', 'annotations'), 'be') (('pattern', 'multiple files'), 'return') (('we', 'something'), 'lose') ", "extra": "['annotation', 'patient', 'test', 'lung']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "annotation", "array", "associated", "blob", "blog", "book", "boolean", "build", "cell", "center", "chest", "chunk", "close", "code", "color", "column", "combine", "community", "contain", "convert", "correct", "could", "create", "csv", "data", "dataset", "dict", "dictionary", "difference", "direction", "entropy", "epoch", "every", "extract", "fetch", "file", "find", "following", "format", "found", "function", "glob", "group", "half", "hot", "idea", "index", "input", "instance", "largest", "learn", "learning", "let", "list", "little", "load", "looking", "lost", "malignant", "match", "matrix", "model", "module", "most", "move", "multiple", "my", "namedtuple", "need", "next", "no", "nodule", "not", "notebook", "optimizer", "order", "out", "output", "package", "part", "path", "patient", "pattern", "performance", "picture", "pixel", "png", "point", "positive", "predict", "preprocessing", "pytorch", "range", "raw", "re", "read", "reading", "recall", "reference", "region", "relative", "remove", "repository", "resolution", "return", "reverse", "right", "run", "save", "scan", "sentence", "separate", "set", "short", "simplicity", "single", "size", "something", "sort", "speed", "step", "store", "strategy", "subset", "system", "those", "through", "time", "total", "train", "training", "try", "tutorial", "understanding", "unique", "unit", "up", "util", "validation", "value", "vector", "version", "visualization", "volume", "voxel", "walk", "work"], "potential_description_queries_len": 148, "potential_script_queries": ["core", "diskcache", "forward", "io", "measure", "morphology", "mplot3d", "numpy", "skimage", "torch", "tqdm"], "potential_script_queries_len": 11, "potential_entities_queries": ["entropy", "positive", "scan", "single"], "potential_entities_queries_len": 4, "potential_extra_queries": ["lung"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 157}