{"name": "advanced uses of shap values ", "full_name": " h1 Recap h1 SHAP Values Review h1 Summary Plots h1 Summary Plots in Code h1 SHAP Dependence Contribution Plots h1 Dependence Contribution Plots in Code h1 Your Turn ", "stargazers_count": 0, "forks_count": 0, "description": "But through some algorithmic cleverness Shap values allow us to decompose any prediction into the sum of effects of each feature value yielding a graph like this Imgur https i. We ll walk through an example plot for the soccer data Imgur https i. Convert from string Yes No to binary package used to calculate Shap values Create object that can calculate shap values calculate shap values. Each dot represents a row of the data. For example consider an ultra simple model y 4 x1 2 x2 If x1 takes the value 2 instead of a baseline value of 0 then our SHAP value for x1 would be 8 from 4 times 2. For instance what is the distribution of effects Is the effect of having a certain value pretty constant or does it vary a lot depending on the values of other feaures. When plotting we call shap_values 1. That value caused one prediction to increase and it caused the other prediction to decrease. The spread suggests that other features must interact with Ball Possession. RecapWe started by learning about permutation importance and partial dependence plots for an overview of what the model has learned. High values of Goal scored caused higher predictions and low values caused low predictionsIf you look for long enough there s a lot of information in this graph. package used to calculate Shap values Create object that can calculate shap values calculate shap values. This didn t require writing a lot of code. These are insightful and relevant for many real world use cases. But sometimes it will jump out at you. Outside of those few outliers the interaction indicated by color isn t very dramatic here. While the primary trend is upward you can visually inspect whether that varies by dot color. The only line that s different from the summary_plot is the last line. If you don t supply an argument for interaction_index Shapley uses some logic to pick one that may be interesting. png Start by focusing on the shape and we ll come back to color in a minute. Calculate shap_values for all of val_X rather than a single row to have more data for plot. For classification problems there is a separate array of SHAP values for each possible outcome. Summary Plots in CodeYou have already seen the code to load the soccer football data We get the SHAP values for all validation data with the following code. Plus with a little effort they can be explained to a non technical audience. png In addition to this nice breakdown for each prediction the Shap library https github. We then learned about SHAP values to break down the components of individual predictions. Consider the following very narrow example for concreteness. The code isn t too complex. Your Turn Test yourself https www. But you ll want to be careful when running these to plot with reasonably sized datasets. Some things you should be able to easily pick out The model ignored the Red and Yellow Red features. For example here we have highlighted two points with similar ball possession values. This provides a great overview of the model but we might want to delve into a single feature. Now we ll expand on SHAP values seeing how aggregating many SHAP values can give more detailed alternatives to permutation importance and partial dependence plots. If a feature has medium permutation importance that could mean it has a large effect for a few predictions but no effect in general or a medium effect for all predictions. But there s a lot they don t show. The fact this slopes upward says that the more you possess the ball the higher the model s prediction is for winning the Man of the Match award. You can interpret this to say In general having the ball increases a team s chance of having their player win the award. com slundberg shap offers great visualizations of groups of Shap values. png Link to larger view https i. The horizontal location is the actual value from the dataset and the vertical location shows what having that value did to the prediction. Index of 1 is explained in text below. We will focus on two of these visualizations. com dansbecker permutation importance is great because it created simple numeric measures to see which features mattered to a model. SHAP Values ReviewShap values show how much a given feature changed our prediction compared to if we made that prediction at some baseline value of that feature. But it doesn t tell you how each features matter. Usually Yellow Card doesn t affect the prediction but there is an extreme case where a high value caused a much lower prediction. Summary Plots Permutation importance https www. Dependence Contribution Plots in CodeWe get the dependence contribution plot with the following code. That s where SHAP dependence contribution plots come into play. These are harder to calculate with the sophisticated models we use in practice. The exception is when using an xgboost model which SHAP has some optimizations for and which is thus much faster. They are both colored purple indicating the team scored one goal. You ll face some questions to test how you read them in the exercise. Have questions or comments Visit the course discussion forum https www. SHAP dependence contribution plots provide a similar insight to PDP s but they add a lot more detail. These visualizations have conceptual similarities to permutation importance and partial dependence plots. But the trick with these techniques is in thinking critically about the results rather than writing code itself. png For comparison a simple linear regression would produce plots that are perfect lines without this spread. This suggests we delve into the interactions and the plots include color coding to help do that. So multiple threads from the previous exercises will come together here. This helped us make comparisons between features easily and you can present the resulting graphs to non technical audiences. SHAP Dependence Contribution PlotsWe ve previously used Partial Dependence Plots to show how a single feature impacts predictions. Calculating SHAP values can be slow. com kernels fork 1699743 with some questions to develop your skill with these techniques. But if they only score one goal that trend reverses and the award judges may penalize them for having the ball so much if they score that little. This is what we will plot. png These two points stand out spatially as being far away from the upward trend. png This plot is made of many dots. In this case we index in to get the SHAP values for the prediction of True. com learn machine learning explainability discussion to chat with other learners. Each dot has three characteristics Vertical location shows what feature it is depicting Color shows whether that feature was high or low for that row of the dataset Horizontal location shows whether the effect of that value caused a higher or lower prediction. But there are a few caveats. It is short enough that we explain it in the comments. SHAP summary plots give us a birds eye view of feature importance and what is driving it. It isn t a problem here because this dataset is small. For example the point in the upper left was for a team that scored few goals reducing the prediction by 0. ", "id": "dansbecker/advanced-uses-of-shap-values", "size": "7159", "language": "python", "html_url": "https://www.kaggle.com/code/dansbecker/advanced-uses-of-shap-values", "git_url": "https://www.kaggle.com/code/dansbecker/advanced-uses-of-shap-values", "script": "sklearn.ensemble sklearn.model_selection RandomForestClassifier numpy pandas train_test_split ", "entities": "(('they', 'lot more detail'), 'provide') (('easily you', 'non technical audiences'), 'help') (('They', 'one goal'), 'be') (('SHAP dependence contribution where plots', 'play'), 's') (('didn t', 'code'), 'require') (('value', 'prediction'), 'be') (('we', 'single feature'), 'provide') (('visualizations', 'permutation importance'), 'have') (('effect', 'higher prediction'), 'have') (('features', 'model'), 'be') (('features', 't you'), 'doesn') (('that', 'perfect spread'), 'png') (('questions', 'course discussion forum https www'), 'have') (('what', 'it'), 'shap') (('two points', 'spatially far away upward trend'), 'stand') (('extreme where high value', 'much lower prediction'), 'affect') (('that', 'summary_plot'), 'be') (('we', 'minute'), 'Start') (('that', '0'), 'be') (('visually that', 'dot color'), 'inspect') (('easily model', 'Yellow Red features'), 'ignore') (('we', 'feature'), 'show') (('you', 'reasonably sized datasets'), 'want') (('trick', 'rather code'), 'be') (('SHAP then value', 'x1'), 'consider') (('We', 'visualizations'), 'focus') (('Dependence Contribution Plots', 'following code'), 'get') (('us', 'Imgur https i.'), 'allow') (('how aggregating', 'permutation importance'), 'expand') (('the higher prediction', 'Match award'), 'say') (('player', 'award'), 'interpret') (('pretty it', 'other feaures'), 'be') (('you', 'graph'), 'cause') (('they', 'non technical audience'), 'explain') (('we', 'practice'), 'be') (('how you', 'exercise'), 'face') (('So multiple threads', 'previous exercises'), 'come') (('which', 'optimizations'), 'be') (('here we', 'ball possession similar values'), 'highlight') (('These', 'world use many real cases'), 'be') (('we', 'True'), 'index') (('We', 'soccer data'), 'walk') (('it', 'general medium predictions'), 'have') (('other features', 'Ball Possession'), 'suggest') (('plots', 'that'), 'suggest') (('so much they', 'little'), 'penalize') (('enough we', 'comments'), 'be') (('model', 'what'), 'start') (('that', 'one'), 'use') (('We', 'individual predictions'), 'learn') (('Create that', 'shap values'), 'calculate') (('We', 'following code'), 'see') (('com slundberg shap', 'Shap values'), 'offer') (('Create that', 'shap values'), 'convert') (('SHAP Dependence Contribution PlotsWe', 'feature impacts how single predictions'), 'use') ", "extra": "['outcome', 'test']", "label": "Perfect_files", "potential_description_queries": ["argument", "array", "award", "baseline", "binary", "calculate", "call", "case", "classification", "code", "coding", "color", "colored", "comparison", "consider", "could", "course", "data", "dataset", "decompose", "develop", "distribution", "dot", "effect", "effort", "expand", "explained", "eye", "face", "fact", "feature", "following", "football", "general", "graph", "help", "high", "importance", "include", "increase", "index", "individual", "instance", "interact", "learn", "learning", "left", "library", "line", "linear", "little", "load", "look", "lot", "lower", "mean", "might", "model", "multiple", "no", "non", "numeric", "object", "out", "overview", "package", "partial", "permutation", "player", "plot", "plotting", "png", "point", "prediction", "present", "problem", "provide", "read", "regression", "row", "running", "score", "separate", "shape", "short", "similar", "single", "spread", "string", "sum", "summary", "team", "test", "text", "those", "through", "trend", "upper", "validation", "value", "vertical", "view", "walk", "world", "xgboost"], "potential_description_queries_len": 105, "potential_script_queries": ["numpy", "sklearn"], "potential_script_queries_len": 2, "potential_entities_queries": ["lower", "similar"], "potential_entities_queries_len": 2, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 107}