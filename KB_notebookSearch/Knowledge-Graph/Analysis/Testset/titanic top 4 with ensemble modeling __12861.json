{"name": "titanic top 4 with ensemble modeling ", "full_name": " h1 Titanic Top 4 with ensemble modeling h3 Yassine Ghouzam PhD h4 13 07 2017 h2 1 Introduction h2 2 Load and check data h3 2 1 Load data h3 2 2 Outlier detection h3 2 3 joining train and test set h3 2 4 check for null and missing values h2 3 Feature analysis h3 3 1 Numerical values h4 SibSP h4 Parch h4 Age h4 Fare h3 3 2 Categorical values h4 Sex h4 Pclass h4 Embarked h2 4 Filling missing Values h3 4 1 Age h2 5 Feature engineering h3 5 1 Name Title h3 5 2 Family size h3 5 3 Cabin h3 5 4 Ticket h2 6 MODELING h3 6 1 Simple modeling h4 6 1 1 Cross validate models h4 6 1 2 Hyperparameter tunning for best models h4 6 1 3 Plot learning curves h4 6 1 4 Feature importance of tree based classifiers h3 6 2 Ensemble modeling h4 6 2 1 Combining models h3 6 3 Prediction h4 6 3 1 Predict and Submit results ", "stargazers_count": 0, "forks_count": 0, "description": "According to the growing cross validation curves GradientBoosting and Adaboost could perform better with more training examples. So Sex might play an important role in the prediction of the survival. 1 Predict and Submit resultsIf you found this notebook helpful or you just liked it some upvotes would be very much appreciated That will keep me motivated Load data Outlier detection iterate over features columns 1st quartile 25 3rd quartile 75 Interquartile range IQR outlier step Determine a list of indices of outliers for feature col append the found outlier indices for col to the list of outlier indices select observations containing more than 2 outliers detect outliers from Age SibSp Parch and Fare Show the outliers rows Drop outliers Fill empty and NaNs values with NaN Check for Null values Infos Summarie and statistics Correlation matrix between numerical values SibSp Parch Age and Fare values and Survived Explore SibSp feature vs Survived Explore Parch feature vs Survived Explore Age vs Survived Explore Age distibution Fill Fare missing values with the median value Explore Fare distribution Apply log to Fare to reduce skewness distribution Explore Pclass vs Survived Explore Pclass vs Survived by Sex Fill Embarked nan values of dataset set with S most frequent value Explore Embarked vs Survived Explore Pclass vs Embarked Explore Age vs Sex Parch Pclass and SibSP convert Sex into categorical value 0 for male and 1 for female Filling missing value of Age Index of NaN age rows Get Title from Name Convert to categorical values Title Drop Name variable Create a family size descriptor from SibSp and Parch Create new feature of family size convert to indicator values Title and Embarked Replace the Cabin number by the type of cabin X if not Take prefix Create categorical values for Pclass Drop useless variables Cross validate model with Kfold stratified cross val Modeling step Test differents algorithms Adaboost Best score RFC Parameters tunning Best score Gradient boosting tunning Best score Best score Concatenate all classifier results. To determine this we need to explore in detail these features SibSPIt seems that passengers having a lot of siblings spouses have less chance to surviveSingle passengers 0 SibSP or with two other persons SibSP 1 or 2 have more chance to surviveThis observation is quite interesting we can consider a new feature describing these categories See feature engineering ParchSmall families have more chance to survive more than single Parch 0 medium Parch 3 4 and large families Parch 5 6. 2 Family sizeWe can imagine that large families will have more difficulties to evacuate looking for theirs sisters brothers parents during the evacuation. Title_2 which indicates the Mrs Mlle Mme Miss Ms category is highly correlated with Sex. At this point i can t explain why first class has an higher survival rate. The computation time is clearly reduced. It doesn t mean that the other features are not usefull. Moreover the more a passenger has parents children the older he is and the more a passenger has siblings spouses the younger he is. The strategy is to fill Age with the median age of similar rows according to Pclass Parch and SibSp. Age distribution seems to be the same in Male and Female subpopulations so Sex is not informative to predict Age. According to the feature importance of this 4 classifiers the prediction of the survival seems to be more associated with the Age the Sex the family size and the social standing of the passengers more than the location in the boat. Firstly I will display some feature analyses then ill focus on the feature engineering. But we can see that passengers with a cabin have generally more chance to survive than passengers without X. 1 Numerical valuesOnly Fare feature seems to have a significative correlation with the survival probability. As we can see Fare distribution is very skewed. So i choosed to create a Fize family size feature which is the sum of SibSp Parch and 1 including the passenger. 2 Outlier detectionSince outliers can have a dramatic effect on the prediction espacially for regression problems i choosed to manage them. So even if Age is not correlated with Survived we can see that there is age categories of passengers that of have more or less chance to survive. Age is not correlated with Sex but is negatively correlated with Pclass Parch and SibSp. 4 Feature importance of tree based classifiersIn order to see the most informative features for the prediction of passengers survival i displayed the feature importance for the 4 tree based classifiers. The correlation map confirms the factorplots observations except for Parch. 4 Feature importance of the tree based classifiers 6. For those who have seen the Titanic movie 1997 I am sure we all remember this sentence during the evacuation Women and children first. 3 Prediction 6. 4 check for null and missing valuesAge and Cabin features have an important part of missing values. IntroductionThis is my first kernel at Kaggle. So i decided to use SibSP Parch and Pclass in order to impute the missing ages. Factorplots of family size categories show that Small and Medium families have more chance to survive than single passenger and large families. It seems that very young passengers have more chance to survive. 3 Plot learning curvesLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy. The 28 89 and 342 passenger have an high Ticket Fare The 7 others have very high values of SibSP. 1 Name TitleThe Name feature contains information on passenger s title. 2 Hyperparameter tunning for best modelsI performed a grid search optimization for AdaBoost ExtraTrees RandomForest GradientBoosting and SVC classifiers. 2 Categorical values SexIt is clearly obvious that Male have less chance to survive than Female. 1 Numerical values 3. 3 joining train and test setI join train and test datasets to obtain the same number of features during categorical conversion See feature engineering. Skewness is clearly reduced after the log transformation 3. GradientBoosting and Adaboost classifiers tend to overfit the training set. 1 Simple modeling 6. Since some passenger with distingused title may be preferred during the evacuation it is interesting to add them to the model. 1 AgeAs we see Age column contains 256 missing values in the whole dataset. No difference between median value of age in survived and not survived subpopulation. It could therefore lead to the actual placement of the cabins within the ship. At this stage we have 22 features. Fsize LargeF MedF Single refer to the size of the passenger family. We detect 10 outliers. We notice that age distributions are not the same in the survived and not survived subpopulations. SVC and ExtraTrees classifiers seem to better generalize the prediction since the training and cross validation curves are close together. An outlier is a row that have a feature value outside the IQR an outlier step. 2 Ensemble modeling 6. Filling missing Values 4. I choosed the Titanic competition which is a good way to introduce feature engineering and ensemble modeling. Then i considered outliers as rows that have at least two outlied numerical values. Be carefull there is an important standard deviation in the survival of passengers with 3 parents children AgeAge distribution seems to be a tailed distribution maybe a gaussian distribution. In this case it is better to transform it with the log function to reduce this skew. I preferred to pass the argument soft to the voting parameter to take into account the probability of each vote. I decided to detect outliers from the numerical values features Age SibSp Sarch and Fare. 1 Cross validate modelsI compared 10 popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure. It means that their predictions are not based on the same features. Which may be more informative. FareSince we have one missing value i decided to fill it with the median value which will not have an important effect on the prediction. 4 Ticket 6 Modeling 6. The first letter of the cabin indicates the Desk i choosed to keep this information only since it indicates the probable location of the passenger in the Titanic. It seems that passenger coming from Cherbourg C have more chance to survive. In the plot of Age in function of Parch Age is growing with the number of parents children. EmbarkedSince we have two missing values i decided to fill them with the most fequent value of Embarked S. 4 check for null and missing values 3 Feature analysis 3. 3 joining train and test set 2. But the general correlation is negative. This can lead to overweigth very high values in the model even if it is scaled. 1 Predict and Submit results 1. The 5 classifiers give more or less the same prediction but there is some differences. The prediction seems to be quite similar for the 5 classifiers except when Adaboost is compared to the others classifiers. Women and children first It is interesting to note that passengers with rare title have more chance to survive. SVC Decision Tree AdaBoost Random Forest Extra Trees Gradient Boosting Multiple layer perceprton neural network KNN Logistic regression Linear Discriminant AnalysisI decided to choose the SVC AdaBoost RandomForest ExtraTrees and the GradientBoosting classifiers for the ensemble modeling. I set the n_jobs parameter to 4 since i have 4 cpu. Let s see the Pclass distribution vs EmbarkedIndeed the third class is the most frequent for passenger coming from Southampton S and Queenstown Q whereas Cherbourg passengers are mostly in first class which have the highest survival rate. Indeed there is a peak corresponding to young passengers that have survived. Survived missing values correspond to the join testing dataset Survived column doesn t exist in test set and has been replace by NaN values when concatenating the train and test set 3. When we superimpose the two densities we cleary see a peak correponsing between 0 and 5 to babies and very young childrens. To adress this problem i looked at the most correlated features with Age Sex Parch Pclass and SibSP. Because of the low number of passenger that have a cabin survival probabilities have an important standard deviation and we can t distinguish between survival probability of passengers in the different desks. Sex and Title_2 Mrs Mlle Mme Miss Ms and Title_3 Mr refer to the gender. My hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown Q Southampton S. My hypothesis is that first class passengers were prioritised during the evacuation due to their influence. 3 CabinThe Cabin feature column contains 292 values and 1007 missing values. It is particularly true for cabin B C D E and F. But in the violin plot of survived passengers we still notice that very young passengers have higher survival rate. Age and Title_1 Master refer to the age of passengers. 2 Categorical values 4 Filling missing Values 4. We can say that Pc_1 Pc_2 Pc_3 and Fare refer to the general social standing of passengers. 1 Combining models 6. First class passengers have more chance to survive than second class and third class passengers. 2 Outlier detection 2. 2 Hyperparamater tunning for best models 6. 3 Plot learning curves 6. Feature engineering 5. Subpopulations in these features can be correlated with the survival. Titanic Top 4 with ensemble modeling Yassine Ghouzam PhD 13 07 2017 1 Introduction 2 Load and check data 2. We note that the four classifiers have different top features according to the relative importance. So i decided to replace the Ticket feature column by the ticket prefixe. This script follows three main parts Feature analysis Feature engineering Modeling 2. Additionally i decided to created 4 categories of family size. 1977 to detect ouliers which defines an interquartile range comprised between the 1st and 3rd quartile of the distribution values IQR. There is 17 titles in the dataset most of them are very rare and we can group them in 4 categories. Theses differences between the 5 classifier predictions are sufficient to consider an ensembling vote. However 1rst class passengers are older than 2nd class passengers who are also older than 3rd class passengers. Since there is subpopulations that have more chance to survive children for example it is preferable to keep the age feature and to impute the missing values. I plot the feature importance for the 4 tree based classifiers Adaboost ExtraTrees RandomForest and GradientBoosting. Nevertheless they share some common important features for the classification for example Fare Title_2 Age and Sex. I used the Tukey method Tukey JW. Last part concerns modeling and predicting the survival on the Titanic using an voting procedure. Tickets with same prefixes may have a similar class and survival. 4 TicketIt could mean that tickets sharing the same prefixes could be booked for cabins placed together. But be carefull this step can take a long time i took me 15 min in total on 4 cpu. We also see that passengers between 60 80 have less survived. PclassThe passenger survival is not the same in the 3 classes. 1 Cross validate models 6. The family size seems to play an important role survival probability is worst for large families. I supposed that passengers without a cabin have a missing value displayed instead of the cabin number. 1 Combining modelsI choosed a voting classifier to combine the predictions coming from the 5 classifiers. Feature analysis 3. This trend is conserved when we look at both male and female passengers. Load and check data 2. 1 Age 5 Feature engineering 5. ", "id": "yassineghouzam/titanic-top-4-with-ensemble-modeling", "size": "12861", "language": "python", "html_url": "https://www.kaggle.com/code/yassineghouzam/titanic-top-4-with-ensemble-modeling", "git_url": "https://www.kaggle.com/code/yassineghouzam/titanic-top-4-with-ensemble-modeling", "script": "cross_val_score sklearn.tree detect_outliers sklearn.discriminant_analysis AdaBoostClassifier plot_learning_curve KNeighborsClassifier sklearn.neural_network DecisionTreeClassifier collections seaborn numpy learning_curve ExtraTreesClassifier GradientBoostingClassifier VotingClassifier sklearn.ensemble sklearn.model_selection RandomForestClassifier LinearDiscriminantAnalysis matplotlib.pyplot pandas LogisticRegression Counter GridSearchCV sklearn.neighbors SVC sklearn.linear_model StratifiedKFold sklearn.svm MLPClassifier ", "entities": "(('why first class', 'survival higher rate'), 'explain') (('i', 'Embarked S.'), 'have') (('Additionally i', 'family size'), 'decide') (('So Sex', 'survival'), 'play') (('class first passengers', 'influence'), 'be') (('passengers', 'X.'), 'see') (('class First passengers', 'second class third passengers'), 'have') (('very we', '4 categories'), 'be') (('which', 'passenger'), 'choose') (('tickets', 'cabins'), 'mean') (('5 classifiers', 'more same prediction'), 'give') (('passenger', 'more chance'), 'seem') (('i', '4 cpu'), 'take') (('2 Hyperparameter', 'AdaBoost ExtraTrees RandomForest SVC GradientBoosting classifiers'), 'perform') (('that', 'Indeed young passengers'), 'be') (('i', 'them'), 'have') (('I', 'Age SibSp Sarch'), 'decide') (('Fare 1 Numerical feature', 'survival probability'), 'seem') (('prediction', 'boat'), 'seem') (('who', 'Queenstown Q Southampton S.'), 'be') (('even it', 'model'), 'lead') (('val Modeling step Test differents algorithms', 'tunning Best score'), 'predict') (('i', 'Age Sex Parch Pclass'), 'look') (('i', '4 tree based classifiers'), 'display') (('It', 'cabin B C D particularly E'), 'be') (('IntroductionThis', 'first Kaggle'), 'be') (('differences', 'ensembling vote'), 's') (('which', 'distribution values'), '1977') (('which', 'survival highest rate'), 'let') (('it', 'model'), 'prefer') (('2 Hyperparamater', 'best models'), 'tun') (('Small families', 'single passenger'), 'show') (('Sex', 'Age'), 'seem') (('curvesLearning 3 Plot learning curves', 'accuracy'), 'be') (('GradientBoosting', 'Adaboost training set'), 'tend') (('TitleThe Name feature', 'title'), '1') (('difference', 'subpopulation'), 'survived') (('Firstly I', 'feature engineering'), 'display') (('still very young passengers', 'survival higher rate'), 'notice') (('which', 'feature good engineering'), 'choose') (('Nevertheless they', 'example'), 'share') (('validation curves', 'training'), 'seem') (('PclassThe passenger survival', '3 classes'), 'be') (('sizeWe large families', 'evacuation'), 'imagine') (('four classifiers', 'relative importance'), 'note') (('Last part', 'voting procedure'), 'concern') (('correlation map', 'Parch'), 'confirm') (('Age', 'Pclass negatively Parch'), 'correlate') (('So i', 'ticket prefixe'), 'decide') (('role survival important probability', 'large families'), 'seem') (('3', 'categorical conversion'), 'join') (('who', 'class also 3rd passengers'), 'be') (('when we', 'male passengers'), 'conserve') (('Cross', 'stratified kfold validation procedure'), 'validate') (('he', 'younger'), 'child') (('Subpopulations', 'survival'), 'correlate') (('Titanic Top', 'data'), 'phd') (('i', '4 cpu'), 'set') (('Pc_1 Pc_2 Pc_3', 'passengers'), 'say') (('very young passengers', 'more chance'), 'seem') (('passengers', 'cabin instead number'), 'suppose') (('GradientBoosting', 'training better more examples'), 'perform') (('clearly Male', 'Female'), 'be') (('SVC Decision Tree AdaBoost Random Extra Trees Gradient Boosting layer perceprton neural KNN Logistic regression Linear Discriminant Multiple AnalysisI', 'GradientBoosting ensemble modeling'), 'forest') (('It', 'ship'), 'lead') (('which', 'prediction'), 'have') (('Age column', 'whole dataset'), 'see') (('Combining modelsI', '5 classifiers'), '1') (('4 check', 'missing values'), 'have') (('only it', 'Titanic'), 'indicate') (('we', 'babies'), 'see') (('it', 'skew'), 'be') (('Skewness', 'log clearly transformation'), 'reduce') (('AgeAge distribution', 'parents 3 children'), 'be') (('that', 'outlier step'), 'be') (('Parch 3 4 families', 'single Parch 0 medium'), 'feature') (('So i', 'missing ages'), 'decide') (('Tickets', 'similar class'), 'have') (('predictions', 'same features'), 'mean') (('CabinThe Cabin feature 3 column', '292 values'), 'contain') (('Age Master', 'passengers'), 'refer') (('script', 'three main parts'), 'follow') (('I', '4 tree based classifiers'), 'plot') (('which', 'highly Sex'), 'correlate') (('we', 'different desks'), 'have') (('age distributions', 'subpopulations'), 'notice') (('that', 'at least two outlied numerical values'), 'consider') (('it', 'missing values'), 'be') (('LargeF MedF Fsize Single', 'passenger family'), 'refer') (('strategy', 'Pclass Parch'), 'be') (('column Survived doesn', '3'), 'survive') (('Mrs Mlle Mme Miss Ms', 'Title_3 gender'), 'sex') (('we', 'more chance'), 'see') (('passengers', 'more chance'), 'woman') (('we', 'evacuation'), 'be') (('when Adaboost', 'others classifiers'), 'seem') (('7 others', 'SibSP'), 'have') (('I', 'vote'), 'prefer') ", "extra": "['gender', 'test', 'procedure']", "label": "Perfect_files", "potential_description_queries": ["account", "accuracy", "age", "append", "argument", "associated", "best", "boosting", "cabin", "case", "categorical", "category", "check", "children", "choose", "classification", "classifier", "close", "col", "column", "combine", "competition", "computation", "consider", "conversion", "convert", "correlation", "could", "create", "data", "dataset", "detail", "detect", "detection", "difference", "display", "distribution", "effect", "empty", "engineering", "ensemble", "ensembling", "evacuation", "evaluate", "even", "explore", "family", "feature", "fill", "found", "frequent", "function", "gaussian", "general", "grid", "group", "high", "ill", "importance", "impute", "including", "join", "kernel", "layer", "lead", "learning", "least", "letter", "list", "log", "look", "looking", "lot", "main", "male", "manage", "map", "matrix", "mean", "median", "method", "might", "min", "missing", "model", "most", "movie", "my", "need", "network", "neural", "new", "not", "notebook", "null", "number", "numerical", "observation", "optimization", "order", "outlier", "overfit", "overfitting", "parameter", "part", "passenger", "peak", "perform", "plot", "point", "predict", "prediction", "prefix", "probability", "problem", "range", "rare", "reduce", "regression", "relative", "replace", "role", "row", "score", "script", "search", "second", "select", "sentence", "set", "similar", "single", "size", "soft", "stage", "standard", "step", "strategy", "sum", "survival", "survived", "test", "testing", "those", "ticket", "time", "title", "total", "train", "training", "transform", "transformation", "tree", "trend", "type", "val", "validate", "validation", "value", "variable", "who", "worst"], "potential_description_queries_len": 162, "potential_script_queries": ["numpy", "seaborn"], "potential_script_queries_len": 2, "potential_entities_queries": ["feature", "learning", "main", "numerical", "regression", "survival", "tree"], "potential_entities_queries_len": 7, "potential_extra_queries": ["procedure"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 165}