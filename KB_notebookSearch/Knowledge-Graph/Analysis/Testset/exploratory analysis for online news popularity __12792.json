{"name": "exploratory analysis for online news popularity ", "full_name": " h1 Exploratory Analysis For Online News Popularity A deep insight analysis h2 Open Book Ideas to consider h2 Reading the data h1 Data Processing h2 Grading the Shares h3 Here we check the class balance h2 Data Visualization h3 Noise Removal h3 Evaluating Expectations h4 Weekdays Variable Effect h4 Data Channel Evaluation h3 Evaluating the Observerd Hypothesis h4 Data channels vs Num images vs popularity h1 Making Recommendations For Good Articles h1 Feature Extraction Selection h4 Feature Selection on the whole dataset h4 Feature selection based on best hypothesis observed h4 Feature Selection Based on Fisher Discrimating Analysis h4 Finding the normal distrubution of the dataset h4 Feature selection based on best hypothesis observed Normal distribution h4 Feature Selection Based on Fisher Discrimating Analysis Normal Distribution h4 Feature Selection on the whole dataset Normal Distribution h2 Visulazing the impact of normal distribution on the data h3 Variables of our features selection are listed below h1 Machine Learning Supervised Learning Classification h3 KNN Classifier h3 Random Forest Classifier h3 SVM Support Vector Machines h1 Summary and Conclusion h1 To Be Continued h2 Varaibles Summary Observation ", "stargazers_count": 0, "forks_count": 0, "description": "The process followed is highlighted below Data Cleaning Noise detection and removal Subjective analysis Using our intuition to evaluate a data variable feature and decide whether a variable influences the popularity of the article or not. Seven popularity classes was derived from the shares class and three machine learning model was built to be able to predict the popularity class of the model. SVM Support Vector MachinesThe maximum accuracy observed with SVM was 50 58. From the insight analysis carried out on the data set the following are some of the things we recommend to improve the popularity of an article The number of words in the article should be less than 1500 words. grouped both data together and create a pandas dataframe from it. n_unique_tokens should be between 0. average_token_length should be between 4 6 The number of keywords in the metedata really influences the shares to a margin. Evaluating the Observerd Hypothesis Data channels vs Num_images vs popularityHere we compared data channels impact with num images in regards to article popularity Earlier we said good articles tend to have high visuals num_images in them but it is not always the case everytime. Coming in third position will be the World and or Tech channels. This record is classifed as a noise and will be remove. We evaluated the impact of this normaility on our modelsAlthough shares doesn t have a normal distrubition we can do a log transformation to give us a normal distrubition data Feature Extraction Selection Here we will be extracting some of the best features we observed out from the data. Normal Distribution analysis for Shares Normality what we mean is that the data should look like a normal distribution. running a pair plot for the kw__terms run a pairplot normalizaling the data with standard scaler we will normalized all the features selections scalled all the feature selections encoding the label set with a label encoder Splitting the data for Training and Testing train and test for a feature selections function for confusion matrix Compute confusion matrix Only use the labels that appear in the data We want to show all ticks. Increase the amount of subjectivity in the title and content. Is this also observed in the other data channels What about the effects of the Worst Best and Avg keywords Is their an influence on the min max or avg shares on each article referenced articles. 4 on the testing data set. png Libaries import linear algebra data processing CSV file I O e. Evaluating Expectations Weekdays Variable EffectIt seems the best popular articles are usually posted on Mondays and Wednesday and a bit of tuesdays Sundays and Saturdays Weekends generally are the worsts days to publish an articles. com in ayoayibiowu In this project the goal is the explore the dataset given and be able to find critical insights that can be used to influence potential article popularity. Articles should have good amount of images. Although our main gain is to build important insight about how popularity of articles are defined we also went ahead in seeing how to predict the popularity of an article. Articles that talks about current trending are better for shares From the scatterplot below it can be seen than good articles will generally tend to have n_tokens_content less than 2000 and greater than 100 words From the scatterplot below it can be seen than good articles will generally tend to have n_tokens_title between 6 and 17 words similar to the above. Coming in third position will be the World and Tech. A value upward of 5 is recommend. t statistics and some machine learning models. this particluar also contains 0 on a lot of attributes. The lesser the better. Making Recommendations For Good Articles n_tokens_content should be less than 1500 words. One important observation is also that Entertaiment channel based articls seems to be persistent in all popularity types. defining the model predict the result predict the result predict the result Plot non normalized confusion matrix iterating through all the possible features commence training NOTE It takes hours to be complete predict the result iterating through all the possible features commence training NOTE It takes hours to be complete predict the result. com thehapyone online news popularity a classification problem. Variables of our features selection are listed below Feature selection based on best hypothesis observed data_feature1 Feature Selection on the whole dataset data_feature2 Feature selection using fisher discriminal analysis data_feature_fisher Feature selection based on the best hypothesis observed but with a normal distribution log transformation data_feature1_normal Feature selection using fisher discriminate analysis on normal distribution dataset data_feature_fisher_normal Feature Selection on the whole dataset data_feature2_normal Machine Learning Supervised Learning Classification Here we are going to apply some machine learing models on our dataset for classifying an article popularity KNN ClassifierThe KNN model which gave the best accuracy of 49. Sundays and Saturdays Weekends generally are the worsts days to publish an articles. The best machine learning model was the Random Forest which was able to attain an accuracy of 51. fetch the number of classes print grouped loop through all features the variance of the feature j mean for class k of feature j calculates the fisher score of the features plot the fisher analysis score feature selection based on fisher score Fisher Index Ratio Filter Remove features with low score indices of features to remove based on fisher ratios we mark for removal remove features with low fisher score use log transformation to transform each features to a normal distribution note log transformation can only be performed on data without zero value applying log transformation only apply to non zero features attempt to only transform the positive values alone only the best observed features are extracted here calculates the fisher score of the features plot the fisher analysis score feature selection based on fisher score Fisher Index Ratio Filter Remove features with low score indices of features to remove based on fisher ratios we mark for removal remove features with low fisher score ihave about 25 features left. They are generally low inrespective of the popularity. Between 1 40 images is great. What is ratio of postive to negative word articles in the dataset Are the mutually balanced and can we make a judgment based on that alone What about the effect of subjectivity on the title and test in the popularity Reading the data Data Processing Grading the Shares Exceptional Top 95 Excellent Top 90 Very Good Top 80 Good Top 60 Average Top 50 Poor Top 35 Very Poor Rest Here we check the class balance Data Visualization Evaluating our hypothesis Noise Removal We observed some noise from the dataset coming from different features. For a more concrete channel The Business and Entertaiment channel are great for the best popularity. It belongs to world news or others. Random Forest ClassifierRandom Forest has the best result for this classification task reaching an accuracy of 51. 6 17 words is the ideal number of words to have for titles. This is peculiar pattern. Entertainment channels generally tend to have high visuals as their popularity increases with the only exception in Average popularity. kw_min_min and related kw_ terms running a pair plot for the kw__terms run a pairplot Finding relationship between rate_positive_words rate_negative_words global_rate_positive_words global_rate_negative_words and shares running a pair plot for the these terms run a pairplot attempt polartiy attempt polartiy Finding relationship between rate_positive_words rate_negative_words global_rate_positive_words global_rate_negative_words and shares running a pair plot for the terms run a pairplot attempt title_subjectivity attempt title_subjectivity running a pair plot for the kw__terms run a pairplot attempt self_reference_min_shares running a pair plot for the kw__terms run a pairplot running a pair plot for the kw__terms run a pairplot extact the weekdays articles distrubution shows the days when articles are usually posted shows relationship with the number of shares and the weekdays shows relationship with the number of shares and the weekdays compare only the best three popularity running a pair plot for the kw__terms run a pairplot extact the weekdays articles distrubution Shows the distribution of the articles across the channels the ranking of the channels in regards to the shares popularity shows relationship with the number of shares and the ranking of the channels compare only the best three popularity histogram and normal probability plot applying log transformation transformed histogram and normal probability plot Check for missing data shares data is not needed for classification convert categorical variables into dummy it use one hot encoding extract the label data in this case popularity only the best observed features are extracted here helper function for evalating the fisher ndex create the fisher output variable A vector of all the features It s expected that the dim1_T and dim1_L be of the same size else this input parameters is nulled. For example using all the features gave an accuracy less than 1 of the highest accuracy observed. Some of the reasons for this low accuracy score is as a result of the large variance in the data set and also the imbalance in the class distribution which drives the prediction models to be bias towards popularity classes with more articles. n_tokens_title should be between 6 17 words. The number of keywords in the metadata really influences the shares to a margin. Tech channels performed generally okay. In order to be able to tune the models for better performances we consider different feature selection techniques but these feature selections technique didn t really made much influence to the performance of the machine learning models. and label them with the respective list entries Rotate the tick labels and set their alignment. creating a grading criteria for the shares create label grades for the classes Update this class label into the dataframe Merging the weekdays columns channels as one single column Merging the data channels as one single column logic to merge data channel merge the the new data into the dataframe Now I drop the old data Evaluating features sensors contribution towards the label Fetch the counts for each class Visualizaing the low expectation hypothesis n_non_stop_words Comment Visualizing the n_non_stop_words data field shows that the present of a record with 1042 value futher observation of that data shows that it belongs to entertainment which is not actually. From the plot below we can see that Business channels generally don t get influnece by the num_images in them. Article title shouldn t be too long or too short. Increase the number of popular unique words in the article to increase the chances of having better popularity. Also having a couple of videos is also nice for article popularity but not too much. Quantitative Analysis How correct is our intuition Here we carry our several analysis to accept or debunk our initial hypothesis Normal Distribuiton Observation on the dataset Feature Selection and Evaluation Machine Learning Classification Summary and Conclusion. Does the number of shares in those referenced articles also influence the number of shares in the main article At what point in the weekend do people share articles the most Can that means people read those articles the most on those days What is the effect of LDA analysis on the article popularity Does article with more text sentiment influence the popularity what the is relationship between the text sentiment and the article publish day Are more sentiment on a particlar day How about the influence of postive negative words in the text sentiment and popularity. Exploratory Analysis For Online News Popularity A deep insight analysis by Ayo Ayibiowu https www. those equals to 0 n_non_stop_unique_tokens a lot of unique words it is better to use a different plot from bar plots line plot box plot box plot of the dataset shows majority 75 of the data inrespective of their shares is in the range of 0. Avoid the use of longer words in the articles. The below criteria will be considered Feature selection based on best hypothesis observed Feature Selection on the whole dataset Feature selection using fisher discriminal analysis Feature selection based on the best hypothesis observed but with a normal distribution log transformation Feature selection using fisher discriminate analysis on normal distribution dataset Feature Selection on the whole dataset Normal Distribution Feature Selection on the whole dataset Feature selection based on best hypothesis observed Feature Selection Based on Fisher Discrimating Analysis Finding the normal distrubution of the dataset Feature selection based on best hypothesis observed Normal distribution Feature Selection Based on Fisher Discrimating Analysis Normal Distribution Feature Selection on the whole dataset Normal Distribution Visulazing the impact of normal distribution on the data In the new transformation the features observation is more clear than before. Due to the nature of Random forest being able to set different number of decision trees features tree depth splitting criteria and others it tends to require a lot of parameter tuning. Here we will go ahead and drop the field of n_non_stop_words remove noise from n_tokens_content. The higher the lower the odds. An observation with SVM is that training start becoming increasing as the number of polynomial degree increases training examples increases C value rises and also the number of features increase which basically makes the model become more complex to draw an hyper plane for separating the classes. Best popular articles are usually posted on Mondays and Wednesday and a bit of Tuesdays. Open Book Ideas to considerSome ideas to consider What is the effect of number of images number of videos on the article popularity Is their a relationship between the number of words in the content and or number of words in the title in the article popularity Is their a concrete relationship between average length of words in the content to the popularity Create a grading rank for the popularity Excellent Good Okay Poor Very Poor How is the ranking of the channels in regards to the shares popularity What data channel has the most popularity and what feature in that particular data channel contributes towards that asserction. Here it can be seen that the best articles with highest share popularity belongs to the Others channel. This is important because several statistic tests rely on this e. We initially carried out a subjective analysis which was based on our own intuition and because we understand it is easily possible for human intuition to be biased or crowded from past experience and use a quantitative analysis to confirm our initial hypothesis by doing univariate analysis using scatter plot boxplot and barplot of each feature with the shares feature. They won t contribute anything describing the data from the data there will be need to normailze the data if their will be need for condersing any machine learning model. Articles that talks about current trending tends to have higher popularity. Easy to read words helps to improve article popularity. Meaning they mightnot always be the best channel to publish for. In the v2 of this project the knowledge gained so far is used to address a classification problem thus improving the prediction accuracy by large margin. Summary and ConclusionIn this project we analyzed the given online news data set and was able to clearly observed some interesting patterns that good articles do have in common. 8 num_hrefs is between 1 and 40 referrence links num_imgs should between 1 40 images num_videos should be between 0 25 vidoes. Those rare cases where the high visuals or low visuals doesn t change anything is in the Business channel. To Be ContinuedThis project is continued here https www. Loop over data dimensions and create text annotations. Best popular articles are usually posted on Mondays and Wednesday and a bit of tuesdays. Do people favours postive worded article. The higher the value the better the shares chances. Sundays and Saturdays Weekends generally are the worsts days to publish an article. So does it offers any uniques No it doesn t. read_csv Here we drop the two non preditive url and timedelta attributes. png attachment image. Articles referencing popular articles have a higher chance of improving their own popularity. Your chances are low Data Channel EvaluationHere it can be seen that the best articles with highest share popularity belongs to the Others channel. Although this was the best accuracy discovered there wasn t much difference with the other models. First extract out the number of features available. Also machine learning models was built to be able to predict the popularity of a given article. The Business and Entertainment channel are great for the best popularity. 11 was based on using the all the data set feature and number of neighbor of 71. Varaibles Summary Observation image. ", "id": "thehapyone/exploratory-analysis-for-online-news-popularity", "size": "12792", "language": "python", "html_url": "https://www.kaggle.com/code/thehapyone/exploratory-analysis-for-online-news-popularity", "git_url": "https://www.kaggle.com/code/thehapyone/exploratory-analysis-for-online-news-popularity", "script": "sklearn.metrics fisher_index_calc KNeighborsClassifier unique_labels seaborn numpy plot_confusion_matrix scipy.stats copy sklearn.ensemble sklearn.model_selection confusion_matrix LabelEncoder RandomForestClassifier matplotlib.pyplot sklearn.utils.multiclass pandas norm StandardScaler probplot accuracy_score GridSearchCV sklearn.neighbors SVC sklearn.preprocessing sklearn.svm train_test_split make_scorer ", "entities": "(('data', 'normal distribution'), 'be') (('is relationship', 'text sentiment'), 'influence') (('number', 'margin'), 'influence') (('Sundays Weekends', 'worsts generally articles'), 'be') (('we', 'article'), 'be') (('png Libaries import linear algebra data', 'CSV file'), 'process') (('which', 'entertainment'), 'create') (('Here we', 'Machine Learning Classification dataset Feature Selection Summary'), 'analysis') (('machine learning Also models', 'given article'), 'build') (('noise', 'different features'), 'be') (('number', 'article'), 'be') (('Sundays Weekends', 'worsts generally articles'), 'seem') (('n_tokens_content', 'Good Articles'), 'be') (('it', 'them'), 'evaluate') (('com thehapyone', 'news classification online problem'), 'popularity') (('KNN ClassifierThe KNN which', '49'), 'list') (('easily human intuition', 'shares feature'), 'carry') (('Coming', 'third position'), 'be') (('it', 'parameter'), 'feature') (('particluar', 'attributes'), 'contain') (('popularity Seven classes', 'model'), 'derive') (('Avg keywords', 'article'), 'observe') (('They', 'generally low popularity'), 'be') (('majority', '0'), 'equal') (('SVM Support Vector MachinesThe', 'maximum SVM'), 'be') (('best articles', 'Others channel'), 'see') (('knowledge', 'large margin'), 'gain') (('Best popular articles', 'tuesdays'), 'post') (('record', 'noise'), 'classife') (('we', 'about 25 features'), 'fetch') (('6 17 words', 'titles'), 'be') (('Sundays Weekends', 'worsts generally article'), 'be') (('features observation', 'new transformation'), 'consider') (('We', 'ticks'), 'run') (('Articles', 'images'), 'have') (('best articles', 'Others channel'), 'be') (('people', 'postive worded article'), 'favours') (('Also couple', 'article also popularity'), 'have') (('it', 'uniques'), 'offer') (('number', 'margin'), 'be') (('Here we', 'n_tokens_content'), 'go') (('good articles', 'similar above'), 'be') (('Random which', '51'), 'be') (('input else parameters', 'same size'), 'run') (('Best popular articles', 'Tuesdays'), 'post') (('low doesn', 'Business channel'), 'be') (('Articles', 'own popularity'), 'have') (('good articles', 'that'), 'Summary') (('that', 'article potential popularity'), 'com') (('It', 'result'), 'predict') (('Business channel', 'best popularity'), 'be') (('don generally t', 'them'), 'see') (('Easy', 'article popularity'), 'help') (('popularity', 'Average popularity'), 'tend') (('that', 'higher popularity'), 'tend') (('read_csv Here we', 'two non preditive url attributes'), 'drop') (('several statistic tests', 'e.'), 'be') (('their', 'machine learning model'), 'win') (('referrence 1 between 1 between 40 images', '25 vidoes'), 'be') (('we', 'data'), 'evaluate') (('this', 'other models'), 'wasn') (('data', '71'), 'base') (('classification task', '51'), 'have') (('respective list', 'alignment'), 'label') (('basically model', 'classes'), 'be') (('which', 'more articles'), 'be') (('variable', 'article'), 'highlight') (('feature', 'asserction'), 'be') (('feature selections technique didn t', 'machine learning models'), 'make') (('Entertaiment also channel', 'popularity based types'), 'be') ", "extra": "['annotation', 'biopsy of the greater curvature', 'test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "appear", "apply", "article", "average", "avg", "balance", "barplot", "become", "best", "bit", "box", "boxplot", "build", "case", "categorical", "channel", "check", "classification", "clear", "column", "compare", "confusion", "consider", "content", "convert", "correct", "create", "criteria", "current", "data", "dataframe", "dataset", "day", "decision", "degree", "depth", "detection", "difference", "distribution", "draw", "drop", "dummy", "effect", "encoder", "encoding", "evaluate", "expected", "experience", "explore", "extract", "feature", "fetch", "field", "file", "find", "following", "forest", "function", "grading", "grouped", "helper", "high", "histogram", "hot", "human", "imbalance", "import", "improve", "increase", "influence", "initially", "input", "intuition", "knowledge", "label", "learning", "length", "line", "linear", "list", "log", "look", "loop", "lot", "lower", "main", "majority", "matrix", "max", "maximum", "mean", "merge", "metadata", "min", "missing", "model", "most", "nature", "need", "negative", "new", "noise", "non", "normal", "normalized", "not", "num", "number", "observation", "order", "out", "output", "pair", "parameter", "past", "people", "performance", "plane", "plot", "png", "point", "position", "positive", "potential", "predict", "prediction", "present", "print", "probability", "problem", "processing", "project", "range", "rank", "ranking", "rare", "ratio", "read", "recommend", "record", "relationship", "remove", "result", "run", "running", "scaler", "scatter", "score", "selection", "sentiment", "set", "several", "similar", "single", "size", "splitting", "standard", "start", "task", "technique", "test", "testing", "text", "those", "through", "timedelta", "title", "train", "training", "transform", "transformation", "tree", "tune", "unique", "url", "v2", "value", "variable", "variance", "vector", "word", "world"], "potential_description_queries_len": 183, "potential_script_queries": ["copy", "norm", "numpy", "probplot", "seaborn", "sklearn"], "potential_script_queries_len": 6, "potential_entities_queries": ["dataset", "import", "linear", "potential", "url"], "potential_entities_queries_len": 5, "potential_extra_queries": ["biopsy"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 190}