{"name": "bayesian optimization of xgboost parameters ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "cv is done this section puts its output into log file. Train and validation scores are also extracted in this section. splitlines Comment out any parameter you don t want to test Define all XGboost parameters verbose_eval 10 This line would have been on top of this section with capture as result After xgb. I d say that 15 20 is usually adequate. Good luck Let me know how it works. AUC will be optimized here. If you commented out any of them above you should do the same here. Note that these are pretty wide ranges for most parameters. For n_iter 25 50 is usually enough. This portions gives the summary and creates a CSV file with results. We can go with separately defined gini scorer and use feval gini below but I don t think it makes any difference because AUC and gini are directly correlated. Make sure to install the superb __Bayesian Optimization__ https github. You will probably want to experiment with values in 0. You can simply specify that 10 20 random parameter combinations init_points below be used. com fmfn BayesianOptimization library. 7 probably not for python 3 contextlib. In a production version you should uncomment the first line in the section below and comment out or delete everything else. This is set to prevent them from showing on screen. Commenting out the capture so there will be no record of xgb. maximize line below is specifying. Note that the learning rate eta is set to 0. stdout out yield out finally sys. The real code starts here. These are the parameters and their ranges that will be used during optimization. I am doing a stratified split and using only 25 of the data. The exact combination of parameters determines exploitation vs. stdout try out StringIO StringIO sys. Note that a number of options must be the same for each parameter and they are applied vertically. stdout olderr oldout out 0 out 0. Note the diff part in the printout below which is the difference between the two scores. Large diff values may indicate that a particular set of parameters is overfitting especially if you check the CV portion of it in the log file and find out that train scores were improving much faster than validation scores. maximize init_points 10 n_iter 50 acq ei xi 0. In my version of sklearn there are many warning thrown out by the GP portion of this code. I am doing only 2 initial points which along with 8 exploratory points above makes it 10 random parameter combinations. Define cross validation variables that are used for parameter search. com fmfn BayesianOptimization blob master examples exploitation 20vs 20exploration. maximize init_points 10 n_iter 50 acq ucb kappa 10 XGB_BO. That is certainly not optimal but it will make the search go faster. If you have a special relationship with your computer and want to know everything it is saying back you d probably want to remove the two warnings lines and slide the XGB_BO line all the way left. exploration https github. XGBoost outputs lots of interesting info but it is not very helpful and clutters the screen when doing grid search. print file log_file for line in result 1 print line file log_file log_file. DMatrix train label target XGB_BO. Scaling is really not needed for XGBoost but I leave it here in case if you do the optimization using ML approaches that need it. If you repeat this run new output will be added to it Load data set and target values We really didn t need to load the test data in the first place unless you are planning to make a prediction at the end of this run. Keep in mind that in such a case you must comment out the matching lines in optimization and explore sections below. It turns out that Kaggle does not have cStringIO so I will comment out this portion. However I like to try couple of high and low end values for each parameter as a starting point and after that fewer random points are needed. So we will run XGboost CV with verbose turned on but will capture stderr in result 0 and stdout in result 1. Obviously this is done to make sure that this notebook can run to completion on Kaggle. This portion of the code is not necessary. flush Define the log file. maximize init_points 10 n_iter 50 acq ucb kappa 1. This will be used to capture stderr and stdout without having anything print on screen. collect dtrain xgb. We will extract the relevant info from these variables later and will print the record of each CV run into a log file. That s what the XGB_BO. There are several commented out maximize lines that could be worth exploring. It is tough to know which would work better without actually trying though in my hands exploitation with expected improvement usually works the best. This line is needed for python 2. contextmanager def capture import sys from cStringIO import StringIO olderr oldout sys. splitlines out 1 out 1. 05 range but beware that it will significantly slow down the process because more iterations will be required to get to early stopping. They must match the parameters that are passed above to the XGB_CV function. Each parameter has its own line so it is easy to comment something out if you wish. Doing 10 fold instead of 5 fold cross validation will also result in a small gain but will double the search time. ", "id": "tilii7/bayesian-optimization-of-xgboost-parameters", "size": "3874", "language": "python", "html_url": "https://www.kaggle.com/code/tilii7/bayesian-optimization-of-xgboost-parameters", "git_url": "https://www.kaggle.com/code/tilii7/bayesian-optimization-of-xgboost-parameters", "script": "matthews_corrcoef sklearn.metrics XGB_CV cross_val_score bayes_opt StratifiedShuffleSplit scale_data MinMaxScaler numpy capture StringIO print_function cStringIO pandas BayesianOptimization log_loss __future__ roc_auc_score sklearn.cross_validation StratifiedKFold sklearn.preprocessing load_data xgboost ", "entities": "(('you', 'same'), 'do') (('Train scores', 'also section'), 'extract') (('these', 'pretty wide most parameters'), 'note') (('learning rate', '0'), 'note') (('it', 'grid when search'), 'output') (('they', 'parameter'), 'note') (('This', 'screen'), 'set') (('that', 'parameter search'), 'cross') (('section', 'log file'), 'do') (('you back d', 'XGB_BO line'), 'have') (('anything', 'screen'), 'use') (('XGboost 10 line', 'xgb'), 'comment') (('train scores', 'validation much faster scores'), 'indicate') (('AUC', 'difference'), 'go') (('exact combination', 'exploitation'), 'determine') (('notebook', 'Kaggle'), 'do') (('So we', '0 result'), 'run') (('so I', 'portion'), 'turn') (('you', 'sections'), 'keep') (('portions', 'results'), 'give') (('that', 'optimization'), 'be') (('it', '8 exploratory points'), 'do') (('You', '0'), 'want') (('diff part', 'two scores'), 'note') (('that', 'it'), 'need') (('work', 'usually best'), 'be') (('you', 'something'), 'have') (('you', 'below everything'), 'uncomment') (('that', 'XGB_CV above function'), 'match') (('portion', 'code'), 'be') (('fewer random points', 'starting point'), 'like') (('more iterations', 'early stopping'), 'range') (('Doing', 'search time'), 'result') (('We', 'log file'), 'extract') (('you', 'run'), 'add') (('I', 'data'), 'do') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["blob", "case", "check", "code", "comment", "computer", "contextmanager", "could", "cv", "data", "def", "diff", "difference", "directly", "double", "end", "everything", "expected", "experiment", "explore", "extract", "faster", "file", "find", "fold", "gini", "grid", "high", "https github", "import", "improvement", "indicate", "info", "kappa", "label", "learning", "leave", "line", "load", "log", "match", "matching", "mind", "most", "my", "need", "new", "no", "not", "notebook", "number", "optimization", "out", "output", "overfitting", "parameter", "part", "place", "point", "prediction", "prevent", "print", "production", "python", "random", "range", "record", "relationship", "remove", "result", "run", "scorer", "screen", "search", "section", "set", "several", "sklearn", "slide", "something", "special", "split", "summary", "target", "test", "think", "train", "try", "validation", "version", "warning", "work"], "potential_description_queries_len": 92, "potential_script_queries": ["numpy", "xgboost"], "potential_script_queries_len": 2, "potential_entities_queries": ["most"], "potential_entities_queries_len": 1, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 94}