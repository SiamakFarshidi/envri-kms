{"name": "tensor p r ocessed a look at tpu responses ", "full_name": " h1 Profiling the TPU Users h1 Executive Summary h3 Importing required packages h3 Reading the data h2 Conclusion h2 References ", "stargazers_count": 0, "forks_count": 0, "description": "com paultimothymooney 2020 kaggle data science machine learning survey5. 3 Colab and Kaggle are the choice for TPU Compute Go back to Executive summaryTPU is a Google designed hardware acclerators and are available in all its cloud platforms like Colab AI notebooks. While SQL C and C make it in top 5. That might be the reason for SQL being used by 50 of TPU users. The adoption is higher among the young professionals 1 between 1 3 years of experience and students who might have completed either Masters or Bachelors degree 4 who are keen to learn new technology in the field and stay relavant. Executive SummaryAccording to TPU launch page https www. 4 More than half of TPU users have either Bachelors or a Masters degree Go back to Executive summary Irrespective of the education qualification TPUs are used for solving the problems. Therefore lets look specifically at computer vision algorithms to get a clear picture. com jpmiller some best practices for analytics reporting2. But we see precisely that regression and tree based models are adopted widely and models are built using those models before jumping into neural network based models. The change comes in the 2nd position where Kaggle learn courses are prefered over the other MOOC platforms. 2 TPU Usage is spread across all Age Group Go back to Executive summaryTPU Users are spread out across all age group and we have 5 of kagglers in age group above 55 years who have adopted TPU for their deep learning requirements. 8 whereas on the other hand it is seen that 42 of the users have used GPU s. It has already got some amazing topics to explore and do not forget to check it out. ConclusionRelatively new to the deep learning world TPUs appear to be giving deep learning researchers ML engineers and students the required computational efficiency for problems that would have taken days to train on a CPU or even a GPU. They were first developed and used by Google to process large image databases such as extracting all the text from street view. com product feedback 129828 TPU s tensor proccessing units are hardware acclerators specialized in deep learning tasks and provide significantly more computational power for mixed percision and matrix multiplications. ai products accelerator costs around 60 but due to the availablity TPU in cloud platforms like Kaggle and Colab most of the data scientist who are just starting their journey remember that our target TPU users are either students or a data scientist starting their career use these platforms rather than buying and setting up the hardware. Availability of GPU s in Colab Kaggle and a surge in ML competition related to image classification NLP makes it a popular tool of choice for kagglers. The graph seems to support our earlier hypothesis that young professionals having 1 2 years of experience are adopting TPU for their ML projects. 4 Data Scientist prefer free cloud resources over setting up their own DL machine Go back to Executive summary TPU Prototype Coral Accelearator https coral. Both Kaggle and Google colab offers eight TPU cores. While Sklearn might be the most used framework 10 for machine learning problems Tensorflow Keras Pytorch frameworks are the go to frameworks for deep learning problems and close to 90 of data scientist would prefer to use Python 9 as a language to code. com tpu utm_source google utm_medium cpc utm_campaign japac IN all en dr bkws all pkws trial e dr 1009137 utm_content text ad none none DEV_c CRE_396375661581 ADGP_Hybrid 20 7C 20AW 20SEM 20 7C 20BKWS 20 20T2 20 7C 20EXA 20 7C 20AI 20Platform 20 7C 20M 3A1 20 7C 20IN 20 7C 20en 20 7C 20cloud 20tpu 20 7C 20pricing 20 20PKWS KWID_43700049545260967 kwd 842288534372 userloc_1007809 network_g utm_term KW_gcp 20cloud 20tpu 20price gclid EAIaIQobChMIr8XCtMOh7QIV6INLBR1XPgs6EAAYASAAEgJtbfD_BwE While the percentage of TPU users remains at 4 it is to be noted that more than half of the respondents have also used GPU computing for their data science projects which suggests that the choice of hardware acclerators depends upon the problem at hand. GPUs dont have this advantage and run out of memory while doing the calculations. 3 India is word 1 in TPU adoption Go back to Executive summaryAccording to the survey India has seen a rapid adoption of the TPU s with over 33 of the response followed by US. This percentage may grow in the coming years with more and more TPU specific competitions datasets are added to the Kaggle datasets. Also to troubleshoot problems and help the community Kaggle has created a separate forum for TPU https www. It is also possible that the responses would have been provided keeping in mind the overall use of the algorithm rather than being TPU specific. While the data used for deep learning tasks are typically big running to few GBs they could be stored either in enterprise cloud databases and one might require SQL to query and fetch them. While the difference is narrow for TPU users a margin of 20 is noticed when overall response is considered between SKlearn and TF. 9 GANs for CV Embeddings for NLP Go back to Executive summary42 of the data scientist frequently used GANs for computer vision followed by equal number of people using image segmentation and classification methods. Using TPUs in Kaggle comes with a time restriction of 30 hrs per week and going by this calculation it is seen that the minimum number of times a data scientist has used the tpu is between 2 5. Data analytics is a rapidly changing field with lot of concepts techniques discovered everyday. While the survey has provided a peek into the world of data scientist in this notebook I am going to specifically focus on TPU users those who have selected TPU s as answer for the question Which type of specialization hardware do you use on a regular basis. GANs are a clever way of training a generative model by framing the problem as a supervised learning problem with two sub models the generator model that we train to generate new examples and the discriminator model that tries to classify examples as either real from the domain or fake generated 6. 2 Minimum TPU Usage is between 2 5 times Go back to Executive summary It is not very clear from the question definition as to what is the time interval used for this frequency is it monthly weekly daily. On text based problems pre BERT technique of word embeddings are widely used while a quarter of data scientists prefer BERT GPT 3 as their first choice. This custom designed machine learning ASIC also powers Google products like Translate Photos Search Assistant and Gmail. This also shows that irrespective of the profession and education data scientists are not restricted to a specific technology and show interest to expand outside their formal education. 7 Coursera and Kaggle Learn are the preferred MOOC Go back to Executive summaryIn line with the global results Coursera is preferred MOOC among the TPU Users. Importing required packages Reading the data The percentage of people using tpu is very nascent and it contributes to 4. These two countries contribute to a significant number of active Kaggle users and hence its very natural to expect them in top 5 list. The purpose of the notebook is to get an insight of the TPU users and answer few questions like Who are the TPU users Are they from all over the world or they belong to typical age group come from a specific background etc Are TPU Users also use GPU If so what percentage From which country the majority of TPU usage comes from What are the common problems for which the TPUs are used How does it differ from a regular user After reading this notebook I hope that you will be inspired to try out interesting problems using TPUs. In our algorithmic analysis one this is clear that data scientists prefer to start with good old established approaches like tree based regression embeddings before adopting neural network based models like CNNs BERT. More than 80 5 of them would also have used GPU for hardware acclearation along with the TPUs who would have completed courses 11 in Coursera and Kaggle Learn to pick up deep learning skills and lay their hands on with either TPU GPU. The percentage of TPU users might be very low 4 according to the survey for this year but it will keep growing in the coming years and it is set to transform the field forever. GPUs and TPUs are both hardware accelerators which are used to speed up complex math computations in a deep learning problem. com what are generative adversarial networks gans Q12_Part_2 Which types of specialized hardware do you use on a regular basis Select all that apply Selected Choice TPUs For how many years have you used machine learning methods Select the title most similar to your current role or most recent title if retired Selected Choice Approximately how much money have you or your team spent on machine learning and or cloud computing services at home or at work in the past 5 years approximate USD Which of the following machine learning frameworks do you use on a regular basis Select all that apply On which platforms have you begun or completed data science courses Select all that apply Selected Choice Coursera Which categories of computer vision methods do you use on a regular basis Select all that apply Selected Choice General purpose image video tools PIL cv2 skimage etc Which of the following natural language processing NLP methods do you use on a regular basis Select all that apply Selected Choice Word embeddings vectors GLoVe fastText word2vec. They have a wide application in image to image translation text to image translation and wherever the data is less to train an adversarial model could be developed to synthesis new images for training. Source https cloud. This has also acclerated the developments in various fields like training on huge text corpus computer vision problems in medicine and biology. com python choropleth maps 6. This is consistent with the overall view as well. This is the reason we see that 82 of the responses is shared between GPU and TPU compute. When it comes to TPU specific users the years of experience is usually around 1 2 years which means professionals and young graduates who are starting their career as a data scientist are adopting the hardware accelerator for their ML requirements. The result is consistent and reflects the overall trend when it comes to language of choice Python and SQL are used by data scientist. com kabure extensive usa youtube eda3. A quarter of these users are from India 3 and an typical user is likely to have used TPU 2 3 times 6. Interactive notebooks rich datasets to try out our concepts and availability of resources like TPU and GPU might be the reasons why the Kaggle learn courses are very popular among data scientists students. This indicates that kagglers are willing to adopt and use new technology and are keeping up with latest technology advancements in the field. This could be the reason why a data scientist using TPU might prefer C or C over R whereas when it is seen for overall data scientist R is preferred over C and C. Easy to use interface free compute resources less downtime might be few reasons for this preference. 5 Python C C are the go to languages for TPU Compute Go back to Executive summary When it comes to programming language of choice we have a clear winner for TPU specific problems. https machinelearningmastery. There is a marginal difference between the users who have used GPU s to those users who have not used either a GPU or TPUs. Profiling the TPU UsersIn yet another year of the comprehensive kaggle data science and machine learning survey there were 20 036 responses from 171 countries. Demographic Analysis1. This might be because of inherent skewness in the survey which is tilted towards bachelors and masters degree. Therefore it is no surprise to note that the top 4 platforms are from Google. Since both are here to solve the same problem it is natural to think GPU as an alternative and use them. The main advantage of using TPU is that it is run on cloud server and it lets to scale operations across different machines. Therefore it is important to keep up with pace adopt and practise new technology. Python frameworks like Tensorflow and Pytorch have XLAs and enable mixed percision which makes it easier to adapt and code for using TPUs. Though there are newer and better models growing day by day Data scientists always prefer to start with either a regression 12 or tree based models before trying their hands on neural networks. Inline with this it is no surprise that the highest percentage of response comes from people with 1 2 years of experience. It is interesting to note that almost an equal percentage of users are using SKLearn for their modelling using TPU compute. Those with masters degree and above 50 have shown an increasing adoption compared to other degrees. Both have the capability to increase the computational performance of problems involving large martrices and hence could be used depending upon the size and type of problem. In terms of percentage Colab slightly has an edge over Kaggle followed by Google Cloud AI notebooks. com rohanrao a deep learning of deep learning4. 8 Regression and Tree based models remains the top choice of algorithms Go back to Executive summaryTPUs are used for training deep neural networks and we expect either RNNs or CNN based networks to be prefered over either regression or tree based networks. 6 Tensorflow Keras has a edge over Pytorch for TPU Compute Go back to Executive summary Tensorflow Keras has an slight edge over Pytorch when it comes to framework of choice for TPU users. Python bindings enable one to use C or a C code to run in Python if there are no built in modules available in Python or to speed up this. 93 of the data scientist code using Python. Time and passion are the only investment for learning. Kaggle learn courses are small micro courses on a specific skill topic with notebooks available for reading and practise. Currenly the learning stack ranges from basic introduction to Python to Reinforcement learning. To approximate the results I have considered this response to the question as minumum frequency of using a TPU. 1 Young Professionals Students are the main users of TPUGo back to Executive summaryTPU hardware is relatively new and came to widespread usage approximately an year ago. Being a product of Google TPU s are widely available 7 and used in Google platforms like Colab and Kaggle by majority of data scientist where one is provided with a limited time 30 hrs weeky in Kaggle while in Colab free account should not exceed more than 12 hrs of continuous usage. 1 TPU Users have also worked on problems with GPU compute Go back to Executive summaryLets filter out only the TPU users for this notebook and analyse them. This has enabled begineers and students alike to access these resourses with zero investment 8. But this could not turn our to be true for either a computer vision or a NLP problem 13 where techniques like GANs and BERT are increasingly used. com tags tpu to encourage its growth and build better optimized networks. 40 of the TPU user would be either a Student or a data scientist. ", "id": "gsdeepakkumar/tensor-p-r-ocessed-a-look-at-tpu-responses", "size": "15375", "language": "python", "html_url": "https://www.kaggle.com/code/gsdeepakkumar/tensor-p-r-ocessed-a-look-at-tpu-responses", "git_url": "https://www.kaggle.com/code/gsdeepakkumar/tensor-p-r-ocessed-a-look-at-tpu-responses", "script": "seaborn numpy matplotlib.pyplot create_dict pandas plotly.express tpu_plot_bar_graph ", "entities": "(('two countries', 'top 5 list'), 'contribute') (('less downtime', 'few preference'), 'be') (('9 GANs', 'image segmentation methods'), 'go') (('machine learning ASIC', 'Translate Photos Search Assistant'), 'design') (('discriminator that', '6'), 'be') (('who', 'TPU'), 'use') (('learn Kaggle courses', 'available reading'), 'be') (('who', 'field'), 'be') (('Data scientists', 'neural networks'), 'prefer') (('Python bindings', 'this'), 'enable') (('also irrespective', 'formal education'), 'show') (('we', 'TPU specific problems'), 'be') (('129828 tensor', 'mixed percision'), 'feedback') (('also responses', 'algorithm'), 'be') (('free account', 'continuous usage'), 'be') (('competitions more TPU specific datasets', 'Kaggle datasets'), 'grow') (('one', 'them'), 'store') (('42', 'GPU s.'), '8') (('SQL', 'TPU users'), 'be') (('Both', 'problem'), 'have') (('courses', 'data scientists very students'), 'be') (('embeddings vectors', 'Selected Choice Word'), 'com') (('This', 'medicine'), 'acclerate') (('highest percentage', 'experience'), 'be') (('close to 90', '9 language'), 'be') (('3 typical user', 'TPU'), 'be') (('that', 'CPU'), 'appear') (('when overall response', 'SKlearn'), 'notice') (('young professionals', 'ML projects'), 'seem') (('when it', 'TPU users'), 'have') (('I', 'TPU'), 'approximate') (('Therefore lets', 'clear picture'), 'look') (('RNNs based networks', 'regression'), 'remain') (('learning Currenly stack', 'Reinforcement learning'), 'range') (('rapid adoption', 'US'), 'be') (('Kaggle colab', 'TPU eight cores'), 'offer') (('kagglers', 'field'), 'indicate') (('GPUs', 'calculations'), 'have') (('modelling', 'TPU compute'), 'be') (('it', 'different machines'), 'be') (('choice', 'hand'), 'cpc') (('who', 'learning deep requirements'), 'spread') (('widely quarter', '3 first choice'), 'use') (('target', 'rather hardware'), 'cost') (('They', 'street view'), 'develop') (('Those', 'other degrees'), 'show') (('who', 'GPU'), 'be') (('young who', 'ML requirements'), 'be') (('TPU Compute', 'Colab AI notebooks'), 'be') (('you', 'regular basis'), 'go') (('when it', 'C'), 'be') (('Coursera', 'TPU preferred Users'), 'be') (('data scientists', 'CNNs BERT'), 'be') (('learn courses', 'MOOC other platforms'), 'come') (('This', 'zero investment'), 'enable') (('Time', 'only learning'), 'be') (('TPUs', 'problems'), 'have') (('which', 'bachelors'), 'be') (('based widely models', 'based models'), 'see') (('very it', '4'), 'import') (('it', 'them'), 'be') (('Python', 'data scientist'), 'be') (('community', 'TPU https www'), 'troubleshoot') (('Young Professionals 1 Students', 'relatively widespread usage'), 'be') (('it', 'TPUs'), 'have') (('TPU 1 Users', 'them'), 'work') (('wherever data', 'training'), 'have') (('com tags', 'better optimized networks'), 'tpu') (('This', 'overall view'), 'be') (('82', 'GPU'), 'be') (('Therefore it', 'new technology'), 'be') (('It', 'it'), 'get') (('Data analytics', 'concepts techniques'), 'be') (('40', 'TPU user'), 'be') (('it', 'field'), 'be') (('Colab', 'Google Cloud AI notebooks'), 'have') (('you', 'TPUs'), 'be') (('hardware which', 'learning deep problem'), 'be') (('Data 4 Scientist', 'Executive summary TPU Prototype Coral Accelearator https back coral'), 'prefer') (('NLP 13 where techniques', 'GANs'), 'turn') (('it', 'frequency'), 'go') (('top 4 platforms', 'Google'), 'be') (('minimum number', 'data tpu'), 'come') (('it', 'kagglers'), 's') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["accelerator", "account", "active", "advantage", "age", "algorithm", "answer", "appear", "application", "apply", "background", "basic", "best", "build", "calculation", "check", "choice", "classification", "classify", "clear", "close", "cloud", "code", "colab", "community", "competition", "compute", "computer", "could", "country", "current", "custom", "cv2", "data", "day", "degree", "difference", "domain", "edge", "education", "efficiency", "en", "enable", "equal", "even", "expand", "experience", "explore", "feedback", "fetch", "field", "filter", "following", "framework", "frequency", "generate", "generated", "generator", "google", "graph", "group", "grow", "growth", "half", "hand", "help", "hope", "https machinelearningmastery", "image", "increase", "interest", "interval", "investment", "kaggle", "language", "launch", "learn", "learning", "line", "look", "lot", "main", "majority", "margin", "math", "matrix", "medicine", "memory", "might", "mind", "minimum", "mixed", "model", "modelling", "most", "network", "neural", "new", "no", "none", "not", "notebook", "number", "out", "overall", "page", "past", "people", "per", "percentage", "performance", "position", "power", "pre", "problem", "processing", "product", "provide", "purpose", "python", "query", "question", "reading", "reason", "regression", "response", "result", "rich", "role", "run", "running", "scale", "science", "segmentation", "selected", "separate", "set", "similar", "size", "skimage", "speed", "spread", "stack", "start", "sub", "summary", "supervised", "support", "surprise", "survey", "target", "team", "technique", "technology", "tensor", "text", "think", "those", "time", "title", "tool", "topic", "train", "training", "transform", "tree", "trend", "trial", "try", "turn", "type", "up", "usage", "user", "video", "view", "vision", "week", "while", "who", "word", "work", "world", "year"], "potential_description_queries_len": 184, "potential_script_queries": ["numpy", "seaborn"], "potential_script_queries_len": 2, "potential_entities_queries": ["overall", "summary"], "potential_entities_queries_len": 2, "potential_extra_queries": ["test"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 187}