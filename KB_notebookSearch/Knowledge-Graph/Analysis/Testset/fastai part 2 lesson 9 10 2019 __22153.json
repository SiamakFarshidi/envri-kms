{"name": "fastai part 2 lesson 9 10 2019 ", "full_name": " h2 This is last part of fastai part 1 lesson 9 though it is mixed with my notes eksperiments and what I found usefull if you want the pure version check fastai github or the following link https github com fastai course v3 tree master nbs dl2 h1 Initial setup h2 data h1 Cross entropy loss h1 Basic training loop h1 Using parameters and optim h1 Registering modules h1 nn ModuleList h1 nn Sequential h1 optim h1 Dataset and DataLoader h2 Dataset h1 DataLoader h1 Random sampling h1 PyTorch DataLoader h1 Validation h1 Part 2 04 callbacks h1 CallbackHandler h1 Runner h2 Third callback how to compute metrics h1 Part 3 05 anneal ipynb h1 Initial setup h1 Annealing h1 Part 4 05b early stopping h2 Early stopping h3 Better callback cancellation h3 Other callbacks h3 LR Finder h1 Part 5 06 cuda cnn hooks init h2 ConvNet h2 CUDA h2 Refactor model from above h1 Hooks h2 Manual insertion h2 Pytorch hooks h2 Hook class h2 Other statistics h2 Generalized ReLU h2 Uniform init may provide more useful initial weights normal distribution puts a lot of them at 0 h1 Part 6 07 batchnorm h1 ConvNet h1 Batchnorm h2 Custom h1 Builtin batchnorm from pythorch h1 With scheduler h1 More norms h2 Layer norm h2 Instance norm h2 Group norm h2 Fix small batch sizes h2 Running Batch Norm h3 What can we do in a single epoch ", "stargazers_count": 0, "forks_count": 0, "description": "5 see code above in _04 try a learning rate at 0. png picture from article about batchnorm. this gives us a eksponential aftagende function and all of this is called linear interpolation and in code is lerp_ We can then use it in training and see how it helps keep the activations means to 0 and the std to 1. now we can just use the ListContainer here to get teh features we want and this means also we now have hooks in it so now we can just go for hooks in each layer m in children of our model ms. export from exp. 5 its defined above check the loss function training loop we just test if the acc is better then 70 we create places for x and y take the lenth of x donda getitem means that when we index into len it returns x i and y i creating a validation and training dataset check that the lenth is right check that the lenth is right check the first 5 values make sure the shape is right make sure the shape is right printing values creating model and we can set them in model and opt since those are the things we return in get_model class creating trainig loop and now we have replaced the to lines of code with one checking the acc score takes a dataset ds and a batch size bs and stores them away iterate through for i in range 0 len self. It will behave a bit like a numpy array in the sense that we can index into it via a single index a slice like 1 5 a list of indices a mask of indices True False False True. now we can go and fit the model and we do 5 epochs and we just chech that the acc. 2 cbfs callbacks functions and sched from above just a class CancelTrainException that inhaied from Exception class pass means that it will not get more opf\u00f8relser then the parant class except so if we call on CancelBatchException this will run. png Group normFrom the PyTorch docs GroupNorm num_groups num_channels eps 1e 5 affine True The input channels are separated into num_groups groups each containing num_channels num_groups channels. Having given an __enter__ and __exit__ method to our Hooks class we can use it as a context manager. Args num_groups int number of groups to separate the channels intonum_channels int number of channels expected in inputeps a value added to the denominator for numerical stability. but insted of that we if the averge moving is 1 we divide it by the momentum and the secound time we take two points divided it by momentum 2 and so on. GroupNorm 6 6 Put all 6 channels into a single group equivalent with LayerNorm m nn. _modules k v we put this in our _modules dirsonary and call it k do whatever the superclass does and here the superclass is an object defalt mode when it isnt defined printing the list if called apon the class now we can define a method called parameters go through everything in _modules in tthis chase is it l1 and l2 and then go through all of their parameters making it a list of layers pytorch are only going to accept something it accept as proper nn. now we can call fit and we will see the same gra as before where the layers means build up and then collapses when we initize it register a forward hook and some function called f we also use self. 5 becouse of the Relu and std is very close to 1 do our hooks note that we use a with block which means make this object Hooks model append_stats and give it this name hooks and when it is finishs it will call something which in our case is __exit__ from Hooks class above do a fit with 2 epochs do our first 10 mean. 1 we can never divide by zero even though v variance is 0. This is defined by hbox softmax x _ i frac e x_ i e x_ 0 e x_ 1 cdots e x_ n 1 or more concisely hbox softmax x _ i frac e x_ i sum_ 0 leq j leq n 1 e x_ j In practice we will need the log of the softmax when we calculate the loss. Basic callbacks from the previous notebook CUDAThis took a long time to run so it s time to use a GPU. our problem is though that our training loop always loop through our training set in order. ModuleList does this for us. shape 0 and for our row indexses it is every singel row we takes so from 0 50000 and target is each column we then take the mean to it all the reason we use input is because it is the negative log likelyhood now we can find the loss by taking the negative log likelyhood with the accatual prediction on the target note we can rewrite the softmax function with the knowledge of reforming the log a b to log a log b and we can see they are the same so the to formulars does the same we can see from above that the code x. other then that we use. 6 and take up 30 procent of our bacthes and fase to will take 70 of our bathes and be a learning from 0. sqrt so if we just set eps to 0. luckily there are another way image. 09 when we try to use in a model but when your doing RNN u have to use something like this same as layernorm but only using 2 3 and not 1 2 3. pass it through our first layer f our model. We can refactor this with a decorator. A hook is attached to a layer and needs to have a function that takes three arguments module input output. Hook classWe can refactor this in a Hook class. device 0 is the GPU number u wonna use if you only have one GPU it is device 0 initiate a model move the model to the device. _modules with a list of all the modules l1 and l2 in this chase __setattr__ goes into self. we have though a problem with batchnorm since it cannot take batches of one or very small ones like 2 4 since at a point it will divide by itself and we will go to infinity. But it can return a lot of things and you can ask for it a lots of times. register_buffer vars since when train on the GPU everything called buffer will be aoutomatic places there aswell start the means on zero tensor and variance on ones tensor caluelate teh mean note 0 2 3 means we averge our these axsis 0 2 and 3. Default 1e 5affine a boolean value that when set to True this module has learnable per channel affine parameters initialized to ones for weights and zeros for biases. so lets try to fix it Generalized ReLUNow let s use our model with a generalized ReLU that can be shifted and with maximum value. Pytorch hooksHooks are PyTorch object you can add to any nn. Part 3 05_anneal. The idea is to use the following formula log left sum_ j 1 n e x_ j right log left e a sum_ j 1 n e x_ j a right a log left sum_ j 1 n e x_ j a right where a is the maximum of the x_ j. c is predefined since we from the beginning defined how many last activation layer need. note we do not use model. nll_loss are combined in one optimized function F. Here the class above TrainEvalCallback use the n_iters in the after_batch function note True is returned so it stops cb_funcs None any callback functions you pass in that is not being defined here new callbacks assign the new callbacks to a attribute with that name. This means also we have four tensors to deal with we are gonna go through all of our layers we do a check to make sure the layer have weight in it update by taking the the gradient to the weight times the learning rate update by taking the the gradient to the bias times the learning rate now we zero those gradients so they are ready to next run the next thing we will do is to make the updata part better and use less code and now we can see the accuracy improved from 0. we don t keep a moving average2. 3 no schedular return always start This monkey patch is there to be able to plot tensors dont think to much about the below code for the combined schedular we are gonna pass in the fases we want so fase one wil be a learning rate from 0. we are gonna try a few things to fix the above problem note we use kwargs since we can get extra arguments from GeneralRelu class note we use kwargs since we can get extra arguments from GeneralRelu class note we use kwargs since we can get extra arguments from GeneralRelu class note we use kwargs since we can get extra arguments from GeneralRelu class better Relu so now we can use subret from the relu which we found was good about 0. GroupNorm 3 6 Separate 6 channels into 6 groups equivalent with InstanceNorm m nn. _modules k v from he code above it would be esayer of pytorch did it for us so the nn. we use the formular E X 2 E X 2 to get the varaince 2. A Hooks classLet s design our own class that can contain a list of objects. so we combine the different schedularsIn practice we ll often want to combine different schedulers the following function does that it uses scheds i for pcts i of the training. register a buffer for sum reister a buffer for kvadratrod so check for minibatches we make a register for counts we do the sum over 0 2 3 dimensions dims and we do x x. NB if you use a Lambda layer with a lambda function your model won t pickle so you won t be able to save it with PyTorch. add_module is the same as self. note we use the outp since it is the last layer we wnat for each layer to calulate the std for the last layer or any module m register forward hook register_forward_hook and pass in a function partial append_stats i which is a call back which will be called when the m. train so we call model. SGD which does the same as our Optimazer in this chase lr 0. so we are now using nearly all of our activations by being cearful about out initialisation and our relu Uniform init may provide more useful initial weights normal distribution puts a lot of them at 0. For that reason we check for best_loss 3 instead of best_loss 10. no_grad for p in model. mom in the updaa_stats function above. Here we store it in opt and now we can use it and here we can see that we get a acc at 0. So this will end up returning 0 5 1 0 and 2 4 this is how we call on all the rows in our target negative log likelyhood input lets look into our predictions range target. This one takes the flat vector of size bs x 784 and puts it back as a batch of images of 28 by 28 pixels We can now define a simple CNN. Bot we see a lot better learning of the fist 10 layers. remove since otherwise if we keep registering more hooks on the same module they are all gonna get called and eventually you are gonna run out of memory note we do this pythosch will outomatically call __del__ for delate when it is done with a module which in return will remove the hook note hook so in our hook we can pop in our to list to store away our mean and std name the to list one for means and one for stds and now we can just append the means for each layer and now we can just append the stds for each layer so now we can just go for hook layer ind children of our model and we are just gonna grap the first 4 layers since it its the conv2d layers that are the most interresting and not hte linear layers and we see it does the same thing as all the other grafs export just a list that contains a lot of usefull features not inparticular needed being clled when we are useing firkant parantesterne for correct operation usage bla bla bla. So this is being done in TrainEvalCallback class above keep track of the numbers of epochs we are doing and kepping track of the learner and note that the learner has not logic in it now we tell each callback what runner they are working with call begin fit go through each epoch set the epoch and call all batches if not means here to keep going because it return False and for no_grad we use begin_validate call all batches if not means here to keep going because it return False it will first stop when and only when True is returned last thing we call after_epoch and after fit to get rid of repetations from CallBackHandler we use __Call__ which note let us tried a object as if it was a function after self after_epoch we are going through all out callbacks getattr get attribute means look inside this object cb and try to find something of the this name cb_name and if you cant find it defound it to None and if it can find cb_name like begin_validate then you can call it note True is returned so it stops and los and metrixces to give all statistic making a property that goes through losses and matrixces and give us the avgerge gennemsnitlige o self. stop defined in the CallBackHandler so lets test it this dictat what order your callbacks run in like this does that you can use self. In order to combine the effects of instance specific normalization and batch normalization we propose to replace the latter by the instance normalization also known as contrast normalization layer label eq inorm y_ tijk frac x_ tijk mu_ ti sqrt sigma_ ti 2 epsilon quad mu_ ti frac 1 HW sum_ l 1 W sum_ m 1 H x_ tilm quad sigma_ ti 2 frac 1 HW sum_ l 1 W sum_ m 1 H x_ tilm mu_ ti 2. because there aren t many samples the variance of one thing is 0. parameters for hooks soeverything in out model with the append_stats print it out to see all the hooks grap a batch of data chech the one batchs of datas mean and std and it is about mean 0 and std about 1 whcich is what we want. So how does that look like and now since we have created it we can use pytorchs dataloader PyTorch DataLoaderNote that PyTorch s DataLoader if you pass num_workers will use multiple threads to call your Dataset. but still there are a better solution and its shown in the code below which is running batch norm Running Batch NormTo solve this problem we introduce a Running BatchNorm that uses smoother running mean and variance for the mean and std. SGD it also handles stuff like momentum which we ll look at later except we ll be doing it in a more flexible way Randomized tests can be very useful. This layer uses statistics computed from input data in both training and evaluation modes. Then use PyTorch s implementation. Here Relu is an activation layer and have no parameters so we just have to linear layers in our difined model above. used in below code but its what happens when you starts the with block defined longer done used in below code when you finish the with block but when called on it will use. note we can say for h in self even thogh h is not in a iteration list in this cell block. this gives us a new matrix that is 8 by 9. randn 20 6 10 10 Separate 6 channels into 3 groups m nn. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link. Question why can t this classify anything Lost in all those norms The authors from the group norm paper have you covered image. modules so we just go through all of the layers self. The cross entropy loss for some target x and some prediction p x is given by sum x log p x But since our x s are 1 hot encoded this can be rewritten as log p_ i where i is the index of the desired target. Let s start with a simple linear schedule going from start to end. ModuleList goes through all the layers and even this can be written esayer so this does the same as SequentialModel class we wrote above pass in some parameter params and a learning rate lr and store those away making step we use no_grad since this is not a part of gradient calulation but just the result from it go through each parameter and update it with the gradient time the learning rate for all of the parameters making zero_grad going through the list of parameters you asked to update and zero those gradients creating model call optimizer class and pass the models paramters. its a Conv2d from the filter nfs i current filter to the next filter nfs i 1 and then the kernel size depends it 5 for the fist layer otherwise it is 3 this is explaned in the picture below last few layers are averge pulling AdaptiveAvgPool2d flattening and a linear layer we can now remove the mnist_view from the previus model since it is a callback. so there a 9 input activations to do out dot product with. train the rest is the same as before here means cb CallBack so all_batches go through every batch the trainig loopp has to loop through every batch and the validation loop has to go throguh every batch these are all the CallBack and for the callback calls we need a callback handler go through the call back and se if we resive a False it means dont keep going any more and then return it we do this for all the callbacks this is just an exsemple of a callback it will use the fit callback set the iterations to 0 and then after every step it will set number of iterations to 1 and print it out and if we get pass 10 iterations we tell the learner to stop here we use the callbacj learn. Let s make our loop much cleaner using a data loader for xb yb in train_dl. 2 still following a cosine. so now we will try to fix it More norms Layer normFrom the paper batch normalization cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small. so it will fire of more processes and each one wil seperated grap stuff from the dataset and then it will collate them afterwoodit usefull if you are opening big jpeg files and all kind of images transformations ValidationYou always should also have a validation set in order to identify if you are overfitting. We will also use it in the next notebook as a container for our objects in the data block API. Shape Input N num_channels Output N num_channels same shape as input Examples input torch. From the histograms we can easily get more informations like the min or max of the activationsand we see in the last layer that over 90 procent of the activation actual zero. training print model. eval before inference because these are used by layers such as nn. train every time we have data. its gonna add up the total matrixces times the batch size we aare gonna use some object to keep track of matricses ans losses one for training and one for valid. lets try to use batchnorm on a small batch sizeso to fix this is to use epsilon eps which just is a very small number u define its normally used to avoid float roundings and we see it typically in the bottum at a division. the name in self doesnt matter raise CancelTrainException so we go back and activiate it in Runner class using an exponential curve to set the learning rate current iteration divided by the maxisimum iteration find learning rate set the leaerning rate in parameters same as we did in run scedular above after each step check if we have done more then maximum iterations 100 and if the loss is much worse then the best we have had this far if either of those happens raise CancelTrainException and stop training check if the current loss is better then the best loss and if it is set the current loss to the best loss the only thing it does is 7 4 2 averge pooling Somewhat more flexible way we create a device be calling torch. training same as before but where torch. The fact that we re passing cb to so many functions is a strong hint they should all be in the same class Runnernew better version of the above code This first callback is reponsible to switch the model back and forth in training or validation mode as well as maintaining a count of the iterations or the percentage of iterations ellapsed in the epoch. Sequential is a convenient class which does the same as the above optimLet s replace our previous manually coded optimization step with torch. It s very important to remove the hooks when they are deleted otherwise there will be references kept and the memory won t be properly released when your model is deleted. A hook will be called when a layer it is registered to is executed during the forward pass forward hook or the backward pass backward hook. com fastai course v3 tree master nbs dl2 Initial setup dataSo now we need a loss function Cross entropy lossFirst we will need to compute the softmax of our activations. for this will be practising godd anniling AnnealingWe define two new callbacks the Recorder to save track of the loss and our scheduled learning rate and a ParamScheduler that can schedule any hyperparameter as long as it s registered in the state_dict of the optimizer. Pytorch also have a class that does the same as DummyModule defined below super setup the _modules in the DummyModule class. and so we loss the randomness of sjoveling it each time. Part 6 07_batchnorm remember that there are to types of numbers in a neural network paramteres thing we learn and activations tings we calulate ConvNetLet s get the data and training interface from where we left in the last notebook. nb_01 import since we know about boardcasting we make a funktion that our tensor x subrat with the mean m and divided with standard diviation s export test if the floats are neer since we cant just compare floats with eachother we made i in last lesson make the model this is the None 1 keepdim True it is to make sure the result will broadcast correctly against x. the fist layer have a std not to far away from 1 but as we whould espect it gets eksponential worse and the last layer in this group is almost 0 which we dont want. But the validation set shouldn t be randomized. Here is an example use 30 of the budget to go from 0. and we dont want to save a history of all of them just to get the averge. when usin RNN batch can also be a problem. l1 dosent start with _ because if it does it maight be _modules or something in python. log Then there is a way to compute the log of the sum of exponentials in a more stable way called the LogSumExp trick. Dataset and DataLoader DatasetIt s clunky to iterate through minibatches of x and y values separately xb x_train start_i end_i yb y_train start_i end_i Instead let s do these two steps together by introducing a Dataset class xb yb train_ds i bs i bs bs DataLoaderPreviously our loop iterated over batches xb yb like this for i in range n 1 bs 1 xb yb train_ds i bs i bs bs. train we replace it with learn. pname in this chase learing rate so if we are trianing in_train we will run our scheduler and set parameters set_param for learning rate in differetn layer groups for linear schedule starting learning rate to end learning like from 1e 01 to 1e 10 now return the learning rate so the start position time the different in the end and start so now to only get posistion pos we return partial and pass the function _inner start and end. we don t average over the batches dimension but over the hidden dimension so it s independent of the batch sizeThought experiment can this distinguish foggy days from sunny days assuming you re using it before the first conv Instance normFrom the paper The key difference between contrast and batch normalization is that the latter applies the normalization to a whole batch of images instead for single ones label eq bnorm y_ tijk frac x_ tijk mu_ i sqrt sigma_i 2 epsilon quad mu_i frac 1 HWT sum_ t 1 T sum_ l 1 W sum_ m 1 H x_ tilm quad sigma_i 2 frac 1 HWT sum_ t 1 T sum_ l 1 W sum_ m 1 H x_ tilm mu_i 2. so the mean get eksponential bigger to they suddenly colaps other that it get slidtly better but not much. bool mask note ater a 100 it is no longer printing them all just 9. in the callbacks defined in AvgStatsCallback. stack just take a bunch of tensor and grue them together we created samplers before so now are gonna use them. but to realy know whats going on we have to see in the activations. note here the training set is shuffled and the validation is not RandomSampler shuffle them as above SequentialSampler order it as above another way of doing the same as RandomSampler another way of doing the same as SequentialSampler same training loop as before Handle batchnorm dropout print model. This makes sure that onces we are out of the with block all the hooks have been removed and aren t there to pollute our memory. has been calulated we can also use register_backward_hook which can be used after the backward are calulated note we use i to nkow which layer we are at. 4 NB pytorch bn mom is opposite of what you d expect note mults are a parameter note we are multiply by tensor of 1 in the last line in forward function which does nothing note adds are a parameter note we are add by a tensor of 0 in the last line in the forward function which does nothing but the are parameters so they can learn they are being updated and note that adds is the same as bias in a linear model. We will calculate and print the validation loss at the end of each epoch. eval so the n_iters which will stop it when we get to 10 iterations wil have to stop. cuda add it to our callback functions grap our model since we use relu and conv2 alot we just make a function for it kernel size 3 ks note this ans stride on 2 are defoults previus when we wrote the transforming of mnist it will only work with it but here we will make a more generel transformation of the data transform the indepened verible X training data for a batch you pass it som etransformation function tfm which its stores away begin batch just replaces the current batch xb with the result of the transformation tfm view_tfm takes and transform the size just to view something at 1 by 28 28 now we can simply append anotheer callback hte numbers of filters per layer now we make a model again the first few layers are for everyone of the filters. Note also here that we only add a little bit of the secound point each time. With the AdaptiveAvgPool this model can now work on any size input image. But most people do it this wayNow our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code Part 2 04_callbacks Factor out the connected pieces of info out of the fit argument list fit epochs model loss_func opt train_dl valid_dl Let s replace it with something that looks like this fit 1 learn This will allow us to tweak what s happening inside the training loop in other places of the code because the Learner object will be mutable so changing any of its attribute elsewhere will be seen in our training loop. but what we do instead is to take each current point multiply it with a momentum mom and add it to the secound point multiplied the 1 mom 1 momentum and therefor we just have one point all the time and we can forget about the rest. cross_entrop and is F. to realy be sure we see the next grafand look at that we now see that in the last layer is lost less then 20 procent of the activations. the first 4 layers and it is bautiful where we see that it doesnt collapse so it should use the fullnes of the activations. so if you are training you model like this it might look like it is training nicely without you knowing that 90 procent o you activations are totaly tabt. GroupNorm 1 6 Activating the module output m input Fix small batch sizesWhat s the problem When we compute the statistics mean and std for a BatchNorm Layer on a small batch it is possible that we get a standard deviation very close to 0. png the the picture above is the first layer. so this is why we need it this does the updates and the parameters use use the function. In PyTorch this is already implemented for us. ipynb Initial setuphere we will se that the first batches is every. no_grad goes through the validation set note also that we dont use backward since we isnt training and the validation set insted we just keep track on the losses and track on the accuracy how big is our dataloader an dhow many batches are there and we devide the nv with the total loss since we dont have to do the backward pass so we have twice as much space therefor we can multipy the batch size with 2. log just takes the log then sum then the exponeltial so in Pytorch we have LogSumExp which we implement as so and the reason we need this trick is because when we take somthe to the power of x. note we did it after the keiming inisilatation of the first layer so now we dont see the first pich where everything went to shit before. we want to take the mean of that but in order to do that in pytorch we have to convert it to a float. exp we can get exstreme big numbers and every big number we have as a float will be inacurrat but the compter think it is the same even though they can be 1000 or more apart lets find the maxisimum lets subrat it from all of w\u00f3ur x ses and then lets do the LogSumExp and in the end we add the maxsimum number to the ligning so it gives the same number pytorchs LogSumExp compare the selfwritten log_softmax with pytorch this is called F. We can use it to write a Hooks class that contains several hooks. note this is our pre batchnorm we can now see the acc. which means we can acces it in append_stats note self. But let s see if we can make things simpler and more flexible so that a single class has access to everything and can change anything at any time. bs taking one batch size bs at the time and each time we go through we will yield our dataset at a index starting at i til i bacth size bs yield means it like a function that dosent return one thing ones. sum so 2 note formular is used so we have to devide the total number in the minibatch numel byt the number of chanels nc and we take the lerp_ the eksponential moving averga of the sums and we take the lerp_ the eksponential moving averga of the sqrs and take a eksponential moving averge of the counts of the batch sizes in the forward function we dont divide by the batchs std and we dont subrat the batch mean but use the moving averge statistic at training time aswell note we divide by the dbias amount and for the variance sqrs divided by counts minus. It returns a function that takes a pos argument going from 0 to 1 such that this function goes from start at pos 0 to end at pos 1 in a linear fashion. Part 4 05b_early_stopping A hidden layer in an artificial neural network is a layer in between input layers and output layers where artificial neurons take in a set of weighted inputs and produce an output through an activation function Early stopping Better callback cancellationso the code is almost the same as before but with a few different things se the comment Other callbacks LR FinderNB You may want to also add something that saves the model before running this and loads it back after running otherwise you ll lose your weights NB In fastai we also use exponential smoothing on the loss. doesnt realy work for classification batch size 2 note we get an acc. note that the mean starts on 0 and the std starts on 1 and note also that the training has gotten rid of the pich and collapses from before Builtin batchnorm from pythorch With schedulerNow let s add the usual warm up annealing. check if the key like self. Hooks don t require us to rewrite the model. to is from pytorch and it means that it moves some parameters to a device remember that run is a class longer up in the code and xb and yb are defined also but ere we just move them to the device another way of doing this is use pytorchs version but here we use only one device Somewhat less flexible but quite convenient note we dont have to move it to a device we just use. for s in sampler means that we are gonna loop through the samples in above like 6 1 7 8 2 9 and so on. 6 following a cosine then the last 70 of the budget to go from 0. So we will start merging some of the parameters the things we want to keep is epochs the rests we want to put in one. __setattr__ and move relu to functional Registering modulesWe can use the original layers approach but we have to register the modules. gamma and beta are learnable per channel affine transform parameter vectorss of size num_channels if affine is True. sum 1 keepdim True. 3 insted create a function so it is easayer to change the learning rate Recorder records the learning rate given in the hyper parameters given in the below code set the learning rates and losses to emty at the beginning so after each batch as long as we are training will return False if training and True if note training there we use if not self. train before training and model. in other words if we get the first batches right the model will get a much better scc score. since we dont pass pos it just takes position a decorator is a function that returns a function the annealer decorator does the same as the partial shift tab works too in Jupyter sched_lin linear schedular and pass the learining as 1 to 2 and ask what should the learning rate be 30 through training note pos 0. n and now we dot hee same as above lets take 10 values from training set here we use shuffling where we set it to False so we dont shuffle and we see that it is ordered here we use shuffling where we set it to True so we do shuffle and we see that it is not ordered create tensorser grap the x and y s and stack them up. so we use 5 or 7 to slide over the full matrix for the fist layer instead. So we can use it for our log_softmax function. so optinen one is to use a higher epsilon value in BatchNorm because he formular is x m v self. Note that the formula log left frac a b right log a log b gives a simplification when we compute the log softmax which was previously defined as x. since most of them culd have a zero gradient that this point. the name in self doesnt matter except so if we call on CancelBatchException this will run. remove function defined here. Basic training loopBasically the training loop repeats over the following steps get the output of the model on a batch of inputs compare the output to the labels we have and compute a loss calculate the gradients of the loss with respect to every parameter of the model update said parameters with those gradients to make them a little bit better Using parameters and optimParametersUse nn. Question Are these validation results correct if batch size varies get_dls returns dataloaders for the training and validation sets Answer it is an incorrect way it is done since if we try to devide with a batch size of first a 10000 and then 1 we get differeent outcomes. A simple Callback can make sure the model inputs and targets are all on the same device. which is our maxisinum y value lets create a class that put together training set and validation set. General equation for a norm layer with learnable affine y frac x mathrm E x sqrt mathrm Var x epsilon gamma beta The difference with BatchNorm is1. And therefor have a combined class for both let define training and validation sets and store them away and thats it we make it optinal to pass in c and we pass None right now because we will use it later but for conviense let make it eseay to grap them from the class now lets use DataBunch and put all our data in a veriable called data and we pass in c which will be used later the input number is size of the training data data. this is now a generaric get cnn model function that return s sequential model with some arbitory set of layers and some arbitory layers nfs lets put the model and the layers intot he function below grap optmisation function grap the optimazer grap our learner note we see both our filters 8 16 32 32 and out kernel size where the first sis 5 and the for the rest it is 3 veiw model train model new model class make a list to use for act_means for _ in layers means a list for every layer make a list to use for act_stds for _ in layers means a list for every layer we have built a SequentialModel before so we will reuse the first to lines go through each layer set x to the current layer of x graps the mean of the output and put it in act_means grap the std of the output and put it in act_stds makae model learning model fit model so the means for every layers hit a pich in the beginning. sub_ means 2 note fomular is used in chase we get the first minibatch that is close to 0 and we dont want that to destoy everything so if you having seem more them 20 items yet just clamp the variance to be no smaller then 0. at the start of an epoch we will reset the statistics and after we got the loss calulated. We will implement an other way later. sothe consurn is that there are a lot of parameters and we dont know if all of the layers get better or just some of them. relu x if you want to subrat something go do that if you want to use maksimum value go do that uniform boolien so some people say uniform is better then normal so it is an optinal see buttom of this part when used since our relu now can be negative since we can subrat and use leak argurments in GeneralRelu we set in 7 note we now use the middel two histrogram bins instead of the first to since we can take negative numbers so now lets use shedular look longer up in this notebook if you dont rember what it is grap our data as before same as before learning rate 0. to about 20 because if the small batch size SO THERE ARE TREE CONCEPT TO THIS IDEA 1. and which we can transform to the 8 1. note this is an explanation to the code above the fist filter is 8 and thereby we make a 3 by 3 matrix to slide over the full matrix. in_train it will append the current learning rate since there are many learning rates we just take the last one therefor we use 1 in param_groups and lr to get the learning rate and append the current loss then we tell it to plot the learning rates and plot the losses it is a good idea to schadule all the things we want to apply on the model like dropouts momentun and learning ratess since we want to change them as the model improves pass in a sched_func and a parameter pname to shedule param_groups same as layer groups in fastai so it group different layers together so they can get a different learning rate the sched_func takes a singel argument which number of epochs divided by total epochs note this will be a float and the result of this will set the hyper parameter pg self. we usally calulate the averge by taken the first 5 point and then the next 5 points and so on like the picture above. so this means that our final layer gets alomost no activations and basic no gradiants. 5 and we can handle leaking leak. And maybe we want a limit so we can use maximum value maxv so if you pass leak from GeneralRelu it will use leaky_relu otherwise we will use normal relu F. 9375 which is okay and now since have impimentet the optimazer class we can just use pytorchs optim. but it is clear that the first pich leaves out model in a very sad place and we clealy see the result of it just look realy bad see the first 10 means here we see the means getting close or are realy close the 0 which is what we want see the std for the fiirst 10 batches we see a problem. So it s best to give a name to the function you re using inside your Lambda like flatten below. Hooks Manual insertionLet s say we want to do some telemetry and want the mean and standard deviation of each activations in the model. plot means plot stds so what we can do is to modufy our append_stats so it also got a histogram histc of our activations histc isn t implemented on the GPU pop them into 40 bins between 0 and 10 note there are no negatives becouse of Relu same as before look up for detais same same Thanks to ste for initial version of histgram plotting code note that most of the histogram is where the yellow is and that is what we realy care about to see how much ylleo there are lets take the first 2 histragrams bins are near 0 so lets get the sum of those two divided byt he sum of all if the bins this gonna tell us what procent of the activations are near or close to 0. ds so loop through from 0 to the size of the dataset so from 0 til 50000. This way we will avoid an overflow when taking the exponential of a big activation. log_softmax and we can see it is the same as our loss so we make a function that can find our accuracy we did in part 1 by hand but we use argmax here that means that we take the highest number in out softmax and the index of that is out prediction and we check if that is to the accuracal yb. step here we can see how they implemented it thoug note that they alson have momentum and weight decay and more of which we will implement later greate a get model where we create the model with optim. parameters from the DummyModule below so we start by defineing tha every time we define a veriable like l1 or l2 in this chase to a linear so we now want to update the self. since we created a validations set that we used in the fit and so we got 50 hidden layer and batch size on 64 we define c as the last layer of activattions we need. but note we also have ListContainer of which we use and it does it. so by makking eps bigger we make sure we get further and further away for our fomular to crachs. But here we just see it as a very good hyperparameter that we should use. And note that the learner has no logi itself now lets create the learn veriable where we pass in the functions from above training loop it is the same as before thought we replace model. tHis givea an out of 9. so to see if something is bad we will see in the activaitions and see how many of them are realy realy small and how well is it realy getting everything activated Other statisticsLet s store more than the means and stds and plot histograms of our activations now. ds i so now we got a list of tensors and we need a way to collate them together into a singel per of tensors and the collate function above does this now we can create our to samplers now we can create our to dataloaders. To refactor layers it s useful to have a Lambda layer that can take a basic function and convert it to a layer you can put in nn. So all we are left with is a mean for each chanel filter keepdim True means its gonna leave an empty unit axisis in position 0 2 3 so it will still boardcast nicely calulate the veriance var save the mean of the last few running batches deeper explornation for lerp_ see pictures below mom momentum save the veriance an of the last few running batches we save those since we doing training chould lose some importen information so we just save the lave few runnings the be save we get the means and veriance from update_stats function above subrat by the mean and divid by the kvadratrod of the veriance and now we multiply mults by the x and add it with adds note we are usin the bottum formular in the picture above create new conv layer note bn BatchNorm layer No bias needed if using bn if we do use batchnorm we dont need bias since adds is a bias uses the class batchnorm above same as before so layernorm is almost the same as batchnorm just without the running averge so instead we make an averge on one image and not call of them though it doesnt realy work this can be seem by the acc. The mean and standard deviation are calculated separately over the each group. on about 94 and this what we will try to improve this by batchnorm Batchnorm CustomLet s start by building our own BatchNorm layer from scratch. this happens since the gradient was so fast that it just falling of a clift klippe and having to star agian this we will ty to fix you can see hooks as the same as hooks create global verible for list for every layer create global verible for list for every layer create a function to use for register_forward_hook and if we look at the docs for it it takes 3 things the modeule that do the callback mod the input to the module inp and the output to the module outp note i is the layer number we are using to caluelate the mean of the last layer. log because in Pytorch the negative log likelyhood expect a log softmax not just a softmax that start looking at our depended veriable and lets just look at the first 3 values so now we want find the proberbillety accosiated with 5 the proberbillety accosiated with 0 and the proberbillety accosiated with 4 this is how we index into one prediction this is how we index into the 3 numbers a list of dimensions 0 1 2 5 0 4 and we have 2 dimensions next is all he row indexses we want 0 1 2 and the 5 0 4 is a list of all he columns indexses we want. since we cant take the mean to an int in pytorch lets check the accuracy function so we grap our first batch so minibatch batch size a mini batch from x predictions now we can take the same size of the dependents veriables and calulate our loss now we can calulate our accuracy and it is so low because we havent trained our model yet so this is just a random answer so now we will train it learning rate how many epochs to train for training loop lets go through i up to n 1 where n is 50000 number of rows in training data we divide it with bs because we wanna do a batch at the time set_trace then we grap everything i times the batch the start of the minibatcj and ending at the start_i the bacth the end of the minibatcj so the to lines above is our first i th minibatch then lets grap one x minibatch one y minibatch and let pass that through out loss fucntion then lets do backward then we do our update and we use no_grad since this is not a part of gradient calulation but just the result from it now we have to update the model so we use only layers that have paramenters weight in them. see it like a subroutine now we can create aa training dataloader and a validation one iter create the co routine and next grap the next thing yielded out of that co routine check shape check shape now we cant fit the model or in other words its a traiing loop go through each epoch go through each batch in the independed and depended veriables in train data calulate the preditiction calulate the gradients update with a learning rate reset the gradients samler class so we take in the dataset ds and the batch size bs and set the shuffle to False and then we store them away but here note we dont store the dataset away just the lenth of the dataset so we know how many items to sample so remember this is the one we can call the next on a lot of times so iif we are shuffling if self. The __iter__ method is there to be able to do things like for x in. is above 90 and the above is actuel acc. zero_grad PyTorch already provides this exact functionality in optim. count dona reprer so it prints it out gonna add up the total loss times the batch size count the size of the batch and for each matrix. so after each module it detaltes and call def remove goes through each layer and removes each registed hook. And here are other scheduler functions this above graf shows the four different schedulars but this enought since we want to get first a higher leairning rate and then a lower oone se grafs a the bottum. First we can do it manually like this Now we can have a look at the means and stds of the activations at the beginning of training. Because the data know how many activations it needs now lets store away the rest from the into above just storing them. This can be done using numpy style integer array indexing. png no memory for more pictures so that. in other words we averge over all the batches 0 and we averge over all x 2 and y 3 koordinates. zero_grad and instead use just opt. shuffle then let us take a random permutation from n til minus 1 and if we are ot shuffling then lets grap the number in order torch. We ll also re create our TestCallbackThird callback how to compute metrics. the batch size can be different from minibatch to minibatch 3. dbiasing we want to make sure at each point that no observation is rated to highly and the normal problem with moving averges is that the first point gets far to much weight because it appers in the firsst moving averge and secound and the third and the fourth so like before when we toke the curretn point multiplied with the momentum added with the next point minus the momentum. eval so the code rough dont do anything so it just says thise are the steps i need to run and the callbacks do the running. So this is done because we a object that contains another object so in the runner object is there a object for callback name function takes the name on fx a class and make it lowerchase and put a _ as a seperation on the ords see eksampel below with function name camel2snake and then the Callbakc are removed keep track of how many epochs it has done and how many iterations it has done so we call model. grad lr model. Here we store the mean and std of the output in the correct position of our list. so you are never gonna get great result by wheasting 90 procent of your actiovations. but since every singel activation has one there can be hundred of millions of activations. 15 to 1 which is much better though do note that the minibach is from the training set that we also do the prediction on so it dosent mean to much but we can see that our model is learning something so this is good. Note that we always call model. we will accumulate the statisstics so we need a class that does the accumulate which is defined above at the end of an epoch we will print out the statistics create our learner add our argstatscallback partial is a function that returns a function so we go in the AvgStatsCallback class and get the functions and pass it to the veriable now we can set it in our runner in the cb_funcs which are for new callbacks defoult a learing rate at 0. png attachment image. nb_02 import from exp. NB In fastai we use a bool param to choose whether to make it a forward or backward hook. since it s equal to its mean. Dropout to ensure appropriate behaviour for these different phases. so now lets add callbacks CallbackHandlerThis was our training loop without validation from the previous notebook with the inner loop contents factored out Add callbacks so we can remove complexity from loop and make it flexible This is roughly how fastai does it now except that the handler can also change and return xb yb and loss. In the above version we re only supporting forward hooks. E X 2 E X 2 This solves the small batch size issue What can we do in a single epoch Now let s see with a decent batch size what result we can get. so to fix this we do random sampling Random samplingWe want our training set to be in a random order and that order should differ each iteration. And this helper function will quickly give us everything needed to run the training. This is last part of fastai part 1 lesson 9 though it is mixed with my notes eksperiments and what I found usefull if you want the pure version check fastai github or the following link https github. note model 0 mean the first means the first layer note our mean is 0 and the std isnt 1 so now we just go ahead and inisiate it with keiming and other that our mean is very close to 0. and because we used yield these sampler are only gonna be calulate when we call on them so we can use these on big dataset then by for i in s we are gonna grap all of the indexses in a given sample and we gonna grap the dataset at that index self. So it will use the Callback class above and then use the function called name and use this. so it stops since our loss got much worse then the best worse i the graf aboveso we can se on the graf below that we dont hit the 100 iterations Part 5 06_cuda_cnn_hooks_init ConvNetHelper function to quickly normalize with the mean and standard deviation from our training set Let s check it behaved properly. lets look inside the model and try to get a higher acc. hook when we are done using a hook module we should call hook. this will be explained in more detail in a later. for the model there are only to things the parameters the things we are updating and the activations the things we calculate and the thing we want on the GPU is the parameters and inputs Now that s definitely faster Refactor model from aboveFirst we can regroup all the conv relu in a single function Another thing is that we can do the mnist resize in a batch transform that we can do with a Callback. soall we are doing is almost nothing since we are going from 9 to 8. ", "id": "nickteim/fastai-part-2-lesson-9-10-2019", "size": "22153", "language": "python", "html_url": "https://www.kaggle.com/code/nickteim/fastai-part-2-lesson-9-10-2019", "git_url": "https://www.kaggle.com/code/nickteim/fastai-part-2-lesson-9-10-2019", "script": "opt combine_scheds GeneralRelu(nn.Module) exp.nb_01 Sampler() AvgStats() get_learn_run DataBunch() set_param children ListContainer() set_runner torch.nn RunningBatchNorm(nn.Module) optim #and now since have impimentet the optimazer class we can just use pytorchs log_softmax nll typing conv2d accumulate functools view_tfm begin_epoch all_batches test_eq __len__ conv_ln conv_rbn normalize LR_Find(Callback) CudaCallback(Callback) Hook() datasets __call__ TestCallback(Callback) sched_no InstanceNorm(nn.Module) torch.utils.data sched_cos after_loss reset Dataset() DummyModule() valid_ds sched_exp SequentialSampler tensor conv_in get_cnn_layers Learner() after_backward test_near RandomSampler init_cnn after_fit flatten pickle DataLoader() Lambda(nn.Module) nn update_stats logsumexp Optimizer() all_stats CallbackHandler() Recorder(Callback) SequentialModel(nn.Module) ParamScheduler(Callback) data gzip avg_stats matplotlib begin_batch cos_1cycle_anneal Callback() test_near_zero sched_lin step normalize_to BatchNorm(nn.Module) get_runner exp.nb_02 __del__ do_stop model Runner() mnist_resize get_dls __repr__ forward TrainEvalCallback(Callback) CancelBatchException(Exception) _inner get_data Hooks(ListContainer) Model(nn.Module) get_hist __delitem__ begin_validate fit name __enter__ IPython.core.debugger __setattr__ fastai __setitem__ remove init_cnn_ __iter__ after_step set_trace __init__ get_model loss_func torch near plot_lr train_ds accuracy plot Path DataLoader __exit__ CancelTrainException(Exception) __getattr__ camel2snake after_batch __getitem__ pathlib AvgStatsCallback(Callback) init one_batch LayerNorm(nn.Module) BatchTransformXCallback(Callback) append_stats matplotlib.pyplot create_learner get_model_func get_cnn_model partial zero_grad torch.nn.functional after_epoch CancelEpochException(Exception) listify plot_loss collate begin_fit conv_layer math parameters get_min annealer ", "entities": "(('module', 'biases'), '5affine') (('we', 'activations'), 'be') (('we', 'self'), 'note') (('Hook', 'Hook class'), 'refactor') (('we', 'division'), 'be') (('dosent', 'thing one ones'), 'bs') (('Bot we', 'fist 10 layers'), 'see') (('where minibatches', 'extremely large distributed models'), 'try') (('2 we', 'acc'), 'realy') (('PyTorch', 'link'), 'note') (('otherwise we', 'normal relu'), 'want') (('PyTorch you', 'nn'), 'be') (('thereby we', 'full matrix'), 'note') (('again first few layers', 'filters'), 'add') (('we', 'problem'), 'be') (('it', '_ maight python'), 'start') (('we', 'that'), 'see') (('this', 'hyper parameter pg self'), 'in_train') (('we', 'mask indices'), 'behave') (('he', 'BatchNorm'), 'be') (('batch small size', 'TREE SO IDEA'), 'are') (('procent', '0'), 'mean') (('we', 'code'), 'shape') (('it', 'it'), 'note') (('0 we', '2'), 'averge') (('we', 'E E X 2 2 varaince'), 'use') (('you', 'nn'), 's') (('this', 'CancelBatchException'), 'matter') (('accuracy', '0'), 'mean') (('1 we', 'order torch'), 'let') (('ds', '50000'), 'loop') (('we', 'one'), 'start') (('we', 'model'), 'say') (('We', 'how metrics'), 'create') (('where i', 'desired target'), 'loss') (('this', 'later'), 'explain') (('HW sum _ l W _ H _ 1 1 m 1 tilm', 'eq inorm'), 'label') (('input later number', 'training data data'), 'have') (('training together validation', 'class'), 'be') (('This', 'style integer array numpy indexing'), 'do') (('We', 'data block API'), 'use') (('we', 'infinity'), 'have') (('we', 'training'), 'go') (('properly when model', 'very hooks'), 's') (('we', '0'), 'store') (('we', 'pytorchs just optim'), '9375') (('We', 'now simple CNN'), 'take') (('affine', 'size num_channels'), 'be') (('mean deviation', 'separately group'), 'calculate') (('it', 'them'), 'n') (('we', 'what'), 'print') (('that', '9'), 'give') (('model', 'size input now image'), 'work') (('we', 'only forward hooks'), 'support') (('validation', 'shouldn t'), 'randomize') (('_ _ log sum _ _ 1 right left 1 where a', 'x _ j.'), 'be') (('so we', 'now self'), 'parameter') (('we', 'device'), 'be') (('log input negative likelyhood lets', 'predictions range target'), 'end') (('parameters', 'function'), 'be') (('activation how many last layer', 'beginning'), 'predefine') (('it', 'activations'), 'layer') (('So it', 'this'), 'use') (('so fase one wil', 'learning 0'), 'start') (('almost we', '8'), 'be') (('DummyModule', 'DummyModule class'), 'have') (('where everything', 'first pich'), 'note') (('it', 'bool param'), 'NB') (('mean', 'very 0'), 'mean') (('acc score', 'len range 0 self'), 'check') (('callbacks', 'running'), 'do') (('we', 'layer'), 'calulate') (('So we', 'log_softmax function'), 'use') (('order', 'iteration'), 'so') (('_ _ setattr _ _', 'self'), 'module') (('so we', 'model'), 'do') (('layer we', 'last layer'), 'happen') (('aren', 'there memory'), 'make') (('loss', 'little bit better parameters'), 'repeat') (('nll_loss', 'one optimized function'), 'combine') (('we', 'just averge'), 'want') (('right model', 'scc much better score'), 'get') (('we', 'index self'), 'go') (('all', 'just them'), 'be') (('_ _ iter _ _ method', 'in'), 'be') (('when we', '10 iterations'), 'have') (('We', 'epoch'), 'calculate') (('we', 'activattions'), 'create') (('it', 'suddenly other'), 'get') (('that', 'several hooks'), 'use') (('we', 'then next 5 so picture'), 'calulate') (('so we', 'difined model'), 'be') (('i', 'range'), 's') (('it', 'registed hook'), 'go') (('most', 'zero gradient'), 'have') (('we', 'loss'), 'part') (('you', 'image'), 'question') (('needs', 'just them'), 'know') (('first callback', 'epoch'), 'be') (('which', '2 epochs'), 'note') (('convenient which', 'torch'), 'be') (('there we', 'training'), 'create') (('also here we', 'secound point'), 'note') (('model inputs', 'same device'), 'make') (('0 2 3 we', 'axsis'), 'start') (('Learner object', 'training elsewhere loop'), 'do') (('we', 'Callback'), 'be') (('s', 'usual warm annealing'), 'note') (('we', 'activations'), 'have') (('now handler', 'xb also yb'), 'let') (('lets', 'higher acc'), 'look') (('we', '1 7 2 9'), 'mean') (('you', 'more 20 yet just variance'), 'mean') (('where we', 'last notebook'), 'remember') (('then lower oone se', 'bottum'), 'be') (('Handle', 'dropout print model'), 'note') (('layers', 'beginning'), 'be') (('we', 'just that'), 'go') (('Hooks don t', 'model'), 'require') (('everything', 'activations'), 'see') (('we', 'modules'), 'use') (('we', 'hook'), 'hook') (('v even variance', 'zero'), 'divide') (('5', '0'), 'see') (('which', 'chase'), 'SGD') (('when we', 'x.'), 'take') (('then we', 'differeent outcomes'), 'be') (('so you', 'actiovations'), 'go') (('when your', 'only 2 3'), '09') (('we', 'scratch'), 'on') (('singel activation', 'activations'), 'have') (('it', 'training'), 'combine') (('example Here use', '0'), 'be') (('thogh even h', 'cell block'), 'note') (('relu Uniform more useful initial weights normal distribution', '0'), 'provide') (('t', 'PyTorch'), 'nb') (('when it', 'block'), 'use') (('pre we', 'now acc'), 'note') (('it', 'training set'), 'stop') (('backward', 'backward hook'), 'call') (('you', 'Lambda'), 's') (('we', 'crachs'), 'make') (('GroupNorm', 'InstanceNorm m equivalent nn'), 'channel') (('90 procent', 'model this'), 'look') (('we', 'indexses'), 'log') (('DataLoaderNote you', 'Dataset'), 'look') (('you', 'self'), 'stop') (('then last 70', '0'), 'go') (('that', 'linear model'), 'be') (('General equation', 'BatchNorm is1'), 'beta') (('that', 'them'), 'check') (('so this', 'something'), 'note') (('how it', '1'), 'give') (('variance', 'one thing'), 'sample') (('we', 'relu'), 'go') (('pure version', 'fastai github'), 'be') (('GroupNorm', 'LayerNorm m equivalent nn'), 'put') (('as long it', 'optimizer'), 'practise') (('secound we', 'momentum'), 'inste') (('inparticular when we', 'operation usage bla bla correct bla'), 'go') (('Shape N num_channels input Output N num_channels same Examples', 'input torch'), 'Input') (('so now we', 'model'), 'use') (('we', 'float'), 'want') (('you', 'models paramters'), 'go') (('lossFirst we', 'activations'), 'course') (('when we', 'loss'), 'define') (('this', 'CancelBatchException'), 'mean') (('learning rate', 'training note pos'), 'take') (('100 it', 'longer them'), 'ater') (('Randomized tests', 'more flexible way'), 'handle') (('so we', 'it'), 'loss') (('we', '2'), 'go') (('way we', 'big activation'), 'avoid') (('so we', 'layers'), 'module') (('result', 'correctly x.'), 'import') (('time we', 'rest'), 'be') (('so we', 'fist layer'), 'use') (('we', 'result'), 'x') (('we', 'also self'), 'call') (('that', 'maximum value'), 'try') (('together we', 'before so now them'), 'take') (('this', 'already us'), 'implement') (('sigma_i 2 epsilon quad mu_i', 'eq bnorm'), 'don') (('layer', 'training modes'), 'use') (('this', 'acc'), 'be') (('helper function', 'training'), 'give') (('that', 'objects'), 'design') (('we', 'very close 0'), 'GroupNorm') (('we', 'model'), 'note') (('much cleaner', 'train_dl'), 'let') (('each', 'num_channels num_groups channels'), 'normFrom') (('it', 'GPU'), 'take') (('that', 'arguments module input three output'), 'attach') (('s', 'end'), 'let') (('we', 'almost which'), 'have') (('Now we', 'training'), 'do') (('so we', 'self'), 'create') (('here we', 'callbacj'), 'be') (('loss', 'statistics'), 'reset') (('function', '1 linear fashion'), 'return') (('4 7 2 Somewhat more flexible way we', 'torch'), 'matter') (('we', 'counts minus'), 'note') (('we', '3 instead best_loss'), 'for') (('where we', 'optim'), 'see') (('it', 'as before rate'), 'relu') (('zero_grad PyTorch', 'optim'), 'provide') (('that', 'mean'), 'be') (('these', 'such nn'), 'eval') (('now we', 'dataloaders'), 'ds') (('you', 'times'), 'return') (('Here we', 'list'), 'store') (('training though loop', 'order'), 'be') (('this', 'pytorch'), 'get') (('it', 'device'), 'be') (('we', 'activation actual zero'), 'get') (('here new callbacks', 'name'), 'use') (('final layer', 'activations'), 'mean') (('we', '2 dimensions 0 3 dims'), 'register') (('we', '_ inner start'), 'pname') (('we', 'context manager'), 'use') (('it', 'nn'), 'modules') (('it', 'previus model'), 'conv2d') (('it', 'proper nn'), 'modules') (('batch size', '3'), 'be') (('that', 'accuracal yb'), 'see') (('so when we', 'momentum'), 'want') (('are', '0'), 'accumulate') (('you', 'order'), 'fire') (('that', 'avgerge gennemsnitlige'), 'do') (('back which', 'i'), 'note') (('more single class', 'time'), 'let') (('which', 'previously x.'), 'note') (('batch size', 'matrix'), 'reprer') ", "extra": "['outcome', 'test']", "label": "Perfect_files", "potential_description_queries": ["acc", "accumulate", "accuracy", "advanced", "affine", "answer", "append", "apply", "approach", "argument", "array", "article", "assign", "attribute", "average", "backward", "basic", "batch", "best", "beta", "bit", "block", "bn", "bool", "boolean", "broadcast", "build", "calculate", "call", "care", "case", "cb", "cell", "channel", "check", "checking", "children", "choose", "classification", "classify", "clear", "close", "cnn", "code", "collate", "column", "combine", "combined", "comment", "compare", "compute", "contain", "context", "contrast", "conv", "convert", "correct", "cosine", "count", "course", "create", "cuda", "current", "curve", "data", "dataset", "decay", "def", "define", "detail", "device", "difference", "dimension", "distributed", "distribution", "dot", "empty", "end", "ensure", "entropy", "epoch", "epsilon", "equal", "equation", "eval", "evaluation", "even", "every", "everyone", "everything", "exp", "expected", "experiment", "explained", "export", "fact", "fastai", "faster", "filter", "final", "find", "fit", "fitting", "fix", "flat", "float", "following", "formula", "forward", "found", "frac", "function", "functional", "gamma", "getitem", "grad", "gradient", "group", "hand", "handle", "helper", "histogram", "history", "hook", "hot", "hyperparameter", "idea", "image", "implement", "import", "improve", "index", "inference", "info", "init", "inner", "input", "instance", "int", "integer", "interpolation", "issue", "iter", "iteration", "itself", "kept", "kernel", "key", "knowledge", "l1", "l2", "label", "latter", "layer", "learn", "learner", "learning", "leave", "left", "len", "lesson", "let", "line", "linear", "link", "list", "little", "loader", "log", "look", "looking", "loop", "lost", "lot", "lower", "lr", "mask", "matrix", "max", "maximum", "mean", "memory", "method", "might", "min", "mini", "mixed", "mnist", "mod", "mode", "model training", "model", "module", "momentum", "monkey", "most", "move", "multiple", "my", "name", "near", "need", "negative", "network", "neural", "new", "next", "no", "norm", "normal", "normalization", "normalize", "not", "notebook", "number", "numerical", "numpy", "object", "observation", "opening", "operation", "opt", "optimization", "optimizer", "order", "ordered", "out", "output", "parameter", "part", "partial", "patch", "people", "per", "percentage", "permutation", "pickle", "picture", "place", "plot", "plotting", "png", "point", "pooling", "position", "power", "practice", "pre", "prediction", "print", "printing", "problem", "product", "property", "provide", "pytorch", "random", "range", "re", "reason", "register", "remove", "replace", "reset", "resize", "rest", "result", "return", "right", "routine", "row", "run", "runner", "running", "sample", "sampler", "sampling", "save", "sched", "schedule", "scheduler", "score", "sense", "separate", "set", "setup", "several", "shape", "shift", "shuffle", "simplification", "single", "size", "slice", "slide", "softmax", "solution", "something", "space", "sqrt", "stack", "standard", "start", "std", "step", "store", "stride", "stuff", "style", "sum", "target", "tensor", "test", "think", "those", "thought", "through", "time", "total", "track", "train", "training", "transform", "transformation", "tree", "try", "uniform", "unit", "up", "update", "usage", "v3", "validation", "value", "var", "variance", "vector", "version", "view", "weight", "work", "write"], "potential_description_queries_len": 342, "potential_script_queries": ["accuracy", "collate", "core", "fastai", "forward", "gzip", "logsumexp", "math", "matplotlib", "name", "nn", "opt", "optim", "pathlib", "plot", "reset", "tensor", "torch"], "potential_script_queries_len": 18, "potential_entities_queries": ["bit", "close", "data", "epsilon", "forward", "input", "iter", "linear", "lower", "negative", "next", "numpy", "print", "right", "sum"], "potential_entities_queries_len": 15, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 348}