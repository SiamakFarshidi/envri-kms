{"name": "vertex ai with mlb player digital engagement ", "full_name": " h1 Getting Started on Vertex AI Notebooks h1 Notebook Setup h3 1 Download this Notebook h3 2 Download Kaggle API Key h3 3 Sign up for Google Cloud Platform h3 4 Create a Project and Enable the Notebook API h3 5 Create a Notebook Instance h3 6 Upload MLB Notebook and API Key h3 7 Authenticate Kaggle API and Download MLB Date h3 9 Run this Tutorial h3 8 Cleaning Up h1 MLB Getting Started h1 Model h1 Evaluate h1 Explain h1 Hyperparameter Tuning with Vizier h2 Setup h2 Define Hyperparameters h2 Create Study h2 Run Study ", "stargazers_count": 0, "forks_count": 0, "description": "com pub tools public publication data pdf bcb15507f4b52991a0783013df4222240e942381. com c mlb player digital engagement forecasting data for a complete description. Download this Notebook Start by creating your own copy of this notebook. The next cell has a helper function to load data for only a single player by default Aaron Judge of the NY Yankees who had the highest overall engagement during the training period. Now in the menubar above click File Download Notebook and save a copy of the notebook to your computer. Evaluate We can get an idea of how well our network was able to fit the data by plotting its predictions. We ll put our model definition and training inside a function we can pass hyperparameters to. Run this Tutorial After you ve completed the setup steps above in Vertex AI Notebooks select Run from the menubar and Run Selected Cell and All Below to run through the rest of this notebook automatically. It s common to apply logarithmic scaling when you want to search efficiently across orders of magnitude like for a learning rate between 1e 6 and 1e 1. So after you re done be sure either to stop the notebook instance from the Notebooks page or else to delete the Cloud project you created. A study conducts trials in order to optimize one or more metrics. In JupyterLab on Vertex AI Notebooks you can also review a tutorial on XAI in the tutorials explainable_ai sdk_tutorial. In Vizier hyperparameters can be one of four types DOUBLE or INTEGER defined by minimum and maximum values or CATEGORICAL or DISCRETE defined by an enumeration of values. We ve computed attributions for the validation set the last 30 days of the data but you could try a different set if you like by changing the instances parameter. Getting Started on Vertex AI Notebooks This notebook demonstrates how to do the following on Vertex AI Google s powerful new machine learning platform run the getting started notebook on Vertex AI Notebooks to load the data create a model generate predictions explore explainable AI on Vertex AI to refine your features tune hyperparameters with VizierIt is a complement to the Getting Started with MLB Digital Engagement https www. Now run the next cell to enable the Vertex AI API on your project and install the Python client in this notebook instance. Authenticate Kaggle API and Download MLB Date Run the next cell to download the competition data. com vertex ai pricing vizier. This will download a fresh authentication token onto your machine. The starter account is free and it comes with 300 credit that you can use. If you don t know your project ID you may be able to retrieve it with this command Define Hyperparameters The search space or feasible space for each hyperparameter is determined by its type. Click the Copy and Edit button to the upper right. Or step through cell by cell if you d prefer. Scroll down to the section of the page labelled API. com and click on Get Started For Free. Be sure to keep the default TensorFlow Enterprise environment. This is a two step sign up process where you will need to provide your name address and a credit card. Let s finish by looking at the best set of hyperparameters we found Set Matplotlib defaults Helper function to unpack json found in daily data Add Output widgets for each pandas DF as tabs children Rename tab bar titles to df names Display corresponding table output for this tab name Define dataframes to load from training set targets features Other dataframes available for features games rosters teamBoxScores transactions standings awards events playerTwitterFollowers teamTwitterFollowers Read training data Convert training data date field to datetime type Unpack nested dataframes and store in dictionary training_dfs Create date index Aggregate multiple games per day by summing Match target dates to feature dates and create date index Aaron Judge of the NY Yankees Process dataframes Merge for processing Process Filter for chosen player Restore features and targets Create temporal features Create train validation splits Aaron Judge of the NY Yankees number of annual seasonal components attributions relative to the median of the target the names of each feature explanations for the validation set needs to match name of InputLayer in the model ALGORITHM_UNSPECIFIED means Bayesian optimization can also be GRID_SEARCH or RANDOM_SEARCH Fill in your project ID These will be automatically filled in Define the hyperparameter feasible space the name of the quantity we want to minimize choose MINIMIZE or MAXIMIZE Call a helper function to create the study Parse hyperparameters Create and train model Optimize the metric monitored by early_stopping val_loss by default The metric needs to be reported in this format set 1 to get suggestions in parallel good for distributed training keep the name the same to resume a trial. We will reupload this in an AI Notebooks instance to take advantage of the Explainable AI service. Note that the Explainable AI XAI walkthrough is at the bottom of this notebook. com vertex ai docs vizier overview for a nice overview. com notebooks docs before you begin to setup a notebook project. Features with little importance you could consider dropping from your feature set. Create Study Now we ll create a study. Enter an Instance name of your choice and then click the blue CREATE button at the end of the page. ipynb fileNow we can look at explanations using the explainable_ai_sdk library. Model Now we ll create a neural network with Keras and fit it to our training data. We also see that the model did make use of several of the Fourier features modeling annual seasonality. From the site header click on your user profile picture then on My Account from the dropdown menu. You could also look at explanations for other players the players dataframe can tell you the playerId for each player. In our case the hyperparameters define a neural net architechture and training regimen and will produce a validation loss the metric we hope to minimize. We ve picked out a few features from the playerBoxScores dataframe but there are lots more you could try see the data documentation https www. Hyperparameters of numeric type may also have their feasible space scaled https cloud. You ll need a copy of your Kaggle credentials to authenticate your account. com vertex ai docs reference rest v1beta1 StudySpec for more about scaling and other options. com notebooks pricing. Create a Project and Enable the Notebook API Follow the directions at https cloud. Download Kaggle API Key We ll use the Kaggle API to download the competition data to the notebook instance. Run the following cell on AI Notebooks with a Cloud TF image to see model explanations. com ryanholbrook getting started with mlb player digital engagement notebook. Notebook Setup 1. The results of Explainable AI will be easier to understand if we restrict our analysis to a single player. com vertex ai docs explainable ai overview. Run Study Look at your Studies https console. You ll need to provide it with your project id. Each of the attributions is computed on a given set of instances relative to a baseline in our case the median on the training set. Create a Notebook Instance Next go to https notebook. See the StudySpec API documentation https cloud. Also be sure to click the Analysis tab for the results presented in a parallel coordinates chart. This tutorial uses Cloud Notebooks a billable component of Google Cloud. You ll be redirected to a page with a list of your notebook instances. The next cell imports the Vizier service and defines some helper functions we ll use to run the hyperparameter searchNow we ll create the client that will communicate with the Vizier service. Once the notebook instance is running click OPEN JUPYTERLAB just to the right of the instance name. Cleaning Up You ll be billed for any time you keep the notebook instance running. Feature attributions describe the contribution each features makes to the final prediction relative to a baseline. This will take you to your account settings at https www. Explain Explainable AI on Vertex AI Notebooks lets you compute feature attributions for neural networks. com ryanholbrook getting started with mlb player digital engagement tutorial which was designed to be run on Kaggle Notebooks. You should see a Feature Attributions chart above. Sign up for Google Cloud Platform If you don t have a GCP account already go to https cloud. Google Research has a great whitepaper describing the capabilities of Vizier in detail Google Vizier A Service for Black Box Optimization https storage. MLB Getting Started The first part of this notebook reproduces the data and model setup of the Getting Started https www. Learn more about Notebooks pricing https cloud. It may take a few minutes for the instance you just created to start up. To create a new token click on the Create New API Token button. Our feature attribution chart suggest that features like atBats and strikeOuts are important for digital engagement with Aaron Judge but stolenBases much less so. Also see the Vizier guide https cloud. To get attributions relative to another baseline the mean say just change the computation for input_baselines above. We ve created one of each below for illustration. That done we ll call another helper function to run the hyperparameter search. Hyperparameter Tuning with Vizier In the second half of this notebook we ll demonstrate Vertex Vizier Vertex AI s hyperparameter optimization service. Among its capabilities are a Bayesian Optimization algorithm to search efficiently within a hyperparameter space transfer learning to make use of information from previous hyperparameter studies and automated early stopping when tuning models that train incrementally like neural nets with stochastic gradient descent or gradient boosted trees. While in preview Vertex Vizier is offered at no charge https cloud. For this step you will need to provide a Google Account i. Increase the number of Fourier components to model seasonality with in more detail. com vertex ai docs reference rest v1beta1 StudySpec scaletype. json and vertex ai with mlb player digital engagement. You should be redirected to a JupyterLab environment. Setup First make sure you ve run everything in the notebook prior to this up to Data Pipeline. Feature attributions can help you tune your model by indicating which features are important and which are not. com vertex ai experiments studies tab and select your study for a summary of the results. your Gmail account to sign in. Upload MLB Notebook and API Key From inside JupyterLab click the Upload Files up arrow button in the file browser on the left and upload the files kaggle. A trial is a selection of hyperparameter values together with the outcome they produce. Read more about Vertex Explainable AI here Introduction to Vertex Explainable AI for Vertex AI https cloud. ", "id": "ryanholbrook/vertex-ai-with-mlb-player-digital-engagement", "size": "9777", "language": "python", "html_url": "https://www.kaggle.com/code/ryanholbrook/vertex-ai-with-mlb-player-digital-engagement", "git_url": "https://www.kaggle.com/code/ryanholbrook/vertex-ai-with-mlb-player-digital-engagement", "script": "create_study params_to_dict List (CalendarFourier ipywidgets explainable_ai_sdk.model configs Path joblib metric_fn StringLookup Dict seaborn numpy unpack_data make_playerBoxScores pathlib Parallel keras.layers.experimental.preprocessing run_study typing sklearn.model_selection unpack_json get_optimal_trials make_targets matplotlib.pyplot VizierServiceClient tensorflow pandas tensorflow.keras explainable_ai_sdk.metadata.tf.v2 SavedModelMetadataBuilder statsmodels.tsa.deterministic layers make_training_data keras join_datasets delayed google.cloud.aiplatform_v1beta1 train_test_split ", "entities": "(('features', 'relative baseline'), 'describe') (('you', 'instances parameter'), 'computed') (('com vertex', 'nice overview'), 'ai') (('we', 'single player'), 'be') (('where you', 'name address'), 'be') (('Each', 'training set'), 'compute') (('This', 'https www'), 'take') (('tutorial', 'Google Cloud'), 'use') (('suggestions', 'trial'), 'let') (('that', 'Vizier service'), 'import') (('you', 'neural networks'), 'let') (('which', 'Kaggle Notebooks'), 'start') (('we', 'hyperparameter search'), 'do') (('You', 'notebook instances'), 'redirect') (('json', 'mlb player digital engagement'), 'ai') (('We', 'illustration'), 'create') (('that', 'gradient stochastic descent'), 'be') (('site header', 'dropdown menu'), 'click') (('we', 'Vertex Vizier Vertex hyperparameter optimization service'), 'tune') (('study', 'one metrics'), 'conduct') (('when you', '1e'), 's') (('notebook instance', 'instance name'), 'run') (('you', 'Data Pipeline'), 'make') (('Model Now we', 'training data'), 'create') (('we', 'hyperparameters'), 'put') (('you', 'that'), 'be') (('who', 'training period'), 'have') (('Run Study', 'Studies https console'), 'look') (('that', 'Aaron Judge'), 'suggest') (('players dataframe', 'player'), 'tell') (('This', 'machine'), 'download') (('ipynb fileNow we', 'explainable_ai_sdk library'), 'look') (('we', 'metric'), 'define') (('mean', 'input_baselines'), 'change') (('Hyperparameters', 'https also feasible space scaled cloud'), 'have') (('Define search space', 'type'), 'know') (('Download Kaggle We', 'notebook instance'), 'api') (('com vertex', 'docs reference rest'), 'ai') (('MLB first part', 'Getting Started https www'), 'start') (('you', 'tutorials'), 'review') (('you', 'instance'), 'take') (('you', 'cell'), 'step') (('you', 'Cloud else project'), 'be') (('they', 'together outcome'), 'be') (('how well network', 'predictions'), 'evaluate') (('lots you', 'data documentation https www'), 'pick') (('com notebooks you', 'notebook project'), 'doc') (('don GCP account', 'https already cloud'), 'sign') (('you', 'Google Account i.'), 'need') (('You', 'project'), 'need') (('you', 'feature set'), 'feature') (('use', 'annual seasonality'), 'see') (('You', 'Feature Attributions chart'), 'see') (('which', 'model'), 'help') (('Explainable AI XAI', 'notebook'), 'note') (('notebook instance', 'time'), 'bill') (('You', 'account'), 'need') (('Google Research', 'Black Box Optimization https storage'), 'have') (('setup ve steps', 'notebook'), 'run') (('hyperparameters', 'MLB Digital Engagement https www'), 'start') (('hyperparameters', 'values'), 'be') (('Scroll', 'API'), 'label') (('Vertex preview Vizier', 'charge https cloud'), 'offer') (('You', 'JupyterLab environment'), 'redirect') (('com vertex', 'scaling'), 'ai') (('We', 'Explainable AI service'), 'reupload') ", "extra": "['outcome']", "label": "Perfect_files", "potential_description_queries": ["account", "advantage", "algorithm", "apply", "baseline", "best", "bottom", "browser", "button", "call", "case", "cell", "chart", "children", "choice", "choose", "client", "command", "competition", "computation", "compute", "consider", "copy", "could", "create", "credit", "data", "dataframe", "date", "datetime", "day", "default", "define", "describe", "detail", "df", "dictionary", "distributed", "download", "enable", "end", "everything", "explore", "feature", "field", "file", "final", "fit", "following", "forecasting", "format", "found", "function", "generate", "gradient", "half", "header", "help", "helper", "hope", "hyperparameter", "idea", "image", "importance", "index", "instance", "json", "labelled", "learning", "left", "list", "little", "load", "look", "looking", "magnitude", "match", "maximum", "mean", "median", "metric", "minimize", "minimum", "model", "multiple", "name", "need", "network", "neural", "new", "next", "no", "notebook", "number", "numeric", "optimization", "optimize", "order", "out", "outcome", "output", "overall", "overview", "page", "parallel", "part", "pdf", "per", "picture", "player", "plotting", "prediction", "processing", "profile", "project", "provide", "pub", "public", "publication", "quantity", "re", "reference", "relative", "rest", "review", "right", "run", "running", "save", "scaled", "scaling", "search", "seasonal", "seasonality", "second", "section", "select", "selection", "service", "set", "setup", "several", "sign", "single", "site", "space", "start", "step", "store", "summary", "table", "target", "temporal", "through", "time", "token", "train", "training", "transfer", "trial", "try", "tune", "tuning", "tutorial", "type", "unpack", "up", "upper", "user", "v1beta1", "validation", "who"], "potential_description_queries_len": 172, "potential_script_queries": ["cloud", "delayed", "experimental", "joblib", "metadata", "numpy", "pathlib", "seaborn", "tensorflow", "tf", "tsa"], "potential_script_queries_len": 11, "potential_entities_queries": ["optimization", "reference", "search"], "potential_entities_queries_len": 3, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 183}