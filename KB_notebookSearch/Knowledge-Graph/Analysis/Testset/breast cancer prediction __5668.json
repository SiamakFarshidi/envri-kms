{"name": "breast cancer prediction ", "full_name": " h1 About this Kernel h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations h3 Observations ", "stargazers_count": 0, "forks_count": 0, "description": "We have to select which of the attributes we want to use in building our model Observations We would need to eliminate the outliers so that it does not affects our model s accuracy. Let s try to see where exactly does this happen Observations From the above plot we may conclude that highest accuracy is achieved at 20th iteration. The columns 3 32 contain 30 real value features that have been computed from digitized images of the cell nuclei which can be used to build a model to predict whether a tumor is benign or malignant. Most of the columns seem to have a numeric entry. The ID column would not help us contributing to predict about the cancer. There are two things that can be done. There are a total of 31 columns which are of float datatype. We can either use only the columns which have greatest correlation or we can continue to use all the columns. Observations So there are 3 columns that have outliers lets plot them and check them out Observations We can clearly see that all our models perform with more than 90 accuracy where DecisionTreeClassifier has the lowest of 90. 1 Malignant Cancerous Present M 0 Benign Not Cancerous Absent B Ten real valued features are computed for each cell nucleus radius mean of distances from center to points on the perimeter texture standard deviation of gray scale values perimeter area smoothness local variation in radius lengths compactness perimeter 2 area 1. This would save our time from mapping the variables. It considers as outliers the samples that have a substantially lower density than their neighbors. The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis M malignant B benign respectively. I will be using all these columns to predict our result You can eliminate a few and see if the accuracy improves Observations Looks wonderful isn t it There are only a handful of columns that show negative correlation with the diagnosis column Around half of our columns are more than 50 positively correlated to diagnosis column. The dataset contains 569 samples of malignant and benign tumor cells. png attachment efa5748c 1e3b 44c4 a93d f37f0421fcc1. We will probably drop it anyway. 50 DecisionTreeClassifier 90. There s only ID column of int type. All feature values are recoded with four significant digits. 05 RandomForestClassifier 96. png attachment a530fc48 7806 483c 9049 9b86635fd7bd. For instance field 3 is Mean Radius field13 is Radius SE field 23 is Worst Radius. Observations In order to conduct our analysis easily we have converted the target column as Malignant 1 Benignant 0 Observations The following columns are the one s that show the greatest correlation with our diagnosis column. Missing attribute values none Class distribution 357 benign 212 malignant Observations The last column named Unaname 32 seems like an erronous coloumn in our dataset. Let us see how well do they correlate with the diagnosis column. 67 Thanks a lot for checking this out till the end linear algebra data processing data visualization data visualization to ignore the warnings for model building Loading the data pair plot split the data to X and y before Local Outlier Factorization 1 inlier 1 outlier Dont fit the scaler while standardizate X_test. 0 concavity severity of concave portions of the contour concave points number of concave portions of the contour symmetry fractal dimension coastline approximation 1 The mean standard error and worst or largest mean of the three largest values of these features were computed for each image resulting in 30 features. 058 and LogisticRegression has the highest of 98. 50 GradientBoostingClassifier 97. 67 AdaBoostClassifier 96 XGBClassifier 97. Let us see if we can further improve the accuracy of our model by adding a few changes to it From the above figure we can see that our model touches somewhere around 99 between 20 25. We might probably just drop it. Observations After dropping the two columns we are now left with 31 columns. png Summary We used a total of 8 models in order to achieve our final result. Let us see if there are any outliers present in the dataset About The Local Outlier Factor The Local Outlier Factor LOF algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. LogisticRegression 99. Observations Only the diagnosis column which we have to predict is of object datatype. png About the Dataset About this Kernel The Breast Cancer datasets is available UCI machine learning repository maintained by the University of California Irvine. We might as well drop it. Wohhoo We have finally built a model with an accuracy of 99. 12 KNeighborsClassifier 95. ", "id": "aditimulye/breast-cancer-prediction", "size": "5668", "language": "python", "html_url": "https://www.kaggle.com/code/aditimulye/breast-cancer-prediction", "git_url": "https://www.kaggle.com/code/aditimulye/breast-cancer-prediction", "script": "sklearn.metrics sklearn.tree LocalOutlierFactor AdaBoostClassifier KNeighborsClassifier DecisionTreeClassifier seaborn numpy GradientBoostingClassifier sklearn.ensemble confusion_matrix sklearn.model_selection RandomForestClassifier matplotlib.pyplot pandas StandardScaler LogisticRegression accuracy_score NeighborhoodComponentsAnalysis sklearn.neighbors SVC sklearn.linear_model sklearn.preprocessing sklearn.svm xgboost train_test_split ", "entities": "(('we', 'now 31 columns'), 'leave') (('highest accuracy', '20th iteration'), 'let') (('that', 'more than positively column'), 'use') (('model', 'somewhere around 99 between 20 25'), 'let') (('dataset', 'tumor malignant cells'), 'contain') (('which', 'float datatype'), 'be') (('1 mean standard error', '30 features'), 'severity') (('Wohhoo We', '99'), 'build') (('us', 'cancer'), 'help') (('one that', 'diagnosis column'), 'observation') (('This', 'variables'), 'save') (('Breast Cancer datasets', 'California Irvine'), 'png') (('feature values', 'four significant digits'), 'recode') (('Summary png We', 'final result'), 'use') (('Mean Radius 3 field13', 'instance field'), 'be') (('it', 'accuracy'), 'have') (('we', 'object datatype'), 'be') (('that', 'neighbors'), 'consider') (('Cancerous Present 1 Malignant M 0 Cancerous Absent B Ten real valued features', 'radius lengths'), 'benign') (('none Class 357 benign 212 malignant last column', 'dataset'), 'miss') (('where DecisionTreeClassifier', '90'), 'observation') (('tumor', 'model'), 'contain') (('Local Outlier Factor LOF detection unsupervised anomaly which', 'neighbors'), 'let') (('LogisticRegression', '98'), '058') (('Most', 'numeric entry'), 'seem') (('we', 'columns'), 'use') (('how well they', 'diagnosis column'), 'let') (('1 outlier', 'scaler'), 'thank') ", "extra": "['biopsy of the greater curvature', 'test', 'diagnosis']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "algorithm", "anomaly", "area", "attribute", "build", "cell", "center", "check", "checking", "column", "contain", "contour", "correlation", "data", "dataset", "detection", "diagnosis", "dimension", "distribution", "drop", "end", "error", "feature", "field", "figure", "final", "fit", "float", "following", "gray", "half", "help", "ignore", "image", "improve", "instance", "int", "largest", "learning", "left", "linear", "local", "lot", "lower", "malignant", "mapping", "mean", "method", "might", "model", "need", "negative", "none", "not", "nuclei", "number", "numeric", "object", "order", "out", "outlier", "pair", "perform", "plot", "png", "point", "predict", "present", "processing", "repository", "result", "save", "scale", "scaler", "select", "split", "standard", "store", "target", "time", "total", "try", "tumor", "unique", "value", "variation", "visualization", "while", "worst"], "potential_description_queries_len": 90, "potential_script_queries": ["numpy", "seaborn", "sklearn", "xgboost"], "potential_script_queries_len": 4, "potential_entities_queries": ["detection", "mean", "standard"], "potential_entities_queries_len": 3, "potential_extra_queries": ["biopsy"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 95}