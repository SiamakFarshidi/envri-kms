{"name": "using shap values for interpretability ", "full_name": " h1 Using SHAP values for XGBoost interpretability h2 The data Wisconsin Breast Cancer Dataset h2 Explaining the model with SHAP values h2 Conclusions ", "stargazers_count": 0, "forks_count": 0, "description": "This notebook aims to be both an introduction to SHAP https github. Contrary to interpretable methods that produce predictions that can be explained by taking a look at the inner workings of the model black box methods present the prediction as a mistery to the user. Therefore a general interpretability of the model is hard and prone to miss plenty of information. Hopefully this kernel helps you find the motivation to look for interpretability of your models and exploit the information that it can provide. The data Wisconsin Breast Cancer DatasetFor this demo we will use the Wisconsin Breast Cancer Dataset which contains 30 features extracted from an image of a fine needle aspirate FNA of a breast mass. Using SHAP values for XGBoost interpretabilityMost of the algorithms whose popularity has risen over the recent years are considered black box methods. This can be confusing now but will get easier with the examples below. 29 and higher than the base value therefore predicted as benign. On the other hand the interpretability of a single sample can be as straightforward as a force plot which provides a lot of visual information. We will be using a 5 folds cross validation to train the model. On the other hand we can see that in an example of malign tissue the 371st sample the concavity texture and perimeter of the cells make the model predict it as a negative case. Note putting it simply in this models red means good and blue means bad. We can drop the id and the original diagnosis column we will not be using them anymore in this notebook. We can see that along with the name of a variable there is a plot. This matrix shap_values is related with the training data The element shap_values i j is the SHAP value of the i sample s j feature. Also this will make the tree return continuous probabilities which are always useful for this problemas. In the summary plot we can see how the most important features for the model are perimeter_worst concave points_mean concave points_worst texture_worst and area_se among others. That way the library helps us find multi dimensional dependences in the data. Another interesting dependence plot is the one of radius_worst that shows two clearly differentiated segments. We can also see how the color denotes that when the perimeter_worst has a low value texture_worst makes the SHAP value lower the higher it is. com slundberg shap which presents itself as a unified approach to explain the output of any machine learning model and a motivation to understand interpretability in a different way than it is popularly imagined. The final results are quite satisfactory. That can be misleading at times so if you are struggling with it you can change the color of it. Interpretability is the only way to demystify ML DL algorithms for non specialized people and clear its inclusion as a general tool to solve different problems in industry or society. The first artifact that can be generated with these values is a summary plot which plots the most important features for the model along with a visual intuition on how they influence it. Lowering the abstraction from the general model to a single prediction makes the interpretation much more intuitive resulting in more useful explanations. In the second variable the perimeter_worst variable is not as disperse in its SHAP values but the majority of values are not in zero which implies no influence in the prediction. We can also see that this relation is not linear but closer to having a threshold around 110. These values are a representation of the impact of a certain feature in a prediction positive values contribute to increase the final probability and negative values contribute to decrease it. Once the eye is trained enough in a glimpse it is possible to see much more than that perimeter points_worst values are correlated with their SHAP values colors seem to advance in order from left to right and it can be a really good indicator of benign tissue the points in the right most side of the graph are further from zero than in other variables. Explaining the model with SHAP valuesUsing the SHAP module we can generate different explainers objects used to compute the SHAP values for each sample. In this case only transforming the diagnosis columns is necessary. We can take a step down the abstraction scale and see individual features dependence plots. Also let s split the data in the conventional names X for the feature matrix and y for the labels vector. These features represent different characterstics like the texture or the simmetry computed for each cell and presented as the mean standard deviation and worst sum of the three largest values of each of them. We can see an example of it in a case of benign tissue the first one in the training data. Due to these model s nature each sample has to be studied individually so most of these graphs are just an aggregation of all these samples. This time it is clear that low values of radius_worst imply a lower probability of it benign and the samples in that segment with high concavity_worst seem to indicate a higher risk of malign tumors. High values of both radius_worst and concavity_worse indicate healthy tissue. This is however out of the scope of the kernel. It is obvious for most data scientist that we will not be able to understand a neural network inner workings as trivially as the ones of a decision tree but studying the relation between its input and outputs using an unified approach as SHAP can help the community and with some pedagogy a client understand how a ML model is making predictions. In the confusion matrix is also possible to see how the discrete predictions out of the probabilities have a great performance an average precision of around 95. Take into account that high SHAP values contribute to the sample being benign and low values contributing to it being malign. One of the most trivially interpretable methods decision trees provide a trivial way of interpreting generating a binary decission tree if X then A else B is enough to traverse the tree checking each sample to understand their prediction. Similarly gradient techniques like the ones used in XGBoost may use dozens of trees for regularization rendering their interpretability as useless for most users. Try running the function with the keyword argument plot_cmap 00AA00 EE2222 to turn it into green means good red means bad. For XGBoost to properly manage the data we must transform all columns to numeric values. We can see how the final SHAP value which can be transformed from a continuous domain to probability using a sigmoid is 2. ConclusionsIt is important to understand that while Linear Models exploit the average of all the training samples Machine Learning usually is able to exploit individual charactersitics to perform a non uniform prediction. Remember kids always cross validate your results This one is a relatively well behaved dataset and we are not in a competition so a minimal parametrization will be performed. This model is able to make accurate predictions but thanks to these SHAP interpretations also its explanations are available which provides practitioners with useful information to understand the model and data scientists with valuable information to tune it. SHAP also plots a second variables value which is automatically chosen depending its interaction with the feature at hand. We can round later if needed. Still take into account that no parameter search or feature engineering has been performed so there is plenty of room for improvement. Creating a binary benign columns indicating if the tumor was benign or not should suffice. These values are not exclusive to XGBoost plenty of other models have optimized explainers in SHAP and the technique itself is model agnostic. We will compute these values for all the samples X. These graphs plot the feature s value respect to its SHAP value letting us understand easily how they are related. Below we can see the dependence plot of perimeter_worst as seen in the dependence plot higher values of the variable are related to higher SHAP values. However SHAP values are especially useful when interpreting individual results. The main cross validation loop runs computing also the scale_pos_weight parameter which compensates unbalanceness in the training data per fold. com slundberg shap for more details. Refer to the repository https github. In it each point being a sample and its color the value of the feature in it we can see its SHAP value in the horizontal axis. The representation shows how despite the texture having anomalous values the concavity and perimeter features finally turn the prediction to the positive side. The highest contribution is the one by concave points_worst. It also contains the diagnosis column as the label of the samples which is B for benign results and M for malign. On the other hand neural networks seem to magically fit to training data and then use their obscure weights to spit a prediction. The example of this are force plots where the horizontal axis are the SHAP value of the final prediction and each feature s contribution is shown a block left of the result the positive SHAP values pushing the probability higher and lower SHAP values in the right pushing the probability lower. The main thing to highlight is that using binary logistic as objective function and auc as the evaluation metric seem to be adequeate for the task. ", "id": "diegovicente/using-shap-values-for-interpretability", "size": "9712", "language": "python", "html_url": "https://www.kaggle.com/code/diegovicente/using-shap-values-for-interpretability", "git_url": "https://www.kaggle.com/code/diegovicente/using-shap-values-for-interpretability", "script": "sklearn.metrics sklearn.model_selection confusion_matrix KFold seaborn numpy matplotlib.pyplot roc_auc_score pandas matplotlib.pylab xgboost ", "entities": "(('which', 'prediction'), 'be') (('how they', 'it'), 'be') (('low values', 'malign tumors'), 'be') (('it', 'other variables'), 'be') (('highest contribution', 'concave points_worst'), 'be') (('tumor', 'binary benign columns'), 'create') (('Similarly gradient techniques', 'useless most users'), 'use') (('Also s', 'labels vector'), 'let') (('which', 'it'), 'be') (('easily how they', 'SHAP value'), 'plot') (('concavity', 'positive side'), 'show') (('This', 'now examples'), 'confuse') (('Interpretability', 'industry'), 'be') (('also relation', '110'), 'see') (('you', 'it'), 'be') (('we', 'sample'), 'explain') (('features', 'them'), 'represent') (('the it', 'low value'), 'see') (('we', 'numeric values'), 'manage') (('which', 'malign'), 'contain') (('minimal parametrization', 'competition'), 'remember') (('SHAP positive values', 'probability'), 'be') (('This', 'kernel'), 'be') (('evaluation metric', 'task'), 'be') (('which', 'breast mass'), 'use') (('SHAP However values', 'especially when individual results'), 'be') (('it', 'that'), 'help') (('We', 'samples'), 'compute') (('individually most', 'just samples'), 'have') (('Machine Learning', 'non uniform prediction'), 'be') (('which', 'hand'), 'plot') (('putting', 'simply models'), 'mean') (('We', 'training data'), 'see') (('that', 'two clearly differentiated segments'), 'be') (('we', 'horizontal axis'), 'be') (('then else B', 'prediction'), 'provide') (('ML how model', 'predictions'), 'be') (('which', 'sigmoid'), 'see') (('perimeter_worst points_mean', 'others'), 'see') (('Therefore general interpretability', 'information'), 'be') (('parameter search', 'improvement'), 'take') (('diagnosis original we', 'anymore notebook'), 'drop') (('us', 'data'), 'help') (('model', 'negative case'), 'see') (('which', 'fold'), 'run') (('which', 'visual information'), 'be') (('popularity', 'recent years'), 'consider') (('only transforming', 'diagnosis columns'), 'be') (('technique', 'SHAP'), 'be') (('values', 'negative it'), 'be') (('individual', 'dependence plots'), 'take') (('that', 'user'), 'present') (('Try', 'good red means'), 'mean') (('higher values', 'SHAP higher values'), 'see') (('j', 'SHAP i j feature'), 'relate') (('it', 'different way'), 'shap') (('notebook', 'SHAP https github'), 'aim') (('which', 'always problemas'), 'make') (('interpretation', 'much more more useful explanations'), 'make') (('benign values', 'it'), 'take') (('also how discrete predictions', 'around 95'), 'be') (('We', 'model'), 'use') (('High values', 'healthy tissue'), 'indicate') (('We', 'variable'), 'see') ", "extra": "['biopsy of the greater curvature', 'diagnosis']", "label": "Perfect_files", "potential_description_queries": ["account", "advance", "approach", "argument", "artifact", "auc", "average", "binary", "block", "box", "breast", "case", "cell", "checking", "clear", "client", "color", "column", "community", "competition", "compute", "confusion", "data", "dataset", "decision", "diagnosis", "discrete", "domain", "drop", "engineering", "evaluation", "explained", "eye", "feature", "final", "find", "fit", "function", "general", "generate", "generated", "gradient", "graph", "green", "hand", "help", "high", "highlight", "id", "image", "increase", "indicate", "individual", "industry", "influence", "inner", "input", "interpretability", "interpretation", "intuition", "itself", "kernel", "label", "largest", "learning", "left", "let", "library", "linear", "look", "loop", "lot", "lower", "main", "majority", "manage", "matrix", "mean", "metric", "model", "module", "most", "motivation", "name", "nature", "negative", "network", "neural", "no", "non", "not", "notebook", "numeric", "objective", "order", "out", "output", "parameter", "people", "per", "perform", "performance", "plot", "point", "positive", "precision", "predict", "prediction", "present", "probability", "provide", "regularization", "relation", "repository", "representation", "result", "return", "right", "risk", "room", "running", "sample", "scale", "scope", "search", "second", "segment", "side", "sigmoid", "single", "split", "standard", "step", "sum", "summary", "technique", "threshold", "time", "tissue", "tool", "train", "training", "transform", "tree", "tumor", "tune", "turn", "uniform", "validate", "validation", "value", "variable", "while", "worst"], "potential_description_queries_len": 154, "potential_script_queries": ["numpy", "seaborn", "sklearn", "xgboost"], "potential_script_queries_len": 4, "potential_entities_queries": ["general", "individual", "positive"], "potential_entities_queries_len": 3, "potential_extra_queries": ["biopsy of the greater curvature", "biopsy"], "potential_extra_queries_len": 2, "all_components_potential_queries_len": 160}