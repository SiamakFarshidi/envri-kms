{"name": "notebook7fb74e7e82 ", "full_name": " h1 Predicting House Prices on Kaggle h2 Downloading and Caching Datasets h2 Kaggle h2 Accessing and Reading the Dataset h2 Data Preprocessing h2 Training h2 K Fold Cross Validation h2 Model Selection h2 Submitting Predictions on Kaggle h2 Summary h2 Exercises ", "stargazers_count": 0, "forks_count": 0, "description": "For example the year of constructionis represented by an integer the roof type by discrete categorical assignments and other features by floating point numbers. This leads to the following root mean squared error between the logarithm of the predicted price and the logarithm of the label price sqrt frac 1 n sum_ i 1 n left log y_i log hat y _i right 2. The platform helps users to interactvia forums and shared code fostering both collaboration and competition. Data PreprocessingAs stated above we have a wide variety of data types. Downloading and Caching DatasetsThroughout the book we will train and test modelson various downloaded datasets. The following code will generate a file called submission. Is it always a good idea to replace missing values by their mean Hint can you construct a situation where the values are not missing at random 1. Improve the score on Kaggle by tuning the hyperparameters through K fold cross validation. Logarithms are useful for relative errors. com d2l ai d2l pytorch colab blob master img kaggle_submit2. If they do it is time to upload them to Kaggle. So you will want to make sure that you have pandas installedbefore proceeding further. The training dataset includes 1460 examples 80 features and 1 label while the test datacontains 1459 examples and 80 features. To get started we will read in and process the datausing pandas which we have introduced in numref sec_pandas. We can use K fold cross validation to select the model and adjust the hyperparameters. edu ml machine learning databases housing housing. Submitting Predictions on KaggleNow that we know what a good choice of hyperparameters should be we might as well use all the data to train on it rather than just 1 1 K of the datathat are used in the cross validation slices. data by default and returns the name of the downloaded file. Note that running on Colab is experimental please report a Githubissue if you have any problem. The pandas package does this automatically for us. After all a small value delta for log y log hat y leq delta translates into e delta leq frac hat y y leq e delta. Submitting data to Kaggle https github. For instance MSZoning assumes the values RL and RM. First it proves convenient for optimization. To verify that this indeed transformsour feature variable such that it has zero mean and unit variance note that E frac x mu sigma frac mu mu sigma 0 and that E x mu 2 sigma 2 mu 2 2 mu 2 mu 2 sigma 2. For convenience we can download and cachethe Kaggle housing datasetusing the script we defined above. Notice that sometimes the number of training errorsfor a set of hyperparameters can be very low even as the number of errors on K fold cross validationis considerably higher. This includes features such as MSZoning. This dataset collected by Bart de Cock in 2011 cite De Cock. If a file corresponding to this datasetalready exists in the cache directoryand its SHA 1 matches the one stored in DATA_HUB our code will use the cached file to avoidclogging up your internet with redundant downloads. According to one hot encoding if the original value of MSZoning is RL then MSZoning_RL is 1 and MSZoning_RM is 0. Hence we remove it from the datasetbefore feeding the data into the model. We will want to partition the training setto create a validation set but we only get to evaluate our models on the official test setafter uploading predictions to Kaggle. In this section we will walk you through details ofdata preprocessing model design and hyperparameter selection. Less overfitting might indicate that our data can support a more powerful model. The following additional libraries are needed to run thisnotebook. Here we implement several utility functionsto facilitate data downloading. It proceeds by slicing out the i mathrm th segmentas validation data and returning the rest as training data. We will put this to good use to select the model designand to adjust the hyperparameters. Thus we tend to care more aboutthe relative error frac y hat y y than about the absolute error y hat y. Click the Submit Predictions or Late Submission button as of this writing the button is located on the right. Kaggle Kaggle https www. Saving the predictions in a csv filewill simplify uploading the results to Kaggle. com c house prices advanced regression techniques The house price prediction competition page. Improve the score by improving the model e. Unlike in previous sections our training functionswill rely on the Adam optimizer we will describe it in greater detail later. Each competition centers on a dataset and manyare sponsored by stakeholders who offer prizesto the winning solutions. With a large enough dataset and the normal sorts of hyperparameters K fold cross validation tends to bereasonably resilient against multiple testing. The steps are quite simple Log in to the Kaggle website and visit the house price prediction competition page. com d2l ai d2l pytorch colab blob master img kaggle. K Fold Cross ValidationYou might recall that we introduced K fold cross validationin the section where we discussed how to dealwith model selection numref sec_model_selection. Finding a good choice can take time depending on how many variables one optimizes over. Next as demonstrated in numref fig_kaggle_submit2 we can submit our predictions on Kaggleand see how they compare with the actual house prices labels on the test set. While leaderboard chasing often spirals out of control with researchers focusing myopically on preprocessing stepsrather than asking fundamental questions there is also tremendous value in the objectivity of a platformthat facilitates direct quantitative comparisonsamong competing approaches as well as code sharingso that everyone can learn what did and did not work. You can see that this conversion increasesthe number of features from 79 to 331. One nice sanity check is to seewhether the predictions on the test setresemble those of the K fold cross validation process. com is a popular platformthat hosts machine learning competitions. png raw 1 width 400px label fig_kaggle On the house price prediction competition page as illustratedin numref fig_house_pricing you can find the dataset under the Data tab submit predictions and see your ranking The URL is right here https www. We use pandas to load the two csv files containing training and test data respectively. The model that we obtain in this waycan then be applied to the test set. Let us take a look at the first four and last two featuresas well as the label SalePrice from the first four examples. 2011 covers house prices in Ames IA from the period of 2006 2010. The price of each house is includedfor the training set only it is a competition after all. First we apply a heuristic replacing all missing valuesby the corresponding feature s mean. We hope that through a hands on approach you will gain some intuitions that will guide youin your career as a data scientist. Finally via the values attribute we can extract the NumPy format from the pandas formatand convert it into the tensorrepresentation for training. Dropping the MSZoning feature two new indicator features MSZoning_RL and MSZoning_RM are created with values being either 0 or 1. Then to put all features on a common scale we standardize the data byrescaling features to zero mean and unit variance x leftarrow frac x mu sigma. We will need to preprocess the data before we can start modeling. The data are fairly generic and do not exhibit exotic structurethat might require specialized models as audio or video might. Throughout training you will want to monitor both numbers. We first need a function that returnsthe i mathrm th fold of the datain a K fold cross validation procedure. The main appeal of this optimizer is that despite doing no better and sometimes worse given unlimited resources for hyperparameter optimization people tend to find that it is significantly less sensitiveto the initial learning rate. While this is convenient it does not carryany information for prediction purposes. Transforming categorical features into indicator features allows us to treat them like one hot vectors. Let us start with the numerical features. This indicates that we are overfitting. We can see that in each example the first feature is the ID. Submit your predictions for this section to Kaggle. The training and verification error averages are returnedwhen we train K times in the K fold cross validation. Intuitively we standardize the datafor two reasons. With house prices as with stock prices we care about relative quantitiesmore than absolute quantities. This helps the model identify each training example. Note that this is not the most efficient way of handling dataand we would definitely do something much smarterif our dataset was considerably larger. In fact this is also the official error measureused by the competition to evaluate the quality of submissions. We also implement two additional utility functions one is to download and extract a zip or tar fileand the other to download all the datasets used in this book from DATA_HUB into the cache directory. The following download function downloads a dataset caching it in a local directory. Rescaling real valued data to zero mean and unit variance is a good default. Predicting House Prices on Kaggle label sec_kaggle_house Now that we have introduced some basic toolsfor building and training deep networksand regularizing them with techniques includingweight decay and dropout we are ready to put all this knowledge into practiceby participating in a Kaggle competition. Not surprisingly our linear model will not leadto a competition winning submissionbut it provides a sanity check to see whetherthere is meaningful information in the data. One way to address this problem is tomeasure the discrepancy in the logarithm of the price estimates. So is replacing missing values with their mean. On the other hand if we err by this amountin Los Altos Hills California this might represent a stunningly accurate prediction there the median house price exceeds 4 million USD. All such datasets are hosted at the sitewhose address is DATA_URL. Click the Upload Submission File button in the dashed box at the bottom of the page and select the prediction file you wish to upload. Next we deal with discrete values. names of Harrison and Rubinfeld 1978 boasting both more examples and more features. Fortunately if you are reading in Jupyter we can install pandas without even leaving the notebook. The house price prediction competitionis a great place to start. What happens if we do not standardize the continuous numerical features like what we have done in this section Discussions https discuss. layers weight decay and dropout. png raw 1 width 400px label fig_house_pricing Accessing and Reading the DatasetNote that the competition data is separatedinto training and test sets. And here is where reality complicates things for some examples some data are altogether missingwith the missing value marked simply as na. Second because we do not know a priori which features will be relevant we do not want to penalize coefficientsassigned to one feature more than on any other. Each record includes the property value of the houseand attributes such as street type year of construction roof type basement condition etc. If we cannot do better than random guessing here then there might be a good chancethat we have a data processing bug. Massive overfitting might suggest that we can gainby incorporating regularization techniques. And if things work the linear model will serve as a baselinegiving us some intuition about how close the simple modelgets to the best reported models giving us a senseof how much gain we should expect from fancier models. We replace them by a one hot encodingin the same way that we previously transformedmulticlass labels into vectors see numref subsec_classification problem. If you want to participate in a Kaggle competition you will first need to register for an account see numref fig_kaggle. TrainingTo get started we train a linear model with squared loss. However if we try an unreasonably large number of optionswe might just get lucky and find that our validationperformance is no longer representative of the true error. com d2l ai d2l pytorch colab blob master img house_pricing. How good are your predictions 1. For instance if our prediction is off by USD 100 000when estimating the price of a house in Rural Ohio where the value of a typical house is 125 000 USD then we are probably doing a horrible job. First we maintain a dictionary DATA_HUB that maps a string the name of the dataset to a tuple containing both the URL to locate the datasetand the SHA 1 key that verifies the integrity of the file. The features consist of various data types. But this added complexity might obfuscate our code unnecessarilyso we can safely omit it here owing to the simplicity of our problem. png raw 1 width 400px label fig_kaggle_submit2 Summary Real data often contain a mix of different data types and need to be preprocessed. Can you improve your model by minimizing the logarithm of prices directly What happens if you try to predict the logarithm of the price rather than the price 1. Click the Make Submission button at the bottom of the page to view your results. It is considerably larger than the famous Boston housing dataset https archive. The Data tab on the competition tabin numref fig_house_pricing has links to download the data. Model SelectionIn this example we pick an untuned set of hyperparametersand leave it up to the reader to improve the model. ai t 107 save save Hit cache save save If pandas is not installed please uncomment the following line pip install pandas save save After standardizing the data all means vanish hence we can set missing values to 0 Dummy_na True considers na missing value as a valid feature value and creates an indicator feature for it To further stabilize the value when the logarithm is taken set the value less than 1 as 1 The Adam optimization algorithm is used here Apply the network to the test set Reformat it to export to Kaggle. ", "id": "georgemitchellswe/notebook7fb74e7e82", "size": "14330", "language": "python", "html_url": "https://www.kaggle.com/code/georgemitchellswe/notebook7fb74e7e82", "git_url": "https://www.kaggle.com/code/georgemitchellswe/notebook7fb74e7e82", "script": "download log_rmse download_extract torch.nn d2l torch numpy pandas get_k_fold_data torch as d2l train_and_pred get_net k_fold train download_all ", "entities": "(('where we', 'model selection numref how sec_model_selection'), 'Fold') (('Model example we', 'model'), 'SelectionIn') (('We', 'training'), 'use') (('we', 'cross validation'), 'be') (('you', 'pandas installedbefore proceeding'), 'want') (('cross validation', 'bereasonably multiple testing'), 'fold') (('sanity One nice check', 'validation cross process'), 'be') (('code', 'redundant downloads'), 'use') (('1 test', '1459 examples'), 'include') (('1 rather just 1 K', 'cross validation slices'), 'submit') (('Hence we', 'model'), 'remove') (('Here we', 'data downloading'), 'implement') (('us', 'label last two well first four examples'), 'let') (('we', 'fancier models'), 'serve') (('us', 'numerical features'), 'let') (('dataset', 'De Cock'), 'cite') (('Adam optimization less than 1 as algorithm', 'Kaggle'), 'save') (('2011', '2006'), 'cover') (('following code', 'file'), 'generate') (('such datasets', 'sitewhose address'), 'host') (('altogether missing value', 'examples'), 'be') (('you', 'problem'), 'note') (('URL', 'ranking'), 'label') (('median house there price', '4 million USD'), 'represent') (('we', 'more other'), 'second') (('only it', 'house'), 'be') (('what', 'platformthat facilitates direct quantitative comparisonsamong competing approaches'), 'be') (('This', 'label price sqrt'), 'lead') (('We', 'hyperparameters'), 'use') (('we', 'Kaggle script'), 'download') (('We', 'hyperparameters'), 'put') (('features', 'data various types'), 'consist') (('we', 'regularization incorporating techniques'), 'suggest') (('we', 'data'), 'need') (('names', 'both more examples'), 'boast') (('it', 'prediction purposes'), 'carryany') (('platform', 'shared collaboration'), 'help') (('you', 'numbers'), 'want') (('steps', 'house price prediction competition page'), 'be') (('It', 'training data'), 'proceed') (('First it', 'optimization'), 'prove') (('Thus we', 'absolute error'), 'tend') (('E frac mu sigma', 'zero mean'), 'note') (('we', 'problem'), 'obfuscate') (('MSZoning_RL', 'values'), 'feature') (('who', 'winning solutions'), 'center') (('very even number', 'validationis'), 'notice') (('they', 'Kaggle'), 'be') (('unit variance', 'zero mean'), 'be') (('it', 'learning significantly less initial rate'), 'be') (('You', '331'), 'see') (('us', 'one hot vectors'), 'allow') (('record', 'construction roof type basement condition etc'), 'include') (('we', 'test then set'), 'apply') (('we', 'numref sec_pandas'), 'read') (('we', 'squared loss'), 'start') (('this', 'submissions'), 'be') (('first feature', 'example'), 'see') (('we', 'even notebook'), 'install') (('we', 'test various downloaded datasets'), 'Downloading') (('png raw 1 width 400px Summary Real data', 'data different types'), 'label') (('one', 'how many variables'), 'take') (('we', 'absolute quantities'), 'with') (('data', 'zero mean'), 'put') (('we', 'Kaggle competition'), 'introduce') (('just validationperformance', 'longer true error'), 'get') (('Data tab', 'data'), 'have') (('One way', 'price estimates'), 'be') (('we', 'training'), 'extract') (('Logarithms', 'relative errors'), 'be') (('K', 'validation procedure'), 'need') (('following additional libraries', 'thisnotebook'), 'need') (('data', 'more powerful model'), 'indicate') (('we', 'greater detail'), 'rely') (('you', 'rather price'), 'improve') (('competition data', 'png raw 1 width 400px DatasetNote'), 'label') (('we', 'ofdata model design selection'), 'walk') (('It', 'Boston housing dataset considerably famous https'), 'be') (('you', 'numref fig_kaggle'), 'need') (('1', 'MSZoning'), 'be') (('model', 'training example'), 'help') (('Discussions https', 'section'), 'happen') (('button', 'right'), 'click') (('Intuitively we', 'datafor two reasons'), 'standardize') (('that', 'data scientist'), 'hope') (('much dataset', 'definitely something'), 'note') (('125 000 then we', 'probably horrible job'), 'for') (('roof', 'point other floating numbers'), 'represent') (('we', 'Kaggle'), 'want') (('you', 'prediction file'), 'click') (('that', 'file'), 'maintain') (('download following function', 'local directory'), 'download') (('MSZoning', 'values'), 'assume') (('how they', 'test set'), 'submit') (('one', 'cache directory'), 'implement') (('here then good we', 'data processing bug'), 'do') (('pandas package', 'automatically us'), 'do') (('whetherthere', 'meaningful data'), 'leadto') (('corresponding feature', 'missing valuesby'), 'apply') (('that we', 'subsec_classification numref problem'), 'see') (('we', 'data types'), 'state') (('where values', 'random'), 'be') (('fairly exotic structurethat', 'audio might'), 'be') ", "extra": "['test', 'procedure']", "label": "Perfect_files", "potential_description_queries": ["absolute", "account", "adjust", "advanced", "algorithm", "apply", "approach", "attribute", "audio", "basement", "basic", "best", "blob", "book", "bottom", "box", "button", "cache", "care", "categorical", "check", "choice", "close", "code", "colab", "compare", "competition", "condition", "contain", "control", "conversion", "convert", "create", "csv", "data", "dataset", "decay", "default", "describe", "detail", "dictionary", "directly", "discrete", "download", "encoding", "error", "evaluate", "even", "everyone", "experimental", "export", "extract", "fact", "feature", "file", "find", "fold", "following", "format", "frac", "function", "generate", "generic", "hand", "hope", "hot", "house", "hyperparameter", "idea", "img", "implement", "improve", "indicate", "instance", "integer", "intuition", "key", "knowledge", "label", "leaderboard", "learn", "learning", "leave", "left", "line", "linear", "load", "local", "log", "look", "main", "mean", "median", "might", "missing", "ml", "model", "most", "mu", "multiple", "name", "need", "network", "new", "no", "normal", "not", "number", "numerical", "official", "optimization", "optimizer", "out", "overfitting", "package", "page", "people", "period", "place", "png", "point", "predict", "prediction", "preprocessing", "price", "priori", "problem", "processing", "property", "pytorch", "random", "ranking", "raw", "read", "reader", "reading", "recall", "record", "register", "regression", "regularization", "relative", "remove", "replace", "report", "rest", "right", "run", "running", "save", "scale", "score", "script", "section", "select", "selection", "set", "several", "simplicity", "situation", "something", "sqrt", "squared", "start", "string", "support", "test", "those", "through", "time", "train", "training", "try", "tuning", "tuple", "type", "under", "unit", "up", "valid", "validation", "value", "variable", "variance", "verify", "video", "view", "visit", "walk", "website", "weight", "while", "who", "width", "work", "year", "zip"], "potential_description_queries_len": 197, "potential_script_queries": ["numpy", "torch"], "potential_script_queries_len": 2, "potential_entities_queries": ["competition", "dataset", "following", "hot", "missing", "mu", "optimization", "price", "raw", "type", "width"], "potential_entities_queries_len": 11, "potential_extra_queries": ["procedure"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 199}