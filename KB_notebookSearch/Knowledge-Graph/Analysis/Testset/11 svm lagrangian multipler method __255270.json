{"name": "11 svm lagrangian multipler method ", "full_name": " h1 Support Vector Machine Lagrangian multipler Method h1 Hypothesis ", "stargazers_count": 0, "forks_count": 0, "description": "h x b w_0x_0 w_1x_1 OR h x W TX where W left begin matrix b w_0 w_1 end matrix right and X left begin matrix 1 x_0 x_1 end matrix right OR h x b wx where w left begin matrix w_0 w_1 end matrix right and x left begin matrix x_0 x_1 end matrix right MarginsDefine the equation H such that W TX 1 when Y 1 W TX 1 when Y 0 d is the shortest distance to the closest positive point d is the shortest distance to the closest negative pointThe margin of a separating hyperplane is M d d. Recall the distance from a point x_0 y_0 to a line A. h X 1 end cases implies Loss begin cases 0 Y. com support vector machine introduction to machine learning algorithms 934a444fca47 SVM Tutorial https www. x_1 b sqrt w_0 2 w_1 2 so The distance between H_0 and H_1 and between H_0 and H_2 is then M d d frac W TX W TX w frac 2 w Optimization ObjectiveIn order to maximize the margin we need to minimize frac 1 2 w 2 with the condition that W TX 1 when Y 1 W TX 1 when Y 0 This is a constrained optimization problem. Support Vector Machine Lagrangian multipler Method SVMs maximize the margin Winston terminology the street around the separating line or hyperplane. h x then equation becomes Loss begin cases 0 Y 1 Y. pdf Generate Data Visualize Data SVM Using Lagrangian Multipler Method Linear Kernel K X i X j X i TX j is dot product. h X frac 1 2 w 2 C is tuning parameter which will affect the Margin. H_0 H_1 and H_2 are the planes H_1 W TX 1 H_2 W TX 1 The points on the planes H_1 and H_2 are the tips of the Support Vectors The plane H_0 is the median in between Maximizing the margin d and d We want a classifier linear separator with as big a margin as possible. pdf not to overwrite Y referenced here Linear Kernel END IF END FOR end while Predict for each X1 and X2 in Grid w1. y_0 c sqrt A 2 B 2 Therefore D displaystyle frac w_0. W TX 1 Our objective is to minimize Error in predicted values. h X Final Loss FuctionNow Objective is mimimize the L W as well as maximize the margin by minimize frac 1 2 w 2 L W C frac 1 n displaystyle sum_ i 1 nmax 0 1 Y. x2 b 0 x2 b w2 w1 w2 x1 Exact Decision Boundry. y c 0 is D displaystyle frac A. h X 1 1 Y. h X 1 end cases implies Loss begin cases 0 1 Y. h X 1 end cases implies Loss begin cases 0 Y 1 Y. h X 1 Y. W TX 1 We predict hat Y 0 if h x 1 i. h X 1 0 Y 1 Y. Read More about dual problems and solving quadratic problems http web. When C is small Margin term will increase. Solving Optimization problem Methods Older methods Used techniques from Quadratic Programming Lagrangian multiplier method Gradient Descent Batch Gradient Descent Stochastic Gradient Descent Solving Lagrangian multiplier requires understanding of Linear Programing Dual Problems. h X 0 1 Y. Objective of this document is to understand how SVM works and tuning parameter C affects the margins. h X 0 end cases implies L W frac 1 n displaystyle sum_ i 1 nmax 0 1 Y. When C is large Margin term will reduce. 01 Training with Small Margin C 5 References LIBSVM Paper https www. tw cjlin papers libsvm. com 2015 06 svm understanding math part 3 Idiot s guide to Support vector machines http web. HypothesisWe start with assumpution equation Called hypothesis which can separte data in two classes. Loss Function We predict hat Y 1 if h x 1 i. pdf Towards Data Science https towardsdatascience. Error hat Y Y Where hat Y h X we define Loss Cost function as follows We calculate loss Loss begin cases 0 Y 1 h X 1 1 h X Y 1 h X 1 h X 1 Y 0 h X 1 0 Y 0 h X 1 end cases It is difficult to represent above equation Therefore we can change Y 0 to Y 1 and h x to Y. Training with Large Margin C 0. ", "id": "manmohan291/11-svm-lagrangian-multipler-method", "size": "255270", "language": "python", "html_url": "https://www.kaggle.com/code/manmohan291/11-svm-lagrangian-multipler-method", "git_url": "https://www.kaggle.com/code/manmohan291/11-svm-lagrangian-multipler-method", "script": "plot_Decision_Boundry numpy matplotlib.pyplot ListedColormap pandas predict matplotlib.colors accurracy SVM_Train ", "entities": "(('Loss We', 'hat Y'), 'function') (('pdf', 'Grid w1'), 'reference') (('L W', '1 displaystyle sum _ n i'), 'imply') (('Loss', '0 Y'), 'imply') (('Support Vector Machine multipler Method Lagrangian SVMs', 'separating line'), 'maximize') (('end equation W W 0 right such 1 Y 1 1 when Y d', 'separating hyperplane'), 'h') (('Loss', 'cases'), 'imply') (('which', 'Margin'), 'frac') (('W 1 objective', 'predicted values'), 'tx') (('W W TX 1 when Y 1 1 Y 0 This', 'condition'), 'sqrt') (('1 L W 2 2 C', 'minimize'), 'mimimize') (('Gradient Descent Batch Gradient Stochastic Gradient Descent Lagrangian multiplier', 'Dual Problems'), 'require') (('y_0 c', 'A'), 'sqrt') (('which', 'two classes'), 'start') (('Therefore we', 'h 1 Y.'), 'Y') (('Loss', 'cases'), 'begin') (('We', 'as big margin'), 'be') (('works', 'margins'), 'be') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["calculate", "classifier", "condition", "data", "define", "distance", "document", "dot", "end", "equation", "frac", "function", "http", "hyperplane", "learning", "left", "line", "linear", "margin", "math", "matrix", "median", "method", "minimize", "need", "negative", "not", "objective", "optimization", "order", "parameter", "part", "pdf", "plane", "point", "positive", "predict", "problem", "quadratic", "right", "shortest", "sqrt", "start", "support", "svm", "term", "tuning", "understanding", "vector", "while"], "potential_description_queries_len": 50, "potential_script_queries": ["numpy"], "potential_script_queries_len": 1, "potential_entities_queries": ["right", "sum"], "potential_entities_queries_len": 2, "potential_extra_queries": ["test"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 53}