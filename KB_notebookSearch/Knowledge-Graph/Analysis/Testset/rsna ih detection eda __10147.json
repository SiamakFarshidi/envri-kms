{"name": "rsna ih detection eda ", "full_name": " h1 What is intracranial hemorrhage h2 Our goal h2 Table of contents h1 Prepare to start h1 Exploratory analysis h2 Sample Submission h2 Evaluation metric h2 Target distribution h3 Insights h2 Number of samples h1 Preprocessing dicom files h2 What is given by a dicom file h2 Why can t we see something without windowing h2 What is covered by raw pixel values h3 Insights h2 What does pixel spacing mean h2 The doctors windows h3 Insights h2 My custom window h3 Insights h2 The image shape h2 Where to go next ", "stargazers_count": 0, "forks_count": 0, "description": "Exploratory analysis explore Sample submission sample_submission Evaluation metric evaluate Target distribution targets Number of samples num_samples 3. Jackpot This was the first time for me I m sorry that this happend. com ciods ct image image plane 00280030 All pixel spacing related Attributes are encoded as the physical distance between the centers of each two dimensional pixel specified by two numeric values. You can speed up a bit if you like by starting at 7min Ok I learnt. Going into details of each subtype we can see that we have to deal with high class imbalance. Now let s setup a window that is centered at 30 and has a width of 80. Let s try to understand the 2000 and 3000 cases of rescaled images Yeah Great We can see that both cases 2000 and 3000 correspond to the outside region ot cylindrical CT scanners. Thank you a lot Guido Zuidhof. By windowing to the same window we can t see differences between the median and the min case. We won t be able to see important changes in the intensity to detect the hemorrhage. Our eye can only detect 6 change in greyscale 16 shades of grey. In my current case can change as listdir choses file order at random it s 195 mm meaning 19. Epidural is the worst case. still don t know what this means. Nonetheless our model should still be able to detect the different types of hemorrhage. The first value is the row spacing in mm that is the spacing between the centers of adjacent rows or vertical spacing. Prepare to start prepare 2. For this purpose we have to make 6 decisions per image 5 subtypes and if there is an occurence any. But the resolution might differ from scan to scan even if patients show same head sizes. The second value is the column spacing in mm that is the spacing between the centers of adjacent columns or horizontal spacing. But we can transform the image to HU units by scaling with the slope and intercept. Let s understand it given our extreme examples. What is intracranial hemorrhage Hmm let s watch a video Thank you Armando I have no medical background and this was really great to understand the topic better Our goalWe are asked to predict the occurence and the subtype of intracranial hemorrhage. What does this mean for us It means that small heads do not automatically mean that this is a child. Preprocessing dicom files dicom What is given by a dicom file dicomfile Why can t we see something without windowing aboutwindows What is covered by raw pixel values pixelarray What does pixelspacing mean pixelspacing The doctors windows docwindows My custom window customwindow The image shape imageshape Prepare to start Exploratory analysis Sample Submission We can clearly see that we have to make several predictions for one image id epidural subdural subarachnoid intraparenchymal intraventricular any this one indicates that at least one subtype is present hence it tells us if the patient has IH or not. In contrast the widths of windows vary between 70 to 150 HUs. Where to go next The kernel had been much longer in the past but. I reached the end of the kaggle kernel world by exceeding the maximum kernel length that can be displayed. Consequently it s related to the physical distance. It will be difficult to train a model that is robust enough and does not tend to overfit. If we would simply put 256 shades of grey into one window this would differ from patient to patient as the given window ranges are different. I would say it s worth to add zooming as image augmentation technique to our workflow later. Let s see how different dicom datasets differ in the distribution of pixel array values Insights Crazy that we observe cases with 1000 and 2000. This is likely to correspond to air. After windowing to a center of 40 and width of 150 we can t see the same nice patterns as for the min and the median cases. Target distribution We need the same for our test data later Insights The first image already shows that we will have much more zero occurences than positive target values. For this type we only have a few 1 of positive occurrences. Table of contents1. This would be close to the median for both cases. Then we can see that mode is located at 1000 HU of air. Given 2000 HU of one image 1000 to 1000 this means that 1 greyscale covers 8 HUs. What does that mean We need to find it out Nonetheless the mode is located at 0 for all example raw pixel value arrays. What to do instead I would like to collect window centers and width of 1000 images to see the varity of doctos favorite windows. Consequently we would introduce a source of variation that is not given by original HU units per image. Consequently we can set these values to 1000 air in HU without worries. 6 times more training images than the test data. If you like to continue with the modelling part you can find it here https www. Consequently there can happen a change of 120 HUs unit our eye is able to detect an intensity change in the image. Then I would like to setup a fixed window level and width that covers the majority of all window properties. Insights The first extreme case seems to be faulty. But I don t know if we can be sure about it. Let s explore if we are better by focusing on our own window size My custom window Ok let s setup our own window width and center The majority of centers is located between 30 and 40 HUs. Is it possible that this can change from stage 1 to stage 2 What about the test data Ok for stage 1 this holds as well. In the description it s given that the log loss is first taken for each subtype s given an image id n l_ n s t_ n s cdot ln y_ n s 1 t_ n s cdot ln 1 y_ n s Then they seem to be added whereas the any subtype obtains a higher weight than the others l_ n sum_ s w_ s cdot l_ n s And finally this loss is averaged over all samples Loss frac 1 N cdot sum_ n l_ n Hopefully I got this right Interestingly the competition host has not provided the weights or did they. Consequently our raw pixel values are not given as HU units. It could also be an adult but zoomed out. Furthermore we have seen such combinations of width and center in our doc_windows dataframe. Evaluation metric The weighted multilabel logarithmic loss is used to score our model performance. What do you think The doctors windows Taking a look at the dicom dataset again we can see that there is already a window center and width given for us. Preprocessing dicom files Ok the filename is given by the ID_alphanum column. To understand this part the following tutorial was extremely helpful for me https www. So I decided to watch a video about understanding CT images. The pixel spacing yields the mm of physical distance of one pixel. WINDOW The level means where this window is centered. different tissues have different HUs. The example of a hemorrhage in the brain shows relevant HUs in the range of 8 70. Hence we can compute the overall distance covered by one image width or height This is the true length in mm covered by our image. Have to learn how to keep complex stuff simple more concise and light weighted. This is the reason why we have to focus 256 shades of grey into a small range window of HU units. But what does that mean Was this done by a doctor who set the range to visualise the hemorrhage Is this important for our algorithm We should be very careful now. that hounsfield units are a measurement to describe radiodensity. What does pixel spacing mean When browsing through the dicom files we can see that a value called pixel spacing changes as well I don t know what that means but perhaps we can understand it by looking at some extremes. Hence this is going to be an LB probing hyperparameter to keep in mind. Ok in this stage 1 we have almost 8. What is covered by raw pixel values No If we browse through the dicom files we can see that this is not true. Number of samples Let s check whether this matches the number of train images we have Ok as expected. com gzuidhof full preprocessing tutorial. I m expected that the dataset holds patients with varying true head sizes ranging from childrens to adults. But pixel values differ The image shape Ok it seems that most images are of shape 512x512. com allunia rsna ih detection baselineHappy kaggling. This way we can compare if a fixed custom window size is better suited that individual doctor window sizes. Reading in the docs https dicom. Insights Uhh Very bad Do you see it There are extreme outliers in the window widths and centers. This does not make sense I would have expected the doctos to focus on the brain tissue and we have already learnt that HU values are roughly between 8 70 in these cases. And in the maximum case it s 500 mm consequently 50 cm. rsplit of pandas we are lucky and can easily load images given the id column What is given by a dicom file Why can t we see something without windowing I haven t worked often with dicom images so far so I still get confused why we have someting like window center width and rescale parameters. ", "id": "allunia/rsna-ih-detection-eda", "size": "10147", "language": "python", "html_url": "https://www.kaggle.com/code/allunia/rsna-ih-detection-eda", "git_url": "https://www.kaggle.com/code/allunia/rsna-ih-detection-eda", "script": "skimage.transform VGG16 listdir rescale_pixelarray seaborn numpy get_window_value Sequence sklearn.model_selection preprocess_input as preprocess_resnet_50 preprocess_input as preprocess_vgg_16 matplotlib.pyplot keras.applications os augmenters pandas ResNet50 keras.applications.vgg16 keras.utils keras.applications.resnet50 resize preprocess_input set_manual_window imgaug train_test_split IPython.display augmenters as iaa HTML ", "entities": "(('1 this', 'stage'), 'be') (('Insights we', '1000'), 'let') (('you', 'it'), 'find') (('related Attributes', 'two numeric values'), 'ciod') (('com gzuidhof', 'full tutorial'), 'preprocessing') (('We', 'algorithm'), 'do') (('it', 'workflow'), 'say') (('Nonetheless mode', 'pixel value example raw arrays'), 'mean') (('com allunia rsna', 'detection'), 'kaggling') (('still why we', 'window center width'), 'be') (('Consequently it', 'physical distance'), 's') (('again we', 'window already us'), 'think') (('I', 'first me'), 'Jackpot') (('don we', 'it'), 'know') (('example', '8 70'), 'show') (('this', 'dicom files'), 'cover') (('dataset', 'adults'), 'expect') (('that', 'adjacent columns'), 'be') (('mode', 'air'), 'see') (('it', 'maximum case'), 's') (('window given ranges', 'patient'), 'differ') (('better goalWe', 'intracranial hemorrhage'), 'be') (('it', 'extreme examples'), 'let') (('that', 'kernel maximum length'), 'reach') (('So I', 'CT images'), 'decide') (('we', 'min'), 'see') (('we', 'slope'), 'transform') (('This', 'image'), 'compute') (('eye', 'image'), 'happen') (('pixel Consequently raw values', 'HU units'), 'give') (('why we', 'HU units'), 'be') (('we', 'Ok'), 'let') (('most images', 'shape'), 'differ') (('majority', '30 HUs'), 'let') (('logarithmic loss', 'model performance'), 'metric') (('we', 'positive occurrences'), 'have') (('filename', 'ID_alphanum column'), 'preprocesse') (('Where go', 'much longer past'), 'be') (('that', 'window properties'), 'like') (('Furthermore we', 'doc_windows dataframe'), 'see') (('Exploratory analysis', 'samples'), 'explore') (('competition Interestingly host', 'they'), 'give') (('even patients', 'head same sizes'), 'differ') (('widths', '70 to 150 HUs'), 'vary') (('hounsfield units', 'radiodensity'), 'be') (('HU already values', '8 roughly between cases'), 'make') (('patient', 'IH'), 'preprocesse') (('perhaps we', 'extremes'), 'mean') (('we', 'class high imbalance'), 'go') (('6 decisions', 'image'), 'have') (('Nonetheless model', 'hemorrhage'), 'be') (('that', 'image'), 'introduce') (('pixel spacing', 'one pixel'), 'yield') (('already we', 'target positive values'), 'distribution') (('it', 'random'), 'change') (('cases', 'CT cylindrical scanners'), 'let') (('eye', 'grey'), 'detect') (('automatically this', 'us'), 'mean') (('instead I', 'doctos favorite windows'), 'like') (('understand', 'me https extremely www'), 'be') (('that', '80'), 'let') (('1 greyscale', '8 HUs'), 'mean') (('we', 'median'), 'see') (('Consequently we', 'worries'), 'set') (('you', 'window extreme widths'), 'insight') (('that', 'model'), 'be') (('Hence this', 'LB probing mind'), 'go') (('We', 'hemorrhage'), 'win') (('that', 'adjacent rows'), 'be') (('This', 'cases'), 'be') ", "extra": "['patient', 'test']", "label": "Perfect_files", "potential_description_queries": ["air", "algorithm", "array", "augmentation", "background", "bit", "brain", "case", "center", "check", "close", "column", "compare", "competition", "compute", "contrast", "could", "ct", "current", "custom", "data", "dataset", "describe", "description", "detect", "detection", "dicom", "distance", "distribution", "doctor", "end", "evaluate", "even", "expected", "explore", "eye", "file", "filename", "find", "fixed", "following", "frac", "head", "height", "high", "hyperparameter", "id", "image", "individual", "intensity", "kaggle", "kernel", "learn", "least", "length", "let", "level", "light", "listdir", "load", "log", "look", "looking", "lot", "majority", "maximum", "mean", "meaning", "measurement", "median", "medical", "metric", "might", "min", "mm", "mode", "model", "modelling", "most", "my", "need", "next", "no", "not", "number", "numeric", "order", "out", "overall", "part", "past", "patient", "per", "pixel", "plane", "positive", "predict", "prepare", "preprocessing", "present", "purpose", "random", "range", "raw", "reason", "region", "rescale", "resolution", "right", "robust", "row", "sample_submission", "scaling", "scan", "score", "second", "sense", "set", "setup", "several", "shape", "size", "something", "source", "speed", "stage", "start", "stuff", "submission", "target", "technique", "test", "think", "through", "time", "tissue", "topic", "train", "training", "transform", "try", "tutorial", "type", "understanding", "unit", "up", "value", "variation", "vertical", "video", "visualise", "weight", "who", "width", "window", "workflow", "world", "worst", "zoomed"], "potential_description_queries_len": 159, "potential_script_queries": ["iaa", "imgaug", "numpy", "resize", "seaborn", "skimage"], "potential_script_queries_len": 6, "potential_entities_queries": ["high", "numeric", "raw"], "potential_entities_queries_len": 3, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 164}