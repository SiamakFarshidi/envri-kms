{"name": "hubmap efficientnet and linknet train ", "full_name": " h1 Versions h1 Refferences h1 tpu h1 todo h1 Init parameters packages gcs paths tpu h2 GCS PATHS h1 Datasets pipeline h1 Model h1 Model fit ", "stargazers_count": 0, "forks_count": 0, "description": "org tutorials images data_augmentation resizing_and_rescaling2. com wrrosa hubmap tf with tpu efficientunet 512x512 subm inference with submission 4. com wrrosa hubmap tf with tpu efficientunet 512x512 tfrecs how to create training and inference tfrecords 2. com vgarshin kidney unet model keras inference scriptVersionId 58536838 and https github. issue https github. org guide data_performance2. com vgarshin kaggle_kidney blob master kidney_train. image to tf sequence layer https www. tf \u7684\u7279\u6027 \u6d41\u7684\u6982\u5ff5 session \u7684\u6982\u5ff5 \u7ef4\u5ea6\u5bf9\u9f504. \u5c1d\u8bd5\u591a\u79cdloss https github. \u8bed\u8a00\u7279\u6027 tf \u7279\u6027 \u548c np \u7684\u5dee\u5f02 3. com tensorflow tensorflow issues 38762 tpu traning guide1. com wrrosa hubmap tf with tpu efficientunet 512x512 train training pipeline 3. com JunMa11 SegLoss Init parameters packages gcs_paths tpu GCS_PATHS Datasets pipeline Model Model fit. org guide distributed_training todo 1. ipynb tpu Issue tpu doesn t support numpy_function 1. Versions V1 V3 efficientnetb4 unet v4 efficientnetb0 linknet dce_jacobian loss v5 efficientnetb0 unet bce_jacobian v6 v8 add guassian data augmentation v14 efficientnetb0 unet 256 tiles v15 efficientnetb0 linknet v9 fix shuffbug v10 v49 try to find best learning rate for batchsize 1024 is about 1e 3 to 5e 4 version 43 best backbone is efficientnet b2 due to image resolution gpu with batchsize 32 run over 9 hours limit in version 48 v50 read paper and try more loss function v54 add extennal data https www. com baesiann glomeruli hubmap external 1024x1024 v60 tuning params for optimizer adam look forward v64 tpu tensorboard for analysis only find tpu profiler for gcp Refferences 1. ", "id": "drzhuzhe/hubmap-efficientnet-and-linknet-train", "size": "1860", "language": "python", "html_url": "https://www.kaggle.com/code/drzhuzhe/hubmap-efficientnet-and-linknet-train", "git_url": "https://www.kaggle.com/code/drzhuzhe/hubmap-efficientnet-and-linknet-train", "script": "albumentations get_custom_objects tensorflow.keras.utils tversky_loss dice_coe load_dataset aug_fn _parse_image_function numpy get_validation_dataset get_training_dataset tversky count_data_items bce_dice_loss bce_jaccard_loss focal_tversky_loss sklearn.model_selection KFold tensorflow_addons functools matplotlib.pyplot tensorflow pandas kaggle_datasets partial segmentation_models.losses dice_loss tensorflow.keras segmentation_models tensorflow_datasets KaggleDatasets backend backend as K datetime _parse_image ", "entities": "(('loss function more v54', 'data https extennal www'), 'add') (('gcs_paths', 'packages'), 'parameter') (('com baesiann glomeruli 1024x1024 v60 tuning hubmap external params', 'gcp Refferences'), 'find') (('com wrrosa hubmap', 'train training pipeline'), '512x512') (('tpu ipynb Issue', 'tpu t numpy_function'), 'support') ", "extra": "", "label": "No_extra_files", "potential_description_queries": ["augmentation", "backbone", "best", "blob", "create", "data", "efficientnet", "external", "find", "fix", "forward", "function", "glomeruli", "gpu", "https github", "image", "inference", "issue", "kidney", "layer", "learning", "look", "model", "np", "optimizer", "pipeline", "read", "resolution", "run", "sequence", "session", "submission", "support", "tensorflow", "tf", "train", "training", "try", "tuning", "version"], "potential_description_queries_len": 40, "potential_script_queries": ["backend", "datetime", "numpy", "partial", "tversky"], "potential_script_queries_len": 5, "potential_entities_queries": ["function", "training"], "potential_entities_queries_len": 2, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 44}