{"name": "lrp for neural machine translation ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. 0 and bias_nb_units D global network relevance conservation if bias_factor 1. load doc into memory open the file as read only read all text close the file split a loaded document into sentences clean a list of lines prepare regex for char filtering normalize unicode characters tokenize on white space convert to lowercase remove punctuation from each token remove non printable chars form each token remove tokens with numbers in them store as string save a list of clean sentences to file load dataset split into english german pairs clean sentences save clean pairs to file spot check load a clean dataset save a list of clean sentences to file load dataset reduce dataset size random shuffle split into train test save load a clean dataset load datasets fit a tokenizer max sentence length prepare english tokenizer prepare german tokenizer encode and pad sequences integer encode sequences pad sequences with 0 values one hot encode target sequence define NMT model define NMT model summarize defined model compile model model. add Dense tar_vocab activation softmax model. add Dense tar_vocab activation softmax compile model summarize defined model ATTENTION PART STARTS HERE ATTENTION PART FINISHES HERE prepare training data prepare validation data define model fit model generate target given source sequence evaluate the skill of the model translate encoded source text calculate BLEU score initialize gates i g f o pre activation gates i g f o activation gates i g f o pre activation gates i g f o activation gates i g f o pre activation gates i g f o activation word embedding dimension indices of the gates i f o for i in hout 5 print i shape 1 M shape D M print Numerator shape 1 M shape D M shape D Note local layer relevance conservation if bias_factor 1. read_csv Input data files are available in the. add Embedding src_vocab n_units input_length src_timesteps mask_zero True attention_probs model. 0 can be used for sanity check word embedding dimension number of classes gate g only gate g only CASE 1 positive AND negative scores occur deepest color is positive deepest color is negative CASE 2 ONLY positive scores occur CASE 3 ONLY negative scores occur forward pass number of classes initialize gate g only gate g only format reminder lrp_linear hin w b hout Rout bias_nb_units eps bias_factor. the below is the mathematical model for LSTM and dense layer we are directly sending the word embedings of input into the for loop where in the above cell we have taken the word embedings for a input sequence. For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. ", "id": "sitaramireddy1994/lrp-for-neural-machine-translation", "size": "220", "language": "python", "html_url": "https://www.kaggle.com/code/sitaramireddy1994/lrp-for-neural-machine-translation", "git_url": "https://www.kaggle.com/code/sitaramireddy1994/lrp-for-neural-machine-translation", "script": "corpus_bleu argmax display RepeatVector keras.layers newaxis as na keras.callbacks rotate getRGB lrp keras.preprocessing.sequence dump Embedding build_model clean_pairs Sequential load LSTM numpy.random shuffle max_length span_word numpy array Input nltk.translate.bleu_score TimeDistributed load_doc save_clean_data create_tokenizer load_clean_sentences Tokenizer merge pickle word_for_id ModelCheckpoint soft_max RNN matplotlib.pyplot Dense keras.utils pandas unicodedata keras.preprocessing.text evaluate_model to_categorical plot_model lrp_linear load_model normalize newaxis Model to_pairs define_model pad_sequences predict_sequence keras.utils.vis_utils rescale_score_by_abs encode_output keras.models IPython.display encode_sequences HTML html_heatmap ", "entities": "(('NMT model', 'model compile model defined model'), 'open') (('read_csv Input data files', 'the'), 'be') (('forward number', 'format lrp_linear hin w only b'), 'use') (('you', 'output'), 'list') (('we', 'input sequence'), 'be') (('1 M shape D print D M Numerator shape 1 M shape M shape D local layer', 'conservation'), 'add') (('It', 'python docker image https kaggle github'), 'come') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["calculate", "cell", "char", "check", "clean", "close", "color", "compile", "convert", "current", "data", "dataset", "define", "dimension", "directly", "directory", "doc", "document", "embedding", "encode", "environment", "evaluate", "file", "fit", "form", "format", "forward", "gate", "generate", "hot", "image", "initialize", "input", "integer", "kaggle", "layer", "length", "linear", "list", "load", "local", "loop", "max", "memory", "model", "negative", "network", "non", "normalize", "number", "open", "pad", "positive", "pre", "prepare", "print", "processing", "punctuation", "python", "random", "read", "reduce", "remove", "run", "running", "save", "score", "sentence", "sequence", "several", "shape", "shuffle", "size", "softmax", "source", "space", "split", "store", "string", "summarize", "target", "test", "text", "token", "tokenize", "train", "training", "translate", "validation", "word", "write"], "potential_description_queries_len": 91, "potential_script_queries": ["array", "display", "merge", "numpy", "pickle", "preprocessing", "rotate", "translate"], "potential_script_queries_len": 8, "potential_entities_queries": ["data", "kaggle", "model", "shape"], "potential_entities_queries_len": 4, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 96}