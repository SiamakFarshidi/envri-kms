{"name": "titanic a step by step intro to machine learning ", "full_name": " h1 Table of content h3 Check other Kaggle notebooks from Yvon Dalat h1 1 Introduction Loading libraries and dataset h2 1 1 Importing Library h2 1 2 Loading dataset h2 1 3 Analysis goal h2 1 4 A very first look into the data h1 2 Exploratory Data Analysis EDA Cleaning and Engineering features h2 2 1 Correcting and completing features h3 Detecting and correcting outliers h3 Completing features h2 2 2 Descriptive analysis univariate h2 2 3 Feature Engineering Bi variate statistical analysis h3 Pclass h3 Name length h3 Gender Sex h3 Age h3 Family SibSp and Parch h3 Fare h3 Cabin h3 Embarked h3 Titles h3 Extracting deck from cabin h2 2 4 Visualising updated dataset h2 2 5 Descriptive statistics h1 3 Correlation analysis Multi variate analysis h2 3 1 Correlation analysis with histograms and pivot tables h2 3 2 Dropping features h2 3 3 Pearson Correlation Heatmap h2 3 4 Pairplots h1 4 Predictive modelling cross validation hyperparameters and ensembling h2 4 1 Logistic Regression h2 4 2 Support Vector Machines supervised h2 4 3 k Nearest Neighbors algorithm k NN h2 4 4 Naive Bayes classifier h2 4 5 Perceptron h2 4 6 Linear SVC h2 4 7 Stochastic Gradient Descent sgd h2 4 8 Decision tree h2 4 9 Random Forests h2 4 10 Model summary h2 4 11 Model cross validation with K Fold h3 Cross validation scores h2 4 12 Hyperparameter tuning learning curves for selected classifiers h2 4 13 Selecting and combining the best classifiers h2 4 14 Ensembling h2 4 15 Summary of most important features h1 5 Producing the submission file for Kaggle h1 6 Credits ", "stargazers_count": 0, "forks_count": 0, "description": "value_counts Mapping Gender plot distributions of age of passengers who survived or did not survive Qcut is a quantile based discretization function to autimatically create categories not used here dataset Age pd. This is a useful frist step of our anblaysis in order to determine the empirical relationship between all features. I will combine those into the first category. The next step after dropping less relevant features is to scale them a very good recommendation from Konstantin s kernel It helps to boost the score. Which model to choose These are the results of my many submissions Submission 1 The prediction with KNeighborsClassifier KNN in Section 4. com ydalat happydb what 100 000 happy moments are telling us find out what 100 000 happy moments are telling us Work Life Balance survey an Exploratory Data Analysis of lifestyle best practices https www. com ydalat Titanic a step by step intro to Machine Learning https www. Correcting and completing features Detecting and correcting outliersReviewing the data there does not appear to be any aberrant or non acceptable data inputs. 7 dataset Fare 3. Model summaryI found that the picture illustrates the various model better than words. Credits Check other Kaggle notebooks from Yvon Dalat https www. com ydalat titanic a step by step intro to machine learning a practice run ar EDA and ML classification HappyDB a step by step application of Natural Language Processing https www. As we say in business diversity brings better results this seems to be true with algorithms as well 5. ageMany feature engineering steps were taken from Anisotropic s excellent kernel. This resulted in a significant improvement of the prediction accuracy on the test data score. value_counts Takes a scalar and returns a string with the css property color red if below 0. Finally the detect_outliers function will select only the outliers happening multiple times. You can try to remove them and rerun the prediction to observe the result with the following function Completing featuresThe. Lastly the right ensembling was best achieved with a votingclassifier with soft voting parameterOne last word please use this kernel as a first project to practice your ML Python skills. Model cross validation with K Fold 4. qcut dataset Name_length 6 labels False train Name_length. What worked better is to use the cross validation on selected algotirhms. qcut dataset Age 6 labels False Using categories as defined above Create new feature FamilySize as a combination of SibSp and Parch Create new feature IsAlone from FamilySize Create new feature Boys from FamilySize Interactive chart using cufflinks Remove all NULLS in the Fare column and create a new feature Categorical Fare Explore Fare distribution Apply log to Fare to reduce skewness distribution dataset. Mr below 20 Extracting deck from cabinA cabin number looks like C123 and the letter refers to the deck a big thanks to Nikas Donge. The problem with less important features is that they create more noice and actually take over the importance of real features like Sex and Pclass. O will drop this feature. This can lead to overweigthing the model with very high values. So many new terms new functions new approaches but the subject really interested me so I dived into it studied one line of code at a time and captured the references and explanations in this notebook. I also used multiple functions from Yassine Ghouzam. This cross validation object is a variation of KFold which returns stratified folds. Observations As indicated before Adaboost has the lowest correlations when compared to other predictors. reset_index drop True Qcut is a quantile based discretization function to autimatically create categories dataset Name_length pd. 3 Feature Engineering Bi variate statistical analysisOne of the first tasks in Data Analytics is to convert the variables into numerical ordinal values. AdaBoost is sensitive to noisy data and outliers. Producing the submission file for KaggleFinally having trained and fit all our first level and second level models we can now output the predictions into the proper format for submission to the Titanic competition. Women 0 higher chance than men 1 Younger people slightly more chance than older Being alone decreased your chance to survive. The lower right shows the classification accuracy on the test set. In data processing it is also known as data normalization. I also welcome your comments questions and feedback. For instance the visualization helps understand how RandomForest uses multiple Decision Trees the linear SVC or Nearest Neighbors grouping sample by their relative distance to each others. In our case cross validation will also be applied to compare the performances of different predictive modeling procedures. In addition I found out that AdaBoost does not do a good job with this dataset as the training score and cross validation score are quite far apart. We will keep this feature. Submission 3 Kaggle Version 85 The prediction with gsrandom_forest in Section 4. The goal there is to create the right categories between survived and not survived. I used the above graphs to optimize the parameters for Adaboost ExtraTrees RandomForest GradientBoost and SVC. This should be taken with a grain of salt as the intuition conveyed by these two dimensional examples does not necessarily carry over to real datasets. Family size of 3 or 4 from first pivot2. Naive Bayes classifiers are highly scalable requiring a number of parameters linear in the number of features in a learning problem. A sample is classified by a majority vote of its neighbors with the sample being assigned to the class most common among its k nearest neighbors k is a positive integer typically small. jpg cb 1452565877 Cross validation scores 4. 14 EnsemblingThis is the final step pulling it together with an amazing Voting function from sklearn. It estimates probabilities using a cumulative logistic distribution The first value shows the accuracy of this model The table after this shows the importance of each feature according this classifier. Loading dataset What are the data types for each feature Survived int Pclass int Name string Sex string Age float SibSp int Parch int Ticket string Fare float Cabin string Embarked string 1. 4 Visualising updated dataset 2. The goal is to predict this dependent variable only using the available independent variables. FacetGrid train_df col Embarked Feature selection X_train all features for training purpose but excluding Survived Y_train survival result of X Train and test are our 3 main datasets for the next sections test data for Kaggle submission Preparing data for Submission 1 print i score Preparing data for Submission 2 Cross validate model with Kfold stratified cross validation Modeling step Test differents algorithms Adaboost ExtraTrees Gradient boosting tunning SVC classifier Best score Random Forest Best score Concatenate all classifier results Preparing data for Submission 3 Submit File. IsAlone alone is not a good predictor of survival. Observations to fine tune our models First let s compare their best score after fine tuning their parameters 1. for the case of a linear kernel. Observations Log Fare categories are 0 to 2. I added many variations and additional features to improve the code as much as I could as well as additional visualization. Analysis goal The Survived variable is the outcome or dependent variable. In addition the slight difference between men and women go in different direction i. Some key take away from my personal experiments and what if analysis over the last couple of weeks The engineering of the right features is absolutely key. We will first compare in the next section the classifiers results between themselves and applied to the same test data. The interquartile range IQR is a measure of statistical dispersion being equal to the difference between the 75th and 25th percentiles or between upper and lower quartiles. Observations from the Pearson analysis Correlation coefficients with magnitude between 0. While others like decision trees can handle null values. Observations The pairplot graph all trivariate analysis into one figure. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. Pearson Correlation HeatmapThe Seaborn plotting package allows us to plot heatmaps showing the Pearson product moment correlation coefficient PPMCC correlation between features. 7 more survivors CabinIt appears that Has_Cabin has a strong impact on the Survival rate. Pearson is bivariate correlation measuring the linear correlation between two features. The general method of calculation is to determine the distribution mean and standard deviation for each feature. 13 Selecting and combining the best classifiers 4. com dp B07BNRRP7J ref_ cm_sw_r_kb_dp_TZzTAbQND85EE tag kpembed 20 linkCode kpe sift through 10 000 responses to help us define our unique path to better living. Name_lengthThe first graph shows the amount of people by Name_length. loc dataset Fare 2. The second one their average survival rates. Decision trees where the target variable can take continuous values typically real numbers are called regression trees. SVC 83It appears that GBC and SVMC are doing the best job on the Train data. This happens when the classifiers use many input features to include noise in each feature on the complete dataset and ends up memorizing the noise instead of finding the signal. Decision submit 3 as best predictor 6. The three rows represent the three different data set on the right. Pearson Correlation heatmap. In this case it is better to transform it with the log function to reduce the skewness and redistribute the data. info function below shows how complete or incomplete the datasets are. Men alone have less chance than accompanied. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. It is a type of linear classifier i. to 2MFO6Iy 360 Living Practical guidance for a balanced life https images na. jpg Note Ever feel burnt out Missing a deeper meaning Sometimes life gets off balance but with the right steps we can find the personal path to authentic happiness and balance. 6 Fare 2 Feature that tells whether a passenger had a cabin on the Titanic O if no cabin number 1 otherwise Remove all NULLS in the Embarked column Mapping Embarked Takes a scalar and returns a string with the css property color red if below 0. Create a new feature Title containing the titles of passenger names Mapping titles Explore Age vs Survived for dataset in full_data dataset Boys 0 dataset. There are potential outliers that we will identify steps from Yassine Ghouzam It creates firset a function called detect_outliers implementing the Tukey method For each column of the dataframe this function calculates the 25th percentile Q1 and 75th percentile Q3 values. Introduction Loading libraries and dataset2. In this case Name Categorical Sex b Numeric or quantitative data Discrete could be ordinal like Pclass or not like Survived. a stronger ability to apply the model to new data. Correlation analysis with histograms and pivot tables Observations for Age graph 0 or blue represent women 1 or orange represent men. A test dataset has been created to test our algorithm. It involves the analysis of one or two features and their relative impact of Survived. In the next section we will cross validate the models using sample data against each others. In simple words it allows to test how well the model performs on new data. Credits Huge credits to Anisotropic Yassine Ghouzam Faron and Sina for pulling together most of the code in this kernel. Scaling features is helpful for many ML algorithms like KNN for example it really boosts their score. We will create a new feature called young boys Observations here are the survivors 1. Don Rev Capt Jonkheer no data4. 7 Stochastic Gradient Descent 4. Naive Bayes classifierThis is a family of simple probabilistic classifiers based on applying Bayes theorem with strong naive independence assumptions between the features. Bottom line it would have been better for women without cabin to pretend that they were alone. The plots show training points in solid colors and testing points semi transparent. Then ensemble them together with an automatic function callled voting. PairplotsFinally let us generate some pairplots to observe the distribution of data from one feature to the other. Descriptive analysis univariate 2. generates a public score of 0. The categories are sized to group passengers with similar Survival rates. This is the case here with Gradient Boost high score but cross validation is very distant. Variance with a lower bias comes typically a higher the variance. It is a binary nominal datatype of 1 for survived and 0 for did not survive. Predictive modelling cross validation hyperparameters and ensembling5. image http scikit learn. The folds are made by preserving the percentage of samples for each class. Cross validation is a way to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available. At the same time there have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores. loc dataset Age 0 dataset Sex 1 Boys 1 dataset Boys. In other words SGD tries to find minima or maxima by iteration. com kagglesharingmarkpeng20151216finalpresented 151216161621 95 general tips for participating kaggle competitions 13 638. Introduction Loading libraries and datasetI created this Python notebook as the learning notes of my first Machine Learning project. Mme Ms Lady Sir Mlle Countess 100. Afterwords we will convert the feature into a numeric variable. Stochastic Gradient Descent sgd This is a stochastic approximation of the gradient descent optimization and iterative method for minimizing an objective function that is written as a sum of differentiable functions. This indicates that it predicts differently than the others when it comes to the test data. 7 indicate variables which can be considered moderately correlated. This can be due to its strong relationship with other features such as Is_Alone or Parch Parent Children. PclassEmbarked does not seem to have a clear impact on the survival rate. We will analyse it further in the next sections and drop it if we cannot demonstrate a proven relationship to Survived. Logistic Regression 4. RandomForest operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes classification or mean prediction regression of the individual trees. Then we divide the values mean is already subtracted of each feature by its standard deviation. This is good because we want to keep the model as close to the training data as possible. drop Outliers_to_drop axis 0. The clustering of red dots indicates the combination of two features results in higher survival rates or the opposite clustering of blue dots lower survival For example Smaller family sizes in first and second class Middle age with Pclass in third category only blue dotThis can be used to validate that we extracted the right features or help us define new ones. 14 after stratification and model cross validation generates a public score of 0. PassengerID 28 89 and 342 passenger have an high Ticket Fare The seven others have very high values of SibSP. But it shows how each classifier algorithm partitions the same data in different ways. All outlier data get then pulled into the outlier_indices dataframe. ExtraTrees implements a meta estimator that fits a number of randomized decision trees a. com ydalat work life balance best practices eda key insights into the factors affecting our work life balance Work Life Balance survey a Machine Learning analysis of best practices to rebalance our lives https www. Define function to extract titles from passenger names If the title exists extract and return it. The algorithm itself is a fork from Anisotropic s Introduction to Ensembling Stacking in Python a great notebook in itself. This is the datadframe that will be returned. Mrs Miss around 70 survival2. And lone men should join a family to improve their survival rates. com images I 61EhntLIyBL. Feature scaling is a method used to standardize the range of independent variables or features of data. It builds the model in a stage wise fashion like other boosting methods do and it generalizes them by allowing optimization of an arbitrary differentiable loss function. I shameless ley sotle and learnt from many Kagglers through my learning process please do the same with the code in this kernel. The outlier_list_col column captures the indices of these outliers. So I decided to keep them. It is OK to select algorithms with various results as there is strenght in diversity. Cross validation process https image. The reality os that the algorithms work with many dimensions 11 in our case. If k 1 then the object is simply assigned to the class of that single nearest neighbor. k Nearest Neighbors algorithm k NN 4. We start to find where most survivors are older women 48 to 64 year old and younger passengers. 12 Hyperparameter tuning learning curves for selected classifiers 4. We will drop unncessary features just before Section 3. Decision treeThis predictive model maps features tree branches to conclusions about the target value tree leaves. Missing values can be bad because some algorithms don t know how to handle null values and will fail. 12 Hyperparameter tuning learning curves for selected classifiers 1. Top right in the graph above first class and age categories 1 and 2 The not so lucky are mostly in men Pclass 3 and age category 1 younger folks Observations The colors represent blue 0 is for women green 1 for men Clearly women had more chance of surviving with or without cabin Interesting is that accompanied women without a cabin had less survival chance than women alone without cabin. 2 dataset Fare 3. Any data points outside 1. 5 time IQR above Q3 is considered an outlier. There are multiple types of data a Qualitative data discrete Nominal no natural order between categories. Fare Observations The Fare distribution is very skewed to the left. Exploratory analysis engineering and cleaning features Bi variate analysis3. A very first look into the dataThis is only a quick of the relationships between features before we start a more detailed analysis. Support Vector Machines supervised Given a set of training samples each sample is marked as belonging to one or the other of two categories. extra trees on various sub samples of the dataset and use averaging to improve the predictive accuracy and control over fitting. All descripotion adapted from Wikipedia. An ensemble is a supervised learning algorithm that it can be trained and then used to make predictions. Adaboost is used in conjunction with many other types of learning algorithms to improve performance. Submission 2 The prediction with random_forest in Section 4. The algorithm allows for online learning in that it processes elements in the training set one at a time. This results in some classifiers Decision_tree and Random_Forest over fitting the model to the training data. All other variables are potential predictor or independent variables. The missing values will be converted to zero. The last line applied the ensemble predictor to the test data for submission. This overfit model will then make predictions based on that noise. Naive Bayes classifier 4. They do not have to be the same size or equally distributed. It provides train test indices to split data in train test sets. 2 with the fillna function dataset Fare dataset Fare. I tried many many different algorightms. Observation This classfier confirms the importance of Name_length FamilySize did not show a strong Pearson correlation with Survived but comes here to the top. PerceptronThis is an algorithm for supervised learning of binary classifiers like the other classifiers before it decides whether an input represented by a vector of numbers belongs to some specific class or not. 15 _images plot_classifier_comparison_0011. Logistic RegressionLogistic regression measures the relationship between the categorical dependent feature in our case Survived and the other independent features. Gender Sex AgeThe best categories for age are 0 Less than 14 1 14 to 30 2 30 to 40 3 40 to 50 4 50 to 60 5 60 and more Family SibSp and ParchThis section creates a new feature called FamilySize consisting of SibSp and Parch. TitlesThere are 4 types of titles 0. We can see the red and green curves from ExtraTrees RandomForest and SVC are pretty close. And therefore the risk that the model will not adapt accurately to new test data. Predictive modelling cross validation hyperparameters and ensembling 4. GradientBoost produces a prediction model in the form of an ensemble of weak prediction models typically decision trees. The approach to to complete missing data is to impute using mean median or mean randomized standard deviation. Random forests or random decision forests are an ensemble learning method for classification regression and other tasks that operate by constructing a multitude of decision trees n_estimators 100 at training time and outputting the class that is the mode of the classes classification or mean prediction regression of the individual trees 4. An easy direction to follow. SVMC or Support Vector Machines. Dropping featuresBottom line of the bi variate and tri variate analysis as well as the feature importance analysis from running the classifiers multiple times I decided to drop less relevant features. Support Vector Machines supervised 4. Importing Library 1. Dr Major Col around 40 5. Similarly having a cabin increases the chance of survival. Summary of most important features 4. The output of the other learning algorithms weak learners is combined into a weighted sum that represents the final output of the boosted classifier. We can see from the red cells that many features are moderately correlated specifically IsAlone Pclass Name_length Fare Sex. This points to a lower variance i. His notebook was itself based on Faron s Stacking Starter as well as Sina s Best Working Classfier. Next we subtract the mean from each feature. Correlation analysis Multi variate analysisThis section summarizes bivariate analysis asthe simplest forms of quantitative statistical analysis. But this is not true for men. But not too close The two major sources of error are bias and variance as we reduce these two then we could build more accurate models Bias The less biased a method the greater its ability to fit data well. vGiven a set of training examples each marked as belonging to one or the other of two categories an SVM training algorithm builds a model that assigns new examples to one category or the other making it a non probabilistic binary linear classifier. 5 time IQR below Q1 or 1. Therefore we re going to extract these and create a new feature that contains a persons deck. Exploratory Data Analysis EDA Cleaning and Engineering featuresWe will start with a standard approach of any kernel correct complete engineer the right features for analysis. There are null values or missing data in the age cabin and embarked field. Load libraries for analysis and visualization collection of functions for data processing and analysis modeled after R dataframes with SQL like features foundational package for scientific computing Regular expression operations Collection of functions for scientific and publication ready visualization Open source library for composing editing and sharing interactive data visualization Machine learning libraries Implementation of gradient boosted decision trees designed for speed and performance that is dominative competitive machine learning Visualization library based on matplotlib provides interface for drawing attractive statistical graphics Collection of machine learning algorithms Load in the train and test datasets from the CSV files Store our passenger ID for easy access Display the first 5 rows of the dataset a first look at our data 5 first row 5 sample rows and basic statistics Outlier detection iterate over features columns 1st quartile 25 3rd quartile 75 Interquartile range IQR outlier step Determine a list of indices of outliers for feature col append the found outlier indices for col to the list of outlier indices select observations containing more than 2 outliers detect outliers from Age SibSp Parch and Fare Show the outliers rows Drop outliers train train. Descriptive statistics Initial observations from the descriptive statistics Only 38 survived a real tragedy Passengers in more expensive classes 1 and 2 had much higher chance of surviving than classes 3 or 4. com ydalat work life balance predictors and clustering discover the strongest predictors of work life balance Interested in more facts and data to balance your life check the 360 Living guide https amzn. Summary of most important featuresNice graphics but the obsevation is unclear in my opinion On one side we hope as analyst that the models come out with similar patterns. Check out how Machine Learning and statistical analysis https www. Random decision forests correct for decision trees habit of overfitting to their training set. png Observations The above models classifiers were applied to a split training and x_test datasets. Feature standardization makes the values of each feature in the data have zero mean when subtracting the mean in the numerator and unit variance. Stratified K Folds is a cross validation iterator. 13 Selecting and combining the best classifiersSo how do we achieve the best trade off beween bias and variance 1. k Nearest Neighbors algorithm k NN This is a non parametric method used for classification and regression. The proposed categories are less than 23 mostly men 24 to 28 29 to 40 41 and more mostly women. 9 generates a public score of 0. What helped best is to group together passengers with the same survival rates. Gender and age seem to have a stronger influece of the survival rate. This is influenced by the following two factors 1 Women versus men and the compounding effect of Name_length and 2 Passengers paying a high price Fare have a higher chance of survival there are also in first class have a title. 3 types of deck 1 with 15 passengers 2 to 6 and 7 to 8 most passengers 2. Correlation analysis Tri variate analysis4. We will do this in section 2. EmbarkedIrrespective of the class passengers embarked in 0 S and 2 Q have lower chance of survival. Model cross validation with K FoldThe fitting process applied above optimizes the model parameters to make the model fit the training data as well as possible. The SVM training algorithm builds a model that assigns new test samples to one category or the other making it a non probabilistic binary linear classifier. We will therefore ensemble the remaining four predictors. The target features take a finite set of values are called classification trees in these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Random ForestsThis is one of the most popular classfier. Also the higher the fare the higher the chance. The reverse also holds the greater the bias the lower the variance. Many overfit the training data up to 90 but do not generate more accurate predictions with the test data. What is statistically interesting is that only young boys Age Category 0 have high survival rates unlike other age groups for men. A high bias method builds simplistic models that generally don t fit well training data. 7 less survivors More than 2. FacetGrid train_df col Pclass hue Survived graph distribution of qualitative data Pclass grid sns. Observations The Detect_Outliers function found 10 outliers. Women and men alone on first class second pivot red showing survival rate below 0. We will this by using StratifiedKFold to train and test the models on sample data from the overall dataset. Linear SVCThis is another implementation of Support Vector Classification similar to 4. IsAlone does not result in a significant difference of survival rate. This happened as an iterative process by reviwing the outcome of the feature importance graph in the next section. I found that dropping the outliers actually lower the prediction. The Seaborn pairplot class will help us visualize the distribution of a feature in relationship to each others. It performs unusually well on its training data but will not necessarilyimprove the prediction quality with new data from the test dataset. ", "id": "ydalat/titanic-a-step-by-step-intro-to-machine-learning", "size": "27097", "language": "python", "html_url": "https://www.kaggle.com/code/ydalat/titanic-a-step-by-step-intro-to-machine-learning", "git_url": "https://www.kaggle.com/code/ydalat/titanic-a-step-by-step-intro-to-machine-learning", "script": "sklearn.metrics cross_val_score sklearn.naive_bayes sklearn.tree detect_outliers plotly.offline AdaBoostClassifier plot_learning_curve KNeighborsClassifier DecisionTreeClassifier color_negative_red collections pyplot LinearSVC numpy seaborn plotly.graph_objs SGDClassifier learning_curve cufflinks sklearn.ensemble sklearn.model_selection confusion_matrix KFold plotly.tools matplotlib.pyplot Perceptron pandas classification_report StandardScaler (RandomForestClassifier LogisticRegression Counter accuracy_score precision_recall_curve GridSearchCV sklearn.neighbors SVC sklearn.linear_model matplotlib sklearn.cross_validation sklearn.svm GaussianNB StratifiedKFold sklearn.preprocessing xgboost get_title train_test_split ", "entities": "(('models', 'similar patterns'), 'be') (('lower right', 'test set'), 'show') (('algorithms', 'don how null values'), 'be') (('don generally t', 'training well data'), 'build') (('that', 'differentiable functions'), 'sgd') (('SGD', 'iteration'), 'try') (('5 time IQR', 'Q3'), 'consider') (('slight difference', 'different direction'), 'go') (('boys young here survivors', 'new feature'), 'create') (('it', 'data'), 'be') (('how well model', 'new data'), 'allow') (('reverse', 'the lower variance'), 'hold') (('Pearson', 'two features'), 'be') (('Introduction Loading libraries', 'Machine Learning first project'), 'create') (('IsAlone', 'alone good survival'), 'be') (('classifiers', 'test same data'), 'compare') (('strongest predictors', 'Living guide https 360 amzn'), 'discover') (('when it', 'test data'), 'indicate') (('Linear SVCThis', 'similar 4'), 'be') (('more one', 'better scores'), 'be') (('that', 'decision trees randomized a.'), 'implement') (('This', 'non parametric classification'), 'algorithm') (('helped', 'survival same rates'), 'be') (('they', 'Sex'), 'be') (('It', 'train test sets'), 'provide') (('graph', 'one figure'), 'observation') (('proposed categories', '40 41 women'), 'be') (('lone men', 'survival rates'), 'join') (('that', 'class labels'), 'take') (('title', 'it'), 'function') (('GradientBoost', 'typically trees'), 'produce') (('Boys', 'full_data'), 'create') (('This', 'features'), 'be') (('It', 'diversity'), 'be') (('Mapping Embarked', '0'), 'feature') (('multiple times I', 'less relevant features'), 'drop') (('Fare Fare distribution', 'very left'), 'observation') (('interquartile range IQR', 'upper quartiles'), 'be') (('This', 'very high values'), 'lead') (('when classifiers', 'instead signal'), 'happen') (('PclassEmbarked', 'survival rate'), 'seem') (('many features', 'IsAlone Pclass Name_length Fare moderately specifically Sex'), 'see') (('when explicit validation', 'hypothetical validation'), 'be') (('score Best Concatenate', 'Submit Submission 3 File'), 'FacetGrid') (('This', 'such Is_Alone'), 'be') (('we', 'Survived'), 'analyse') (('input', 'specific class'), 'be') (('summarizes', 'quantitative statistical analysis'), 'analysis') (('training points', 'solid colors'), 'show') (('that', 'boosted classifier'), 'combine') (('100 000 happy moments', 'lifestyle'), 'com') (('it', 'loss arbitrary differentiable function'), 'build') (('data types', 'Survived Pclass int Name Sex string int Age'), 'loading') (('14 EnsemblingThis', 'sklearn'), 'be') (('ley sotle', 'kernel'), 'shameless') (('we', 'more detailed analysis'), 'be') (('subsequent weak learners', 'previous classifiers'), 'be') (('picture', 'better words'), 'find') (('overfit model', 'noise'), 'make') (('I', 'comments also questions'), 'welcome') (('others', 'null values'), 'handle') (('The', 'data'), 'close') (('notebook', 'Stacking Starter'), 'be') (('k', 'most common k nearest neighbors'), 'classify') (('categories', 'Survival similar rates'), 'size') (('First s', 'parameters'), 'observation') (('It', 'score'), 'be') (('we', 'training close data'), 'be') (('Women', 'alone chance'), 'decrease') (('it', 'time'), 'allow') (('png models above classifiers', 'split training'), 'observation') (('predictions', 'feature vector'), 'algorithm') (('graph distribution', 'sns'), 'FacetGrid') (('model', 'Section'), 'be') (('it', 'really score'), 'be') (('which', 'variables'), 'indicate') (('who', 'Age here pd'), 'be') (('us', 'new ones'), 'indicate') (('outlier data', 'outlier_indices then dataframe'), 'get') (('Observations Log Fare categories', '2'), 'be') (('they', 'cabin'), 'line') (('folds', 'class'), 'make') (('learning supervised it', 'then predictions'), 'be') (('so I', 'notebook'), 'approach') (('typically real numbers', 'continuous values'), 'call') (('data', 'Detecting'), 'appear') (('IsAlone', 'survival rate'), 'result') (('EDA Cleaning', 'analysis'), 'start') (('qcut', 'False Name_length'), 'dataset') (('Observations Detect_Outliers function', '10 outliers'), 'find') (('We', 'overall dataset'), 'will') (('Name Categorical Sex quantitative Discrete', 'Survived'), 'b') (('test dataset', 'algorithm'), 'create') (('It', 'relative Survived'), 'involve') (('which', 'stratified folds'), 'be') (('0 Sex', '1 1 Boys'), 'dataset') (('Predictive modelling', '4'), 'cross') (('decision Random forests', 'training set'), 'correct') (('this', 'algorithms'), 'bring') (('zero', 'numerator variance'), 'make') (('Naive Bayes classifiers', 'learning problem'), 'be') (('three rows', 'right'), 'represent') (('I', 'Adaboost ExtraTrees RandomForest GradientBoost'), 'use') (('us', 'others'), 'help') (('goal', 'only available independent variables'), 'be') (('Random ForestsThis', 'most popular classfier'), 'be') (('Drop outliers', 'train train'), 'library') (('Predictive modelling', 'validation hyperparameters'), 'cross') (('It', 'survived'), 'be') (('Feature scaling', 'data'), 'be') (('model', 'training data'), 'cross') (('as much I', 'additional code'), 'add') (('GBC', 'Train data'), 'appear') (('Variance', 'typically a higher variance'), 'come') (('we', 'others'), 'validate') (('it', 'one category'), 'build') (('we', 'Titanic competition'), 'output') (('Next we', 'feature'), 'subtract') (('this', 'classifier'), 'estimate') (('us', 'better living'), 'com') (('worked', 'selected algotirhms'), 'be') (('We', 'just Section'), 'drop') (('that', 'alone cabin'), 'right') (('Categorical Fare Explore Fare distribution Apply', 'distribution skewness dataset'), 'dataset') (('red curves', 'ExtraTrees RandomForest'), 'see') (('ar practice EDA', 'Natural Language Processing https www'), 'com') (('Name_lengthThe first graph', 'Name_length'), 'show') (('Adaboost', 'performance'), 'use') (('sample', 'two categories'), 'supervise') (('validation score', 'training score'), 'find') (('value_counts', '0'), 'take') (('it', 'data also normalization'), 'know') (('Credits', 'Yvon Dalat https www'), 'check') (('C123', 'Nikas big thanks Donge'), 'look') (('Afterwords we', 'numeric variable'), 'convert') (('intuition', 'necessarily real datasets'), 'take') (('Family 0 Less than 14 1 14 to 30 2 30 to 40 3 40 to 50 4 50 to SibSp section', 'SibSp'), 'Sex') (('price high Fare', 'title'), 'influence') (('relationship', 'case'), 'measure') (('that', 'persons deck'), 'go') (('Many', 'test data'), 'overfit') (('I', 'Yassine Ghouzam'), 'use') (('You', 'following function'), 'try') (('last line', 'submission'), 'apply') (('Adaboost', 'when other predictors'), 'observation') (('that', 'individual trees'), 'operate') (('statistically only young boys', 'men'), 'be') (('Men', 'alone less chance'), 'have') (('dropping', 'actually prediction'), 'find') (('This', 'test data score'), 'result') (('missing values', 'zero'), 'convert') (('I', 'first category'), 'combine') (('Only 38', 'classes'), 'survive') (('work life com ydalat balance best practices', 'lives https www'), 'eda') (('importance', 'here top'), 'Observation') (('how we', 'beween bias'), 'achieve') (('AdaBoost', 'noisy data'), 'be') (('model stratification validation', '0'), '14') (('It', 'test dataset'), 'perform') (('us', 'features'), 'Correlation') (('discretization quantile based autimatically categories', 'Name_length pd'), 'drop') (('PairplotsFinally us', 'other'), 'let') (('Gender', 'survival rate'), 'seem') (('EmbarkedIrrespective', 'survival'), 'embark') (('variate', 'numerical ordinal values'), '3') (('how RandomForest', 'others'), 'help') (('detect_outliers Finally function', 'only outliers'), 'select') (('algorithm partitions', 'same different ways'), 'show') (('This', 'next section'), 'happen') (('we', 'authentic happiness'), 'Note') (('12 Hyperparameter', 'selected classifiers'), 'tuning') (('algorithms', '11 case'), 'os') (('model predictive maps', 'target value tree leaves'), 'feature') (('Qualitative data', 'natural categories'), 'be') (('therefore model', 'test accurately new data'), 'risk') (('engineering', 'right features'), 'take') (('validation', 'Gradient here Boost high score'), 'be') (('algorithm', 'great itself'), 'be') (('Naive classifierThis', 'features'), 'Bayes') (('that', 'individual trees'), 'be') (('last word', 'ML Python skills'), 'achieve') (('general method', 'standard feature'), 'be') (('feature engineering steps', 'excellent kernel'), 'take') (('Similarly cabin', 'survival'), 'have') (('values', 'standard deviation'), 'divide') (('validation', 'modeling different predictive procedures'), 'cross') (('approach', 'standard deviation'), 'be') (('Fare', 'Fare'), 'dataset') (('seven others', 'SibSP'), 'have') (('outlier_list_col column', 'outliers'), 'capture') (('1 then object', 'single nearest neighbor'), 'assign') (('function', 'Q3 values'), 'be') (('We', 'therefore remaining four predictors'), 'ensemble') (('Has_Cabin', 'Survival rate'), 'appear') ", "extra": "['biopsy of the greater curvature', 'gender', 'outcome', 'test', 'procedure']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "age", "algorithm", "appear", "append", "application", "apply", "approach", "average", "balance", "basic", "best", "binary", "boosting", "build", "cabin", "calculation", "case", "categorical", "category", "cb", "chart", "check", "choose", "classification", "classifier", "cleaning", "clear", "close", "clustering", "code", "coefficient", "col", "collection", "color", "column", "combine", "combined", "compare", "competition", "control", "convert", "correct", "correlation", "correlations", "could", "create", "data", "dataframe", "dataset", "decision", "define", "dependent", "detect", "detection", "difference", "direction", "discrete", "distance", "distribution", "diversity", "drop", "eda", "effect", "engineering", "ensemble", "ensembling", "equal", "error", "estimator", "expression", "extract", "family", "fare", "fashion", "feature", "file", "final", "find", "fit", "fitting", "float", "following", "form", "format", "found", "function", "general", "generate", "gradient", "graph", "green", "grid", "group", "guidance", "handle", "help", "high", "hope", "http", "hue", "image", "implementation", "importance", "improve", "improvement", "impute", "include", "indicate", "individual", "info", "input", "instance", "int", "integer", "interactive", "intuition", "itself", "job", "join", "kaggle", "kernel", "key", "lead", "learning", "let", "letter", "level", "library", "life", "line", "linear", "list", "log", "look", "lower", "magnitude", "main", "major", "majority", "matplotlib", "mean", "meaning", "measure", "median", "men", "meta", "method", "missing", "mode", "model", "modelling", "moment", "most", "multiple", "my", "naive", "nearest", "new", "next", "no", "noise", "non", "not", "notebook", "null", "number", "numeric", "numerical", "object", "objective", "opinion", "optimization", "optimize", "order", "out", "outcome", "outlier", "output", "overall", "overfit", "overfitting", "package", "passenger", "path", "people", "percentage", "performance", "picture", "plot", "plotting", "png", "positive", "potential", "practice", "predict", "prediction", "predictor", "price", "print", "problem", "processing", "product", "project", "property", "public", "publication", "purpose", "random", "range", "re", "recommendation", "reduce", "regression", "relationship", "relative", "remove", "result", "return", "reverse", "right", "risk", "row", "run", "running", "sample", "scale", "scaling", "scientific", "scikit", "score", "second", "section", "select", "selected", "selection", "semi", "sense", "set", "side", "similar", "single", "size", "soft", "source", "speed", "split", "stage", "standard", "standardization", "start", "step", "string", "sub", "subject", "submission", "subtract", "sum", "supervised", "survey", "survival", "survived", "table", "tag", "target", "test", "testing", "those", "through", "time", "titanic", "title", "tragedy", "train", "training", "transform", "tree", "try", "tune", "tuning", "type", "unique", "unit", "up", "upper", "validate", "validation", "value", "variable", "variance", "variation", "vector", "visualization", "visualize", "vote", "who", "wise", "word", "work", "year"], "potential_description_queries_len": 300, "potential_script_queries": ["numpy", "pyplot", "seaborn", "sklearn", "xgboost"], "potential_script_queries_len": 5, "potential_entities_queries": ["best", "data", "engineering", "int", "lower", "new", "practice", "similar", "supervised", "time", "value"], "potential_entities_queries_len": 11, "potential_extra_queries": ["biopsy", "procedure"], "potential_extra_queries_len": 2, "all_components_potential_queries_len": 306}