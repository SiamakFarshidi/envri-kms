{"name": "a data science framework to achieve 99 accuracy ", "full_name": " h1 How a Data Scientist Beat the Odds h1 A Data Science Framework h1 Step 1 What is the problem h1 Step 2 Where is the dataset h1 Step 3 Prepare data for consumption h2 3 1 Import Libraries h2 3 11 Load Data Modelling Libraries h2 3 2 Meet and Great Data h2 3 21 Explore Data with Descriptive Statistics h2 3 22 Clean Data h2 3 23 Cleanup Formats h2 3 24 Da Double Check Cleaned Data h2 3 25 Split Training and Testing Data h1 Step 4 Perform Exploratory Analysis h1 Step 5 Model Data h2 5 1 Model Optimization h3 Data Science 101 Determine a Baseline Accuracy h3 Data Science 101 How to Create Your Own Model h1 Credits ", "stargazers_count": 0, "forks_count": 0, "description": "It may be external or internal structured or unstructured static or streamed objective or subjective etc. neighbors Support Vector Machines SVM http scikit learn. This is both good and bad. It s bad because a lower barrier to entry means more people will not know the tools they are using and can come to incorrect conclusions. There are no date or currency formats but datatype formats. Missing values can be bad because it skews our data model. 24 Da Double Check Cleaned DataNow that we ve cleaned our data let s do a discount da double check 3. However just like autocorrect spellcheck technology sometimes we humans can be too smart for our own good and actually underperform a coin flip. statistics linear algebra etc. 1 Model OptimizationLet s recap with some basic data cleaning analysis and machine learning algorithms MLA we are able to predict passenger survival with 85 accuracy. CreditsProgramming is all about borrowing code because knife sharpens knife. Introduction to Machine Learning with Python A Guide for Data Scientists. As a data scientist your strategy should be to outsource developer operations and application plumbing so you have more time to focus on recommendations and design. ensemble Generalized Linear Models GLM http scikit learn. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. Machine Learning Selection SciKit Estimator Overview http scikit learn. For the first model iteration this variable will be excluded from analysis. Data Science 101 How to Create Your Own Model Our accuracy is increasing but can we do better Are there any signals in our data To illustrate this we re going to build our own decision tree model because it is the easiest to conceptualize and requires simple addition and multiplication calculations. Step 1 What is the problem For this project the problem statement is given to us on a golden plater develop an algorithm to predict the survival outcome of passengers on the Titanic. The idea is why write ten lines of code when you can write one line. It is a binary nominal datatype of 1 for survived and 0 for did not survive. Got it Let s go Question 1 Were you on the Titanic If Yes then majority 62 died. Before we move on to the next step let s deal with formatting. In laymen terms this means it either occurred or did not occur. One side note logistic regression while it has regression in the name is really a classification algorithm. html SciKit Estimator Detail http scikit learn. Remember the name of the game is to create subsets using a decision tree model to get survived 1 in one bucket and dead 0 in another bucket. But thanks to computer science a lot of the heavy lifting is done for you. Unsupervised learning is where you train the model using a training dataset that does not include the correct target response. org api _as_gen matplotlib. Instead it s best to impute missing values. 25 Split Training and Testing DataAs mentioned previously the test file provided is really for the competition submission data and does not provide the outcome target variable for us to validate our model. com questions 33925494 seaborn produces separate figures in subplots more side by side comparisons how does family size factor with sex survival how does class factor with sex survival how does embark port factor with class sex and survival plot distributions of Age of passengers who survived or did not survive pair plots correlation heatmap Machine Learning Algorithm MLA Selection and initialization Ensemble Methods Gaussian Processes Navies Bayes Nearest Neighbor Discriminant Analysis create table to compare MLA index through MLA and save to table set name column get and set algorithm execution time and accuracy print and sort table https pandas. So chances are the dataset s already exist somewhere in some format. The cabin variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the incident occurred and SES from deck levels. Most data scientist come from one of the three fields so they tend to lean towards that disciple. naive_bayes Nearest Neighbors http scikit learn. org stable modules generated sklearn. Data wrangling includes implementing data architectures for storage and processing developing data governance standards for quality and control data extraction i. My goal is to add to the Data Science Community 1 a framework that teaches how to think like a data scientist vs what to think code and 2 concise code and clear documentation because simple is better than complex. Our rule of thumb will be the majority rules. The PassengerID and Ticket variables are assumed to be random unique identifiers that have no impact on the outcome variable. Since are dead group is less than 9 we will stop going down this branch. html we will use matplotlib. Below are common classes to load. At worst it makes completing the project impossible or even worst incorrect. com 2013 09 08 basic feature engineering with the titanic data to derive the gender from title family size from surname and SES from titles like doctor or master however these variables already exist. Thus it s important to fix before starting analysis. Nonetheless if we assumed everybody died our sample accuracy is 62. This subgroup 55 died and 33 survived since majority died we need to find a signal to identify the 33 or a subgroup to change them from dead to survived and improve our model accuracy. 11 25 17 Added enhancements of published notebook and started step 4. Since our problem is predicting if a passenger survived or did not survive this is a discrete target variable. If you have a fair coin and you guessed heads or tail then you have a 50 50 chance of guessing right. For data visualization we will use the matplotlib and seaborn library. It s important to note more predictor variables do not make a better model but the right variables. An analogy would be asking someone to hand you a Philip screwdriver and they hand you a flathead screwdriver or worst a hammer. Create a pivot table with survival in the columns count and of row count in the values and the features described below in the rows. This helps ensure you haven t overfit your model or made it so specific to the selected subset it does not accurately fit another subset from the same dataset. When creating a decision tree you want to ask questions that gives you better information about your outcome by segregated the survived 1 from the dead 0. classification_report. As the saying goes you don t have to reinvent the wheel you just have to know where to find it. Also the dead subgroup is 9 or less so we will stop. But we have information about the dataset so we should be able to do better. Question 2 Are you male or female Male majority 81 died. com questions 46327494 python pandas dataframe copydeep false vs copydeep true vs cleanup age with median preview data again cleanup embarked with mode preview data again delete the cabin feature column and others previously stated to exclude preview data again convert to explicit category data type Quantitative Descriptive Statistics Qualitative Descriptive Statistics define x and y variables for original features aka feature selection define x and y variables for dummy features aka feature selection split train and test data with function defaults Correlation by Survival excluding continuous variables of age and fare using group by https pandas. com wp content uploads 2016 03 hilarious autocorrect fails 20x. It should be noted that if they were unreasonable values for example age 800 instead of 80 then it s probably a safe decision to fix now. value_counts https pandas. However since there are many null values it does not add value and thus is excluded from analysis. First you must understand that the purpose of machine learning is to solve human problems. Note our sample survival is different than our population of 68. org stable auto_examples model_selection plot_confusion_matrix. html collection of functions for data processing and analysis modeled after R dataframes with SQL like features collection of functions for scientific and publication ready visualization foundational package for scientific computing collection of functions for scientific computing and advance mathematics pretty printing of dataframes in Jupyter notebook collection of machine learning algorithms measure execution of code snippets https docs. subplots graph distribution of quantitative data we will use seaborn graphics for multi variable comparison https seaborn. For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. html prettify using pyplot https matplotlib. In this stage you will find yourself doing one of three things completing missing information correcting outliers or creating new features for analysis. html to organize our graphics will use figure https matplotlib. Our model accuracy increases to 81. What happens when technology is too smart for its own good Funny Autocorrect http 15858 presscdn 0 65. html Family Size create a new feature and initialize to yes 1 is alone now update to no 0 if family size is greater than 1 qcut or frequency bins https pandas. com questions 30211923 what is the difference between pandas qcut and pandas cut Fare Bins Buckets using qcut Age Bins Buckets using cut simple frequency table of class and sex using crosstabs https pandas. There are two common methods either delete the record or populate the missing value using a reasonable input. Binary events create an interesting dynamic because we know statistically a random guess should achieve a 50 accuracy rate without creating one single algorithm or writing one single line of code. isnull https pandas. We know this is a binary problem because there are only two possible outcomes passengers survived or died. Question 3A going down the female branch with count 344 Are you in class 1 2 or 3 Class 1 majority 97 survived and Class 2 majority 92 survived. Machine Learning Classification Algorithms Ensemble Methods http scikit learn. html optional plotting w pandas https pandas. In this kernel I use Kaggle s Getting Started Competition Titanic Machine Learning from Disaster to walk the reader through how to use the data science framework to beat the odds. So let s set 68 as a D grade because again if your model accuracy is any worst that that then why do I need you when I can just assume if you were on the Titanic that day you died and have a 68 accuracy. They will be converted to dummy variables for mathematical calculations. Anybody who has ever worked with data knows garbage in garbage out GIGO. Giving us an accuracy of 79. DataFrame https pandas. The Survived variable is our outcome or dependent variable. html Question 1 Were you on the Titanic majority died Question 2 Are you female majority survived Question 3A Female Class and Question 4 Embarked gain minimum information Question 5B Female FareBin set anything less than. How a Data Scientist Beat the Odds It s the classical problem predict the outcome of a binary event. jpg A Data Science Framework1. qualitative vs quantitative. 22 Clean DataNow that we know what to clean let s execute our code. We will use scikit train_test_split function http scikit learn. Therefore it is important to deploy descriptive and graphical statistics to look for potential problems patterns classifications correlations and comparisons in the dataset. The Sex and embarked variables are a nominal datatype. Not bad for a few lines of code. html graph distribution of qualitative data Pclass we know class mattered in survival now let s compare class and a 2nd feature graph distribution of qualitative data Sex we know sex mattered in survival now let s compare sex and a 2nd feature graph individual features by survival close factor plot facetgrid we don t need https stackoverflow. 0 predict survived 1 predict died 0 score random guess of survival. Supervised learning is where you train the model by presenting it a training dataset that includes the correct target response. accuracy_score group by or pivot table https pandas. A common business application is churn or customer retention. A basic methodology for qualitative data is impute using mode. Adding another level does not seem to gain much more information. org pandas docs stable categorical. For model optimization we have a couple options 1 find a better algorithm 2 tune our current algorithm parameters 3 feature engineer new variables to find new signals in the data or 4 we can go back to the beginning and determine if we asked the right questions got the right data and made the right decisions along the process. So problems that once required graduate mathematic degrees now only take a few lines of code. This is part science and part art so let s just play the 21 question game to show you how it works. 23 Cleanup FormatsWe will convert Sex and Embarked from objects to dummy variables for mathematical analysis. Therefore If I just guessed that 100 of people died then I would be right 67. com kaggle docker python For example here s several helpful packages to load in access to system parameters https docs. In two lines of code we can quickly determine we have a little bit of data cleaning ahead of us. fit train1_dummy_x train1_dummy_y barplot using https seaborn. Thanks 12 2 17 Updated section 4 with exploratory statistics and section 5 with more classifiers. Both are dicrete quantitative datatypes. Nonetheless I want to give credit where credit is due. So we ll stop here for now. So think of it like a coin flip. So use caution when selecting your tool because at the end of the day you are still the master craft wo man. 21 Explore Data with Descriptive StatisticsDescriptive statistics is a quick and dirty way to identify aberrant missing or outlier data. Source Documentation pandas. Class 3 is even at a 50 50 split. pyplot https matplotlib. subplot and subplotS https matplotlib. Question 3B going down the male branch with count 577 Going back to question 2 we know the majority of males died. Female majority 74 survived. org stable tutorial machine_learning_map index. html qcut vs cut https stackoverflow. Since step 2 was provided to us on a golden plater so is step 3. 12 3 17 Update section 4 with improved graphical statistics. 2 Meet and Great DataThis is the meet and great step. This can be used for feature engineering to create a family size or is alone variable. We know that 1 502 2 224 or 67. org stable modules classes. and y dependent target outcome response etc. Step 4 Perform Exploratory AnalysisNow that our data is cleaned let s use descriptive and graphical statistics to describe and summarize our data. 5 to 1 for majority survived Score Decision Tree Model Accuracy Summary Report with http scikit learn. Data Science 101 Determine a Baseline Accuracy Before we decide how to make our model better let s determine if 85 is good enough. Get to know your data by first name and learn a little bit about it. Perform exploratory analysis. Therefore normal processes in data wrangling such as data architecture governance and extraction are out of scope. html create a 2nd copy of our data Feature Engineering Note we will not do any imputing missing data at this time coin flip model with random 1 survived 0 died Iterate over DataFrame rows as index Series pairs https pandas. 11 Load Data Modelling LibrariesWe will use the popular scikit learn library to develop our machine learning algorithms. Think of it like a first date before you jump in and start poking it in the bedroom. com categorical encoding. Statistical classification is also important to understand the overall datatype structure i. There are many machine learning algorithms however they can be reduced to four categories classification regression clustering or dimensionality reduction depending on your target variable and data modeling goals. We can generalize that a continuous target variable requires a regression algorithm and a discrete target variable requires a classification algorithm. Prepare data for consumption. The SibSp represents number of siblings spouse aboard and Parch represents number of parents or children aboard. html Choosing Estimator Cheat Sheet https s3. The same is true in data modelling. com c titanic data Step 3 Prepare data for consumption. With that being said this step requires advanced knowledge in mathematics. For example you won or did not win you passed the test or did not pass the test you were accepted or not accepted and you get the point. In this stage you will find yourself classifying features and trying to determine their relationship with the target variable and each other. 5 in female node decision tree back to 0 Question 3B Male Title set anything greater than. An intermediate methodology is to use the basic methodology based on specific criteria like the average age by class or embark port by fare and SES. faster than it was before. pdf Now that we identified our solution as a supervised learning classification algorithm. 12 7 17 Updated section 5 with Data Science 101 Le Hello this is my debut project on Kaggle using the popular Getting Started Competition. Evaluate Data Model After you ve trained your model based on a subset of your data it s time to test your model. Accuracy with very simple data cleaning and logistic regression is 82. Thus they will be excluded from analysis. html Choosing Estimator Mind Map http scikit learn. So with very little information we get to 82 accuracy. In scikit algorithms are called Estimators and implemented in their own classes. This step is often referred to as data wrangling a required process to turn wild data into manageable data. Meaning the algorithm is so specific to a given subset it cannot accurately generalize another subset from the same dataset. Thanks 11 23 17 Cleaned up published notebook and updated through step 3. And reinforced learning is a hybrid of the previous two where the model is not given the correct target response immediately but later in a sequence of events to reinforce learning. html Pandas Categorical dtype https pandas. We ll give that a grade of a C for average or our baseline. Question 5A going down the female class 3 embarked S branch with count 88 So far it looks like we made good decisions. A basic methodology for quantitative data is impute using mean median or mean standard deviation. What does it look like datatype and values what makes it tick independent feature variables s what s its goals in life dependent target variable s. Step 5 Model DataData Science is a multi disciplinary field between mathematics i. Improved model to 85 accuracy. Continue to up vote and I will continue to develop this notebook. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. ETL and web scraping and data cleaning to identify aberrant missing or outlier data points. qualitative vs quantitative in order to select the correct hypothesis test or data model. The Age and fare variable are continuous quantitative datatypes. So we will use scikit function to split the training data in two datasets 75 25 split. It s important our algorithm has not seen the subset we will use to test so it doesn t cheat by memorizing the answers. In this step we will also define our x independent features explanatory predictor etc. However data science is like a three legged stool with no one leg being more important than the other. We then hope to present it a new subset from the same dataset and have similar results in prediction accuracy. Also we will stop if the subgroup is 10 of our total dataset or 9 cases and or our model accuracy plateaus or decreases. At worst it makes completing the project impossible. computer science i. Too often we are quick to jump to the new shiny technology tool or algorithm before determining the actual problem we are trying to solve. At this stage we see we may have potential outliers in age and fare. classification_report Where recall score true positives true positive false negative w 1 being best http scikit learn. We can narrow our list of choices. It is not recommended to delete the record especially a large percentage of records unless it truly represents an incomplete record. html random number generator https docs. To begin this tasks we first import our data. get_dummies https pandas. Reviewing the data there does not appear to be any aberrant or non acceptable data inputs. We will use a classification algorithm from the scikit library to begin our analysis. One I found was fare 0 8 majority survived. Before we do let s code what we just wrote above. html Random float x 0. In addition we will explicitly convert Pclass to integer categorical variables so not to be confused with quantitative variables. Click here for the Source Data Dictionary https www. However since they are reasonable values we will wait until after we complete our graphical statistics to determine if we should include or exclude from analysis. In the next step we will worry about transforming dirty data to clean data. html Indexing and Selecting Data https pandas. Once you re able to package your ideas this becomes your currency exchange rate. The Name variable is a nominal datatype. In this challenge we ask you to complete the analysis of what sorts of people were likely to survive. and business management i. Previously I used the analogy of asking someone to hand you a Philip screwdriver and they hand you a flathead screwdriver or worst a hammer. All other variables are potential predictor or independent variables. So now that I ve hammered no pun intended my point I ll show you what to do and most importantly WHY you do it. Implement Optimize and Strategize This is the bionic man step where you iterate back through the process to make it better. Problems before requirements requirements before solutions solutions before design and design before technology. Libraries provide pre written functionality to perform necessary tasks. html Ignore warnings Input data files are available in the. Use shortcut 1 Right Guess and 0 Wrong Guess the mean of the column will then equal the accuracy assume prediction wrong set to 1 for correct prediction we can also use scikit s accuracy_score function to save us a few lines of code http scikit learn. However we want to use caution when we modify data from its original value because it may be necessary to create an accurate model. In particular we ask you to apply the tools of machine learning to predict which passengers survived the tragedy. com blog_assets Scikit_Learn_Cheat_Sheet_Python. figure subplot https matplotlib. Thus only data cleaning is in scope. Source Documentation Categorical Encoding http pbpython. html Compute confusion matrix Plot non normalized confusion matrix Plot normalized confusion matrix. What is the problem If data science big data machine learning predictive analytics business intelligence or any other buzzword is the solution then what is the problem As the saying goes don t put the cart before the horse. But can we do better By making a better decision tree new features etc. org generated seaborn. It could be used for feature engineering https triangleinequality. Surprisingly class or even embarked didn t matter title does and gets us to 82. html cut or value bins https pandas. There are more complex methodologies however before deploying it should be compared to the base model to determine if complexity truly adds value. variables for data modeling. So let s set 50 as an F grade because if your model accuracy is any worse than that then why do I need you when I can just flip a coin Okay so with no information about the dataset we can always get 50 with a binary problem. Common Model Algorithms Common Model Helpers Visualization Configure visualizations matplotlib inline show plots in Jupyter Notebook browser load as dataframe Note The test file is really validation data for competition submission because we do not know the survival status We will create real test data in a later section so we can evaluate our model before competition submission preview data https pandas. org pandas docs stable generated pandas. At best it shows a complete lack of understanding. programming languages computer systems etc. Where is the dataset John Naisbitt wrote in his 1984 book Megatrends we are drowning in data yet staving for knowledge. That s why I focus on teaching you not just what to do but why you re doing it. Question 4A going down the female class 3 branch with count 144 Did you embark from port C Q or S We gain a little information. Your dataset and expected results will determine the algorithms available for use. This is important so we don t overfit our model. If you want to follow along on your own download the train dataset and import into Excel. We can play with our features. f1_score Credit http scikit learn. S the majority 63 died. On April 15 1912 during her maiden voyage the Titanic sank after colliding with an iceberg killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. There are null values or missing data in the age cabin and embarked field. Practice Skills Binary classification Python and R basics Step 2 Where is the dataset The dataset is also given to us on a golden plater with test and train data at Kaggle s Titanic Machine Learning from Disaster https www. So keep that in mind. Meaning if the majority or 50 or more survived then everybody in our subgroup survived 1 but if 50 or less survived then if everybody in our subgroup died 0. C and Q the majority still survived so no change. It s a small sample size 11 9 but one often used in statistics. Guess and checking other features none seem to push us past 82. Another popular use case is healthcare s mortality rate or survival analysis. But the question we always ask is can we do better and more importantly get an ROI return on investment for our time invested For example if we re only going to increase our accuracy by 1 10th of a percent is it really worth 3 months of model optimization. org pandas docs stable visualization. Model Data Like descriptive and inferential statistics data modeling can either summarize the data or predict future outcomes. Next we use the info and sample function to get a quick and dirty overview of variable datatypes i. recall_score And F1 score weighted average of precision and recall w 1 being best http scikit learn. So we are looking for a feature that identifies a subgroup that majority survived. Subsequent model iterations will modify this decision to determine if it improves the model s accuracy. There are multiple ways to encode categorical variables we will use the pandas method scikit also has LabelEncoder and OneHotEncoder. org stable user_guide. To do that we have to go back to the basics of data science 101. html timeit logreg. linear_model Naive Bayes http scikit learn. Update 11 22 17 Please note this kernel is currently in progress but open to feedback. It s important to remember algorithms are tools and not magical wands. The Pclass variable is an ordinal datatype for the ticket class a proxy for socio economic status SES representing 1 upper class 2 middle class and 3 lower class. It s good because these algorithms are now accessible to more people that can solve more problems in the world. org pandas docs stable indexing. Machine learning can be categorized as supervised learning unsupervised learning and reinforced learning. Project Summary The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. svm Decision Trees http scikit learn. No new information to improve our model is gained. html highlight matplotlib 20pyplot 20subplots matplotlib. 1 Import LibrariesThe following code is written in Python 3. html Quantitative Descriptive Statistics Qualitative Descriptive Statistics create a copy of data remember python assignment or equal passes by reference vs values so we use the copy function https stackoverflow. So we will change females class 3 embarked S from assuming they survived to assuming they died. Our categorical data imported as objects which makes it difficult for mathematical calculations. communication subject matter knowledge etc. For this dataset we will convert object datatypes to categorical dummy variables. 11 26 17 Skipped ahead to data model since this is a published notebook. We are doing supervised machine learning because we are training our algorithm by presenting it with a set of features and their corresponding target. If you work in research maybe the answer is yes but if you work in business mostly the answer is no. For this dataset age will be imputed with the median the cabin attribute will be dropped and embark will be imputed with mode. If this were a college course that would be a B grade. fillna https pandas. It can be used during subsequent iterations to evaluate if more complexity improves the base model accuracy. Although there was some element of luck involved in surviving the sinking some groups of people were more likely to survive than others such as women children and the upper class. describe https pandas. We slightly improve our accuracy but not much to move us past 82. ", "id": "ldfreeman3/a-data-science-framework-to-achieve-99-accuracy", "size": "26317", "language": "python", "html_url": "https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy", "git_url": "https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy", "script": "RFECV display #pretty printing of dataframes in Jupyter notebook discriminant_analysis scatter_matrix Imputer StratifiedShuffleSplit IPython scale Normalizer naive_bayes seaborn numpy plot_confusion_matrix matplotlib.pylab time linear_model sklearn sklearn.model_selection metrics timeit LabelEncoder Image neighbors matplotlib.pyplot gaussian_process correlation_heatmap pandas subprocess OneHotEncoder check_output pandas.tools.plotting svm sklearn.feature_selection plot_distribution scipy GridSearchCV sklearn.preprocessing StratifiedKFold RandomizedSearchCV matplotlib tree train_test_split IPython.display ensemble ", "entities": "(('so we', 'copy function https stackoverflow'), 'create') (('basic methodology', 'mode'), 'be') (('subgroup', 'total dataset'), 'stop') (('we', 'categorical dummy variables'), 'convert') (('basic methodology', 'standard deviation'), 'be') (('2016 03 hilarious autocorrect', '20x'), 'upload') (('It', 'feature engineering https triangleinequality'), 'use') (('it', 'back process'), 'Optimize') (('when you', 'one line'), 'be') (('embark', 'mode'), 'impute') (('Thus they', 'analysis'), 'exclude') (('then you', '50 50 chance'), 'have') (('you', 'day'), 'use') (('html', 'figure https matplotlib'), 'use') (('we', 'features'), 'do') (('html collection', 'code snippets https docs'), 'model') (('1 2 3 Class 1 97 Class 2 majority', '344 class'), 'be') (('you', 'just where it'), 'have') (('you', 'point'), 'win') (('often data', 'manageable data'), 'refer') (('less than we', 'branch'), 'be') (('da', 'double 3'), 'clean') (('it', 'accuracy'), 'modify') (('dataset', 'Disaster https www'), 'classification') (('11 22 17 kernel', 'currently progress'), 'update') (('information Question 5B Female minimum FareBin', 'anything'), 'die') (('s', 'data'), 'AnalysisNow') (('we', 'age'), 'see') (('you', 'output'), 'list') (('I', 'odds'), 'use') (('it', 'simple addition calculations'), 'Science') (('shipwreck', 'enough passengers'), 'be') (('sample small 11 9 one', 'often statistics'), 's') (('it', 't answers'), 's') (('Instead it', 'missing values'), 's') (('Therefore it', 'dataset'), 'be') (('Model DataData Step 5 Science', 'mathematics multi disciplinary i.'), 'be') (('They', 'mathematical calculations'), 'convert') (('how it', 'you'), 'be') (('Guess', '82'), 'seem') (('Machine learning', 'supervised learning unsupervised learning'), 'categorize') (('new information', 'model'), 'gain') (('Load Data Modelling 11 LibrariesWe', 'machine learning algorithms'), 'use') (('dataset', 'already somewhere format'), 'be') (('we', 'learning classification supervised algorithm'), 'pdf') (('name', '0 bucket'), 'remember') (('It', 'python docker image https kaggle github'), 'come') (('variable', 'analysis'), 'exclude') (('everybody', 'sample accuracy'), 'be') (('feature aka selection', 'https pandas'), 'question') (('data However science', 'more other'), 'be') (('we', 'competition submission preview data https pandas'), 'show') (('Next we', 'variable datatypes'), 'use') (('we', 'good decisions'), '5a') (('Adding', 'much more information'), 'seem') (('this', 'ideas'), 'become') (('then majority', 'Titanic'), 'let') (('we', 'predictor explanatory etc'), 'define') (('Data wrangling', 'data extraction quality i.'), 'include') (('Explore 21 Data', 'missing outlier data'), 'be') (('then I', 'people'), 'be') (('We', 'little information'), 'go') (('didn t matter even title', '82'), 'do') (('algorithms', 'own classes'), 'call') (('sensational tragedy', 'ships'), 'shock') (('who', 'out GIGO'), 'know') (('html Ignore warnings Input data files', 'the'), 'be') (('so we', 'model'), 'be') (('fit train1_dummy_y barplot', 'https seaborn'), 'train1_dummy_x') (('We', 'average'), 'give') (('they', 'conclusions'), 's') (('com difference', 'crosstabs https pandas'), 'question') (('majority', 'that'), 'look') (('1 predict', 'survival'), 'die') (('s', 'formatting'), 'let') (('majority', 'males'), '3b') (('that', 'world'), 's') (('we', 'yet knowledge'), 'be') (('many null it', 'thus analysis'), 'however') (('Thus it', 'analysis'), 's') (('we', 'https stackoverflow'), 'distribution') (('it', 'accurate model'), 'want') (('we', '82 accuracy'), 'get') (('status socio economic SES', '1 upper class 2 middle class'), 'be') (('when incident', 'deck levels'), 'be') (('passengers', 'tragedy'), 'ask') (('it', 'same dataset'), 'be') (('concise 2 clear simple', 'code'), 'be') (('Thanks', 'step'), 'clean') (('where credit', 'credit'), 'want') (('you', 'bedroom'), 'think') (('lot', 'you'), 'do') (('Reviewing', 'data'), 'appear') (('we', 'code http scikit'), 'assume') (('so they', 'disciple'), 'come') (('it', 'model really 3 months optimization'), 'be') (('more complexity', 'base model accuracy'), 'use') (('family size', 'now 0'), 'create') (('random unique that', 'outcome variable'), 'assume') (('we', 'first data'), 'import') (('we', 'index Series pairs https pandas'), 'create') (('5', 'anything'), 'set') (('set name column', 'algorithm execution time'), 'produce') (('most importantly WHY you', 'it'), 'intend') (('we', 'also LabelEncoder'), 'be') (('predictor more variables', 'better model'), 's') (('It', 'survived'), 'be') (('statistically random guess', 'code'), 'create') (('This', 'family size'), 'use') (('they', 'flathead screwdriver'), 'use') (('At best it', 'understanding'), 'show') (('when technology', 'Funny too own good Autocorrect'), 'happen') (('we', 'actual problem'), 'be') (('intermediate methodology', 'fare'), 'be') (('dataset results', 'available use'), 'determine') (('Import 1 LibrariesThe following code', 'Python'), 'write') (('that', 'dead 0'), 'create') (('yourself', 'analysis'), 'find') (('mostly answer', 'business'), 'be') (('we', 'binary problem'), 'let') (('why you', 'it'), 's') (('us', 'model'), 'mention') (('Le 101 this', 'Getting Competition'), 'section') (('where model', 'learning'), 'be') (('that', 'target correct response'), 'be') (('complexity', 'truly value'), 'be') (('it', 'data model'), 'be') (('Statistical classification', 'datatype structure also overall i.'), 'be') (('qualitative', 'hypothesis correct test'), 'quantitative') (('sometimes we humans', 'coin actually flip'), 'be') (('we', '85 accuracy'), '1') (('we', 'clean data'), 'worry') (('it', 'terms'), 'mean') (('85', 'Baseline Data 101 Accuracy'), 'Science') (('quickly we', 'ahead us'), 'determine') (('don t', 'horse'), 'be') (('many machine learning however they', 'dimensionality data target variable goals'), 'be') (('it', 'model'), 'Model') (('it', 'same dataset'), 'help') (('Therefore normal processes', 'scope'), 'be') (('aboard Parch', 'parents'), 'represent') (('we', 'what'), 'let') (('day you', '68 accuracy'), 'let') (('however variables', 'doctor'), 'com') (('C majority', 'still so change'), 'survive') (('Cleanup 23 FormatsWe', 'mathematical analysis'), 'convert') (('sample survival', '68'), 'note') (('yourself', 'target variable'), 'find') (('Score Decision Tree Model Accuracy Summary Report', 'http scikit'), 'survive') (('we', 'quantitative variables'), 'convert') (('http best scikit', '1'), 'learn') (('Accuracy', 'data very simple cleaning'), 'be') (('we', 'comparison https multi variable seaborn'), 'use') (('At worst it', 'project'), 'make') (('it', 'name'), 'be') (('we', 'dataset'), 'have') (('Project sinking', 'history'), 'Summary') (('that', 'code'), 'take') (('regression algorithm', 'target classification discrete algorithm'), 'generalize') (('application so you', 'recommendations'), 'be') (('we', 'analysis'), 'wait') (('So we', '75 25 split'), 'use') (('you', 'train Excel'), 'want') (('we', 'matplotlib library'), 'use') (('Libraries', 'necessary tasks'), 'provide') (('we', 'process'), 'have') (('We', '82'), 'improve') (('rule', 'thumb'), 'be') (('then everybody', 'subgroup'), 'mean') (('problem statement', 'Titanic'), 'step') (('this', 'data ahead model'), 'skip') (('age 800 instead 80 then it', 'unreasonable example'), 'note') (('We', 'prediction accuracy'), 'hope') (('sorts', 'people'), 'ask') (('s', 'code'), 'DataNow') (('knife', 'knife'), 'be') (('We', 'analysis'), 'use') (('step', 'golden plater'), 'provide') (('it', 's.'), 'look') (('purpose', 'human problems'), 'understand') (('http best scikit', 'true positive false negative w'), 'learn') (('we', 'data science'), 'have') (('step', 'mathematics'), 'require') (('classical problem', 'binary event'), 'beat') (('they', 'flathead screwdriver'), 'ask') (('it', 'truly incomplete record'), 'recommend') (('they', '3 S'), 'change') (('we', 'model dead accuracy'), 'die') (('I', 'notebook'), 'continue') (('it', 'mathematical calculations'), 'datum') ", "extra": "['gender', 'outcome', 'test', 'bag']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "advance", "advanced", "age", "algorithm", "answer", "api", "appear", "application", "apply", "architecture", "art", "assignment", "attribute", "average", "barplot", "basic", "best", "binary", "bit", "book", "branch", "browser", "build", "cabin", "case", "categorical", "category", "challenge", "check", "checking", "children", "classification", "clean", "cleaning", "clear", "close", "clustering", "code", "collection", "column", "community", "compare", "comparison", "competition", "computer", "confusion", "content", "control", "convert", "copy", "correct", "correlation", "correlations", "could", "count", "course", "create", "credit", "criteria", "currency", "current", "customer", "cut", "data", "dataframe", "dataset", "date", "day", "decision", "define", "dependent", "derive", "describe", "develop", "difference", "dimensionality", "directory", "discrete", "distribution", "doctor", "double", "download", "dummy", "encode", "end", "engineering", "ensemble", "ensure", "environment", "equal", "evaluate", "even", "execute", "execution", "expected", "external", "extraction", "factor", "family", "fare", "faster", "feature", "field", "figure", "file", "find", "fit", "fix", "flip", "float", "following", "found", "framework", "frequency", "function", "future", "game", "gender", "generated", "generator", "grade", "graph", "group", "hand", "heatmap", "highlight", "hope", "http", "human", "idea", "image", "import", "improve", "impute", "include", "increase", "index", "individual", "info", "initialize", "input", "integer", "investment", "iteration", "kaggle", "kernel", "knowledge", "learn", "learning", "let", "level", "library", "life", "line", "linear", "list", "little", "load", "look", "looking", "lot", "lower", "majority", "male", "matplotlib", "matrix", "mean", "measure", "median", "method", "middle", "minimum", "missing", "mode", "model", "mortality", "most", "move", "multiple", "my", "name", "need", "negative", "new", "next", "no", "node", "non", "none", "normal", "normalized", "not", "notebook", "null", "number", "object", "objective", "open", "optimization", "order", "organize", "out", "outcome", "outlier", "overall", "overfit", "overview", "package", "pair", "part", "passenger", "past", "pdf", "people", "percent", "percentage", "perform", "plot", "plotting", "point", "population", "position", "positive", "potential", "pre", "precision", "predict", "prediction", "predictor", "present", "print", "printing", "problem", "processing", "project", "provide", "publication", "purpose", "pyplot", "python", "question", "random", "re", "reader", "recall", "record", "reference", "regression", "relationship", "research", "response", "return", "right", "row", "run", "running", "sample", "save", "science", "scientific", "scikit", "score", "seaborn", "section", "select", "selected", "selection", "separate", "sequence", "set", "several", "sex", "ship", "side", "signal", "similar", "single", "size", "solution", "sort", "split", "stage", "standard", "start", "step", "storage", "strategy", "structure", "subgroup", "subject", "submission", "subplot", "subset", "summarize", "supervised", "survival", "survived", "svm", "system", "table", "target", "technology", "test", "think", "through", "thumb", "ticket", "time", "titanic", "title", "tool", "total", "tragedy", "train", "training", "tree", "tune", "turn", "tutorial", "type", "unique", "until", "up", "update", "upper", "validate", "validation", "value", "variable", "visualization", "vote", "walk", "web", "while", "who", "work", "worst", "write"], "potential_description_queries_len": 339, "potential_script_queries": ["display", "matplotlib", "numpy", "scale", "scipy", "sklearn", "time"], "potential_script_queries_len": 7, "potential_entities_queries": ["classification", "correct", "discrete", "engineering", "execution", "following", "http", "kaggle", "middle", "model", "negative", "null", "overall", "random", "submission", "target", "upper", "worst"], "potential_entities_queries_len": 18, "potential_extra_queries": ["bag"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 342}