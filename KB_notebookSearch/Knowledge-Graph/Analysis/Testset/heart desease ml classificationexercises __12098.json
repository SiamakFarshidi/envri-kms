{"name": "heart desease ml classificationexercises ", "full_name": " h2 If you find this kernel helpful Please UPVOTES h2 Problem Definition h2 Data contains h2 READ DATA AND EXPLORING DATA h2 SOME VISUALIZATION h3 Count of disease and not desease h3 Distribution of disease and not disease with scatter h3 Distrbution of age with distplot h3 Distribution of age with boxplot h3 Dividing into age groups h3 Dividing into age groups with barplot h3 Dividing into age groups with pieplot h3 Distrubution of Age and Target with violinplot h3 sex and ca hue target with barplot h3 Sex and Oldpeak hue restecg h3 Count of target with hue sex h3 Number of people who have heart disease according to age h2 Correlation matrix heatmap h3 Interpretation of heatmap h3 correlation only with target and other variables h2 Target and Thalech h3 Interpretation h4 CONCLUSION OF VISUALIZATION h2 LETS NORMALIZE THE VARIABLES h3 Normalization h1 LETS TRY CLASSIFICATIONS METHODS h2 1 LOGISTIC REGRESSION h3 A Train test splitting h3 B Modeling of Logistic R Method h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h2 2 NAIVE BAYES METHOD h3 A Train test splitting h3 B Modeling of Naive B Method h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h2 3 KNN METHOD h3 A Train test splitting h3 B Modeling of KNN Medhod h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 Look at accuracy score h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h3 Conclusion KNN h2 4 SVM SUPPORT VECTOR MACHINES h3 A Train test splitting h3 B Modeling of SVM Medhod h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h3 E TUNING THE PREDICTION WE can tune our prediction h4 Tuning1 change C and gamma h4 Tuning2 changing kernel linear c 100 h3 Conclusion h2 5 RANDOM FOREST METHOD h3 A Train test splitting h3 B Modeling of SVM Medhod h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h3 Lets look at importance 6 variables h4 Conclusion h2 6 DECISION TREE METHOD h3 A Train test splitting h3 B Modeling of Decision Tree h3 C Lets control the succes score prediction accuracy score confusion m on test data h3 D Model tuning h3 Lets look at 6 importance variables h2 LETS SEE ALL SCORE OF CLASSIFICATIONS METHODS h2 FINISH ", "stargazers_count": 0, "forks_count": 0, "description": "We ll train it find the patterns on the training set. In this method we need to choose k value. C nin k\u00fc\u00e7\u00fck olmas\u0131 yanl\u0131\u015f s\u0131n\u0131fland\u0131rmaya neden olur. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session chose randon sample from row DISTRUBUTION OF AGE WITH DISTPLOT DISTRUBUTION OF AGE WITH BOXPLOT we can see in pie. read_csv For data visualization Plotly for interactive graphics Disabling warnings Input data files are available in the read only. There is a C parameter inside the SVM algoritma and the default value of C parameter is 1. There are many kinds of metric in KNN. We can see that the thalach variable is slightly negatively skewed. Coding is the same for all supervised classes and we jus need to change the last part of the code. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. predict x_test accuracy_score y_test y_pred uzun suruyor RandomForestClassifier bootstrap True ccp_alpha 0. Number of people who have heart disease according to age Let s make our correlation matrix a little prettier with jitter with boxplot We can see what there is in lr icinde hangi secenekler vargormek icin LogisticRegression C 1. CONCLUSION OF VISUALIZATIONFindings of Bivariate Analysis are as follows There is no variable which has strong positive correlation with target variable. And we use this avarage to determine the class of the test point. Conclusion KNN knn_tuned_bestscore 85 and cmknn_best are our best best score and our best confusion matrix 4 SVM SUPPORT VECTOR MACHINES Support Vector Machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. on test_data Look at accuracy_score HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap. E TUNING THE PREDICTION WE can tune our prediction If we tune our data for nb it increase a little. confusion matrixle tahmin etme sayilarini bulduk 1 icin 31 i dogru tahmin 0 icin 35 i dogru tahmin En ustte import edildi. If we change metric and use tuned n neigbors acurracy_score is best. LETS NORMALIZE THE VARIABLES Normalization LETS TRY CLASSIFICATIONS METHODS Now we ve got our data split into training and test sets it s time to build a machine learning model. target and oldpeak variable are also mildly negatively correlated correlation coefficient 0. CLASSICICATION REPORT we can also see classification report. Tree models where the target variable can take a finite set of values are called classification trees in these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels. 0 class_weight None criterion gini max_depth None max_features auto max_leaf_nodes None max_samples None min_impurity_decrease 0. SVM i\u00e7erisinde C parametresi vard\u0131r ve C parametresinin default de\u011feri 1 dir. Given a set of training samples each marked as belonging to one or the other of two categories an SVM training algorithm builds a model that assigns new test samples to one category or the other making it a non probabilistic binary linear classifier. There is no correlation between target and fbs. A sample is classified by a majority vote of its neighbors with the sample being assigned to the class most common among its k nearest neighbors k is a positive integer typically small. C b\u00fcy\u00fckse overfitting e neden olur. There is no variable which has strong negative correlation with target variable. A Train test splitting B Modeling of Decision Tree C Lets control the succes score prediction accuracy_score confusion m. The people suffering from heart disease target 1 have relatively higher heart rate thalach as compared to people who are not suffering from heart disease target 0. si 1 olma olasiligi oranlari we can look at which option is there in GaussionNB nb confusion matrixle tahmin etme sayilarini bulduk 1 icin 32 i dogru tahmin 0 icin 30 i dogru tahmin En ustte import edildi. Method C Lets control the succes score prediction accuracy_score confusion m. Decision trees where the target variable can take continuous values typically real numbers are called regression trees. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. It means that we chose k number of points of classes which are nearest to the out test point. target and thal variable are also waekly negatively correlated correlation coefficient 0. 0001 verbose 0 warm_start False sabit katsayi degisken katsayilari The y predicted by the y in the test are compared test deki y ile tahmin edilen yler karsilastiriliyor. A Train test splitting B Modeling of SVM Medhod C Lets control the succes score prediction accuracy_score confusion m. We can call this small data set. Random forests or random decision forests are an ensemble learning method for classification regression and other tasks that operate by constructing a multitude of decision trees n_estimators 100 300 at training time and outputting the class that is the mode of the classes classification or mean prediction regression of the individual trees This methods basically use multiple number of decision trees and take the avarage of the results of these decision trees. target and ca variable are weakly negatively correlated correlation coefficient 0. target and exang variable are mildly negatively correlated correlation coefficient 0. The cp and thalach variables are mildly positively correlated with target variable. SVM hem regresyon hem de s\u0131n\u0131fland\u0131rma problemleri i\u00e7in kullan\u0131l\u0131r ancak genellikle s\u0131n\u0131fland\u0131rma i\u00e7in kullan\u0131l\u0131r. fit x_train y_train y_pred svm. A Train test splitting B Modeling of Naive B. And we ll test it use the patterns on the test set. And we put the most important one to the top of the related tree. si 1 olma olasiligi oranlari KNeighborsClassifier algorithm auto leaf_size 30 metric minkowski metric_params None n_jobs None n_neighbors 3 p 2 weights uniform Hepsi icin yapilabilir ilk 10 datatest deki tahminlerimiz 1. If C is big it causes ovetfitting. If we use n_neighbors 21 we can obtain best score. si 1 olma olasiligi oranlari Hepsi icin yapilabilir tree_cv_model. This is one of ensamble method which uses multiple classes to predict the target and very powerfull technique. A Train test splitting B Modeling of Logistic R. According to \u0131nformation entropy we can determine which feature is the most important. we changed the kernel We can use linear poly rbf. 0001 degree 9 kernel poly svm_tune1. nb_tuned_bestscore 89 and cmnb_best are our best best score and our best confusion matrix 3 KNN METHOD In pattern recognition the k Nearest Neighbors algorithm or k NN for short is a non parametric method used for classification and regression. on test_data HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap. Problem Definition Given clinical parameters about a patient can we predict whether or not they have heart disease Data contains age age in years sex 1 male 0 female cp chest pain type gogus agrisi tipi trestbps resting blood pressure in mm Hg on admission to the hospital kan basinci chol serum cholestoral in mg dl mg dl cinsinden serum kolesterol\u00fc fbs fasting blood sugar 120 mg dl 1 true 0 false restecg resting electrocardiographic results dinlenme elektrokardiyografik sonu\u00e7lar\u0131 thalach maximum heart rate achieved ula\u015f\u0131lan maksimum kalp at\u0131\u015f h\u0131z\u0131 exang exercise induced angina 1 yes 0 no egzersize ba\u011fl\u0131 anjina 1 evet 0 hay\u0131r oldpeak ST depression induced by exercise relative to rest dinlenmeye g\u00f6re egzersizin neden oldu\u011fu ST depresyonu slope the slope of the peak exercise ST segment en y\u00fcksek egzersiz ST segmentinin e\u011fimi ca number of major vessels 0 3 colored by flourosopy thal 3 normal 6 fixed defect 7 reversable defect target have disease or not 1 yes 0 no hastal\u0131\u011f\u0131 var m\u0131 yok mu 1 evet 0 hay\u0131r READ DATA AND EXPLORING DATA SOME VISUALIZATION Count of disease and not desease Distribution of disease and not disease with scatter Distrbution of age with distplot Distribution of age with boxplot Dividing into age groups Dividing into age groups with barplot There are a few young ages Dividing into age groups with pieplot Distrubution of Age and Target with violinplot sex and ca hue target with barplot Sex and Oldpeak hue restecg Count of target with hue sex Number of people who have heart disease according to age Correlation matrix heatmap Interpretation of heatmapFrom the above correlation heat map we can conclude that target and cp variable are mildly positively correlated correlation coefficient 0. LOOK AT ALL PREDICTION VALUE ON TEST DATA. Decision tree builds classification or regression models in the form of a tree structure. E TUNING THE PREDICTION WE can tune our prediction we can tune n_neigbors metric. So we need to try C parameter to find best value. We count the number of classes in the small dataset and determine the highest number of class. 0 decision_function_shape ovr degree 9 gamma scale kernel poly max_iter 1 probability False random_state None shrinking True tol 0. If C is small it causes the misclassification. predict x_test cok uzun suruyor we changed the kernel We can use linear poly rbf. target and thalach variable are also mildly positively correlated correlation coefficient 0. We re going to try machine learning models 1 Logistic Regression 2 K Nearest Neighbours Classifier 3 Support Vector machine 4 Decision Tree Classifier 5 Random Forest Classifier 1 LOGISTIC REGRESSION Logistic Regression is a useful model to run early in the workflow. If you find this kernel helpful Please UPVOTES. Lojistik regresyon k\u00fcm\u00fclatif lojistik da\u011f\u0131l\u0131m olan bir lojistik fonksiyon kullanarak olas\u0131l\u0131klar\u0131 tahmin ederek kategorik ba\u011f\u0131ml\u0131 de\u011fi\u015fken \u00f6zellik ile bir veya daha fazla ba\u011f\u0131ms\u0131z de\u011fi\u015fken \u00f6zellik aras\u0131ndaki ili\u015fkiyi \u00f6l\u00e7er. nesnesi tanimlandi Hepsi icin yapilabilir svm SVC C 5 break_ties False cache_size 200 class_weight None coef0 0. 0 min_impurity_split None min_samples_leaf 1 min_samples_split 2 min_weight_fraction_leaf 0. Decision tree classification can be used for both binary and multi classes Coding is the same for all supervised classes and we jus need to change the last part of the code. svc_tuned SVC C 100 gamma 0. Bu y\u00fczden en iyi de\u011feri bulmak i\u00e7in C parametresini denememiz gerekiyor. fit x_train y_train y_pred svc_tuned. Hepsi icin yapilabilir ilk 10 datatest deki tahminlerimiz 1. 001 verbose False Hepsi icin yapilabilir EN UYGUN C VE GAMMA DEGERI BULMA svm_tune1 SVC C 100 gamma 0. si 1 olma olasiligi oranlari ERROR ON TRAIN DATA We use Grid for tuning we obta cross validation yontemi kullaniliyor. on test_data D Model tuning Lets look at 6 importance variables LETS SEE ALL SCORE OF CLASSIFICATIONS METHODS FINISH This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. E TUNING THE PREDICTION WE can tune our prediction n_estimators importance variables Lets look at importance 6 variables Conclusion rf2_score 84 is the best score and c_rf2 is the best confusion matrix 6 DECISION TREE METHODThis model uses a Decision Tree as a predictive model which maps features tree branches to conclusions about the target value tree leaves. 0 class_weight None dual False fit_intercept True intercept_scaling 1 l1_ratio None max_iter 100 multi_class auto n_jobs None penalty l2 random_state None solver liblinear tol 0. Finally we can say our test point belongs to the class. E TUNING THE PREDICTION WE can tune our prediction Look at c kernel gamma Tuning1 change C and gamma Tuning2 changing kernel linear c 100Tuning3 changing kernel rbf c 100 Conclusion svm_score1 84 is the best score and c_svm is the best confusion matrix 5 RANDOM FOREST METHODRandom Forests is one of the most popular model. 0 n_estimators 100 n_jobs None oob_score False random_state None verbose 0 warm_start False Hepsi icin yapilabilir yukarda import edildi ilk 10 datatest deki tahminlerimiz 1. 0001 kernel linear svc_tuned. LOOK AT ALL PREDICTION VALUE ON TEST DATA E TUNING THE PREDICTION WE can tune our prediction We can see If we change our condition for probobilty our prediction and confusion matrix and accuracy_score change 2 NAIVE BAYES METHOD In machine learning Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes theorem with strong naive independence assumptions between the features. on test_data CLASSICICATION REPORT we can also see classification report. correlation only with target and other variables Target and Thalech Interpretation We can see that those people suffering from heart disease target 1 have relatively higher heart rate thalach as compared to people who are not suffering from heart disease target 0. K 1 SECERSEK OVERFITTING OLABILIR K BUYUK SECERSEK UNDERFITTING OLABILIR A Train test splitting B Modeling of KNN Medhod C Lets control the succes score prediction accuracy_score confusion m. Logistic regression measures the relationship between the categorical dependent variable feature and one or more independent variables features by estimating probabilities using a logistic function which is the cumulative logistic distribution. Dogru tahmin etme yuzdesi bulunuyor We found the numbers of guessing with confusion matrix 31 for 1 correct guess 0 for 35 correct guess The top was imported. we tune the knn than our score increase. Naive Bayes classifiers are highly scalable requiring a number of parameters linear in the number of variables features in a learning problem. SVM is used fo both regression and classification problems but generally for classification. target and slope variable are weakly positively correlated correlation coefficient 0. While choosing k number we have to be carefull because small k value causes overfitting while big k value causes underfitting. ", "id": "ozericyer/heart-desease-ml-classificationexercises", "size": "12098", "language": "python", "html_url": "https://www.kaggle.com/code/ozericyer/heart-desease-ml-classificationexercises", "git_url": "https://www.kaggle.com/code/ozericyer/heart-desease-ml-classificationexercises", "script": "sklearn.metrics cross_val_score sklearn.naive_bayes sklearn.tree plotly.offline DecisionTreeRegressor KNeighborsRegressor KNeighborsClassifier scale DecisionTreeClassifier RandomForestClassifier  #n_estimotors=11 is best r2_score ShuffleSplit mean_squared_error seaborn numpy iplot plotly.graph_objs sklearn.ensemble confusion_matrix sklearn.model_selection sklearn RandomForestClassifier matplotlib.pyplot pandas classification_report LogisticRegression accuracy_score confusion_matrix   #Hepsi icin yapilabilir GridSearchCV BaggingRegressor model_selection roc_auc_score sklearn.neighbors sklearn.preprocessing roc_curve matplotlib init_notebook_mode sklearn.linear_model GaussianNB sklearn.svm SVC train_test_split ", "entities": "(('We', 'class'), 'count') (('SVM', 'classification generally classification'), 'use') (('It', 'kaggle python Docker image https github'), 'look') (('We', 'poly linear rbf'), 'change') (('cp variable', 'correlation mildly positively coefficient'), 'Definition') (('target', 'correlation oldpeak also mildly negatively coefficient'), 'correlate') (('which', 'logistic function'), 'measure') (('Input data files', 'read'), 'be') (('thal target variable', 'correlation also waekly negatively coefficient'), 'correlate') (('that', 'classification analysis'), 'be') (('ile tahmin', 'yler karsilastiriliyor'), 'verbose') (('So we', 'best value'), 'need') (('Naive Bayes classifiers', 'learning problem'), 'be') (('it', 'nb'), 'tuning') (('which', 'out test point'), 'mean') (('We', 'poly linear rbf'), 'predict') (('test point', 'class'), 'say') (('it', 'test set'), 'test') (('it', 'machine learning model'), 'NORMALIZE') (('it', 'training set'), 'train') (('gamma scale kernel False 0 decision_function_shape ovr degree 9 poly max_iter 1 None', 'True'), 'probability') (('warm_start False 0 Hepsi', 'yapilabilir yukarda import edildi ilk'), 'n_estimators') (('we', 'code'), 'use') (('we', 'n neigbors tuned acurracy_score'), 'be') (('k', 'most common k nearest neighbors'), 'classify') (('cp', 'thalach target mildly positively variable'), 'correlate') (('it', 'misclassification'), 'cause') (('i', '0 30'), 'si') (('that', 'class labels'), 'call') (('which', 'target'), 'be') (('icinde hangi secenekler vargormek', 'LogisticRegression C'), 'number') (('we', 'metric'), 'tuning') (('we', 'validation yontemi kullaniliyor'), 'datum') (('it', 'one category'), 'mark') (('we', 'classification also report'), 'see') (('We', 'Heatmap'), 'on') (('SVM i\u00e7erisinde parametresi vard\u0131r ve C parametresinin C default', '1 dir'), 'de\u011feri') (('CLASSICICATION we', 'classification also report'), 'see') (('methods', 'decision trees'), 'be') (('target', 'thalach correlation also mildly positively coefficient'), 'correlate') (('we', 'k value'), 'need') (('which', 'target variable'), 'be') (('Method C Lets', 'succes score prediction accuracy_score confusion m.'), 'control') (('feature', '\u0131nformation entropy'), 'determine') (('k big value', 'underfitting'), 'choose') (('we', 'test point'), 'use') (('we', 'best score'), 'obtain') (('bir veya daha \u00f6zellik ile fazla', 'aras\u0131ndaki ili\u015fkiyi de\u011fi\u015fken \u00f6zellik \u00f6l\u00e7er'), 'de\u011fi\u015fken') (('we', 'code'), 'be') (('UYGUN C VE GAMMA DEGERI', 'SVC svm_tune1 C 100 gamma'), 'verbose') (('who', 'heart disease target'), 'have') (('Naive Bayes classifiers', 'features'), 'at') (('target variable', 'correlation weakly positively coefficient'), 'correlate') (('we', 'related tree'), 'put') (('kernel', 'UPVOTES'), 'find') (('SECERSEK OLABILIR K BUYUK K 1 UNDERFITTING', 'KNN Medhod C Lets'), 'overfitte') (('target', 'exang correlation mildly negatively coefficient'), 'correlate') (('oranlari KNeighborsClassifier algorithm auto minkowski None p weights si 1 olma olasiligi leaf_size 30 metric n_neighbors 3 2 Hepsi', 'yapilabilir ilk'), 'metric_param') (('top', '0 35 correct guess'), 'tahmin') (('who', 'heart disease target'), 'correlation') (('Decision tree', 'tree structure'), 'build') (('confusion RANDOM FOREST METHODRandom 84 best best 5 Forests', 'most popular model'), 'tuning') (('Hepsi', 'yapilabilir ilk'), 'icin') (('default value', 'C parameter'), 'be') (('nb_tuned_bestscore 89', 'k short non parametric classification'), 'be') (('Logistic Nearest Support LOGISTIC REGRESSION Logistic 1 Regression 2 Neighbours Classifier 3 Vector machine 4 Decision Tree 5 Random Forest Classifier 1 Regression', 'useful early workflow'), 'go') (('SVM hem regresyon hem de s\u0131n\u0131fland\u0131rma problemleri i\u00e7in kullan\u0131l\u0131r ancak', 's\u0131n\u0131fland\u0131rma i\u00e7in kullan\u0131l\u0131r'), 'genellikle') (('we', 'pie'), 'list') (('typically real numbers', 'continuous values'), 'call') (('i', '0 35'), 'icin') (('olma oranlari si 1 olasiligi Hepsi', 'yapilabilir'), 'icin') (('decision associated tree', 'same time'), 'break') (('target', 'correlation weakly negatively coefficient'), 'correlate') (('Train test', 'succes score prediction accuracy_score confusion m.'), 'control') (('maps', 'target value tree leaves'), 'tuning') ", "extra": "['disease', 'patient', 'test']", "label": "Perfect_files", "potential_description_queries": ["admission", "age", "algorithm", "analyze", "associated", "auto", "barplot", "best", "binary", "blood", "boxplot", "build", "call", "categorical", "category", "chest", "choose", "classification", "coefficient", "colored", "condition", "confusion", "control", "correct", "correlation", "count", "create", "criterion", "current", "data", "dataset", "decision", "default", "defect", "degree", "dependent", "directory", "disease", "distplot", "en", "ensemble", "entropy", "environment", "exercise", "family", "feature", "file", "find", "fit", "fixed", "form", "found", "function", "gamma", "gini", "heart", "heatmap", "hospital", "hue", "image", "import", "importance", "increase", "individual", "input", "integer", "interactive", "jitter", "kaggle", "kernel", "l2", "lead", "learning", "linear", "list", "little", "load", "look", "lr", "major", "majority", "male", "map", "matrix", "max_depth", "max_features", "max_iter", "maximum", "mean", "method", "metric", "mm", "mode", "model", "most", "mu", "multiple", "naive", "nb", "nearest", "need", "negative", "new", "no", "non", "normal", "not", "number", "option", "out", "output", "overfitting", "parameter", "part", "patient", "pattern", "peak", "people", "point", "poly", "positive", "predict", "prediction", "probability", "processing", "python", "random", "re", "read", "regression", "relationship", "relative", "rest", "row", "run", "running", "sample", "scale", "scatter", "score", "segment", "session", "set", "several", "sex", "short", "split", "splitting", "supervised", "svm", "target", "test", "those", "time", "train", "training", "tree", "try", "tune", "tuning", "type", "under", "uniform", "up", "validation", "value", "var", "variable", "version", "visualization", "vote", "while", "who", "write", "y_test"], "potential_description_queries_len": 175, "potential_script_queries": ["iplot", "matplotlib", "numpy", "seaborn", "sklearn"], "potential_script_queries_len": 5, "potential_entities_queries": ["associated", "best", "correct", "data", "disease", "import", "learning", "max_iter", "nearest", "poly", "python", "score", "target", "tree"], "potential_entities_queries_len": 14, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 178}