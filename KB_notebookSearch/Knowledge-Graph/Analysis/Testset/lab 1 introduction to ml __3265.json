{"name": "lab 1 introduction to ml ", "full_name": " h1 Creators h1 xsub train h2 Our xsub col choices h2 Notes Discussion h4 max vals h1 kNN Tests h1 Discussion h2 All cols and subsetting h2 kNN Results ", "stargazers_count": 0, "forks_count": 0, "description": "This can be seen with the two subsets 5 code blocks up one is commented out. In both cases subsetting the data could improve the accuracy of classifying the classes moreso classifying class 1. kNN Results k 1 97 accuracy classifying class 0 95 accuracy classifying class 1 k 5 99 accuracy classifying class 0 98 accuracy classifying class 1Using the neighbor classifier model improved the performance over the original SVC model used. In this case subsetting was able to improve the accuracy of classifying class 1 from 85 to 92 a significant difference. Class 0 was now classified 99 of the time and class 1 was classified 98 of the time. After reloading the page the accuracy then dropped to around 95. For more conclusive results we would need to test with a larger dataset. In conclusion it looks like a nearest neighbor model with k 5 is the best approach for the provided dataset. linear algebra data processing CSV file I O e. However it can be said that subsetting improved our performance with a good subset. Creators Morgan Dally 1313361 Reece Breebaart 1314828 xsub train Our xsub col choices nuclei size or shape these two will be related features so try either or adhesion single Notes Discussion Class 0 looks to be easier to distinguish from class 1 We therefore want to select in a way that makes class 1 features more prominent We experimented and eyeballed the accuaracy tried to determine which classes were detremental to perfomance it seemed that selecting columns with a dominant class provided the largest accuracy in the end we were able to make a subset that contradicted any results we found trying to find any patterns we were not able to determine good or bad individual columns we ended up with 5 2 3 4 as they improved accuracy max vals 1 139 2 373 5 3 346 4 393 3 5 376 4 seems to influence results badly contradicted as we made a good subset with it improving performance 6 402 2 7 161 8 432 1 kNN Tests DiscussionFrom the above tests our results were x_train 100 accuracy classifying class 0 85 accuracy classifying class 1 xsub_train 100 accuracy classifying class 0 92 accuracy classifying class 1 Nearest neighbor model k 1 97 accuracy classifying class 0 95 accuracy classifying class 1 Nearest neighbor model k 5 99 accuracy classifying class 0 98 accuracy classifying class 1 All cols and subsettingModifying the test and train set to only use a subset of columns saw varying results. To compensate for this the random state used for splitting data has been pinned. When k 1 it could be seen that the accuracy improve for classifying class 1 92 95 however the accuracy for finding class 0 dropped 100 97. In general it seems that with this dataset subsetting the data can improve accuracy. The largest accuracy difference was actually found by modifying the random_state value when initially splitting into the train and test sets. read_csv get rid of NaN entries select all entries then get from col1 and select up until the end exlcuding it select all entries then get col 10 selecting rows to increase accuracy eyeballed 96 accuracy experimenting getting bad accuracy came out with 92 cols 5 4 8 1 when k value is 1 when k value is 1. However it was possible to also degrade the performance of our model. For example at one point the initial split saw 98 accuracy. When setting k 5 however or accuracy improved greatly. ", "id": "mjdall/lab-1-introduction-to-ml", "size": "3265", "language": "python", "html_url": "https://www.kaggle.com/code/mjdall/lab-1-introduction-to-ml", "git_url": "https://www.kaggle.com/code/mjdall/lab-1-introduction-to-ml", "script": "sklearn.metrics sklearn.model_selection confusion_matrix seaborn numpy print_predicted_stats SVC sklearn.svm pandas classification_report train_test_split ", "entities": "(('accuracy largest difference', 'when initially train sets'), 'find') (('we', 'larger dataset'), 'need') (('Nearest neighbor 5 0 1 k 99 accuracy classifying class 98 accuracy classifying 1 cols', 'varying results'), 'dally') (('1 0 5 0 neighbor classifier Results k 97 accuracy classifying class 95 accuracy classifying class 1 k 99 accuracy classifying class 98 accuracy classifying 1Using model', 'SVC original model'), 'class') (('neighbor nearest model', 'best provided dataset'), 'look') (('initial split', '98 accuracy'), 'see') (('random state', 'data'), 'pin') (('however accuracy', 'When 5'), 'set') (('This', 'one'), 'comment') (('subsetting', 'classes moreso classifying class'), 'improve') (('accuracy', 'then around 95'), 'drop') (('dataset', 'accuracy'), 'seem') (('k when value', '92 cols'), 'rid') (('subsetting', 'good subset'), 'say') (('class', '1 98 time'), 'classify') (('However it', 'model'), 'be') (('however accuracy', 'class'), 'see') (('subsetting', 'significant difference'), 'be') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "approach", "best", "case", "classifier", "code", "col", "conclusion", "could", "data", "dataset", "difference", "dominant", "end", "file", "find", "found", "general", "improve", "increase", "individual", "influence", "initially", "largest", "linear", "max", "model", "nearest", "need", "not", "nuclei", "out", "page", "performance", "point", "processing", "random", "select", "set", "shape", "single", "size", "split", "splitting", "state", "subset", "test", "time", "train", "try", "until", "up", "value"], "potential_description_queries_len": 53, "potential_script_queries": ["numpy", "seaborn", "sklearn"], "potential_script_queries_len": 3, "potential_entities_queries": ["classifier", "largest", "train"], "potential_entities_queries_len": 3, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 56}