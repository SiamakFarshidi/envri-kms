{"name": "a guide on xgboost hyperparameters tuning ", "full_name": " h1 A Guide on XGBoost hyperparameters tuning h1 Table of Contents h1 1 What are hyperparameters h1 2 XGBoost hyperparameters h2 2 1 General Parameters h3 2 1 1 booster h3 2 1 2 verbosity h3 2 1 3 nthread h2 2 2 Booster Parameters h3 2 2 1 eta h3 2 2 2 gamma h3 2 2 3 max depth h3 2 2 4 min child weight h3 2 2 5 max delta step h3 2 2 6 subsample h3 2 2 7 colsample bytree colsample bylevel colsample bynode h3 2 2 8 lambda h3 2 2 9 alpha h3 2 2 10 tree method h3 2 2 11 scale pos weight h3 2 2 12 max leaves h2 2 3 Learning Task Parameters h3 2 3 1 objective h3 2 3 2 eval metric h3 2 3 3 seed h1 3 Basic Setup h3 3 1 Import libraries h3 3 2 Read dataset h3 3 3 Declare feature vector and target variable h3 3 4 Split data into separate training and test set h1 4 Bayesian Optimization with HYPEROPT h2 4 1 What is HYPEROPT h2 4 2 4 parts of Optimization Process h2 4 3 Bayesian Optimization implementation h3 4 3 1 Initialize domain space for range of values h3 4 3 2 Define objective function h3 4 3 3 Optimization algorithm h3 4 3 4 Print Results h1 5 Results and Conclusion h1 6 References ", "stargazers_count": 0, "forks_count": 0, "description": "3 Learning Task Parameters Table of Contents 0. Subsample ratio of the training instances. A Guide on XGBoost hyperparameters tuning Hello friends In my previous kernel XGBoost k fold CV Feature Importance https www. 7 colsample_bytree colsample_bylevel colsample_bynode 2. approx Approximate greedy algorithm using quantile sketch and gradient histogram. In this section we discuss one of the most accurate and successful hyperparameter tuning method which is Bayesian Optimization with HYPEROPT. range 0 1 Typical final values 0. Please visit Parameters for Tree Booster https xgboost. range 0 2. 1 In this kernel we have discussed the XGBoost hyperparameters which are divided into 3 categories general parameters booster parameters and learning task parameters. html general parameters https medium. convert labels into binary values again preview the y label. colsample_by parameters work cumulatively. 1 subsample default 1 It denotes the fraction of observations to be randomly samples for each tree. com prashant111 bayesian optimization using hyperopt for more information on the optimization process using HYPEROPT. I hope you find this kernel useful and enjoyable. 1 These parameters guide the overall functioning of the XGBoost model. Please see my kernel Bayesian Optimization using HYPEROPT https www. Hyperopt is a Python library which is used to tune model hyperparameters. 1 eta Table of Contents 0. In this section we will discuss three hyperparameters booster verbosity and nthread. 2 gamma Table of Contents 0. org wiki Mean_absolute_error logloss negative log likelihood https en. html https xgboost. Columns are subsampled from the set of columns chosen for the current level. Typical values 0. If this helped in your learning then please UPVOTE as they are the source of motivation Happy Learning Table of Contents 1 What are hyperparameters 1 2 XGBoost hyperparameters 2 2. The values can vary depending on the loss function and should be tuned. Now XGBoost algorithm provides large range of hyperparameters. For very large dataset approximate algorithm approx will be chosen. 12 max_leaves Table of Contents 0. The objective options are below 2. 8 lambda Table of Contents 0. org stable modules generated sklearn. 2 4 parts of Optimization Process Table of Contents 0. There are other general parameters like disable_default_eval_metric default 0 num_pbuffer set automatically by XGBoost no need to be set by user and num_feature set automatically by XGBoost no need to be set by user. The value 0 is only accepted in lossguided growing policy when tree_method is set as hist and it indicates no limit on depth. Because old behavior is always use exact greedy in single machine user will get a message when approximate algorithm is chosen to notify this choice. com analytics vidhya hyperparameter tuning hyperopt bayesian optimization for xgboost and neural network 8aedf278a1c9 https www. gbtree and dart use tree based models while gblinear uses linear models. It can be used in case of very high dimensionality so that the algorithm runs faster when implemented. In this case we want to minimize the validation error of a machine learning model with respect to the hyperparameters. 4 min_child_weight 2. The larger min_child_weight is the more conservative the algorithm will be. trials is an object that contains or stores all the relevant information such as hyperparameter loss functions for each set of parameters that the model has been trained. 1 The above result give best set of hyperparameters. Results and Conclusion Table of Contents 0. 3 max_depth Table of Contents 0. 1 These parameters are used to define the optimization objective the metric to be calculated at each step. 4 4 Bayesian Optimization with HYPEROPT 4 4. So these parameters are taken care by XGBoost algorithm itself. XGBoost supports approx hist and gpu_hist for distributed training. 2 eval_metric Table of Contents 0. We have found the best hyperparameters for the XGBoost ML model. colsample_bytree is the subsample ratio of columns when constructing each tree. In tree based models hyperparameters include things like the maximum depth of the tree the number of trees to grow the number of variables to consider when building each tree the minimum number of samples on a leaf and the fraction of observations used to build a tree. 9 alpha Table of Contents 0. 1 We have 2 types of boosters tree booster and linear booster. 1 lambda default 1 alias reg_lambda L2 regularization term on weights analogous to Ridge regression. com prashant111 xgboost k fold cv feature importance. A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence. There are other hyperparameters like sketch_eps updater refresh_leaf process_type grow_policy max_bin predictor and num_parallel_tree. Hence it should be tuned using CV. We have discussed the 4 parts of optimization process. This is used to handle the regularization part of XGBoost. 2 4 Parts of Optimization Process 4. We have discussed Bayesian Optimization with HYPEROPT. This parameter is ignored in R package use set. 1 Initialize domain space for range of values Table of Contents 0. 3 Bayesian Optimization Implementation 4. A typical value to consider sum negative instances sum positive instances. Command line parameters Before running a XGBoost model we must set three types of parameters general parameters booster parameters and task parameters. Should be tuned using CV. 11 scale_pos_weight 2. 1 Import libraries 3. 2 Define objective function 4. Choices auto exact approx hist gpu_hist auto Use heuristic to choose the fastest method. For example running this by clicking run or pressing Shift Enter will list all files under the input directory Any results you write to the current directory are saved as output. 1 objective default reg squarederror It defines the loss function to be minimized. Only relevant when grow_policy lossguide is set. The default values are rmse for regression error for classification and mean average precision for ranking. 1 Generally the XGBoost hyperparameters have been divided into 4 categories. After each boosting step we can directly get the weights of new features and eta shrinks the feature weights to make the boosting process more conservative. exact Exact greedy algorithm. So we will start with HYPEROPT. Bayesian Optimization with HYPEROPT Table of Contents 0. 1 The ideas and concepts in this kernel are taken from the following websites. 3 nthread Table of Contents 0. gpu_hist GPU implementation of hist algorithm. More information on Hyperopt can be found at the following link https hyperopt. 1 eval_metric default according to objective The metric to be used for validation data. It makes the algorithm conservative. XGBoost is a very powerful algorithm. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree. 1 alpha default 0 alias reg_alpha L1 regularization term on weights analogous to Lasso regression. 1 max_delta_step default 0 In maximum delta step we allow each tree s weight estimation to be. hist Fast histogram optimized approximate greedy algorithm. uniform label low high Returns a value uniformly between low and high. 1 scale_pos_weight default 1 It controls the balance of positive and negative weights It is useful for imbalanced classes. This refers to min sum of weights of observations while GBM has min number of observations. It is used to control over fitting as higher depth will allow model to learn relations very specific to a particular sample. io hyperopt source post_page 4. 1 nthread default maximum number of threads available if not set This is number of parallel threads used to run XGBoost. For the predictions the evaluation will regard the instances with prediction value larger than 0. Python users must pass the metrices as list of parameters pairs instead of map. It uses some performance improvements such as bins caching. It helps us to select the type of model to run at each iteration. Subsampling occurs once for every new depth level reached in a tree. 10 tree_method Table of Contents 0. io en latest parameter. This is similar to min_child_leaf in GBM but not exactly. For detailed discussion of these hyperparameters please visit Parameters for Tree Booster https xgboost. org wiki Likelihood_function Log likelihood error Binary classification error rate 0. com yassinealouini hyperopt the xgboost modelSo now we will come to the end of this kernel. 5 max_delta_step Table of Contents 0. quniform label low high q Returns a value round uniform low high q q i. 3 alias learning_rate It is analogous to learning rate in GBM. Range 0 2. All colsample_by parameters have a range of 0 1 the default value of 1 and specify the fraction of columns to be subsampled. Valid values are 0 silent 1 warning 2 info 3 debug. choice label options Returns one of the options which should be a list or tuple. e it rounds the decimal values and returns an integer. 1 max_depth default 6 The maximum depth of a tree same as GBM. If it is set to a positive value it can help making the update step more conservative. 3 Optimization algorithm Table of Contents 0. 1 Initialize domain space for range of values 4. 4 Split data into separate training and test set Table of Contents 0. If the value is set to 0 it means there is no constraint. 1 booster default gbtree booster parameter helps us to choose which booster to use. 5 means that XGBoost would randomly sample half of the training data prior to growing trees. 3 Declare feature vector and target variable Table of Contents 0. 1 min_child_weight default 1 It defines the minimum sum of weights of all observations required in a child. com prashant111 xgboost k fold cv feature importance we have discussed XGBoost and develop a simple baseline XGBoost model. 1 What is HYPEROPT 4. 1 In this kernel we will discuss the critical problem of hyperparameter tuning in XGBoost model. Subsampling occurs once for every tree constructed. 1 General Parameters 2. 3 3 Basic Setup 3 3. Subsampling will occur once in every boosting iteration. html auc Area under the curve https en. 2 Read dataset Table of Contents 0. We can see that the y label contain values as 1 and 2. randint label upper Returns a random integer between the range 0 upper. General parameters 2. For instance the combination colsample_bytree 0. 4 5 Results and Conclusion 5 6 References 6 1. 6 subsample Table of Contents 0. 1 booster Table of Contents 0. As stated earlier XGBoost provides large range of hyperparameters. org wiki Receiver_operating_characteristic Area_under_curve aucpr Area under the PR curve https en. Basic Setup Table of Contents 0. If you wish to run on all cores value should not be entered and algorithm will detect automatically. io en latest tutorials param_tuning. References Table of Contents 0. In tree based models like XGBoost the learnable parameters are the choice of decision variables at each node. Most commonly used values are given below reg squarederror regression with squared loss. Columns are subsampled from the set of columns chosen for the current tree. 5 as positive instances and the others as negative instances. We will need to convert it into 0 and 1 for further analysis. 4 Split data into separate training and test set 3. It has 3 options gbtree gblinear or dart. Typical values 3 10 2. 1 tree_method string default auto The tree construction algorithm used in XGBoost. 7 colsample_bytree colsample_bylevel colsample_bynode Table of Contents 0. 1 objective Table of Contents 0. 1 I will skip the EDA part as I have done it in previous kernel XGBoost k fold CV Feature Importance https www. They are only used in the console version of XGBoost. In this process we train the model with various possible range of parameters until a best fit model is obtained. 1 HYPEROPT is a powerful python library that search through an hyperparameter space of values and find the best possible values that yield the minimum of the loss function. 2 Booster Parameters Table of Contents 0. 3 Learning Task Parameters 2. Booster parameters 3. Then the function should return the negative of that metric. fmin is an optimization function that minimizes the loss function and takes in 4 inputs fn space algo and max_evals. colsample_bylevel is the subsample ratio of columns for each level. They are used to specify the learning task and the corresponding learning objective. The result contains predicted probability of each data point belonging to each class. 2. mlogloss Multiclass logloss https scikit learn. We will limit our discussion to tree booster because it always outperforms the linear booster and thus the later is rarely used. 3 seed Table of Contents 0. 3 Bayesian Optimization implementation Table of Contents 0. Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under fitting. Increasing this value will make model more conservative. XGBoost hyperparameters Table of Contents 0. colsample_bynode is the subsample ratio of columns for each node split. Set it to value of 1 10 might help control the update. 4 Print Results Table of Contents 0. So it will have more design decisions and hence large hyperparameters. What are hyperparameters Table of Contents 0. html parameters for tree booster for detailed discussion on booster parameters. 3 Declare feature vector and target variable 3. 3 Optimization algorithm 4. Learning task parameters 4. It is used to control over fitting. 11 scale_pos_weight Table of Contents 0. They are as follows 1. Increasing this value will make the model more complex and more likely to overfit. 1 Now let s take a look at feature vector X and target variable y. Optimization algorithm It is the method used to construct the surrogate objective function and choose the next values to evaluate. 1 The available hyperopt optimization algorithms are hp. 5 colsample_bylevel 0. Hyperparameters are certain values or weights that determine the learning process of an algorithm. 2 verbosity Table of Contents 0. It is calculated as wrong cases all cases. 1 Bayesian optimization is optimization or finding the best parameter for a machine learning or deep learning algorithm. Bayesian Optimization technique uses Hyperopt to tune the model hyperparameters. It can be used for generating reproducible results and also for parameter tuning. 2 Define objective function Table of Contents 0. org wiki Root mean square_deviation mae mean absolute error https en. The most powerful ML algorithm like XGBoost is famous for picking up patterns and regularities in the data by automatically tuning thousands of learnable parameters. The same technique can be applied to find the optimum hyperparameters for any other ML model. The fourth type of parameters are command line parameters. It makes the model more robust by shrinking the weights on each step. 5 1 range 0 1 2. com blog 2016 03 complete guide parameter tuning xgboost with codes python https www. 1 General Parameters Table of Contents 0. We should know how to tune these hyperparameters to improve and take full advantage of the XGBoost model. 5 max_delta_step 2. Hence in this kernel we will discuss main hyperparameters of the XGBoost model and how to tune these hyperparameters. html general parameters for detailed discussion on general parameters. Hence we will not discuss these further. Setting it to 0. Define objective function The objective function can be any function which returns a real value that we want to minimize. So we will skip these parameters and limit our discussion to the first three type of parameters. 1 gamma default 0 alias min_split_loss A node is split only when the resulting split gives a positive reduction in the loss function. 1 max_leaves default 0 Maximum number of nodes to be added. The most common values are given below rmse root mean square error https en. 5 with 64 features will leave 8 features to choose from at each split. 1 verbosity default 1 Verbosity of printing messages. Although we focus on optimizing XGBoost hyperparameters in this kernel the concepts discussed in this kernel applies to any other advanced ML algorithm as well. html parameters for tree booster 2. Optimization is the process of finding a minimum of cost function that determines an overall better performance of a model on both train set and test set. Thank you Go to Top 0 import pandas for data wrangling import numpy for Scientific computations import machine learning libraries import packages for hyperparameters tuning Input data files are available in the. range 0 0 is only accepted in lossguided growing policy when tree_method is set as hist. 1 What is HYPEROPT Table of Contents 0. 1 colsample_bytree colsample_bylevel colsample_bynode default 1 This is a family of parameters for subsampling of columns. 4 min_child_weight Table of Contents 0. We will do it as follows We can see that our target variable y has been converted into 0 and 1. 1 seed default 0 The random number seed. Usually this parameter is not needed but it might help in logistic regression when class is extremely imbalanced. merror Multiclass classification error rate. So let s get started. Hyperparameter tuning helps in determining the optimal tuned parameters and return the best fit model which is the best practice to follow while building an ML or DL model. 1 The optimization process consists of 4 parts which are as follows 1. If the real value is accuracy then we want to maximize it. 1 Here best_hyperparams gives us the optimal parameters that best fit model and better loss function value. Initialize domain space The domain space is the input values over which we want to search. 2 Booster Parameters 2. It is the step size shrinkage used in update to prevent overfitting. This will prevent overfitting. For small to medium dataset exact greedy exact will be used. We should be careful when setting large value of max_depth because XGBoost aggressively consumes memory when training a deep tree. Gamma specifies the minimum loss reduction required to make a split. This is used for parallel processing and number of cores in the system should be entered. This makes predictions of 0 or 1 rather than producing probabilities. normal label mean std Returns a real value that s normally distributed with mean and standard deviation sigma. Your comments and feedback are most welcome. reg squaredlogerror regression with squared log loss 1 2 log pred 1 log label 1 2. Algorithm used is tpe. 5 colsample_bynode 0. org wiki Precision_and_recall 2. Subsampling occurs once every time a new split is evaluated. 1 Import libraries Table of Contents 0. The larger gamma is the more conservative the algorithm will be. All input labels are required to be greater than 1. Results Results are score or value pairs that the algorithm uses to build the model. multi softmax set XGBoost to do multiclass classification using the softmax objective you also need to set num_class number of classes multi softprob same as softmax but output a vector of ndata nclass which can be further reshaped to ndata nclass matrix. reg logistic logistic regression binary logistic logistic regression for binary classification output probability binary logitraw logistic regression for binary classification output score before logistic transformation binary hinge hinge loss for binary classification. These are parameters specified by hand to the algo and fixed throughout a training phase. We can add multiple evaluation metrics. We can leverage the maximum power of XGBoost by tuning its hyperparameters. Too high values can lead to under fitting. Please visit XGBoost General Parameters https xgboost. Experimental support for external memory is available for approx and gpu_hist. ", "id": "prashant111/a-guide-on-xgboost-hyperparameters-tuning", "size": "24437", "language": "python", "html_url": "https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning", "git_url": "https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning", "script": "sklearn.metrics Trials sklearn.model_selection objective STATUS_OK hp numpy tpe hyperopt pandas fmin xgboost train_test_split accuracy_score ", "entities": "(('So we', 'parameters'), 'skip') (('1 What', 'Happy Learning Contents'), 'please') (('Maximum number', 'nodes'), 'default') (('XGBoost k', 'CV Feature Importance https www'), 'Guide') (('which', 'tree'), 'prevent') (('which', 'options'), 'return') (('We', 'further analysis'), 'need') (('same technique', 'ML other model'), 'apply') (('1 Bayesian optimization', 'machine learning'), 'be') (('Set', 'update'), 'help') (('we', 'that'), 'define') (('algorithm', 'model'), 'be') (('boosting process', 'eta feature weights'), 'get') (('convert', 'y again label'), 'preview') (('1 nthread', 'XGBoost'), 'default') (('XGBoost Now algorithm', 'hyperparameters'), 'provide') (('ML most powerful algorithm', 'learnable parameters'), 'be') (('default values', 'ranking'), 'be') (('subsample 1 1 It', 'randomly tree'), 'default') (('1 2 log', 'log 1 label'), 'regression') (('input labels', '1'), 'require') (('Python users', 'parameters pairs'), 'pass') (('Subsampling', 'tree'), 'occur') (('you', 'output'), 'list') (('min_child_weight 1 1 It', 'child'), 'default') (('it', 'depth'), 'accept') (('Now s', 'feature vector X'), 'let') (('XGBoost', 'hyperparameters Contents'), 'table') (('colsample_bytree colsample_bylevel colsample_bynode 1 1 This', 'columns'), 'default') (('It', 'iteration'), 'help') (('We', 'HYPEROPT'), 'discuss') (('It', 'overfitting'), 'be') (('XGBoost', 'prior growing trees'), 'mean') (('thus later', 'always linear booster'), 'limit') (('We', 'hyperparameters'), 'leverage') (('which', '1'), 'consist') (('Python which', 'model hyperparameters'), 'be') (('So parameters', 'XGBoost algorithm'), 'take') (('Valid values', '0 silent 1 warning 2 info'), 'be') (('We', 'XGBoost model'), 'know') (('algorithm', 'cores value'), 'enter') (('parameters', 'step'), 'use') (('more too small values', 'fitting'), 'make') (('which', 'task parameters'), '1') (('update', 'positive value'), 'help') (('model', 'step'), 'make') (('This', 'XGBoost'), 'use') (('values', 'loss function'), 'vary') (('we', 'how hyperparameters'), 'discuss') (('Bayesian Optimization technique', 'model hyperparameters'), 'use') (('It', 'imbalanced classes'), '1') (('It', 'very specific particular sample'), 'use') (('above result', 'hyperparameters'), '1') (('0 num_pbuffer', 'user'), 'be') (('We', 'boosters tree booster'), '1') (('Gamma', 'split'), 'specify') (('5 with 64 features', 'split'), 'leave') (('Too high values', 'fitting'), 'lead') (('Then function', 'metric'), 'return') (('They', 'learning task'), 'use') (('org wiki Likelihood_function', 'likelihood error Binary classification error rate'), 'Log') (('now we', 'kernel'), 'com') (('We', 'optimization process'), 'discuss') (('com analytics vidhya hyperparameter', 'https neural www'), 'tune') (('when approximate algorithm', 'choice'), 'get') (('gblinear', 'linear models'), 'use') (('us', 'booster'), 'help') (('They', 'XGBoost'), 'use') (('fit best model', 'parameters'), 'train') (('0', 'Contents'), 'be') (('ideas', 'following websites'), '1') (('typical value', 'positive instances'), 'sum') (('GBM', 'observations'), 'refer') (('which', 'nclass further ndata matrix'), 'set') (('Here best_hyperparams', 'optimal parameters'), '1') (('which', 'best ML model'), 'tuning') (('colsample_bynode', 'node split'), 'be') (('It', 'cases'), 'calculate') (('This', 'system'), 'enter') (('hyperparameters', 'tree'), 'include') (('it', 'faster convergence'), 'use') (('it', 'integer'), 'round') (('It', 'parameter also tuning'), 'use') (('It', 'GBM'), 'alias') (('only when resulting split', 'loss function'), 'alia') (('y label', '1'), 'see') (('earlier XGBoost', 'hyperparameters'), 'provide') (('when class', 'logistic regression'), 'need') (('XGBoost', 'distributed training'), 'support') (('we', 'hyperparameters'), 'want') (('we', 'hyperparameters booster three verbosity'), 'discuss') (('These', 'training phase'), 'be') (('that', 'loss function'), 'be') (('parameters', 'XGBoost model'), 'guide') (('kernel', 'Bayesian HYPEROPT https www'), 'see') (('Columns', 'current level'), 'subsample') (('More information', 'link https following hyperopt'), 'find') (('predictions', 'rather probabilities'), 'make') (('certain that', 'algorithm'), 'be') (('Choices approx hist gpu_hist auto auto exact Use', 'fastest method'), 'heuristic') (('colsample_by parameters', 'columns'), 'have') (('Optimization It', 'next values'), 'algorithm') (('XGBoost Generally hyperparameters', '4 categories'), '1') (('then we', 'it'), 'want') (('algorithm', 'very high dimensionality'), 'use') (('fourth type', 'parameters'), 'be') (('we', 'XGBoost model'), '1') (('we', 'parameters booster parameters general parameters'), 'parameter') (('parameter', 'R package use set'), 'ignore') (('It', 'options gbtree 3 gblinear'), 'have') (('quniform label', 'low high q'), 'return') (('learnable parameters', 'node'), 'be') (('colsample_bytree', 'when tree'), 'be') (('hist Fast histogram', 'approximate greedy algorithm'), 'optimize') (('target variable', '0'), 'do') (('we', 'baseline XGBoost simple model'), 'com') (('It', 'bins such caching'), 'use') (('you', 'the'), 'thank') (('Experimental support', 'approx'), 'be') (('weight estimation', 'delta 0 maximum step'), '1') (('randint label', 'upper random range'), 'return') (('evaluation', 'larger 0'), 'regard') (('Split 4 data', '3'), 'set') (('XGBoost', 'when deep tree'), 'be') (('colsample_bylevel', 'level'), 'be') (('contains', 'class'), 'predict') (('Subsampling', 'once boosting iteration'), 'occur') (('Most commonly used values', 'squared loss'), 'give') (('that', 'train set'), 'be') (('it', '0'), 'set') (('So it', 'design more decisions'), 'have') (('we', 'input which'), 'initialize') (('This', 'GBM'), 'be') (('that', 'deviation normally mean standard sigma'), 'mean') (('which', 'Bayesian HYPEROPT'), 'discuss') (('when tree_method', 'hist'), 'accept') (('model', 'value'), 'make') (('alpha 1 default', 'Lasso analogous regression'), 'alias') (('model', 'parameters'), 'be') (('concepts', 'ML other advanced algorithm'), 'apply') (('optimization that', 'inputs fn space 4 algo'), 'be') (('XGBoost k', 'CV Feature Importance https www'), 'skip') (('Columns', 'current tree'), 'subsample') (('We', 'XGBoost ML model'), 'find') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["absolute", "accuracy", "advanced", "advantage", "algorithm", "auc", "auto", "average", "balance", "baseline", "behavior", "best", "binary", "blog", "boosting", "build", "care", "case", "choice", "choose", "classification", "command", "consider", "contain", "control", "convert", "cost", "current", "curve", "cv", "data", "dataset", "decision", "default", "define", "depth", "detect", "develop", "dimensionality", "directly", "directory", "distributed", "domain", "en", "end", "error", "estimation", "evaluation", "every", "external", "family", "faster", "feature", "feedback", "final", "find", "fit", "fitting", "fixed", "fmin", "fn", "fold", "following", "found", "function", "gamma", "general", "generated", "gradient", "grow", "half", "hand", "handle", "help", "high", "hist", "histogram", "hope", "hyperopt", "hyperparameter", "imbalance", "implementation", "import", "importance", "improve", "include", "info", "input", "instance", "integer", "io", "kernel", "label", "lead", "leaf", "learn", "learning", "learning_rate", "leave", "let", "level", "library", "likelihood", "line", "linear", "link", "list", "log", "look", "main", "max_depth", "maximum", "mean", "memory", "message", "method", "metric", "might", "min", "minimize", "minimum", "model", "most", "motivation", "multiple", "my", "need", "negative", "network", "neural", "new", "next", "no", "node", "normal", "not", "number", "numpy", "object", "objective", "optimization", "output", "overall", "overfitting", "package", "parallel", "parameter", "part", "performance", "point", "positive", "power", "practice", "precision", "pred", "prediction", "predictor", "prevent", "printing", "probability", "problem", "processing", "python", "randint", "random", "range", "ratio", "reg", "regression", "regularization", "result", "return", "rmse", "robust", "run", "running", "sample", "scikit", "score", "search", "section", "select", "selected", "separate", "set", "similar", "single", "size", "softmax", "source", "space", "split", "square", "squared", "standard", "start", "std", "step", "string", "sum", "support", "system", "target", "task", "technique", "term", "test", "through", "time", "train", "training", "transformation", "tree", "tune", "tuning", "type", "under", "uniform", "until", "up", "update", "upper", "user", "validation", "value", "variable", "vector", "version", "visit", "warning", "weight", "while", "work", "write", "xgboost"], "potential_description_queries_len": 235, "potential_script_queries": ["hp", "numpy", "sklearn", "tpe"], "potential_script_queries_len": 4, "potential_entities_queries": ["auto", "boosting", "error", "fn", "general", "hist", "maximum", "most", "package", "space", "tree", "warning"], "potential_entities_queries_len": 12, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 236}