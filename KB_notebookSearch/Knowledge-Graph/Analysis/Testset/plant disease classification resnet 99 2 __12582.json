{"name": "plant disease classification resnet 99 2 ", "full_name": " h1 PLANT DISEASE CLASSIFICATION USING RESNET 9 h1 Description of the dataset h1 Our goal h2 Importing necessary libraries h1 Exploring the data h4 Visualizing the above information on a graph h4 Images available for training h1 Data Preparation for training h4 Image shape h2 Some Images from training dataset h1 Modelling h4 Some helper functions h2 Building the model architecture h4 Residual Block code implementation h2 Defining the final architecture of our model h1 Training the model h3 We got an accuracy of 99 2 h1 Plotting h4 Helper functions for plotting h2 Validation Accuracy h2 Validation loss h2 Learning Rate overtime h1 Testing model on test data h1 Saving the model h1 Conclusion h1 References h4 Hope you all learned something from this kernel Do upvote if you find this useful h4 Happy Learning h4 Catch you guys on the next one h4 Peace ", "stargazers_count": 0, "forks_count": 0, "description": "It is heplful so that batches between epochs do not look alike. html text A 20PyTorch 20Tensor 20is 20basically used 20for 20arbitrary 20numeric 20computation. The algorithm takes the first 100 samples from 1st to 100th from the training dataset and trains the network. 2 Plotting Helper functions for plotting Validation Accuracy Validation loss Learning Rate overtime Testing model on test data We only have 33 images in test data so let s check the model on all images We can see that the model predicted all the test images perfectly Saving the model There are several ways to save the model in Pytorch following are the two most common ways 1. This also helps in preventing vanishing gradient problem https towardsdatascience. Description of the dataset This dataset is created using offline augmentation from the original dataset. Here is a simple residual block image https www. Saving a model in this way will save the entire module using Python s pickle https docs. Now declare some hyper parameters for the training of the model. ConclusionResNets perform significantly well for image classification when some of the parameters are tweaked and techniques like scheduling learning rate gradient clipping and weight decay are applied. Let s get started. ai aakashns 05b cifar10 resnet PyTorch docs https pytorch. We are using this function other than just an accuracy metric that is likely not going to be differentiable this would mean that the gradient can t be determined which is necessary for the model to improve during training A quick look at the PyTorch docs that yields the cost function cross_entropy https pytorch. Save Load Entire Model This save load process uses the most intuitive syntax and involves the least amount of code. Failing to do this will yield inconsistent inference results. epoch_end We also want to print validation losses accuracies train losses and learning rate too because we are using learning rate scheduler which will change the learning rate after every batch of training after each epoch. Next it takes the second 100 samples from 101st to 200th and trains the network again. com method resnet In ResNets unlike in traditional neural networks each layer feeds into the next layer we use a network with residual blocks each layer feeds into the next layer and directly into the layers about 2 3 hops away to avoid over fitting a situation when validation loss stop decreasing at a point and then keeps increasing while training loss still decreases. Even if you have used TensorFlow in the past and are new to PyTorch hang in there everything is explained clearly and concisely. png Next after loading the data we need to transform the pixel values of each image 0 255 to 0 1 as neural networks works quite good with normalized data. It also helps in loading custom datasets. The model is able to predict every image in test set perfectly without any errors References CIFAR10 ResNet Implementation https jovian. In fit_one_cycle we have use some techniques Learning Rate Scheduling Instead of using a fixed learning rate we will use a learning rate scheduler which will change the learning rate after every batch of training. The entire array of pixel values is converted to torch tensor https pytorch. We need to build a model which can classify between healthy and diseased crop leaves and also if the crop have any disease predict which disease is it. The original PlantVillage Dataset can be found here https github. datasets is a class which helps in loading all common and famous datasets. eval to set dropout and batch normalization layers to evaluation mode before running inference. Remember that you must call model. To seamlessly use a GPU if one is available we define a couple of helper functions get_default_device to_device and a helper class DeviceDataLoader to move our model data to the GPU as required Some helper functionsChecking the device we are working withWrap up our training and validation data loaders using DeviceDataLoader for automatically transferring batches of data to the GPU if available Building the model architecture We are going to use ResNet which have been one of the major breakthrough in computer vision since they were introduced in 2015. and then divided by 255. Weight Decay We also use weight decay which is a regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function. Do upvote if you find this useful. png Residual Block code implementation Then we define our ImageClassificationBase class whose functions are training_step To figure out how wrong the model is going after training or validation step. We got an accuracy of 99. Note The following cell may take 15 mins to 45 mins to run depending on your GPU. For instance let s say you have 1050 training samples and you want to set up a batch_size equal to 100. A new directory containing 33 test images is created later for prediction purpose. We can keep doing this procedure until we have propagated all samples through of the network. I have used subclass torchvision. com spMohanty PlantVillage Dataset. Since Kaggle provides a 2 core CPU I have set it to 2 Modelling It is advisable to use GPU instead of CPU when dealing with images dataset because CPUs are generalized for general purpose and GPUs are optimized for training deep learning models as they can process multiple computations simultaneously. com why data should be normalized before training a neural network c626b7f66c7d post. Doing so will eventually make our model more robust. The total dataset is divided into 80 20 ratio of training and validation set preserving the directory structure. org tutorials beginner examples_tensor two_layer_net_tensor. ImageFolder which helps in loading the image data when the data is arranged in this way root dog xxx. In kaggle P100 GPU it took around 20 mins of Wall Time. We can change it if result is not satisfactory. Image shape We can see the shape 3 256 256 of the image. num_workers denotes the number of processes that generate batches in parallel. PLANT DISEASE CLASSIFICATION USING RESNET 9 DISCLAIMER This notebook is beginner friendly so don t worry if you don t know much about CNNs and Pytorch. com the vanishing gradient problem 69bf08b15484 and allow us to train deep neural networks. The reason for this is because pickle does not save the model class itself. com understanding and visualizing resnets 442284831be8 text ResNet 20Layers layers 20remains 20the 20same 20 E2 80 94 204. Getting a nicely formatted summary of our model like in Keras. Importing necessary librariesLet s import required modulesWe would require torchsummary library to print the model s summary in keras style nicely formatted and pretty to look as Pytorch natively doesn t support that Exploring the data Loading the data The above cell extract the number of unique plants and number of unique diseasesSo we have images of leaves of 14 plants and while excluding healthy leaves we have 26 types of images that show a particular disease in a particular plant. Rather it saves a path to the file containing the class which is used during load time. Additionally computations in deep learning need to handle huge amounts of data this makes a GPU s memory bandwidth most suitable. Saving the model s state_dict with the torch. The disadvantage of this approach is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved. validation_step Because an accuracy metric can t be used while training the model doesn t mean it shouldn t be implemented Accuracy in this case would be measured by a threshold and counted if the difference between the model s prediction and the actual label is lower than that threshold. This dataset consists of about 87K rgb images of healthy and diseased crop leaves which is categorized into 38 different classes. If you want to learn more about ResNets read the following articles Understanding and Visualizing ResNets https towardsdatascience. save function will give you the most flexibility for restoring the model later which is why it is the recommended method for saving models. Note This description is given in the dataset itself Our goal Goal is clear and simple. You will get a good overview of how to use PyTorch for image classification problems. 9 chance of getting the right answer or you can say model randomly chooses a class. 3 is the number of channels RGB and 256 x 256 is the width and height of the image Some Images from training dataset batch_size is the total number of images given as input at once in forward propagation of the CNN. We also define an accuracy function which calculates the overall accuracy of the model on an entire batch of outputs so that we can use it as a metric in fit_one_cycle Defining the final architecture of our model Now we define a model object and transfer it into the device with which we are working. If you have more cores in your CPU you can set it to number of cores in your CPU. It helps in loading large and memory consuming datasets. We ll also record the learning rate used for each batch. DataLoader is a subclass which comes from torch. Pytorch doesn t support it natively. Let s check our validation loss and accuracySince there are randomly initialized weights that is why accuracy come to near 0. Visualizing the above information on a graphWe can see that the dataset is almost balanced for all classes so we are good to go forward Images available for training Data Preparation for training torchvision. It takes in batch_size which denotes the number of samples contained in each generated batch. com remotesensing remotesensing 11 01896 article_deploy html images remotesensing 11 01896 g001. A common PyTorch convention is to save models using either a. org Hope you all learned something from this kernel. They have a large number of cores which allows for better computation of multiple parallel processes. Overview of ResNet and its variants https towardsdatascience. Because of this your code can break in various ways when used in other projects or after refactors. Save Load state_dict Recommended When saving a model for inference it is only necessary to save the trained model s learned parameters. com an overview of resnet and its variants 5281e2f56035 Paper with code implementation https paperswithcode. This simple yet effective technique is called gradient clipping. Catch you guys on the next one Peace for working with files for numerical computationss for working with dataframes Pytorch module for plotting informations on graph and images using tensors for creating neural networks for dataloaders for checking images for functions for calculating loss for transforming images into tensors for data checking for working with classes and images for getting the summary of our model printing the disease names unique plants in the dataset number of unique plants number of unique diseases Number of images for each disease converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column plotting number of images available for each disease datasets for validation and training total number of classes in train set for checking some images from training dataset Setting the seed value setting the batch size DataLoaders for training and validation helper function to show a batch of training instances Images for first batch of training for moving data into GPU if available for moving data to device CPU or GPU for loading in the device GPU if available else CPU Moving data into GPU ReLU can be applied before or after adding the input for calculating the accuracy base class for the model Generate predictions Calculate loss Generate prediction Calculate loss Calculate accuracy Combine loss Combine accuracies Architecture for training convolution block with BatchNormalization resnet architecture out_dim 128 x 64 x 64 out_dim 256 x 16 x 16 out_dim 512 x 4 x 44 xb is the loaded batch defining the model and moving it to the GPU getting summary of the model for training scheduler for one cycle learniing rate Training gradient clipping recording and updating learning rates validation since images in test folder are in alphabetical order Convert to a batch of 1 Get predictions from model Pick index with highest probability Retrieve the class label predicting first image getting all predictions actual label vs predicted saving to the kaggle working directory saving the entire model to working directory. validation_epoch_end We want to track the validation losses accuracies and train losses after each epoch and every time we do so we have to make sure the gradient is not being tracked. Setting shuffle True shuffles the dataset. Let s start training our model. If you are not familiar why normalizing inputs help neural network read this https towardsdatascience. Basically batch size defines the number of samples that will be propagated through the network. Gradient Clipping Apart from the layer weights and outputs it also helpful to limit the values of gradients to a small range to prevent undesirable changes in parameters due to large gradient values. So we need to install the torchsummary library discussed earlier Training the model Before we train the model Let s define a utility functionan evaluate function which will perform the validation phase and a fit_one_cycle function which will perform the entire training process. There are many strategies for varying the learning rate during training and the one we ll use is called the One Cycle Learning Rate Policy which involves starting with a low learning rate gradually increasing it batch by batch to a high learning rate for about 30 of epochs then gradually decreasing it to a very low value for the remaining epochs. ", "id": "atharvaingle/plant-disease-classification-resnet-99-2", "size": "12582", "language": "python", "html_url": "https://www.kaggle.com/code/atharvaingle/plant-disease-classification-resnet-99-2", "git_url": "https://www.kaggle.com/code/atharvaingle/plant-disease-classification-resnet-99-2", "script": "torch.utils.data ImageFolder  # for working with classes and images __init__ ResNet9(ImageClassificationBase) validation_epoch_end accuracy DataLoader # for dataloaders Image           # for checking images summary              # for getting the summary of our model plot_accuracies fit_OneCycle forward torch.nn training_step numpy epoch_end plot_lrs evaluate PIL get_default_device matplotlib.pyplot torchvision.datasets pandas ImageClassificationBase(nn.Module) plot_losses __len__ torch.nn.functional show_image to_device SimpleResidualBlock(nn.Module) validation_step DeviceDataLoader() torchsummary predict_image __iter__ get_lr ConvBlock torchvision.utils show_batch make_grid       # for data checking torchvision.transforms ", "entities": "(('actual label', 'working directory'), 'catch') (('gradient', 'train epoch'), 'want') (('you', 'equal 100'), 'let') (('which', 'epoch'), 'want') (('Saving', 'pickle https docs'), 'save') (('Next it', 'network'), 'take') (('disease', 'crop healthy leaves'), 'need') (('Image We', 'image'), 'shape') (('we', 'training torchvision'), 'see') (('why it', 'saving recommended models'), 'give') (('which', 'torch'), 'be') (('20Tensor 20is 20PyTorch 20basically', '20for'), 'use') (('total dataset', 'directory structure'), 'divide') (('when data', 'way'), 'ImageFolder') (('that', 'particular plant'), 'require') (('difference', 'actual threshold'), 'validation_step') (('You', 'image classification problems'), 'get') (('load process', 'code'), 'Model') (('everything', 'PyTorch'), 'use') (('you', 'ResNets https towardsdatascience'), 'read') (('algorithm', 'network'), 'take') (('PyTorch common convention', 'a.'), 'be') (('pickle', 'model class'), 'be') (('org you', 'kernel'), 'Hope') (('memory bandwidth', 'data'), 'make') (('training_step how model', 'training'), 'implementation') (('why accuracy', '0'), 'be') (('entire array', 'torch tensor https pytorch'), 'convert') (('model', 'perfectly errors'), 'be') (('we', 'which'), 'define') (('neural network', 'https towardsdatascience'), 'help') (('it', 'only trained model'), 'recommend') (('which', 'common datasets'), 'be') (('PlantVillage original Dataset', 'https here github'), 'find') (('We', 'batch'), 'record') (('us', 'deep neural networks'), 'com') (('goal Goal', 'dataset'), 'note') (('training then loss', 'point'), 'resnet') (('which', 'training'), 'use') (('directory exact when model', 'specific classes'), 'be') (('you', 'CNNs'), 'classification') (('code', 'refactors'), 'break') (('following cell', 'GPU'), 'note') (('result', 'it'), 'change') (('training batch_size', 'CNN'), 'be') (('techniques', 'learning rate gradient scheduling clipping'), 'perform') (('that', 'cross_entropy https pytorch'), 'mean') (('it', 'Wall Time'), 'take') (('why data', 'network c626b7f66c7d neural post'), 'com') (('you', 'CPU'), 'set') (('dataset', 'original dataset'), 'description') (('that', 'network'), 'define') (('that', 'parallel'), 'denote') (('1 neural networks', 'quite normalized data'), 'png') (('Setting', 'dataset'), 'shuffle') (('which', 'load time'), 'save') (('It', 'custom also datasets'), 'help') (('It', 'consuming large datasets'), 'help') (('it', 'large gradient values'), 'clip') (('model', 'Pytorch'), 'have') (('Cycle Learning Rate One which', 'remaining epochs'), 'be') (('they', 'multiple computations'), 'provide') (('Failing', 'inference inconsistent results'), 'yield') (('new directory', 'prediction later purpose'), 'create') (('which', 'multiple parallel processes'), 'have') (('fit_one_cycle which', 'training entire process'), 'need') (('regularization which', 'loss function'), 'Decay') (('which', 'generated batch'), 'take') (('they', '2015'), 'define') (('batches', 'epochs'), 'be') (('model', 'randomly class'), 'chance') (('we', 'network'), 'keep') (('This', 'gradient problem https also vanishing towardsdatascience'), 'help') (('which', '38 different classes'), 'consist') ", "extra": "['disease', 'test', 'procedure']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "algorithm", "answer", "approach", "architecture", "array", "augmentation", "batch", "batch_size", "block", "build", "call", "case", "cell", "check", "checking", "cifar10", "classification", "classify", "clear", "code", "column", "computation", "computer", "convolution", "core", "cost", "crop", "custom", "cycle", "data", "dataframe", "dataset", "decay", "define", "description", "device", "dictionary", "difference", "directly", "directory", "disease", "epoch", "equal", "eval", "evaluate", "evaluation", "every", "everything", "explained", "extract", "figure", "file", "final", "find", "fitting", "fixed", "flexibility", "folder", "following", "forward", "found", "function", "general", "generate", "generated", "gradient", "graph", "handle", "height", "help", "helper", "high", "image", "implementation", "import", "improve", "index", "inference", "input", "instance", "itself", "kaggle", "label", "layer", "learn", "learning", "least", "let", "library", "load", "look", "lower", "major", "mean", "memory", "method", "metric", "mode", "model", "module", "most", "move", "multiple", "name", "near", "need", "network", "neural", "new", "next", "normalization", "normalized", "not", "notebook", "number", "numerical", "object", "offline", "order", "out", "overall", "overview", "parallel", "past", "path", "perform", "pickle", "pixel", "plotting", "png", "point", "predict", "prediction", "prevent", "print", "printing", "probability", "problem", "procedure", "propagation", "purpose", "range", "ratio", "read", "reason", "record", "recording", "regularization", "residual", "result", "rgb", "right", "run", "running", "save", "saving", "scheduler", "second", "set", "several", "shape", "shuffle", "situation", "size", "something", "start", "structure", "style", "summary", "support", "technique", "tensor", "term", "test", "text", "threshold", "through", "time", "torch", "torchsummary", "total", "track", "train", "training", "transfer", "transform", "understanding", "unique", "until", "up", "validation", "value", "vision", "weight", "while", "width"], "potential_description_queries_len": 197, "potential_script_queries": ["checking", "evaluate", "model", "nn", "numpy", "torch"], "potential_script_queries_len": 6, "potential_entities_queries": ["gradient", "normalized", "problem"], "potential_entities_queries_len": 3, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 199}