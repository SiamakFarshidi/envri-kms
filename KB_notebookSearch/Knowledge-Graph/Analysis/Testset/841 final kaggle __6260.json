{"name": "841 final kaggle ", "full_name": " h2 Packages Helper Functions h2 File Path Operations if using images without Stain norm h2 Stain Normalization done pre run h2 Data Loaders Sampling Normalization Setup h4 torch datasets and dataloaders h4 Balancing strategies using samplers h4 Visualization of patch distribution per patient done pre run h4 Find means and sds for all channels done pre run h2 Patch Classifier Selection Training h4 DenseNet121 ResNet18 VGG11 h4 Optimizer Loss function h4 Training with early stopping learning rate scheduler h2 Patch level Inference Postprocessing h3 If only postprocessing Download checkpoints of all runs h3 Get patch level prediction performance feature vectors for image level test h3 Get feature vectors labels for image level training h2 Image Classifier with Max Vote SVM XGB h3 Max Voting h3 SVM votes h3 XGB votes h3 SVM features h3 XGB features h2 References ", "stargazers_count": 0, "forks_count": 0, "description": "Sun Deep Residual Learning for Image Recognition 2015. XGB votes Runs XGBoost on the votes Original paper of XGBoost 7 SVM features Runs SVM on the feature vectors XGB features Runs XGBoost on the feature vectors We use a random search package optuna 8 and stratified k fold to tune hyperparameters for the XGBoost classifier. Visualization of patch distribution per patient done pre run Here we plot bar graphs of the number of patches per patient. DenseNet121 ResNet18 VGG11Original paper of DenseNets 3 The selection of layers to freeze is based on 4 Original paper of ResNets 5 Original paper of VGG networks 6 Optimizer Loss function Training with early stopping learning rate scheduler Patch level Inference PostprocessingAfter training the patch classifier we d like to extract info from the well trained supposedly patch classifier to train a patient or image level classifier. com content_cvpr_2017 papers Huang_Densely_Connected_Convolutional_CVPR_2017_paper. Thomas A Method for Normalizing Histology Slides for Quantitative Analysis University of North Carolina Chapel Hill 2009. Original paper 1 http wwwx. Zisserman Very Deep Convolutional Networks for Large Scale Image Recognition 2015. Search spaces are intuitively determined. Accessed 21 Dec 2021 This cell imports helper functions from the linked dataset histopathsn custom pytorch dataset class reset the layers in the network add a patient column for the metadata split train and validation by creating two subset copies of the annotation csvs Max 512 add simple augmentations to the training set for all datasets perform resize and normalization with means stds pre computed means and stds differ across stain normalized patches and normal patches get the data loaders if balancing strategy class Downsample majority class and oversample minority class if balancing strategy patient Downsample majority patients and oversample minority patients specify model_name choose among ResNet VGG or DenseNet use gpu if available freeze up lower level layers freeze up lower level layers freeze up lower level layers create an optimizer object mean squared error loss get the val accu history for plotting validate at the end of epoch update the early stopper and record val accuracy print end of epoch results if early stopper says stop then stop update scheduler register the final layer before classifier so that the model outputs that layer as well during loader iterations The outputs from this layer would serve as the feature vector for the non neural model to learn predict get the best model of all training epochs prepare patch level statistics for test for the non neural model prepare patch level statistics for training for the non neural model. Accessed 21 Dec 2021 4 A. To rerun the three cells below simply choose one balancing strategy and create train_loader in the cell above. 1 Say a patient has m patches then the patch classifier would output m labels that we call votes each label could be 0 1 or 2. So for each patient we count the three way votes and divide them by the sum of votes. Both are tree level bootstrapping hyperparameters. pdf Data Loaders Sampling Normalization Setup torch datasets and dataloaders Balancing strategies using samplersTo balance with respect to target labels we adopt an external script for the sampler written by 2. Weinberger Densely Connected Convolutional Networks 2016. Accessed 21 Dec 2021 7 T. org kdd2016 papers files rfp0697 chenAemb. edu mn sites default files macenko2009. Packages Helper Functions File Path Operations if using images without Stain norm Stain Normalization done pre run Should be conducted with both training test sets. Tizhoosh Fine Tuning and Training of DenseNet for Histopathology Image Representation Using TCGA Diagnostic Slides 2021. Accessed 21 Dec 2021 6 K. Find means and sds for all channels done pre run See the torchvision. However with experiments we discover that running optuna on Colab and Kaggle kernels might generate different results. Accessed 21 Dec 2021 2 M. Now for a patient with m patches they would have three numbers as the input into the patient classifier m_1 m m_2 m m_3 m with m_i being the vote count for the i th label. Accessed 21 Dec 2021 5 K. The x axis represents different patients and the y axis is the number of patches. Available https www. Yang Imbalanced Dataset Sampler GitHub 2021. Finally we tune alpha which is the L1 regularization factor. After that we reduce the learning rate and increase the number of trees to get the final XGBoost classifier on the feature vectors. van der Maaten and K. So we then compute the element wise average of the feature vectors so that regardless of how many patches a patient has their input for the patient classifier would always be of size L. the patch classifier. Normalize functions in the data loader cell for the outputs Patch Classifier Selection TrainingChoose one of the following models as the patch level classifier. Available http wwwx. Guestrin XGBoost A Scalable Tree Boosting System 2016. org openaccess content_cvpr_2016 papers He_Deep_Residual_Learning_CVPR_2016_paper. If only postprocessing Download checkpoints of all runs Get patch level prediction performance feature vectors for image level test Get feature vectors labels for image level training Image Classifier with Max Vote SVM XGB Max VotingReturns the label that has the maximum votes in each patient SVM votes Runs SVM on the votes. com ufoym imbalanced dataset sampler. 2 Another method is to extract a feature vector for each patch using the final layer of the convolutional network right before the fully connected layers. Available https openaccess. For both the training and test sets we perform the same postprocesses and only pass the results from the training side to the patient classifier. Accessed 17 Dec 2021 3 G. Initially fix some main hyperparameters tune max_depth and min_child_weight firstThen we tune the gamma minimum loss reduction required to make a split Then we tune subsample proportion of dataset in the bootstrapped data and colsample_bytree proportion of features in the bootstrapped data. For a patient with m patches they will have m feature vectors with length L dependent on the CNN model i. And to balance with respect to patients we implement our own sampler by modifying the script by 2. Koyama Optuna A Next generation Hyperparameter Optimization Framework 2019. There are two approaches to extract such info. Accessed 21 Dec 2021 8 T. Available https arxiv. Available https github. ", "id": "jaredfeng/841-final-kaggle", "size": "6260", "language": "python", "html_url": "https://www.kaggle.com/code/jaredfeng/841-final-kaggle", "git_url": "https://www.kaggle.com/code/jaredfeng/841-final-kaggle", "script": "torch.utils.data sklearn.metrics torch optim read_image pyplot as plt hook DataLoader get_activation *  # custom pytorch dataset class BalancingPatientPatchSampler torchsampler pyplot numpy seaborn CustomSampler XGBClassifier torchvision time ImageData tune4 torchvision.io nn confusion_matrix EarlyStopper sklearn.model_selection tune3 summary pandas Dataset * accuracy_score ImbalancedDatasetSampler tune2 transforms objective SVC torchsummary matplotlib StratifiedKFold sklearn.svm roc_auc_score reset_parameters xgboost tune1 torch.backends.cudnn ", "entities": "(('we', '2'), 'dataset') (('input', 'always size'), 'compute') (('Stain norm Stain Normalization', 'training test pre sets'), 'conduct') (('which', 'alpha'), 'tune') (('patch level statistics', 'non neural model'), 'access') (('we', 'feature vectors'), 'reduce') (('Both', 'tree hyperparameters'), 'be') (('we', 'votes'), 'count') (('we', 'patient classifier'), 'perform') (('method', 'right fully connected layers'), '2') (('Tizhoosh Fine Tuning', 'TCGA Diagnostic Slides'), 'use') (('label', 'votes'), 'say') (('Find', 'torchvision'), 'do') (('learning rate scheduler Patch level Inference early stopping PostprocessingAfter', 'level patient classifier'), 'paper') (('We', 'XGBoost classifier'), 'Runs') (('org kdd2016 papers', 'rfp0697 chenAemb'), 'file') (('they', 'CNN dependent model'), 'have') (('Here we', 'patient'), 'do') (('running optuna', 'Kaggle different results'), 'discover') (('x axis', 'y patches'), 'represent') (('input', 'label'), 'have') (('that', 'votes'), 'get') (('Then we', 'bootstrapped data'), 'fix') (('we', '2'), 'balance') ", "extra": "['annotation', 'patient', 'test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "annotation", "average", "balance", "best", "call", "cell", "choose", "classifier", "column", "compute", "convolutional", "could", "count", "create", "custom", "data", "dataset", "default", "dependent", "distribution", "end", "epoch", "error", "external", "extract", "feature", "final", "fix", "fold", "following", "function", "gamma", "generate", "generation", "gpu", "helper", "history", "http", "https arxiv", "https github", "image", "implement", "increase", "info", "input", "label", "layer", "learn", "learning", "length", "level", "loader", "lower", "main", "majority", "max_depth", "maximum", "mean", "metadata", "method", "might", "minimum", "model", "network", "neural", "non", "norm", "normal", "normalization", "normalized", "number", "object", "optimizer", "optuna", "output", "package", "patch", "patient", "pdf", "per", "perform", "performance", "plot", "plotting", "pre", "predict", "prediction", "prepare", "print", "pytorch", "random", "record", "reduce", "register", "regularization", "reset", "resize", "right", "run", "running", "sampler", "scheduler", "script", "search", "selection", "set", "side", "size", "split", "squared", "stain", "strategy", "subset", "sum", "target", "test", "torch", "train", "training", "tree", "tune", "up", "update", "val", "validate", "validation", "vector", "vote", "wise"], "potential_description_queries_len": 130, "potential_script_queries": ["cudnn", "dataset", "hook", "matplotlib", "nn", "numpy", "objective", "optim", "plt", "pyplot", "seaborn", "summary", "time", "torchsummary", "torchvision", "xgboost"], "potential_script_queries_len": 16, "potential_entities_queries": ["level", "norm", "pre", "test"], "potential_entities_queries_len": 4, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 143}