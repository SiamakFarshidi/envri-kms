{"name": "automated essay grading using nlp part1 ", "full_name": " h3 The Automated Student Assessment Prize s ASAP Dataset sponsored by Hewitt Packard comprised of 13 000 essays 8 different datasets of different genre Each dataset was a collection of responses to its own prompt Each essay set has it s own rubric marking scheme to decide the final score of the essay Each essay has one or more human scores and a final resolved score h3 Part 1 Text Preprocessing on the Data h1 Preprocessing the Data h4 A Regular Expression is a text string that describes a search pattern which can be used to match or replace patterns inside a string with a minimal amount of code h3 Currently I am learning NLP by practicing it on various datasets This is all I have done by far in text preprocessing If there are certain things that should be changed updated or I have missed out please comment and let me know Also If there could be some inputs on Semantic Analysis like what should be the approach and algorithms that could be used for Semantic Analysis please share h3 Thankyou ", "stargazers_count": 0, "forks_count": 0, "description": "Each dataset was a collection of responses to its own prompt. Hence count of lemmas is less than count of noun adj verb and adverb. The Automated Student Assessment Prize s ASAP Dataset sponsored by Hewitt Packard comprised of 13 000 essays 8 different datasets of different genre. Lemmatization using POS_Tagging5. Tokenization Word Tokenize and Sentence Tokenize 3. This is all I have done by far in text preprocessing If there are certain things that should be changed updated or I have missed out please comment and let me know. create a list of tuples Only for essay_set 1. Cleaning the text using regex function removing url remove numbers and lowercase the text Eg caps1 will be removed remove punctuation After cleaning the data Here we are using ascii encoding on the string ignoring the ones that can t be converted and then again decoding it. Orthography Spelling Mistakes and Punctuation A Regular Expression is a text string that describes a search pattern which can be used to match or replace patterns inside a string with a minimal amount of code. Also If there could be some inputs on Semantic Analysis like what should be the approach and algorithms that could be used for Semantic Analysis please share. Importing Libraries Importing Data Finding the number of records for each column for each of the eight essay sets to know that data is consistent. Calculating number of nouns adjectives verbs and adverbs in an essay this will give the real count. Cleaning the Data using regex function2. Each essay set has it s own rubric marking scheme to decide the final score of the essay. Each essay has one or more human scores and a final resolved score Part 1 Text Preprocessing on the Data Preprocessing the Data1. Numeric Features like word_count char_count sentence_count etc. Tokenizing the sentences to words For Splitting sentences in the paragraph using PunktSentenceTokenizer Data After tokenizing Data after removing stopwords After removing the words having length 1 calculating number of words in an essay equivalent to a zA Z0 9 Calculating the number of characters matches a single whitespace character space newline return tab form Number of sentences using sent_tokenize to convert paragraph into sentences Average word length Lemmatization using POS Tagging Here we are appending all the POS tags in the lemma create list of tuples Lemmatizing the wprds We only need POS_tags not the word the words that are neither of above are by default tagged as NOUN. Currently I am learning NLP by practicing it on various datasets. ", "id": "sakshisaku3000/automated-essay-grading-using-nlp-part1", "size": "14", "language": "python", "html_url": "https://www.kaggle.com/code/sakshisaku3000/automated-essay-grading-using-nlp-part1", "git_url": "https://www.kaggle.com/code/sakshisaku3000/automated-essay-grading-using-nlp-part1", "script": "clean_length convert_essay_to_wordlist stopwords WordNetLemmatizer wordnet tokenize_essay seaborn numpy count_pos SpellChecker spell_count remove_stopwords pandas word_tokenize nltk.stem extract_features pos_tag count_lemmas decode_essay nltk.tokenize nltk.corpus char_count average_word_length matplotlib process_text word_count spellchecker sent_count nltk ", "entities": "(('which', 'code'), 'be') (('that', 'Semantic Analysis'), 'share') (('dataset', 'own prompt'), 'be') (('ASAP Dataset', 'different genre'), 's') (('data', 'essay eight sets'), 'import') (('Hence count', 'noun adj verb'), 'be') (('t', 'then again it'), 'clean') (('it', 'essay'), 'have') (('that', 'NOUN'), 'tokenize') (('Text final resolved Part 1 Preprocessing', 'Data1'), 'have') (('Currently I', 'various datasets'), 'learn') (('me', 'by far text'), 'be') (('this', 'real count'), 'adjective') ", "extra": "", "label": "No_extra_files", "potential_description_queries": ["approach", "ascii", "character", "cleaning", "collection", "column", "comment", "convert", "could", "count", "create", "data", "dataset", "default", "encoding", "essay", "final", "form", "function", "human", "learning", "length", "let", "list", "match", "need", "newline", "not", "number", "out", "pattern", "preprocessing", "punctuation", "remove", "replace", "return", "score", "search", "set", "single", "space", "string", "text", "url", "word"], "potential_description_queries_len": 45, "potential_script_queries": ["matplotlib", "nltk", "numpy", "seaborn", "wordnet"], "potential_script_queries_len": 5, "potential_entities_queries": [], "potential_entities_queries_len": 0, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 50}