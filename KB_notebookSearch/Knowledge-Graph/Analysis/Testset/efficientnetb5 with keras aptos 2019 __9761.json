{"name": "efficientnetb5 with keras aptos 2019 ", "full_name": " h1 Implementation of EfficientNetB5 for the APTOS 2019 competition with Keras h2 Table Of Contents h2 Dependencies h2 Preparation h2 Metric Quadratic Weighted Kappa h2 EDA Exploratory Data Analysis h2 Preprocessing h2 Modeling EfficientNetB5 h2 Evaluation h2 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "For these images there some to be increasingly more spots and stains on the retina as diabetic retinopathy worsens. We also stop training if the MSE on the validation set doesn t go down for 4 epochs. This is the mean of squared differences between our predictions and labels as showed in the formula below. EfficientNet was released this June 2019 by Google AI and is the new state of the art on ImageNet. This kernel provides weights for EfficientNetB0 through B5. After creating the submission I always check the format and the distribution of the test predictions. Submission Since the test set is not that large we will not be using a generator for making the final predictions on the test set. We train all layers in the network. org stable modules generated sklearn. com christofhenkel weighted kappa loss for keras tensorflow. Batch Normalization becomes unstable with small batch sizes 16 and that is why we use Group Normalization https arxiv. com benjaminwarner resized 2015 2019 blindness detection images 35000 additional images. An implementation of a QWK loss function for Tensorflow Keras can be found in this Kaggle kernel https www. Big thanks to Somshubra Majumdar https github. png Since we are not provided with that much data 3662 images we will augment the data to make the model more robust. This is the traditional transfer learning approach were we can optimize and model for almost any image content. pdf as it signifies a fundamental shift in how the Deep Learning community will approach model scaling Also check out this video on EfficientNet by Henry AI Labs https youtu. We could also try to use additional data from previous competitions to increase performance. Preprocessing Here we will use the auto cropping method with Ben s preprocessing as explained in this kernel https www. Note that the distribution on the test set can still vary wildly from the training data. com c aptos2019 blindness detection competition. Another option we could use is to directly use Quadratic Weighted Kappa as a loss function. org api rest_v1 media math render svg 2a496e1cef7d812b83bdbb725d291748cf0183f5 In this case we are going to optimize Mean Squared Error MSE See Modeling section since we are using regression and by optimizing MSE we are also optimizing QWK as long as we round predictions afterwards. Do they makes sense given the label distribution of the training data That s it Feel free to experiment with this kernel and try a larger implementation of EfficientNet. pdf for EfficientNetB5 is 456x456x3. If you like this Kaggle kernel feel free to give an upvote and leave a comment I will try to implement your suggestions in this kernel Repository source https github. Just loop through the layers and replace each Batch Normalization layer with a Group Normalization layer. Lastly we will divide the data by 128 for normalization. For a more detailed and practical explanation of QWK I highly recommend this Kaggle kernel https www. Also we will flip the data both horizontally and vertically. The metric that we try to optimize is the Mean Squared Error https en. We will rotate the data on any angle. Thanks to the amazing wrapper by qubvel https github. As mentioned earlier we use custom thresholds to optimize our score. Evaluation To evaluate our performance we predict values from the generator and round them of to the nearest integer to get valid predictions. com CyberZHG keras radam blob master keras_radam optimizers. After each epoch we save the model if it is better than the previous one according to the Quadratic Weighted Kappa score on the validation set. GPU limit for this competition is set to 9 hours. com ratthachat aptos updatedv14 preprocessing ben s cropping. com tensorflow tpu master models official efficientnet g3doc params. We can optimize the validation score by doing a Grid Search https scikit learn. We will optimize a pre trained EfficientNetB5 with a few added layers. The OptimizedRounder class by Abhishek Thakur https www. We specify the input shape and that we want the model without the top the final Dense layer. com abhishek optimizer for quadratic weighted kappa. Through examining these three aspects we can get a good sense of the problem. Keras makes it incredibly easy to replace layers. pdf since it often yields better convergence than Vanilla Adam. After that we calculate the Quadratic Weighted Kappa score on the training set and the validation set. html over rounding thresholds instead of doing normal rounding. py Apply constraints. Although I do not implement this in the kernel feel free to experiment with adding data. com qubvel efficientnet for sharing an amazing wrapper to get the EfficientNet architecture in one line of code Preparation By examining the data we can readily see that we do not have that much data 700 samples per class. Implementation of EfficientNetB5 for the APTOS 2019 competition with KerasIn this kernel we will implement EfficientNet for medical images APTOS 2019 https www. Load in EfficientNetB5 Replace all Batch Normalization layers by Group Normalization layers Initialize model For tracking Quadratic Weighted Kappa score Monitor MSE to avoid overfitting and save best model Begin training Visualize mse Load best weights according to MSE Calculate QWK on train set Calculate score Calculate QWK on validation set Calculate score Optimize on validation data and evaluate again Place holder for diagnosis column For preprocessing test images Make final predictions round predictions and save to csv Remove. In this way we are more flexible in our optimization and we can yield higher scores than solely optimizing for accuracy. EDA Exploratory Data Analysis For EDA on image datasets I think one should at least examine the label distribution the images before preprocessing and the images after preprocessing. We can see that EfficientNet achieves state of the art and uses a lot less parameters than most modern CNN architectures. The original class can be found in this Kaggle kernel https www. com qubvel efficientnet Standard dependencies Machine Learning Path specifications Specify title of our final model Set seed for reproducability For keeping time. This will increase performance when we train our EfficientNet model. com cimages multimages 16 4e7cf150 0179 4d89 86f2 5cbb1f51c266_meansquarederrorformula. Modeling EfficientNetB5 Since we want to optimize the Quadratic Weighted Kappa score we can formulate this challenge as a regression problem. By optimizing this metric we are also optimizing for Quadratic Weighted Kappa if we round the predictions afterwards. Source EfficientNet Paper https arxiv. com aroraaman quadratic kappa metric explained in 5 simple steps. org wiki Mean_squared_error. com abhishek is a great way to do this. We will visualize a random image from every label to get a general sense of the distinctive features that seperate the classes. com qubvel efficientnet we can load in a model like the Keras API. Note that we will use the RAdam optimizer https arxiv. The original image size from the EfficientNet paper https arxiv. After preprocessing we have managed to enhance the distinctive features in the images. com ratthachat efficientnet keras weights b0b5. com titu1994 for building an implementation of Group Normalization for Keras. It is probably a good idea to use data augmentation to increase robustness of our model See the modeling section. Weights for EfficientNetB6 and B7 can be found in Google AI s repository for EfficientNet https github. The same thresholds should be used when creating the final predictions. For this kernel we will use the B5 version but feel free to play with the larger models. We are however not bound by this and can use a smaller size if we want. Since the pre trained model was trained on ImageNet http www. Feel free to experiment with this. com c aptos2019 blindness detection overview evaluation The formula for weighted kappa is https wikimedia. I highly recommend you to read the EfficientNet paper https arxiv. png from ids Check submission Label distribution Distribution of predictions Check kernels run time. reshape inputs to new group shape prepare broadcast shape In this case we must explicitly broadcast all parameters. Additionally we are going to same the model which achieves the best QWK score on the validation data through a custom Keras Callback. png Table Of Contents Dependencies 1 Preparation 2 Metric Quadratic Weighted Kappa 3 EDA Exploratory Data Analysis 4 Preprocessing 5 Modeling EfficientNetB5 6 Evaluation 7 Submission 8 Dependencies Special thanks to qubvel https github. Additional data can be found in this Kaggle dataset https www. It introduces a systematic way to scale CNN Convolutional Neural Networks in a nearly optimal way. We will take this into account and try to enhance these features in our preprocessing. com tensorflow tpu tree master models official efficientnet. The original image sizes used for every version of EfficientNet are EfficientNetB0 224 224 3 EfficientNetB1 240 240 3 EfficientNetB2 260 260 3 EfficientNetB3 300 300 3 EfficientNetB4 380 380 3 EfficientNetB5 456 456 3 EfficientNetB6 528 528 3 EfficientNetB7 600 600 3 Metric Quadratic Weighted Kappa The metric that is used for this competition is Quadratic Weighted Kappa QWK Kaggle s Explanation https www. Prepare broadcasting shape. Then we load in the weights which are provided in this Kaggle dataset https www. Thanks to CyberZHG who implemented RAdam for Keras https github. This way we can counter overfitting. org and not on medical images there are some limitations to this approach for out challenge. File sizes and specifications Add extension to id_code Add extension to id_code Specify image size Flatten list of numpy arrays Get predictions and convert to integers We can use sklearns implementation of QWK straight out of the box as long as we specify weights as quadratic Label distribution Example from every label If for some reason we only have two channels If we have a normal RGB images image is too dark so that we crop out everything return original image Example of preprocessed images from every label Labels for training data We use a small batch size so we can handle large images easily Add Image augmentation to our generator Use the dataframe to define train and validation generators Code Source https github. be 3svIm5UC94I for a clear explanation If you like this Kaggle kernel feel free to give an upvote and leave a comment Image an overview of model architectures and their performance on ImageNet http www. ", "id": "carlolepelaars/efficientnetb5-with-keras-aptos-2019", "size": "9761", "language": "python", "html_url": "https://www.kaggle.com/code/carlolepelaars/efficientnetb5-with-keras-aptos-2019", "git_url": "https://www.kaggle.com/code/carlolepelaars/efficientnetb5-with-keras-aptos-2019", "script": "Flatten sklearn.metrics get_custom_objects keras.preprocessing.image __init__ keras.layers keras.callbacks cohen_kappa_score _kappa_loss elu GlobalAveragePooling2D predict random build_model EarlyStopping Dropout Sequential get_preds_and_labels compute_output_shape InputSpec Adam on_epoch_end Conv2D Layer preprocess_image build numpy OptimizedRounder(object) ReduceLROnPlateau initializers PIL get_config get_updates constraints ImageDataGenerator RAdam(keras.optimizers.Optimizer) functools keras.utils.generic_utils Image matplotlib.pyplot coefficients Dense tensorflow call pandas keras.optimizers partial crop_image_from_gray fit tqdm keras.engine Callback scipy Metrics(Callback) regularizers on_train_begin EfficientNetB5 backend backend as K keras keras.models keras.activations GroupNormalization(Layer) efficientnet ", "entities": "(('It', 'modeling section'), 'be') (('GPU limit', '9 hours'), 'set') (('which', 'Keras custom Callback'), 'go') (('CyberZHG keras', 'blob master keras_radam optimizers'), 'com') (('we', 'RAdam optimizer https arxiv'), 'note') (('we', 'that'), 'be') (('kappa loss', 'keras'), 'weight') (('we', 'problem'), 'get') (('we', 'generators Code Source https train github'), 'add') (('original class', 'Kaggle kernel https www'), 'find') (('we', 'images'), 'manage') (('Also we', 'data'), 'flip') (('com qubvel efficientnet Standard dependencies', 'time'), 'specification') (('I', 'test predictions'), 'check') (('we', 'larger models'), 'use') (('model', 'data'), 'augment') (('formula', 'weighted kappa'), 'blindness') (('EfficientNet', 'ImageNet'), 'release') (('we', 'solely accuracy'), 'be') (('org', 'out challenge'), 'be') (('We', 'performance'), 'try') (('that we', 'test set'), 'be') (('that', 'competition'), 'be') (('we', 'smaller size'), 'bind') (('it', 'Vanilla Adam'), 'pdf') (('I', 'EfficientNet paper https arxiv'), 'recommend') (('Additional data', 'Kaggle dataset https www'), 'find') (('same thresholds', 'when final predictions'), 'use') (('we', 'valid predictions'), 'predict') (('it', 'incredibly layers'), 'make') (('who', 'Keras https github'), 'thank') (('we', 'regression problem'), 'model') (('png', 'time'), 'run') (('you', 'www'), 'be') (('EfficientNet', 'CNN most modern architectures'), 'see') (('I', 'kernel Repository source https github'), 'feel') (('we', 'loss function'), 'be') (('there some', 'diabetic retinopathy worsens'), 'be') (('com qubvel we', 'Keras API'), 'efficientnet') (('distribution', 'training still wildly data'), 'note') (('I', 'data'), 'feel') (('it', 'validation set'), 'save') (('doesn', '4 epochs'), 'stop') (('We', 'validation score'), 'optimize') (('final predictions', 'Remove'), 'replace') (('we', 'predictions'), 'optimize') (('Ben', 'kernel https www'), 'use') (('com ratthachat aptos', 'updatedv14 cropping'), 'preprocesse') (('com aroraaman kappa quadratic metric', '5 simple steps'), 'explain') (('validation', 'training set'), 'calculate') (('com c aptos2019', 'detection competition'), 'blindness') (('kernel we', 'https 2019 www'), 'implement') (('that', 'classes'), 'visualize') (('Deep Learning how community', 'Henry AI Labs https youtu'), 'pdf') (('we', 'explicitly parameters'), 'prepare') (('kernel', 'B5'), 'provide') (('Lastly we', 'normalization'), 'divide') (('implementation', 'Kaggle kernel https www'), 'find') (('as long we', 'predictions'), 'render') (('when we', 'EfficientNet model'), 'increase') (('we', 'top'), 'specify') (('We', 'few added layers'), 'optimize') (('com benjaminwarner', '35000 additional images'), 'resize') (('I', 'Kaggle kernel https highly www'), 'recommend') (('This', 'formula'), 'be') (('com cimages', '16'), 'multimage') (('readily we', 'class'), 'efficientnet') (('Weights', 'EfficientNet https github'), 'find') (('We', 'preprocessing'), 'take') (('earlier we', 'score'), 'use') (('which', 'Kaggle dataset https www'), 'load') (('it', 'EfficientNet'), 'make') (('learning traditional transfer we', 'image almost any content'), 'be') (('It', 'nearly optimal way'), 'introduce') (('pre trained model', 'www'), 'http') (('why we', 'Group Normalization https arxiv'), 'become') (('one', 'images'), 'Analysis') ", "extra": "['test', 'diagnosis']", "label": "Perfect_files", "potential_description_queries": ["account", "api", "approach", "architecture", "art", "augment", "augmentation", "auto", "batch", "best", "blob", "box", "broadcast", "calculate", "case", "challenge", "check", "clear", "code", "column", "comment", "community", "competition", "convergence", "convert", "could", "crop", "csv", "custom", "data", "dataframe", "dataset", "define", "detection", "diagnosis", "directly", "distribution", "efficientnet", "enhance", "epoch", "evaluate", "evaluation", "every", "everything", "experiment", "explained", "extension", "final", "flip", "format", "formula", "found", "function", "general", "generated", "generator", "group", "handle", "http", "idea", "image", "implement", "implementation", "increase", "input", "integer", "kappa", "kernel", "label", "layer", "learning", "least", "leave", "line", "list", "load", "loop", "lot", "math", "mean", "medical", "method", "metric", "model", "most", "nearest", "new", "normal", "not", "numpy", "official", "optimization", "optimize", "optimizer", "option", "out", "overfitting", "overview", "pdf", "per", "performance", "png", "pre", "predict", "prepare", "preprocessing", "py", "quadratic", "random", "read", "reason", "recommend", "regression", "render", "replace", "repository", "reshape", "retina", "return", "rotate", "rounding", "run", "save", "scale", "scaling", "scikit", "score", "section", "sense", "set", "shape", "shift", "size", "source", "squared", "state", "submission", "tensorflow", "test", "think", "through", "title", "train", "training", "transfer", "tree", "try", "valid", "validation", "version", "video", "visualize", "who", "wrapper"], "potential_description_queries_len": 154, "potential_script_queries": ["backend", "build", "call", "fit", "partial", "preprocessing", "scipy", "tqdm"], "potential_script_queries_len": 8, "potential_entities_queries": ["dataset", "final", "kernel", "optimizer", "source", "transfer"], "potential_entities_queries_len": 6, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 157}