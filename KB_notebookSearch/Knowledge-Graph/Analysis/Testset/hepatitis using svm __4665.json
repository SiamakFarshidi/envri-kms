{"name": "hepatitis using svm ", "full_name": " h2 Dataset Reading and Pre Processing steps h4 4 Check the datatype of each variable h4 5 Drop columns which are not significant h4 6 Identify the Categorical Columns and store them in a variable cat cols and numerical into num cols h4 7 Checking the null values h4 8 Split the data into X and y h4 9 Split the data into X train X test y train y test with test size 0 20 using sklearn h4 10 Check null values in train and test check value counts in y train and y test h4 11 Impute the Categorical Columns with mode and Numerical columns with mean h4 Convert all the categorical columns to Integer Format before dummification 2 0 as 2 etc h4 12 Dummify the Categorical columns h4 13 Scale the numeric attributes age bili alk sgot albu protime h2 MODEL BUILDING SVM h4 Non Linear SVM RBF h3 SVM with Grid Search for Paramater Tuning ", "stargazers_count": 0, "forks_count": 0, "description": "xj If there is a function which could calculate the dot product and the result is the same as when we transform the data into higher dimension it would be fantastic. Check for value counts in target variable 4. When gamma is low the curve of the decision boundary is very low and thus the decision region is very broad. This function is called a kernel function. Identify the Categorical Columns and store them in a variable cat_cols and numerical into num_cols 7. fatique no yes 7. 20 using sklearn 10. It looks like not possible because the data is not linearly separable. When C is small the classifier is okay with misclassified data points high bias low variance. histology no yes16. liverFirm no yes 11. Split the data into X_train X_test y_train y_test with test_size 0. anorexia no yes 9. age 10 20 30 40 50 60 70 803. iloc 0 Impute on test df_cat_test df_cat_test. mean Combine numeric and categorical in train Combine numeric and categorical in test Train Test Train Test scale on train scale on test Create a SVC classifier using a linear kernel Train the classifier Get the best parameters. Impute the Categorical Columns with mode and Numerical columns with mean Convert all the categorical columns to Integer Format before dummification 2. To solve this problem we actually only care about the result of the dot product xi. In essence what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions. gender male 1 female 2 no 2 yes 1 4. sgot 13 100 200 300 400 500 19. liverBig no yes 10. Dummify the Categorical columns 13. C C is a parameter of the SVC learner and is the penalty for misclassifying a data point. Attribute information 1. spiders no yes13. Split the data into X and y 9. When C is large the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points low bias high variance. When gamma is high the curve of the decision boundary is high which creates islands of decision boundaries around data points. mean Impute on test df_num_test df_num_test. Check the datatype of each variable 5. iloc 0 Impute on train df_num_train df_num_train. Drop columns which are not significant 6. protime 10 20 30 40 50 60 70 80 90 NA s are represented with Dataset Reading and Pre Processing stepsimport required libraries 1. varices no yes15. Read the HEPATITIS dataset and check the data shapes 2. spleen no yes 12. alk 33 80 120 160 200 250 18. ascites no yes 14. Check null values in train and test check value_counts in y_train and y_test 11. The problem is if we have a large dataset containing say millions of examples the transformation will take a long time to run. However if we transform the two dimensional data to a higher dimension say three dimension or even ten dimension we would be able to find a hyperplane to separate the data. antivirals no yes 6. Kernel Trick Image you have a two dimensional non linearly separable dataset you would like to classify it using SVM. Checking the null values 8. malaise no yes 8. Gamma Gamma is a parameter of the RBF kernel and can be thought of as the spread of the kernel and therefore the decision region. SVM with Grid Search for Paramater Tuning Code to ignore warnings target 1 Die 2 Live null values in train null values in test Impute on train df_cat_train df_cat_train. steroid no yes 5. Check basic summary statistics of the data 3. Scale the numeric attributes age bili alk sgot albu protime MODEL BUILDING SVM Non Linear SVM RBF Radial Basis Function is a commonly used kernel in SVC where x i x2212 x j 2 is the squared Euclidean distance between two data points xi and xjIt is only important to know that an SVC classifier using an RBF kernel has two parameters gamma and C. target DIE 1 LIVE 2 2. ", "id": "mragpavank/hepatitis-using-svm", "size": "4665", "language": "python", "html_url": "https://www.kaggle.com/code/mragpavank/hepatitis-using-svm", "git_url": "https://www.kaggle.com/code/mragpavank/hepatitis-using-svm", "script": "sklearn.metrics sklearn.model_selection confusion_matrix f1_score GridSearchCV numpy SVC sklearn.preprocessing SimpleImputer pandas sklearn.impute sklearn.svm StandardScaler train_test_split accuracy_score ", "entities": "(('you', 'SVM'), 'image') (('we', 'dot product xi'), 'care') (('it', 'higher dimension'), 'xj') (('classifier', 'data misclassified points high bias low variance'), 'be') (('SVM', 'train'), 'target') (('Gamma Gamma', 'kernel'), 'be') (('C C', 'data point'), 'be') (('transformation', 'long time'), 'be') (('which', 'data points'), 'be') (('protime', 'Pre Processing required libraries'), 'represent') (('decision very thus region', 'decision boundary'), 'be') (('therefore bends', 'data backwards misclassified points low high variance'), 'penalize') (('classifier', 'best parameters'), 'mean') (('HEPATITIS', 'data shapes'), 'read') (('even ten we', 'data'), 'say') (('does', 'higher dimensions'), 'be') (('Check', 'target'), 'variable') (('SVC only classifier', 'parameters two gamma'), 'sgot') ", "extra": "['gender', 'test']", "label": "Perfect_files", "potential_description_queries": ["age", "basic", "best", "boundary", "calculate", "care", "categorical", "check", "classifier", "classify", "could", "curve", "data", "dataset", "decision", "dimension", "distance", "dot", "essence", "even", "find", "function", "gamma", "gender", "high", "hyperplane", "ignore", "kernel", "learner", "linear", "male", "mean", "mode", "no", "non", "not", "null", "numeric", "numerical", "parameter", "problem", "product", "region", "result", "scale", "separate", "sklearn", "spread", "squared", "store", "summary", "target", "test", "thought", "time", "train", "transform", "transformation", "value", "variable", "y_test"], "potential_description_queries_len": 61, "potential_script_queries": ["numpy"], "potential_script_queries_len": 1, "potential_entities_queries": ["product"], "potential_entities_queries_len": 1, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 62}