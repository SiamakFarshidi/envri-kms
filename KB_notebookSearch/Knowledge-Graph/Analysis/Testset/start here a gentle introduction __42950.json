{"name": "start here a gentle introduction ", "full_name": " h1 Introduction Home Credit Default Risk Competition h1 Data h2 Metric ROC AUC h2 Imports h2 Read in Data h1 Exploratory Data Analysis h2 Examine the Distribution of the Target Column h2 Examine Missing Values h2 Column Types h2 Encoding Categorical Variables h3 Label Encoding and One Hot Encoding h3 Aligning Training and Testing Data h2 Back to Exploratory Data Analysis h3 Anomalies h3 Correlations h3 Effect of Age on Repayment h3 Exterior Sources h2 Pairs Plot h1 Feature Engineering h2 Polynomial Features h2 Domain Knowledge Features h4 Visualize New Variables h1 Baseline h2 Logistic Regression Implementation h2 Improved Model Random Forest h3 Make Predictions using Engineered Features h4 Testing Domain Features h2 Model Interpretation Feature Importances h1 Conclusions h3 Follow up Notebooks h1 Just for Fun Light Gradient Boosting Machine ", "stargazers_count": 0, "forks_count": 0, "description": "Therefore we have to find a way to encode represent these variables as numbers before handing them off to the model. predict_proba remember that we want probabilities and not a 0 or 1. uk resources uploaded pearsons. This will get us slightly better results than the default LogisticRegression but it still will set a low bar for any future models. com willkoehrsen introduction to manual feature engineering Manual Feature Engineering Part Two https www. com williamkoehrsen and can be reached on Twitter at https twitter. org wiki Interaction_ statistics because they capture the interactions between variables. The only change we will make from the default model settings is to lower the regularization parameter http scikit learn. 754 when submitted to the public leaderboard indicating that the domain features do improve the performance Feature engineering https en. com getting_started install. html for those who want more information. These predictions will also be available when we run the entire notebook. This runs the entire notebook and then lets us download any files that are created during the run. For example we can create variables EXT_SOURCE_1 2 and EXT_SOURCE_2 2 and also variables such as EXT_SOURCE_1 x EXT_SOURCE_2 EXT_SOURCE_1 x EXT_SOURCE_2 2 EXT_SOURCE_1 2 x EXT_SOURCE_2 2 and so on. Jake VanderPlas writes about polynomial features in his excellent book Python for Data Science https jakevdp. Once we get into more sophisticated machine learning models we can weight the classes http xgboost. Let s look at this relationship in another way average failure to repay loans by age bracket. __ Make Predictions using Engineered FeaturesThe only way to see if the Polynomial Features and Domain knowledge improved the model is to train a test a model on these features We can then compare the submission performance to that for the model without these features to gauge the effect of our feature engineering. We will use LogisticRegression from Scikit Learn http scikit learn. org stable modules linear_model. The following code performs both of these preprocessing steps. com willkoehrsen automated feature engineering basics Advanced Automated Feature Engineering https www. The relationship is not very strong in fact they are all considered very weak http www. Both of these books present the theory and also the code needed to make the models in R and Python respectively. It generally starts out with a high level overview then narrows in to specific areas as we find intriguing areas of the data. org wiki Dimensionality_reduction to reduce the size of the datasets. Once the data exploration data preparation and feature engineering was complete we implemented a baseline model upon which we hope to improve. io en latest parameter. bureau data concerning client s previous credits from other financial institutions. In this frame of mind we can make a couple features that attempt to capture what we think may be important for telling whether a client will default on a loan. com willkoehrsen introduction to manual feature engineering p2 Introduction to Automated Feature Engineering https www. In this case it is a common classification metric known as the Receiver Operating Characteristic Area Under the Curve ROC AUC also sometimes called AUROC https stats. First we can show the correlations of the EXT_SOURCE features with the target and with each other. The threshold starts at 0 in the upper right to and goes to 1 in the lower left. A model with a high ROC AUC will also have a high accuracy but the ROC AUC is a better representation of model performance. com willkoehrsen automated model tuning Model Tuning Results https www. html logistic regression C which controls the amount of overfitting a lower value should decrease overfitting. Some general interpretations of the absolute value of the correlation coefficent http www. To visualize the effect of the age on the target we will next make a kernel density estimation plot https en. html sphx glr auto examples model selection plot underfitting overfitting py. When we measure a classifier according to the ROC AUC we do not generation 0 or 1 predictions but rather a probability between 0 and 1. com a complete machine learning walk through in python part one c62152f39420 1. With these projects it s best to build up an understanding of the problem a little at a time rather than diving all the way in and getting completely lost Metric ROC AUCOnce we have a grasp of the data reading through the column descriptions https www. A machine learning model unfortunately cannot deal with categorical variables except for some models such as LightGBM http lightgbm. Each previous application has one row and is identified by the feature SK_ID_PREV. Most of the categorical variables have a relatively small number of unique entries. When we do the align we must make sure to set axis 1 to align the dataframes based on the columns and not on the rows The training and testing datasets now have the same features which is required for machine learning. Aligning Training and Testing DataThere need to be the same features columns in both the training and testing data. Although if we want to have any hope of seriously competing we need to use all the data for now we will stick to one file which should be more manageable. com gentle introduction gradient boosting algorithm machine learning using the LightGBM library http lightgbm. If you don t understand this code that s all right Plotting in Python can be overly complex and for anything beyond the simplest graphs I usually find an existing implementation and adapt the code don t repeat yourself In this plot the red indicates loans that were not repaid and the blue are loans that are paid. com jsaguiar updated 0 792 lb lightgbm with simple features for exploring these features. except for TARGET because the correlation of a variable with itself is always 1 Looking at the documentation DAYS_BIRTH is the age in days of the client at the time of the loan in negative days for whatever reason. There does appear to be a moderate positive linear relationship between the EXT_SOURCE_1 and the DAYS_BIRTH or equivalently YEARS_BIRTH indicating that this feature may take into account the age of the client. The training data has 307511 observations each one a separate loan and 122 features variables including the TARGET the label we want to predict. We will create a dataframe in this format from the test set and the predictions called submit. com WillKoehrsen Machine Learning Projects master one_hot_encoding. com willkoehrsen introduction to feature selection Intro to Model Tuning Grid and Random Search https www. The predictions represent a probability between 0 and 1 that the loan will not be repaid. At some point we probably will want to try dimensionality reduction removing features that are not relevant https en. The findings may be interesting in their own right or they can be used to inform our modeling choices such as by helping us decide which features to use. As Andrew Ng is fond of saying applied machine learning is basically feature engineering. 678 when submitted to the competition exactly the same as that without the engineered features. com WillKoehrsen Machine Learning Projects master label_encoding. Let s make sure to create the new column and fill in the existing column with np. The correlation is positive but the value of this feature is actually negative meaning that as the client gets older they are less likely to default on their loan ie the target 0. To see these stats in years we can mutliple by 1 and divide by the number of days in a year Those ages look reasonable. In later work we will use models such as XGBoost that can handle missing values with no need for imputation https stats. pdf but these variables will still be useful for a machine learning model to predict whether or not an applicant will repay a loan on time. In future notebooks we will look at incorporating more sources of data building more complex models by following the code of others and improving our scores. Now that the model has been trained we can use it to make predictions. First we extract the target column from the training data because this is not in the testing data but we need to keep this information. We see that all four of our hand engineered features made it into the top 15 most important This should give us confidence that our domain knowledge was at least partially on track. com blog secret to data science success. They both teach with the mindset that the best way to learn is by doing and they are very effective To get a baseline we will use all of the features after encoding the categorical variables. After the run is complete this should take about 10 minutes you can then access the files that were created by going to the versions tab and then the output sub tab. One way to support anomalies quantitatively is by looking at the statistics of a column using the describe method. We will use the seaborn kdeplot for this graph. 735 on the leaderboard. In this notebook we will use Label Encoding for any categorical variables with only 2 categories and One Hot Encoding for any categorical variables with more than 2 categories. A curve that is to the left and above another curve indicates a better model. Testing Domain FeaturesNow we can test the domain features we made by hand. In the example above programmer recieves a 4 and data scientist a 1 but if we did the same process again the labels could be reversed or completely different. We can calculate the Pearson correlation coefficient between every variable and the target using the. There are a total of 9 files 1 main file for training with target 1 main file for testing without the target 1 example submission file and 6 other files containing additional information about each loan. https datascience. For example if I wanted to build a model that could detect terrorists with 99. 9999 accuracy I would simply make a model that predicted every single person was not a terrorist. __Follow up Notebooks__For those looking to keep working on this problem I have a series of follow up notebooks Manual Feature Engineering Part One https www. We will need to find a way to deal with these categorical variables Encoding Categorical VariablesBefore we go any further we need to deal with pesky categorical variables. The first column is the probability of the target being 0 and the second column is the probability of the target being 1 so for a single row the two columns must sum to 1. This will let us establish a baseline that we can then improve upon. The correlation coefficient is not the greatest method to represent relevance of a feature but it does give us an idea of possible relationships within the data. html The Gradient Boosting Machine is currently the leading model for learning on structured datasets especially on Kaggle and we will probably need some form of this model to do well in the competition. I m not sure what this exactly means but it may be a cumulative sort of credit rating made using numerous sources of data. com wp content uploads 2016 08 ROC curve. When it comes time to build our machine learning models we will have to fill in these missing values known as imputation. This allows the entire world to see your work Follow up NotebooksFor those looking to keep working on this problem I have a series of follow up notebooks Manual Feature Engineering Part One https www. installments_payment payment history for previous loans at Home Credit. Feature engineering refers to a geneal process and can involve both feature construction adding new features from the existing data and feature selection choosing only the most important features or other methods of dimensionality reduction. Interaction terms are commonly used in statistical models https www. org stable auto_examples model_selection plot_underfitting_overfitting. The Area Under the Curve AUC http gim. In this notebook we will stick to using only the main application training and testing data. org stable modules generated sklearn. object columns contain strings and are categorical features. These may be due to mis typed numbers errors in measuring equipment or they could be valid but extreme measurements. Exploratory Data AnalysisExploratory Data Analysis EDA is an open ended process where we calculate statistics and make figures to find trends anomalies patterns or relationships within the data. In this case since all the anomalies have the exact same value we want to fill them in with the same value in case all of these loans share something in common. I write for Towards Data Science at https medium. Nonetheless we can try out a few to see if they might help our model to predict whether or not a client will repay a loan. To access the submission at the end of the notebook we will hit the blue Commit Run button at the upper right of the kernel. io en latest Quick Start. The target 1 curve skews towards the younger end of the range. com questions 235489 xgboost can handle missing data in the forecasting phase. One hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. For all of these we will make the same KDE plot colored by the value of the TARGET. This is a great article on the subject https www. In other words while two variables by themselves may not have a strong influence on the target combining them together into a single interaction variable might show a relationship with the target. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. Examine Missing ValuesNext we can look at the number and percentage of missing values in each column. Every loan has its own row and is identified by the feature SK_ID_CURR. com jsaguiar updated 0 792 lb lightgbm with simple features by Aguiar CREDIT_INCOME_PERCENT the percentage of the credit amount relative to a client s income ANNUITY_INCOME_PERCENT the percentage of the loan annuity relative to a client s income CREDIT_TERM the length of the payment in months since the annuity is the monthly amount due DAYS_EMPLOYED_PERCENT the percentage of the days employed relative to the client s ageAgain thanks to Aguiar and his great script https www. In this notebook as mentioned previously we will stick to the main data sources and simple models which we can build upon in future work. com blog 2015 07 dimension reduction methods to reduce the number of dimensions while still trying to preserve information. From this information we see this is an _imbalanced class problem_ http www. Introduction Home Credit Default Risk CompetitionThis notebook is intended for those who are new to machine learning competitions or want a gentle introduction to the problem. Examine the Distribution of the Target ColumnThe target is what we are asked to predict either a 0 for the loan was repaid on time or a 1 indicating the client had payment difficulties. There are many techniques we can use to both create features and select features. org wiki Kernel_density_estimation KDE colored by the value of the target. According to the documentation these features represent a normalized score from external data source. credit_card_balance monthly data about previous credit cards clients have had with Home Credit. The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application. __ Improved Model Random ForestTo try and beat the poor performance of our baseline we can update the algorithm. Therefore we will keep all of the columns for now. png Moreover we are provided with the definitions of all the columns in HomeCredit_columns_description. This metric is between 0 and 1 with a better model scoring higher. htm explains itself by its name It is simply the area under the ROC curve. Here is a good Stack Overflow discussion https datascience. edu gareth ISL and Hands On Machine Learning with Scikit Learn and TensorFlow http shop. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns. org wiki Feature_engineering is going to be a critical part of this competition as it is for all machine learning problems numpy and pandas for data manipulation sklearn preprocessing for dealing with categorical variables File system manangement Suppress warnings matplotlib and seaborn for plotting List files available Training data Testing data features Function to calculate missing values by column Funct Total missing values Percentage of missing values Make a table with the results Rename the columns Sort the table by percentage of missing descending Print some summary information Return the dataframe with missing information Missing values statistics Number of each type of column Number of unique classes in each object column Create a label encoder object Iterate through the columns If 2 or fewer unique categories Train on the training data Transform both training and testing data Keep track of how many columns were label encoded one hot encoding of categorical variables Align the training and testing data keep only columns present in both dataframes Add the target back in Create an anomalous flag column Replace the anomalous values with nan Find correlations with the target and sort Display correlations Find the correlation of the positive days since birth and target Set the style of plots Plot the distribution of ages in years KDE plot of loans that were repaid on time KDE plot of loans which were not repaid on time Labeling of plot Age information into a separate dataframe Bin the age data Group by the bin and calculate averages Graph the age bins and the average of the target as a bar plot Plot labeling Extract the EXT_SOURCE variables and show correlations Heatmap of correlations iterate through the sources create a new subplot for each source plot repaid loans plot loans that were not repaid Label the plots Copy the data for plotting Add in the age of the client in years Drop na values and limit to first 100000 rows Function to calculate correlation coefficient between two columns Create the pairgrid object Upper is a scatter plot Diagonal is a histogram Bottom is density plot Make a new dataframe for polynomial features imputer for handling missing values Need to impute missing values Create the polynomial object with specified degree Train the polynomial features Transform the features Create a dataframe of the features Add in the target Find the correlations with the target Display most negative and most positive Put test features into dataframe Merge polynomial features into training dataframe Merge polnomial features into testing dataframe Align the dataframes Print out the new shapes iterate through the new features create a new subplot for each source plot repaid loans plot loans that were not repaid Label the plots Drop the target from the training data Feature names Copy of the testing data Median imputation of missing values Scale each feature to 0 1 Fit on the training data Transform both training and testing data Repeat with the scaler Make the model with the specified regularization parameter Train on the training data Make predictions Make sure to select the second column only Submission dataframe Save the submission to a csv file Make the random forest classifier Train on the training data Extract feature importances Make predictions on the test data Make a submission dataframe Save the submission dataframe Impute the polynomial features Scale the polynomial features Train on the training data Make predictions on the test data Make a submission dataframe Save the submission dataframe Impute the domainnomial features Scale the domainnomial features Train on the training data Extract feature importances Make predictions on the test data Make a submission dataframe Save the submission dataframe Sort features according to importance Normalize the feature importances to add up to one Make a horizontal bar chart of feature importances Need to reverse the index to plot most important on top Set the yticks and labels Plot labeling Show the feature importances for the default features Extract the ids Extract the labels for training Remove the ids and target One Hot Encoding Align the dataframes by the columns No categorical indices to record Integer label encoding Create a label encoder List for storing categorical indices Iterate through each column Map the categorical features to integers Record the categorical indices Catch error if label encoding scheme is not valid Extract feature names Convert to np arrays Create the kfold object Empty array for feature importances Empty array for test predictions Empty array for out of fold validation predictions Lists for recording validation and training scores Iterate through each fold Training data for the fold Validation data for the fold Create the model Train the model Record the best iteration Record the feature importances Make predictions Record the out of fold predictions Record the best score Clean up memory Make the submission dataframe Make the feature importance dataframe Overall validation score Add the overall scores to the metrics Needed for creating dataframe of validation scores Dataframe of validation scores Test the domain knolwedge features. nan and then create a new boolean column indicating whether or not the value was anomalous. This does not mean the bank should discriminate against younger clients but it would be smart to take precautionary measures to help younger clients pay on time. We will preprocess the data by filling in the missing values imputation and normalizing the range of the features feature scaling. We can clearly see that this feature has some relationship to the likelihood of an applicant to repay a loan. com visualizing data with pair plots in python f228cf529166 is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. fit and then we make predictions on the testing data using. Then we performed a fairly simple EDA to try and identify relationships trends or anomalies that may help our modeling. We can use a degree of 3 to see the results when we are creating polynomial features we want to avoid using too high of a degree both because the number of features scales exponentially with the degree and because we can run into problems with overfitting http scikit learn. Exterior SourcesThe 3 variables with the strongest negative correlations with the target are EXT_SOURCE_1 EXT_SOURCE_2 and EXT_SOURCE_3. com willkoehrsen tuning automated feature engineering exploratory Feature Selection https www. I think and this is just a personal opinion for categorical variables with many classes one hot encoding is the safest approach because it does not impose arbitrary values to categories. 07 correlation coefficient this variable is likely going to be useful in a machine learning model because it does affect the target. png A single line on the graph indicates the curve for a single model and movement along a line indicates changing the threshold used for classifying a positive instance. We may use these feature importances as a method of dimensionality reduction in future work. Read in Data First we can list all the available data files. This is the integral of the curve. There are 7 different sources of data application_train application_test the main training and testing data with information about each loan application at Home Credit. I hope this notebook was able to get you up and running in this machine learning competition and that you are now ready to go out on your own with help from the community and start working on some great problems __Running the notebook__ now that we are at the end of the notebook you can hit the blue Commit Run button to execute all the code at once. Let s use a slightly more sophisticated model for our actual baseline Logistic Regression. com kaggle media competitions home credit home_credit. From here the submission files can be submitted to the competition or downloaded. The only downside to one hot encoding is that the number of features dimensions of the data can explode with categorical variables with many categories. html to help us out. Since there are several models in this notebook there will be multiple output files. A model that simply guesses at random will have an ROC AUC of 0. html for our first model. The Random Forest is a much more powerful model especially when we use hundreds of trees. com willkoehrsen model tuning results random vs bayesian opt notebook I ll add more notebooks as I finish them Thanks for all the comments ImportsWe are using a typical data science stack numpy pandas sklearn matplotlib. To get the names we have to use the polynomial features get_feature_names method. EXT_SOURCE_3 displays the greatest difference between the values of the target. The only way to tell for sure is to try them out BaselineFor a naive baseline we could guess the same value for all examples on the testing set. Back to Exploratory Data Analysis AnomaliesOne problem we always want to be on the lookout for when doing EDA is anomalies within the data. From experience this will definitely help our model Model Interpretation Feature ImportancesAs a simple method to see which variables are the most relevant we can look at the feature importances of the random forest. The submission has now been saved to the virtual environment in which our notebook is running. Therefore when we perform label encoding the model might use the relative value of the feature for example programmer 4 and data scientist 1 to assign weights which is not what we want. There are far more loans that were repaid on time than loans that were not repaid. Several of the new variables have a greater in terms of absolute magnitude correlation with the target than the original features. This may be confusing because we usually like to think in terms of accuracy but when we get into problems with inbalanced classes we will see this is the case accuracy is not the best metric. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle histograms on the diagonal and 2D kernel density plots and correlation coefficients on the lower triangle. aspx a service dedicated to provided lines of credit loans to the unbanked population. com c home credit default risk data helps immensely we need to understand the metric by which our submission is judged. We can see the different relationships within the data. com 2017 01 23 a kaggle master explains gradient boosting. int64 and float64 are numeric variables which can be either discrete or continuous https stats. bureau_balance monthly data about the previous credits in bureau. First we can make a histogram of the age. This diagram shows how all of the data is related image https storage. nz cosc453 student_tutorials principal_components. The test set is considerably smaller and lacks a TARGET column. In later notebooks we will do more feature engineering https docs. I purposely avoid jumping into complicated models or joining together lots of data in order to show the basics of how to get started in machine learning Any comments or suggestions are much appreciated. This creates a considerable number of new features. com questions 132777 what does auc stand for and what is it. com en us minitab express 1 help and how to modeling statistics regression supporting topics basics what are categorical discrete and continuous variables. image https raw. We first made sure to understand the data our task and the metric by which our submissions will be judged. See you in the next notebook This submission should score about 0. com product 0636920052289. For label encoding we use the Scikit Learn LabelEncoder and for one hot encoding the pandas get_dummies df function. Logistic Regression ImplementationHere I will focus on implementing the model rather than explaining the details but for those who want to learn more about the theory of machine learning algorithms I recommend both An Introduction to Statistical Learning http www bcf. This is true for the most part as the winning models at least for structured data all tend to be variants on gradient boosting http blog. 5 for all observations on the test set. Each row is one month of a credit card balance and a single credit card can have many rows. Each previous credit has its own row in bureau but one loan in the application data can have multiple previous credits. There is a clear trend younger applicants are more likely to not repay the loan The rate of failure to repay is above 10 for the youngest three age groups and beolow 5 for the oldest age group. in class imbalance problem. html that creates the polynomials and the interaction terms up to a specified degree. However by attempting to understand how our models make decisions we can try to improve them or examine the mistakes in order to correct the errors. As a solution we will fill in the anomalous values with not a number np. 0 very strong Let s take a look at some of more significant correlations the DAYS_BIRTH is the most positive correlation. Feature EngineeringKaggle competitions are won by feature engineering those win are those who can create the most useful features out of the data. A kernel density estimate plot shows the distribution of a single variable https chemicalstatistician. Along the way we performed necessary preprocessing steps such as encoding categorical variables imputing missing values and scaling features to a range. This is information that could be directly used by the bank because younger clients are less likely to repay the loan maybe they should be provided with more guidance or financial planning tips. The predictions must be in the format shown in the sample_submission. org wiki Receiver_operating_characteristic graphs the true positive rate versus the false positive rate image http www. html by using the information from the other data sources. This will get us a Reciever Operating Characteristic Area Under the Curve AUC ROC of 0. html by their representation in the data to reflect this imbalance. We followed the general outline of a machine learning project https towardsdatascience. Understand the problem and the data2. CorrelationsNow that we have dealt with the categorical variables and the outliers let s continue with the EDA. In the following code we create polynomial features using the EXT_SOURCE variables and the DAYS_BIRTH variable. corr dataframe method. The distribution looks to be much more in line with what we would expect and we also have created a new column to tell the model that these values were originally anomalous becuase we will have to fill in the nans with some value probably the median of the column. Effect of Age on RepaymentAs the client gets older there is a negative linear relationship with the target meaning that as clients get older they tend to repay their loans on time more often. com 2013 06 09 exploratory data analysis kernel density estimation in r on ozone pollution data in new york and ozonopolis and can be thought of as a smoothed histogram it is created by computing a kernel usually a Gaussian at each data point and then averaging all the individual kernels to develop a single smooth curve. Pairs PlotAs a final exploratory plot we can make a pairs plot of the EXT_SOURCE variables and the DAYS_BIRTH variable. Don t worry even if this code looks intimidating it s just a series of small steps that build up to a complete model. The anomalous values seem to have some importance so we want to tell the machine learning model if we did in fact fill in these values. We will put the x axis in years to make the plot a little more understandable. For example the blue model is better than the red model which is better than the black diagonal line which indicates a naive random guessing model. Scikit Learn has a useful class called PolynomialFeatures http scikit learn. It s hard to say ahead of time if these new features will be useful. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in using Imputation before machine learning. Another option would be to drop columns with a high percentage of missing values although it is impossible to know ahead of time if these columns will be helpful to our model. There are 35 features with individual features raised to powers up to degree 3 and interaction terms. To make this graph first we cut the age category into bins of 5 years each. Predicting whether or not a client will repay a loan or have difficulty is a critical business need and Home Credit is hosting this competition on Kaggle to see what sort of models the machine learning community can develop to help them in this task. Let s now look at the number of unique entries in each of the object categorical columns. That s a little confusing so we will take the absolute value of the feature and then the correlation will be negative. Let s start looking at this variable. We see that there are only a handful of features with a significant importance to the model which suggests we may be able to drop many of the features without a decrease in performance and we may even see an increase in performance. previous_application previous applications for loans at Home Credit of clients who have loans in the application data. No new columns are created. As an extremely important note anything we do to the training data we also have to do to the testing data. Then for each bin we calculate the average value of the target which tells us the ratio of loans that were not repaid in each age category. Many times in machine learning the only way to know if an approach will work is to try it out Domain Knowledge FeaturesMaybe it s not entirely correct to call this domain knowledge because I m not a credit expert but perhaps we could call this attempts at applying limited financial knowledge. com willkoehrsen intro to model tuning grid and random search Automated Model Tuning https www. csv and an example of the expected submission file. Handling the anomalies depends on the exact situation with no set rules. Let s try using a Random Forest on the same training data to see how that affects performance. com questions 9443 when to use one hot encoding vs labelencoder vs dictvectorizor. Feature importances are not the most sophisticated method to interpret a model or perform dimensionality reduction but they let us start to understand what factors our model takes into account when it makes predictions. If we only have two unique values for a categorical variable such as Male Female then label encoding is fine but for more than 2 unique categories one hot encoding is the safe option. 5 in the competition random guessing on a classification task will score a 0. Given these results it does not appear that our feature construction helped in this case. The actual assignment of the integers is arbitrary. Now we can see whether any of these new features are correlated with the target. The Pairs Plot https towardsdatascience. com interpreting interactions in regression to capture the effects of multiple variables but I do not see them used as often in machine learning. We will certainly best that in future work Again we see tha some of our features made it into the most important. The final part is to share the share the notebook go to the settings tab and change the visibility to Public. By itself the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. The Reciever Operating Characteristic ROC curve https en. In future notebooks we will see how to work with more advanced models which mostly means adapting existing code to make it work better feature engineering and feature selection. Next we can look at the distribution of each of these features colored by the value of the target. Here I m going to use five features that were inspired by this script https www. There is one row for every made payment and one row for every missed payment. We want the probability the loan is not repaid so we will select the second column. Then we constructed new features out of the existing data to see if doing so could help our model. io PythonDataScienceHandbook 05. com willkoehrsen intro to model tuning grid and random search As always I welcome feedback and constructive criticism. The training application data comes with the TARGET indicating 0 the loan was repaid or 1 the loan was not repaid. io en latest Features. I added this code just to show what may be in store for this project and because it gets us a slightly better score on the leaderboard. In this method we make features that are powers of existing features as well as interaction terms between existing features. To remove the columns in the training data that are not in the testing data we need to align the dataframes. Clearly this would not be effective the recall would be zero and we use more advanced metrics such as ROC AUC or the F1 score https en. Column TypesLet s look at the number of columns of each data type. Each row is one month of a previous credit and a single previous credit can have multiple rows one for each month of the credit length. Making sure this data is as relevant to the task as possible is the job of the data scientist and maybe some automated tools https docs. All three EXT_SOURCE featureshave negative correlations with the target indicating that as the value of the EXT_SOURCE increases the client is more likely to repay the loan. One way to try and understand the data is by looking for correlations between the features and the target. Each current loan in the application data can have multiple previous loans. Well that is extremely interesting It turns out that the anomalies have a lower rate of default. Although this is not a significant correlation 0. ConclusionsIn this notebook we saw how to get started with a Kaggle machine learning competition. This process may need to change as we get further into the project but for now we will see where this gets us. Label Encoding and One Hot EncodingLet s implement the policy described above for any categorical variable dtype object with 2 unique categories we will use label encoding and for any categorical variable with more than 2 unique categories we will use one hot encoding. 679 when submitted which probably shows that the engineered features do not help in this model however they do help in the Gradient Boosting Model at the end of the notebook. As expected the most important features are those dealing with EXT_SOURCE and DAYS_BIRTH. com questions 266387 can auc roc be between 0 0 5. The submission files can be directly submitted to the competition from this tab or they can be downloaded to a local machine and saved. Going forward we will need to think about whatother domain knowledge features may be useful for this problem or we should consult someone who knows more about the financial industry This model scores about 0. An example is shown below image https raw. Data cleaning and formatting this was mostly done for us 3. pdf or other dimensionality reduction methods https www. This will let us visualize the effect of this variable on the target. com questions 206 what is the difference between discrete data and continuous data. Each row is one month of a previous point of sale or cash loan and a single previous loan can have many rows. The other columns with DAYS in the dataframe look to be about what we expect with no obvious outliers. In this notebook we will take an initial look at the Home Credit default risk machine learning competition currently hosted on Kaggle. 04 feature engineering. nan in the testing data. This is a standard supervised classification task __Supervised__ The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features __Classification__ The label is a binary variable 0 will repay loan on time 1 will have difficulty repaying loan DataThe data is provided by Home Credit http www. We want to predict the probabilities of not paying a loan so we use the model predict. We will do a lot of feature engineering when we start using the other data sources but in this notebook we will try only two simple feature construction methods Polynomial features Domain knowledge features Polynomial FeaturesOne simple feature construction method is called polynomial features http scikit learn. This returns an m x 2 array where m is the number of observations. The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. While choosing the right model and optimal settings are important the model can only learn from the data it is given. There are two main ways to carry out this process Label encoding assign each unique category in a categorical variable with an integer. org wiki F1_score to more accurately reflect the performance of a classifier. How about the days of employment That doesn t look right The maximum value besides being positive is about 1000 years Just out of curiousity let s subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients. __The logistic regression baseline should score around 0. There is some debate about the relative merits of these approaches and some models can deal with label encoded categorical variables with no issues. POS_CASH_BALANCE monthly data about previous point of sale or cash loans clients have had with Home Credit. com koehrsen_willWill Just for Fun Light Gradient Boosting MachineNow if you want this part is entirely optional we can step off the deep end and use a real machine learning model the gradient boosting machine https machinelearningmastery. Visualize New VariablesWe should explore these __domain knowledge__ variables visually in a graph. We also carried out an experiment to determine the effect of adding the engineering variables. We are asked to predict the probability of not repaying the loan so if we are entirely unsure we would guess 0. We will use 100 trees in the random forest. Since we already know what score we are going to get we don t really need to make a naive baseline guess. The ROC AUC may sound intimidating but it is relatively straightforward once you can get your head around the two individual concepts. Let s take a look at these variables. We can first examine the number of loans falling into each category. There are no outliers for the age on either the high or low end. If we were using these predictions to classify applicants we could set a probability threshold for determining that a loan is risky. Once we run the notebook the files created are available in the Versions tab under the Output sub tab. csv file where there are only two columns SK_ID_CURR and TARGET. Here we use the familiar Scikit Learn modeling syntax we first create the model then we train the model using. We can also see that DAYS_BIRTH is positively correlated with EXT_SOURCE_1 indicating that maybe one of the factors in this score is the client age. To deal with this we can perform one hot encoding followed by PCA http www. Given the correlations we saw in the exploratory data analysis we should expect that the most important features are the EXT_SOURCE and the DAYS_BIRTH. The number of features has grown significantly due to one hot encoding. Then we built a second slightly more complicated model to beat our first score. com questions 806 advantages of auc vs standard accuracy Not that we know the background of the data we are using and the metric to maximize let s get into exploring the data. When we build machine learning models we can try with and without these features to determine if they actually help the model learn. The following code makes the predictions and selects the correct column. We will add these features to a copy of the training and testing data and then evaluate models with and without the features. Exploratory Data Analysis4. These features that are a combination of multiple individual variables are called interaction terms https en. png One hot encoding create a new column for each unique category in a categorical variable. png The problem with label encoding is that it gives the categories an arbitrary ordering. This represents one of the patterns in machine learning feature engineering has a greater return on investment than model building and hyperparameter tuning. __This model should score around 0. Model interpretation just a little Machine learning competitions do differ slightly from typical data science problems in that we are concerned only with achieving the best performance on a single metric and do not care about the interpretation. The goal of EDA is to learn what our data can tell us. We will also not use any dimensionality reduction in this notebook but will explore in future iterations. ", "id": "willkoehrsen/start-here-a-gentle-introduction", "size": "42950", "language": "python", "html_url": "https://www.kaggle.com/code/willkoehrsen/start-here-a-gentle-introduction", "git_url": "https://www.kaggle.com/code/willkoehrsen/start-here-a-gentle-introduction", "script": "sklearn.metrics missing_values_table lightgbm Imputer model MinMaxScaler seaborn numpy PolynomialFeatures corr_func plot_feature_importances sklearn.ensemble sklearn.model_selection LabelEncoder RandomForestClassifier KFold matplotlib.pyplot pandas LogisticRegression roc_auc_score sklearn.linear_model sklearn.preprocessing ", "entities": "(('data', 'data possible scientist'), 'be') (('First we', 'age'), 'make') (('Visualize New VariablesWe', 'visually graph'), 'explore') (('We', 'engineering variables'), 'carry') (('DAYS_BIRTH', 'more significant correlations'), '0') (('it', 'only data'), 'choose') (('number', 'significantly one hot encoding'), 'grow') (('bureau data', 'previous other financial institutions'), 's') (('We', 'features'), 'add') (('Here we', 'correlation lower triangle'), 'use') (('we', 'algorithm'), 'try') (('that', 'complete model'), 'worry') (('we', 'second column'), 'want') (('classes', 'xgboost'), 'get') (('we', 'EXT_SOURCE_1'), 'create') (('you', 'code'), 'hope') (('any further we', 'pesky categorical variables'), 'need') (('client', 'payment difficulties'), 'examine') (('Label encoding', 'integer'), 'be') (('it', 'data'), 'be') (('submissions', 'which'), 'make') (('we', 'scaling range'), 'preprocessing') (('all', 'something'), 'have') (('current loan', 'multiple previous loans'), 'have') (('we', 'data'), 'want') (('two columns', '1'), 'be') (('it', 'target'), 'go') (('previous application', 'feature'), 'have') (('that', 'features'), 'want') (('one loan', 'multiple previous credits'), 'have') (('that', 'modeling'), 'perform') (('submission files', 'competition'), 'submit') (('then we', 'model'), 'use') (('machine learning community', 'task'), 'be') (('client', 'loan'), 'try') (('we', 'number np'), 'fill') (('when it', 'predictions'), 'be') (('actual assignment', 'integers'), 'be') (('they', 'time'), 'get') (('that', 'age category'), 'calculate') (('rate', 'age 5 oldest group'), 'be') (('maybe one', 'score'), 'see') (('feature', 'client'), 'appear') (('models', 'issues'), 'be') (('png One hot encoding', 'categorical variable'), 'create') (('applicant', 'loan'), 'be') (('exactly it', 'data'), 'm') (('loan', 'feature'), 'have') (('feature construction', 'case'), 'appear') (('they', 'equipment'), 'be') (('monthly data', 'cash loans Home Credit'), 'POS_CASH_BALANCE') (('that', 'interaction as well existing features'), 'make') (('credit single card', 'many rows'), 'be') (('s', 'data'), 'question') (('One hot encoding', 'testing data'), 'create') (('we', 'TARGET'), 'make') (('they', 'loan current application'), 'be') (('domain knowledge', 'at least partially track'), 'see') (('we', 'EXT_SOURCE variables'), 'PlotAs') (('following code', 'correct column'), 'make') (('org wiki Interaction _ they', 'variables'), 'statistic') (('originally anomalous we', 'probably column'), 'look') (('we', 'one hot encoding'), 'implement') (('then them', 'machine learning'), 'be') (('however they', 'notebook'), 'do') (('org Kernel_density_estimation KDE', 'target'), 'wiki') (('it', 'arbitrary ordering'), 'png') (('correlations', 'other'), 'show') (('that', 'loans'), 'understand') (('Therefore we', 'columns'), 'keep') (('first we', '5 years'), 'cut') (('com willkoehrsen', 'feature engineering exploratory Feature Selection https automated www'), 'tuning') (('who', 'problem'), 'intend') (('ages', 'much other'), 'tell') (('machine applied learning', 'basically engineering'), 'be') (('Jake VanderPlas', 'Data Science https jakevdp'), 'write') (('again labels', 'same process'), 'recieve') (('Feature engineering', 'dimensionality other reduction'), 'refer') (('We', 'LogisticRegression'), 'use') (('it', 'Curve ROC AUC'), 'be') (('235489 xgboost', 'forecasting phase'), 'com') (('class _ imbalanced _', 'www'), 'see') (('we', 'application only main training data'), 'stick') (('submission', 'about 0'), 'score') (('interaction', 'specified degree'), 'term') (('doing', 'so model'), 'construct') (('ConclusionsIn notebook we', 'Kaggle machine learning how competition'), 'see') (('relatively you', 'two individual concepts'), 'sound') (('com jsaguiar', 'features'), 'update') (('org wiki', 'classifier'), 'reflect') (('Training DataThere', 'features same training data'), 'need') (('we', 'www'), 'perform') (('number', 'many categories'), 'be') (('cash single previous loan', 'many rows'), 'be') (('we', 'method'), 'get_feature_names') (('they', 'target'), 'be') (('Moreover we', 'HomeCredit_columns_description'), 'png') (('who', 'data'), 'win') (('1 loan', '0'), 'come') (('ages', 'year'), 'mutliple') (('how that', 'performance'), 'let') (('it', 'leaderboard'), 'add') (('single line', 'positive instance'), 'png') (('Examine Missing we', 'column'), 'ValuesNext') (('accuracy', 'inbalanced classes'), 'confusing') (('We', 'feature engineering'), 'be') (('we', 'testing set'), 'be') (('most important features', 'data exploratory analysis'), 'expect') (('DAYS_BIRTH', 'reason'), 'except') (('We', 'the'), 'calculate') (('better model', '0'), 'be') (('s', 'EDA'), 'correlationsnow') (('we', 'that'), 'let') (('object columns', 'strings'), 'contain') (('trends', 'data'), 'be') (('Data cleaning', 'mostly us'), 'do') (('we', 'kernel'), 'hit') (('DataThe data', 'Home Credit http www'), 'be') (('plot', 'years'), 'put') (('we', 'features 122 TARGET'), 'have') (('test set', 'TARGET considerably column'), 'be') (('that', 'multiple individual variables'), 'call') (('value', 'then new boolean column'), 'create') (('we', 'data'), 'start') (('younger clients', 'time'), 'mean') (('This', 'new features'), 'create') (('value', 'category'), 'be') (('we', 'currently Kaggle'), 'take') (('entirely we', '0'), 'ask') (('numpy pandas', 'sklearn matplotlib'), 'model') (('that', 'loans'), 'be') (('we', 'http scikit'), 'use') (('scikit', 'regularization parameter http'), 'be') (('One way', 'describe method'), 'be') (('applicant', 'time'), 'be') (('We', 'data'), 'see') (('many we', 'features'), 'be') (('it', 'single smooth curve'), 'com') (('We', 'future iterations'), 'use') (('where m', 'observations'), 'return') (('Both', 'R'), 'present') (('which', 'modeling choices'), 'be') (('First we', 'data available files'), 'list') (('simply guesses', '0'), 'model') (('much more powerful especially when we', 'trees'), 'be') (('which', 'overfitting'), 'decrease') (('we', 'future work'), 'stick') (('we', 'don baseline really naive guess'), 'know') (('kernel density estimation plot next https', 'target'), 'make') (('we', 'probabilities'), 'remember') (('Most', 'unique entries'), 'have') (('which', 'guessing naive random model'), 'be') (('We', 'feature scaling'), 'preprocess') (('that', '99'), 'for') (('two variables', 'target'), 'show') (('we', 'errors'), 'by') (('loan', '0'), 'represent') (('observation', 'other new columns'), 'recieve') (('we', 'obvious outliers'), 'look') (('then output', 'versions tab'), 'take') (('we', 'well competition'), 'be') (('get_dummies df', 'one hot encoding'), 'use') (('s', 'object categorical columns'), 'let') (('notebook', 'Public'), 'be') (('s', 'np'), 'let') (('threshold', 'lower left'), 'start') (('that', 'imputation https stats'), 'use') (('single person', 'simply model'), 'make') (('loan', 'probability'), 'use') (('We', 'random forest'), 'use') (('most important features', 'EXT_SOURCE'), 'be') (('tha some', 'most important'), 'best') (('entirely we', 'boosting machine https gradient machinelearningmastery'), 'koehrsen_willWill') (('Several', 'original features'), 'have') (('columns', 'model'), 'be') (('them', 'machine as often learning'), 'com') (('that', 'script https www'), 'm') (('This', 'https great subject www'), 'be') (('we', 'information'), 'extract') (('we', 'performance'), 'see') (('perhaps we', 'limited financial knowledge'), 'be') (('Therefore we', 'model'), 'have') (('actually model', 'features'), 'build') (('that', 'better model'), 'indicate') (('org wiki', 'www'), 'graph') (('following code', 'preprocessing steps'), 'perform') (('we', 'categorical variables'), 'teach') (('who', 'application data'), 'application') (('files', 'Output sub tab'), 'be') (('who', 'model scores'), 'need') (('notebook', 'which'), 'save') (('we', 'interpretation'), 'interpretation') (('s', 'baseline Logistic actual Regression'), 'let') (('who', 'more information'), 'html') (('I', 'notebooks Manual Feature Engineering Part https follow One www'), 'Follow') (('new features', 'ahead time'), 's') (('We', 'test set'), 'create') (('us', 'as well single variables'), 'be') (('we', 'ROC such AUC'), 'be') (('we', 'feature engineering https more docs'), 'do') (('winning models', 'http blog'), 'be') (('that', 'run'), 'run') (('it', 'what'), 'com') (('where this', 'us'), 'need') (('it', 'future models'), 'get') (('predictions', 'sample_submission'), 'be') (('we', 'scores'), 'look') (('most we', 'random forest'), 'help') (('predictions', 'testing data'), 'fit') (('we', '0'), 'generation') (('I', 'https medium'), 'write') (('we', 'dataframes'), 'need') (('Exterior SourcesThe 3 variables', 'target'), 'be') (('we', 'column descriptions https www'), 's') (('domain features', 'performance'), '754') (('also when we', 'entire notebook'), 'be') (('This', '0'), 'get') (('they', 'local machine'), 'submit') (('example', 'image https raw'), 'show') (('ROC AUC', 'model better performance'), 'have') (('Then we', 'first score'), 'build') (('how all', 'data'), 'show') (('they', 'very fact'), 'consider') (('I', 'notebooks Manual Feature Engineering Part https follow One www'), 'allow') (('credit_card_balance monthly data', 'Home Credit'), 'have') (('single previous credit', 'credit length'), 'be') (('One data', 'features'), 'way') (('It', 'ROC simply curve'), 'explain') (('we', 'model'), 'want') (('we', 'values'), 'seem') (('machine complete learning', 'python part'), 'com') (('any', 'target'), 'see') (('s', 'age bracket'), 'let') (('data', 'us'), 'be') (('Handling', 'set rules'), 'depend') (('we', 'imputation'), 'come') (('one', 'model building'), 'represent') (('we', 'more than 2 categories'), 'use') (('submission', 'which'), 'help') (('We', 'machine learning project https towardsdatascience'), 'follow') (('we', 'which'), 'implement') (('which', 'one file'), 'stick') (('they', 'clients'), 'look') (('We', 'future work'), 'use') (('we', 'EXT_SOURCE variables'), 'create') (('maybe they', 'more guidance financial tips'), 'be') (('we', 'predictions'), 'use') (('5', '0'), 'score') (('client', 'more loan'), 'indicate') (('annuity', 'thanks Aguiar'), 'update') (('one hot encoding', 'more than 2 unique categories'), 'be') (('kernel density estimate plot', 'https single variable chemicalstatistician'), 'show') (('we', 'what'), 'use') (('validation Overall score', 'domain knolwedge features'), 'go') (('random always I', 'feedback criticism'), 'com') (('EXT_SOURCE_3', 'target'), 'display') (('clearly feature', 'loan'), 'see') (('http polynomial scikit', 'Domain knowledge'), 'do') (('Interaction terms', 'models https commonly statistical www'), 'use') (('it', 'better feature engineering selection'), 'see') (('We', 'category'), 'examine') (('anomalies', 'default'), 'be') (('safest it', 'categories'), 'think') (('client', 'loan'), 'make') (('us', 'target'), 'let') (('which', 'machine learning'), 'make') (('features', 'data external source'), 'represent') (('machine learning model', 'such LightGBM http lightgbm'), 'deal') (('We', 'graph'), 'use') (('2017 01 23 kaggle master', 'gradient boosting'), 'com') (('what', 'topics basics'), 'com') (('I', 'www bcf'), 'ImplementationHere') (('we', 'hand'), 'test') (('we', 'testing also data'), 'have') (('I', 'comments'), 'avoid') (('Next we', 'target'), 'look') (('206 difference', 'discrete data'), 'com') (('then correlation', 'feature'), 's') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["absolute", "account", "accuracy", "advanced", "age", "algorithm", "appear", "application", "approach", "area", "array", "article", "assign", "assignment", "auc", "auto", "average", "background", "balance", "bank", "baseline", "best", "bin", "binary", "blog", "book", "boolean", "boosting", "build", "button", "calculate", "call", "care", "case", "categorical", "category", "chart", "classification", "classifier", "classify", "cleaning", "clear", "client", "code", "coefficient", "colored", "column", "community", "compare", "competition", "confidence", "contain", "content", "copy", "corr", "correct", "correlation", "correlations", "could", "create", "credit", "csv", "current", "curve", "cut", "data", "dataframe", "default", "degree", "describe", "detect", "develop", "df", "diagonal", "difference", "dimension", "dimensionality", "directly", "discrete", "distribution", "domain", "download", "drop", "effect", "en", "encode", "encoder", "encoding", "end", "engineering", "environment", "error", "estimation", "evaluate", "even", "every", "execute", "expected", "experience", "experiment", "explore", "external", "extract", "fact", "failure", "feature engineering", "feature", "feedback", "file", "fill", "final", "find", "fit", "fold", "following", "forecasting", "forest", "form", "format", "formatting", "forward", "frame", "function", "future", "general", "generated", "generation", "gradient", "graph", "grid", "guidance", "hand", "handle", "head", "help", "high", "histogram", "history", "hope", "hot", "http", "hyperparameter", "idea", "image", "imbalance", "implement", "implementation", "importance", "improve", "impute", "including", "increase", "index", "individual", "industry", "influence", "int64", "interpretation", "investment", "io", "iteration", "itself", "job", "kaggle", "kernel", "knowledge", "label", "labeling", "leaderboard", "learn", "learning", "least", "left", "length", "let", "level", "library", "lightgbm", "likelihood", "line", "linear", "list", "little", "loan", "local", "look", "looking", "lost", "lot", "lower", "magnitude", "main", "manual", "matplotlib", "maximum", "mean", "meaning", "measure", "median", "memory", "method", "metric", "might", "mind", "missing", "model", "month", "most", "multiple", "naive", "name", "need", "negative", "new", "next", "no", "normalized", "not", "notebook", "np", "number", "numeric", "numpy", "object", "objective", "observation", "open", "opinion", "opt", "option", "order", "out", "output", "overall", "overfitting", "overview", "pair", "parameter", "part", "pdf", "percentage", "perform", "performance", "person", "plot", "plotting", "png", "point", "positive", "predict", "preprocessing", "present", "probability", "problem", "product", "project", "public", "python", "random", "range", "rating", "ratio", "reading", "recall", "recommend", "record", "recording", "reduce", "regression", "regularization", "relationship", "relative", "remove", "representation", "rest", "return", "reverse", "right", "risk", "roc", "row", "run", "running", "scaler", "scaling", "scatter", "science", "scikit", "score", "scoring", "script", "seaborn", "search", "second", "secret", "select", "selection", "separate", "service", "set", "several", "single", "situation", "size", "sklearn", "smooth", "solution", "something", "sort", "sound", "source", "stack", "standard", "start", "step", "store", "style", "sub", "subject", "submission", "subplot", "subset", "sum", "summary", "supervised", "support", "system", "table", "target", "task", "test", "testing", "theory", "think", "those", "thought", "threshold", "through", "time", "tool", "total", "track", "train", "training", "trend", "try", "tuning", "type", "under", "understanding", "unique", "up", "update", "upper", "valid", "validation", "value", "variable", "visualization", "visualize", "walk", "weight", "while", "who", "work", "world", "write", "xgboost", "year"], "potential_description_queries_len": 369, "potential_script_queries": ["model", "preprocessing", "sklearn"], "potential_script_queries_len": 3, "potential_entities_queries": ["boolean", "current", "estimation", "feature", "guidance", "http", "kaggle", "least", "main", "new", "notebook", "plot", "random", "smooth", "subject", "unique"], "potential_entities_queries_len": 16, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 369}