{"name": "principal component analysis pca ", "full_name": " h1 Principal Component Analysis PCA h3 It is a data transformation technique The final objective of PCA is DIMENSIONALITY REDUCTION h1 Using Kmeans on the transformed data using PCA h1 Kmeans on original data ", "stargazers_count": 0, "forks_count": 0, "description": "The following plot represents this redundant data FIGURE 3 alt text https i. t their corresponding eigen values. Using Kmeans on the transformed data using PCA Kmeans on original dataAs we can see that the inertia value on transformed data using PCA is less as compared to the inertia value on the original data. On the other hand there is a lot of redundant overlapping data due to high correlation in these 2 features which gives rise to multicollinearity. But on the other hand we can t affort to lose the information from a cancerous patient eg. Thus it gives us an improved performance after transforming the data using PCA. We can afford to lose some information from a healthy patient eg. Now to transform our data to a data which has its dimensionality reduced we need to do a dot product between our originally scaled data and the PCs. This would mean that we are considering using only the important information from the high variance zone FIGURE 2 and eliminating all the redundant noise less important information by elimininating the PCs which explain the low variance region FIGURE 3. So if we plot a histogram of these 2 features a majority of the part will be overlapping as they are highly correlated. The final objective of PCA is DIMENSIONALITY REDUCTION. Consider an example of predicting a cancerous patient. So we can say that we are not sacrificing the feature but we are sacrificing the information content at the low frequency low variance zone. To know more about why we compare inertia values for comparing models please do checkout my kernel Kmeans Clustering Guide https www. png This redundant data causes multicolliearity. png The important information which these 2 features will provide us will be those data points which are not redundant non overlapping and lie at the extreme ends of the histograms as shown in the plot below FIGURE 2 alt text https i. Our scaled data is 150x4 and our selected PCs are 2X4. Please UpVote if you like the work Principal Component Analysis PCA It is a data transformation technique. com pratikasarkar kmeans clustering guide Please UpVote if you like the work. The general myth that the idea of dimensionality reduction means that PCA will drop some of the weak features is WRONG. png This is the information which we cannot afford to lose because the information in these records are responsible for significantly differentiating the 2 features from each other. abnormal bp range abnormal rbc wbc level bcoz it could lead to a loss of extensive information and our model could behave unexpectedly. Out these there are 2 features which are highly correlated to each other. So we would need to transpose our selected PCs to do the dot product. 52354627 So as we can see that most of the variance in data has been explained by the 1st two principal components that is about 96 percent. You can see the plot below FIGURE 1 alt text https i. Consider we have some features which are highly significant in our data out of these some are highly correlated to out target variable and some a weakly correlated. normal bp range normal rbc wbc level as it will not affect our prediction result to an extreme level. So most of the information essential to differentiate the feature from each other has been captured by the 1st two PCs. So the intention of PCA is to reduce that noise. This multicolliearity effect is the NOISE in PCA. We need to sort these eigen vectors in descending order w. The actual representation of principal components will be the transpose of eigen vectors as follows PC1 0. Reducing high number of PCs could lead to introducing bias error in our model. Consider we have 15 features in a dataset. cov calculates the covariance row wise. Eigen Vectors here are the principal components PCs. So even if we lose some data from this part there won t be much information loss. There is a chance that those weakly correlated features have high correlation among themselves eventually adding redundancy and multicolliearity to our models. Lets have a look at it into with an example more intuitively. So we transpose the dataset to represent the features as rows. Overdoing PCA is not advisable. The least percentage advisable is around 90 percent. ", "id": "pratikasarkar/principal-component-analysis-pca", "size": "4645", "language": "python", "html_url": "https://www.kaggle.com/code/pratikasarkar/principal-component-analysis-pca", "git_url": "https://www.kaggle.com/code/pratikasarkar/principal-component-analysis-pca", "script": "seaborn numpy sklearn.cluster sklearn.preprocessing KMeans pandas StandardScaler ", "entities": "(('So intention', 'noise'), 'be') (('information', 'other'), 'png') (('multicolliearity effect', 'PCA'), 'be') (('data which', 'alt text https FIGURE 2 i.'), 'be') (('You', 'alt text https FIGURE 1 i.'), 'see') (('Reducing', 'model'), 'lead') (('actual representation', 'PC1'), 'be') (('weakly correlated features', 'models'), 'be') (('we', 'patient cancerous eg'), 'can') (('So we', 'dot product'), 'need') (('it', 'extreme level'), 'result') (('2 which', 'highly other'), 'be') (('we', 'originally scaled data'), 'transform') (('we', 'dataset'), 'consider') (('We', 'order descending w.'), 'need') (('we', 'frequency variance low low zone'), 'say') (('final objective', 'PCA'), 'be') (('We', 'healthy patient eg'), 'afford') (('png redundant data', 'multicolliearity'), 'cause') (('they', 'part'), 'feature') (('inertia value', 'original data'), 'see') (('which', 'multicollinearity'), 'be') (('Kmeans', 'Guide https www'), 'checkout') (('which', 'PCs'), 'mean') (('Thus it', 'PCA'), 'give') (('So even we', 'there t'), 'win') (('So we', 'rows'), 'transpose') (('model', 'extensive information'), 'behave') (('some', 'weak features'), 'be') (('following plot', 'data redundant FIGURE'), 'represent') (('a', 'some'), 'correlate') (('you', 'work'), 'cluster') (('It', 'work Principal Component Analysis PCA'), 'please') (('So most', '1st two PCs'), 'capture') (('that', '1st two principal components'), '52354627') (('Lets', 'example'), 'have') ", "extra": "['biopsy of the greater curvature', 'patient']", "label": "Perfect_files", "potential_description_queries": ["clustering", "compare", "content", "correlation", "could", "covariance", "data", "dataset", "dimensionality", "dot", "drop", "effect", "error", "even", "explained", "feature", "final", "following", "frequency", "general", "hand", "high", "histogram", "idea", "kernel", "lead", "least", "level", "look", "lot", "majority", "mean", "model", "most", "my", "need", "noise", "non", "normal", "not", "number", "objective", "order", "out", "part", "patient", "percentage", "performance", "plot", "png", "prediction", "principal", "product", "provide", "range", "reduce", "region", "representation", "result", "row", "scaled", "selected", "sort", "target", "text", "those", "transform", "transformation", "transpose", "value", "variable", "variance", "work"], "potential_description_queries_len": 73, "potential_script_queries": ["numpy", "seaborn"], "potential_script_queries_len": 2, "potential_entities_queries": ["patient", "scaled", "variance"], "potential_entities_queries_len": 3, "potential_extra_queries": ["biopsy of the greater curvature", "biopsy"], "potential_extra_queries_len": 2, "all_components_potential_queries_len": 77}