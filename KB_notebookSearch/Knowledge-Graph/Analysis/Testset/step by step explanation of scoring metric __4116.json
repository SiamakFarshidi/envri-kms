{"name": "step by step explanation of scoring metric ", "full_name": " h3 Rationale h3 Picking a test image h3 Intersection Over Union for a single Prediction GroundTruth comparison h3 Thresholding the IoU value for a single GroundTruth Prediction comparison h3 Single threshold precision for a single image h3 Multi threshold precision for a single image h3 Mean average precision for the dataset ", "stargazers_count": 0, "forks_count": 0, "description": "5 a predicted object is considered a hit if its intersection over union with a ground truth object is greater than 0. For each submitted nuclei prediction calculate the Intersection of Union metric with each ground truth mask in the image. Single threshold precision for a single imageNow in our example we ve created 7 prediction masks P_i to compare with 7 ground truth masks GT_j. Their intersections and unions look like this Notice how the intersection will always be less than or equal to the size of the GroundTruth object and the Union will always be greater than or equal to that size. In other words at a threshold of 0. Multi threshold precision for a single image The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold Avg. Now for each prediction mask P we ll get a comparison with every ground truth mask GT. The number of true positives is equal to the number of predictions with a hit on a true object. The metric used for this competition is defined as the mean average precision at different intersection over union IoU thresholds. Precision t frac TP t TP t FP t FN t TP a single predicted object matches a ground truth object with an IoU above the threshold FP a predicted object had no associated ground truth object. Precision frac 1 n_ thresh sum_ t 1 nprecision t Here we simply take the average of the precision values at each threshold to get our mean precision for the image. RationaleI found the explanation for the scoring metric on this competition a little confusing and I wanted to create a guide for those who are just entering or haven t made it too far yet. Calculate whether this mask fits at a range of IoU thresholds. This tells us there are a few different steps to getting the score reported on the leaderboard. Calculate the mean of the average precision for each image. The number of false positives is equal to the number of predictions that don t hit anything. Hope you found this helpful I know it helped me to work through it Get image Get masks Make messed up masks Plot the objects Minor ticks and turn grid on. The number of false negatives is equal to the number of ground truth objects that aren t hit. 95 with a step size of 0. Average the precision across thresholds. 664 Thresholding the IoU value for a single GroundTruth Prediction comparison Next we sweep over a range of IoU thresholds to get a vector for each mask comparison. The precision value is based on the number of true positives TP false negatives FN and false positives FP in this hit matrix. Picking a test imageI m going to pick a sample image from the training dataset load the masks then create a mock predict set of masks from it by moving and dilating each individual nucleus mask. At each threshold we will have a 7 7 matrix showing whether there was a hit with that object. In most cases this will be zero since nuclei shouldn t overlap but this also allows flexibility in matching up each mask to each potential nucleus. At each threshold calculate the precision across all your submitted masks. Here s the dataset Intersection Over Union for a single Prediction GroundTruth comparison The IoU of a proposed set of object pixels and a set of true object pixels is calculated as IoU A B frac A B A B Let s take one of the nuclei masks from our GroundTruth and Predicted volumes. FN a ground truth object had no associated predicted object. So for this set of masks the IoU metric is calculated as IoU A B frac A B A B frac 564 849 0. Therefore the leaderboard metric will simply be the mean of the precisions across all the images. Mean average precision for the dataset Lastly the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset. The threshold values range from 0. ", "id": "stkbailey/step-by-step-explanation-of-scoring-metric", "size": "4116", "language": "python", "html_url": "https://www.kaggle.com/code/stkbailey/step-by-step-explanation-of-scoring-metric", "git_url": "https://www.kaggle.com/code/stkbailey/step-by-step-explanation-of-scoring-metric", "script": "get_iou_vector scipy ndimage numpy matplotlib.pyplot iou_thresh_precision pathlib pandas Path ", "entities": "(('1 _ thresh _ 1 nprecision n sum Here we', 'image'), 'frac') (('FN', 'ground truth associated predicted object'), 'have') (('aren', 'that'), 'be') (('Therefore leaderboard metric', 'images'), 'be') (('mask P we', 'ground truth mask GT'), 'get') (('don t', 'anything'), 'be') (('average precision', 'Avg'), 'precision') (('precision value', 'hit matrix'), 'base') (('this', 'potential nucleus'), 'be') (('we', 'ground truth masks 7 GT_j'), 'precision') (('Next we', 'mask comparison'), '664') (('Union', 'always size'), 'look') (('score', 'test dataset'), 'precision') (('we', 'object'), 'have') (('masks', 'Minor grid'), 'hope') (('intersection', '0'), 'consider') (('Intersection', 'image'), 'calculate') (('masks', 'nucleus individual mask'), 'create') (('number', 'true object'), 'be') (('who', 'haven just it'), 'find') (('IoU A B frac A B A B', 'masks'), 'calculate') (('predicted object', 'ground truth associated object'), 'frac') (('metric', 'union IoU thresholds'), 'define') (('a few different score', 'leaderboard'), 'tell') (('s', 'Predicted volumes'), 's') (('mask', 'IoU thresholds'), 'calculate') ", "extra": "['biopsy of the greater curvature', 'test']", "label": "Perfect_files", "potential_description_queries": ["associated", "average", "calculate", "compare", "comparison", "competition", "create", "dataset", "equal", "every", "flexibility", "found", "frac", "grid", "ground", "image", "individual", "intersection", "leaderboard", "little", "load", "look", "mask", "matching", "matrix", "mean", "metric", "most", "no", "nuclei", "number", "object", "overlap", "potential", "precision", "predict", "prediction", "range", "sample", "score", "scoring", "set", "single", "size", "step", "test", "those", "threshold", "through", "training", "turn", "up", "value", "vector", "who", "work"], "potential_description_queries_len": 56, "potential_script_queries": ["ndimage", "numpy", "pathlib", "scipy"], "potential_script_queries_len": 4, "potential_entities_queries": ["frac", "leaderboard", "sum"], "potential_entities_queries_len": 3, "potential_extra_queries": ["biopsy of the greater curvature", "biopsy"], "potential_extra_queries_len": 2, "all_components_potential_queries_len": 62}