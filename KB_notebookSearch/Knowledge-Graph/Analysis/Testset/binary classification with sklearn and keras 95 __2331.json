{"name": "binary classification with sklearn and keras 95 ", "full_name": " h2 Predicting presence of Heart Disease using Machine Learning h2 I Importing essential libraries h2 II Importing and understanding our dataset h4 Verifying it as a dataframe object in pandas h4 Shape of dataset h4 Printing out a few columns h4 Description h4 Let s understand our columns better h4 Analysing the target variable h4 Clearly this is a classification problem with the target variable having values 0 and 1 h3 Checking correlation between columns h2 Exploratory Data Analysis EDA h3 First analysing the target variable h3 We ll analyse sex cp fbs restecg exang slope ca and thal features h3 Analysing the Sex feature h3 Analysing the Chest Pain Type feature h3 Analysing the FBS feature h3 Analysing the restecg feature h3 Analysing the exang feature h3 Analysing the Slope feature h3 Analysing the ca feature h2 IV Train Test split h2 V Model Fitting h3 Logistic Regression h3 Naive Bayes h3 SVM h3 K Nearest Neighbors h3 Decision Tree h3 Random Forest h3 XGBoost h3 Neural Network h2 VI Output final score h3 We observe that we can achieve the best accuracy of 95 08 using Random Forest h3 I m a beginner to Kaggle and am learning to practice Machine Learning code I hope this Kernel can help out a few others like myself who are looking for a beginner friendly kernel for getting better at Machine Learning h3 I d love some feedback If there s anything I ve done wrong or anything new or better that I can add to my code I d really appreciate valuable feedback on it h3 Cheers ", "stargazers_count": 0, "forks_count": 0, "description": "Output final score We observe that we can achieve the best accuracy of 95. Predicting presence of Heart Disease using Machine Learning I. count 100 303 countNoDisease len df df. print Percentage of patience with heart problems str y. 08 using Random Forest I m a beginner to Kaggle and am learning to practice Machine Learning code. Model Fitting Logistic Regression Naive Bayes SVM K Nearest Neighbors Decision Tree Random Forest XGBoost Neural Network VI. If there s anything I ve done wrong or anything new or better that I can add to my code I d really appreciate valuable feedback on it. Importing and understanding our dataset Verifying it as a dataframe object in pandas Shape of dataset Printing out a few columns Description Let s understand our columns better Analysing the target variable Clearly this is a classification problem with the target variable having values 0 and 1 Checking correlation between columns Exploratory Data Analysis EDA First analysing the target variable We ll analyse sex cp fbs restecg exang slope ca and thal features Analysing the Sex feature We notice that as expected the sex feature has 2 unique features We notice that females are more likely to have heart problems than males Analysing the Chest Pain Type feature As expected the CP feature has values from 0 to 3 We notice that chest pain of 0 i. com a 136542 helped a lot in avoiding overfitting Note Accuracy of 85 can be achieved on the test set by setting epochs 2000 and number of nodes 11. Train Test split V. target 0 countHaveDisease len df df. count 100 303 print Percentage of patience with heart problems str y. Exercise induced angina are much less likely to have heart problems Analysing the Slope feature We observe that Slope 2 causes heart pain much more than Slope 0 and 1 Analysing the ca feature ca 4 has astonishingly large number of heart patients IV. target 1 number of major vessels 0 3 colored by flourosopy https stats. Importing essential libraries II. I hope this Kernel can help out a few others like myself who are looking for a beginner friendly kernel for getting better at Machine Learning I d love some feedback. the ones with typical angina are much less likely to have heart problems Analysing the FBS feature Nothing extraordinary here Analysing the restecg feature We realize that people with restecg 1 and 0 are much more likely to have a heart disease than with restecg 2 Analysing the exang feature People with exang 1 i. Cheers This shows that most columns are moderately correlated with target but fbs is very weakly correlated. ", "id": "jashsheth5/binary-classification-with-sklearn-and-keras-95", "size": "2331", "language": "python", "html_url": "https://www.kaggle.com/code/jashsheth5/binary-classification-with-sklearn-and-keras-95", "git_url": "https://www.kaggle.com/code/jashsheth5/binary-classification-with-sklearn-and-keras-95", "script": "sklearn.metrics sklearn.naive_bayes sklearn.tree keras.layers Sequential KNeighborsClassifier DecisionTreeClassifier seaborn numpy sklearn.ensemble sklearn.model_selection sklearn RandomForestClassifier matplotlib.pyplot Dense pandas LogisticRegression accuracy_score svm sklearn.neighbors sklearn.linear_model GaussianNB xgboost keras.models train_test_split ", "entities": "(('We', 'chest 0 i.'), 'verify') (('fbs', 'moderately target'), 'cheer') (('I', 'Machine Learning code'), '08') (('ca 4', 'heart patients IV'), 'be') (('I d', 'feedback'), 'hope') (('I d', 'it'), 's') (('people', '1 i.'), 'be') (('we', '95'), 'score') (('136542', '2000 nodes'), 'help') ", "extra": "['disease', 'patient', 'test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "avoiding", "best", "chest", "classification", "code", "colored", "correlation", "count", "dataframe", "dataset", "df", "disease", "expected", "feature", "feedback", "final", "heart", "help", "hope", "kernel", "learning", "len", "looking", "lot", "major", "most", "my", "new", "number", "object", "out", "overfitting", "people", "practice", "print", "problem", "score", "set", "sex", "split", "str", "target", "test", "understanding", "unique", "variable", "who"], "potential_description_queries_len": 48, "potential_script_queries": ["numpy", "seaborn", "sklearn", "svm", "xgboost"], "potential_script_queries_len": 5, "potential_entities_queries": [], "potential_entities_queries_len": 0, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 53}