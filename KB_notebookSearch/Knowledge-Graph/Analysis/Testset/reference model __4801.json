{"name": "reference model ", "full_name": " h1 Level 5 Kaggle Reference Model h3 Outline h4 B Creating input and targets h4 C Training a network to segment objects h4 D Inference and postprocessing h4 E Visualizing the results not included in this kernel h4 F Evaluation h3 Train Validation split h2 B Creating input and targets h2 C Training a network to segment objects h4 You can interpret the above visualizations as follows h3 Model limitations ", "stargazers_count": 0, "forks_count": 0, "description": "Note This kernel is a work in progressI didn t want to hold off on releasing this kernel as I think it will help with getting started in this competition as it is. Bring box to car space We only care about the bottom corners Drop z coord Don t worry about it being mirrored. 04597 fully convolutional neural network to predict whether a car or other object is present for every pixel in a birds eye view of the world centered on the car. 8 the actual size we need to adjust for that Determine the rotation of the box XYZW WXYZ order of elements. We only use LIDAR data and we only use one lidar sweep. Inference and postprocessing4. The above image looks pretty well separated some boxes seem to be wrongly merged together and may be problematic. At this point we have pred_box3ds and gt_box3ds they are the predictions and the targets on the validation set. We train a U Net https arxiv. Optional Creating a GIF of a scene. We can use that to test the functions we ll define next that transform the data to the format we want to input into the model we are training. Creating input and targets1. This implementation was copied from https github. This could use some refactoring. We already multiprocess ourselves this would mean every subprocess produces even more threads which would lead to a lot of context switching slowing things down a lot. Level 5 Kaggle Reference ModelAuthor Guido Zuidhof gzuidhof lyft. Loading the ground truth8. That means that we go from a list of coordinates of points to a X by Y by Z space. Width and height is arbitrary we don t know what way the vehicles are pointing from our prediction segmentation It doesn t matter for evaluation so no need to worry about that here. Next steps Compute mAP on the validation set using the evaluation script provided in the SDK Run inference on the test set. 75 meters tall and is at the same height of the ego vehicle which is surely a wrong assumption. Next we take our predicted boxes transform them back into world space and make them 3D. The model is barely converged we could train for longer. Visualizing the results not included in this kernel x. Creating input and targetsLet s load the first sample in the train set. N 3 H W N H W with class indices 0 1 N 2 H W We quantize to uint8 here to conserve memory. com In this Kernel we provide a near end to end example solution for the Lyft Level 5 Kaggle competition. As we evaluate with IoUs between 0. Loading the dataset2. For each scene we fit boxes to the segmentations. Homogeneous transformation matrix from car frame to world frame. We compress the height dimension into only 3 channels. Defining datasets dataloaders2. Running this on all of the data in parallel C. bev stands for birds eye view take one channel only Transpose the input volume CXY to XYC order which is what matplotlib requires. Train Validation splitLet s split the data by car to get a validation set. Alternatively we could consider doing it by scenes date or completely randomly. Homogeneous transformation matrix from sensor coordinate frame to ego car frame. The height of the lidar points are separated into three bins which visualized like this these are the RGB channels of the image. Note We scaled our targets to be 0. Predicting our validation set. To get the center of the box in 3D we ll have to add half the height to it. We do this to keep training and inference time low. Let s try it with some example values Note X and Y are flipped So that the values in the voxels range from 0 1 we set a maximum intensity. Note We may be able to train for longer and expect better results the reason this number is low is to keep the runtime short. com jvanvugt pytorch unet it is MIT licensed. The boxes are imprecise the input has a very low resolution one pixel is 40x40cm in the real world and we arbitrarily threshold the predictions and fit boxes around these boxes. We perform an opening morphological operation to filter tiny detections Note that this may be problematic for classes that are inherently small e. For each box and each class we write it s probability in the center pixel. Model limitations The model performs very poorly on uncommon classes. The input image or semantic input map not in this kernel blended together with targets predictions4. We produce top down images and targets2. Above is an example of what the input for our network will look like. com mailto gzuidhof lyft. Note that red green yellow. N 4 2 N 4 2 N 4 2 2 N 4 Add Z dimension 2 N 4 3 N 4 We don t know at where the boxes are in the scene on the z axis up down let s assume all of them are at the same height as the ego vehicle. Sanity check let s count the amount of connected components in an image Let s take the center pixel value as the confidence value Let s remove candidates with very low probability Visualize the boxes in the first sample Visualize their probabilities Load annotations and filter predictions and annotations. Training the model D. It s a top down projection of the world around the car the car faces to the right in the image. In other words Black True Negative Green False Negative Yellow True Positive Red False Positive 2. Note Each of these boxes describes the ground corners of a 3D box. Splitting all data into a train and validation set by car B. Defining the network architecture U net 3. Let s load the ground truth for the validation set. backprojecting our predicted boxes into world space E. Some hyperparameters we ll need to define for the system We scale down each box so they are more separated when projected into our coarse voxel space. Creating top down visualizations of the ground truth and predictions using the nuScenes SDK. Our code will generate data visualization and model checkpoints they will be persisted to disk in this folder Disable multiprocesing for numpy opencv. The lidar pointcloud is defined in the sensor s reference frame. You can expect to train the model in a couple of hours on a modern GPU with inference times under 30ms per image. Training a network to segment objects1. Creating an index and splitting into train and validation scenes1. Performing a morphological closing operation to filter out tiny objects presuming they are false positives 7. N 1 H W N H W with class indices 0 1 N 2 H W Visualize the first prediction Get probabilities for non background Arbitrary threshold in our system to create a binary image to fit boxes around. We want it in the car s reference frame so we transform each point A sanity check the points should be centered around 0 in car space. You can interpret the above visualizations as follows There are four different visualizations stacked on top of eachother 1. The top images have two color channels red for predictions green for targets. Creating a dataframe with one scene per row. Training a network to segment objectsWe train a U net fully convolutional neural network we create a network that is less deep and with only half the amount of filters compared to the original U net paper implementation. 75 we can expect that to hurt the score. Thresholding the probability map. We re allocating 20GB of memory otherwise. The predictions thresholded at 0. We can then threshold this probability map and fit boxes around each of the detections. 3 N 4 N 4 3 We don t know the height of our boxes let s assume every object is the same height. Optional for multi GPU training and inference Only select the first n images We weigh the loss for the 0 class lower to account for some of the big class imbalance. As input for our network we voxelize the LIDAR points. We assume every object is 1. Making a submission. ", "id": "gzuidhof/reference-model", "size": "4801", "language": "python", "html_url": "https://www.kaggle.com/code/gzuidhof/reference-model", "git_url": "https://www.kaggle.com/code/gzuidhof/reference-model", "script": "Box __init__ lyft_dataset_sdk.lyftdataset prepare_training_data_for_scene multiprocessing UNetConvBlock(nn.Module) Rotation LyftDataset scale_boxes recall_precision draw_boxes transform_matrix lyft_dataset_sdk.eval.detection.mAP_evaluation tqdm_notebook center_crop move_boxes_to_car_space forward torch.nn numpy normalize_voxel_intensities BEVImageDataset(torch.utils.data.Dataset) __getitem__ PIL scipy.spatial.transform create_voxel_pointcloud LidarPointCloud load_groundtruth_boxes Quaternion visualize_lidar_of_sample UNet(nn.Module) functools Image matplotlib.pyplot visualize_predictions pandas UNetUpBlock(nn.Module) Box3D partial __len__ create_transformation_matrix_to_voxel_space tqdm car_to_voxel_coords view_points lyft_dataset_sdk.utils.geometry_utils torch.nn.functional get_unet_model Pool Rotation as R lyft_dataset_sdk.utils.data_classes transform_points datetime ", "entities": "(('rotation', 'elements'), 'size') (('we', 'model'), 'use') (('all', 'ego vehicle'), 'n') (('we', 'center pixel'), 's') (('we', 'lidar only one sweep'), 'use') (('these', 'RGB image'), 'separate') (('Alternatively we', 'scenes date'), 'consider') (('implementation', 'https github'), 'copy') (('You', 'eachother'), 'interpret') (('that', 'paper original U net implementation'), 'create') (('probabilities', 'annotations'), 'let') (('input', 'network'), 'be') (('them', 'world back space'), 'take') (('we', 'it'), 'have') (('Each', '3D box'), 'note') (('it', 'Drop z coord Don worry'), 'bring') (('You', 'image'), 'expect') (('points', 'car around 0 space'), 'transform') (('H 0 1 2 We', 'here memory'), '3') (('lidar pointcloud', 'reference frame'), 'define') (('we', 'LIDAR points'), 'voxelize') (('It', 'so that'), 'be') (('matplotlib', 'what'), 'take') (('1 we', 'maximum intensity'), 'let') (('s', 'validation set'), 'let') (('we', 'Kaggle Lyft Level 5 competition'), 'com') (('We', 'class big imbalance'), 'select') (('first prediction', 'boxes'), '1') (('Train Validation s', 'validation set'), 'splitLet') (('that', 'classes'), 'perform') (('semantic map', 'together targets'), 'blend') (('which', 'ego vehicle'), 'tall') (('they', 'tiny objects'), 'be') (('object', 'boxes'), '3') (('We', 'detections'), 'threshold') (('We', 'this'), 'do') (('number', 'runtime'), 'be') (('it', 'competition'), 'note') (('we', 'Z space'), 'mean') (('Model model', 'very poorly uncommon classes'), 'limitation') (('We', 'only 3 channels'), 'compress') (('they', 'voxel more when coarse space'), 'need') (('model they', 'Disable numpy opencv'), 'generate') (('other pixel', 'car'), 'network') (('We', 'memory'), 'allocate') (('which', 'lot'), 'multiprocess') (('car', 'image'), 's') (('top images', 'targets'), 'have') (('we', 'fit boxes'), 'be') (('we', 'segmentations'), 'fit') (('they', 'validation set'), 'have') ", "extra": "['annotation', 'test']", "label": "Perfect_files", "potential_description_queries": ["account", "adjust", "architecture", "background", "binary", "blended", "bottom", "box", "car", "care", "center", "channel", "check", "closing", "code", "color", "competition", "confidence", "consider", "context", "convolutional", "could", "count", "create", "data", "dataframe", "date", "define", "dimension", "disk", "end", "evaluate", "evaluation", "even", "every", "eye", "filter", "fit", "folder", "format", "frame", "generate", "green", "ground", "half", "height", "help", "image", "implementation", "index", "inference", "input", "kernel", "lead", "let", "lidar", "list", "load", "look", "lot", "lower", "map", "matplotlib", "matrix", "maximum", "mean", "memory", "model", "near", "need", "network", "neural", "next", "no", "non", "not", "number", "numpy", "object", "opening", "operation", "order", "out", "parallel", "per", "perform", "pixel", "point", "pointcloud", "predict", "prediction", "present", "probability", "projection", "provide", "pytorch", "range", "re", "reason", "reference", "remove", "resolution", "right", "runtime", "sample", "scale", "scaled", "scene", "script", "segment", "segmentation", "select", "sensor", "set", "size", "solution", "space", "split", "splitting", "system", "test", "think", "threshold", "time", "train", "training", "transform", "transformation", "try", "under", "up", "validation", "value", "view", "visualization", "volume", "voxel", "work", "world", "write"], "potential_description_queries_len": 140, "potential_script_queries": ["datetime", "detection", "eval", "forward", "multiprocessing", "nn", "partial", "spatial", "tqdm"], "potential_script_queries_len": 9, "potential_entities_queries": [], "potential_entities_queries_len": 0, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 147}