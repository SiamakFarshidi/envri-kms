{"name": "features selection for multiple linear regression ", "full_name": " h2 Import Advertising data h2 Is there a relationship between sales and advertising h2 Is at least one of the features useful in predicting Sales h2 How strong is the relationship h2 Summary h2 Which media contribute to sales h3 Forward selection h2 Plotting the model h2 Is there synergy among the advertising media ", "stargazers_count": 0, "forks_count": 0, "description": "com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. Now we have only three variables here. 9 which I can write as 0. Now we have a best model M1 which contains TV advertising. In other words it is clear that the true relationship is not additive. 26 0 0 Let s plot the actual values as red points and the model predictions as a cyan plane Is there synergy among the advertising media Adding radio to the model leads to a substantial improvement in R squared. Plotting the modelThe M2 model has two variables therefore can be plotted as a plane in a 3D chart. read_csv Input data files are available in the. But when advertising is split between the two media then the model tends to underestimate sales. Following is an example of features selection for the linear regression. Well the model with TV AND Radio greatly decreased RSS and increased R2 so that will be our M2 model. In marketing this is known as a synergy effect. Is at least one of the features useful in predicting Sales We use a hypothesis test to answer this question. Recall that we already fitted and evaluated a model with all features just at the beginning. Summarystatsmodels has a handy function that provides the above metrics in one single table One thing to note is that R squared will always increase when more variables are added to the model even if those variables are only weakly associated with the response. Unfortunately there are a total of 2 p models that contain subsets of p variables. For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. However this simple model may be incorrect. The quality of a linear regression fit is typically assessed using two related quantities the residual standard error RSE and the R squared statistic the square of the correlation of the response and the variable when close to 1 means high correlation. You can find more notebook examples around linear regression on my GitHub. 7 for the model M2 that predicts sales using TV and radio without an interaction term. 68 units while the mean value for the response is 14 022 indicating a percentage error of roughly 12. The task of determining which predictors are associated with the response in order to fit a single model involving only those predictors is referred to as variable feature selection. The p value for the interaction term TV radio is extremely low indicating that there is strong evidence for Ha \u03b23 not zero. The lowest RSS and the highest R2 are for the TV medium. M3 is slightly better than M2 but remember that R2 always increases when adding new variables so we call the approach completed and decide that the M2 model with TV and Radio is the good compromise. Next year no budget for newspaper advertising and that amount will be used for TV and Radio instead. We try here the forward selection. In this case F is far larger than 1 at least one of the three advertising media must be related to sales. 05 by chance this is the effect infamously leveraged by the so called p hacking. The F statistic does not suffer from this problem because it adjusts for the number of predictors. This approach is continued until some stopping rule is satisfied. 8 compared to only 89. Instead we can use other approaches. Forward selectionWe start with a null model no features we then fit three p 3 simple linear regressions and add to the null model the variable that results in the lowest RSS. The three classical ways are the forward selection start with no features and add one after the other until a threshold is reached the backward selection start with all features and remove one by one and the mixed selection a combination of the two. 9 0 Its normal is 0. Thus we have to calculate d and we re set dot product create x y calculate corresponding z. Second the R2 statistic records the percentage of variability in the response that is explained by the predictors. The M2 model can be described by this equation Sales 0. In contrast the coefficient for newspaper represents the average effect negligible of increasing newspaper spending by 1000 dollars while holding TV and radio fixed. But as just seen if p is large then we are likely to make some false discoveries. It seems likely that if any one of the p values for the individual features is very small then at least one of the predictors is related to the response. But be careful looking only at these individual p values instead of looking at the overall F statistic. In this situation given a fixed budget of 100K spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or to radio. Therefore an adjusted R squared is provided which is R squared adjusted by the number of predictors. The distance from any point in a collection of data to the mean of the data is the deviation. We can then select the best model out of all of the models that we have considered for example the model with the smallest RSS and the biggest R squared other used metrics are the Akaike Information Criterion AIC Bayesian Information Criterion BIC and the adjusted R2. This hypothesis test is performed by computing the F statistic Now we need the Total Sum of Squares TSS the total variance in the response Y and can be thought of as the amount of variability inherent in the response before the regression is performed. How strong is the relationship Once we have rejected the null hypothesis in favour of the alternative hypothesis it is natural to want to quantify the extent to which the model fits the data. Another thing to note is that the summary table provides also a t statistic and a p value for each single feature. All of them are visible in the summary model. The multiple linear regression model takes the form Sales \u03b20 \u03b21 TV \u03b22 Radio \u03b23 Newspaper \u03b5 where Beta are the regression coefficients we want to find and epsilon is the error that we want to minimise. com Mashimo datascience This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. Which media contribute to sales To answer this question we could examine the p values associated with each predictor s t statistic. we need first to calculate the Residual Sum of Squares RSS mean of sales we have three predictors TV Radio and Newspaper we have 200 data points input samples a plane is a x b y c z d 0 a b c is the normal. We interpret these results as follows for a given amount of TV and newspaper advertising spending an additional 1000 dollars on radio advertising leads to an increase in sales by approximately 189 units. The dataset contains statistics about the sales of a product in 200 different markets together with advertising budgets in each of these markets for different media channels TV radio and newspaper. The results strongly suggest that the model that includes the interaction term is superior to the model that contains only main effects. The F statistic is the ratio between TSS RSS p and RSS n p 1 When there is no relationship between the response and predictors one would expect the F statistic to take on a value close to 1. 1 1 https github. Mathematically this corresponds to testing H0 \u03b21 \u03b22 \u03b23 \u03b24 0 versus Ha at least one \u03b2i is non zero. Notice that when levels of either TV or radio are low then the true sales are lower than predicted by the linear model. 7 69 of the variability in sales that remains after fitting the additive model has been explained by the interaction term. This implies that a model that uses TV and radio expenditures to predict sales is substantially better than one that uses only TV advertising. Suppose that spending money on radio advertising actually increases the effectiveness of TV advertising so that the slope term for TV should increase as radio increases. We can decide to stop at M2 or use an M3 model with all three variables. The most common hypothesis test involves testing the null hypothesis of H0 There is no relationship between the media and sales versus the alternative hypothesis Ha There is some relationship between the media and sales. In the multiple linear regression above the p values for TV and radio are low but the p value for newspaper is not. Adding the newspaper could possibly overfits on new test data. For three predictors it would still be manageable only 8 models to fit and evaluate but as p increases the number of models grows exponentially. The figure above suggests that such an effect may be present in the advertising data. A linear model that uses radio TV and an interaction between the two to predict sales takes the form sales \u03b20 \u03b21 TV \u03b22 radio \u03b23 radio TV \u03b5We can interpret \u03b23 as the increase in the effectiveness of TV advertising for a one unit increase in radio advertising or vice versa. These provide information about whether each individual predictor is related to the response high t statistic or low p value. We then add to this M1 model the variable that results in the lowest RSS for the new two variable model. Ideally we could perform the variable selection by trying out a lot of different models each containing a different subset of the features. However this logic is flawed especially when you have many predictors statistically about 5 of the p values will be below 0. 05 1 and a point on the plane is 2. You may be interested in answering questions such as which media contribute to sales Do all three media TV radio and newspaper contribute to sales or do just one or two of the media contribute Import Advertising data Is there a relationship between sales and advertising First of all we fit a regression line using the Ordinary Least Square algorithm i. On the other hand if Ha is true then we expect F to be greater than 1. the line that minimises the squared differences between the actual Sales and the line itself. It is based on the Advertising Dataset taken from the masterpiece book Introduction to Statistical Learning by Hastie Witten Tibhirani James. The R squared for this model is 96. This suggests that only TV and radio are related to sales. The predictors explain almost 90 of the variance in sales. Imaging being the Marketing responsible and you need to prepare a new advertising plan for next year. ", "id": "mashimo/features-selection-for-multiple-linear-regression", "size": "10077", "language": "python", "html_url": "https://www.kaggle.com/code/mashimo/features-selection-for-multiple-linear-regression", "git_url": "https://www.kaggle.com/code/mashimo/features-selection-for-multiple-linear-regression", "script": "evaluateModel Axes3D numpy matplotlib.pyplot mpl_toolkits.mplot3d statsmodels.formula.api pandas subprocess check_output ", "entities": "(('lowest RSS', 'TV highest medium'), 'be') (('F', 'close 1'), 'be') (('hypothesis most common test', 'media'), 'involve') (('We', 'question'), 'be') (('distance', 'data'), 'be') (('this', 'synergy effect'), 'know') (('that', 'interaction term'), 'explain') (('F', '1'), 'expect') (('each', 'features'), 'perform') (('It', 'python docker image https kaggle github'), 'com') (('We', 'three variables'), 'decide') (('at least one', 'sales'), 'be') (('All', 'summary model'), 'be') (('biggest R', 'other used metrics'), 'select') (('that', 'predictors'), 'record') (('individual predictor', 'response'), 'provide') (('predictors', 'sales'), 'explain') (('above effect', 'advertising data'), 'suggest') (('You', 'GitHub'), 'find') (('68 mean value', 'roughly 12'), 'unit') (('only TV', 'sales'), 'suggest') (('M2 model', 'TV'), 'be') (('amount', 'TV'), 'use') (('we', 'that'), 'take') (('p value', 'newspaper'), 'be') (('that', 'new two variable model'), 'add') (('model', 'data'), 'be') (('We', 'approximately 189 units'), 'interpret') (('M2 model', 'equation'), 'describe') (('then true sales', 'linear model'), 'notice') (('true relationship', 'other words'), 'be') (('then model', 'sales'), 'tend') (('p value', 'Ha extremely low strong \u03b23'), 'be') (('two variables', '3D chart'), 'have') (('which', 'TV advertising'), 'have') (('that', 'p variables'), 'be') (('you', 'output'), 'list') (('summary table', 'p single feature'), 'be') (('at least one \u03b2i', '0 Ha'), 'correspond') (('that', 'only main effects'), 'suggest') (('even variables', 'only weakly response'), 'have') (('\u03b20 \u03b21 TV \u03b22 radio \u03b23 radio TV \u03b5We', 'radio advertising'), 'take') (('R', 'predictors'), 'provide') (('coefficient', 'TV'), 'represent') (('we', 'just beginning'), 'recall') (('Adding', 'R'), 'let') (('regression', 'inherent response'), 'perform') (('half', 'radio'), 'increase') (('when close to 1', 'high correlation'), 'assess') (('read_csv Input data files', 'the'), 'be') (('this', 'p infamously so called hacking'), '05') (('it', 'predictors'), 'suffer') (('that', 'lowest RSS'), 'start') (('newspaper', 'test possibly new data'), 'add') (('dataset', 'media channels TV different radio'), 'contain') (('increased that', 'greatly RSS'), 'decrease') (('dot product y', 'corresponding z.'), 'have') (('then we', 'false discoveries'), 'be') (('It', 'Hastie Witten Tibhirani James'), 'base') (('x c z b 0 c', 'data points input 200 samples'), 'need') (('order', 'feature variable selection'), 'refer') (('that', 'TV only advertising'), 'imply') (('that', 'interaction term'), '7') (('p still only 8 number', 'models'), 'be') (('statistically about 5', '0'), 'be') (('we', 'Least Square algorithm Ordinary i.'), 'be') (('that', 'actual Sales'), 'line') (('very then at least one', 'response'), 'seem') (('responsible you', 'next year'), 'image') (('slope term', 'radio increases'), 'suppose') (('we', 'predictor'), 'examine') (('backward selection', 'two'), 'be') (('Following', 'linear regression'), 'be') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["algorithm", "answer", "approach", "associated", "average", "backward", "best", "book", "calculate", "call", "case", "clear", "close", "coefficient", "collection", "contain", "contrast", "correlation", "could", "create", "current", "data", "dataset", "directory", "distance", "dot", "effect", "environment", "epsilon", "equation", "error", "evaluate", "even", "evidence", "explained", "extent", "feature", "figure", "file", "find", "fit", "fitting", "fixed", "form", "forward", "function", "half", "hand", "high", "https github", "image", "improvement", "increase", "individual", "input", "kaggle", "least", "line", "linear", "list", "load", "looking", "lot", "lower", "main", "mean", "mixed", "model", "most", "multiple", "my", "need", "new", "next", "no", "non", "normal", "not", "notebook", "null", "number", "order", "out", "overall", "percentage", "perform", "plane", "plot", "point", "predict", "predictor", "prepare", "present", "problem", "processing", "product", "provide", "python", "question", "ratio", "re", "regression", "relationship", "remove", "residual", "response", "run", "running", "select", "selection", "set", "several", "single", "situation", "split", "square", "squared", "standard", "start", "subset", "summary", "table", "task", "term", "test", "testing", "those", "thought", "threshold", "total", "try", "unit", "until", "value", "variability", "variable", "variance", "while", "write", "year"], "potential_description_queries_len": 140, "potential_script_queries": ["formula", "numpy"], "potential_script_queries_len": 2, "potential_entities_queries": ["close", "image", "kaggle", "main", "most", "product", "variable"], "potential_entities_queries_len": 7, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 142}