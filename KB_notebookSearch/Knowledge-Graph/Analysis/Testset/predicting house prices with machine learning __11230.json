{"name": "predicting house prices with machine learning ", "full_name": " h1 Predicting House Prices with Machine Learning h2 Introduction h2 0 Understanding the Client and their Problem h2 1 Loading Data and Packages h2 2 Analyzing the Test Variable Sale Price h2 3 Multivariable Analysis h2 4 Impute Missing Data and Clean Data h3 Imputing Missing Values h2 5 Feature Transformation Engineering h3 Fixing skewed features h2 6 Modeling and Predictions h3 Stacked models h3 XGBoost h3 LightGBM h3 Ensemble Prediction h3 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "Functional data description says NA means typical. Here we fix all of the skewed data to be more normal so that our models will be more accurate when making predictions. BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath and BsmtHalfBath Replacing missing data with 0. What People pay more for better quality Nothing new here. Like the features that make up a person an educated party would want to know all aspects that give a house its value. Let s combine both training and test data into one dataset to impute missing values and do some cleaning. They have their locations of interest ready. August 2017 IntroductionThis notebook is going to be focused on solving the problem of predicting house prices for house buyers and house sellers. Is it because of inflation or stock market crashes Let s leave the years alone. GarageType GarageFinish GarageQual and GarageCond Replacing missing data with None. Client Houseseller Think of the average house flipper. Load Data and Packages2. So we can fill in missing values with RL. Multivariable AnalysisLet s check out all the variables There are two types of features in housing data categorical and numerical. I had to take a double take at a point since I consider myself a house browsing enthusiast With an average house price of 180921 it seems like I should relocated to Iowa Looks like a normal distribution Not quite Looking at the kurtosis score we can see that there is a very nice peak. Electrical It has one NA value. GrLivArea Above grade ground living area square feet3. Multivariable Analysis4. With this study they can understand which features ex. In our previous example we could tell that our categories don t follow a particular order. Here we stack the models to average their scores. For the other 9 they are as listed. 02 point increase in the Pearson R Score. GarageCars Size of garage in car capacity4. With 81 features how could we possibly tell which feature is most related to house prices Good thing we have a correlation matrix. Again with the bottom two data points. We can then safely remove it. Number of bathrooms location etc. However looking at the skewness score we can see that the sale prices deviate from the normal distribution. Sale Price Of course. The client will invest on making rooms at a small cost to get a large return. Impute Missing Data and Clean Data5. We are going to break everything into logical steps that allow us to ensure the cleanest most realistic data for our model to make accurate predictions from. The types listed here are codes not values. Let s engineer one feature to combine square footage this may be useful later on. TotRmsAbvGrd Total rooms above grade does not include bathrooms 9. Who could have thought These features are in a linear relationship with each other. MSSubClass Identifies the type of dwelling involved in the sale. A house value is simply more than location and square footage. We can replace missing values with None 5. It seems like houses with more than 11 rooms come with a 100k off coupon. Going to have to fix this later We want our data to be as normal as possible. We ll add in XGBoost and LightGBM later. What does that mean Is a 90 type 3 times better than a 30 type This feature was interpreted as numerical when it is actually categorical. Although it seems like house prices decrease with age we can t be entirely sure. OverallQual Rates the overall material and finish of the house 1 Very Poor 10 Very Excellent 2. Check data size after dropping the Id variable Getting Description Plot Histogram Get the fitted parameters used by the function Checking Categorical Data Checking Numerical Data Correlation Matrix Heatmap Top 10 Heatmap number of variables for heatmap Overall Quality vs Sale Price Living Area vs Sale Price Removing outliers manually Two points in the bottom right Living Area vs Sale Price Garage Area vs Sale Price Removing outliers manually More than 4 cars less than 300k Garage Area vs Sale Price Garage Area vs Sale Price Removing outliers manually More than 1000 sqft less than 300k Garage Area vs Sale Price Basement Area vs Sale Price First Floor Area vs Sale Price Total Rooms vs Sale Price Total Rooms vs Sale Price Combining Datasets Find Missing Ratio of Dataset Percent missing data by feature Check if there are any missing values left MSSubClass The building class Changing OverallCond into a categorical variable Year and month sold are transformed into categorical features. Exterior1st and Exterior2nd Both Exterior 1 2 have only one missing value. It makes sense that people would pay for the more living area. What doesn t make sense is the two datapoints in the bottom right of the plot. We will just substitute in the most common string SaleType Fill in again with most frequent which is WD MSSubClass Na most likely means No building class. Numerical data is data in number form. We are going to take advantage of all of the feature variables available to use and use it to analyze and predict house prices. MasVnrArea and MasVnrType NA most likely means no masonry veneer for these houses. Now they want to know if the house price matches the house value. KitchenQual Only one NA value and same as Electrical we set TA which is the most frequent for the missing value in KitchenQual. The test set has 1459 rows and 80 features. Feature Transformation EngineeringLet s take a look at some features that may be misinterpreted to represent something it s not. Note removal of data is totally discretionary and may or may not help in modeling. Analyzing the Test Variable Sale Price 3. Feature Transformation Engineering6. Predicting House Prices with Machine Learning Eric Kim B. This makes sense because we are trying to predict it 2. Loading Data and PackagesSo the training set has 1460 rows and 81 features. It looks like an outlier but I ll let it slide. Moreover from a substantive perspective we need to ensure that the missing data process is not biased and hiding an inconvenient truth. Utilities For this categorical feature all records are AllPub except for one NoSeWa and 2 NA. TotalBsmtSF Total square feet of basement area6. 1stFlrSF First Floor square feet7. We need to take care of this What we will do is remove these outliers manually. This can prevent us from proceeding with the analysis. Let s remove those outliers. For example take a feature of Downtown. Since this feature has mostly SBrkr we can set that for the missing value. However today that is not the case. We can fill 0 for the area and None for the type. Checking performance of base models by evaluating the cross validation RMSLE error. Since the house with NoSewa is in the training set this feature won t help in predictive modelling. Let s zoom into the top 10 features most related to Sale Price. GarageArea Size of garage in square feet5. Impute Missing Data and Clean DataImportant questions when thinking about missing data How prevalent is the missing data Is missing data random or does it have a pattern The answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. Thus we need to feature transformation with this and many other features. From looking at the head of both sets we can see that the only difference in features is Sale Price. 20 1 STORY 1946 NEWER ALL STYLES 30 1 STORY 1945 OLDER 40 1 STORY W FINISHED ATTIC ALL AGES 45 1 1 2 STORY UNFINISHED ALL AGES 50 1 1 2 STORY FINISHED ALL AGES 60 2 STORY 1946 NEWER 70 2 STORY 1945 OLDER 75 2 1 2 STORY ALL AGES 80 SPLIT OR MULTI LEVEL 85 SPLIT FOYER 90 DUPLEX ALL STYLES AND AGES 120 1 STORY PUD Planned Unit Development 1946 NEWER 150 1 1 2 STORY PUD ALL AGES 160 2 STORY PUD 1946 NEWER 180 PUD MULTILEVEL INCL SPLIT LEV FOYER 190 2 FAMILY CONVERSION ALL STYLES AND AGESSo the average is a 57 type. Well the most correlated feature to Sale Price is. 01 point Pearson R Score increase but looks much better Everything looks fine here. The response is either Near Far Yes and No. txt comes to the rescue again Kitchen Quality Ex Excellent Gd Good TA Typical Average Fa Fair Po PoorIs a score of Gd better than TA but worse than Ex I think so let s encode these labels to give meaning to their specific orders. 4 car garages result in less Sale Price That doesn t make much sense. Modeling and PredictionsFor our models we are going to use lasso elastic net kernel ridge gradient boosting XGBoost and LightGBM regression. Process columns and apply LabelEncoder to categorical features Check shape Adding Total Square Feet feature We use the numpy fuction log1p which applies log 1 x to all elements of the column Check the new distribution Get the fitted parameters used by the function Check the skew of all numerical features Cross validation with k folds we define clones of the original models to fit the data in Train cloned base models Now we do the predictions for cloned models and average them We again fit the data on clones of the original models Train cloned base models then create out of fold predictions that are needed to train the cloned meta model Now train the cloned meta model using the out of fold predictions Example. Modeling and Predictions 0. It isn t necessarily linear but it follows some kind of pattern. If all matches they can ensure that they are getting a fair price. Here is a short description of each. influence the final price of the house. They typically want to buy a house at a low price and invest on the features that will give the highest return. Imputing Missing Values PoolQC data description says NA means No Pool MiscFeature data description says NA means no misc feature Alley data description says NA means no alley access Fence data description says NA means no fence FireplaceQu data description says NA means no fireplace LotFrontage Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood we can fill in missing values by the median LotFrontage of the neighborhood. The location is extremely correlated with Sale Price. That looks much better. Here we average ENet GBoost KRR and lasso. Use at your own preference. This client wants to take advantage of the features that influence a house price the most. Thank you hipsters So we can t really establish any particular order of response to be better or worse than the other. What about categories that do Let s take a look at Kitchen Quality. Fixing skewed features. For example a 2 000 square foot place is 2 times bigger than a 1 000 square foot place. Stacked models XGBoost LightGBM Ensemble PredictionNote To get our weights for each model we ll take the inverse of each regressor and average it out of 100 Submission set png here when working on notebook Load train and Test set Check the numbers of samples and features Save the Id column Now drop the Id column since it s unnecessary for the prediction process. BsmtQual BsmtCond BsmtExposure BsmtFinType1 and BsmtFinType2 For all these categorical basement related features NaN means that there isn t a basement. For example buying a house at a good location but small square footage. GarageYrBlt GarageArea and GarageCars Replacing missing data with 0. Categorical data is just like it sounds. Thank you data_description. Back then living in downtown usually meant that you couldn t afford to live in uptown. Thus it could be implied that downtown establishments cost less to live in. FullBath Full bathrooms above grade8. YearBuilt Original construction dateLet s take a look at how each relates to Sale Price and do some pre cleaning on each feature if necessary. Here data_description. Let s do it It s a nice overview but oh man is that a lot of data to look at. Since our lasso model performed the best we ll use it as a meta model. Analyzing the Test Variable Sale Price Let s check out the most interesting feature in this study Sale Price. Understanding the Client and their ProblemA benefit to this study is that we can have two clients at the same time Think of being a divorce lawyer for both interested parties However in this case we can have both clients with no conflict of interest Client Housebuyer This client wants to find their next dream home with a reasonable price tag. MSZoning The general zoning classification RL is by far the most common value. Important Note This data is from Ames Iowa. ", "id": "erick5/predicting-house-prices-with-machine-learning", "size": "11230", "language": "python", "html_url": "https://www.kaggle.com/code/erick5/predicting-house-prices-with-machine-learning", "git_url": "https://www.kaggle.com/code/erick5/predicting-house-prices-with-machine-learning", "script": "sklearn.metrics __init__ cross_val_score rmsle lightgbm Ridge ExtraTreesRegressor boxcox1p predict LassoCV LassoLarsIC scipy.stats.stats sklearn.kernel_ridge skew clone Lasso Normalizer StackingAveragedModels(BaseEstimator collections mean_squared_error seaborn numpy sklearn.pipeline AdaBoostRegressor RandomForestRegressor TransformerMixin) learning_curve GradientBoostingRegressor scipy.stats make_pipeline sklearn.base sklearn.ensemble sklearn.model_selection LabelEncoder KFold rmsle_cv matplotlib.pyplot BayesianRidge AveragingModels(BaseEstimator stats pandas norm pearsonr LassoLarsCV StandardScaler Counter RobustScaler ElasticNet fit RegressorMixin scipy ElasticNetCV GridSearchCV sklearn.linear_model sklearn.preprocessing scipy.special BaseEstimator xgboost TransformerMixin train_test_split LinearRegression KernelRidge ", "entities": "(('2 foot 000 square place', '1 foot 2 times 000 square place'), 'be') (('make', 'plot'), 'be') (('only difference', 'features'), 'from') (('client', 'large return'), 'invest') (('s', 'cleaning'), 'let') (('which', 'WD MSSubClass building most likely class'), 'substitute') (('data', 'this'), 'go') (('stock market s', 'years'), 'be') (('this', 'square footage'), 'let') (('training set', '1460 rows'), 'Loading') (('they', 'fair price'), 'ensure') (('house price', 'house value'), 'want') (('s', 'study Sale Price'), 'let') (('We', 'None'), 'replace') (('we', 'neighborhood'), 'say') (('we', 'age'), 'be') (('data missing process', 'inconvenient truth'), 'need') (('doesn', 'much sense'), 'result') (('SPLIT 190 FAMILY MULTILEVEL INCL LEV 2 average', '70 1 2 1945 75 2 2 STORY'), 'story') (('usually you', 'uptown'), 'mean') (('Note removal', 'totally modeling'), 'be') (('OverallQual', 'house'), 'rate') (('we', 'meta model'), 'perform') (('Here we', 'scores'), 'stack') (('records', 'one NoSeWa'), 'be') (('which', 'ex'), 'understand') (('s', 'Kitchen Quality'), 'about') (('missing data', 'sample size'), 'question') (('it', 'something'), 'take') (('necessarily it', 'pattern'), 'linear') (('houses', 'coupon'), 'seem') (('pre', 'feature'), 'dateLet') (('sale prices', 'normal distribution'), 'see') (('house', 'predictive modelling'), 'win') (('it', 'prediction process'), 'model') (('Multivariable AnalysisLet', 'housing data'), 'check') (('which', 'KitchenQual'), 'value') (('categories', 'don particular order'), 'tell') (('we', 'kurtosis quite score'), 'have') (('test set', '1459 rows'), 'have') (('that', 'value'), 'like') (('that', 'house price'), 'want') (('most correlated feature', 'Sale Price'), 'be') (('GarageYrBlt GarageArea', '0'), 'replace') (('we', 'outliers'), 'be') (('Total rooms', 'bathrooms'), 'totrmsabvgrd') (('We', 'house prices'), 'go') (('it', 'outlier'), 'look') (('response', 'Far'), 'be') (('we', 'XGBoost'), 'go') (('s', 'Sale most related Price'), 'let') (('we', 'missing value'), 'set') (('So we', 'RL'), 'fill') (('features', 'other'), 'think') (('client', 'price reasonable tag'), 'understand') (('we', '2'), 'make') (('we', 'correlation matrix'), 'feature') (('This', 'analysis'), 'prevent') (('Important data', 'Ames Iowa'), 'Note') (('that', 'predictions fold Example'), 'column') (('more models', 'more when predictions'), 'fix') (('Thus we', 'this'), 'need') (('house value', 'simply location'), 'be') (('so s', 'specific orders'), 'come') (('that', 'highest return'), 'want') (('hipsters So we', 'other'), 'thank') (('downtown establishments', 'less'), 'imply') (('We', 'type'), 'fill') (('GarageType GarageFinish GarageQual', 'None'), 'replace') (('zoning classification general RL', 'most common value'), 'mszone') (('locations', 'interest'), 'have') (('Here we', 'GBoost KRR'), 'average') (('IntroductionThis 2017 notebook', 'house buyers'), 'August') (('missing values', 'categorical features'), 'check') (('location', 'Sale extremely Price'), 'correlate') (('Exterior', '1 2 only one missing value'), 'Exterior1st') (('that', 'data'), 'let') (('MasVnrArea', 'houses'), 'mean') (('numerical when it', '90 3 times better 30 type'), 'be') (('people', 'more living area'), 'make') (('People', 'Nothing'), 'pay') (('us', 'accurate predictions'), 'go') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["advantage", "age", "analyze", "answer", "apply", "area", "average", "basement", "best", "boosting", "bottom", "car", "care", "case", "categorical", "check", "classification", "cleaning", "client", "column", "combine", "consider", "correlation", "cost", "could", "create", "data", "dataset", "define", "description", "difference", "distribution", "double", "drop", "encode", "ensure", "everything", "feature", "fill", "final", "find", "fit", "fix", "fold", "frequent", "function", "general", "grade", "gradient", "ground", "head", "heatmap", "help", "house", "impute", "include", "increase", "inflation", "influence", "interest", "kernel", "leave", "left", "let", "linear", "log", "look", "looking", "lot", "market", "mean", "meaning", "median", "meta", "misc", "missing", "model", "month", "most", "need", "neighborhood", "new", "next", "no", "normal", "not", "notebook", "number", "numerical", "numpy", "order", "out", "outlier", "overall", "overview", "pattern", "people", "performance", "person", "place", "png", "point", "pre", "predict", "prediction", "prevent", "price", "problem", "property", "random", "relationship", "remove", "replace", "response", "result", "right", "sample", "score", "sense", "set", "shape", "short", "similar", "size", "skew", "something", "square", "stack", "string", "test", "think", "those", "thought", "time", "train", "training", "transformation", "type", "up", "validation", "value", "variable", "zoom"], "potential_description_queries_len": 143, "potential_script_queries": ["clone", "lightgbm", "norm", "pearsonr", "rmsle", "scipy", "seaborn", "sklearn", "xgboost"], "potential_script_queries_len": 9, "potential_entities_queries": ["fold", "market", "most"], "potential_entities_queries_len": 3, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 148}