{"name": "i commonlit explore xgbrf repeatedfold model ", "full_name": " h1 CommonLit Target Understanding and Text FE h2 The problem is more complex than you think h1 1 Introduction h3 Libraries h3 Custom Functions Below h1 2 The Data h1 3 The Target h1 4 The Error h3 Story Time h2 Understanding the Standard Error h2 Target vs Error Comparison h2 Target Segmentation h3 We ll segment the target into 3 groups h1 5 The Word Embeddings h3 What does tokenization mean h2 I Doc2Vec h2 II SentenceBERT h1 6 Text Preprocessing h1 7 English Word Frequency Model Submission h2 I Data Preprocessing h2 II Create More Features h2 III RAPIDS XGBoost h2 IV XGBRF Cross Validation h1 My Specs ", "stargazers_count": 0, "forks_count": 0, "description": "Now try for another one. Custom Functions Below 2. So the coders dissagreed. How hard it is and how much dissagreement is between the coders or raters. There is subjectivity in our text too. What does this mean It means that if in the test set we ll have more examples that have the target value more towards the end of the histogram then our Machine Learning might have even more troubles classifying them. so you don t have to IV. RAPIDS XGBoostI will try just a basic XGBoost as it is usually the best performing one. This one looks simple in terms of understanding the problem goal and competition metric. ai andrada commonlit workspace user andrada Learn more on why and how to use W B here Experiment Tracking using W B https www. pkl rb X_test transformer. Note The target in our case has a distribution very close to normal. com analytics vidhya text classification using word embeddings and deep learning in python classifying tweets from 6fe644fcfc81. On a second look there s more to it than it allows to show on the surface. It means that the word_frequencies dataset is actually helpful Yay This is a big improvement The RMSE dropped from a value of 2. Luckly we don t have to do much in this case. If humans dissagreed half a point on so many paragraphs how is our AI going to perform Target vs Error Comparison Note How do we interpret this plot When the target is 1 so the complexity is quite neutral the error decreases a little. XGBRF Cross ValidationLet s try a different approach. ai Link to my W B Dashboard here https wandb. Create More FeaturesNow we can create some features from the new created dataset. csv so you won t get an error CSS STYLE Libraries GPU Libraries Environment check Secrets Custom colors Set Style Cosine Similarity Get similarity between 2 vectors. Now let s take 2 Word Embeddings one by one as our possible methods for the models. 5 on a normal rating. This will later help us understand how long we ll need to make the embeddings. from the text frequencies Get more info from text itself Scale these features as they are HUGE TFIDF Vectorizer Create final X variable containing all info This is how our data looks now Libraries for models Basic Data Validation Create DMatrix is optimized for both memory efficiency and training speed. Note If this line throws an error try using wandb. Coders are people that are qualified to assess images and follow a strict set of rules to classify if for example in an image the person is happy or sad. This way we can choose afterwards which one to choose when creating the more complex models more details are coming in my second notebook. Some could find a paragraph being a bit more easy than others. 0 and the error as well. I will try to improve this one a bit but I was just curious to see if the dataset would help. beautifly beautiful beautify beautification beautiful 7. So now our distribution would look like this Ok now let s look at the standard_error in terms of segmentation. We ll segment the target into 3 groups high complexity medium complexity low complexity We ll follow the natural distribution of the histogram and we ll segment the data into 3 thirds. The TargetOur target variable starts at 3. English Word Frequency Model SubmissionI was super curious to see if I would create a SUPER simple model using only some basic features from text the word frequencies dataset here https www. Let s look at a few more examples of text and the target difficutly that it was given 4. is the test set following the same pattern The CV technique in this competition will prove very valuable. 55 Log plot into W B Plot Arrow Log plot into W B Create segments Plot Plot show_values_on_bars ax h_v v space 1 Log into W B Plot Log plot into W B Tokenize each paragraph Represents a document along with a tag Train model Example of new paragraph Encode the paragraphs Test with an example Print the most similar sentence to our example Tokenize convert to lower case Remove punctuation non alphabetic characters from each word Filter out stopwords Lemmatizer Example Apply to the entire text This cell was taking me 3 hours to run on how I wrote the code very poorly But adityaecdrid came to the resque with this amazing script Now it runs in less than a second English Word Frequencies Dataset Convert it into a dict i. To test how effective is our Embedding method Read in data Plot Save data to W B Dashboard Plot Log plot into W B Plot sns. The RMSE is huge considering the fact that we have numbers between 3 and 2. It will ask for the API key to login which you can get from your W B profile click on Profile Settings scroll to API keys. The texts are very clean as they re paragraphs from academia. The Error Story TimeLet s forget for a second about the classification problem and the fact that we want to train an AI to distinguish between a more complex text and a rather easy one. Understanding the Standard ErrorOur error is very skewed to the left. To use it for inference follow the code below transformer TfidfTransformer saved CountVectorizer decode_error replace vocabulary pickle. kdeplot train standard_error fill my_colors 0 color my_colors 0 lw 0. as the answer becomes subjective. Quite long I would say. Text PreprocessingAnother thing to be done besides words embeddings is preprocessing our text in a manner that it would be much cleaner and easier for the models to digest. Usually a face can express many more feelings like happiness love disgust and a little bit of surprise in the same time. I ll try to improve the model by choosing a different method making a simple RepeatedKFold on the data. Libraries RAPIDS info here https rapids. CommonLit Target Understanding and Text FEThe problem is more complex than you think 1. Data PreprocessingWe ll use parallelization to append to each word the frequency from the english_word_frequency dataset. Competition Goal Rating the complexity of literary passages from grades 3 to 12. Note We can see that freq_sum freq_min and freq_mean are the most important features although we have more than 11 700 columns for the words in our texts. IntroductionYet another amazing competition brought by Kaggle My mother is a teacher and I know the struggle of keeping kids involved and interested in reading so I can say this competition is a bit closer to my heart. You can observe that indeed the error decreases a little for medium complexity. After our first experiment the W B Dashboard looks like this Target SegmentationNow let s look a bit at the standard_error in terms of segmentation. If we ll choose to create embeddings of 100 numbers a vector and we have 200 words in a paragraph and we have 2 834 paragraphs then we ll end up with and object of size 100 x 200 x 2 834 56 680 000. In front of such picture even the best rules and the most skilled coders can fail. However I wanted to see how a very simple baseline would perform. 71 which is the lowest difficulty dinosaurs and pretty things. Let s see how this fares We can also look at feature importance to see which features out of the ones we ve already created are the most important. In this case we ll have an error. 93 if you have any questions on how to submit don t hesitate to ask don t forget to name your submission submission. Let s assume that WE as people not as Data Scientists need to rate these texts by hand. What does tokenization mean Tokenization is a fancy word for saying splitting sequences into words. However we can still make some adjustments lower casing all words removing punctuation filtering stopwords lemmatization of the tokens bringing the word to the root e. And who can blame the machine if even the humans are in such dissagreement We also have a little guy completely off charts. Meaning that we have MANY texts with more than 0. This competition differenciates itself from others because there is only 1 feature to be used the text which can be highly subjective. However we can state that usually we ll encounter a standard_error of 0. load open tfidfvectorizer. Note I also chose to use a TfIdfVectorizer for the text feature to add more information to the models. Note My Inspo from this article https www. Oooook so we looked at the target and its error to observe what we re dealing with in terms of text. Create Train the model Make prediction TRAIN Save the model Libraries for models Convert data to CPU Create Folds TRAIN Save the model Plot. My Specs Z8 G4 Workstation 2 CPUs 96GB Memory NVIDIA Quadro RTX 8000 RAPIDS version 0. We don t want to turn into a smoothie after the shakeup as Laura Fink https www. com rtatman english word frequency what would be the RMSE score Using embeddings and more advanced techniques will MOST DEFINITELY render better results. The Data Let s observe the structure of the data first 3. Is the second one simpler or more complex And how big is the difference between the 2 For picking up emotion in images expressions like anger fear sadness coders are trained to score the images. fit_transform X_test III. com ayuraj experiment tracking with weights and biases. There is no correlation between the target and standard_error. com blog 2020 08 top 4 sentence embedding techniques using python and this one https medium. hashmap Tokenize full text Get word count for each word Save data to W B Dashboard Get sum mean std etc. Note It seems that our paragraphs have between 140 and 210 words. It is an outlier with the complexity set to 0. 17 Leaderboard And the leaderboard score for the XGBRF Model using Repeated Folds is 0. So I saved the trained information into a pickle. However at the ends of the distribution the standard error increases slightly meaning that there is more dissagreement between coders in these cases. The target might be missleading as well as it might behave differently in the test data than what we see in the training data. The Word Embeddings What is a Word Embedding Embedding words is the process of vectorizing text meaning that we re changing the characters we understand to numbers that the computer can understand. 67 the highest possible difficulty text I can t understand myself and stops at 1. But problems are NEVER that easy. Try to give a score to one of the texts. Bonus So we can t retrain the TfIdfVectorizer when we submit the data. Let s first see how many words we usually have in a paragraph. However don t be rush in throwing it into a model. 82 I would call this a win especially because we didn t really do much to our dataset. ", "id": "andradaolteanu/i-commonlit-explore-xgbrf-repeatedfold-model", "size": "10664", "language": "python", "html_url": "https://www.kaggle.com/code/andradaolteanu/i-commonlit-explore-xgbrf-repeatedfold-model", "git_url": "https://www.kaggle.com/code/andradaolteanu/i-commonlit-explore-xgbrf-repeatedfold-model", "script": "show_values_on_bars sklearn.metrics TaggedDocument train_xgb_model stopwords WordNetLemmatizer create_wandb_hist UserSecretsClient TfidfVectorizer css_styling Doc2Vec XGBRFRegressor matplotlib.patches gensim.models.doc2vec _show_on_single_plot seaborn numpy mean_squared_error sklearn.feature_extraction.text scipy.stats color wandb_callback save_dataset_artifact IPython.core.display sklearn.model_selection cuml.preprocessing.model_selection XGBRegressor matplotlib.pyplot cosine_similarity SentenceTransformer pandas word_tokenize nltk.stem pearsonr StandardScaler wandb.xgboost cuml.metrics sentence_transformers nltk.tokenize nltk.corpus pandarallel matplotlib sklearn.preprocessing RepeatedKFold clean_paragraph create_wandb_plot xgboost train_test_split kaggle_secrets HTML ", "entities": "(('it', 'just basic XGBoost'), 'try') (('one', 'problem goal'), 'look') (('still adjustments', 'root e.'), 'make') (('line', 'wandb'), 'note') (('CV technique', 'competition'), 'be') (('W B Dashboard Get sum', 'data'), 'Tokenize') (('how long we', 'embeddings'), 'help') (('MOST more advanced DEFINITELY', 'better results'), 'frequency') (('TargetOur target variable', '3'), 'start') (('when we', 'data'), 'Bonus') (('we', 'training data'), 'missleade') (('error standard increases', 'cases'), 'mean') (('now s', 'segmentation'), 'look') (('you', 'API keys'), 'ask') (('XGBRF Cross ValidationLet s', 'different approach'), 'try') (('person', 'image'), 'be') (('competition', 'bit heart'), 'introductionyet') (('we', 'ones'), 'let') (('we', 'more than 0'), 'mean') (('s', 'data'), 'let') (('I', 'models'), 'note') (('Embedding how method', 'W B Plot sns'), 'test') (('we', 'more complex text'), 'forget') (('quite error', 'little'), 'go') (('just dataset', 'one'), 'try') (('big RMSE', '2'), 'mean') (('beautifly beautiful beautify beautification', '7'), 'beautiful') (('highest possible I', '1'), 'difficulty') (('TRAIN', 'Plot'), 'train') (('which', 'only 1 text'), 'differenciate') (('Basic Data Validation Create DMatrix', 'memory efficiency'), 'get') (('much models', 'manner'), 'preprocesse') (('especially we', 'dataset'), 'call') (('CountVectorizer decode_error', 'vocabulary pickle'), 'follow') (('We', 'completely off charts'), 'blame') (('you', 'submission submission'), 'hesitate') (('It', '0'), 'be') (('Usually face', 'same time'), 'express') (('very they', 'academia'), 'be') (('don However t', 'model'), 'be') (('Luckly we', 'case'), 'have') (('usually we', '0'), 'state') (('Understanding', 'very left'), 'be') (('paragraph', 'bit more others'), 'find') (('Tokenization', 'words'), 'mean') (('we', 'new created dataset'), 'create') (('we', 'usually paragraph'), 'let') (('workspace user andrada', 'Experiment W B https here www'), 'commonlit') (('Data PreprocessingWe', 'english_word_frequency dataset'), 'use') (('indeed error', 'medium complexity'), 'observe') (('we', '3 thirds'), 'segment') (('more you', '1'), 'be') (('Now it', 'dict i.'), 'show_values_on_bars') (('leaderboard score', 'Repeated Folds'), 'be') (('difficutly it', '4'), 'let') (('I', 'data'), 'try') (('how much dissagreement', 'coders'), 'be') (('So I', 'pickle'), 'save') (('we', 'text'), 'Oooook') (('Data Scientists', 'hand'), 'let') (('Machine then Learning', 'them'), 'mean') (('paragraphs', 'between 140 words'), 'note') (('Now s', 'models'), 'let') (('Specs GB Memory Z8 G4 Workstation 2 CPUs 96 Quadro', 'RAPIDS 8000 version'), 'NVIDIA') (('word frequencies', 'text'), 'be') (('s', 'segmentation'), 'look') (('most important we', 'texts'), 'note') (('computer', 'that'), 'Embeddings') (('it', 'surface'), 's') (('then we', '100 200'), 'choose') (('target', 'very close normal'), 'note') (('t', 'Laura Fink https www'), 'don') (('more details', 'second notebook'), 'come') (('sadness coders', 'images'), 'be') (('Competition Goal', '3 12'), 'rate') (('Secrets Custom colors', '2 vectors'), 'csv') (('we', '3'), 'be') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["advanced", "answer", "append", "article", "baseline", "basic", "best", "bit", "blog", "call", "case", "cell", "check", "choose", "classification", "classify", "clean", "close", "code", "color", "competition", "computer", "convert", "correlation", "could", "count", "create", "csv", "data", "dataset", "dict", "difference", "distribution", "document", "efficiency", "embedding", "end", "error", "even", "experiment", "face", "fact", "feature", "fill", "final", "find", "following", "frequency", "half", "help", "high", "histogram", "image", "importance", "improve", "improvement", "inference", "info", "itself", "key", "leaderboard", "learning", "let", "line", "little", "load", "look", "lower", "manner", "mean", "meaning", "memory", "method", "might", "model", "most", "my", "name", "need", "new", "no", "non", "normal", "not", "object", "open", "out", "outlier", "pattern", "people", "perform", "performing", "person", "picture", "plot", "point", "prediction", "preprocessing", "problem", "profile", "punctuation", "python", "re", "reading", "render", "replace", "run", "score", "script", "second", "segment", "sentence", "set", "similar", "similarity", "size", "space", "splitting", "standard", "state", "std", "structure", "submission", "sum", "surprise", "tag", "target", "technique", "test", "text", "think", "tokenization", "train", "training", "transformer", "try", "turn", "understanding", "up", "user", "value", "variable", "vector", "version", "who", "word"], "potential_description_queries_len": 146, "potential_script_queries": ["core", "matplotlib", "numpy", "pandarallel", "pearsonr", "seaborn", "xgboost"], "potential_script_queries_len": 7, "potential_entities_queries": ["basic", "standard", "user"], "potential_entities_queries_len": 3, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 151}