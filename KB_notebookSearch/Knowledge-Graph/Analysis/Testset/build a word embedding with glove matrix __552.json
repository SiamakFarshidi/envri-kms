{"name": "build a word embedding with glove matrix ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "read_csv Split train in train and validate evaluate the model evaluate the model stands for background positive subgroup negative stands for background negative subgroup positive. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. read_csv Input data files are available in the. linear algebra data processing CSV file I O e. Tutorial to build a GloVe wordembedding Preparing the data Tokenization load the GloVe matrix Building the model Evaluation on a local test with Jigsaw s metric Predict test set Process to prepare the data Tokenization Parsing the GloVe word embeddings file Building the word embedding matrix Building the model Look at the loss and the gain in accuracy for each epoch Run final model with the right number of iteration Test my word embedding on my local test with Jigsaw metric Apply the model on the test This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. ", "id": "francoisdubois/build-a-word-embedding-with-glove-matrix", "size": "552", "language": "python", "html_url": "https://www.kaggle.com/code/francoisdubois/build-a-word-embedding-with-glove-matrix", "git_url": "https://www.kaggle.com/code/francoisdubois/build-a-word-embedding-with-glove-matrix", "script": "Flatten TweetTokenizer GlobalAveragePooling1D cross_val_score Bidirectional keras.layers lightgbm get_final_metric calculate_overall_auc keras.preprocessing.sequence Embedding compute_bpsn_auc Dropout TfidfVectorizer Sequential LSTM InputSpec Adam compute_auc Layer callbacks numpy gensim.models sklearn.feature_extraction.text Word2Vec GRU MaxPooling1D optimizers Input initializers GlobalMaxPooling1D Tokenizer sklearn.model_selection sklearn metrics CuDNNGRU constraints seed_everything matplotlib.pyplot Activation Dense GlobalMaxPool1D SpatialDropout1D pandas word_tokenize tensorflow keras.optimizers set_random_seed keras.preprocessing.text compute_subgroup_auc sent_tokenize BatchNormalization compute_bias_metrics_for_model keras.engine load_model Conv1D regularizers nltk.tokenize compute_bnsp_auc Model concatenate layers Add backend power_mean pad_sequences backend as K keras CuDNNLSTM add keras.models train_test_split ", "entities": "(('you', 'output'), 'list') (('read_csv Input data files', 'the'), 'be') (('It', 'python docker image https kaggle github'), 'set') (('model', 'background negative subgroup positive'), 'evaluate') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "background", "build", "current", "data", "directory", "embedding", "environment", "epoch", "evaluate", "file", "final", "image", "input", "iteration", "kaggle", "linear", "list", "load", "local", "matrix", "metric", "model", "my", "negative", "number", "positive", "prepare", "processing", "python", "right", "run", "running", "set", "several", "subgroup", "test", "train", "validate", "word", "write"], "potential_description_queries_len": 41, "potential_script_queries": ["backend", "concatenate", "lightgbm", "numpy", "preprocessing", "sklearn", "tensorflow"], "potential_script_queries_len": 7, "potential_entities_queries": ["image", "negative"], "potential_entities_queries_len": 2, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 48}