{"name": "patterns of colorectal cancer wally ", "full_name": " h2 What can you find in this kernel h3 Table of contents h3 In progress or planned h3 Short overview h3 Sense and meanings of a baseline model h2 Loading packages and data h2 Setting up the loss h3 The cancer target distribution h3 The likelihood function h3 Why is it good to know something about likelihood functions h3 Where to go next h2 Exploring the data h3 How many samples per cancer class are present h3 Take Away h3 How bright or dark are images per cancer class h3 Take away h2 Split into train test and validation h2 Building a model with tensorflow Wally h3 Wallys Code h2 Data Preprocessing h3 One hot encoding of cancer targets h3 Image normalization h3 Take Away h4 Per Image Mean Centering and Unit Variance h4 Min Max Scaling h2 Going live A robot starts to learn h3 Setup information h2 What has Wally learnt h3 Do we need more learning steps h3 Take Away h3 How do the learnt convolutional kernels look like h3 Take Away h3 How good is Wally in classifying cancer h2 Going deeper Can we improve Wallys visual range h2 Reduce overfitting It s not about details Wally h2 What features are highlighted by the weight kernels ", "stargazers_count": 0, "forks_count": 0, "description": "We have found a suitable loss function. We would set the further to zero and would check the solution found given the second derivative. It s not a good idea to clinch bright images and to strech dark ones. Take a look at these kernels https en. Our baseline model to discover and learn will be a small working unit nothing amazing. One can see that the choice of the cross entropy below is not arbitrary and has some deeper motivation. Where to go next For our case study and baseline model we are almost done. My acutal level is what you call a beginner in this field and I m currently interested in details like. Fully connected Layer Activation cdot K4. Consequently images with bright regions always will be normalized to lower values and their distributions are clinched. What has Wally learnt We can see from the print output that Wally has learnt something as loss was decreasing for train and test. Hence our first question is. During improvement of the baseline we will explore different additional layers and concepts like dropout batch normalization weight regularization different activation layers and much more. How do the kernel weights look like that were learnt by a CNN How do they change when we add more layers Can I analyse exploding or vanishing gradients Why are some network architectures better than others How should I preprocess the images given this specific medical problem Etc. Data Preprocessing Before we start let s transform all labels to a binary representation with one hot encoding as only one cancer class is true for each image. Going live A robot starts to learn. Let s assume that all N samples were drawn independently from the same distribution. Subtract the overall mean and scale to unit variance from this mean. This way we could try out different strategies to solve problems that are caused by such imbalance. This skeleton is called the tensorflow graph. Both have similar skewed intensity distributions in their originals. io convolutional networks I like to start with a very reduced network architecture 1. 5 of samples with the corresponding type of cancer. 5 does only act on the value range but leaves the shape of the distribution the same as in the original cases. It somehow reminds me of the small robot WALL E https en. In our case this means to begin with grayscaled data instead of colored and a middle ranged resolution that is sufficient to work with but does not necassarily lead to computation performance issues. Hence before going deeper it might be better to reduce overfitting. Setting up the loss Before we start with building a small baseline model let s discover the data we are working with. Hence instead of values from 0 up to 255 bring them to 0 up to 1. With adding this minus sign our task changes to minimization of the negative log likelihood which is our well known loss function. theta MLE max_ theta p T X theta Probably you know that optimizing a function with respect to some variable means that we need to compute its first and second dervative. Fully connected Layer Output As this is our first session with our baseline model and start simply we will set N and K to 1. How bright or dark are images per cancer class As the cancer types are probably located in different tissue types the images between classes could differ in their intensities. In contrast the test loss has sattled down. If we do this normalization we would end up with different valued cancers that were similar once. And does empty meen no image at all We should try to unterstand if each sample is distributed that way of if we have high variance between samples. Wallys CodeIf you unfold the hidden code you can find the network as described above. Setup informationAnd let s define some further information Wally needs to learn like How many pixels features does an image have How is the height and width of each image How many color channels does it have How big should the learning rate step size be How many learning steps epochs should be performed Ok now let s wake up Wally. One example What if you have a mutlilabel classification problem to solve but with couplings between target variables What if independence assumption between observations is not true If you like you can but something on top and customise your objective as you need it. Consequently the mean of the first image is higher than of the second. But before we can do this for cancer classification we should describe all data samples all images their predictions and target cancer classes. But how to do it I often find these kind of techniques but the motivation for them often seem to be unclear Subtract the mean of each image and scale to unit variance. Or choose some other min max range like 0. What can you find in this kernel This kernel is part of a series of notebooks that correspond to my personal deep learing journey. This way we can visualize for some example images what wally tried to extract Take Away We can see that all kernels are different and weight spacial positions differently. I like this imagination of some aritifical organism that you can feed with data and fill with life by running specific functions. so far I can t conclude something out of it. This comes especially clear for the third image. Hence our normalization has caused a shift Similar bright regions of the originals are now different from each other. Convolutional Layer Activation cdot N3. loss_op the code knows that all parts of our skeleton that are needed to compute the loss have to be run. Take a look at the first two images. This kernel has not the goal to answer all these questions but by writing it I will hopefully gain more insights and more intuition of the learning process and perhaps find some answers by the way. We should find out their meanings as aidpose may be very tough to distinguish from empty. To make it tractable we can use the natural logarithm. The variation of image intensity distributions between classes motivates further analysis based on image statistics. Data preprocessing preprocessing 6. By fitting a model to our data we are trying to maximize how likely it is that our predictions suite to target values. The function we build this way is often called an objective function. Consequently we can describe the target distribution by a multinomial distribution this way p t prod_ k 1 8 y_ k t_ k Given an image our model tries to predict which cancer class is true and for each possible class it yields a probability y_ k. A learn method that calls some attributes of our class that act like ports to breathe life into Wally. As it s a monotonically increasing function the maximization of the log likelihood is equivalent to maximization of the likelihood. what are Wallys kernels doing Well. Let s describe all weights and biases by one parameter theta. For this purpose we can add regularization terms to our loss What features are highlighted by the weight kernels You probably know that the weight kernels we learnt for our convolutional layers act like kernels one can use for image preprocessing. Using our multinomial distribution we are able to describe how well the model predictions fit to our target values. t_ N whereby each x_ n stands for one image and t_ n for its corresponding target vector. A build Wally method that calls all important methods that we need to build up our tensorflow model. Where to go next If you are new to the likelihood concept this way probably full of new stuff. How good is Wally in classifying cancer Going deeper Can we improve Wallys visual range That s pretty interesting The last time we obtained a score of 0. This way it is much easier to figure out how to improve and which steps are necessary to build a simple but powerful model that is able to generalize as well. Consequently building a deeper network was not the key This time overfitting is even more bad Reduce overfitting It s not about details Wally Ok going deeper was not advantegous. I hope this makes it easier to understand for you as well. Consequently p t reaches its maximum value of 1 when y_ k 1. To cool down let s switch do data exploration. What has Wally learnt results 8. Exploring the data How many samples per cancer class are present Take Away We can see that the target distributions are balanced. read_csv The number of pixels per image Image dimensions The number of unique cancer classes The learning rate of our loss optimizer Let s define a method to feed Wally with data and fill in placeholders. As the target distribution is often the origin to setup an objective function or loss it s worth to look at it. As we have already seen during dataset overview the resolution of them is sufficient to recognize different tissues and cancer classes. Knowing this way can be an entry point to build own likelihoods that suite better to your current problem. Looking at the equation you can see that all classes that are not the target class yield y_ k 0 1. What about a bigger receptive field Perhaps we need to stack a second convolutional layer on top of the first. Table of contents1. Consequently we should not suffer under model confusion caused by imbalanced classes. Just bring the image into a new range. com allunia patterns of colorectal cancer image clustering Split into train test and validation Building a model with tensorflow Wally Following this nice tutorial http cs231n. org wiki Kernel_ image_processing for example. Why is it good to use this scaling method To find an answer we should take a look at the activation function we are working with. Let s start simple by forcing the weights of the neural net to be small. The training loss is still decreasing if you ignore th jumps. As a take away we can say that we can setup a so called likelihood function by probability distributions that tell us how well predictions Y fit to our targets T. Debris and mucosa have broad distributions. Reduce overfitting It s not about details Wally regularisation In progress or planned Regularisation. In case of neural networks this is done by adjusting the model parameters the weights between neurons. This way we could say if the solution found is a maximum minimum or a saddle point for example. This shift was caused by the skewness of the distribution as the mean is not robust towards outliers and shifted towards higher values. Calling this so called tensorflow operations via session. Setting up the loss with maximum likelihood loss 3. Going deeper Can we improve Wallys visual range bigeye 9. In addition it subpresses underflow of numerical precision of the computer caused by products of a large number of very small single sample probabilites. linear algebra data processing CSV file I O e. Once we find estimated its weights theta MLE by learning they are fixed and the neural network doesn t change. Build and setup a model quicky that can already detect some patterns Understand key concepts and difficulties of your task Experimental platform to increase performance step by step Loading packages and data For the baseline model we may start with the 64x64 grayscaled images given in hmnist_64_64_L. Hence weights are shared over all samples regardless if we are computing predictions for the first image of our dataset or for the last one. Loading packages and data load 2. x_ N and target cancer classes T t_ 1. The latter has k elements with one hot and all others zero. Images of adipose and empty are of very high intensity. org wiki WALL E so let s name it similar. partial_ theta_ i j l ln p T Y frac partial partial theta_ i j l sum_ n 1 N sum_ k 1 K t_ n k cdot ln y_ n k Now the derivative acts on sums which can be done for each summand separately. E ln p T Y sum_ n 1 N sum_ k 1 K t_ n k cdot ln y_ n k Why is it good to know something about likelihood functions Within this kernel you have seen that one can build a loss function out of a distribution that describes the data. How do the learnt convolutional kernels look like If you take a look into the code you can see that I stored the weight values of the first convolutional layer after learning. Hence the target per sample is following a one hot coding scheme All elements of t are zero except from the true class that holds one. The likelihood functionNow we are given a data set of observed images X x_ 1. Going live A robot starts to learn live 7. If you compare the structure of edge detecting kernels with our learnt kernels we can concluce that ours are different. It looks balanced somehow but perhaps this was not enough and we need more feature maps in our first convolutional layer. Thus let s have a look at the overall intensitiy distribution per class Take away The intensity values of all samples in the data are distributed very differently. After that we will go one step back to find out how the derivatives alias gradients of our model will look like and how the model will be influenced during learning. Thus my first questions is. If we like we could split the data such that we will enforce imbalanced classes. One hot encoding of cancer targets Image normalizationWe should normalize the images to improve convergence speed. If you have never written software in tensorflow you might be confused as you can find two different parts in Wally 1. build it and start learning. Only the probability of the true class contributes with y_ k 1 y_ k. In addition we need a very simple model to play with. After normalization we can see that the bright regions 200 of the first image now have lower values higher than 3. The one of debris looks even trimodal. In contrast the bright values 200 of the second image are now given by values higher than 4. With this independence assumption we can split into n factors and describe the target distribution of our data set as follows p T Y prod_ n 1 N prod_ k 1 K y_ n k t_ n k Our goal now is to maximize this function by computing nice predictions that fit well to the targets. Wally uses relu as activation function. Using the log everything turns out to be nice and smart. This is the case of a perfect prediction Then our model is sure with 100 that this image belongs to the true class. It describes how things have to be computed but the compution is not done. run fills in life to our robot. We don t know if cancer cells are always similar bright. Do we need more learning steps Take AwayWe can see that we started to overfit badly after roughly 50 steps. 6 for test images but this time we only reached 0. Choose a scaling normalisation method you like to discover Take Away under construction Per Image Mean Centering and Unit VarianceWe can see that this kind of normalization causes some problems. Improve reproducibility Explore Classification results Short overview 8 classes of cancer tissues multiclass classification Kather_texture_2016_image_tiles_5000 150 x 150 pixel in size 5000 samples Sense and meanings of a baseline modelA general strategy to solve complex problems is to start simple and grow complex during the analysis. Using the dataset of colorectal cancer images I like to understand how convolutional neural network learn and what I can do to improve their success. If you perform session. The cancer target distributionFor each image there is only 1 possible true target class t_ k out of 8 classes. As this is a topic of its own is covered by a new kernel notebook image clustering. Class wise normalizationLet s take a look at each kind of normalization and let s try to figure out what they are doing and if it makes sense to use them to improve speed or model prediction performance. But we are dependent on this similarity as our kernel weights only can work this way. Whereas the distributions of tumor stroma complex and lympho are still similar they are completely different from debris mucosa adipose and empty. Min Max ScalingScaling to min of 0. What you obtain is like a skeleton a dead shell without life. Building a model with tensorflow Wally wally 5. But we have observed jumps in the loss and overfitting on the training images in both cases. I think this does not make sense. But the accuracy scores are still low. Exploring the data explore 4. Then we compute all predictions Y given the images X and weights theta Y Y X theta By using a neural network we try to find weights that are able to describe the mapping between images predictions and targets for all samples. Cool You might have already detected that the log likelihood looks like a cross entropy apart from a missing minus sign. For each class there are 12. Regardless if we can do this procedure analytically by hand or using numerical optimization we need to find the first derivative of our likelihood function with respect to neural network parameters theta for each layer l and each connection between neurons i and j partial_ theta_ i j l p T Y frac partial partial theta_ i j l prod_ n 1 N prod_ k 1 K y_ n k t_ n k Due to the products this job is infeasible. ", "id": "allunia/patterns-of-colorectal-cancer-wally", "size": "17968", "language": "python", "html_url": "https://www.kaggle.com/code/allunia/patterns-of-colorectal-cancer-wally", "git_url": "https://www.kaggle.com/code/allunia/patterns-of-colorectal-cancer-wally", "script": "__init__ learn get_convolutional_block get_dense_block listdir build_wally seaborn numpy blood get_output_block Wally tell_fortune wonder min_max_scaling sklearn.model_selection norm_image matplotlib.pyplot sleep os tensorflow pandas feed OneHotEncoder Id heart_beat \\n {} body sklearn.preprocessing BigEyeWally(Wally) train_test_split create_placeholder ", "entities": "(('Debris', 'broad distributions'), 'have') (('mean', 'higher values'), 'cause') (('it', 'you'), 'hope') (('Y', 'targets'), 'say') (('that', 'equation'), 'see') (('all', 'predictions'), 'describe') (('How I', 'specific medical problem'), 'look') (('so far I', 'it'), 'conclude') (('that', 'well targets'), 'split') (('N samples', 'independently same distribution'), 'let') (('Both', 'originals'), 'have') (('we', 'hmnist_64_64_L.'), 'understand') (('We', 'aidpose'), 'find') (('Consequently mean', 'second'), 'be') (('it', '_ k.'), 'describe') (('image', 'true class'), 'be') (('choice', 'below deeper motivation'), 'see') (('kernel weights', 'similarity'), 'be') (('we', 'data'), 'let') (('s', 'neural net'), 'let') (('going', 'details'), 'be') (('intensity away values', 'data'), 'let') (('that', 'Wally'), 'method') (('deeper we', 'Wallys visual range'), 'go') (('it', 'sample very small single probabilites'), 'subpresse') (('you', 'specific functions'), 'like') (('Where next you', 'way probably new stuff'), 'go') (('cross', 'apart missing minus sign'), 'cool') (('that', 'different valued cancers'), 'end') (('org WALL so s', 'it'), 'wiki') (('that', 'better current problem'), 'be') (('I', 'first convolutional layer'), 'look') (('This', 'especially third image'), 'come') (('still they', 'debris mucosa completely adipose'), 'be') (('Perhaps we', 'first'), 'need') (('which', 'summand'), 'theta') (('test loss', 'contrast'), 'sattle') (('instead middle ranged that', 'computation performance necassarily issues'), 'mean') (('monotonically increasing maximization', 'likelihood'), 'be') (('p Consequently t', '1'), 'reach') (('samples 5000 Sense', 'analysis'), 'be') (('regardless we', 'last one'), 'share') (('images', 'intensities'), 'be') (('now s', 'color How many channels'), 'let') (('We', 'second derivative'), 'set') (('I', 'success'), 'use') (('we', 'tensorflow model'), 'build') (('kernels', 'example images'), 'visualize') (('still you', 'jumps'), 'decrease') (('we', 'first dervative'), 'p') (('predictions', 'values'), 'try') (('time we', 'only 0'), '6') (('baseline we', 'case Where next study'), 'do') (('pretty last time we', '0'), 'be') (('simply we', '1'), 'connected') (('cancer only one class', 'image'), 'Preprocessing') (('obtain', 'dead life'), 'be') (('we', 'very simple model'), 'need') (('variation', 'image statistics'), 'motivate') (('how model', 'learning'), 'go') (('that', 'such imbalance'), 'try') (('motivation', 'unit variance'), 'find') (('200', 'higher 3'), 'see') (('we', 'first convolutional layer'), 'look') (('you', 'Wally'), 'confuse') (('topic', 'kernel notebook image new clustering'), 'cover') (('model how well predictions', 'target values'), 'be') (('Consequently we', 'imbalanced classes'), 'suffer') (('target distributions', 'cancer class'), 'explore') (('job', '_ _ _ 1 products'), 'need') (('I', 'way'), 'have') (('it', 'speed'), 'normalizationLet') (('It', 'dark ones'), 's') (('likelihood we', 'observed images'), 'functionnow') (('latter', 'zero'), 'have') (('Images', 'very high intensity'), 'be') (('we', 'dropout batch normalization weight activation regularization different layers'), 'explore') (('one', 'image preprocessing'), 'add') (('we', 'samples'), 'empty') (('Similar bright regions', 'now other'), 'cause') (('we', 'badly roughly 50 steps'), 'need') (('Unit kind', 'problems'), 'choose') (('we', 'natural logarithm'), 'use') (('we', 'cases'), 'observe') (('we', 'activation function'), 'be') (('we', 'imbalanced classes'), 'split') (('Hence deeper it', 'overfitting'), 'be') (('neural network doesn', 'MLE'), 'estimate') (('that', 'learing personal deep journey'), 'find') (('that', 'one'), 'follow') (('that', 'simple model'), 'be') (('it', 'it'), 's') (('I', 'currently details'), 'be') (('I', 'network very reduced architecture'), 'io') (('solution', 'saddle maximum example'), 'say') (('cancer target', 'image'), 'distributionFor') (('that', 'data'), 'sum') (('loss', 'train'), 'learn') (('5', 'same original cases'), 'act') (('distributions', 'always values'), 'normalized') (('200', 'higher 4'), 'in') (('you', 'network'), 'unfold') (('ours', 'learnt kernels'), 'concluce') (('s', 'placeholders'), 'read_csv') (('cancer cells', 't'), 'don') (('everything', 'log'), 'turn') (('It', 'small robot'), 'remind') (('you', 'it'), 'example') (('s', 'parameter one theta'), 'let') (('loss', 'skeleton'), 'know') (('this', 'neurons'), 'do') (('which', 'log negative likelihood'), 'with') (('resolution', 'different tissues'), 'be') (('t _ x _ whereby n', 't _ target corresponding vector'), 'N') (('switch', 'data exploration'), 'let') (('that', 'samples'), 'compute') (('Image normalizationWe', 'convergence speed'), 'normalize') (('It', 'Wally progress'), 'reduce') ", "extra": "['test', 'mucosa', 'procedure']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "answer", "architecture", "baseline", "batch", "binary", "build", "call", "cancer", "case", "check", "choice", "choose", "classification", "clear", "clustering", "code", "coding", "color", "colored", "compare", "computation", "compute", "computer", "concept", "confusion", "connection", "contrast", "convergence", "convolutional", "could", "current", "data", "dataset", "define", "dependent", "derivative", "describe", "detect", "detected", "distributed", "distribution", "edge", "empty", "encoding", "end", "entropy", "equation", "even", "everything", "explore", "extract", "feature", "feed", "field", "figure", "file", "fill", "find", "fit", "fitting", "fixed", "following", "found", "frac", "function", "general", "grow", "hand", "height", "high", "hope", "hot", "http", "idea", "ignore", "image", "improve", "improvement", "increase", "intensity", "intuition", "io", "job", "kernel", "key", "latter", "layer", "lead", "learn", "learning", "let", "level", "life", "likelihood", "linear", "load", "log", "look", "lower", "mapping", "max", "maximum", "mean", "medical", "method", "middle", "might", "min", "minimum", "missing", "model", "motivation", "mucosa", "my", "name", "need", "negative", "network", "neural", "new", "next", "no", "normalization", "normalize", "normalized", "not", "notebook", "number", "numerical", "objective", "optimization", "optimizer", "out", "output", "overall", "overfit", "overfitting", "overview", "parameter", "part", "partial", "per", "perform", "performance", "pixel", "point", "precision", "predict", "prediction", "preprocessing", "present", "print", "probability", "problem", "procedure", "processing", "purpose", "question", "range", "reduce", "regularization", "representation", "reproducibility", "resolution", "robust", "run", "running", "sample", "scale", "scaling", "score", "second", "sense", "session", "set", "setup", "shape", "shift", "sign", "similar", "similarity", "single", "size", "solution", "something", "speed", "split", "stack", "start", "step", "strategy", "stroma", "structure", "target", "task", "tensorflow", "test", "think", "time", "tissue", "topic", "train", "training", "transform", "try", "tumor", "tutorial", "type", "under", "unique", "unit", "up", "validation", "value", "variable", "variance", "variation", "visualize", "weight", "width", "wise", "work"], "potential_description_queries_len": 223, "potential_script_queries": ["blood", "body", "learn", "listdir", "numpy", "seaborn"], "potential_script_queries_len": 6, "potential_entities_queries": ["convolutional", "high", "maximum", "middle", "mucosa", "network", "next", "notebook", "regularization", "target", "weight"], "potential_entities_queries_len": 11, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 228}