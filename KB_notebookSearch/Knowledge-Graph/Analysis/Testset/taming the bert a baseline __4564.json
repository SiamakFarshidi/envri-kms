{"name": "taming the bert a baseline ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "We use the method defined above to parse the contextual embeddings for each of the 3 GAP data files. Here are some helper functions to keep track of the offsets of the target words. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. So instead of removing rows make them 0 Will train on data from the gap test and gap validation files in total 2454 rows Will predict probabilities for data from the gap development file initializing the predictions testing predictions Training and cross validation split training and validation data Define the model re initializing for each fold train the model make predictions on validation and test data oof valid_index pred_valid. Updates V7 In the previous version I was a little worried by the large variance of the model. See also the BERT paper https arxiv. The best LB score I got with this method is 0. Convert them to lower case since we re using the uncased version of BERT For each word find the offset not counting spaces. 5 L2 regularization in the output layer of 0. Use a pre trained version for the BERT transformer model to obtain contextual word embeddings for the 3 target words in each passage A B and Pronoun. The other two files gap validation. Look at all occurences of the target words A and B in the text instead of just the one specified by the offset. So we ll make predictions on it. The current version uses a much smaller MLP for the supervised classification problem with more regularization. Specifically the current MLP has only one hidden layer of size 37 down from two hidden layers of sizes 59 31 dropout rate of 0. This achieves the same mean CV score with lower variance. The kernel needs an Internet connection to do this so make sure it s enabled. There are many things to try that could improve this and I may attempt some of these in the future 1. Downloading the pre trained BERT Base Uncased model. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. Dropout dropout_rate seed 9 X Output layer Create model Sorting the DataFrame because reading from the json file messed with the order Concatenate features One hot encoding for labels Read development embeddigns from json file this is the output of Bert There may be a few NaN values where the offset of a target word is greater than the max_seq_length of BERT. 04805 by Devlin et al. Feed this into a multi layer perceptron MLP which learns to solve the coreference resolution problem as a supervised classification task. The data I m using comes from the 3 GAP files available here https github. Next we feed BERT the data from these three files. Read the three GAP files pass them through BERT and write the contextual embeddings in json files. downloading weights and cofiguration file for the model From the current file take the text only and write it in a file which will be passed to BERT The script extract_features. Of course gap development also contains the true labels but I m not using these when making predictions. Next in order to feed our data to the model we ll use some scripts from the bert repo on GitHub. Finally let s download all the data from the GAP repo. We define a model with two hidden layers and one output layer in Keras. They are very few so I m just dropping the rows. I only use the true labels to evaluate the predictions made by my model. Fine tune the BERT model instead of using the pre trained weights. You can check it out here https www. tsv with 454 rows and gap test. The other two files gap test and gap validation are used for training the model. This is necessary for comparison with the output of BERT Figure out the length of A B not counting spaces or special characters Initialize embeddings with zeros Initialize counts Get the BERT embeddings for the current line in the data file Iterate over the BERT tokens for the current line we skip over the first 2 tokens which don t correspond to words See if the character count until the current token matches the offset of any of the 3 target words print token print token print token Update the character count Taking the average between tokens in the span of A or B so divide the current value by the count t Work out the label of the current piece of text Put everything together in emb n_test 100 L2 regularization First dense layer Second dense layer tX layers. Below we will use it 3 times once for each of the files gap test gap development gap validation. vanilla neural network which learns to classify the triples of embeddings emb_A emb_B emb_P as A B or NEITHER. reshape 1 Print CV scores as well as score on the test data Write the prediction to file for submission. Now that we have the embeddings we pass them to a multi layer perceptron i. I m using the GitHub repo for the BERT project https github. Activation relu X tX layers. For each line in the data file get the words A B Pronoun. read_csv Input data files are available in the. 05Ceshine Lee independently published a kernel with a very neat PyTorch implementation of the same idea. We want predictions for all development rows. py runs forward propagation through BERT and writes the output in the file output. Unfortunately I wasn t able to silence TensorFlow so it s giving a lot of information and warnings when I run this cell. For coreference resolution they use the OntoNotes and Definite Pronoun Resolution datasets but not GAP. jsonl I m lazy so I m only saving the output of the last layer. Tune some of the hyperparameters of the MLP model I haven t played with them at all. com google research datasets gap coreference. net forum id SJzSgnRcKX What do you learn from context Probing for sentence structure in contextualized word representations by Tenney et al. Keep in mind that we will use X_test and X_validation for training and then make predictions on X_development. Use a mix of the BERT layers instead of just the output of the last layer. tsv data contains the same 2000 rows as test_stage_1. com ceshine pytorch bert baseline public score 0 54. Dense dense_layer_sizes 0 name dense1 X tX layers. The idea for the architecture 1 2 above comes from the paper https openreview. As such the MLP hyperparameters they use may not be the best for our current task. tsv with 2000 rows will be used for training. For each line we want to obtain contextual embeddings for the 3 target words A B Pronoun. The following method takes the data from a file passes it through BERT to obtain contextual embeddings for the target words then returns these embeddings in the emb DataFrame. The variable names here may be a bit counter intuitive. 6 in the hidden layer up from 0. BatchNormalization name bn1 X tX layers. The hyperparameters I use below are quite different from theirs. In this kernel I m trying to obtain a baseline for the following model 1. The gap development file contains the same data as the test_stage_1 file that we re trying to make predictions on. Feel free to change layers 1 to save the output of other layers. com google research bert to obtain the pre trained model. ", "id": "mateiionita/taming-the-bert-a-baseline", "size": "4564", "language": "python", "html_url": "https://www.kaggle.com/code/mateiionita/taming-the-bert-a-baseline", "git_url": "https://www.kaggle.com/code/mateiionita/taming-the-bert-a-baseline", "script": "sklearn.metrics cross_val_score compute_offset_no_spaces callbacks numpy models count_length_no_special optimizers initializers constraints sklearn.model_selection KFold run_bert optimizers as ko tensorflow pandas parse_json count_chars_no_special log_loss regularizers callbacks as kc build_mlp_model layers backend keras train_test_split ", "entities": "(('they', 'current task'), 'be') (('Below we', 'files gap test gap development gap validation'), 'use') (('offset', 'spaces'), 'find') (('I', 'future'), 'be') (('which', 'script extract_features'), 'take') (('helper Here functions', 'target words'), 'be') (('Finally s', 'GAP repo'), 'let') (('com ceshine pytorch bert', 'public score'), 'baseline') (('We', 'GAP data 3 files'), 'use') (('initializing', 'validation'), 'make') (('tsv', 'training'), 'use') (('com google research', 'gap coreference'), 'dataset') (('I', 'GAP 3 files'), 'come') (('Lee', 'same idea'), '05Ceshine') (('py', 'file output'), 'run') (('current token', 'Second dense layers'), 'be') (('I', 'model'), 'v7') (('we', 'X_development'), 'keep') (('Print CV 1 scores', 'submission'), 'reshape') (('we', 'multi layer'), 'now') (('I', 'below quite theirs'), 'be') (('Specifically current MLP', 'dropout 59 31 0'), 'have') (('I', 'BERT project https github'), 'm') (('which', 'A B'), 'emb_A') (('Next we', 'three files'), 'feed') (('you', 'output'), 'list') (('idea', 'paper https 1 2 openreview'), 'come') (('when I', 'cell'), 'wasn') (('we', 'target 3 words'), 'want') (('GAP three files', 'json files'), 'read') (('following method', 'emb DataFrame'), 'take') (('so it', 'this'), 'need') (('we', 'GitHub'), 'next') (('I', 'when predictions'), 'contain') (('very so I', 'just rows'), 'be') (('I', 'method'), 'be') (('I', 'them'), 'tune') (('read_csv Input data files', 'the'), 'be') (('tsv data', 'test_stage_1'), 'contain') (('perceptron which', 'classification supervised task'), 'feed') (('you', 'Tenney et al'), 'forum') (('files gap other two test', 'gap model'), 'use') (('This', 'lower variance'), 'achieve') (('predictions', 'that'), 'contain') (('I', 'following model'), 'm') (('We', 'development rows'), 'want') (('It', 'python docker image https kaggle github'), 'come') (('so I', 'last layer'), 'jsonl') (('current version', 'more regularization'), 'use') (('NaN a few where offset', 'BERT'), 'Create') (('they', 'Definite Pronoun OntoNotes datasets'), 'use') (('We', 'output one Keras'), 'define') (('I', 'model'), 'use') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["architecture", "average", "baseline", "bert", "best", "bit", "case", "character", "check", "classification", "classify", "comparison", "connection", "context", "could", "count", "course", "current", "data", "define", "development", "directory", "download", "encoding", "environment", "evaluate", "everything", "feed", "file", "find", "fold", "following", "forward", "future", "gap", "google", "helper", "hot", "id", "idea", "image", "implementation", "improve", "input", "json", "kaggle", "kernel", "label", "layer", "lazy", "learn", "length", "let", "line", "linear", "list", "little", "load", "lot", "lower", "mean", "method", "mind", "model", "my", "name", "network", "neural", "not", "offset", "oof", "order", "out", "output", "parse", "pre", "predict", "prediction", "print", "problem", "processing", "project", "propagation", "public", "py", "python", "pytorch", "re", "reading", "regularization", "research", "reshape", "resolution", "run", "running", "save", "saving", "score", "script", "sentence", "several", "size", "special", "split", "structure", "supervised", "target", "test", "testing", "text", "through", "token", "total", "track", "train", "training", "transformer", "try", "tune", "until", "up", "validation", "value", "vanilla", "variable", "variance", "version", "word", "write"], "potential_description_queries_len": 129, "potential_script_queries": ["backend", "kc", "ko", "numpy", "sklearn", "tensorflow"], "potential_script_queries_len": 6, "potential_entities_queries": ["data", "gap", "image", "project", "supervised"], "potential_entities_queries_len": 5, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 135}