{"name": "masters final project model lb 1 392 ", "full_name": " h1 Kaggle LANL Earthquake Prediction Modeling h3 Kevin Maher h3 Regis University MSDS696 Data Science Practicum II h3 Associate Professor Dr Robert Mason h4 May 2 2019 h4 Spring 2019 In partial fullfillment of the Master of Science in Data Science degree Regis University Denver CO h3 Introduction h3 Publication h3 Problem Approach h3 Processing Issues h3 Code Setup h3 Feature Creation h3 Feature Creation using Wavelets h3 Models h3 Feature Selection h3 Individual Model Cross Validation Results h3 Hyperparameter Tuning h3 Model Stacking h3 Lessons Learned h3 Future Research Possibilities h3 Conclusions h3 Acknowledgements h3 Test Environment h3 Author and License Information h3 References ", "stargazers_count": 0, "forks_count": 0, "description": "Model Fit Metrics undated. Build features from the Kaggle test data files. io en stable van der Walt S. The NumPy Array A Structure for Efficient Numerical Computation. Ensembles of models may therefore perform best where their individual weaknesses and variance tend to somewhat cancel out Demir. Consequently the individual functions often require many hours or days to run even when using multiprocessing. XGBRegressor n_estimators 1000 learning_rate 0. As noted below these models usually scored higher individual scores on the Kaggle leader board. Stratified random sampling will be performed on the data. 05 as representing significance this value was chosen for the model s feature reduction algorithm. All of this helps to avoid crashes and allow the feature building portion of the script to run overnight. This discloses my code which is being submitted and shared to my professor and class for grading. Helper functions for feature generation. These scripts appear to underfit the public leader board in the sense that cross validation CV scores tend to be higher worse than the public leader board score. The test signals are all 150k samples in length thus it seems best to extract 150k sample sets from the training data. Function to restructure wavelet signal peak detection so that the peaks are ordered by index rather than peak value. Manager function to build the feature fields that are extracted from the acoustic signal for the training set only. com c LANL Earthquake PredictionScipy 2019. This allows for selection of the section of the overall data on which to work. 2017 libraries for machine learning and support. The test set 2624 samples was run as a single process over two days. Without them and their predecessor kernel scripts on Kaggle any progress made by this effort would have been far more difficult. Note that the signal is 150k lines long hence by the Nyquist criteria there are 75k valid frequency lines before aliasing. For this and the models that follow remember to adjust the number of jobs treads or processes based on the CPU capabilities available. When done this provided 20 features that passed a p value test with a threshold of at or below 0. ReferencesBilogur A. These methods give very different and opposing CV results but very similar Kaggle public leader board scores. It would be easy to keep more computers busy on this project and if it were to be repeated I would try to locate more resources. Code was tested using an IDE not this notebook. An example of this is the Preda 2019 kernel but there are many other excellent scripts using this approach that the reader might review on Kaggle. These features may have has a very small beneficial effect upon the model and a significant number of these features were deemed statistically significant by the Pearson s correlation performed in the feature reduction section of the modeling. More features and samples. com artgor earthquakes fe more features and samples Mart A. Note for example that this author s XGBoost model with essentially non overlapped cross validation had a CV score of 2. This is a problem where it is difficult to score well. XGBoost A scalable tree boosting system. The acoustic data provided is used to create statistical features which are fed into supervised learning algorithms which then seek to predict the time until an earthquake from test signals. If the create_features_pk_det function is called to obtain wavelet generated peak detection features it may take two days to run. For example colsample_bytree in XGBoost was set to 0. Also working on data chunks that represent only a portion of the very large input data set means that the whole data set is not loaded into memory multiple times once for each process. WIndowed features were not subjected to the digital filters since the windowing is a type of filter. The indices did not survive the process but mostly the peak values and some slope values appear to have some merit. The data above may be too small to make many observations regarding the relationship between CV score and Kaggle public leader board results. While the features remain in the model it is arguable that their benefit was not worth 5 days of compute time. EDA showed that most if not all of the signal above the 20 000 frequency line was likely to be noise so the frequency bands will concentrate on the region below that. Even the current Kaggle public leader board result obtained is suspect because of the small amount of the test data that it contains but a better test set does not appear to be available at present. Most of the published kernel scripts that this author has reviewed on Kaggle use a data row creation method that slices the 629 million row acoustic input data evenly into 4194 non overlapping chunks of data that are equivalent in length to the 150k sample size of the Kaggle test samples. Given around 629m potential training samples one challenge is how best to extract effective but still computationally tractable training sets from the given signal. Scipy considers this p value to be reasonably reliable for sample sizes above 500 which clearly is true for the models presented here Scipy 2019. By combining the models a score of 1. Alternate choices for the width and number of frequency bands have not been investigated and might prove worthwhile. Retrieved from https arxiv. ModelsRun a LightGBM model and save for a submission to Kaggle. The feature builder function took so long that it was run as 6 concurrent processes in order to speed it up. First there are four models with 24 000 data rows that have performed well on the Kaggle leader board. After the competition is over and the full test set is made available for scoring this problem would be resolved for non contest entries. Best public leader board scores for LightGBM is 1. 7 Anaconda environments. Pandas is very helpful for its ability to support data manipulation and feature creation McKinney 2010. The Preda 2019 model is his LightGBM single model as run by this author. Or one could continue to use the Pearson s approach to feature generation and experiment with various cutoff values. The function below can be called for either the training or test sets. LightGBM A Highly Efficient Gradient Boosting Decision Tree. Feature Creation using WaveletsFeature creation by using wavelets to extract peak value and index information from the signal was also explored. These are the LightGBM and XGBoost models both run with substantially different cross validation methods. This method also helps create more accurate models by averaging the results of different splits of the training data into training and validation sets. This is based on the EDA where the magnitude of the Fourier transform looks important but the phase response seems to be mostly noise. In the table below only one model fell in Kaggle scoring when the feature set was reduced and the divergence was only 0. Kaggle LANL Earthquake Prediction Modeling Kevin Maher Regis University MSDS696 Data Science Practicum II Associate Professor Dr. Many of these libraries request a citation when used in an academic paper. The code has been written and run in Python 3. If it fails part way down due to a path name being wrong then it is not necessary to re run every function. Keeping up will probably require new breakthroughs. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD 16 785 794 New York NY USA. Because this 150k sample overlap is so small compared to the 100 million plus samples in each 1 6th slice of the signal leakage is effectively negligible. Random CV row selection on 24k rows of data where the rows do overlap in the original signal changes the picture significantly from CV results reported for the above. Actually in this model there is very slight leakage because this author was not fully cognizant of these possible effects and allowed a small 150k sample overlap between the 6 segments. Computing in Science Engineering 13 22 30 DOI 10. It would have been helpful to try more hyperparameter tuning. AcknowledgementsI feel compelled to note again the contributions of the Preda 2019 Lukayenko 2019 and Scirpus 2019 scripts to this work. These were sourced from a Kaggle kernel script Preda 2019. This changes the CV relationship back to that of the 4194 sample models reported and again the CV score is much worse than the Kaggle leader board score. com blog 2016 01 07 big p little n Demir N. The Jupyter environment in which this project is presented is a descendant of the IPython environment originated by P\u00e9rez Granger 2007. Another area of exploration is that the frequency bands used to create additional features were selected somewhat arbitrarily except for the understanding of the general frequency range desired that was obtained in the EDA by Fourier analysis. These two outside models are the ones noted by Preda 2019 and Scirpus 2019. 24 000 data rows were created in a stratified and then randomized manner. One should be cautious in doing so it has proven all to easy to drastically overfit the training data and trials too numerous to document here have resulted in superb CV scores but lessened Kaggle leader board results. Also tried were Random Forest Brieman 2001 decision tree based algorithm and a model based on the Keras Tenorflow Deep learning library Chollet et al. An XGBoost model improved from 1. 437 for the XGBoost models. Hyperparamater tuning was particularly difficult because of the disconnect between leader board score and cross validation scores. Test Environment Author and License InformationKevin Maher Email Vettejeep365 gmail. com Upvotes and or github stars appreciated This code herein has been released under the span style color 337AB7 text decoration none text underline none Apache 2. It was fortunate to have two reasonably powerful computers available for much of the project. Both Preda 20190 and Scirpus 2019 scripts report lower Kaggle public leader board scores than their CV scores. Andrews Script plus a Genetic Program Model. Retrieved from https gplearn. It appears that for this challenge problem the CV score is not a reliable predictor of public leader board score at least for the small current public leader board test data sample. Because it uses genetic programming rather than the decision trees used by LightGBM and XGBoost it offers possible diversity to the model. No final stopping point was ever found for this model. PublicationIn an effort to comply with both university and Kaggle requirements this Jupyter notebook is being published on GitHub and on Kaggle. This allowed some parallel development to take place and helped when there were more ideas available than CPU power to investigate them. The goal of the project is to predict the time that an earthquake will occur in a laboratory test. Due to extremely high computational time this was only performed for the 24 000 sample models. This is the variant of the model with feature elimination performed by Pearson s correlation. Similarly the number of leaves was decreased and the minimum data in a leaf increased for the same reason in LightGBM. The difference does not appear to be significant and might change if further parameter tuning were performed. Retrieved from https docs. Feature reduction via Pearson s correlation appears to have had a moderate beneficial effect upon most of the individual models as evidenced by Kaggle public leader board scores for equivalent individual models. SciPy is utilized to provide signal processing functions especially filtering and for Pearson s correlation metrics Jones E. Filter design helper functions. The parameter proc_id is the multiprocessing identifier passed in by the multiprocessing caller. It is probably not worthwhile to spend time on Support Vector Machines and Nearest Neighbors algorithms in other regression models within the author s experience these seem antiquated and appear to under perform newer gradient boosting decision tree based methods such as LightGBM or XGBoost. However the script CV scores appear to be just above 2. Define some constants. 2011 XGBoost Chen Guestrin 2016 and LightGBM Ke et al. The Kaggle submission shown here was made on April 28th 2019. The Random Forest also did not perform as well as LightGBM and XGBoost on the feature generation set presented here. Also tried was increasing the number of data rows to 40 000. com scirpus andrews script plus a genetic program model Singhal G. Genetic programming in Python with a scikit learn inspired API. Convert to float Copy for LTA Compute the STA and the LTA Pad zeros Avoid division by zero by setting zero values to tiny float low pass filter high pass filter band pass filter FFT transform values train_X create_features_pk_det seg_id seg train_X start_idx end_idx 1 on success 0 if fail just tqdm in IDE train_X create_features_pk_det seg_id seg train_X start_idx end_idx do something like this in the IDE call the functions above in order if __name__ __main__ function name predictions mean absolute error root mean squared error index needed it is seg id do this in the IDE call the function if __name__ __main__ lgb_base_model model xgb. Instead of the Kaggle public leader board score being better than the CV score now the CV score is very low for random CV sampling. The signal constants define how the signal and Fourier transforms will be filtered to produce bandwidth limited features. best_iteration_ num_iteration model. com tensorflow tensorflow 0. best_iteration_ index needed it is seg id do this in the IDE call the function above if __name__ __main__ lgb_trimmed_model. This was actually not true for the randomly sampled CV model with Light GBM. The author did not have time to fully investigate it and may have been hampered by a lack of experience with the algorithm. Second it helped greatly with computational time and memory usage because multiprocessing can then be employed. Individual Model Cross Validation ResultsIndividual model cross validation CV results are shown below both for the models presented in this project and two reference models taken from the Kaggle Kernels. 0 random_state 777 fold_ n_jobs 12 verbosity 2 model. It is currently unknown whether this will change when the full private leader board is revealed at the end of the competition. Then 24 000 data rows are created with 900 features extracted from the signal. The predecessor scripts can be found from citation links in these scripts and full links are referenced below. Hyperparameter tuning was therefore less extensive than would be desirable. The simulated earthquakes tend to occur somewhat periodically because of the test setup but this periodicity is not guaranteed to the researcher attempting to predict the time until an earthquake. The sta_lta refers to the short term average divided by the long term average. Lessons LearnedSeveral modeling types that this author had no previous experience with were tried on the 24k row data features. 0 tensorflow g3doc index. While there are 2624 test signals provided by Kaggle only 13 341 are used for the public leader board Rouet Leduc et. Code SetupBelow are the imports needed to run the code. Takes overnight to run 6 processes on the input data. The training data set contains more than 629 million acoustic signal samples and is 10GB in size so there is a lot of data to process. These were added to allow for obtaining statistics on the signal in a bandwidth limited manner. This will be explored when model results are presented below. Running 6 processes on 24 000 samples required 3 days to complete. Many of the feature creation ideas here appear to owe their origins to the Lukayenko 2019 script and it s cited predecessors. Another item to note is that the models mentioned herein are averaged ensembles of a cross validation CV. The true origin of this modeling approach is unknown to this author. The output is a usable training set for both features and targets the earthquake prediction times. Semester time constraints made tuning efforts difficult as it would have required too many days to perform effective grid searches on the problem. At the time of submission this was good for the top 1 of 3200 plus competitors. Retrieved from https scikit learn. LightGBM training times then became almost identical to those of XGBoost. Numpy is utilized to provide many numerical functions for feature creation van der Walt Colbert Varoquaux 2011. This Kaggle competition comes with cash prizes and this attracts many fine competitors so it will not be surprising that this result will fall as more entrants submit models. Data Structures for Statistical Computing in Python. This function joins the results of the multiprocessing build into one training set for model building. cc paper 6907 lightgbm a highly efficient gradient boosting decision tree Prokhorenkova L. Processing IssuesMany of the processes and functions below are very long running possibly taking overnight or days to complete. Feature SelectionEarly on feature selection was performed via feature ranking output from a LightGBM model. 9 colsample_bytree 0. In terms of obtaining a good leader board score and to hopefully generalize well to the full leader board 3 models were averaged here and submitted to Kaggle. 001 MAE in the Kaggle public leader board score. This is a common regression metric. 6 and the library dependencies as noted below near the end of this document and in the imports. Put the feature creation functions together and create the features. This possibly could help with the large p small n issue that might arise if a model with only 4194 training rows was tried and 900 features obtained from splitting the signal up with digital filters. This appeared to help even with gradient boosted decision tree algorithms and is necessary with many other machine learning algorithms. I have used the Jupyter notebook here only for documentation and presentation purposes. Removing some 150 features by this method provided a very tiny increase 0. LANL Earthquake Prediction. This will also output feature importance. Manager function to call the create features functions in multiple processes. com residentmario model fit metrics Brieman L. A problem is that this algorithm is very computationally expensive. Hyperparameter TuningEffective hyperparameter tuning proved to be a very large challenge in this project. It has not been tested and probably will not run in the Kaggle environment. Note the use of the Scikit Learn Pedregosa et al. Thus in addition to random selection for cross validation working with 5 slices for training and 1 slice for validation as a 6 fold CV is also an option. LANL Earthquake EDA and Prediction. The Scipy pearsonr function provides a p value that takes account of the sample size of the model. This resulted in worse overfitting and a lower CV score. Also some of the code uses multiprocessing and this can be troublesome if run from Jupyter Singhal 2018. This model scored 1. I believe that the Andrew script is the one by Andrew Lukayenko Lukayenko 2019. Some of these functions can take a long time to run so it is recommended that it be done from an IDE and one function at a time. Future Research PossibilitiesPrincipal components analysis PCA appears to be worth trying especially if one were to apply the mass feature generation used here on 4194 sample data. In order to avoid having to load the whole 629m data set into memory 6 times only the smaller slices with 1 6 of the data were loaded one into each process. Inspired by script from Preda 2019 and Lukayenko 2019. Computing in Science Engineering 9 21 29 DOI 10. When I tried the Preda 2019 script run from an IDE this author obtained a public leader board score on Kaggle of 1. Because this made a computationally intensive approach to the problem even more computationally difficult this effort was quickly dropped. If the create_features_pk_det function is called to obtain wavelet generated peak detection features it may take three days to run. ConclusionsIt might be tempting to build and evaluate models based strictly on the training data in projects outside of the Kaggle competition. The remainder are reserved for the final scoring that will be done after the competition concludes and after this course is finished. Please review the EDA for additional perspective on the problem. This produces the test file that will be used for prediction and submission to Kaggle. Journal of Machine Learning Research. These models required 6 hours to train with LightGBM and 30 minutes with XGBoost. This is probably caused by information leakage between the samples because they are derived from signals that overlap in time. This perhaps could have been more easily acomplished with the skiprows and nrows parameters of the Python Pandas read csv function rather than creating 6 new files. Also there is the Scirpus 2019 script to consider based upon 4194 sample rows. CatBoost given its good reputation and being a modern gradient boosting machine is also worth further study. While the Kaggle public leader board appears to be the best test set for model ranking currently available there might be a lot of variance in the results when the remaining 87 of the test data is revealed. The Preda 2019 reference script had a CV of 2. Changes the Fourier transform to evaluate based on magnitude and phase and also to do so in a bandwidth limited manner as compared to the reference scripts. Yet the XGBoost model in question had a Kaggle public leader board MAE of 1. Keras The Python Deep Learning library. The laboratory test applies shear forces to a sample of earth and rock containing a fault line. The training signal is provided by Kaggle in the form of a continuous acoustic signal that is over 629m samples long. The main function to create features. Scikit learn Machine Learning in Python. com gpreda lanl earthquake eda and prediction Pedregosa et al. There are two possible approaches to model cross validation CV because of the stratification used by the multiprocessing. fit X_tr y_tr predictions num_iteration model. The idea of creating the models in this way was taken from the Preda 2019 script as well as many others too numerous to cite that are present on the Kaggle website. As noted above the feature importance from the LightGBM model was abandoned as a feature selection mechanism in favor of Pearson s correlation. Using 6 fold cross validation with slices that do not overlap eliminates the leakage because of the splitting of the signal into 6 segments before the random selection was performed. Building an effective model for a Kaggle challenge where the data is noisy and the leader board that utilizes only 13 of the potential test data is a significant challenge. In spite of the information leakage and possible overfitting this method produced the best individual model Kaggle public leader board score in this report of 1. Machine Learning 45 1 5 32. Problem ApproachThis problem has been approached here by regression modeling. For example a LightGBM model with 8 fold random cross validation improved from an MAE of 1. 1 max_depth 6 subsample 0. It is hoped that the diversity of model types and feature generation will help to stabilize the predictions submitted to Kaggle such that the model does generalize well when the full leader board is revealed at the end of the competition and after this university course has completed. While Keras and Tensorflow work very well on speech and vision applications it does not to this author apppear fully competitive with the best tree based gradient boosting models in a regression problem. This algorithm uses a Mexican Hat wavelet in the SciPy library and by interference with Mexican Hat wavelets finds the peak and peak index locations for the signal. CatBoost unbiased boosting with categorical features. Butterworth 4 pole IIR filters are utilized to obtain the signal split into frequency bands. set for local environment set for local environment the test signals are 150k samples long Nyquist is thus 75k. Only a result function containing the genetic algorithm s output mathematical functions relationships and coefficients seem to be given by the author. General Chairs Thirty first Conference on Neural Information Processing Systems NIPS 2017. The script by Scirpus is interesting in being a very effective genetic programming model Scirpus 2019. While 60k estimators was eventually chosen from experience the model would appear to continue to train up to 100k estimators or beyond. It is be best to transfer them to an IDE in order to run them. The Scirpus 2019 genetic programming model result is also as obtained by this author in testing and agrees with the Kaggle leader board result reported for this script by its original author at the time the script was run by this author. Thus we note that these are laboratory earthquakes not real earthquakes. 556 for the LightGBM model presented in that script. While indices for slicing data out of the model were chosen randomly they were chosen from 6 slices of the original model data. Experience below will show both the successes and challenges of this alternate method of feature creation. Multiprocessing allowed the main feature creation to run overnight instead of possibly requiring days which might have been required with a single process. Retrieved from https medium. The Preda 2019 script references a script by Andrew and one by a Kaggle user named Scirpus. Added frequency bandwidth limiting to the time domain features. 082 average using LightGBM. This training data is accompanied by a ground truth time to failure time until the next earthquake for each acoustic sample. A 6 fold XGBoost model where the folds did not overlap in the training signal time domain improved from 1. This makes the feature building more memory efficient. The author please requests a citation for the use or derivation of this work. This code will work best if there is available at least a four core 8 hyper thread CPU it was primarily tested on a Windows 10 operating system with an AMD Ryzen 7 CPU 8 cores 16 logical threads and 16GB of RAM. This allows usage of all of the training data for creating a Kaggle submission file while still reserving validation holdout sets. A model stack built by simple averaging was submitted to the Kaggle leader board for scoring using the two best models by this author plus output from the Scirpus 2019 script. SciPy Open Source Scientific Tools for Python Retrieved from http www. Function to add a slope value that adds a slope representing the peak vs its distance from ths signal end. 667 where only 2 3 of the columns are selected for a split at each tree level was chosen because the documentation for XGBoost indicates that this helps with overfitting. Realization that a sectionalized cross validation was also practical shortened LightGBM training times because the model eventually reached a point where it stopped improving on the validation data. The author s inexperience with both of these algorithms appear to be the primary culprit. Multiprocessing in Python on Windows and Jupyter Ipython Making it work. com grvsinghal speed up your python code using multiprocessing on windows and jupyter or ipython 2714b49d6facStevens T. For much of the semester the author worked with the random cross validation strategy. It ensures relatively even coverage of the width of the input signal and it allows for multiprocessing so that the script runs in a reasonable time. Retrieved from http papers. The exploratory data analysis notebook for this project will also be published in the same manner. This slicing accomplishes several objectives. Long run times for many functions and algorithms made the project more of a challenge than it otherwise might have been. Because of limited time and observed overfitting hype parameter tuning was performed by theoretical changes in directions that might reduce overfitting. These were CatBoost Prokhorenkova Gusev Vorobev Dorogush Gulin 2017 and genetic programming via the gplearn library Stephens 2016. Ensemble Methods Elegant Techniques to Produce Improved Machine Learning Results. The trend feature is a linear regression on a portion of the signal. com machine learning ensemble methods machine learning Jones E. This is for several reasons. Accuracy and Kaggle scoring position appear to be gained by using this technique at a significant cost in additional model training time and complexity. The user is left to decide how to extract information from the test signal in order to provide training data for their chosen machine learning algorithms. Examination of other scripts on the Kaggle kernels section leads this author to believe that this is typical for scripts where the data rows were bread sliced from the original acoustic signal and the 4914 resulting data rows do not overlap or leak information. 437 better than obtained for the Preda 2019 LightGBM model. Exploratory data analysis EDA is performed in a separate Jupyter notebook located with this file in the github repository https github. Partly because of the extensive exploration of slicing the data into 4194 non overlapping slices in the Kaggle kernels by other challenge participants and partly to set out on an individual exploration of modeling this data a different approach is tried in the primary models presented here. These are obtained from 6 simple slices of the original data each slice is used to randomly create 4k data rows. Proceedings of the 9th Python in Science Conference 51 56. More success was achieved by calculating the Pearson s correlation of the features with the target time to failure. IPython A System for Interactive Scientific Computing. It seems though that CV score is not a good predictor of the eventual Kaggle public leader board score and this has caused significant challenges throughout the project especially with hyperparameter tuning. 0 open source license. Robert Mason May 2 2019 Spring 2019 In partial fullfillment of the Master of Science in Data Science degree Regis University Denver CO IntroductionPresented here are a set of models for the Kaggle LANL Earthquake Challenge Rouet Leduc et. Retrieved from https www. Model StackingSeveral issues affect possible model stacking given the state of this project. 0 seems best within 0. Since statisticians generally consider p values below 0. Kaggle limits submission models to two per day and this proved to become a limitation for experimentation as the project due date approached. As an alternate view one could argue that the CV is needed anyway so why not take advantage of it as a direct model. com Vettejeep MSDS696 Masters Final Project. Slicing the data into 4194 chunks avoids overlap and possible information leakage between these slices as they then do not share any signal information. Retrieved from https keras. This may help the machine learning see the peaks in a more time ordered manner. When scripts take overnight or even days to run having more computers available is clearly advantageous and allows trying more ideas on the project. The metric used by Kaggle in this competition is Mean Absolute Error MAE and thus a lower value is better with zero representing a perfect fit Bilogur. First it tends to help spread the random generation of data across the signal without risk of bunching too many slices into a compact region of the original signal. best_iteration_ mean absolute error root mean squared error training for over fit num_iteration model. 440 under similar conditions. Large Scale Machine Learning on Heterogeneous Systems. The code is written using Python 3. Build six sets of random indices. org doc scipy reference generated scipy. Both suffered from long training times in this scenario and CV scores that were not encouraging. Unfortunately the C code that the genetic algorithm has been written in does not appear to be publicly available. However it was difficult to know where to set a threshold for feature removal due to their being few obvious cut points in the feature scores. 67 reg_lambda 1. Retrieved from https chromium. Please see the table below for examples of CV and public leader board score. The notebook was designed for a university course. Feature CreationFunction to split the raw data into 6 groups for later multiprocessing. Kaggle leader board values are those obtained by this author in testing. ", "id": "vettejeep/masters-final-project-model-lb-1-392", "size": "33831", "language": "python", "html_url": "https://www.kaggle.com/code/vettejeep/masters-final-project-model-lb-1-392", "git_url": "https://www.kaggle.com/code/vettejeep/masters-final-project-model-lb-1-392", "script": "classic_sta_lta sklearn.metrics join_mp_build multiprocessing lightgbm des_bw_filter_bp build_fields build_rnd_idxs hann create_features mean_squared_error numpy run_mp_build split_raw_data lgb_trimmed_model scale_fields sklearn.model_selection KFold convolve add_trend_feature build_test_fields stats create_features_pk_det pandas StandardScaler tqdm hilbert lgb_base_model des_bw_filter_hp scipy scipy.signal mean_absolute_error sklearn.linear_model sklearn.preprocessing xgboost des_bw_filter_lp LinearRegression ", "entities": "(('Kaggle leader board values', 'testing'), 'be') (('feature building portion', 'script'), 'help') (('Jupyter notebook', 'Kaggle'), 'PublicationIn') (('i', 'function'), 'convert') (('project', 'experimentation'), 'limit') (('that', 'model'), 'pearsonr') (('this', '24 sample only 000 models'), 'perform') (('seem', 'such LightGBM'), 'be') (('it', 'gradient boosting regression based problem'), 'work') (('memory multiprocessing', 'greatly computational time'), 'help') (('that', 'training'), 'function') (('Kaggle submission', 'April here 28th'), 'make') (('data where rows', 'information'), 'lead') (('It', 'them'), 'be') (('randomly they', 'model original data'), 'choose') (('therefore best where individual weaknesses', 'somewhat Demir'), 'perform') (('parallel development', 'them'), 'allow') (('which', 'grading'), 'disclose') (('which', 'single process'), 'allow') (('user', 'machine learning chosen algorithms'), 'leave') (('inexperience', 'algorithms'), 'appear') (('Feature reduction', 'equivalent individual models'), 'appear') (('it', 'one time'), 'take') (('true origin', 'author'), 'be') (('Code SetupBelow', 'code'), 'be') (('Preda 2019 model', 'LightGBM single author'), 'be') (('table', 'leader board CV public score'), 'see') (('I', 'presentation here only documentation purposes'), 'use') (('CV score', 'leader board test data at least small current public sample'), 'appear') (('signal', 'Nyquist 150k long hence criteria'), 'note') (('LightGBM model', '1'), 'improve') (('reference Preda 2019 script', '2'), 'have') (('Hyperparamater tuning', 'validation scores'), 'be') (('Genetic programming', 'inspired API'), 'learn') (('Removing', 'very tiny increase'), 'provide') (('result Only function', 'author'), 'seem') (('then it', 'function'), 'be') (('Processing', 'processes'), 'be') (('This', 'validation holdout still sets'), 'allow') (('This', 'correlation'), 'be') (('where it', 'validation data'), 'realization') (('Feature SelectionEarly', 'LightGBM model'), 'perform') (('author', 'algorithm'), 'have') (('frequency bands', 'that'), 'show') (('code herein', 'none text none Apache'), 'com') (('test over full set', 'contest non entries'), 'resolve') (('author', '24k row data features'), 'try') (('Preda 2019 script', 'Kaggle user'), 'reference') (('slicing', 'several objectives'), 'accomplish') (('random_state', '777 _ 12 verbosity 2 model'), '0') (('Andrew script', 'Andrew Lukayenko Lukayenko'), 'believe') (('cc paper', 'gradient boosting decision 6907 highly efficient tree'), 'lightgbm') (('only 13 341', 'leader public board'), 'be') (('CV that', 'scenario'), 'suffer') (('progress', 'effort'), 'be') (('validation CV scores', 'leader board higher public score'), 'appear') (('data analysis exploratory notebook', 'also same manner'), 'publish') (('this', 'Jupyter Singhal'), 'use') (('This', 'feature also importance'), 'output') (('This', 'which'), 'allow') (('trend feature', 'signal'), 'be') (('It', 'Kaggle probably environment'), 'test') (('that', 'continuous acoustic signal'), 'provide') (('course', 'final scoring'), 'reserve') (('colsample_bytree', '0'), 'set') (('Model StackingSeveral issues', 'project'), 'affect') (('difficult it', 'problem'), 'make') (('peaks', 'peak rather value'), 'function') (('hyper thread best at least a four core 8 it', '16 RAM'), 'work') (('notebook', 'university course'), 'design') (('AcknowledgementsI', '2019 work'), 'feel') (('stopping final point', 'ever model'), 'find') (('XGBoost model', '2'), 'note') (('benefit', 'compute 5 days time'), 'be') (('150k samples long Nyquist', 'local environment'), 'set') (('machine learning', 'more time ordered manner'), 'help') (('24 data 000 rows', 'then manner'), 'create') (('Numpy', 'feature creation van der Walt Colbert Varoquaux'), 'utilize') (('that', 'Kaggle leader well board'), 'be') (('that', 'overfitting'), 'perform') (('scripts Preda 20190 2019 report', 'CV scores'), 'low') (('they', 'then signal information'), 'slice') (('it', 'Lukayenko 2019 script'), 'appear') (('output', 'earthquake prediction times'), 'be') (('i', '_ _ name'), 'need') (('multiprocessing identifier', 'multiprocessing caller'), 'be') (('how signal', 'bandwidth limited features'), 'define') (('training data', 'so data'), 'contain') (('Experience', 'feature creation'), 'show') (('Model Cross Validation ResultsIndividual model cross validation CV Individual results', 'reference Kaggle two Kernels'), 'show') (('slice', '4k data randomly rows'), 'obtain') (('thus it', 'training data'), 'be') (('when remaining 87', 'test data'), 'appear') (('that', 'CPU treads capabilities'), 'remember') (('leader board Best public scores', 'LightGBM'), 'be') (('especially one', 'sample here 4194 data'), 'appear') (('laboratory test', 'fault line'), 'apply') (('it', 'challenge'), 'make') (('Feature Creation', 'index signal'), 'explore') (('so long it', 'it'), 'take') (('CV again score', 'Kaggle leader board much score'), 'change') (('error absolute root', 'fit num_iteration model'), 'square') (('that', 'Kaggle'), 'produce') (('I', 'more resources'), 'be') (('pole IIR Butterworth 4 filters', 'frequency bands'), 'utilize') (('script CV However scores', 'just 2'), 'appear') (('frequency Added bandwidth', 'time domain features'), 'limit') (('Stratified random sampling', 'data'), 'perform') (('It', 'project'), 'be') (('which', 'clearly models'), 'consider') (('project', 'P\u00e9rez Granger'), 'be') (('that', 'ths signal end'), 'function') (('university course', 'competition'), 'hope') (('com grvsinghal', 'windows'), 'speed') (('More success', 'failure'), 'achieve') (('These', 'Kaggle kernel script Preda'), 'source') (('Hyperparameter TuningEffective hyperparameter tuning', 'very large project'), 'prove') (('decision rather LightGBM it', 'model'), 'use') (('Alternate choices', 'frequency bands'), 'investigate') (('Many', 'when academic paper'), 'request') (('that', '0'), 'provide') (('two outside models', 'Preda'), 'be') (('LightGBM training times', 'XGBoost'), 'become') (('CV now score', 'CV very random sampling'), 'be') (('models', 'cross validation CV'), 'be') (('reader', 'Kaggle'), 'be') (('that', 'test potential data'), 'build') (('data whole set', 'multiple times once process'), 'mean') (('model stack', 'Scirpus 2019 script'), 'submit') (('Jupyter it', 'Windows'), 'make') (('SciPy', 'correlation metrics Jones especially E.'), 'utilize') (('This', 'Light GBM'), 'be') (('full links', 'scripts'), 'find') (('author', 'work'), 'request') (('this', 'hyperparameter especially tuning'), 'seem') (('Consequently individual functions', 'even when multiprocessing'), 'require') (('test', 'two days'), 'run') (('This', 'many other machine learning algorithms'), 'appear') (('random selection', '6 segments'), 'use') (('com artgor earthquakes', 'Mart A.'), 'fe') (('that earthquake', 'laboratory test'), 'be') (('These', 'bandwidth limited manner'), 'add') (('model', 'up to 100k estimators'), 'choose') (('minimum data', 'LightGBM'), 'decrease') (('this', 'overfitting'), '667') (('peak mostly values', 'slope merit'), 'survive') (('periodicity', 'earthquake'), 'tend') (('leader hopefully well full board 3 models', 'here Kaggle'), 'average') (('These', 'CatBoost Prokhorenkova Gusev Vorobev Dorogush gplearn 2017 genetic library'), 'be') (('one challenge', 'given signal'), 'be') (('Keeping', 'probably new breakthroughs'), 'require') (('CV', 'direct model'), 'argue') (('zero', 'fit perfect Bilogur'), 'be') (('XGBoost model', '1'), 'have') (('leader when full private board', 'competition'), 'be') (('Exploratory data analysis EDA', 'github repository https github'), 'perform') (('which', 'test signals'), 'use') (('it', 'two days'), 'call') (('even more computationally effort', 'problem'), 'drop') (('where folds', '1'), 'improve') (('as more entrants', 'models'), 'come') (('statisticians', '0'), 'consider') (('Code', 'notebook'), 'test') (('556', 'script'), 'present') (('code', 'Python'), 'write') (('divergence', 'Kaggle scoring'), 'fall') (('script', 'Scirpus'), 'be') (('sample 150k overlap', 'signal leakage'), 'be') (('method', 'training sets'), 'help') (('this', '3200'), 'be') (('script', 'author'), 'be') (('it', 'three days'), 'call') (('where rows', 'above'), 'change') (('Build', 'Kaggle test data files'), 'feature') (('significant number', 'modeling'), 'have') (('that', 'Kaggle test samples'), 'use') (('overnight even more computers', 'project'), 'take') (('ConclusionsIt', 'Kaggle competition'), 'tempting') (('windowing', 'filter'), 'subject') (('First it', 'original signal'), 'tend') (('methods', 'leader board very Kaggle public scores'), 'give') (('scoring Accuracy position', 'model training additional time'), 'appear') (('value', 'feature reduction algorithm'), '05') (('Pandas', 'data manipulation'), 'be') (('It', 'more hyperparameter tuning'), 'be') (('training only 4194 rows', 'digital filters'), 'help') (('However it', 'feature scores'), 'be') (('data', 'CV score'), 'be') (('XGBoost both', 'cross validation substantially different methods'), 'be') (('phase response', 'Fourier transform'), 'base') (('24 data Then 000 rows', 'signal'), 'create') (('author', '1'), 'obtain') (('models', '30 XGBoost'), 'require') (('CatBoost', 'boosting modern gradient also further study'), 'give') (('function', 'below training sets'), 'call') (('function', 'model building'), 'join') (('that', 'Fourier analysis'), 'be') (('script', 'reasonable time'), 'ensure') (('This', 'worse overfitting'), 'result') (('it', 'test better present'), 'be') (('very slight author', '6 segments'), 'be') (('also as well XGBoost', 'feature generation set'), 'perform') (('Problem ApproachThis problem', 'regression here modeling'), 'approach') (('The', 'term long average'), 'refer') (('author', 'cross validation random strategy'), 'for') (('so it', 'CV too numerous here superb scores'), 'be') (('CatBoost', 'unbiased categorical features'), 'boost') (('Regis University Denver CO IntroductionPresented', 'Kaggle LANL Earthquake Challenge Rouet Leduc et'), 'Mason') (('algorithm', 'signal'), 'use') (('This', 'rather 6 new files'), 'read') (('that', 'time'), 'cause') (('as well many too numerous that', 'Kaggle website'), 'take') (('training data', 'acoustic sample'), 'accompany') (('one', 'cutoff various values'), 'continue') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["absolute", "account", "adjust", "advantage", "algorithm", "appear", "apply", "approach", "area", "author", "average", "become", "best", "blog", "board", "boosting", "build", "call", "categorical", "challenge", "code", "color", "competition", "compute", "concurrent", "consider", "core", "correlation", "cost", "could", "course", "create", "creation", "criteria", "csv", "current", "cut", "data", "date", "day", "decision", "define", "degree", "detection", "development", "difference", "distance", "divergence", "diversity", "division", "doc", "document", "domain", "earthquake", "eda", "effect", "effort", "en", "end", "ensemble", "environment", "error", "evaluate", "even", "every", "experience", "experiment", "extract", "fail", "failure", "feature", "file", "filter", "filtered", "final", "fit", "float", "fold", "form", "found", "frequency", "function", "general", "generated", "generation", "gradient", "grid", "ground", "help", "helper", "high", "http", "hyperparameter", "id", "idea", "importance", "increase", "index", "individual", "input", "io", "issue", "item", "kernel", "leaf", "learn", "learning", "learning_rate", "least", "left", "length", "level", "library", "lightgbm", "line", "linear", "little", "load", "local", "lot", "lower", "magnitude", "main", "manner", "max_depth", "mean", "memory", "method", "metric", "might", "minimum", "model", "most", "multiple", "multiprocessing", "my", "name", "near", "new", "next", "no", "noise", "non", "none", "not", "notebook", "number", "numerical", "open", "order", "ordered", "out", "output", "overall", "overfit", "overfitting", "overlap", "parallel", "parameter", "part", "partial", "path", "peak", "pearsonr", "per", "perform", "picture", "place", "point", "position", "potential", "power", "predict", "prediction", "predictor", "present", "problem", "processing", "project", "provide", "public", "python", "question", "random", "range", "ranking", "raw", "re", "read", "reader", "reason", "reduce", "reference", "region", "regression", "relationship", "report", "repository", "response", "result", "review", "risk", "row", "run", "running", "sample", "sampling", "save", "scenario", "scikit", "scipy", "score", "scoring", "script", "section", "selected", "selection", "sense", "separate", "set", "setup", "several", "shear", "short", "signal", "similar", "single", "six", "size", "slice", "something", "source", "speed", "split", "splitting", "spread", "squared", "stack", "state", "style", "submission", "supervised", "support", "system", "table", "target", "technique", "tensorflow", "term", "test", "testing", "text", "those", "threshold", "time", "tqdm", "train", "training", "transfer", "transform", "tree", "trend", "try", "tuning", "type", "under", "understanding", "until", "up", "usage", "user", "valid", "validation", "value", "variance", "view", "vision", "while", "width", "work"], "potential_description_queries_len": 280, "potential_script_queries": ["convolve", "multiprocessing", "numpy", "tqdm", "xgboost"], "potential_script_queries_len": 5, "potential_entities_queries": ["best", "boosting", "core", "current", "decision", "even", "generation", "hyperparameter", "kernel", "least", "new", "none", "potential", "public", "regression", "row", "script", "single", "text", "training"], "potential_entities_queries_len": 20, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 283}