{"name": "drgrading ", "full_name": " h1 Train Data Read h1 Data Generator h1 Model ResNet50 h1 Model Compile and Fit ", "stargazers_count": 0, "forks_count": 0, "description": "read_csv import os for dirname _ filenames in os. Train Data Read Data Generator Model ResNet50 Model Compile and Fit import numpy as np linear algebra import pandas as pd data processing CSV file I O e. join dirname filename Here we will perform an 90 10 split of the dataset with stratification to keep similar distribution in validation set I will oversample such that all classes have the same number of images as the maximum filling images filling labels Retrieve filters First Layer Second Layer Third Layer Final Step Add shortcut to F X and pass it through relu activation Retrieve Filters First Layer Second Layer Third Layer Shortcut Path Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 output layer Create model model. walk kaggle input for filename in filenames print os. summary val_generator data_gen val_df enc validation_data val_generator validation_steps int len val_df bs serialize and save model serialize model to JSON serialize weights to HDF5. ", "id": "karticamodi/drgrading", "size": "74", "language": "python", "html_url": "https://www.kaggle.com/code/karticamodi/drgrading", "git_url": "https://www.kaggle.com/code/karticamodi/drgrading", "script": "Flatten sklearn.utils sklearn.metrics data_gen keras.layers ZeroPadding2D identity_block train_test_split make_classification convolutional_block imread shuffle Conv2D get_file AveragePooling2D numpy Input model_to_dot confusion_matrix sklearn.model_selection Resnet50 image matplotlib.pyplot Activation Dense tensorflow keras.utils pandas OneHotEncoder resize sklearn.datasets imshow BatchNormalization plot_model preprocess_input load_model keras.utils.data_utils keras.preprocessing keras.initializers balance_data Model layer_utils layers MaxPooling2D Add keras.applications.imagenet_utils sklearn.preprocessing keras keras.utils.vis_utils glorot_uniform keras.models cv2 ", "entities": "(('classes', 'relu activation Retrieve First Layer Second Layer Third Layer Shortcut output Filters Path Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 layer'), 'filename') (('read_csv import', '_ filenames os'), 'os') (('I', 'pd data processing CSV file'), 'Model') (('summary val_generator data_gen', 'HDF5'), 'serialize') ", "extra": "", "label": "No_extra_files", "potential_description_queries": ["data", "dataset", "distribution", "file", "filename", "import", "input", "int", "join", "kaggle", "layer", "len", "linear", "maximum", "model", "np", "number", "numpy", "output", "pd", "perform", "print", "processing", "save", "serialize", "set", "similar", "split", "summary", "through", "validation", "walk"], "potential_description_queries_len": 32, "potential_script_queries": ["cv2", "image", "imread", "imshow", "resize", "shuffle", "tensorflow"], "potential_script_queries_len": 7, "potential_entities_queries": ["output"], "potential_entities_queries_len": 1, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 38}