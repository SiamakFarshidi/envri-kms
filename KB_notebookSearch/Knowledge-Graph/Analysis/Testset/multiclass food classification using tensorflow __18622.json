{"name": "multiclass food classification using tensorflow ", "full_name": " h1 Multiclass Classification using Keras and TensorFlow on Food 101 Dataset h3 Overview h3 Download and extract Food 101 Dataset h3 Understand dataset structure and files h3 Visualize random image from each of the 101 classes h3 Split the image data into train and test using train txt and test txt h3 Create a subset of data with few classes 3 train mini and test mini for experimenting h3 Fine tune Inception Pretrained model using Food 101 dataset h3 Visualize the accuracy and loss plots h3 Predicting classes for new images from internet using the best trained model h3 Fine tune Inceptionv3 model with 11 classes of data h3 Model Explainability h3 Let us now check the model we trained and understand how it sees and classifies h3 Look into the sparse activations in the layer activation 1 h3 Attribution h3 See how the class activation map looks for a different image h3 Lets see if we can break the model or see what it does when we surpise it with different data h3 More surprise data to the model h3 QUESTION h3 ANSWER h3 Summary of the things I tried h3 Further Improvements h3 References h3 Feedback ", "stargazers_count": 0, "forks_count": 0, "description": "So why are we giving it input images with multiple classes ANSWER Once we train a model it is also necessary to try it in a way we know it may not work like giving images with multiple classes of objects in it. ch datasets_extra food 101 This dataset has 101000 images in total. They save a lot of time and computation. html The Building Blocks of Interpretability https distill. All the filters that are used in different layers come from this pretrained model Look into the sparse activations in the layer activation_1 We have two blank sparse activations in layer 6 Below cell displays one of the sparse activations We can see that the activation has all nan values it was all zeros when executed outside Kaggle Im yet to figure out why its showing all nan values here To know why we have all zero nan values for this activation lets visualize the activation at same index 13 from previous layer All the values in the above activation map from the layer batch_normalization_1 are negative This activation in batch_normalization_1 is passed to the next layer activation_1 as input As the name says activation_1 is an activation layer and ReLu is the activation function used ReLu takes an input value returns 0 if its negative the value otherwise Since the input to activation array contains all negative values the activation layer fills its activation map with all zeros for the index Now we know why we have those 2 sparse activations in activation_1 layer Show the activation outputs of 1st 2nd and 3rd Conv2D layer activations to compare how layers get abstract with depth Attribution So far we were doing activation maps visualization This helps in understanding how the input is transformed from one layer to another as it goes through several operations At the end of training we want the model to classify or detect objects based on features which are specific to the class For example when training a dogs vs cats classifier the model should detect dogs based on features relevant to dog but not cats To validate how model attributes the features to class output we can generate heat maps using gradients to find out which regions in the input images were instrumental in determining the class In the above plot we see on left the input image passed to the model heat map in the middle and the class activation map on right Heat map gives a visual of what regions in the image were used in determining the class of the image Now it s clearly visible what a model looks for in an image if it has to be classified as an applepie See how the class activation map looks for a different image We can see how the heat map is different for a different image i. 9 More surprise data to the model. io Twitter https twitter. txt contains the list of all classes of food Visualize random image from each of the 101 classes Split the image data into train and test using train. jpg The above image is taken from the paper Visualizing and Understanding Convolutional Networks https arxiv. Also this time lets randomly pick the food classes Create the new data subset of n classes Let s use a pretrained Inceptionv3 model on subset of data with 11 food classes Loading the best saved model to make predictions Downloading images from internet using the URLs If you have an image in your local computer and want to try it uncomment the below code to upload the image files from google. upload Make a list of downloaded images and test the trained model Load the saved model trained with 3 classes normalize tensor center on 0. We flipped the image but the model didnt flip its output It s an apple pie again with 52 confidence QUESTION But hey we did not train our model to do multi label classification. txt Create a subset of data with few classes 3 train_mini and test_mini for experimenting Fine tune Inception Pretrained model using Food 101 dataset Visualize accuracy and loss plots Predicting classes for new images from internet Scale up and fine tune Inceptionv3 model with 11 classes of data Model Explainability Summary of the things I tried Further improvements Feedback Download and extract Food 101 Dataset Commented the below cell as the Food 101 dataset is available from Kaggle Datasets and need not be downloaded. txt contains the list of images that belong to test set classes. io building powerful image classification models using very little data. This way the model also learns how to classify totally unseen unrelated data I am yet to try these methods and not sure about the results Recently published paper Rethinking ImageNet Pretraining https arxiv. pub 2017 feature visualization Feedback Did you find any issues with the above code or have any suggestions or corrections There must be many ways to improve the model its architecture hyperparameters. This will lead to new observations insights and maybe new conclusions if not inventions Further Improvements Try more augmentation on test images Fine tune the model on the entire dataset for a few epochs atleast Play with hyper parameters their values and see how it impacts model performance There is currently no implementation to handle out of distribution no class scenario. txt Create a subset of data with few classes 3 train_mini and test_mini for experimenting We now have train and test data ready But to experiment and try different architectures working on the whole data with 101 classes takes a lot of time and computation To proceed with further experiments I am creating train_min and test_mini limiting the dataset to 3 classes Since the original problem is multiclass classification which makes key aspects of architectural decisions different from that of binary classification choosing 3 classes is a good start instead of 2 Fine tune Inception Pretrained model using Food 101 dataset Keras and other Deep Learning libraries provide pretrained models These are deep neural networks with efficient architectures like VGG Inception ResNet that are already trained on datasets like ImageNet Using these pretrained models we can use the already learned weights and add few layers on top to finetune the model to our new data This helps in faster convergance and saves time and computation when compared to models trained from scratch We currently have a subset of dataset with 3 classes samosa pizza and omelette Use the below code to finetune Inceptionv3 pretrained model Visualize the accuracy and loss plots The plots show that the accuracy of the model increased with epochs and the loss has decreased Validation accuracy has been on the higher side than training accuracy for many epochs This could be for several reasons We used a pretrained model trained on ImageNet which contains data from a variety of classes Using dropout can lead to a higher validation accuracy Predicting classes for new images from internet using the best trained model Setting compile False and clearing the session leads to faster loading of the saved model Withouth the above addiitons model loading was taking more than a minute Yes The model got them all right Fine tune Inceptionv3 model with 11 classes of data We trained a model on 3 classes and tested it using new data The model was able to predict the classes of all three test images correctly Will it be able to perform at the same level of accuracy for more classes FOOD 101 dataset has 101 classes of data Even with fine tuning using a pre trained model each epoch was taking more than an hour when all 101 classes of data is used tried this on both Colab and on a Deep Learning VM instance with P100 GPU on GCP But to check how the model performs when more classes are included I m using the same model to fine tune and train on 11 randomly chosen classes The plots show that the accuracy of the model increased with epochs and the loss has decreased Validation accuracy has been on the higher side than training accuracy for many epochs This could be for several reasons We used a pretrained model trained on ImageNet which contains data from a variety of classes Using dropout can lead to a higher validation accuracy The model did well even when the number of classes are increased to 11 Model training on all 101 classes takes some time It was taking more than an hour for one epoch when the full dataset is used for fine tuning Model Explainability Human lives and Technology are blending more and more together The rapid advancements in technology over the past few years can be attributed to how Neural Networks have evolved Neural Networks and Deep Learning are now being used in so many fields and industries healthcare finance retail automative etc Thanks to the Deep Learning libraries which enable us to develop applications models with few lines of code which a decade ago only those with a lot of expertise and research could do All of this calls for the need to understand how neural networks do what they do and how they do it This has led to an active area of research Neural Network Model Interpretability and Explainability https s3 media2. jpg Overview Download and extract Food 101 dataset Understand dataset structure and files Visualize random image from each of the 101 classes Split the image data into train and test using train. We already have in these few layers activations which are blank sparse for ex the 2 blank activations in the layer activation_1 These blank sparse activations are caused when any of the filters used in that layer didn t find a matching pattern in the input given to it By plotting more layers specially those towards the end of the network we can observe more of these sparse activations and how the layers get more abstract Get the activations for a different input food The feature maps in the above activations are for a different input image We see the same patterns discussed for the previous input image It is interesting to see the blank sparse activations in the same layer activation_1 and for same filters when a different image is passed to the network Remember we used a pretrained Inceptionv3 model. org publication preview visualizing and understanding convolutional networks page 4 medium. com bphoto 7BlRoSOG3AsAWHMPOaG7ng ls. html Helper method to split dataset into train and test folders Prepare train dataset by copying images from food 101 images to food 101 train using the file train. txt Check how many files are in the train folder Check how many files are in the test folder List of all 101 types of foods sorted alphabetically remove. txt Prepare test data by copying images from food 101 images to food 101 test using the file test. 4 confidence and an applie pie with 18 confidence Now let s flip the image vertically and see what the model does Well the model flipped its output too The model now thinks its an apple pie with 49. Compute the gradient of the input picture wrt this loss Normalization trick we normalize the gradient This function returns the loss and grads given the input picture We start from a gray image with some noise Run gradient ascent for 40 steps Now let s display our feature maps This is the number of features in the feature map The feature map has shape 1 size size n_features We will tile the activation channels in this matrix We ll tile each filter into this big horizontal grid Post process the feature to make it visually palatable Display the grid We start with index 1 instead of 0 as input layer is at index 0 We now initialize a model which takes an input and outputs the above chosen layers Get the index of activation_1 layer which has sparse activations Get the index of batch_normalization_1 layer which has sparse activations. Can try below methods Set a threshold for the class with highest score. But that gets too tedious So let s choose a few layers to visualize As seen below the 10 chosen layers contain 3 convolution 3 batch normalization 3 activation and 1 max pooling layers Get the names of all the selected layers Provide an input to the model and get the activations of all the 10 chosen layers activations contain the outputs of all the 10 layers which can be plotted and visualized Visualize the activations of intermediate layers from layer 1 to 10 What we see in the above plots are the activations or the outputs of each of the 11 layers we chose The activations or the outputs from the 1st layer conv2d_1 don t lose much information of the original input They are the results of applying several edge detecting filters on the input image With each added layer the activations lose visual input information and keeps building on the class ouput information As the depth increases the layers activations become less visually interpretabale and more abstract By doing so they learn to detect more specific features of the class rather than just edges and curves We plotted just 10 out of 314 intermediate layers. 08883 claims that training from random initialization instead of using pretrained weights is not only robust but also gives comparable results Pre trained models are surely helpful. 2901 The image contains the features of a trained model along with the kind of objects they would detect In the first row and first column we have a grid of edge detecting features in layer 1 and some curve detectors in layer 2 in the 2nd column The last column in 1st row are the kind of objects that get detected using those curvy features With layer three in 2nd row the model starts looking for patterns with edges and curves The second column in second row contains examples of patterns that are detected in layer 3 of the model With layer 4 the model starts detecting parts of object specific features and in layer 5 the model knows what s in the image Using feature visualization we can know what a neural network layer and its features are looking for Using attribution we can understand how the features impact the output and what regions in the image led the model to the generated output Let us now check the model we trained and understand how it sees and classifies Load the saved model and a test image Summary of the model gives us the list of all the layers in the network along with other useful details Defining some helper functions Check how many layers are in the trained model this includes the 1st input layer as well Can we visualize the outputs of all the layers Yes we can. e the model looks for a totally different features regions if it has to classify it as a pizza Lets see if we can break the model or see what it does when we surpise it with different data We trained our model to perform multi class classification and it seems to be doing well with 95 of accuracy What will the model do when we give it an image which has more than one object that model is trained to classify Given an image with pizza and applepie the model thinks its a pizza with 75. io how convolutional neural networks see the world. This will lead to new observations insights and maybe new conclusions if not inventions Flipping the image vertically has flipped the outputs whereas doing a horizontal flip didnt cause any change in output class As a next step we can try with more images from different classes and check if the same pattern exists Summary of the things I tried This notebook is the refactored and organised version of all the experiments and training trials I made I used this very useful Keras blog https blog. html How Convolutional Neural Networks See the World https blog. upload Make a list of downloaded images and test the trained model Helper function to select n random food classes We are picking n random food classes Lets try with more classes than just 3. Please do let me know Avinash Kappa https theimgclist. This comes mostly in the form of intense colors and sometimes wrong labels. ch datasets_extra food 101 static img food 101. Understand dataset structure and files The dataset being used is Food 101 https www. The entire dataset is 5GB in size images folder contains 101 folders with 1000 images each Each folder contains images of a specific food class meta folder contains the text files train. DS_Store from the list Helper method to create train_mini and test_mini data samples removing dataset_mini if it already exists folders so that we will have only the classes that we want picking 3 food items and generating separate data folders for the same Loading the best saved model to make predictions Downloading images from internet using the URLs If you have an image in your local computer and want to try it uncomment the below code to upload the image files from google. 05 fontsize 24 helped me fix the suptitle overlapping with axes issue returns the list of all files present in each food category picks one food item from the list as choice takes a list and returns one random item https matplotlib. Yet that shouldn t be the reason to not try to train a model from scratch Time taking yet productive experiment would be to try and train a model on this dataset from scratch Do more experiments with Model Interpretability and see what can be observed References Deep Learning with Python by Francois Cholett must read really Building Powerful Image Classification Models https blog. It s a food dataset with 101 categories multiclass Each type of food has 750 training samples and 250 test samples Note found on the webpage of the dataset On purpose the training images were not cleaned and thus still contain some amount of noise. 7 confidence and a pizza with 31. This time it s applie pie with 73 and a pizza with 19 confidence Let s try one last horizontal flip this is the last really No surprise from model this time. jpg Neural Networks learn incrementally How does a neural network know what is in the image and how does it conclude that its a dog The best analogy to understand the incremental learning of the model here is to think about how we would hand sketch the dog You can t start right away by drawing eyes nose snout etc To have any of those dogly features you need a lot of edges and curves You start with edges lines put many of them together Use edges with curves to sketch patterns The patterns with more finer details will help us draw the visible features of a dog like eyes ears snout etc Neural networks adopt a very similar process when they are busy detecting what s in the provided data examples https images. Multiclass Classification using Keras and TensorFlow on Food 101 Dataset alt text https www. html for reference I spent considerable amount of time in fixing things even before getting to the model training phase For example it took some time to get the image visualization plots aligned withouth any overlap It is easier to go through a notebook and understand code someone else has taken hours to finish I started with VGG16 pretrained model. VGG was taking more time for each epoch and since inception was also giving good validation accuracy I chose Inception over VGG For data augmentation I sticked to the transformations used in the above blog I didnt use TTA except rescaling test images Model explainability helped me understand many new things Once we train a model it is also necessary to try it in a way we know it may not work like giving images with multiple classes of objects in it. org users tight_layout_guide. It did give good validation accuracy after training for few epochs I then tried Inceptionv3. com in avinash kappa Check if GPU is enabled Helper function to download data and extract Download data and extract it to folder Uncomment this below line if you are on Colab Check the extracted dataset folder Visualize the data showing one image per class from 101 classes Adding y 1. colab import files image files. pub 2018 building blocks Feature Visualization https distill. When model gives prediction score below the threshold for its top prediction the prediction can be classified as NO CLASS UNSEEN Add a new class called NO CLASS provide data from different classes other than those in the original dataset. All images were rescaled to have a maximum side length of 512 pixels. 1 clip to 0 1 convert to RGB array Build a loss function that maximizes the activation of the nth filter of the layer considered. txt contains the list of images that belong to training set test. com avinashso13 Linkedin https www. ", "id": "theimgclist/multiclass-food-classification-using-tensorflow", "size": "18622", "language": "python", "html_url": "https://www.kaggle.com/code/theimgclist/multiclass-food-classification-using-tensorflow", "git_url": "https://www.kaggle.com/code/theimgclist/multiclass-food-classification-using-tensorflow", "script": "Flatten files tensorflow.keras.optimizers google.colab deprocess_image tensorflow.keras.layers GlobalAveragePooling2D tensorflow.keras.applications.inception_v3 tensorflow.keras.preprocessing.image tensorflow.keras.callbacks tensorflow.keras.regularizers dataset_mini generate_pattern rmtree Dropout Sequential SGD tensorflow.keras.models prepare_data defaultdict collections AveragePooling2D numpy InceptionV3 models shutil copy get_data_extract get_activations get_attribution ModelCheckpoint ImageDataGenerator copytree image pick_n_random_classes matplotlib.pyplot Activation Dense tensorflow l2 predict_class tensorflow.keras.backend tensorflow.keras CSVLogger load_model regularizers show_activations plot_loss Model MaxPooling2D tensorflow.keras.preprocessing keras Convolution2D matplotlib.image plot_accuracy ZeroPadding2D ", "entities": "(('I', 'then Inceptionv3'), 'give') (('I', 'Keras blog https very useful blog'), 'lead') (('we', 'Inceptionv3 pretrained model'), 'have') (('I', 'Recently paper'), 'learn') (('feature pub 2017 you', 'architecture hyperparameters'), 'visualization') (('training images', 'noise'), 's') (('Lets', 'just 3'), 'make') (('model', '75'), 'look') (('I', 'Kaggle Datasets'), 'create') (('above image', 'paper'), 'take') (('how many files', 'foods'), 'check') (('only also comparable Pre trained models', 'instead pretrained weights'), 'claim') (('CLASS UNSEEN', 'original dataset'), 'classify') (('that', 'test set classes'), 'contain') (('that', 'layer'), 'build') (('upload', '0'), 'make') (('too model', '49'), 'let') (('heat how map', 'i.'), 'come') (('This', 'intense colors'), 'come') (('rather just We', '314 intermediate layers'), 'get') (('jpg Overview Download', 'train'), 'Split') (('me', 'Avinash Kappa https theimgclist'), 'let') (('it', 'it'), 'take') (('images', '512 pixels'), 'rescale') (('Convolutional Neural How Networks', 'World https blog'), 'html') (('how it', 'class scenario'), 'lead') (('io how convolutional neural networks', 'world'), 'see') (('what', 'Image Classification Models https really Powerful blog'), 'be') (('this', 'last really model'), 's') (('what', 'https images'), 'learn') (('ch datasets_extra 101 dataset', 'total'), 'food') (('you', 'google'), 'DS_Store') (('I', 'VGG16 pretrained model'), 'html') (('choice', 'item https one random matplotlib'), 'help') (('Visualize random image', 'train'), 'contain') (('you', 'google'), 'pick') (('we', 'layers'), '2901') (('which', 'sparse activations'), 'normalize') (('it', 'it'), 'give') (('folder', 'text files train'), 'be') (('dataset', 'dataset structure'), 'be') (('we', 'label multi classification'), 'flip') (('This', 'research Neural Network Explainability https s3 Model Interpretability media2'), 'create') (('org publication preview', '4 medium'), 'page') (('that', 'set test'), 'contain') (('dataset folder extracted data', '1'), 'com') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["abstract", "accuracy", "active", "architecture", "area", "array", "augmentation", "batch", "become", "best", "binary", "blog", "category", "cause", "cell", "center", "check", "choice", "choose", "classification", "classifier", "classify", "clip", "code", "colab", "column", "compare", "compile", "computation", "computer", "confidence", "contain", "convert", "convolution", "convolutional", "could", "create", "curve", "data", "dataset", "depth", "detect", "detected", "develop", "display", "distribution", "download", "draw", "edge", "enable", "end", "epoch", "even", "ex", "experiment", "extract", "faster", "feature", "figure", "file", "filter", "find", "fix", "flip", "folder", "form", "found", "function", "generate", "generated", "gradient", "gray", "grid", "hand", "handle", "help", "helper", "image", "img", "implementation", "import", "improve", "inception", "index", "initialize", "input", "instance", "io", "issue", "item", "kappa", "key", "label", "layer", "lead", "learn", "learning", "left", "length", "let", "level", "line", "list", "little", "local", "looking", "lot", "map", "matching", "matrix", "max", "maximum", "meta", "method", "middle", "minute", "model", "multiple", "name", "need", "negative", "network", "neural", "new", "next", "no", "noise", "normalization", "normalize", "not", "notebook", "number", "object", "out", "output", "overlap", "page", "past", "pattern", "per", "perform", "performance", "picture", "pie", "plot", "plotting", "pooling", "pre", "predict", "prediction", "present", "pretrained", "problem", "provide", "pub", "publication", "purpose", "random", "read", "reason", "reference", "research", "right", "robust", "row", "save", "score", "scratch", "second", "select", "selected", "separate", "session", "set", "several", "shape", "side", "similar", "size", "sparse", "split", "start", "step", "structure", "subset", "surprise", "technology", "tensor", "test", "text", "think", "those", "threshold", "through", "tile", "time", "train", "training", "try", "tune", "tuning", "type", "understanding", "up", "validate", "validation", "value", "version", "visualization", "visualize", "work"], "potential_description_queries_len": 211, "potential_script_queries": ["copy", "copytree", "defaultdict", "l2", "numpy", "preprocessing", "shutil", "tensorflow"], "potential_script_queries_len": 8, "potential_entities_queries": ["convolutional", "neural", "pub", "random"], "potential_entities_queries_len": 4, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 218}