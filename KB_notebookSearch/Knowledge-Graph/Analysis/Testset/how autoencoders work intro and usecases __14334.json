{"name": "how autoencoders work intro and usecases ", "full_name": " h2 How Autoencoders work Understanding the math and implementation h3 Contents h2 1 Introduction h2 1 1 What are Autoencoders h2 1 2 How Autoencoders work h2 Different Rules for Different data h2 2 Implementation h2 2 1 UseCase 1 Image Reconstruction h3 2 Dataset Prepration h3 3 Create Autoencoder architecture h2 2 2 UseCase 2 Image Denoising h3 Noisy Images h2 2 3 UseCase 3 Sequence to Sequence Prediction using AutoEncoders h4 Autoencoder Architecture h3 Excellent References ", "stargazers_count": 0, "forks_count": 0, "description": "A typical autoencoder architecture comprises of three main components Encoding Architecture The encoder architecture comprises of series of layers with decreasing number of nodes and ultimately reduces to a latent view repersentation. The answer is straigtforward there is no fixed equation but a best possible equation is obtained by the unsupervised learning process. Noisy Images We can intentionally introduce the noise in an image. u1B u2B encoding where W is the weight matrix of hidden layer. d L d 0 after rotation This is the output of the encoding process and repersents our data in low dimensions. But the key question here is with what logic or rule point B can be represented in terms of A and angle L. Unlike CNNs in image example in this use case we will use LSTMs. To repersent this data we are currently using 2 dimensions X and Y. If we recall the fundamental equation of a neural network with weights and bias of every layer then d 0 W. The following image describes this property https i. How Autoencoders work Understanding the math and implementation Contents 1. In the previous example we input an image which was a basicaly a 2 dimentional data in this example we will input a sequence data as the input which will be 1 dimentional. Encoding architecture is connected to latent view space comprising of 10 nodes which is then connected to decoding architecture with 500 1200 and 2000 nodes. Thanks to ColinMorris for suggesting the correction in salt and pepper noise. But instead of 3 maxpooling layers we will be adding 3 upsampling layers. Most of the code of this section is taken from the following reference shared by Jason Brownie in his blog post. png A highly fine tuned autoencoder model should be able to reconstruct the same input which was passed in the first layer. To apply convolutions on image data we will reshape our inputs in the form of 28 28 matrix. If we can define following Reference Point on the line A Angle L with a horizontal axis then any other point say B on line A can be repersented in terms of Distance d from A and angle L. We can define the number of LSTM memory units in the LSTM layer Each unit or cell within the layer has an internal memory cell state often abbreviated as c and outputs a hidden state often abbreviated as h. Lets first of all generate a sequence dataset containing random sequences of fixed lengths. Again the activation function will be same relu and padding in convolution layers will be same as well. com blog 2018 06 unsupervised deep learning computer vision 2. Here is the summary of our autoencoder architecture. 3 UseCase 3 Sequence to Sequence Prediction 1. Before adding noiseAfter adding noiseLets now create the model architecture for the autoencoder. Lets obtain the predictions of the modelIn this implementation I have not traiened this network for longer epoochs but for better predictions you can train the network for larger number of epoochs say somewhere in the range of 500 1000. The encoding part comprises of three layers with 2000 1200 and 500 nodes. One such variation can be introduction of noise. No matter how much shifts and rotation are applied original data cannot be recovered. 2 UseCase 2 Image DenoisingAutoencoders are pretty useful lets look at another application of autoencoders Image denoising. Unlike other recurrent neural networks the network s internal gates allow the model to be trained successfully using backpropagation through time or BPTT and avoid the vanishing gradients problem. jpg In this autoencoder network we will add convolutional layers because convolutional networks works really well with the image inputs. png Step1 Repersent the points in Latent View Space If the coordinates of point A and B in the data representation space are Point A x1A x2A Point B x1B x2B then their coordinates in the latent view space will be x1A x2A 0 0 x1B x2B u1B u2B Point A 0 0 Point B u1B u2B Where u1B and u2B can be represented in the form of distance between the point and the reference point u1B x1B x1A u2B x2B x2A Step2 Represent the points with distance d and angle L Now u1B and u2B can represented as a combination of distance d and angle L. Relu is used as the activation function in the convolution layers and padding is kept as same. Lets understand this process from a autoencoder perspective. Encoding Architecture The encoding architure is composed of 3 Convolutional Layers and 3 Max Pooling Layers stacked one by one. Similarly the decoding architecture converts back this representation to original form u1B u2B and then x1 x2. The main idea behind autoencoders is to learn a low level repersenation of a high level dimentional data. And if we rotate this by angle L towards the horizontal axis L will become 0. 1 UseCase 1 Image Reconstruction1. com wp content uploads 2017 11 denoising autoencoder 600x299. Different types of noises can be added to the images. Next we will train the model with early stopping callback. io building autoencoders in keras. sample rate the frequency of the signal the points on the x axis for plotting Lets add sample noise Salt and Pepper input layer encoding architecture decoding architecture compile the model convert the elements to categorical using keras api remove unnecessary dimention. First lets prepare the train_x and val_x data contianing the image pixels. Lets see it in action. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. Consider a data repersentation space N dimentional space which is used to repersent the data and consider the data points repersented by two variables x1 and x2. These low level features are then deformed back to project the actual data. com develop encoder decoder model sequence sequence prediction keras Autoencoder Architecture The architecuture of this use case will contain an encoder to encode the source sequence and second to decode the encoded source sequence into the target sequence called the decoder. There are two components An encoder architecture which takes a sequence as input and returns the current state of LSTM as the output A decoder architecture which takes the sequence and encoder LSTM states as input and returns the decoded output sequence We are saving and accessing hidden and memory states of LSTM so that we can use them while generating predictions on unseen data. Decoding Architecture Similarly in decoding architecture the convolution layers will be used having same dimentions in reverse manner as the encoding architecture. Or in other terms what is the equation among B A and L. First lets understand the internal working of LSTMs which will be used in this architecture. It presents itself as sparsely occurring white and black pixels. When this type of data is projected in latent space a lot of information is lost and it is almost impossible to deform and project it to the original shape. 2 How Autoencoders Work 2. 3 UseCase 3 Sequence to Sequence Prediction using AutoEncodersNext use case is sequence to sequence prediction. For example Salt and Pepper Noise Gaussian Noise Periodic Noise Speckle Noise Lets introduce salt and pepper noise to our data which is also known as impulse noise. d 0 decoding The reduced form of data x1 x2 is d 0 in the latent view space which is obtained from the encoding architecture. This noise introduces sharp and sudden disturbances in the image signal. An important point is that Rules Learning function encoding decoding equation will be different for different types of data. png Lets implement an autoencoder using keras that first learns the features from an image and then tries to project the same image as the output. Dataset Prepration Load the dataset separate predictors and target normalize the inputs. 1 What are Autoencoders Autoencoders are a special type of neural network architectures in which the output is same as the input. We will create a function to generate random sequences. For example in the previous example we projected a linear data manifold in one dimention and eliminated the angle L. In this type of data the key problem will be to obtain the projection of data in single dimention without loosing information. Lets look at other use case of autoencoders Image denoising or removal of noise from the image. Create Autoencoder architectureIn this section lets create an autoencoder architecture. Role of upsampling layer is to upsample the dimentions of a input vector to a higher resolution dimention. Lets understand what type of network needs to be created for this problem. For example consider the following data manifold view. This usecase can be applied in machine translation. Data Manifold is the space inside the data repersentation space in which the true data resides. For more information related to CNN refer to my previous kernel https www. Decoding Architecture The decoding architecture is the mirro image of the encoding architecture but in which number of nodes in every layer increases and ultimately outputs the similar almost input. An autoencoder is a regression task where the network is asked to predict its input in other words model the identity function. I am using imaug package which can be used to augment the images with different variations. In this kernel I will walk you through the working of autoencoders and their implementation. Increase the number of epochs to a higher number for better results. Load the required libraries 2. Consider the autoencoder with no hidden layers the inputs x1 and x2 are encoded to lower repersentation d which is then further projected into x1 and x2. The final layer comprises of exact number of nodes as the input layer. com shivamb a very comprehensive tutorial nn cnn. https machinelearningmastery. So how does neural networks solves this problem The intution is In the manifold space deep neural networks has the property to bend the space in order to obtain a linear data fold view. For example consider the following data in 2dimentional space. Lets plot the original and predicted image Inputs Actual Images Predicted Autoencoder Output So we can see that an autoencoder trained with 20 epoochs is able to reconstruct the input images very well. Latent View Repersentation Latent view repersents the lowest level space in which the inputs are reduced and information is preserved. But it is possible to reduce the dimensions of this space into lower dimensions ie. Lets look at the model summariesNow lets train the autoencoder model using Adam optimizer and Categorical Cross Entropy loss functionLets write a function to predict the sequence based on input sequence Generate some predictions Excellent References1. edu people karpathy convnetjs demo autoencoder. 1 What are Autoencoders 1. This layer applies a max filter to non overlapping subregions of the initial representation. The max pooling operation is non invertible however an approximate inverse can be obtained by recording the locations of the maxima within each pooling region. By using Keras we can access both output states of the LSTM layer as well as the current states of the LSTM layers. Implementation and UseCases 2. The Long Short Term Memory or LSTM is a recurrent neural network that is comprised of internal gates. Autoencoder architectures applies this property in their hidden layers which allows them to learn low level representations in the latent view space. Many a times input images contain noise in the data autoencoders can be used to get rid of those images. 2 UseCase 2 Noise Removal 2. These networks has a tight bottleneck of a few neurons in the middle forcing them to create effective representations that compress the input into a low dimensional code that can be used by the decoder to reproduce the original input. com develop encoder decoder model sequence sequence prediction keras Thanks for viewing the kernel please upvote if you liked it. Autoencoders are widly used with the image data and some of their use cases are Dimentionality Reduction Image Compression Image Denoising Image Generation Feature Extraction 1. Lets try to understand the encoding process with an example. 1 UseCase 1 Image Reconstruction 2. Example of sequence data are time series data and text data. X1 repersents the input sequence containing random numbers X2 repersents the padded sequence which is used as the seed to reproduce the other elements of the sequence y repersents the target sequence or the actual sequence Next lets create the architecture of our model in Keras. https towardsdatascience. Here is the model summaryTrain the model with early stopping callback. Role of max pooling layer is to downsample the image dimentions. In simple terms the learning process can be defined as a rule equation which converts B in the form of A and L. u1B u2B Inverse W. 2 How Autoencoders work Lets understand the mathematics behind autoencoders. com applied deep learning part 3 autoencoders 1c083af4d7983. Lets now create an autoencoder architecutre for learning and producing sequences made up of LSTM layers. Different Rules for Different dataSame rules cannot be applied to all types of data. Generate the predictions on validation data. But what if the data manifold cannot be projected properly. Umsampling layers make use of this property to project the reconstructions from a low dimentional feature space. Reference https machinelearningmastery. Since we know that the decoding process is the mirror image of the encoding process. ", "id": "shivamb/how-autoencoders-work-intro-and-usecases", "size": "14334", "language": "python", "html_url": "https://www.kaggle.com/code/shivamb/how-autoencoders-work-intro-and-usecases", "git_url": "https://www.kaggle.com/code/shivamb/how-autoencoders-work-intro-and-usecases", "script": "argmax randint keras.layers keras.callbacks plotly.offline random EarlyStopping MaxPool2D LSTM Conv2D iplot numpy plotly.graph_objs Input PIL sklearn.model_selection Image matplotlib.pyplot Dense keras.utils augmenters pandas to_categorical UpSampling2D reverse_onehot dataset_preparation init_notebook_mode Model imgaug define_models array_equal keras.models predict_sequence train_test_split ", "entities": "(('convolution layers', 'encoding architecture'), 'use') (('you', '500 1000'), 'obtain') (('Autoencoders', 'Image Generation'), 'use') (('non however approximate inverse', 'pooling region'), 'be') (('which', 'A'), 'define') (('following image', 'property https i.'), 'describe') (('Next actual lets', 'Keras'), 'repersent') (('Encoding encoder architecture', 'view ultimately latent repersentation'), 'comprise') (('it', 'dimensions lower ie'), 'be') (('edu people', 'demo autoencoder'), 'convnetjs') (('we', 'LSTM as well current layers'), 'access') (('Lets', 'autoencoder perspective'), 'understand') (('One such variation', 'noise'), 'be') (('key problem', 'information'), 'be') (('Autoencoders', 'input data'), 'train') (('output', 'input'), '1') (('we', 'upsampling 3 layers'), 'add') (('This', 'low dimensions'), 'd') (('A', 'L.'), 'say') (('Different Rules', 'data'), 'apply') (('pretty useful lets', 'autoencoders Image denoising'), '2') (('It', 'sparsely occurring white pixels'), 'present') (('First lets', 'image val_x pixels'), 'prepare') (('architecuture', 'target sequence'), 'develop') (('u2B u1B where W', 'weight hidden layer'), 'encode') (('equation', 'B A'), 'be') (('Encoding encoding architure', 'one'), 'architecture') (('We', 'random sequences'), 'create') (('which', 'different variations'), 'use') (('which', 'also impulse noise'), 'introduce') (('we', 'layer'), 'recall') (('that', 'original input'), 'have') (('which', 'architecture'), 'understand') (('which', 'first layer'), 'be') (('main idea', 'high level dimentional data'), 'be') (('summaryTrain', 'stopping early callback'), 'be') (('we', '28 28 matrix'), 'reshape') (('Umsampling layers', 'feature low dimentional space'), 'make') (('Most', 'blog post'), 'take') (('noise', 'image signal'), 'introduce') (('3 UseCase 3 Sequence', 'prediction'), 'be') (('which', 'input'), 'input') (('architecture decoding model', 'unnecessary dimention'), 'rate') (('we', 'angle'), 'project') (('rule here B', 'A'), 'be') (('it', 'original shape'), 'lose') (('work', 'autoencoders'), 'understand') (('decoding process', 'mirror encoding process'), 'know') (('Noisy We', 'image'), 'Images') (('Different types', 'images'), 'add') (('autoencoder', 'input images'), 'plot') (('we', 'LSTMs'), 'use') (('Now u1B', 'distance d'), 'repersent') (('layer', 'initial representation'), 'apply') (('section architectureIn lets', 'autoencoder architecture'), 'create') (('type', 'problem'), 'understand') (('decoding', 'data'), 'be') (('Next we', 'stopping early callback'), 'train') (('input Many times images', 'images'), 'contain') (('Role', 'resolution higher dimention'), 'be') (('summariesNow lets', 'predictions'), 'look') (('which', '500 1200 nodes'), 'connect') (('them', 'view latent space'), 'apply') (('level low features', 'then back actual data'), 'deform') (('that', 'output'), 'implement') (('Dataset Prepration', 'inputs'), 'Load') (('we', 'unseen data'), 'be') (('number', 'ultimately similar almost input'), 'decode') (('encoding part', '2000'), 'comprise') (('input', 'identity function'), 'be') (('Lets', 'image'), 'look') (('data points', 'variables two x1'), 'consider') (('which', 'then further x1'), 'encode') (('model', 'gradients vanishing problem'), 'allow') (('true data', 'which'), 'be') (('activation Again function', 'convolution same layers'), 'be') (('deep neural networks', 'view'), 'solve') (('recurrent neural that', 'internal gates'), 'be') (('L', 'horizontal axis'), 'become') (('com blog', 'computer 2018 06 deep learning vision'), 'unsupervise') (('Here summary', 'autoencoder architecture'), 'be') (('information', 'which'), 'repersent') (('Lets', 'fixed lengths'), 'generate') (('Lets', 'LSTM layers'), 'create') (('activation function', 'convolution layers'), 'use') (('How Autoencoders', 'math'), 'work') (('I', 'autoencoders'), 'walk') (('fixed best possible equation', 'learning unsupervised process'), 'be') (('final layer', 'input layer'), 'comprise') (('we', 'dimensions currently 2 X'), 'use') (('We', 'often h.'), 'define') (('usecase', 'machine translation'), 'apply') (('which', 'encoding architecture'), 'be') (('convolutional networks', 'image really well inputs'), 'jpg') (('Lets', 'example'), 'try') (('Role', 'image dimentions'), 'be') (('Example', 'sequence data'), 'be') (('you', 'it'), 'develop') ", "extra": "['biopsy of the greater curvature']", "label": "Perfect_files", "potential_description_queries": ["answer", "api", "application", "apply", "architecture", "article https towardsdatascience", "augment", "autoencoder", "backpropagation", "become", "best", "blog", "case", "categorical", "cell", "code", "compile", "computer", "consider", "contain", "content", "convert", "convolution", "convolutional", "correction", "create", "current", "data", "dataset", "decode", "decoder", "define", "develop", "distance", "downsample", "encode", "encoder", "encoding", "equation", "every", "feature", "filter", "final", "fixed", "fold", "following", "form", "frequency", "function", "generate", "high", "https machinelearningmastery", "https towardsdatascience", "idea", "identity", "image", "implement", "implementation", "input", "io", "itself", "karpathy", "kept", "kernel", "key", "layer", "learn", "learning", "level", "line", "linear", "look", "lost", "lot", "lower", "main", "manifold", "manner", "math", "matrix", "max", "memory", "middle", "model", "my", "network", "neural", "nn", "no", "noise", "non", "normalize", "not", "number", "operation", "optimizer", "order", "output", "package", "padding", "part", "people", "plot", "plotting", "png", "point", "pooling", "predict", "prediction", "prepare", "problem", "project", "projection", "property", "question", "random", "range", "recall", "reconstruct", "recording", "recurrent", "reduce", "reference", "regression", "remove", "representation", "reshape", "resolution", "reverse", "rotate", "sample", "saving", "second", "section", "separate", "sequence", "signal", "similar", "single", "source", "space", "special", "state", "summary", "target", "task", "text", "those", "through", "time", "train", "try", "tutorial", "type", "unit", "up", "upsampling", "validation", "variation", "vector", "view", "vision", "walk", "weight", "while", "work", "write"], "potential_description_queries_len": 167, "potential_script_queries": ["imgaug", "iplot", "numpy", "randint"], "potential_script_queries_len": 4, "potential_entities_queries": ["current", "encoding", "level", "neural"], "potential_entities_queries_len": 4, "potential_extra_queries": ["biopsy of the greater curvature", "biopsy", "curvature"], "potential_extra_queries_len": 3, "all_components_potential_queries_len": 174}