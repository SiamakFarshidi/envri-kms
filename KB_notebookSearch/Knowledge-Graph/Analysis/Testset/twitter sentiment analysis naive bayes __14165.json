{"name": "twitter sentiment analysis naive bayes ", "full_name": " h1 Naive Bayes h1 Part 1 Process the Data h2 Part 1 1 Implementing your helper functions h4 Instructions ", "stargazers_count": 0, "forks_count": 0, "description": "Expected Output 0. Compute the positive probability of each word P W_ pos negative probability of each word P W_ neg using equations 4 5. You can then compute the loglikelihood log left frac P W_ pos P W_ neg right tag 6. append word stemming word freqs. Append a dictionary to a list where the key is the word and the dictionary is the dictionary pos_neg_ratio that is returned by the get_ratio function. The prior is the ratio of the probabilities frac P D_ pos P D_ neg. 55 Part 4 Filter words by Ratio of positive to negative counts Some words have more positive counts than others and can be considered more positive. UNQ_C6 UNIQUE CELL IDENTIFIER DO NOT EDIT return this properly if the prediction is 0 the predicted class is 1 otherwise the predicted class is 0 append the predicted class to the list y_hats error is the average of the absolute values of the differences between y_hats and test_y Accuracy is 1 minus the error UNQ_C7 UNIQUE CELL IDENTIFIER DO NOT EDIT You do not have to input any code in this cell but it is relevant to grading so please do not change anything Run this cell to test your function print s f tweet naive_bayes_predict tweet logprior loglikelihood print f tweet p. Notice how the word rather appears twice in the list of tweets and so its count value is 2. In this freqs dictionary the key is the tuple word label The value is the number of times it has appeared. download twitter_samples Part 1 Process the DataFor any machine learning project once you ve gathered the data the first step is to process it to make useful inputs to your model. 1 Implementing your helper functionsTo help train your naive bayes model you will need to build a dictionary where the keys are a word label tuple and the values are the corresponding frequency. Assume that the result dictionary that is input will contain clean key value pairs you can assume that the values will be integers that can be incremented. Log likelihoodTo compute the loglikelihood of that very same word we can implement the following equations text loglikelihood log left frac P W_ pos P W_ neg right tag 6 Create freqs dictionary Given your count_tweets function you can compute a dictionary called freqs that contains all the frequencies. Naive BayesIf you are running this notebook in your local computer don t forget to download the twitter samples and stopwords from nltk. We ll also remove stock market tickers retweet symbols hyperlinks and hashtags because they can not tell you a lot of information on the sentiment. Part 5 Error AnalysisIn this part you will see some tweets that your model missclassified. However please remember to include the logprior because whenever the data is not perfectly balanced the logprior will be a non zero value. Add the pos_neg_ratio to the dictionary otherwise do not include this word in the list do nothing Test your function find negative words at or below a threshold Test your function find positive words at or above a threshold Some error analysis done for you Test with your own tweet feel free to modify my_tweet. Finally you want to use stemming to only keep track of one variation of each word. The key in the dictionary is a tuple containing the stemmed word and its class label e. that would not give us enough information on the sentiment. Calculate N_ pos and N_ neg Using freqs dictionary you can also compute the total number of positive words and total number of negative words N_ pos and N_ neg. P D_ neg is the probability that the document is negative. org wiki Additive_smoothing explains more about additive smoothing. Calculate the ratio of positive divided by negative counts ratio frac text pos_words 1 text neg_words 1 Where pos_words and neg_words correspond to the frequency of the words in their respective classes. Calculate V You can then compute the number of unique words that appear in the freqs dictionary to get your V you can use the set function. download stopwords nltk. 41 great great great great 8. Once we re able to calculate these ratios we can also filter a subset of words that have a minimum ratio of positivity negativity or higher. p logprior sum_i N loglikelihood_i NoteNote we calculate the prior from the training data and that the training data is evenly split between positive and negative labels 4000 positive and 4000 negative tweets. Note that we can also use the log likelihood calculations to compare relative positivity or negativity of words. For each tweet sum up loglikelihoods of each word in the tweet. The function takes in the tweet logprior loglikelihood. InstructionsCreate a function count_tweets that takes a list of tweets as input cleans all of them and returns a dictionary. This means that the ratio of positive to negative 1 and the logprior is 0. First use naive_bayes_predict function to make predictions for each tweet in text_x. Similarly we can also filter a subset of words that have a maximum ratio of positivity negativity or lower words that are at least as negative or even more negative than a given threshold. InstructionsGiven a freqs dictionary train_x a list of tweets and a train_y a list of labels for each tweet implement a naive bayes classifier. It is good practice to check the datatype before incrementing the value but it s not required here. We have given you the function process_tweet that does this for you. In other words if we had no specific information and blindly picked a tweet out of the population set what is the probability that it will be positive versus that it will be negative That is the prior. Notice how the words i and am are not saved since it was removed by process_tweet because it is a stopword. 29 this movie should have been great. Emojis like and words like me tend to have a negative connotation. Likewise some words can be considered more negative than others. You will create a probability for each class. 2f p_category Feel free to check the sentiment of your own tweet below UNQ_C8 UNIQUE CELL IDENTIFIER DO NOT EDIT use lookup to find positive counts for the word denoted by the integer 1 use lookup to find negative counts for the word denoted by integer 0 calculate the ratio of positive to negative counts for the word UNQ_C9 UNIQUE CELL IDENTIFIER DO NOT EDIT get the positive negative ratio for a word if the label is 1 and the ratio is greater than or equal to the threshold. We ll use these to compute the positive and negative probability for a specific word using this formula P W_ pos frac freq_ pos 1 N_ pos V tag 4 P W_ neg frac freq_ neg 1 N_ neg V tag 5 Notice that we add the 1 in the numerator for additive smoothing. In other words the positive frequency of a word is the number of times the word is counted with the label of 1. Also add the logprior to this sum to get the predicted sentiment of that tweet. Why do you think the misclassifications happened Were there any assumptions made by the naive bayes model Part 6 Predict with your own tweetIn this part you can predict the sentiment of your own tweet. This wiki article https en. 57 The sentiment is positive. The key is the word the value is the log likelihood of that word. You also want to remove all the punctuation from a tweet. We can calculate the ratio of positive to negative frequencies of a word. In other words we ll treat motivation motivated and motivate similarly by grouping them within the same stem of motiv. The reason for doing this is because we want to treat words with or without the punctuation as the same word instead of treating happy happy happy happy and happy. Calculate the probability that a document tweet is positive P D_ pos and the probability that a document tweet is negative P D_ neg Calculate the logprior the logprior is log D_ pos log D_ neg Calculate log likelihood Finally you can iterate over each word in the vocabulary use your lookup function to get the positive frequencies freq_ pos and the negative frequencies freq_ neg for that specific word. It takes a short time to train and also has a short prediction time. Implement get_ratio Given the freqs dictionary of words and a particular word use lookup freqs word 1 to get the positive count of the word. Remove noise You will first want to remove noise from your data that is remove words that don t tell you much about the content. 5 Notice the difference between the positive and negative ratios. The value the number of times this word appears in the given collection of tweets an integer. Add the pos_neg_ratio to the dictionary If the label is 0 and the pos_neg_ratio is less than or equal to the threshold. These include all common words like I you are is etc. We will use this dictionary in several parts of this assignment. You may find it useful to use the zip function to match each element in tweets with each element in ys. Calculate D D_ pos D_ neg Using the train_y input list of labels calculate the number of documents tweets D as well as the number of positive documents tweets D_ pos and number of negative documents tweets D_ neg. Expected Output The expected output is around 1. get the positive and negative frequency of the word calculate the probability that each word is positive and negative calculate the log likelihood of the word UNQ_C3 UNIQUE CELL IDENTIFIER DO NOT EDIT You do not have to input any code in this cell but it is relevant to grading so please do not change anything UNQ_C4 UNIQUE CELL IDENTIFIER DO NOT EDIT process the tweet to get a list of words initialize probability to zero add the logprior check if the word exists in the loglikelihood dictionary add the log likelihood of that word to the probability UNQ_C5 UNIQUE CELL IDENTIFIER DO NOT EDIT You do not have to input any code in this cell but it is relevant to grading so please do not change anything Experiment with your own tweet. If we set the label to 0 then we ll look for all words whose threshold of positive negative is at most as low as the given threshold or lower. get word label 0 print cleaned tweet UNQ_C1 UNIQUE CELL IDENTIFIER DO NOT EDIT define the key which is the word and label tuple if the key exists in the dictionary increment the count else if the key is new add it to the dictionary and set the count to 1 Testing your function Build the freqs dictionary for later uses UNQ_C2 UNIQUE CELL IDENTIFIER DO NOT EDIT calculate V the number of unique words in the vocabulary calculate N_pos and N_neg if the label is positive greater than zero Increment the number of positive words by the count for this word label pair else the label is negative increment the number of negative words by the count for this word label pair Calculate D the number of documents Calculate D_pos the number of positive documents hint use sum Calculate D_neg the number of negative documents hint compute using D and D_pos Calculate logprior For each word in the vocabulary. P D_ pos is the probability that the document is positive. Use the get_ratio function to get a dictionary containing the positive count negative count and the ratio of positive to negative counts. V is the number of unique words in the entire set of documents for all classes whether positive or negative. 0 means that when we add the logprior to the log likelihood we re just adding zero to the log likelihood. Remember to check if the key in the dictionary exists before adding that key to the dictionary or incrementing its value. Calculate freq_ pos and freq_ neg Using your freqs dictionary you can compute the positive and negative frequency of each word freq_ pos and freq_ neg. P W_ pos frac freq_ pos 1 N_ pos V tag 4 P W_ neg frac freq_ neg 1 N_ neg V tag 5 Note We ll use a dictionary to store the log likelihoods for each word. An example key value pair would have this structure happi positive 10 negative 20 ratio 0. Congratulations on completing this assignment. So how do you train a Naive Bayes classifier The first part of training a naive bayes classifier is to identify the number of classes that you have. Similarly use the lookup function to get the negative count of that word. Use the formulas as follows and store the values in a dictionary P D_ pos frac D_ pos D tag 1 P D_ neg frac D_ neg D tag 2 Where D is the total number of documents or tweets in this case D_ pos is the total number of positive tweets and D_ neg is the total number of negative tweets. Implement test_naive_bayes Instructions Implement test_naive_bayes to check the accuracy of your predictions. 9940 Expected Output I am happy 2. It returns the probability that the tweet belongs to the positive or negative class. So the logprior can also be calculated as the difference between two logs text logprior log P D_ pos log P D_ neg log D_ pos log D_ neg tag 3 Positive and Negative Probability of a WordTo compute the positive probability and the negative probability for a specific word in the vocabulary we ll use the following inputs freq_ pos and freq_ neg are the frequencies of that specific word in the positive or negative class. Words Positive word count Negative Word Count glad 41 2 arriv 57 4 1 3663 0 378 Implement get_words_by_threshold freqs label threshold If we set the label to 1 then we ll look for all words whose threshold of positive negative is at least as high as that threshold or higher. Hints Please use the process_tweet function that was imported above and then store the words in their respective dictionaries and sets. One way for us to define the level of positiveness or negativeness without calculating the log likelihood is to compare the positive to negative frequency of the word. See you next week add folder tmp2 from our local workspace containing pre downloaded corpora files to nltk s data path get the sets of positive and negative tweets split the data into two pieces one for training and one for testing validation set avoid assumptions about the length of all_positive_tweets remove stock market tickers like GE remove old style retweet text RT remove hyperlinks remove hashtags only removing the hash sign from the word tokenize tweets remove stopwords remove punctuation tweets_clean. The function takes in your test_x test_y log_prior and loglikelihood It returns the accuracy of your model. Prior and LogpriorThe prior probability represents the underlying probability in the target population that a tweet is positive versus negative. Expected Accuracy 0. 28 great great great 6. You will also implement a lookup helper function that takes in the freqs dictionary a word and a label 1 or 0 and returns the number of times that word and label tuple appears in the collection of tweets. For example given a list of tweets i am rather excited you are rather happy and the label 1 the function will return a dictionary that contains the following key value pairs rather 1 2 happi 1 1 excit 1 1 Notice how for each word in the given string the same label 1 is assigned to each word. Other words like glad community and arrives tend to be found in the positive tweets. Note that log frac A B is the same as log A log B. We can take the log of the prior to rescale it and we ll call this the logprior text logprior log left frac P D_ pos P D_ neg right log left frac D_ pos D_ neg right. 09089 Part 3 Test your naive bayesNow that we have the logprior and loglikelihood we can test the naive bayes function by making predicting on some tweets Implement naive_bayes_predict Instructions Implement the naive_bayes_predict function to make predictions on tweets. Note that the labels we ll use here are 1 for positive and 0 for negative. N_ pos and N_ neg are the total number of positive and negative words for all documents for all tweets respectively. Expected Output happi 1 1 trick 0 1 sad 0 1 tire 0 2 Part 2 Train your model using Naive BayesNaive bayes is an algorithm that could be used for sentiment analysis. ", "id": "pratikbarua/twitter-sentiment-analysis-naive-bayes", "size": "14165", "language": "python", "html_url": "https://www.kaggle.com/code/pratikbarua/twitter-sentiment-analysis-naive-bayes", "git_url": "https://www.kaggle.com/code/pratikbarua/twitter-sentiment-analysis-naive-bayes", "script": "TweetTokenizer stopwords test_naive_bayes get_ratio twitter_samples PorterStemmer test_lookup numpy train_naive_bayes count_tweets lookup os get_words_by_threshold pandas nltk.stem nltk.tokenize nltk.corpus naive_bayes_predict getcwd process_tweet ", "entities": "(('it', 'function print'), 'IDENTIFIER') (('we', 'log likelihood'), 'mean') (('test_y It', 'model'), 'take') (('That', 'population'), 'set') (('they', 'sentiment'), 'remove') (('that', 'you'), 'give') (('Instructions', 'predictions'), 'implement') (('particular word', 'word'), 'get_ratio') (('function', 'tweet logprior loglikelihood'), 'take') (('it', 'value'), 'be') (('threshold', 'at most as given threshold'), 'look') (('dictionary that', 'get_ratio function'), 'append') (('training data', 'evenly positive labels 4000 positive 4000 negative tweets'), 'logprior') (('you', 'word freq _ pos'), 'pos') (('ratio', '1'), 'mean') (('You', 'frac P W _ pos P W _ neg'), 'compute') (('input', 'dictionary'), 'InstructionsCreate') (('You', 'tweet'), 'want') (('we', 'instead happy happy'), 'be') (('perfectly balanced', 'logprior'), 'remember') (('_ pos _ neg', 'frac D'), 'take') (('word', '1'), 'be') (('it', 'times'), 'be') (('same label', '1 word'), 'be') (('We', 'assignment'), 'use') (('P W _ neg frac _ neg V V 4 _ neg 1 5 We', 'word'), 'freq') (('D _ pos', 'D _ total negative tweets'), 'use') (('Other words', 'positive tweets'), 'tend') (('number', 'vocabulary'), 'get') (('we', 'motiv'), 'treat') (('You', 'class'), 'create') (('N _ pos', 'tweets'), 'be') (('Likewise words', 'more others'), 'consider') (('positive frequencies', 'specific word'), 'calculate') (('don t', 'content'), 'noise') (('frac text 1 1 Where pos_words', 'respective classes'), 'calculate') (('key', 'value'), 'remember') (('word label values', 'dictionary'), '1') (('tweet', 'positive class'), 'return') (('key', 'stemmed word'), 'be') (('first step', 'model'), 'be') (('value', 'log word'), 'be') (('count so value', 'tweets'), 'notice') (('Finally you', 'word'), 'want') (('it', 'own tweet'), 'get') (('V', 'classes'), 'be') (('that', 'positivity negativity'), 'filter') (('log frac A B', 'log log B.'), 'note') (('Implement Instructions', 'tweets'), '09089') (('that', 'respective dictionaries'), 'use') (('Emojis', 'negative connotation'), 'tend') (('Naive BayesIf you', 'nltk'), 'run') (('we', 'words'), 'note') (('It', 'prediction also short time'), 'take') (('you', 'set function'), 'calculate') (('lower that', 'at least as even more given threshold'), 'filter') (('that', 'value clean key pairs'), 'assume') (('5', 'positive ratios'), 'notice') (('prior', 'probabilities'), 'be') (('word', 'tweets'), 'implement') (('threshold', 'at least as threshold'), 'glad') (('ratio', 'threshold'), '2f') (('that', 'frequencies'), 'compute') (('pos_neg_ratio', 'threshold'), 'add') (('it', 'process_tweet'), 'notice') (('you', 'that'), 'train') (('error analysis', 'own tweet'), 'include') (('that', 'sentiment analysis'), 'happi') (('org wiki Additive_smoothing', 'additive smoothing'), 'explain') (('N _ neg Using you', 'total negative words'), 'dictionary') (('P W _ neg frac _ V V 4 _ neg 1 5 we', 'additive smoothing'), 'use') (('that', 'sentiment'), 'give') (('part you', 'own tweet'), 'think') (('We', 'word'), 'calculate') (('we', 'here positive negative'), 'note') (('tweet', 'target population'), 'represent') (('words', 'others'), 'word') (('we', 'positive class'), 'calculate') (('word', 'integer'), 'appear') (('model', 'that'), 'AnalysisIn') (('it', 'ys'), 'find') (('you', 'I'), 'include') (('us', 'word'), 'be') (('Using', 'tweets _ D negative documents'), 'd') (('stopwords', 'punctuation tweets_clean'), 'add') (('list', 'bayes naive classifier'), 'InstructionsGiven') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["absolute", "accuracy", "algorithm", "appear", "append", "article", "average", "build", "calculate", "call", "case", "cell", "check", "classifier", "clean", "code", "collection", "community", "compare", "compute", "computer", "contain", "could", "count", "create", "data", "define", "dictionary", "difference", "document", "download", "equal", "error", "even", "expected", "filter", "find", "folder", "following", "formula", "found", "frac", "frequency", "function", "grading", "hash", "help", "helper", "high", "implement", "include", "initialize", "input", "integer", "key", "label", "learning", "least", "left", "length", "level", "likelihood", "list", "local", "log", "look", "lookup", "lot", "lower", "market", "match", "maximum", "minimum", "model", "most", "motivation", "movie", "naive", "need", "neg", "negative", "new", "next", "nltk", "no", "noise", "non", "not", "notebook", "number", "out", "output", "pair", "part", "path", "population", "positive", "practice", "pre", "predict", "prediction", "print", "probability", "project", "punctuation", "ratio", "re", "reason", "relative", "remove", "rescale", "result", "return", "right", "running", "sentiment", "set", "several", "short", "sign", "split", "stem", "step", "store", "string", "structure", "style", "subset", "sum", "tag", "target", "test", "testing", "text", "think", "threshold", "time", "tokenize", "total", "track", "train", "training", "tuple", "unique", "up", "validation", "value", "variation", "week", "word", "zip"], "potential_description_queries_len": 151, "potential_script_queries": ["getcwd", "numpy"], "potential_script_queries_len": 2, "potential_entities_queries": ["even", "freq", "label", "log", "naive", "negative", "short", "total"], "potential_entities_queries_len": 8, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 152}