{"name": "beating everything with depthwise convolution ", "full_name": " h3 How many samples for each class are there in the dataset h3 Preparing validation data h2 Augmentation h3 Training data generator h2 Model ", "stargazers_count": 0, "forks_count": 0, "description": "The aim of this kernel is to develop a robust deep learning model from scratch on this limited amount of data. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. For example Depthwise SeparableConv is a good replacement for Conv layer. We will load those weights and will run the inference on the test set using those weights only. We will insert the data into this list in img_path label format Go through all the normal cases. Nice So our model has a 98 recall. Preparing validation dataWe will be defining a generator for the training dataset later in the notebook but as the validation data is small so I can read the images and can load the data without the need of a generator. We all know that deep learning models are data hungry but if you know how things work you can build good models even with a limited amount of data. Let s look at how a normal case is different from that of a pneumonia case. PNEUMONIA This directory contains those samples that are the pneumonia cases. But if you notice the precision is only 80. This dataset consists pneumonia samples belonging to the first two classes. read_csv Input data files are available in the. Let s grab the dataset We will first go through the training dataset. I will explain this in detail but before that I would love to share one of the best practices when it comes to building deep learning models from scratch on limited data. It can be either 1 Bacterial pneumonia 2 Viral Pneumonia 3 Mycoplasma pneumonia and 4 Fungal pneumonia. For example if your dataset contains 95 negatives and 5 positives having a model with 95 accuracy doesn t make sense at all. Set the seed for hash based operations in python Set the numpy seed Disable multi threading in tensorflow ops Set the random seed in tensorflow at graph level Define a tensorflow session with above session configs Set the session in keras Make the augmentation sequence deterministic Define path to the data directory Path to train directory Fancy pathlib. We will be doing partial transfer learning and rest of the model will be trained from scratch. You should transfer learn but wisely. This is one thing to notice. The label for these cases will be 1 Get a pandas dataframe from the data we have in our list Shuffle the data How the dataframe looks like Get the counts for each class Plot the results Get few samples for both the classes Concat the data in a single list and del the above two list Plot the data Get the path to the sub directories Get the list of all the images List that are going to contain validation images data and the corresponding labels Some images are in grayscale while majority of them contains 3 channels. We have almost with thrice pneumonia cases here as compared to the normal cases. The classifier might label every example as negative and still achieve 95 accuracy. Machine Learning and Deep Learning have a huge scope in healthcare but applying them in healthcare isn t that simple. Hello everyone Hope everything is fine and you are enjoying things on Kaggle as usual. Happy Kaggling This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. So if the image is grayscale we will convert into a image with 3 channels. Model This is the best part. but for your reference I will attach the screenshot of the training steps here. I have uploaded the weights of the best model I achieved so far. For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. This is exactly what the code block given below is doing. Choose layers that introduce a lesser number of parameters. path Path to validation directory Path to test directory Get the path to the normal and pneumonia sub directories Get the list of all the images An empty list. Pneumonia is a very common disease. Each of the above directory contains two sub directories NORMAL These are the samples that describe the normal no pneumonia case. If you look at other kernels on this dataset everyone is busy doing transfer learning and fine tuning. Use batch norm with convolutions. Feel free to use it for further fine tuning of the network. How many samples for each class are there in the dataset As you can see the data is highly imbalanced. We will set a numer of things in order to make sure that the results are almost reproducible if not fully. You can do more than this if you want but I think at this point this is more than enough I need. But if applied very carefully it can benefit the world in enormous ways. If you look carefully then there are some cases where you won t be able to differentiate between a normal case and a pneumonia case with the naked eye. Do it for the depth of your network too. opt RMSprop lr 0. Add dense layers with reasonable amount of neurons. It comes with a very clean api and you can do hell of augmentations with it. And as a Machine learning engineer it s our responsibility to help people as much as we can in all possible ways. It s more than just a classification problem. I like imgaug https imgaug. That s it folks I hope you enjoyed this kernel. We will initialize the weights of first two convolutions with imagenet weights I have commented out the training step as of now as it will train the network again while rendering the notebook and I would have to wait for an hour or so which I don t want to. AugmentationData augmentation is a powerful technique which helps in almost every case for improving the robustness of a model. It s worth exploring In the next code block I will define a augmentation sequence. We will get the confusion matrix from our predictions and see what is the recall and precision of our model. either there will be too many normal cases or there will be too many cases with the disease. You will notice Oneof and it does exactly that. We will do some analysis on that look at some of the samples check the number of samples for each class etc. This is all that I have done in the next code block. We will normalize the pixel values and resizing all the images to 224x224 Normal cases Pneumonia cases Convert the list into numpy arrays Augmentation sequence horizontal flips roatation random brightness Get total number of samples in the data Define two numpy arrays for containing batch data and labels Get a numpy array of all the indices of the input data Initialize a counter Get the next batch one hot encoding read the image and resize check if it s grayscale cv2 reads in BGR mode by default normalize the image pixels generating more samples of the undersampled class Open the VGG16 weight file Select the layers for which you want to set weight. fit_generator train_data_gen epochs nb_epochs steps_per_epoch nb_train_steps validation_data valid_data valid_labels callbacks es chkpt class_weight 0 1. Reproducibility is a great concern when doing deep learning. The label for these cases will be 0 Go through all the pneumonia cases. You can generate different samples of undersampled class in order to try to balance the overall distribution. Initialize the first few layers from a network that is pretrained on imagenet. Precision and Recall are really good metrics for such kind of problems. training https i. We will look at somes samples from our training data itself. 0001 decay 1e 6 Get a train data generator Define the number of training steps history model. You can read about Xception and Depthwise Separable Convolutions in this https arxiv. Instead of randomly initialized weights for these layers it would be much better if you fine tune them. Hence we need to look for alternative metrics. The dataset consists of only very few samples and that too unbalanced. The stake is very high. The rage for competing on Kaggle should never end. png When a particular problem includes an imbalanced dataset then accuracy isn t a good metric to look for. If we can build a robust classifier it would be a great assist to the doctor too. Train with a higher learning rate and experiment with the number of neurons in the dense layers. The dataset is divided into three sets 1 train set 2 validation set and 3 test set. As the network becomes deeper batch norm start to play an important role. This is because first few layers capture general details like color blobs patches edges etc. Precision and Recall follows a trade off and you need to find a point where your recall as well as your precision is more than good but both can t increase simultaneously. In such problems a good recall value is expected. It introduces lesser number of parameters as compared to normal convolution and as different filters are applied to each channel it captures more information. Choose a simple architecture. Once you know a good depth start training your network with a lower learning rate along with decay. At each iteration it will take one augmentation technique out of the three and will apply that on the samples Training data generator Here I will define a very simple data generator. This situation is very normal when it comes to medical data. But augmentation can be much more helpful where the dataset is imbalanced. The data will always be imbalanced. There was a good discussion on KaggleNoobs slack regarding this. 4 Load the model weights Preparing test data Evaluation on test dataset Get predictions Original labels Get the confusion matrix Calculate Precision and Recall. Xception a powerful network is built on top of such layers only. There is one case in the above plot at least for me which is too much confusing. ", "id": "aakashnain/beating-everything-with-depthwise-convolution", "size": "7132", "language": "python", "html_url": "https://www.kaggle.com/code/aakashnain/beating-everything-with-depthwise-convolution", "git_url": "https://www.kaggle.com/code/aakashnain/beating-everything-with-depthwise-convolution", "script": "Flatten sklearn.metrics keras.preprocessing.image makedirs skimage.transform Concatenate data_gen keras.layers keras.callbacks train_test_split isdir VGG16 build_model EarlyStopping Dropout Path Sequential SGD Adam imread listdir Conv2D exists seaborn numpy os.path pathlib SeparableConv2D plot_confusion_matrix Input PIL img_to_array ModelCheckpoint sklearn.model_selection join ImageDataGenerator confusion_matrix load_img Image matplotlib.pyplot isfile RMSprop Dense os keras.applications.vgg16 pandas expanduser keras.utils keras.optimizers GlobalMaxPooling2D imgaug.augmenters keras.layers.merge StandardScaler resize to_categorical BatchNormalization mlxtend.plotting preprocess_input Callback tensorflow remove Model MaxPooling2D imgaug getcwd abspath sklearn.preprocessing backend backend as K keras keras.models matplotlib.image keras.layers.normalization skimage.io ", "entities": "(('as much we', 'possible ways'), 's') (('it', 'great doctor'), 'be') (('very carefully it', 'enormous ways'), 'benefit') (('I', 'augmentation sequence'), 's') (('rage', 'Kaggle'), 'end') (('don t', 'which'), 'initialize') (('dataset', 'only very few samples'), 'consist') (('Precision', 'problems'), 'be') (('We', 'scratch'), 'do') (('We', 'here normal cases'), 'have') (('we', '3 channels'), 'convert') (('data', 'there dataset'), 'be') (('you', 'it'), 'come') (('You', 'Separable https arxiv'), 'read') (('that', 'imagenet'), 'initialize') (('I', 'generator'), 'define') (('majority', '3 channels'), 'be') (('0', 'pneumonia cases'), 'be') (('t', 'sense'), 'make') (('as well more both', 'point'), 'follow') (('Original labels', 'confusion matrix Calculate Precision'), 'Load') (('Here I', 'data very simple generator'), 'take') (('Machine Learning', 'healthcare isn t'), 'have') (('Reproducibility', 'great when deep learning'), 'be') (('path Path', 'empty list'), 'get') (('powerful which', 'model'), 'be') (('I', 'best model'), 'upload') (('When particular problem', 'accuracy isn then good metric'), 'png') (('dataset', 'first two classes'), 'consist') (('it', 'more information'), 'introduce') (('Depthwise SeparableConv', 'Conv good layer'), 'be') (('when it', 'limited data'), 'explain') (('good depth', 'decay'), 'start') (('aim', 'data'), 'be') (('you', 'weight'), 'normalize') (('We', 'class etc'), 'do') (('It', 'python docker image https kaggle github'), 'happy') (('I', 'enough'), 'do') (('you', 'output'), 'list') (('code block', 'exactly what'), 'be') (('classifier', 'negative still 95 accuracy'), 'label') (('carefully then where you', 'pneumonia naked eye'), 'be') (('recall good value', 'such problems'), 'expect') (('how normal case', 'pneumonia case'), 'let') (('first few layers', 'etc'), 'be') (('very when it', 'medical data'), 'be') (('We', 'training first dataset'), 'let') (('what', 'model'), 'get') (('it', 'exactly that'), 'notice') (('you', 'Kaggle'), 'Hope') (('you', 'kernel'), 's') (('augmentation sequence', 'directory Fancy pathlib'), 'set') (('You', 'overall distribution'), 'generate') (('which', 'at least me'), 'be') (('read_csv Input data files', 'the'), 'be') (('I', 'code next block'), 'be') (('Choose that', 'parameters'), 'layer') (('We', 'normal cases'), 'insert') (('everyone', 'transfer learning'), 'be') (('We', 'weights'), 'load') (('It', 'classification just problem'), 's') (('We', 'training data'), 'look') (('Hence we', 'alternative metrics'), 'need') (('I', 'training steps'), 'attach') (('1 train', '2 validation'), 'divide') (('powerful network', 'such layers'), 'build') (('that', 'samples'), 'PNEUMONIA') (('that', 'pneumonia normal case'), 'contain') (('results', 'order'), 'set') (('much you', 'fine them'), 'be') (('you', 'data'), 'know') (('batch deeper norm', 'important role'), 'start') ", "extra": "['disease', 'test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "api", "apply", "array", "attach", "augmentation", "balance", "batch", "best", "block", "build", "case", "channel", "check", "classification", "classifier", "clean", "code", "color", "confusion", "contain", "convert", "convolution", "current", "cv2", "data", "dataframe", "dataset", "decay", "default", "define", "del", "depth", "describe", "detail", "develop", "directory", "doctor", "empty", "encoding", "environment", "even", "every", "everyone", "everything", "experiment", "file", "find", "format", "general", "generate", "generator", "grab", "graph", "grayscale", "hash", "help", "history", "hope", "hot", "image", "imagenet", "imgaug", "increase", "inference", "initialize", "input", "insert", "iteration", "kaggle", "kernel", "label", "learn", "learning", "least", "level", "linear", "list", "load", "look", "lower", "lr", "majority", "matrix", "medical", "metric", "might", "mode", "model", "need", "negative", "network", "next", "no", "norm", "normal", "normalize", "not", "notebook", "number", "numpy", "opt", "order", "out", "overall", "partial", "path", "people", "pixel", "plot", "pneumonia", "png", "point", "precision", "pretrained", "problem", "processing", "python", "random", "read", "recall", "reference", "replacement", "resize", "rest", "robust", "run", "running", "scope", "scratch", "sense", "sequence", "session", "set", "several", "single", "situation", "slack", "start", "step", "sub", "technique", "tensorflow", "test", "think", "those", "through", "total", "train", "training", "transfer", "try", "tune", "tuning", "validation", "value", "weight", "while", "work", "world", "write"], "potential_description_queries_len": 161, "potential_script_queries": ["abspath", "backend", "expanduser", "getcwd", "imread", "io", "isdir", "isfile", "join", "listdir", "pathlib", "preprocessing", "remove", "seaborn"], "potential_script_queries_len": 14, "potential_entities_queries": ["image", "least", "next"], "potential_entities_queries_len": 3, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 172}