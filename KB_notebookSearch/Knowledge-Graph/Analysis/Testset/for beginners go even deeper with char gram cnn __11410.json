{"name": "for beginners go even deeper with char gram cnn ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "By defining the dimension of the filter you can control the window of infomation you want to summarize. Notice that we have set padding to same. But if you really want to have the resulting matrix to be reduced you can set the padding parameter to valid. As mentioned in the Keras documentation we have to include the shape for the very first layer and Keras will automatically derive the shape for the rest of the layers. There s something I deliberately missed out earlier filters. com sbongo for beginners tackling toxic using keras A brief glance at Convolutional Neural Network CNNs CNN is basically a feed forward neural network that consists of several layers such as the convolution pooling and some densely connected layers that we are familiar with. Hence if we could go deeper by splitting the sentence into a list of characters instead of words the chances that the same characters that are present in both training and prediction set are much higher. 9829Epoch 5 6143613 143613 3023s 21ms step loss 0. Hyper parameter tunings3. png Consider this simplified image of max pooling operation above. This function allows Tokenizer to create an index of the tokenized unique characters. jpg Why do we consider the idea of using char gram features You might noticed that there are a lot of sparse misspellings due to the nature of the dataset. 9823Epoch 4 6143613 143613 2991s 21ms step loss 0. We are going to use it again to help us split the text into characters by setting the char_level parameter to true. Since there are sentences with varying length of characters we have to get them on a constant size. For instance the stride size which determine how often the filter will be applied narrow VS wide CNN etc. That also means that we are projecting characters on a 240 dimension vector space. In this notebook we are going to tackle the same toxic classification problem just like my previous notebooks but this time round we are going deeper with the use of Character level features and Convolutional Neural Network CNNs. You would begin the convolution process by using filters of different dimensions to slide across your initial matrix to get a lower dimension feature map. a 1 b 2 etcThen we get back a list of sentences with the sequence of indexes which represent each character. Due to Kaggle kernel time limit I have pasted the training output of these 6 epochs. Therefore I believe it deserve a writeup and without much ado let s see how we can apply CNN to our competition at hand. Indirectly we are only concerned about the existence of 8 and not the location of it. Okay Let s see how we could implement CNN in our competition. In the early stages you would want to preserve as much information as possible so you will want to have a 32 x 32 x 3 matrix back. And that is why we get an output of num of sentences X 125 X 100 matrix. Experiment with different architecture layersThank you for your time in reading and if you like what I wrote support me by upvoting my notebook. In the above example we slide a 2 X 2 filter window across our dataset in strides of 2. png For simplicity sake let s imagine we have a 32 x 32 x 3 input matrix and a 5 x 5 x 3 filter if you apply the filter on the matrix with 1 stride you will end up with a 28 x 28 x 3 matrix. As usual we would then feed into a normal densely connected layer that outputs to a softmax function which gives the probabilities of each class. 9831I hope this notebook serves as a good start for beginners who are interested in tackling NLP problems using the CNN angle. The convolution layer works by sliding a window across the input data and as it slides the window filter applies some matrix operations with the underlying data that falls in the window. We use an embedding size of 240. When you do max pooling on this list you will only retain the value 8. Afterwhich we apply a max pooling again but this time round it s a global max pooling. With the output of embedding layer we feed it into a convolution layer. First we set up our input layer. So for a list of 10 sentences that consists of 50 characters each using a 30 dimensional embedding will allow us to feed in a 10x50x30 matrix into our convolution layer. You could imagine that this approach introduce another problem an explosion of dimensions. Now that we have a 2D matrix it s convenient to plug it into the densely connected layer followed by a relu activation function. In our context some characters in each filter would be selected through this max pooling process based on their values. 9829Epoch 6 6143613 143613 2961s 21ms step loss 0. 9816Epoch 3 6143613 143613 2471s 17ms step loss 0. gif The sliding window that I mention earlier are actually filters that are designed to capture different distinctive features in the input data. Sometimes that wouldn t affect the model s capability to make good judgement but most of the time it s unable to correctly classify because the misspelt words are not in the model s dictionary. Train on 143613 samples validate on 15958 samplesEpoch 1 6143613 143613 2580s 18ms step loss 0. You could experiment with the dropout rate and size of the dense connected layer to see it could decrease overfitting. png Firstly as seen in the above picture we feed the data image in this case into the convolution layer. How does this apply to NLP in our case Now forget about real pixels about a minute and imagine using each tokenized character as a form of pixel in our input matrix. There are different ways to down sample the data such as min pooling average pooling and in max pooling you simply take the maximum value of the matrix. Traditionally CNN is used to solve computer vision problems but there s an increased trend of using CNN not just in Kaggle competitions but also in papers written by researchers too. We ll pass it through a dropout layer and a densely connected layer that eventually passes to a sigmoid function. Again with the down sized after pooled matrix you could feed it to a densely connected layer which eventually leads to prediction. There are some ideas which you could use to push the performance further such as 1. Each character is represented in a row 8 characters and each embedding dimension is represented in a column 5 dimensions in this starting matrix. As it s sliding it grabs the maximum value and put it into a smaller sized matrix. com sbongo do pretrained embeddings give you the extra edge For Beginners Tackling Toxic Using Keras https www. Here s the meat of our notebook. What s the difference between this and the previous max pooling attempt In the previous max pooling attempt we merely down sampled a single 2nd dimension which contains the number of characters. That also means we slides a window across the 240 dimensions of embeddings for each of the 500 characters and it will result in a num of sentences X 500 X 100 matrix. Despite it s simplicity it s works quite well and it s a pretty niffy way to reduce the data size. The training loss decreases steadily along with validation loss until at the 5th or 6th epoch where traces of overfitting starts to emerge. We have talked about embedding layer in my previous notebooks so feel free to check them out. What does this padding means https i. Tweak CNN parameters such as number of strides different padding settings window size. Note that my explanation hides some technical details to facilitate understanding. And when you eventually collect all the result of the matrix operations you will have a condensed output in another matrix we call it a feature map. With the toxic competition coming to an end in a month I wish everyone godspeed uncomment below to train in your local machine. From a matrix of num of sentences X 500 X 100 it becomes num of sentences X 125 X 100 which is still a 3d matrix. gif With the resulting matrix at hand you do a max pooling that basically down samples or in another words decrease the number of dimensions without losing the essence. Finally we move on to train the model with 6 epochs and the results seems pretty decent. It will output a num of sentences X 500 X 240 matrix. jpg Looking at the above picture let s just focus for now on 1 sentence instead of a list. Then we pass it to the max pooling layer that applies the max pool operation on a window of every 4 characters. Next we would apply a max pooling to get the maximum value in each feature map. If we add padding some zeros around the original input matrix we will be sure that the result output matrix dimension will be the same. We use a window size of 4 remember it s 5 in the earlier picture above and 100 filters it s 6 in the earlier picture above to extract the features in our data. There s a whole load of things that you could tweak with CNN. When we train our model using the word vectors from our training set we might be missing out some genuine words and mispellings that are not present in the training set but yet present in our prediction set. I have skipped some elaboration of some concepts like embeddings which I have went through in my previous notebooks so take a look at these if you are interested in learning more Do Pretrained Embeddings Give You The Extra Edge https www. As always we start off with the importing of relevant libraries and dataset Split into training and test set Store the comments as seperate variables for further processing. Just like word vectors we could also have character vectors that gives a lower dimension representation. Let s put them to a length of 500 characters for each sentence Just in case you are wondering the reason why I used 500 is because most of the number of characters in a sentence falls within 0 to 500 Finally we can start buliding our model. Next we pass it to the Bidriectional LSTM that we are famliar with since the previous notebook. One of the ways to tackle this problem is to use CNN as it s designed to solve high dimensional dataset like images. Imagine that you have a list 1 4 0 8 5. To translate back in the picture each of the feature maps could contain 1 high level representation of the embeddings for each character. So it outputs a num of sentences X 120 2D matrix. 9806Epoch 2 6143613 143613 2426s 17ms step loss 0. In our previous notebook we have began using Kera s helpful Tokenizer class to help us do the gritty text processing work. But in global max pooling we perform pooling operation across several dimensions 2nd and 3rd dimension into a single dimension. ", "id": "sbongo/for-beginners-go-even-deeper-with-char-gram-cnn", "size": "11410", "language": "python", "html_url": "https://www.kaggle.com/code/sbongo/for-beginners-go-even-deeper-with-char-gram-cnn", "git_url": "https://www.kaggle.com/code/sbongo/for-beginners-go-even-deeper-with-char-gram-cnn", "script": "codecs Bidirectional keras.layers keras.callbacks train_test_split keras.preprocessing.sequence Embedding EarlyStopping Dropout LSTM numpy GRU MaxPooling1D optimizers Input initializers Tokenizer ModelCheckpoint constraints sklearn.model_selection matplotlib.pyplot Activation Dense os GlobalMaxPool1D re keras.preprocessing.text csv Conv1D regularizers load_model Model layers pad_sequences keras sys keras.models ", "entities": "(('always we', 'further processing'), 'start') (('which', 'eventually prediction'), 'feed') (('it', 'relu activation function'), 's') (('we', 'previous notebook'), 'pass') (('we', 'layers'), 'have') (('time round we', 'Character level features'), 'go') (('s', 'just 1 sentence'), 'let') (('Do Pretrained Embeddings', 'Extra Edge https www'), 'skip') (('everyone', 'uncomment below local machine'), 'wish') (('each', 'character'), 'contain') (('where traces', '5th epoch'), 'decrease') (('You', 'dataset'), 'consider') (('it', 'images'), 'be') (('we', 'densely connected that'), 'be') (('we', 'constant size'), 'be') (('how we', 'competition'), 'okay') (('correctly misspelt words', 'model'), 's') (('you', '28 28 3 matrix'), 'let') (('Finally we', 'model'), 'let') (('you', 'valid'), 'set') (('I', 'deliberately earlier filters'), 's') (('that', 'window'), 'work') (('earlier actually that', 'input data'), 'gif') (('How this', 'input matrix'), 'apply') (('possible you', '32 32 3 matrix'), 'want') (('we', 'it'), 'have') (('result output matrix dimension', 'input original matrix'), 'be') (('which', 'characters'), 's') (('that', 'essence'), 'gif') (('how we', 'hand'), 'believe') (('which', 'character'), 'a') (('we', 'single dimension'), 'perform') (('Traditionally CNN', 'researchers'), 'use') (('why we', 'X 125 X 100 matrix'), 'be') (('results', '6 epochs'), 'move') (('It', 'sentences'), 'output') (('explanation', 'understanding'), 'note') (('you', 'only value'), 'retain') (('you', 'further such 1'), 'be') (('embedding dimension', 'starting matrix'), 'represent') (('also we', 'dimension vector 240 space'), 'mean') (('characters', 'values'), 'select') (('it', 'overfitting'), 'experiment') (('that', 'training set'), 'go') (('You', 'dimension feature lower map'), 'begin') (('this', 'max pooling operation'), 'consider') (('100 filters it', 'data'), 'use') (('again us', 'true'), 'go') (('we', 'convolution layer'), 'feed') (('which', 'class'), 'feed') (('we', 'text processing gritty work'), 'begin') (('pretrained embeddings', 'Toxic Using Keras https www'), 'give') (('us', 'convolution layer'), 'allow') (('densely connected that', 'eventually sigmoid function'), 'pass') (('who', 'CNN angle'), 'hope') (('you', 'matrix'), 'be') (('I', '6 epochs'), 'paste') (('I', 'notebook'), 'experiment') (('X 125 100 which', 'sentences'), 'from') (('143613 samples', '15958'), 'train') (('that', '4 characters'), 'pass') (('Indirectly we', 'it'), 'be') (('you', 'CNN'), 's') (('We', 'so them'), 'talk') (('So it', 'sentences'), 'output') (('how often filter', 'instance'), 'size') (('Tokenizer', 'tokenized unique characters'), 'allow') (('it', 'smaller sized matrix'), 'grab') (('it', 'sentences'), 'mean') (('approach', 'dimensions'), 'imagine') (('that', 'dimension lower representation'), 'have') (('again time it', 'max'), 'apply') (('you', 'infomation'), 'control') (('Next we', 'feature map'), 'apply') (('we', '2'), 'slide') (('that', 'prediction'), 'miss') (('quite well it', 'data pretty niffy size'), 'simplicity') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["apply", "approach", "architecture", "average", "call", "case", "char", "character", "check", "classification", "classify", "column", "competition", "computer", "consider", "contain", "context", "control", "convolution", "could", "create", "data", "dataset", "derive", "difference", "dimension", "edge", "embedding", "end", "epoch", "every", "everyone", "experiment", "extract", "feature", "feed", "filter", "form", "forward", "function", "hand", "help", "high", "hope", "idea", "image", "implement", "include", "index", "input", "instance", "kernel", "layer", "learning", "length", "let", "level", "list", "load", "local", "look", "lot", "lower", "matrix", "max", "maximum", "mention", "might", "min", "minute", "missing", "model", "month", "most", "move", "my", "nature", "network", "neural", "normal", "not", "notebook", "num", "number", "operation", "out", "output", "overfitting", "padding", "parameter", "perform", "performance", "picture", "pixel", "png", "pooling", "prediction", "present", "pretrained", "problem", "processing", "reading", "reason", "reduce", "representation", "rest", "result", "row", "sample", "selected", "sentence", "sequence", "set", "several", "shape", "sigmoid", "simplicity", "single", "size", "slide", "softmax", "something", "sparse", "split", "splitting", "start", "step", "stride", "support", "test", "text", "through", "time", "train", "training", "translate", "trend", "unique", "until", "up", "validate", "validation", "value", "vector", "vision", "who", "window", "word"], "potential_description_queries_len": 148, "potential_script_queries": ["csv", "numpy", "preprocessing", "re"], "potential_script_queries_len": 4, "potential_entities_queries": ["level", "lower", "output", "processing", "time", "vector"], "potential_entities_queries_len": 6, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 152}