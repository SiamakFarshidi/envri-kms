{"name": "densenet169 with wsi split and focalloss ", "full_name": " h1 Count per WSI id h2 Take only Ids of which we know the WSI h2 extract all images we dont know the WSI id of will go into validation set h2 Get 20 of the WSIs as validation set maybe later make sure its stratified h2 get the integer index for all the images with WSIs of the validation set h3 Warm up with frozen weight is done on a subset so we dont have to waste an entire epoch h3 Predit the validation data using TTA h3 Now predict on test set h3 prepare submission h2 I add the score to the name of the file so I can later plot the leaderboard score versus my validation score ", "stargazers_count": 0, "forks_count": 0, "description": "THis gives a nice extra percent or two when compared to the auc above after training where not TTA is used. For now only the diherdral and rotations are used. We can then compare the predictions on for example the image and the image flipped roated slightly different crop lighting stretched etc. com sermakarevich for the WSI set and Idea and Taylor https www. since fname_clean is the id we can just use that as index when adding the correct label in our dataframe. I also test if mean or max is better to use on the image and its augments but it can t conclude anything yet. Sometimes its important in which order the ids in the submissions are so to make sure I don t mess up I put them in the same order. I add the score to the name of the file so I can later plot the leaderboard score versus my validation scoreIn the fastai course Jeremy mentions that if you have a monotonic relation between validation and LB score the way you set up your validation set matches what the test set consists of. My first submission had a 50 score so I somewhere messed up the order oder the matching of id to label. com guntherthepenguin fastai v1 densenet169 kernel. Now predict on test set prepare submissionI now load in the sample submission and put my predictions in the label column and save to a new file. Apprently 2 cpus per kaggle node so 4 threads I think. com c histopathologic cancer detection discussion 83760 to reduce overfitting and correlations between validation and training set. com tywangty for uploading it on kaggleDefining a metric so after epoch I get the validation ROC AUC scoreIn Case I want to run quick tests use a subsample Count per WSI id Take only Ids of which we know the WSI extract all images we dont know the WSI id of will go into validation set Get 20 of the WSIs as validation set maybe later make sure its stratified get the integer index for all the images with WSIs of the validation set Warm up with frozen weight is done on a subset so we dont have to waste an entire epoch Predit the validation data using TTAHere for every image we want to predict on n_augs images are augmented form the original image. Thanks to SM https www. The only thing new is I included the WSI ids https www. This is a Fork of my Densenet169 https www. com tywangty histopathologiccancerwsi from this discussion https www. ", "id": "guntherthepenguin/densenet169-with-wsi-split-and-focalloss", "size": "2380", "language": "python", "html_url": "https://www.kaggle.com/code/guntherthepenguin/densenet169-with-wsi-split-and-focalloss", "git_url": "https://www.kaggle.com/code/guntherthepenguin/densenet169-with-wsi-split-and-focalloss", "script": "sklearn.metrics __init__ FocalLoss(nn.Module) fastai.vision Path skmultilearn.model_selection forward torch.nn MultiLabelBinarizer numpy pathlib sklearn.model_selection f1_score iterative_train_test_split matplotlib.pyplot auc_score pandas tqdm fastai roc_auc_score sklearn.preprocessing torchvision.models train_test_split ", "entities": "(('where TTA', 'above training'), 'give') (('it', 't anything'), 'test') (('new I', 'WSI ids https www'), 'be') (('I', 'same order'), 'sometimes') (('i', 'label'), 'have') (('test set', 'what'), 'add') (('image crop slightly different lighting', 'etc'), 'compare') (('I', 'so 4 threads'), 'think') (('This', 'Densenet169 https www'), 'be') (('we', 'augmented original image'), 'com') (('i we', 'dataframe'), 'use') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["auc", "cancer", "column", "compare", "correct", "correlations", "course", "crop", "data", "detection", "epoch", "every", "extract", "fastai", "file", "form", "id", "image", "index", "integer", "kaggle", "label", "leaderboard", "load", "matching", "max", "mean", "metric", "my", "name", "new", "node", "not", "order", "overfitting", "per", "percent", "plot", "predict", "prepare", "reduce", "relation", "run", "sample", "save", "score", "set", "submission", "subset", "test", "training", "up", "validation", "weight"], "potential_description_queries_len": 54, "potential_script_queries": ["forward", "numpy", "pathlib", "sklearn", "tqdm"], "potential_script_queries_len": 5, "potential_entities_queries": [], "potential_entities_queries_len": 0, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 58}