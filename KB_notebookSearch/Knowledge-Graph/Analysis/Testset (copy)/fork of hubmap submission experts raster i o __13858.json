{"name": "fork of hubmap submission experts raster i o ", "full_name": " h1 Introduction h1 Training Tricks h2 Disclamer h2 Color spaces augmentation trick h2 Multi scale input tiles h2 Pseudo labelling h2 Short training pipeline h1 Inference h2 Inference in a nutshell h2 Parameters h3 Models h1 Detailed Description of our Inference Approach h2 Data Preprocessing h3 Loading and Downsampling Whole Slide Images h2 Making Predictions h3 Padding for the first pass h3 Dividing the Image into Tiles h3 Generating the Segmentation Mask h3 Saving the Segmentation Mask h2 Generating Final Predictions h1 Submission h1 Insights ", "stargazers_count": 0, "forks_count": 0, "description": "Final submissions are ensembles of models trained with and without Color Space Augmenatations and Cutmix in order to increase the robustness. Color spaces augmentation trick For the training we found and decided to use various Color Space Augmentations in combination with custom stochastic kernel this is one of the most important things that allowed our models to stay robust and segment glomeruli despite the data source type of the images FFPE fresh frozen even others and color related variations. In order to handle the WSI images size we used memmap to load the images and also used temporary saving of processed patches to perform the ensembling. Padding for the first passAs images are very big we must divide them into tiles. COLOR_RGB2HSV cv2. png Tiles with both augmentations image. png attachment 9bb3d996 0be8 44ec 9a9a 05d8135be713. The _generate_masks_1st and _generate_masks_2nd functions are based on the Model_pred_1st and Model_pred_2nd iterator like class that is used to generate predictions. COLOR_RGB2BGR cv2. 069 afa5e8098 21632 577 0. The aforementioned DataLoader is generated using the following _make_tiles_dataloader_1st and _make_tiles_dataloader_1st functions. Ensembling predictions with different padding dimensions allows us to avoid bad predictions due to glomeruli located on edges of the tiles. 070 aa05346ff 55608 1122 0. COLOR_BGR2LAB cv2. COLOR_RGB2Luv cv2. 068 cb2d976f4 13378 435 0. png This methodology demonstrated its effectiveness and power during both stages of the competition but mostly with the first set of the data maybe due to the different test sets since we dont know yet results on the private test data. Eventually we need a DataLoader to feed data to our model. Inference in a nutshell We make an inference on the bigger patch size than was used for training. COLOR_RGB2LUV cv2. This DataLoader is based on the HuBMAPTestDataset1st and the HuBMAPTestDataset2nd class below. Additionally we used only non empty tiles of the bigger reduction rates to balance more the training dataset. 071 57512b7f1 38216 741 0. 33 that wasn t used because of the inference time constraints. The median perimeter was similarly correlated respectively 0. COLOR_RGB2YUV In the batch sampling part if self. load facebookresearch semi supervised ImageNet1K models resnext50_32x4d_ssl aspp with customized dilatations aspp with customized dilatations m torch. COLOR_RGB2HLS cv2. load facebookresearch semi supervised ImageNet1K models resnet50_swsl aspp with customized dilatations aspp with customized dilatations Model name to class Import models Get tile Reduce if needed Save tile Return dimension after reduction Tiles are deleted when read X overlap padding Y overlap padding Check that width is OK Check that height is OK Generate tmp directory if needed Remove black or gray images based on saturation check print x1 x2 y1 y2 p. Image names from public and private test sets are retrieved from the sample_submission. That approach was giving us very good results on the public leaderboard and on cross validation before the change of test data but unfortunately for some reason we could not see that improvement afterwards. We copy our group finding code for information. COLOR_BGR2HSV cv2. 069 b9a3865fc 15859 483 0. 068 8242609fa 32320 683 0. Obvious visual differences can be noticed in both types of WSI the structure of FFPE tissues is better preserved than fresh frozen ones. COLOR_BGR2Luv cv2. png attachment af404411 8d4e 4df3 9bb8 311a51da9d32. Loading and Downsampling Whole Slide ImagesThe main issue with WSI is that they are very big. ParametersHere are the parameters that we use for inference. Segmentation mask tiles are generated for all possible sets of padding parameters for the first pass using the following get_mask_tiles_1st and get_mask_tiles_2nd functions. The _generate_masks_1st and _generate_masks_2nd function outputs a zero padded mask. Then we generate masks for each image in the test set. SubmissionHere is the final part of this kernel dealing with the actual submission of a CSV file containing generated masks. Save tiles in HDD memory Make tiles dataset Generate masks First generate mask tiles Then reconstruct mask from tiles Get bounding boxes Eventually convert to rle https www. random 1 len cspaces cspace random. After loading the image using the load_image function above we apply the _tile_resize_save function below. 059 2f6ecfcdf 34158 708 0. The second version using a two pass segmentation is described in a separate notebook. Dealing with such big images is not convenient we found that we should downsample data for better performance. We also share the code as we implemented this idea. And finally we generate a submission. IntroductionThis kernel contains the inference code of the submission_experts team for the 2021 HuBMAP competition version 1. We added zero padding to make the dimensions of the image dividable by the tile size. com bguberfain memory aware rle encoding watch out for the bug This simplified method requires first and last pixel to be zero. ModelsThe models that we can use for inference are defined below. We also used classical TTA based on transformations of the D4 group but we were limited by the kernel inference time. 069 54f2eec69 32392 684 0. csv file that is provided. png attachment c3ebe340 3998 4698 a504 39214ecbfa4f. com iafoss 256x256 images Models for first pass Models for second pass Tiles selection https www. This also gave us significant boost both in CV and LB. The Model_pred_2nd class makes predictions on pre selected tiles and then only keeps glomeruli whose intersection with a center square of predefined dimensions is non empty this is to avoid problems encountered with detections close to edges that are often wrong. COLOR_RGB2XYZ cv2. For example improvement is from 0. The RLE conversion is done using the following rle_encode_less_memory function obtained from https www. Saving the Segmentation MaskThe segmentation mask is then saved as tiles containing scores it is not binarized yet. rand 3 3 K stoch. 89 Dice depending on the fold. 068 2ec3f1bb9 19719 529 0. com bguberfain memory aware rle encoding https www. 044 d488c759a 11376 403 0. Detailed Description of our Inference ApproachIn this section we describe our approach version 1. 069 aaa6a05cc 13466 436 0. InferenceIn this section we describe how we make predictions. COLOR_BGR2LUV cv2. If PUBLIC_ONLY True then we only make the prediction for public test data. 065 c68fe75ea 34348 720 0. The goal of the competition was to provide with automated glomeruli segmentation models for WSI of human kidneys. During the second pass predictions are upsampled to match the original image. 067 4ef6695ce 22636 574 0. png Tiles with Color Space augmentation image. Additionally we observed interesting behaviour such as robust segmentation even of some suspicious glomeruli in the d48 sample from public test set. Generating the Segmentation MaskThe segmentation mask is then generated. 65 in the standard pipeline up to 0. exp K K elif random. Training Tricks DisclamerWe used NO hand labelling and manual annotation since in our vision it is not consistent with the initial goal and objective of the competition. 067 e79de561c 33504 704 0. We also initially wanted to find to which group fresh frozen vs FFPE they belonged to but as we said before we finally did not follow that approach. import cv2cspaces cv2. The next step for the first pass is to remove padding. Locally we used smaller step of 0. Generating Final PredictionsFinal predictions are generated stacking the outputs of the make_predictions_1st function and the make_predictions_2nd function through the get_mask_tiles_1st and get_mask_tiles_2nd functions. 070 We found that the median area of glomeruli was negatively correlated to the weight correlation coefficient of 0. On the images below one could see how the naturally looked tiles are transformed being augmented with this method. 5xtile_size in both X and Y directions to cut the patches from test images. einsum ijk kl ijl img stoch This method also allowed to obtain much more focused predictions we observed very high soft dice metrics even without proper binarization threshold. Then for the first pass we select tiles on which we will make predictions based on their color saturations. 071 b2dc8411c 24430 594 0. But it still demonstrated higher CV when models were trained independently for each group. Then we must reconstruct the image from resampled tiles that we saved. csv file containing all our predictions. Therefore we initially tried to apply different models with respect to the type of WSI that we were processing. Before the data update the best one was of 1024 and after 512 while the training is performed on 256x256. Eventually the obtained mask is converted into RLE format. sqrt K stoch stoch K img np. Tiles with CutMix augmentation image. Only after the second pass function _generate_masks_2nd the mask is upsampled to match the original image. For that purpose we apply the following _reconstruct_img function. InsightsThanks to our predictions we could compute some metrics about glomeruli. Dividing the Image into TilesThe following _split_image_1st function allows us to divide images into tiles. shape Iterator like wrapper that returns predicted masks List of models Dataloader Half precision Reduction on reduced image Prepare input Make predictions Quantize probablities to save memory Output predictions List of models Dataloader Half precision Reduction on reduced image Prepare input Make predictions Upsample to initial shape py new_py Output predictions Add padding to make the image dividable into tiles Split image into tiles using the reshape transpose trick Final shape nb_x nb_y TILE_SZ TILE_SZ 3 Select tiles for running the model Make tiles dataset Generate masks Reshape tiled masks into a single mask and crop padding A little bit of cleaning. 069 26dc41664 28316 649 0. 071 095bf7a1f 29992 651 0. Making PredictionsIn this section we describe how we make predictions. After discussion we decided not to follow that approach in our final submissions. Some additional insights are provided at the end of the notebook. This information was crucial before the data update and allowed us to get different models that worked for each of the groups. Data PreprocessingData preprocessing consists in downsampling images. The median area perimeter\u00b2 describing how complex the shape of the glomeruli were has been found to be correlated to the weight 0. You can see normal tiles with the cutmix augs color spaced without cutmix and color spaced with cutmix. Pseudo labellingPublic test set pseudo labeled with own models and included in the training procedure Short training pipelineUnet with resnext50_32x4d resnext101_32x16d efficientnet b7 b5 backbones ASPP and FPN modules complex loss combination of FocalLoss DiceLoss SymmetricLovasz OneCycleLR Heavy Augmentations CutMix ColorSpaced augs 5 fold patient based CV. com iafoss 256x256 images Saturation blancking threshold Threshold for the minimum number of pixels Size of center check box Inference Make predictions only on public LB Overlap between tiles during prediction X axis Overlap between tiles during prediction Y axis Reduction for two types of models Threshold for first pass Threshold for second pass Number of bins when saving mask tiles Final prediction Size of saved mask tiles m torch. As said before the HuBMAPTestDataset2nd does not need the tile selection process we mentioned. In particular we were interested in the median area per patient the median perimeter per patient and the median ratio area perimeter\u00b2 per patient. This version makes a two pass segmentation a first pass aims at finding where glomeruli are detection threshold is very low and a second pass aims at refining segmentations by applying our models on tiles centered on glomeruli found during the first pass. COLOR_BGR2Lab cv2. All put together the result is the following load_resize function. It is not yet binarized. choice cspaces img cv2. cvtColor img cspace if self. 071 1e2425f28 8768 356 0. However these findings must be confirmed on more patients as they are based on a very small sample of people that is not necessarily representative of the general population. COLOR_BGR2HLS cv2. This is only necessary during the first pass as the second pass will operate on pre selected tiles. 070 3589adb90 28081 630 0. COLOR_BGR2YUV cv2. Here is how we load an image using NumPy MemMap. We used overlapping step of 0. We import them as follows. Imports TODO remove PARAMETERS Printing parameters Data processing Input data directory Reduce the original images by x times Size of tiles on which inference is done https www. In this competition these WSI can be divided into two categories those containing fresh frozen FFPE tissues and those containing formalin fixed paraffin embedded tissues. Such approach naturally increases the training dataset and enriches variation of glomeruli size and its proportion in the considered tile of fixed size. Here are the values we obtained based on the GT for images in the training set and based on our predictions for images in the test set. Very important point that we wanted to highlight that we were able to split all test images into two groups with respect to the ffpe ff type using the PhysicalSizeY from the tiff. sum 0 keepdims True if random. Multi scale input tilesWe used multiple image reduction rates 3 4 5 6 and 8 in order to create the tiles from original images. id median area \u00b5m\u00b2 median perimeter \u00b5m median area perimeter\u00b2 0486052bb 5610 309 0. COLOR_RGB2LAB cv2. However by some reasons we did not discover which this stopped working after the data update at least for the public LB. COLOR_BGR2XYZ cv2. COLOR_RGB2Lab cv2. COLOR_BGR2RGB cv2. com bguberfain memory aware rle encoding. All these functions are combined in make_one_prediction_1st and make_one_prediction_2nd that aim at saving segmentation mask tiles based on an image and a given set of padding parameters for the first pass. ", "id": "dannyelkallab/fork-of-hubmap-submission-experts-raster-i-o", "size": "13858", "language": "python", "html_url": "https://www.kaggle.com/code/dannyelkallab/fork-of-hubmap-submission-experts-raster-i-o", "git_url": "https://www.kaggle.com/code/dannyelkallab/fork-of-hubmap-submission-experts-raster-i-o", "script": "torch.utils.data __init__ HuBMAPTestDataset1st(Dataset) UneXt101(nn.Module) _generate_masks_2nd FPN(nn.Module) rle_encode_less_memory torchvision.models.resnet segmentation_models_pytorch UneXt50(nn.Module) Unet50(nn.Module) DataLoader _split_image_1st _reshape_depad_mask_1st PixelShuffle_ICNR _init_weight HuBMAPTestDataset2nd(Dataset) forward UnetBlock(nn.Module) torch.nn Model_pred_1st Model_pred_2nd _make_tiles_dataloader_2nd numpy __getitem__ _reconstruct_img _get_nored_pads_1st load_resize ConvLayer # TODO: remove _ASPPModule(nn.Module) PIL load_image ASPP(nn.Module) UneSt200(nn.Module) _tile_resize_save make_one_prediction_1st fastai.vision.all Effb7Unet Image matplotlib.pyplot ResNet make_predictions_2nd tqdm.notebook pandas Bottleneck Dataset __len__ tqdm _make_tiles_dataloader_1st torch.nn.functional _add_padding_1st img2tensor make_one_prediction_2nd Effb5Unet make_predictions_1st resnest.torch read_tiff _generate_masks_1st __iter__ _save_mask_tiles_1st get_mask_tiles_2nd get_mask_tiles_1st _select_tiles_1st = {\"ux50\" ", "entities": "(('WSI', 'formalin fixed paraffin embedded tissues'), 'divide') (('models', 'fresh even others'), 'trick') (('This', 'CV'), 'give') (('Eventually we', 'model'), 'need') (('Image names', 'sample_submission'), 'retrieve') (('load', 'NumPy MemMap'), 'be') (('we', '_ tile_resize_save function'), 'apply') (('it', 'scores'), 'save') (('they', 'WSI'), 'Slide') (('particular we', 'ratio median patient'), 'be') (('we', '_ reconstruct_img following function'), 'apply') (('Additionally we', 'training more dataset'), 'use') (('_ mask', 'original image'), 'after') (('goal', 'human kidneys'), 'be') (('SubmissionHere', 'generated masks'), 'be') (('that', 'close edges'), 'make') (('dimensions', 'tile size'), 'add') (('height', 'saturation check print x1 x2 y1 y2 p.'), 'supervise') (('put', 'together result'), 'be') (('PUBLIC_ONLY then we', 'test public data'), 'make') (('training', '256x256'), 'update') (('we', 'test private data'), 'demonstrate') (('additional insights', 'notebook'), 'provide') (('we', 'tiff'), 'point') (('scale', 'original images'), 'use') (('_ generate_masks_1st', '_ generate_masks_2nd zero padded mask'), 'output') (('we', 'kernel inference time'), 'use') (('that', 'first pass'), 'combine') (('simplified method', 'first pixel'), 'aware') (('Generating', 'Segmentation MaskThe segmentation mask'), 'generate') (('we', 'test set'), 'be') (('how we', 'predictions'), 'make') (('we', 'idea'), 'share') (('we', 'binarization even proper threshold'), 'stoch') (('how we', 'predictions'), 'inferencein') (('ColorSpaced CutMix augs', '5 patient based CV'), 'set') (('how shape', 'weight'), 'describe') (('we', 'ensembling'), 'use') (('we', 'that'), 'try') (('that', 'predictions'), 'base') (('we', 'that'), 'reconstruct') (('that', 'groups'), 'be') (('inference', 'https www'), 'remove') (('aforementioned DataLoader', '_ following make_tiles_dataloader_1st'), 'generate') (('group', 'information'), 'copy') (('com 256x256 images', 'Tiles selection https www'), 'iafoss') (('DataLoader', 'HuBMAPTestDataset1st'), 'base') (('us', 'tiles'), 'allow') (('Then we', 'test set'), 'generate') (('Additionally we', 'test public set'), 'observe') (('we', 'better performance'), 'be') (('very we', 'tiles'), 'divide') (('where glomeruli', 'first pass'), 'make') (('wasn t', 'inference time constraints'), '33') (('structure', 'better fresh frozen ones'), 'notice') (('we', 'color saturations'), 'select') (('median area', '0'), '070') (('You', 'cutmix'), 'see') (('Final submissions', 'robustness'), 'be') (('second version', 'separate notebook'), 'describe') (('RLE conversion', 'https www'), 'do') (('that', 'necessarily general population'), 'confirm') (('we', 'inference'), 'define') (('we', 'glomeruli'), 'InsightsThanks') (('Generate masks Reshape', 'cleaning'), 'shape') (('when models', 'independently group'), 'demonstrate') (('we', 'inference'), 'be') (('we', 'tile selection process'), 'say') (('section we', 'approach version'), 'description') (('Eventually obtained mask', 'RLE format'), 'convert') (('load facebookresearch semi', 'dilatations m customized torch'), 'supervise') (('IntroductionThis kernel', 'HuBMAP competition 2021 version'), 'contain') (('we', 'improvement'), 'give') (('how naturally looked tiles', 'method'), 'see') (('this', 'at least public LB'), 'discover') (('Get boxes', 'rle https Eventually www'), 'save') (('next step', 'padding'), 'be') (('it', 'competition'), 'use') (('inference', 'training'), 'inference') (('Generating', 'make_predictions_2nd get_mask_tiles_1st functions'), 'generate') (('Segmentation mask tiles', 'following get_mask_tiles_1st functions'), 'generate') (('we', 'final submissions'), 'decide') (('Such approach', 'fixed size'), 'increase') (('we', 'finally approach'), 'want') (('second pass', 'selected tiles'), 'be') ", "extra": "['annotation', 'patient', 'test', 'procedure']", "label": "Perfect_files", "potential_description_queries": ["256x256", "annotation", "apply", "approach", "area", "augmentation", "balance", "batch", "best", "bit", "bounding", "box", "center", "check", "choice", "close", "code", "coefficient", "color", "combined", "competition", "compute", "conversion", "convert", "copy", "correlation", "could", "create", "crop", "csv", "custom", "cut", "cutmix", "data", "dataset", "describe", "detection", "dice", "dimension", "directory", "downsample", "efficientnet", "empty", "encoding", "end", "even", "exp", "feed", "ff", "file", "final", "find", "fixed", "fold", "following", "found", "function", "general", "generate", "generated", "glomeruli", "gray", "group", "hand", "handle", "height", "high", "highlight", "human", "id", "image", "img", "import", "improvement", "increase", "inference", "initially", "input", "intersection", "issue", "kernel", "labeled", "leaderboard", "least", "len", "little", "load", "main", "manual", "mask", "match", "median", "memory", "method", "minimum", "model", "most", "multiple", "name", "need", "next", "non", "normal", "not", "number", "objective", "order", "out", "overlap", "padding", "part", "patch", "patient", "people", "per", "perform", "pipeline", "pixel", "png", "point", "power", "pre", "precision", "prediction", "preprocessing", "print", "procedure", "processing", "provide", "public", "purpose", "py", "rand", "random", "ratio", "read", "reason", "reconstruct", "remove", "reshape", "result", "rle", "robust", "running", "sample", "sampling", "save", "saving", "scale", "second", "section", "segment", "segmentation", "select", "selected", "selection", "semi", "separate", "set", "shape", "single", "size", "soft", "source", "split", "sqrt", "square", "standard", "step", "structure", "submission", "sum", "supervised", "team", "test", "those", "threshold", "through", "tile", "time", "training", "transpose", "type", "up", "update", "validation", "variation", "version", "vision", "weight", "while", "width", "wrapper"], "potential_description_queries_len": 193, "potential_script_queries": ["forward", "nn", "numpy", "torch", "tqdm"], "potential_script_queries_len": 5, "potential_entities_queries": ["check", "even", "following", "inference", "mask", "patient", "public", "selection"], "potential_entities_queries_len": 8, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 197}