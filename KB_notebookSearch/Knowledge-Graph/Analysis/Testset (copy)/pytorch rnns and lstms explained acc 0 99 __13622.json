{"name": "pytorch rnns and lstms explained acc 0 99 ", "full_name": " h1 1 Introduction h1 2 Before we start h1 3 RNN with 1 Layer h2 3 1 Youtube Videos to Save you Time h2 3 2 RNN with 1 Layer and 1 Neuron h2 3 3 RNN with 1 Layer and Multiple Neurons h2 3 4 Vanilla RNN for MNIST Classification h3 3 4 1 Import the Data h3 3 4 2 RNN Architecture for MNIST Classification h3 Understanding the Model h3 3 4 3 Training h1 4 Multilayer RNNs h2 4 1 Why multilayers h2 4 2 Multilayer RNN for MNIST Classification h3 Understanding the Model h3 Training h1 5 LSTM Long Short Term Memory RNNs h2 5 1 Material to Save you Time h2 5 2 Why RNN might not be the best idea h2 5 3 Vanishing Gradient Problem h2 5 4 How does LSTM work h2 5 5 LSTM for MNIST Classification h3 How the Model Works h3 Training on ALL IMAGES h1 6 Bonuses h2 6 1 Confusion Matrix h2 6 2 Why shouldn t you use Transfer Learning h1 Other How I taught myself Deep Learning Notebooks h1 References ", "stargazers_count": 0, "forks_count": 0, "description": "I am by no means a teacher but in this notebook I will 1. 5 LSTM for MNIST Classification Bidirectional LSTMs are an extension of traditional LSTMs that can improve model performance on sequence classification problems. backpropagation uses the loss to adjust the weights in the network going BACKWARDS 5. Multilayer RNNs 4. and returns different kinds of outputs the next word letter in the sequence paired with an FNN it can return a classification etc. Ilustrated Guide to Recurrent Neural Networks Understanding the IntuitionFrom Michael Phi 3. So if you are frustrated just like I was when I started this stuff I hope the following guidelines will help you. The loop passes the input forward sequentialy while retaining information about it4. Also try changing the batch_size to 20 instead of 64. Download data The Neural Network RNN Layer Fully Connected Layer Initialize hidden state with zeros Creating RNN Log probabilities Reshaped out STATICS how many images to be trained in one iteration image 28 by 28 can be changed to any number neurons 10 different digits Create a train_loader to select a batch from it Select one full batch from the data Reshape Creating the model Understand Model Parameters Create dataloader for training dataset so we can train on multiple batches Shuffle after every epoch Create criterion and optimizer Train the data multiple times Save Train and Test Loss Set model in training mode Get rid of the channel Create log probabilities Clears the gradients from previous iteration Computes loss how far is the prediction from the actual Computes gradients for neurons Updates the weights Save Loss Accuracy after each iteration Print Average Train Loss Accuracy after each epoch Save Test Accuracy Evaluation mode Get rid of the channel Create logit predictions Add Accuracy of this batch Print Final Test Accuracy STATICS Instantiate the model TRAIN Create RNN Create FNN Instantiate hidden_state at timestamp 0 Compute RNN. How I taught myself Deep Learning Vanilla NNs https www. It uses previous information to affect later ones2. The accuracy of the model now is impressive but by altering some of these hyperparameters can change this sweet spot we found instantly. RNN from PyTorch. They train the model forward and backward on the same input so for 1 layer LSTM we get 2 hidden and cell states How the Model Works Below is a schema of how the example code works Training on ALL IMAGES Now we get even HIGHER accuracies than the ones before. I observed that when using RNNs is just another mathematical method in which the computer learns numbers and can therefore identify patterns. 01 and see what happens. It is not like CNNs when we know we put many filters on the image to extract the essence. We ll have 2 timesteps 0 and 1. Accuracy improves faster compared to the Vanilla RNN while final TEST Accuracy is slightly bigger. 1 Why multilayers Why use multiple layers rather than 1 to create higher level abstractions and capture more non linearities between the data Multilayers in RNN we want to create such abstractions and at the same time enforce their correlation with the previous inputs for 2 layers there are 2 hidden states as output for 3 layers there are 3 hidden states and so on Activation functions ReLU vs Tanh use them to try to erase the vanishing gradients problem we ll come back to these in the next chapter LSTM 4. com andradaolteanu convolutional neural nets cnns explained Why ConvNets Convolutions Explained Computing Activation Maps Kernels Padding Stride AlexNet MNIST Classification using Convolutions 3. Lastly LSTMs were the best performing ones 99 accuracy. a forward step through the network 2. com andradaolteanu how i taught myself deep learning 1 pytorch fnn Convolutional Neural Nets CNNs Explained https www. Current Cell State ft Ct 1 it Ct 4. There are only 3 matrixes U V W that contain weights as parameters. So please be patient with yourself and if you don t understand something right away continue reading coding and it will all make sense in the end. This is why RNNs might be weird in the approach for image classification but nevertheless very effective. There are 3 layers Input Output and Hidden where the information is stored 3. mm matrix multiplication 3. For the moment we ll stick with 1. You will understand how LSTMs are different from RNNs how they works and what is Vanishing Gradient Problem. For somebody that starts in this area with no background whatsoever it can be very confusing especially because I seem to be unable to find code with many explanations and comments. 1 Confusion Matrix A good way to visualize better how the model is performing is through a confusion matrix. Read more about this here. Before we start This is my third notebook in the series How I taught myself Deep Learning. This notebook is made to bring more clear understanding of concepts and coding so this would also help me add modify and improve it. 3 RNN with 1 Layer and Multiple Neurons Difference vs RNN 1 neuron 1 layer size of output changes because size of n_neurons changes size of the bias changes it s the size of n_neurons and W matrix 3. 3 Vanishing Gradient Problem What is vanishing gradient it is due to the nature of backpropagation during the optimization process the steps are 1. 2 Why shouldn t you use Transfer Learning Transfer learning is a genious way to use the weights of a pretrained model on another set of images. 1 Import the Data Note to further augmentations on the data check albumentations for PyTorch https albumentations. As you see the previous examples can t support large inputs and outputs as we would have to input the information at every timestep and output the results. If we make a recap FNNs from my previous notebook had an accuracy of 80 CNNs had and accuracy of almost 90 while RNN reached 97. RNN is a very powerful neural net. Input Gate Xt ht 1 creates a candidate with what information to remain 3. An RNN takes in different kind of inputs text words letters parts of an image sounds etc. io en latest api augmentations. com In recurrent neural networks like LSTMs is it possible to do transfer learning Has there been any research in this area Other How I taught myself Deep Learning Notebooks How I taught myself Deep Learning Vanilla NNs https www. RNNs model sequential data meaning they have sequential memory. com andradaolteanu how i taught myself deep learning vanilla nns or CNNs https www. com watch v 8HyCNIVRbSU Also the blog post https towardsdatascience. 4 How does LSTM work An LSTM is more complex than an simple RNN it is composed by cell states and gates it has the purpose to LEARN what to remember and forget reduntant information it uses SIGMOID functions instead of TANH Composition of the cell in LSTM the cell has 2 outputs the cell state and the hidden state 1. Share articles videos I watched that TRULY helped2. Convolutional Neural Nets CNNs Explained https www. This technique is often used in deep learning classification problems that use CNN like EffNets ResNets etc. Try adding weight_decay to the optimizer functions. LSTM Long Short Term Memory RNNs 5. Introduction This notebook is just me being frustrated on deep learning and trying to understand in baby steps what is going on here. backpropagation calculates the gradients of the nodes in each layer if the GRADIENT is big the adjustment in weight is big and vice versa PROBLEM during backpropagation each node calculates its gradient with the respect of the effects of the gradients in the layer before it. Let s see how the model performs by adding 1 more layer. com illustrated guide to lstms and gru s a step by step explanation 44e9eb85bf21 Understanding LSTM Networks https colah. 1 Youtube Videos to Save you Time I highly recommend watching the following to better understand RNNs. So you can see how well each label is predicted and what labels the model confuses with other labels for example a 7 can be sometimes confused with 1. 4 Vanilla RNN for MNIST Classification From now on we ll use the build in nn. detach is required to prevent vanishing gradient problem Compute FNN We get rid of the second size STATICS neurons layers Taking a single batch of the images Remove channel from shape Create model instance Making log predictions STATICS Instantiate the model We ll use TANH as our activation function TRAIN Step1 the LSTM model Step2 the FNN we ll have 2 more layers Set initial states Hidden state Cell state Hidden state Cell state LSTM Reshape FNN STATICS width of image number of hidden neurons number of layers possible choices Taking a single batch of the images Remove channel from shape Creating the Model Making log predictions STATICS Instantiate the model We ll use TANH as our activation function TRAIN First we make sure we disable Gradient Computing Model in Evaluation Mode. As you ll see it s performance is far grater than a normal FNN or CNN Side Note Images used as input NEED to have 1 channel so need to be B W 3. Try changing the learning_rate to 0. Pro Tip Use print a lot if you don t understand what is happening helps you visualize Understanding the Model Here is what s happening to the batch below If we unfold the RNN 3. In addition it usually works with images and text while ML usually works with tabular data. Explain code along the way to the best of my ability Note Deep learning coding is VERY different in structure than the usual sklearn for machine learning. com andradaolteanu how i taught myself deep learning 1 pytorch fnn PyTorch and Tensors Neural Network Basics Perceptrons and a Plain Vanilla Neural Net model MNIST Classification using FNN Activation Functions Forward Pass Backpropagation Loss and Optimizer Functions Batching Iterations and Epochs Computing Classification Accuracy Overfitting Data Augmentation Weight Decay Learning Rate Dropout and Layer Optimization 2. If you liked this upvote Cheers References Illustrated Guide to Recurrent Neural Networks Understanding the Intuition https www. 2 RNN Architecture for MNIST Classification Note Don t bother with the prints they are there for later only to understand what s happening inside the network. com andradaolteanu convolutional neural nets cnns explained If you have any questions please do not hesitate to ask. Forget Gate Xt ht 1 desides what information to FORGET the closer to 0 is forget the closer to 1 is remain 2. This is mainly because recurrent data cannot really be generalized like static data images can be. However this is not a regular technique used for RNN networks. RNN with 1 Layer Recurrent Neural Networks are very different from FNNs https www. 2 Why RNN might not be the best idea Issues in Vanilla RNNs have short term memory caused by the vanishing gradient problem as the RNN process has more steps timestamps it has more and more difficulty retaining information from previous steps 5. Creating the model Checking the output Parameters STATICS RNN inputs Creating the model Checking the output Parameters Customized transform transforms to tensor here you can normalize perform Data Augmentation etc. 2 RNN with 1 Layer and 1 Neuron You can always increase the number of neurons in an RNN. com andradaolteanu how i taught myself deep learning vanilla nns but with some changes suited to the RNN s needs. This info is stored in the hidden state 5. Output Gate Xt ht 1 ct desides what the next hidden state should be which contains info about previous inputs Note Check THIS blog post for more detailed explanation. io posts 2015 08 Understanding LSTMs Imports To display youtube videos When running on the CuDNN backend two further options must be set Set a fixed value for the hash seed STATICS RNN inputs The Neural Network __init__ the function where we create the architecture Weights are random at first U contains connection weights for the inputs of the current time step for 1 neuron size 4 rows and 1 column W contains connection weights for the outputs of the previous time step for 1 neuron size 1 row and 1 column The bias for 1 neuron size 1 row and 1 column forward function where we apply the architecture to the input Computes two outputs one for each time step two overall. We ll use get_accuracy and train_network functions from my previous notebook https www. com illustrated guide to lstms and gru s a step by step explanation 44e9eb85bf21 The Stacked LSTM is like the Multilayer RNN it has multiple hidden LSTM layers which contain multiple memory cells 5. Illustrated Guide to LSTM s and GRU s A step by step explanationAlso this amazing blog post explains the Vanishing Gradient Problem and LSTMs 5. Note When using RNN in image classification it is hard to find the logic of why exactly are we doing this. The Architecture of our class will look like the figure below torch. 1 Material to Save you Time I highly recommend going through the references below before continuing. com watch v LHXXI4 IEns Illustrated Guide to LSTM s and GRU s A step by step explanation https www. 2 Multilayer RNN for MNIST Classification Understanding the Model Here is what s happening in the batch below If we unfold the Multilayer RNN Example Training. computing the predictions 3. Side Note It s AMAZING how important hyperparameters are. https towardsdatascience. comparing the predictions with the actual values and computing a LOSS function 4. So if the adjustment in the previous layer is small then the adjustment in the current layer will be smaller CONSEQUENCE the first layers of the network don t learn because the adjustemnt are extremely small 5. These DON T change with the input they stay the same through the entire sequence. com andradaolteanu how i taught myself deep learning convnet cnns. ", "id": "andradaolteanu/pytorch-rnns-and-lstms-explained-acc-0-99", "size": "13622", "language": "python", "html_url": "https://www.kaggle.com/code/andradaolteanu/pytorch-rnns-and-lstms-explained-acc-0-99", "git_url": "https://www.kaggle.com/code/andradaolteanu/pytorch-rnns-and-lstms-explained-acc-0-99", "script": "torch.optim train_network __init__ RNNVanilla(nn.Module) YouTubeVideo forward torch.nn seaborn numpy get_confusion_matrix get_accuracy matplotlib.pyplot set_seed VanillaRNN_MNIST(nn.Module) torch.nn.functional MultilayerRNN_MNIST(nn.Module) LSTM_MNIST(nn.Module) IPython.display torchvision.transforms ", "entities": "(('RNN', 'FNNs https very www'), 'be') (('following guidelines', 'you'), 'be') (('info', 'hidden state'), 'store') (('where we', 'time step'), 'posts') (('also me', 'it'), 'make') (('we', 'sweet spot'), 'be') (('com how i', 'Tensors Neural Network Basics Plain Vanilla Neural Net model MNIST FNN Activation Functions'), 'andradaolteanu') (('We', 'notebook https previous www'), 'use') (('you', 'questions'), 'explain') (('loop', 'it4'), 'pass') (('better how model', 'confusion matrix'), '1') (('how model', '1 more layer'), 'let') (('that', 'sequence classification problems'), 'be') (('it', 'classification'), 'return') (('I', 'highly references'), 'material') (('Convolutional Neural Nets CNNs', 'https www'), 'explain') (('we', 'results'), 'support') (('TANH instead Composition', 'cell state'), '4') (('computer', 'therefore patterns'), 'observe') (('U V only 3 matrixes that', 'parameters'), 'be') (('1 You', 'RNN'), 'RNN') (('com how i', 'needs'), 'andradaolteanu') (('we', 'nn'), 'use') (('adjustemnt', 'network don t'), 'be') (('Architecture', 'torch'), 'look') (('steps', 'optimization process'), '3') (('RNN', '97'), 'have') (('we', 'essence'), 'be') (('it', 'n_neurons'), 's') (('How I', 'Deep Vanilla https Learning NNs www'), 'com') (('we', 'next chapter'), 'want') (('below we', 'RNN'), 'print') (('which', 'more detailed explanation'), 'desides') (('com how i', 'convnet deep learning cnns'), 'andradaolteanu') (('it', 'end'), 'be') (('information', '1 candidate'), 'create') (('Convolutional Neural Nets CNNs', 'https www'), 'andradaolteanu') (('Transfer Learning Transfer learning', 'images'), 'use') (('blog amazing post', 'Gradient Vanishing Problem'), 'Guide') (('TEST final Accuracy', 'Vanilla faster RNN'), 'improve') (('Now we', 'ones'), 'train') (('node', 'it'), 'calculate') (('we', 'Evaluation Mode'), 'require') (('ConvNets Why Convolutions', 'Convolutions'), 'explain') (('why exactly we', 'this'), 'note') (('I', 'notebook'), 'be') (('closer to 1', '0'), 'deside') (('what', 'Gradient Problem'), 'understand') (('backpropagation', 'BACKWARDS'), 'use') (('7', 'sometimes 1'), 'see') (('com how i', 'vanilla deep nns'), 'andradaolteanu') (('what', 'baby steps'), 'introduction') (('CNN Side Note input', '1 channel'), 'be') (('Deep learning coding', 'machine learning'), 'explain') (('why RNNs', 'image classification'), 'be') (('STATICS', 'TRAIN RNN Create FNN Instantiate Compute timestamp 0 RNN'), 'datum') (('letters parts', 'image'), 'take') (('RNNs model sequential they', 'sequential memory'), 'datum') (('it', 'previous steps'), 'be') (('However this', 'RNN regular networks'), 'be') (('here you', 'etc'), 'static') (('It', 'later ones2'), 'use') (('there only what', 'network'), 'bother') (('very especially I', 'many explanations'), 'be') (('com', 'step explanation'), 's') (('ML', 'usually tabular data'), 'work') (('which', 'memory multiple cells'), 's') (('that', 'etc'), 'use') (('How I', 'Deep Vanilla https Learning NNs www'), 'teach') (('How I', 'Deep Learning'), 'be') (('you', 'Intuition https www'), 'like') (('they', 'same entire sequence'), 'change') (('highly following', 'better RNNs'), 'Videos') (('below we', 'Multilayer RNN Example Training'), 'RNN') ", "extra": "['patient', 'test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "adjust", "api", "apply", "approach", "architecture", "area", "article https towardsdatascience", "backend", "background", "backpropagation", "backward", "batch", "batch_size", "best", "blog", "build", "cell", "channel", "check", "classification", "clear", "code", "coding", "column", "computer", "confusion", "connection", "contain", "convolutional", "correlation", "create", "criterion", "ct", "current", "data", "dataset", "display", "en", "epoch", "even", "every", "explained", "extension", "extract", "faster", "figure", "final", "find", "fixed", "following", "forward", "found", "ft", "function", "gradient", "hash", "help", "hope", "https towardsdatascience", "idea", "image", "improve", "increase", "info", "input", "instance", "io", "iteration", "label", "layer", "learn", "learning", "learning_rate", "letter", "level", "log", "look", "loop", "lot", "matrix", "meaning", "memory", "method", "might", "mm", "mode", "model", "moment", "multiple", "my", "nature", "need", "network", "neural", "neuron", "next", "no", "node", "non", "normal", "normalize", "not", "notebook", "number", "optimization", "optimizer", "out", "output", "patient", "perform", "performance", "performing", "post", "prediction", "pretrained", "prevent", "print", "problem", "purpose", "pytorch", "random", "reading", "recommend", "recurrent", "research", "return", "right", "row", "running", "second", "select", "sense", "sequence", "set", "shape", "short", "single", "size", "sklearn", "something", "start", "state", "step", "structure", "stuff", "support", "tabular", "technique", "tensor", "term", "text", "through", "time", "timestamp", "train", "training", "transfer", "transform", "try", "understanding", "value", "vanilla", "visualize", "weight", "while", "width", "word", "work"], "potential_description_queries_len": 169, "potential_script_queries": ["nn", "numpy", "seaborn", "torch"], "potential_script_queries_len": 4, "potential_entities_queries": ["faster", "learning", "multiple", "timestamp"], "potential_entities_queries_len": 4, "potential_extra_queries": ["test"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 173}