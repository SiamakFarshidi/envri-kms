{"name": "in depth guide to google s bert ", "full_name": " h1 Understanding BERT for Disaster NLP h1 Introduction h3 Welcome to this kernel In this kernel we are going to explore Google s BERT I will try to cover all the topics which helps to understand BERT clearly Finally we will see how to implement BERT for disaster NLP h3 Without wasting time lets get started h2 Table of Contents h3 References and credits h1 1 Intution behind RNN based Sequence to Sequence Model h1 1 1 Limitations of RNN S h1 2 Introduction to the Transformer h2 2 1 Transformer s Model Architecture h2 2 2 Understanding self attention h2 2 3 Limitations of the Transformer h1 3 Understanding BERT Bidirectional Encoder Representations from Transformers h2 3 1 How Does BERT Work h3 3 1 1 BERT s Architecture h2 3 2 Text processing for BERT h3 1 Masked LM MLM h3 2 Next Sentence Prediction NSP h2 3 3 How to use BERT Fine tuning h2 3 4 Takeaways h2 3 5 Compute considerations training and applying h1 4 Implementation of BERT using TFHub h4 Credits This part was taken from this kernel please refer to this awesome kernel and consider upvoting Thanks to the author of the kernel xhlulu h2 4 1 Importing necessary modules h3 Getting tokenizer h2 4 2 Helper Functions h2 4 3 Loading BERT from the Tensorflow Hub h2 4 4 Loading data h2 4 5 Loading tokenizer from the bert layer h2 4 6 Encoding the text into tokens masks and segment flags h2 4 7 Model Build Train Predict Submit h1 Please consider Upvoting if you like this kernel h1 Thank you for reading this kernel h1 Happy learning sharing ", "stargazers_count": 0, "forks_count": 0, "description": "I will try to cover all the topics which helps to understand BERT clearly. Let s take a simple example of a sequence to sequence model. 1 Limitations of RNN S 2. In the above example all the tokens marked as EA belong to sentence A and similarly for EB Token Embeddings These are the embeddings learned for the specific token from the WordPiece token vocabularyFor a given token its input representation is constructed by summing the corresponding token segment and position embeddings. Calculating the probability of IsNextSequence with softmax. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. 1 BERT s Architecture The BERT architecture builds on top of Transformer. How do Transformers Work in NLP A Guide to the Latest State of the Art Models https www. This helps the decoder focus on the appropriate parts of the input sequence. As a consequence the model converges slower than directional models a characteristic which is offset by its increased context awareness. Calculating the probability of each word in the vocabulary with softmax. Without wasting time lets get started. BERT Explained State of the art language model for NLP https towardsdatascience. png For bert every input embedding is a combination of 3 embeddings Position Embeddings BERT learns and uses positional embeddings to express the position of words in a sentence. Adding a classification layer on top of the encoder output. In technical terms the prediction of the output words requires 1. One way to deal with this is to consider both the left and the right context before making a prediction. When training the BERT model Masked LM and Next Sentence Prediction are trained together with the goal of minimizing the combined loss function of the two strategies. During training 50 of the inputs are a pair in which the second sentence is the subsequent sentence in the original document while in the other 50 a random sentence from the corpus is chosen as the second sentence. Masked LM MLM Before feeding word sequences into BERT 15 of the words in each sequence are replaced with a MASK token. Using BERT a Q A model can be trained by learning two extra vectors that mark the beginning and the end of the answer. In addition to the two sub layers in each encoder layer the decoder inserts a third sub layer which performs multi head attention over the output of the encoder stack. com av blog media wp content uploads 2019 06 Screenshot from 2019 06 17 20 03 14. com max 1321 0 m_kXt3uqZH9e7H4w. This contains information about the input sequence This context vector is then passed to the decoder and it is then used to generate the target sequence English phrase If we use the Attention mechanism then the weighted sum of the hidden states are passed as the context vector to the decoder 1. com av blog media wp content uploads 2019 09 bert_emnedding. 2 Understanding self attention 2. 5 Compute considerations training and applying 4. 3 Limitations of the Transformer3. 0 when trained on 1M steps 128 000 words batch size compared to 500K steps with the same batch size. 1 Importing necessary modules Getting tokenizer 4. read_csv Input data files are available in the. The first is a multi head self attention mechanism and the second is a simple positionwise fully connected feed forward network. In other words the text is split without respecting the sentence or any other semantic boundary But undoutedly transformer inspired BERT and all the following breakthroughs in NLP. Thanks to the author of the kernel xhlulu https www. Many models predict the next word in a sequence a directional approach which inherently limits context learning. Decoder The decoder is also composed of a stack of N 6 identical layers. Now that we know the overall architecture of BERT let s see what kind of text processing steps are required before we get to the model building phase. These combinations of preprocessing steps make BERT so versatile. com av blog media wp content uploads 2019 06 Screenshot from 2019 06 17 22 47 53. That is the output of each sub layer is LayerNorm x Sublayer x where Sublayer x is the function implemented by the sub layeritself. com av blog media wp content uploads 2019 06 seq2seq. png Take a look at the above image. Demystifying BERT A Comprehensive Guide to the Groundbreaking NLP Framework https www. com av blog media wp content uploads 2019 06 Screenshot from 2019 06 17 20 01 32. 3 Limitations of the Transformer Attention can only deal with fixed length text strings. The output of the CLS token is transformed into a 2 1 shaped vector using a simple classification layer learned matrices of weights and biases. With enough training data more training steps higher accuracy. To help the model distinguish between the two sentences in training the input is processed in the following way before entering the model 1. Similar to the encoder we employ residual connections around each of the sub layers followed by layer normalization. 2 Understanding self attention Self attention sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Check out the below illustration https s3 ap south 1. 1 Transformer s Model Architecture https s3 ap south 1. com av blog media wp content uploads 2019 09 sent_context. 3 How to use BERT Fine tuning Using BERT for a specific task is relatively straightforward BERT can be used for a wide variety of language tasks while only adding a small layer to the core model 1. In Named Entity Recognition NER the software receives a text sequence and is required to mark the various types of entities Person Organization Date etc that appear in the text. In Question Answering tasks e. 1 Transformer s Model Architecture 2. The outputs are concatenated and linearly transformed as shown in the figure below https s3 ap south 1. The encoder and decoder blocks are actually multiple identical encoders and decoders stacked on top of each other. For instance on the MNLI task the BERT_base accuracy improves by 1. For example running this by clicking run or pressing Shift Enter will list all files under the input directory Any results you write to the current directory are saved as output. com max 986 0 ViwaI3Vvbnd CJSQ. Introduction to the Transformer The Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequence aligned RNNs or convolution. 5 Loading tokenizer from the bert layer 4. Understanding BERT for Disaster NLP Introduction Welcome to this kernel. That s why it learns a unique embedding for the first and the second sentences to help the model distinguish between them. 3 How to use BERT Fine tuning 3. BERT s bidirectional approach MLM converges slower than left to right approaches because only 15 of words are predicted in each batch but bidirectional training still outperforms left to right training after a small number of pre training steps. 5 Compute considerations training and applying https miro. 1 Importing necessary modules 4. com blog 2019 06 understanding transformers nlp state of the art models 3. Let s see an example to illustrate this. When the model is processing the word it self attention tries to associate it with animal in the same sentence. Intution behind RNN based Sequence to Sequence Model Sequence to sequence seq2seq models in NLP are used to convert sequences of Type A to sequences of Type B. com av blog media wp content uploads 2019 09 bert_encoder. png All of these Transformer layers are Encoder only blocks. com bert explained state of the art language model for nlp f8b21a9b6270 2. 1 Limitations of RNN S Dealing with long range dependencies is still challenging The sequential nature of the model architecture prevents parallelization. com av blog media wp content uploads 2019 06 Screenshot from 2019 06 17 19 53 10. For example if a sentence is split from the middle then a significant amount of context is lost. com max 875 1 LgbpLsRUGbtTPmMSUO2Drw. It is therefore referred to as Multi head Attention. That s exactly what BERT does. The model then attempts to predict the original value of the masked words based on the context provided by the other non masked words in the sequence. The Encoder block has 1 layer of a Multi Head Attention followed by another layer of Feed Forward Neural Network. A positional embedding is added to each token to indicate its position in the sequence. gif The above seq2seq model is converting a German phrase to its English counterpart. Next Sentence Prediction NSP In the BERT training process the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. 1 the software receives a question regarding a text sequence and is required to mark the answer in the sequence. Now focus on the below image. 1 How Does BERT Work 3. https s3 ap south 1. png Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions. 2 Helper Functions 4. 7 Model Build Train Predict Submit Please consider Upvoting if you like this kernel. BERT is a deeply bidirectional model. We also modify the self attentionsub layer in the decoder stack to prevent positions from attending to subsequent positions. Attention Is All You Need https arxiv. To facilitate these residual connections all sub layers in the model as well as the embeddinglayers produce outputs of dimension dmodel 512. As a result the pre trained BERT model can be finetuned with just one additional output layer to create state of the art models for a wide range of tasks such as question answering and language inference without substantial taskspecific architecture modifications. png Encoder The encoder is composed of a stack of N 6 identical layers. Model size matters even at huge scale. Implementation of BERT using TFHub 4. com xhlulu disaster nlp keras bert using tfhub please refer to this awesome kernel and consider upvoting. This masking combined with fact that the output embeddings are offset by one position ensures that thepredictions for position i can depend only on the known outputs at positions less than i. The entire input sequence goes through the Transformer model. Happy learning sharing Go to TOP This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. Understanding BERT Bidirectional Encoder Representations from Transformers BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. png If we try to predict the nature of the word bank by only taking either the left or the right context then we will be making an error in at least one of the two given examples. A CLS token is inserted at the beginning of the first sentence and a SEP token is inserted at the end of each sentence. These are added to overcome the limitation of Transformer which unlike an RNN is not able to capture sequence or order information Segment Embeddings BERT can also take sentence pairs as inputs for tasks Question Answering. The concept and implementation of positional embedding are presented in the Transformer paper. Both the encoder stack and the decoder stack have the same number of units. The bidirectionality of a model is important for truly understanding the meaning of a language. Classification tasks such as sentiment analysis are done similarly to Next Sentence classification by adding a classification layer on top of the Transformer output for the CLS token. When training language models there is a challenge of defining a prediction goal. Implementation of BERT using TFHub Credits This part was taken from this kernel https www. com max 1773 0 KONsqvDohE7ytu_E. 3 Loading BERT from the Tensorflow Hub 4. Intution behind RNN based Sequence to Sequence Model 1. The idea behind Transformer is to handle the dependencies between input and output with attention and recurrence completely. The assumption is that the random sentence will be disconnected from the first sentence. png Let s see how this setup of the encoder and the decoder stack works The word embeddings of the input sequence are passed to the first encoder These are then transformed and propagated to the next encoder The output from the last encoder in the encoder stack is passed to all the decoders in the decoder stack as shown in the figure below An important thing to note here in addition to the self attention and feed forward layers the decoders also have one more layer of Encoder Decoder Attention layer. A sentence embedding indicating Sentence A or Sentence B is added to each token. png Let s first focus on the Encoder and Decoder parts only. We employ a residual connection around each ofthe two sub layers followed by layer normalization. Thank you for reading this kernel. png To predict if the second sentence is indeed connected to the first the following steps are performed 1. The number of encoder and decoder units is a hyperparameter https s3 ap south 1. com av blog media wp content uploads 2019 06 Screenshot from 2019 06 17 22 31 11. Can you figure out what the term it in this sentence refers to Is it referring to the street or to the animal It s a simple question for us but not for an algorithm. Table of Contents 1. There are two sentences in this example and both of them involve the word bank https s3 ap south 1. Let s break it down Both Encoder and Decoder are RNNs At every time step in the Encoder the RNN takes a word vector xi from the input sequence and a hidden state Hi from the previous time step The hidden state is updated at each time step The hidden state from the last unit is known as the context vector. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2. BERT_large with 345 million parameters is the largest model of its kind. Understanding BERT 3. Disaster NLP Keras BERT using TFHub https www. Such a comprehensive embedding scheme contains a lot of useful information for the model. In this kernel we are going to explore Google s BERT. 2 Text processing for BERT 3. BERT Pre training of Deep Bidirectional Transformers for Language Understanding https arxiv. 7 Model Build Train Predict Submit References and credits 1. Each layer has two sub layers. The text has to be split into a certain number of segments or chunks before being fed into the system as inputThis chunking of text causes context fragmentation. The decoder on the other hand has an extra Masked Multi Head Attention. Finally we will see how to implement BERT for disaster NLP. Introduction to the Transformers 2. To overcome this challenge BERT uses two training strategies 1. png The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non masked words. com blog 2019 09 demystifying bert groundbreaking nlp framework 4. For example translation of English sentences to German sentences is a sequence to sequence task. 2 Text processing for BERT https s3 ap south 1. Self attention is computed not once but multiple times in the Transformer s architecture in parallel and independently. Using BERT a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label. We currently have two variants available BERT Base 12 layers transformer blocks 12 attention heads and 110 million parameters BERT Large 24 layers transformer blocks 16 attention heads and 340 million parameters https s3 ap south 1. Self attention allows the model to look at the other words in the input sequence to get a better understanding of a certain word in the sequence. com xhlulu disaster nlp keras bert using tfhub Thanks to all the above blogs and article contributors. Vanishing Gradient Problem Issue of increasing gradients at each step called as exploding gradients. Bidirectional means that BERT learns information from both the left and the right side of a token s context during the training phase. Thanks you very much for you open source contibutions. 6 Encoding the text into tokens masks and segment flags 4. It is demonstrably superior on small scale tasks to BERT_base which uses the same architecture with only 110 million parameters. Multiplying the output vectors by the embedding matrix transforming them into the vocabulary dimension. ", "id": "ratan123/in-depth-guide-to-google-s-bert", "size": "18072", "language": "python", "html_url": "https://www.kaggle.com/code/ratan123/in-depth-guide-to-google-s-bert", "git_url": "https://www.kaggle.com/code/ratan123/in-depth-guide-to-google-s-bert", "script": "Adam ModelCheckpoint tensorflow.keras.optimizers Model numpy Dense tensorflow tensorflow.keras.layers pandas bert_encode tensorflow.keras.callbacks Input build_model tensorflow_hub tensorflow.keras.models ", "entities": "(('which', '110 only million parameters'), 'be') (('png encoder', 'N 6 identical layers'), 'Encoder') (('decoder', 'input sequence'), 'help') (('as well embeddinglayers', '512'), 'produce') (('right then we', 'given examples'), 'png') (('BERT pre trained model', 'language taskspecific architecture substantial modifications'), 'finetune') (('Self attention', 'parallel'), 'compute') (('chunking', 'context fragmentation'), 'have') (('you', 'output'), 'list') (('second sentence', 'subsequent original document'), 'NSP') (('s', 'first Encoder parts'), 'let') (('decoders', 'Encoder Decoder Attention layer'), 'let') (('outputs', 'https s3'), 'concatenate') (('therefore Multi', 'Attention'), 'refer') (('110 layers million Large 24 transformer', 'attention 16 heads'), 'have') (('other semantic undoutedly transformer', 'following NLP'), 'split') (('model', 'sequence'), 'allow') (('following steps', 'indeed first'), 'png') (('BERT_base accuracy', '1'), 'improve') (('decoder', 'Masked Multi Head extra Attention'), 'have') (('software', 'sequence'), 'receive') (('translation', 'task'), 'be') (('Text 2 processing', 'BERT https s3'), 'ap') (('It', 'algorithm'), 'figure') (('we', 'BERT'), 'go') (('why it', 'them'), 's') (('above model', 'English counterpart'), 'gif') (('It', 'python docker image https kaggle github'), 'go') (('idea', 'attention'), 'be') (('only 15', 'training pre steps'), 'approach') (('RNN S 1 Dealing', 'model architecture prevents parallelization'), 'limitation') (('prediction', '1'), 'require') (('com bert', 'nlp f8b21a9b6270 2'), 'explain') (('models', 'Type B.'), 'use') (('then significant amount', 'context'), 'lose') (('which', 'BERT'), 'try') (('positional embedding', 'sequence'), 'add') (('Transformer', 'RNNs'), 'introduction') (('BERT', 'steps'), 'make') (('BERT loss png function', 'non masked words'), 'take') (('order Segment Embeddings BERT', 'tasks Question Answering'), 'add') (('BERT 1 architecture', 'Transformer'), 'architecture') (('multi', 'encoder stack'), 'layer') (('hidden state', 'context vector'), 'let') (('All', 'Transformer layers'), 'png') (('model', 'different positions'), 'allow') (('i', 'less i.'), 'ensure') (('input entire sequence', 'Transformer model'), 'go') (('both', 'word bank https s3'), 'be') (('BERT', 'NLP https towardsdatascience'), 'explain') (('concept', 'Transformer paper'), 'present') (('Classification tasks', 'CLS token'), 'do') (('We', 'layer normalization'), 'employ') (('Disaster NLP Keras', 'TFHub https www'), 'BERT') (('relatively straightforward BERT', 'core model'), 'be') (('com understanding 2019 06 transformers', 'art models'), 'blog') (('other 50 random sentence', 'second sentence'), 'be') (('3 Limitations', 'length text only fixed strings'), 'deal') (('Masked Next Sentence Prediction', 'two strategies'), 'train') (('attention', 'same sentence'), 'try') (('Understanding', 'right layers'), 'design') (('output', 'weights'), 'transform') (('input embedding', 'sentence'), 'be') (('model', 'sequence'), 'attempt') (('s', 'sequence'), 'let') (('that', 'answer'), 'train') (('sometimes called', 'sequence'), 'be') (('BERT', 'training phase'), 'mean') (('we', 'model building phase'), 'now') (('characteristic which', 'context increased awareness'), 'converge') (('random sentence', 'first sentence'), 'be') (('How Transformers', 'Art Models https www'), 'work') (('that', 'text'), 'receive') (('decoder', 'N 6 identical layers'), 'decoder') (('you', 'kernel'), 'consider') (('We', 'subsequent positions'), 'modify') (('Finally we', 'disaster NLP'), 'see') (('training enough data', 'training more steps higher accuracy'), 'with') (('com xhlulu disaster nlp keras bert', 'awesome kernel'), 'refer') (('embedding comprehensive scheme', 'model'), 'contain') (('then weighted sum', 'decoder'), 'contain') (('Encoder block', 'Feed Forward Neural Network'), 'have') (('part', 'kernel https www'), 'implementation') (('bidirectionality', 'language'), 'be') (('we', 'layer normalization'), 'employ') (('Vanishing', 'exploding gradients'), 'call') (('which', 'context inherently learning'), 'predict') (('SEP token', 'sentence'), 'insert') (('input', 'model'), 'help') (('One way', 'right prediction'), 'be') (('encoder blocks', 'other'), 'be') (('Compute 5 considerations', 'https miro'), 'train') (('Model Build 7 Train', 'Submit References'), 'predict') (('read_csv Input data files', 'the'), 'be') (('that', 'NER label'), 'train') (('overcome', 'training two strategies'), 'use') (('Compute 5 considerations', '4'), 'train') (('input representation', 'corresponding token segment embeddings'), 'mark') (('number', 'encoder units'), 'be') (('Sentence embeddings', '2'), 'be') (('encoder stack', 'units'), 'have') (('LayerNorm Sublayer where x', 'sub'), 'be') (('sentence', 'Sentence token'), 'add') ", "extra": "['organization', 'test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "animal", "answer", "ap", "appear", "approach", "architecture", "art", "article", "author", "bank", "batch", "bert", "block", "blog", "boundary", "challenge", "classification", "combined", "compute", "concept", "connection", "consider", "consideration", "content", "context", "convert", "core", "create", "current", "data", "decoder", "depend", "dimension", "directory", "disaster", "document", "embedding", "encoder", "end", "environment", "error", "even", "every", "explained", "explore", "fact", "feed", "figure", "file", "fixed", "following", "forward", "framework", "function", "generate", "hand", "handle", "head", "help", "hyperparameter", "idea", "image", "implement", "implementation", "indicate", "inference", "input", "instance", "kaggle", "kernel", "language", "largest", "layer", "learning", "least", "left", "length", "let", "linear", "list", "load", "look", "lot", "masked", "masking", "matrix", "max", "meaning", "middle", "model", "multiple", "nature", "next", "nlp", "non", "not", "number", "offset", "open", "order", "out", "output", "overall", "pair", "parallel", "part", "png", "position", "positional", "pre", "predict", "prediction", "preprocessing", "prevent", "probability", "processing", "python", "question", "random", "range", "reading", "representation", "residual", "result", "right", "run", "running", "scale", "second", "segment", "sentence", "sentiment", "sequence", "setup", "several", "side", "similar", "single", "size", "source", "split", "stack", "state", "step", "sub", "sum", "system", "target", "task", "term", "text", "through", "time", "token", "training", "transformer", "try", "tuning", "under", "understanding", "unique", "unit", "unlabeled", "value", "vector", "while", "word", "write", "xhlulu"], "potential_description_queries_len": 170, "potential_script_queries": ["numpy", "tensorflow"], "potential_script_queries_len": 2, "potential_entities_queries": ["bank", "disaster", "image", "masked", "png", "random", "text", "understanding"], "potential_entities_queries_len": 8, "potential_extra_queries": ["test"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 173}