{"name": "image segmentation using unet tfrecords ", "full_name": " h1 Image segmentation using Unet architecture and TFrecords h3 Authors Muhammad Valiallah and Martin Page h2 Abstract h3 In this document we show h2 Introduction h2 Data h3 Encoding h3 Images h2 Evaluation Metric Dice Coefficient h2 Libraries to import h1 We tackle this problem into two general steps h2 Data Processing TF Records h2 Generating a TFRecord h2 Using TPU from Kaggle h2 U Net Model h2 Acknowlegements ", "stargazers_count": 0, "forks_count": 0, "description": "ImagesThe image files are large 1 2GB and difficult to train computationally intensive. Link GCS drive to Kaggle Notebook Add ons Google Cloud SDK 3. connected consecutively. These images are supplemented with a CSV file where for each image the pixel location of glomeruli cells spherical capsules are indicated. png Libraries to import We tackle this problem into two general steps 1. The U Net architecture uses a contracting path and an expansive path encoder decoder which thus give the U shape. 23 Convolution operations2. 0s to 1s For the mask BytesList won t unpack a string from an EagerTensor. How to sub sample a large image into smaller images a process referred to as tiling. 7080Epoch 14 30119 119 ETA 0s dice_coeff 0. Using TPU from KaggleA TPU is a Tensor Processing Unit it it has a built in distribution strategy with 8 cores. 7080Epoch 00029 saving model to kaggle working unet tpu. pdf using Python with tensorflow and keras. The differences being that a few normalisation layers were added to the downsampling block and we used strides of 1 for the convolution layers and different size input images. These records can be accessed by referring to the global storage pathWe now split the training data into a training set and validation set. Evaluation Metric Dice CoefficientThe dice coefficient is used to compare the pixel wise agreement between a predicted segmentation and the true value ground truth and is defined as 2 times the area of overlap between the predicted and actual value divided by the total number of pixels in both images see more https towardsdatascience. 7080Epoch 7 30119 119 ETA 0s dice_coeff 0. In this document we show 1. 7080Epoch 00011 saving model to kaggle working unet tpu. 7080Epoch 00025 saving model to kaggle working unet tpu. 7080Epoch 3 30119 119 ETA 0s dice_coeff 0. Segmentation creates a representation of an image that is easier and more meaningful to analyse. 7080Epoch 00003 saving model to kaggle working unet tpu. Create a tfrecord file and write the messages image and its associated features to it Serialisation function and its helpers Creating the TF record The code below is used to package all steps into a single function which will generate a separate TFrecord file for. The image_feature_description dictionary must be loaded the keys ultimtaley refer to the image_feature_description dictionary needs to be 4 dimensional for the model the model cannot take in bool so must cast to float image_distribution0. Building a CNN and reading the serialized images Model Definition Model Compilation Model Fitting Model Tuning Data Processing TF RecordsThe Tensorflow record is a format to store a sequence of binary record from large datasets. png We now need to read in the TF records. THe run length encoding would be 789 3 900 2 904 1 906 1. 7080Epoch 00007 saving model to kaggle working unet tpu. 7080Epoch 10 30119 119 ETA 0s dice_coeff 0. You will then be provided with the code snippet belowYou can now add a TPU by setting the accelerator to TPU v image. The first number refers to the starting pixel in 1D and the second number is the number of successive pixels that object is present on. scope block to ensure the TPU is used. Break down the image into smaller images tiling 2. com marcosnovaes hubmap 3 unet models with keras cpu gpu https www. 7080Epoch 28 30119 119 ETA 0s dice_coeff 0. Here the pixel locations label of an object are represented by two numbers. Look for all the tiff files in the training set. 7080Epoch 11 30119 119 ETA 0s dice_coeff 0. How to use a TPU for faster machine learning7. DataThe data set https www. The images are stained with Periodic acid Schiff stain histology tissue sections of the kidney. 7080Epoch 00019 saving model to kaggle working unet tpu. 7080Epoch 00028 saving model to kaggle working unet tpu. 7080Epoch 00026 saving model to kaggle working unet tpu. Image segmentation using Unet architecture and TFrecords Authors Muhammad Valiallah and Martin Page AbstractWe are presented with images of kidney tissue. Pick an image size to subsample the image The image tile chosen must have equal dimensions and be a multiple of the number of filters in the convolutional kernals. 7275Epoch 2 30119 119 ETA 0s dice_coeff 0. Add the features to a message6. Preparing the images for processinng Normalising Tiling Serialising 2. com metrics to evaluate your semantic segmentation model 6bcb99639aa2. 7080Epoch 00010 saving model to kaggle working unet tpu. Creat a function to decode the messages into images and masks3. The image formatsSteps 1. The basic architecture is UNet https www. com marcosnovaes hubmap unet keras model fit with tpu this formula adds epsilon to the numerator and denomincator to avoid a divide by 0 error in case a slice has no pixels set the relative values are important so this addition does not effect the coefficient defined as 1 minues the dice coefficient Convert strings to integers and subtract 1 because of python s 0 indexing add 0 to start and end so that first change will be 1s This gives us all changes in sequence we add 1 to get index of 1s we want every second change i. 7080Epoch 5 30119 119 ETA 0s dice_coeff 0. The contracting pathways reduce the spatial information while increasing feature information thus the resolution of the output is increased at each step the number of features is doubled. 7080Epoch 00005 saving model to kaggle working unet tpu. 7080Epoch 22 30119 119 ETA 0s dice_coeff 0. 7080Epoch 9 30119 119 ETA 0s dice_coeff 0. Num_tile_cols Parse the input tf. Print out the image shapes to see if there is consistency. Reference the GCS bucket1. Additional information is also available for each image such as demographic information. 7080Epoch 8 30119 119 ETA 0s dice_coeff 0. To convert images into TFrecord files we utilize the protocol tensorflow. 7080Epoch 15 30119 119 ETA 0s dice_coeff 0. h5119 119 169s 1s step dice_coeff 0. The training images come with an associated mask that identifies the areas of interest that can be accessed as in both an unencoded JSON form and as a run length encoded RLE form from a CSV file which stores a sequence of data in a single value. The model designed is very similar to the one in the paper https arxiv. The first step is to have a look at the image arrays to get an idea of 1. 7080Epoch 00021 saving model to kaggle working unet tpu. 7080Epoch 00016 saving model to kaggle working unet tpu. We define a pandas DataFrame to store the image names along with the number of tiles this is helpful in getting an idea of how many images can be trained on. How to construct a class of encoder decoder models UNet type model 6. 7080Epoch 13 30119 119 ETA 0s dice_coeff 0. Use the Secret Keys5. How to convert images into TensorFlow record files TFrecord to save on storage space as well as faster processing4. Create a Features message using tf. How to associate pixel lables masks to the image3. 7080Epoch 24 30119 119 ETA 0s dice_coeff 0. How to read a TFrecord5. Downsampling operations Conv 3x3 Relu Conv 3x3 Relu MaxPooling3. 7080Epoch 29 30119 119 ETA 0s dice_coeff 0. com c hubmap kidney segmentation where a structure in the kidney called the glomeruli cells and capillaries that facilitate the filtration of waste products 100 350 \u03bcm in diameter spherical shape needed to be identified in unlablled images on a pixel level. Create a feature dictionary which will be the contents of message. 7080Epoch 4 30119 119 ETA 0s dice_coeff 0. Uncompress the TFrecord and pass it to the decode function Reading the TF record We now have a function that reads a TFrecord and can be used to generate images and masks that can be passed to a CNN U Net ModelThe UNet architecture was proposed in 2015. As a general rule of thumb many smaller images are faster to train and process than one large image of the same disk size. The below information summarises this Epoch 1 30 119 Unknown 167s 1s step dice_coeff 0. The authors of the paper recommended initailising the kernal weights to have a Gaussian Normal Distribution due to the repeated Convultions and Pooling layers6. In order to use the TPU functionality on Kaggle the Tfrecords need to be uploaded to Google Cloud Storage. The task is to take the image files as well as the pixel location of the glomeruli referred to as masks and train a machine learning model to find the pixel locations of glomeruli on an unlabelled data set of kidney images. 7080Epoch 00004 saving model to kaggle working unet tpu. 7080Epoch 00017 saving model to kaggle working unet tpu. Create a bucket on Google Cloud Storage GCS 2. Read the images into arrays using the tifffile library. 7080Epoch 00022 saving model to kaggle working unet tpu. 7080Epoch 16 30119 119 ETA 0s dice_coeff 0. This is how we associate the image to the mask4. h5119 119 106s 888ms step dice_coeff 0. 7080Epoch 00014 saving model to kaggle working unet tpu. Login to your GCS account image. 7080Epoch 00012 saving model to kaggle working unet tpu. 7080Epoch 25 30119 119 ETA 0s dice_coeff 0. We did not use all the images as we the number of files was quite large and we wanted to first generate a training model and see what the performance is like before loading all the images. Convert the image masks into image arrays 0s and 1s of the same size as their associated images. com prvnkmr unet architecture breakdown. 7080Epoch 19 30119 119 ETA 0s dice_coeff 0. Link Notebook to GCS image. 7080Epoch 00009 saving model to kaggle working unet tpu. 7080Epoch 21 30119 119 ETA 0s dice_coeff 0. As can be seen from the above example the RLE will only be smaller than the original image if the object of interest is dense i. So for example if an image has an object with pixels labelled at 789 790 791 900 901 904 906. Create a dictionary mapping the features of the messages in the TFrecord to a label and specifying the data type2. 7080Epoch 12 30119 119 ETA 0s dice_coeff 0. 7080This plots a graph comparing training to validation AcknowlegementsThis workflow is based on the below sources mainly by reading other Kaggle Kernals in particular the Notebooks of the Hacking the Kidney coach Marcos Novaes notebook series. No padding in the layersWe now define the model within a with strategy. 7080Epoch 18 30119 119 ETA 0s dice_coeff 0. We now compile the modelWe then fit the model to the data set using a batch size of 128 and 30 epochs. 7080Epoch 00008 saving model to kaggle working unet tpu. 7080Epoch 00013 saving model to kaggle working unet tpu. 7080Epoch 27 30119 119 ETA 0s dice_coeff 0. We run the garbage collector to free up memory space after calling the function RLE decoder function This function is used to encode an image into its run length Generating a TFRecord A Tensorflow record file consists of serialised messages which is a dictionary of a feature label and its associated value. 7080Epoch 30 30119 119 ETA 0s dice_coeff 0. We now call the above function which will generate tfrecord files. This problem is an image segmentation type problem and can be solved using an encoder decoder architecture where a CNN is first downsampled to extract features and then subsequently symmetrically upsampled to reproduce the image and identify the pixels containing glomeruli. A major advantage of this format is the datasets that are too large to be stored fully in memory can be loaded in batches from the disk and processed see more https towardsdatascience. h5119 119 106s 887ms step dice_coeff 0. Upsampling operations Upsampling Conv 2x2 Concatenation Conv 3x3 Relu Conv 3x3 Relu 4. For this problem there is only 1 object for detection so glomeruli pixels so binary classification is used where 1 represents the presence of a glomeruli and 0 not a glomeruli. Parse the input tf. Create helper functions to cast datatypes into 1 of the type lists integer float and bytes 3. 7080Epoch 26 30119 119 ETA 0s dice_coeff 0. Convert the images into a TFrecord steps outlined later This code is used to read all the tiff image file paths in the training set and put it in a list Normalisation function We noticed that the colour channel sometimes occurred in the first column and sometimes in the third. Last Level Conv 1x1 2D softmax function which is the sigmoid function 5. net profile Alan_Jackson9 publication 323597886 figure fig2 AS 601386504957959 1520393124691 Convolutional neural network CNN architecture based on UNET Ronneberger et al. Here pixels are numbered in 1D first from top to bottom row wise then from left to right. 7080Epoch 00018 saving model to kaggle working unet tpu. 7275Epoch 00001 saving model to kaggle working unet tpu. Problem example An example of a image segmentation task is a recent Kaggle challenge https www. 7080Epoch 17 30119 119 ETA 0s dice_coeff 0. How to train a UNet type model Introduction Image segmentation is the process of separating a digital image into specific partitions by creating a pixel wise mask for an object s of interest. EncodingRun Length Encoding RLE is a lossless compression format. Objective of Notebook This notebook will walk you through a pipeline using TF records and the convolutional neural network U Net architecture for biomedical image segmentation also see this academic article https arxiv. The contracting path is a typical convolutional network with repeated application of convolutions with ReLU and max pooling activations downsampling operations. 7080Epoch 00020 saving model to kaggle working unet tpu. So the strategy is to first sub sample the large images into smaller images before passing them on to a CNN. It was based off a similar architecture called FCN but it has no dense layer. In order to use a TPU the datasets need to be stored on Google Cloud Storage Google Colab also has TPUs but TFrecords are not supported. png Run the code below to see ther tensorflow version as well as if the TPU is infact runningWe created tfrecords for all of the training image files. 7080Epoch 00002 saving model to kaggle working unet tpu. png To summarize the architecture 1. We first ran the training model without a validation dataset to get an idea of the training times. Pixels are labelled if they share a certain shared characteristic. Set the Accelarator to TPU on the Notebook4. Example proto using the dictionary above. 7080Epoch 20 30119 119 ETA 0s dice_coeff 0. Some images allso had leading dimensions of size 1 which we remove using the squeeze function. com max 858 1 yUd5ckecHjWZf6hGrdlwzA. png attachment image. 7080Epoch 00024 saving model to kaggle working unet tpu. The functional tissue unit FTU that needs to be identified is a block of cells around a capillary a 3D sphere. com c hubmap kidney segmentation data contains 8 training and 5 test images as TIFF files. Normalise Standardise the images so that they are in the same format. Convert the features into to bytes a process called serialization5. 7080Epoch 00015 saving model to kaggle working unet tpu. 7080Epoch 6 30119 119 ETA 0s dice_coeff 0. 7080Epoch 00006 saving model to kaggle working unet tpu. The expansive pathway combines the feature and spatial information through a sequence of up convolutions down sampling that halves the number of feature channels and concatenations with the high resolution features from the contracting pathway see more https www. 7080Epoch 23 30119 119 ETA 0s dice_coeff 0. h5119 119 105s 886ms step dice_coeff 0. 7080Epoch 00027 saving model to kaggle working unet tpu. 7080Epoch 00030 saving model to kaggle working unet tpu. 7080Epoch 00023 saving model to kaggle working unet tpu. h5119 119 105s 885ms step dice_coeff 0. Here is the formula Large frac 2 X Y X Y And here is its visualisation Dice https miro. ", "id": "muhammadvaliallah/image-segmentation-using-unet-tfrecords", "size": "19759", "language": "python", "html_url": "https://www.kaggle.com/code/muhammadvaliallah/image-segmentation-using-unet-tfrecords", "git_url": "https://www.kaggle.com/code/muhammadvaliallah/image-segmentation-using-unet-tfrecords", "script": "keras.engine.topology get_custom_objects skimage.transform tensorflow.keras.optimizers tensorflow.keras.layers serialiaze_images keras.layers.core UserSecretsClient tensorflow.keras.callbacks _parse_image_function2 unet_model EarlyStopping Dropout chain skimage.morphology compute_output_shape tensorflow.keras.models Adam itertools create_tf_record imread LayerNormalization Conv2D Layer seaborn numpy _bytes_feature Input _int64_feature imread_collection Conv2DTranspose convert_rle_to_image dice_coeff read_tf_dataset2 ModelCheckpoint keras.utils.generic_utils matplotlib.pyplot label call tensorflow RLE_ENCODER pandas Lambda dice_loss LayerNormalization(Layer) resize tqdm tensorflow.keras imshow load_model normalize UpSampling2D Model concatenate MaxPooling2D layers generate_tf_records backend backend as K _float_feature concatenate_images kaggle_secrets skimage.io ", "entities": "(('number', 'https more www'), 'combine') (('Here pixels', 'then right'), 'number') (('where CNN', 'glomeruli'), 'be') (('We', 'training times'), 'run') (('7080Epoch', 'unet kaggle working tpu'), 'saving') (('So strategy', 'CNN'), 'be') (('object', 'successive pixels'), 'refer') (('Model Definition Model Compilation Model Fitting Tuning Processing RecordsThe Tensorflow Data record', 'large datasets'), 'be') (('model', 'paper https arxiv'), 'be') (('png We', 'TF now records'), 'need') (('many smaller images', 'disk same size'), 'be') (('object', 'interest'), 'be') (('image tile', 'convolutional kernals'), 'pick') (('example', 'image segmentation task'), 'example') (('that', 'capillary'), 'be') (('number', 'features'), 'reduce') (('below information', 'Epoch'), 'summarise') (('graph', 'Kidney Marcos Novaes notebook Hacking series'), 'base') (('it', 'dense layer'), 'base') (('path encoder expansive which', 'thus shape'), 'use') (('where 1', 'glomeruli'), 'be') (('we', 'squeeze function'), 'have') (('we', 'protocol tensorflow'), 'utilize') (('authors', 'repeated Convultions'), 'recommend') (('how many images', 'idea'), 'be') (('they', 'certain shared characteristic'), 'label') (('model', 'so image_distribution0'), 'load') (('value ground true 2 times area', 'https more towardsdatascience'), 'use') (('How train', 'interest'), 'be') (('notebook', 'article https also academic arxiv'), 'Objective') (('7275Epoch', 'unet kaggle working tpu'), 'save') (('Link GCS', 'Kaggle Notebook Add ons'), 'drive') (('padding', 'strategy'), 'define') (('pixel locations Here label', 'two numbers'), 'represent') (('code snippet then belowYou', 'v image'), 'provide') (('ModelThe UNet architecture', '2015'), 'have') (('Tensor Processing it it', '8 cores'), 'use') (('we', 'convolution layers'), 'be') (('performance', 'images'), 'use') (('that', 'image'), 'create') (('which', 'tfrecord files'), 'call') (('png We', 'two general steps'), 'Libraries') (('Image segmentation', 'kidney tissue'), 'present') (('spherical capsules', 'glomeruli cells'), 'supplement') (('which', 'feature label'), 'run') (('colour channel', 'sometimes third'), 'use') (('test 5 TIFF', '8 training'), 'contain') (('how we', 'mask4'), 'be') (('task', 'kidney images'), 'be') (('that', 'https more towardsdatascience'), 'be') (('which', 'single value'), 'come') (('Tfrecords', 'Google Cloud Storage'), 'need') (('image', '904 789 790 791 900 901 906'), 'have') (('validation', 'training set'), 'access') (('Additional information', 'such demographic information'), 'be') (('they', 'same format'), 'Standardise') (('images', 'kidney'), 'stain') (('we', 'second change'), 'fit') (('first step', '1'), 'be') (('which', 'message'), 'create') (('7080Epoch', 'unet kaggle working tpu'), 'save') (('mask BytesList', 'EagerTensor'), '0s') (('which', 'TFrecord separate file'), 'create') (('that', 'pixel level'), 'segmentation') (('net publication', 'UNET Ronneberger et al'), 'profile') (('runningWe', 'training image files'), 'create') (('contracting path', 'operations'), 'be') (('TFrecords', 'also TPUs'), 'need') (('now modelWe', '128 epochs'), 'compile') ", "extra": "['test', 'bag']", "label": "Perfect_files", "potential_description_queries": ["accelerator", "account", "advantage", "application", "architecture", "area", "article", "associated", "basic", "batch", "binary", "block", "bool", "bottom", "call", "case", "challenge", "channel", "classification", "code", "coefficient", "column", "compare", "compile", "convert", "convolution", "convolutional", "data", "dataset", "decode", "decoder", "define", "detection", "dice", "dictionary", "disk", "distribution", "document", "drive", "effect", "encode", "encoder", "encoding", "end", "ensure", "epsilon", "equal", "error", "evaluate", "every", "extract", "faster", "feature", "figure", "file", "find", "fit", "float", "form", "format", "formula", "frac", "function", "functional", "general", "generate", "glomeruli", "gpu", "graph", "ground", "helper", "high", "idea", "image", "import", "index", "input", "integer", "interest", "kaggle", "kidney", "label", "labelled", "learning", "left", "length", "list", "look", "major", "mapping", "mask", "max", "memory", "message", "model", "multiple", "need", "network", "neural", "no", "not", "notebook", "number", "object", "order", "out", "output", "overlap", "package", "padding", "path", "pdf", "performance", "pipeline", "pixel", "png", "pooling", "present", "problem", "profile", "publication", "python", "read", "reading", "record", "reduce", "relative", "remove", "representation", "resolution", "row", "run", "sample", "sampling", "save", "saving", "scope", "second", "segmentation", "separate", "sequence", "set", "shape", "sigmoid", "similar", "single", "size", "slice", "softmax", "space", "spatial", "split", "stain", "start", "step", "storage", "store", "strategy", "string", "structure", "sub", "subtract", "summarize", "task", "tensorflow", "test", "tfrecord", "through", "thumb", "tiff", "tifffile", "tile", "tissue", "total", "train", "training", "type", "unit", "unpack", "up", "validation", "value", "version", "walk", "while", "wise", "workflow", "write"], "potential_description_queries_len": 188, "potential_script_queries": ["backend", "chain", "concatenate", "imread", "imshow", "io", "normalize", "numpy", "resize", "seaborn", "tqdm"], "potential_script_queries_len": 11, "potential_entities_queries": ["general", "image", "notebook", "separate"], "potential_entities_queries_len": 4, "potential_extra_queries": ["bag"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 197}