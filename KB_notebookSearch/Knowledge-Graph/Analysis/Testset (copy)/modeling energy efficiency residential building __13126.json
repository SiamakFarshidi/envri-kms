{"name": "modeling energy efficiency residential building ", "full_name": " h1 Introduction h1 1 Introduction h1 2 Data Preparation h1 A Data import h1 Variable s Information h1 B Spliting the data in X and Y h1 3 Fitting modeling h1 4 Models parameters tuning h1 A Decision Tree Regressor parameters turning h1 Parameters h1 B Tune Random Forests Parameters h1 C Gradient Boosting Regression Hyperparameter Tuning h1 D CatBoostRegressor h1 E And suprising MLPRegressor h1 3 CONCLUSION ", "stargazers_count": 0, "forks_count": 0, "description": "Just improved a little bit accuracy. Fitting modelingCreate a DataFrame to store computation results obtained with different models. min_samples_leaf The minimum number of samples each branch must have after splitting a node an integer between 1 and 1e6 inclusive. A small change in a training dataset may effect the model predictive accuracy. Wow very impressive with the GradientBoostingRegressor. Leaf is the end node of a decision tree. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. In this Notebook diferent models were applied for predicting heating and cooling loads of a building based on a dataset for building energy performance. RandomSearch GridSearchCV and Bayesian optimization are generally used to optimize hyperparameters. An important thing to remember is that we can t use the test set to tune parameters otherwise we ll overfit to the test set. A smaller leaf makes the model more prone to capturing noise in train data. There are several hyperparameters we need to tune and they are as follows Learning rate The learning rate is the weight that each tree has on the final prediction. Handling Categorical features automatically We can use CatBoost without any explicit pre processing to convert categories into numbers. Preventing Overfitting CatBoost provides a nice facility to prevent overfitting. This black box itself have a few levers we can play with. Subsample Subsample is the proportion of the sample to use. The model is based on decision rules extracted from the training data. Splitting the dataset into Training and Test set. Min samples split The minimum number of samples required to split an internal node Max depth Maximum depth of the individual regression estimators. There are a few attributes which have a direct impact on model training speed. Hence you need to strike the right balance and choose the optimal max_features. Data import Variable s Information Relative Compactness Surface Area m\u00b2 Wall Area m\u00b2 Roof Area m\u00b2 Overall Height m Orientation 2 North 3 East 4 South 5 West Glazing Area 0 10 25 40 of floor area Glazing Area Distribution Variance 1 Uniform 2 North 3 East 4 South 5 West Heating Load kWh Cooling Load kWhGood I m lucky since not have to process the NaN data. However this is not necessarily true as this decreases the diversity of individual tree which is the USP of random forest. Increasing max_features generally improves the performance of the model as at each node now we have a higher number of options to be considered. Parameter Search is usually known as grid search basically looking for parameter values that score the best. Decision Tree Regressor parameters turningDecision Tree algorithm has become one of the most used machine learning algorithm both in competitions like Kaggle as well as in business environment. And suprising MLPRegressorNow it s time to summary all our BEST models 3. In regression problem the model uses the value instead of class and mean squared error is used to for a decision accuracy. The more trees the more likely to overfit. Can be any integer up to 32. And try to check the correlation between variables. Boost comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library. Not easy to find out a clear correlation we tried to check the correlation matrix. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. Gradient Boosting Regression Hyperparameter TuningWhat we will do now is make an instance of the GradientBoostingRegressor. We will then take this grid and place it inside GridSearchCV function so that we can prepare to run our model. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session. IntroductionBuildings energy consumption is put away around 40 of total energy use. Therefore automation of hyperparameters tuning is important. Models parameters tuningBoosting machine learning algorithms are highly used because they give better accuracy over simple ones. So when the values vary a lot in an independent variable we use feature scaling so that all the values remain in the comparable range. Finding hyperparameters manually is tedious and computationally expensive. More sustainable consumption of energy can be ensured by a proper examination of the energy performance of buildings EPB and optimal designing of the HVAC system. You should choose as high value as your processor can handle because this makes your predictions stronger and more stable. min_sample_leaf If you have built a decision tree before you can appreciate the importance of minimum sample leaf size. It also reduces the need for extensive hyper parameter tuning and lower the chances of overfitting also which leads to more generalized models. A split that causes fewer remaining samples is discarded. Tune Random Forests ParametersRandom forest is an ensemble tool which takes a subset of observations and a subset of variables to build a decision trees. read_csv Input data files are available in the read only. With this tuning we can see that the mean squared error is lower than with the baseline model. In this sense some studies have focused on evaluating comfortable yet energy saving spaces. CONCLUSIONIn this Notebook building energy performance has been investigated using different models to predict Heating and Cooling loads We tried to learn how to tune different parameters in the models and obtained a very good prediction result 99. But for sure you decrease the speed of algorithm by increasing the max_features. Wow we are happy with this improvement then. There are some arguments that need to be set inside the GridSearchCV function such as estimator grid cv etc. Let s begin to process the data for fitting modeling. Now let s try to select some Regressors to check its performance. Performance of these algorithms depends on hyperparameters. Set variables target functions. Decision tree model is not good in generalization and sensitive to the changes in training data. Predicting heating and cooling loads of a building in the initial phase of the design to find out optimal solutions amongst different designs is very important as well as in the operating phase after the building has been finished for efficient energy. The maximum depth limits the number of nodes in the tree. We generally see a random forest as a black box which takes in input and gives out predictions without worrying too much about what calculations are going on the back end. 5 on both Heating and Cooling loads compared to the experimental data set. edu based on research by Tsanas and Xifara. CatBoost name comes from two words Category and Boosting. It yields state of the art results without extensive data training typically required by other machine learning methods andProvides powerful out of the box support for the more descriptive data formats that accompany many business problems. Higher number of trees give you better performance but makes your code slower. In this Notebook we calculate the best parameters for the model using GridSearchCV. max_leaf_nodes The maximum number of leaf nodes a tree in the forest can have an integer between 1 and 1e9 inclusive. According to the items mentioned above many engineers have tried to develop different predictive and evaluative tools the primary aim in order to produce an optimal approximation of building energy consumption. It can be an integer or one of the two following methods auto square root of the total number of predictors. To top it up it provides best in class accuracy. Input variables are relative compactness roof area overall height surface area glazing are a wall area glazing area distribution of a building orientation. However you should try multiple leaf sizes to find the most optimum for your use case. We can now move to the final step of taking these hyperparameter settings and see how they do on the dataset. There are primarily 3 features which can be tuned to improve the predictive power of the model Max_features These are the maximum number of features Random Forest is allowed to try in individual tree. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features. Required cooling and heating capacities are estimated mainly according to the basic factors including building properties its utilization and climate conditions. Each of these levers have some effect on either the performance of the model or the resource time balance. The model was trained and validated on 33 on the data set and the accuracy for the prediction test was 99. Number of estimators The number of estimators is show many trees to create. It affects the overall time of training the smaller the value the more iterations are required for training. If you are having problems with overfitting it would be a good idea to try this. n_estimators This is the number of trees you want to build before taking the maximum voting or averages of predictions. It can also return very good result with relatively less data unlike DL models that need to learn from a massive amount of data. Parameters max_features The number of randomly chosen features from which to pick the best feature to split on a given tree node. Feature scaling or data normalization is a method used to normalize the range of independent variables or features of data. I didn t see any improvement on this dataset though probably because there are so many train points it is more difficult to overfit on. This parameter tells the engine how many processors is it allowed to use. Decision Tree can be used both in classification and regression problem. IntroductionIn a building thermal energy involves two measures of cooling load CL and heating load HL and these measures are regulated by heating ventilation and air conditioning HVAC system. If you set iterations to be high the classifier will use many trees to build the final classifier and you risk overfitting. Parameters in random forest are either to increase the predictive power of the model or to make it easier to train the model. Some observations will be shown in the graphs bellow. Good values in the range 1 10. learning_rate used for reducing the gradient step. Data PreparationIn this Notebook we used the dataset taken from https cml. The HVAC system is designed to compute the HL and CL of the space and thereby provide a desirable indoor air condition. In this Notebook we just mention about the n_jobs parameter. Tune this parameter for best performance the best value depends on the interaction of the input variables. CatBoostRegressorCatBoost is a recently open sourced machine learning algorithm from Yandex. Spliting the data in X and YImporting necessary Libraries. Generally I prefer a minimum leaf size of more than 50. Now check the distribution of different variables. Although many countries take such measures there is still a high level of energy consumption and it is projected to increase globally. max_depth The maximum depth for growing each tree an integer between 1 and 100 inclusive. It can work with diverse data types to help solve a wide range of problems that businesses face today. iterations The maximum number of trees that can be built when solving machine learning problems. A value of 1 means there is no restriction whereas a value of 1 means it can only use one processor. Yes the accuracy of the model has been improved. CatBoost provides state of the art results and it is competitive with any leading machine learning algorithm on the performance front. It builds multiple such decision tree and amalgamate them together to get a more accurate and stable prediction. Although CatBoost has multiple parameters to tune and it contains parameters like the number of trees learning rate regularization tree depth fold size bagging temperature and others. We will create our grid with the various values for the hyperparameters. If you set use_best_model True and eval_metric Accuracy when initialising and then set eval_set to be a validation set then CatBoost won t use all the iterations it will return the iteration that gives the best accuracy on the eval set. max number of predictors. An optimal set of parameters can help to achieve higher accuracy. depth Depth of the tree. Output variables heating loads and cooling loads of the building. Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection recommendation items forecasting and it performs well also. As observed in the fitting calculation section we will try to tuning model parameters using the training data set of Cooling load or y2_train. This is similar to early stopping used in neural networks. ", "id": "winternguyen/modeling-energy-efficiency-residential-building", "size": "13126", "language": "python", "html_url": "https://www.kaggle.com/code/winternguyen/modeling-energy-efficiency-residential-building", "git_url": "https://www.kaggle.com/code/winternguyen/modeling-energy-efficiency-residential-building", "script": "sklearn.metrics randint as sp_randint randint matplotlib.style keras.layers sklearn.tree DecisionTreeRegressor train_test_split MultiOutputRegressor SVR KNeighborsRegressor Sequential sklearn.neural_network MinMaxScaler r2_score CatBoostRegressor seaborn numpy AdaBoostRegressor RandomForestRegressor GradientBoostingRegressor scipy.stats plotly.express AAD sklearn.ensemble sklearn.model_selection f1_score sklearn MLPRegressor matplotlib.pyplot Dense pandas accuracy_score GridSearchCV BaggingRegressor sklearn.neighbors sklearn.multioutput SVC sklearn.preprocessing catboost sklearn.svm roc_auc_score datasets keras.models ", "entities": "(('building', 'efficient energy'), 'be') (('sure you', 'max_features'), 'decrease') (('we', 'improvement'), 'be') (('Preventing', 'overfitting'), 'provide') (('that', 'business many problems'), 'yield') (('It', 'together more accurate prediction'), 'build') (('IntroductionBuildings energy consumption', 'energy total use'), 'put') (('observations', 'graphs bellow'), 'show') (('many engineers', 'energy consumption'), 'try') (('However you', 'use case'), 'try') (('studies', 'energy saving comfortable spaces'), 'focus') (('we', 'correlation matrix'), 'easy') (('compactness roof height surface area relative overall glazing', 'building orientation'), 'be') (('we', 'few levers'), 'have') (('many trees', 'estimators'), 'number') (('Now s', 'performance'), 'let') (('we', 'Cooling load'), 'try') (('tree', 'final prediction'), 'be') (('it', 'class best accuracy'), 'provide') (('Decision Tree', 'classification problem'), 'use') (('businesses', 'that'), 'work') (('accuracy', 'model'), 'improve') (('Data Notebook we', 'https cml'), 'PreparationIn') (('it', 'fraud detection recommendation items forecasting'), 'be') (('West Heating kWh Cooling 1 2 3 4 South 5 I', 'NaN m data'), 'Area') (('s', 'fitting modeling'), 'let') (('two measures', 'heating ventilation'), 'involve') (('We', 'hyperparameters'), 'create') (('Feature scaling normalization', 'data'), 'be') (('it', 'how many processors'), 'tell') (('model', 'training data'), 'base') (('accuracy', 'prediction test'), 'train') (('mean squared error', 'baseline model'), 'see') (('that', 'best'), 'know') (('otherwise we', 'test set'), 'be') (('heating Required cooling capacities', 'utilization conditions'), 'estimate') (('that', 'data'), 'return') (('Performance', 'hyperparameters'), 'depend') (('predictions', 'high value'), 'choose') (('Therefore automation', 'hyperparameters'), 'be') (('that', 'eval set'), 'set') (('a few which', 'model training speed'), 'be') (('you', 'predictions'), 'n_estimators') (('squared error', 'decision accuracy'), 'use') (('that', 'estimator such grid'), 'be') (('optimal set', 'higher accuracy'), 'help') (('that', 'machine learning when problems'), 'iteration') (('you', 'sample leaf minimum size'), 'min_sample_leaf') (('calculations', 'back end'), 'see') (('maximum depth', 'tree'), 'limit') (('model', 'train data'), 'make') (('t', 'outside current session'), 'list') (('Hence you', 'optimal max_features'), 'need') (('it', 'learning rate regularization tree depth size bagging temperature'), 'have') (('This', 'neural networks'), 'be') (('small change', 'model predictive accuracy'), 'effect') (('CatBoostRegressorCatBoost', 'machine recently open sourced learning Yandex'), 'be') (('we', 'just parameter'), 'mention') (('highly they', 'simple ones'), 'parameter') (('We', '99'), 'CONCLUSIONIn') (('turningDecision Tree algorithm', 'business as well environment'), 'parameter') (('HVAC system', 'air thereby desirable indoor condition'), 'design') (('best value', 'input variables'), 'depend') (('library', 'boosting gradient library'), 'come') (('are', 'individual tree'), 'be') (('which', 'random forest'), 'be') (('values', 'comparable range'), 'vary') (('Each', 'model'), 'have') (('tree', '1 inclusive'), 'max_leaf_nodes') (('branch', 'integer'), 'have') (('automatically We', 'numbers'), 'feature') (('MLPRegressorNow it', 'BEST models'), 's') (('now we', 'options'), 'have') (('Decision tree model', 'training data'), 'be') (('it', 'model'), 'be') (('it', 'energy still high consumption'), 'be') (('Min samples', 'Max regression Maximum individual estimators'), 'split') (('more iterations', 'training'), 'affect') (('it', 'only one processor'), 'value') (('how they', 'dataset'), 'move') (('we', 'model'), 'take') (('RandomSearch GridSearchCV optimization', 'generally hyperparameters'), 'use') (('Generally I', 'more than 50'), 'prefer') (('you', 'overfitting'), 'use') (('It', 'predictors'), 'be') (('CatBoost', 'categorical features'), 'convert') (('CatBoost name', 'two words'), 'come') (('read_csv Input data files', 'read'), 'be') (('ensemble which', 'decision trees'), 'be') (('it', 'performance front'), 'provide') (('best parameters', 'GridSearchCV'), 'calculate') (('train though probably so many it', 'dataset'), 'see') (('It', 'kaggle python Docker image https github'), 'come') (('More sustainable consumption', 'HVAC optimal system'), 'ensure') (('it', 'good this'), 'be') (('we', 'GradientBoostingRegressor'), 'make') (('code', 'better performance'), 'give') (('Subsample Subsample', 'sample'), 'be') (('also which', 'more generalized models'), 'reduce') ", "extra": "['biopsy of the greater curvature', 'test', 'bag']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "air", "algorithm", "area", "art", "auto", "bagging", "balance", "baseline", "basic", "become", "best", "bit", "boosting", "box", "branch", "build", "calculate", "calculation", "categorical", "check", "choose", "classification", "classifier", "clear", "code", "computation", "compute", "convert", "correlation", "create", "current", "cv", "data", "dataset", "decision", "depth", "detection", "develop", "directory", "distribution", "diversity", "effect", "end", "energy", "ensemble", "environment", "error", "estimator", "eval", "examination", "experimental", "face", "feature", "file", "final", "find", "fitting", "floor", "fold", "following", "forecasting", "forest", "function", "generalization", "gradient", "grid", "handle", "height", "help", "high", "hyperparameter", "idea", "image", "import", "importance", "improve", "improvement", "including", "increase", "individual", "input", "instance", "integer", "iteration", "itself", "kaggle", "leaf", "learn", "learning", "learning_rate", "let", "level", "library", "linear", "list", "little", "load", "looking", "lot", "lower", "max", "max_depth", "max_features", "maximum", "mean", "mention", "method", "minimum", "model", "most", "move", "multiple", "name", "need", "neural", "no", "node", "noise", "normalization", "normalize", "not", "number", "numerical", "open", "optimization", "optimize", "order", "out", "output", "overall", "overfit", "overfitting", "parameter", "performance", "place", "power", "pre", "predict", "prediction", "prepare", "prevent", "problem", "processing", "provide", "python", "random", "range", "read", "recommendation", "regression", "regularization", "relative", "research", "result", "return", "right", "risk", "run", "running", "sample", "saving", "scaling", "score", "search", "section", "select", "sense", "set", "several", "similar", "size", "space", "speed", "split", "splitting", "square", "squared", "state", "step", "store", "subset", "summary", "support", "surface", "system", "target", "test", "time", "tool", "total", "train", "training", "tree", "try", "tune", "tuning", "under", "up", "validation", "value", "variable", "version", "weight", "work", "write"], "potential_description_queries_len": 206, "potential_script_queries": ["catboost", "numpy", "randint", "seaborn", "sklearn"], "potential_script_queries_len": 5, "potential_entities_queries": ["bagging", "current", "depth", "energy", "height", "image", "leaf", "minimum", "overall", "recommendation", "regularization", "saving", "size", "surface", "training"], "potential_entities_queries_len": 15, "potential_extra_queries": ["bag", "biopsy"], "potential_extra_queries_len": 2, "all_components_potential_queries_len": 213}