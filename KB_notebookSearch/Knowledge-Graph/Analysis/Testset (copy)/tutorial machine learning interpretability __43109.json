{"name": "tutorial machine learning interpretability ", "full_name": " h1 Interpreting your Machine Learning Model Why and How h3 IMPORTANT NOTE h1 Load dependencies h1 Load the Red Wine Quality Dataset h2 Data Description h1 Quick EDA and Visualization h2 Univariate Analysis features and target quality h3 Features distributions h3 Target distribution h2 Multivariate Analysis h3 Correlation matrix h2 Bivariate Features vs Target h3 exemple alcohol vs quality plot h1 Building Train and Test Datasets h1 Training our classification model h1 Model Performance Evaluation h1 Making predictions on the test data and performance evaluation h2 Prediction h2 Evaluation of performance h3 Accuracy Precision Recall F1 Metrics Confusion matrix h3 AUC ROC Curve h1 The trade off Accuracy Interpretability h1 Default Model Interpretation Methods h2 Features importance h1 ELI5 Model Interpretation h2 Installation h2 Feature Importances h2 Explaining Model Prediction Decisions with ELI5 h3 Predicting when a particular wine quality will be 5 Low Quality h3 Predicting when a particular wine quality will be 5 Hight Quality h3 Features Permutation Importances h1 Partial Dependence Plots PD plot h2 Installation h2 Univariate PD Plots h2 Univariate ICE plot h3 Bivariate PD plot h1 SKATER Model Interpretation h2 Global Local Interpretations h2 Creating an interpretation object h2 Installation h2 Workflow Interpretation object in memory Model Interpretation h2 Feature Importances with Skater h2 Partial Dependence plots with SKATER h3 PD plot of pH affecting model prediction h3 Bivariate PD Plot showing interactions between features pH and fixed acidity and their effect on the quality classification h1 Local Interpretations with Skater h2 Local Interpretable Model Agnostic Explanations LIME h2 Explaining Model Predictions with Skater using LIME h3 Predicting when a particular wine quality will be 5 Low Quality h3 Predicting when a particular wine quality will be 5 Hight Quality h2 Tree Surrogates with SKATER h3 Using the interpreter instance invoke call to the TreeSurrogate h3 Using the surrogate model to learn the decision boundaries learned by the base estimator h3 Visualizing the Surrogate Tree h3 Interesting rules from the surrogate tree h3 Surrogate Model Performance Evaluation h1 Model Interpretation with SHAP h2 Installation h2 Explain predictions with SHAP h2 Feature Importances with SHAP h3 Predicting when a particular wine quality will be 5 Low Quality h3 Predicting when a particular wine quality will be 5 Hight Quality h2 Visualizing and explaining multiple predictions h2 SHAP Summary Plot h2 SHAP Dependence Plots h3 PD plot of pH and influence of fixed acidity affecting model prediction h1 Hands on FairML Bonus h1 Last but not least going further about MLI ", "stargazers_count": 0, "forks_count": 0, "description": "Partial Dependence Plots PD plot pd_plots Individual Conditional Expectation ICE plots ice_plots 3. Installation Workflow Interpretation object in memory Model Interpretation Feature Importances with SkaterFeature importance is generic term for the degree to which a predictive model relies on a particular feature. pdf Briefly Trepan constructs a decision tree in a best first manner. GradientExplainer Support TensorFlow and Keras models. html __ by Savvas Tjortjoglou A great online book about MLI __ Interpretable Machine Learning A Guide for Making Black Box Models Explainable https christophm. The Shapley value __ \u03d5_ ij __ is the average marginal contribution of feature value __ x_ ij __ by joining whatever features already entered the room before i. I don t understand why. Bivariate PD plotLet s now explore a bivariate PD plot between pH and the fixed acidity feature Going further a tutorial dedicated to PDPbox and ICE plots https towardsdatascience. The stored subset of training examples consists simply of those examples that reach the node. Typically its LimeTabularExplainer class helps in explaining predictions on tabular i. _chlorides_ the amount of salt in the wine. With each node in the queue Trepan stores a subset of the training examples another set of instances query instances a set of constraints. DeepExplainer DEEP SHAP Support TensorFlow and Keras models by using DeepLIFT and Shapley values. 7 chance that the wine will also be classified as Hight Quality one. com human interpretable machine learning part 1 the need and importance of model interpretation 2ed758f5f476 __ __ Model Interpretation Strategies https towardsdatascience. For numerical features it perturbs them by sampling from a Normal 0 1 and doing the inverse operation of mean centering and scaling according to the means and stds in the training data. Since XGBoost has some issues with feature name ordering when building models with dataframes so we will use our yet fitted xgb_array model to make LIME work without additional hassles of feature re ordering. This has to be a pandas dataframe with no missing data. If two vectors are orthogonal to one another then no linear transformation of one vector can produce the other. Ideally the score should be 0 for truthful explanation both globally and locally. Predicting when a particular wine quality will be 5 Low Quality We can see that this model has taken this particular decision 82 chance that this wine will be scored as Low Quality by puting forward the alcohol sulphates and volatile acidity features and we could know to which degree by looking at the associated values. Skater is a unified framework to enable Model Interpretation for all forms of models to help one build an Interpretable machine learning system often needed for real world use cases using a model agnostic approach. Such form of comprehensive evaluation helps in generating explanations which are locally faithful but may not align with the global behavior. com oracle Skater __ for more informations. _alcohol_ the percent alcohol content of the wine. Default Model Interpretation Methods__Non parametrics models__ like tree based models as XGBoost are more difficult to interprete because their total number of parameters is not fixed and will grow with the volume of data used for the training. Of course feel free to derive more interesting rules from this and also from your own models Let s look at how our surrogate model performs on the test dataset now. io interpretable ml book lime. _sulphates_ a wine additive which can contribute to sulfur dioxide gas S02 levels wich acts as an antimicrobial and antioxidant. com jphall663 awesome machine learning interpretability __ dedicated to the subject with plenty of ressources. Features Importance without and with permutation features_importance 2. Its novel components include the identification of a new class of additive feature importance measures and theoretical results showing there is a unique solution in this class with a set of desirable properties. com 2017 03 09 fairml auditing black box predictive models. Local interpretation algorithms which only deal with regions of the domain such as the marginal distribution of a feature. FairML only requires that it has a predict function. As we will see later Shapley values for exemple offer us such a consistent way. Making predictions on the test data and performance evaluation Prediction Evaluation of performance Accuracy Precision Recall F1 Metrics Confusion matrix AUC ROC CurveAUC ROC Curve is probably a better evaluations for this classification task So F1 0. More accuracy cannot stand alone for buiding trust on our models because of possible overfitting of the model correlations of the features and noise in the data. This is only used to infer output types and formats. So let s now make a quick exploration data analysis to see how the data are distributed and correlated. SHAP Summary PlotA SHAP value for a feature of a specific prediction represents how much the model prediction changes when we observe that feature. Local Interpretable Model Agnostic Explanations LIME LIME is an algorithm designed by Riberio Marco Singh Sameer Guestrin Carlos to access the behavior of the any base estimator model using interpretable surrogate models e. We recommend you to read the LIME chapter https christophm. com introducing pdpbox 2aa820afd312. ai __ Ideas on interpreting machine learning https www. Model Prediction Explanations with Local Interpretation LIME lime 4. After that treshold the influence is still positive but decreasing progressively and before that it was neutral very slightly negative. The default method used is prediction variance which is the mean absolute value of changes in predictions given perturbations in the data. orderings val text features. Extracting Thee Structured Representations of Thained Networks_ https papers. If a small change to an input feature dramatically changes the output the model is sensitive to the feature. Predicting when a particular wine quality will be 5 Hight Quality And in this one sulfate total sulfur dioxide and volatile acidity seems to be prevalent. Internally the Interpretation will generate a DataManager to handle data requests and sampling. We get a matrix of SHAP values with the same shape as the original X_test set. Global Local InterpretationsPredictive models maps an input space to an output space. cc paper 7062 a unified approach to interpreting model predictions for details. For exemple The first 90 test samples all probably are classified as Hight Quality wines and that have a hight alcohol degree 11. the SHAP value of that feature across many samples. Anyway the current preoccupation for this subject alone shows that ML has reached a sufficient maturity so that we human beings could now learn very interesting insights from the internal behavior of the very same ML AI models we produce. com cog data ML_Interpretability_tutorial blob master Machine_Learning_Interpretability_tutorial. Model Prediction Explanation with SHAP values shap 7. com adebayoj fairml __ is a new Python library that audits black box predictive models. Features Permutation ImportancesEli5 provides a way to compute feature importances for any black box estimator by measuring how score decreases when a feature is not available the method is also known as permutation importance or Mean Decrease Accuracy MDA. Operationalized Model If your model is accessible through an API use a DeployedModel which wraps the requests library. The visualization shows for exemple some interesting model prediction pattern decisions. com TeamHG Memex eli5 pull 261 pip install pdpbox Create the data that we will plot plot it plot the PD univariate plot for ICE plot we must specify the numbers of similarity clusters we want here 24 we use plot_type grid as the default and better option contour has a bug which is being corrected cf. Creating an interpretation objectThe general workflow within the skater package is to create an interpretation create a model and run interpretation algorithms. j The following figure from the KDD 18 paper _Consistent Individualized Feature Attribution for Tree Ensembles_ https arxiv. TreeExplainer Support XGBoost LightGBM CatBoost and scikit learn models by Tree SHAP. edu machine learning shavlik group craven. _volatile acidity_ the amount of acetic acid in wine which at too high of levels can lead to an unpleasant vinegar taste. Predicting when a particular wine quality will be 5 Low Quality In this successfully individual prediction the top 3 influential features seems to be after the bias the pH total sulfur dioxide and chlorides. For more information about the Shapley values explanations check out Christoph Molnar s book chapter on Shapley Values https christophm. The explain_instance function generates explanations for a prediction. SHAP provides multiple explainers for different kind of models. Typically this difference helps us in explaining why the model is inclined on predicting a specific class outcome. ELI5 Model InterpretationAccording to their documentation __ ELI5 https github. Fit a weighted interpretable surrogate model on the dataset with the variations. phi_ ij sum_ text All. 017 and sulphates 0. Load dependencies Load the Red Wine Quality DatasetThe Red Wine Quality dataset can be downloaded __ on this Kaggle page https www. First we generate neighborhood data by randomly perturbing features from the instance. There is two types of interpretation algorithms Global interpretation algorithms who offers statistics and metrics on the joint distribution of the entire training set which generally must reduce by aggregating or subseting the feature space to be human interpretable. html in Christoph Molnar s excellent book on Model Interpretation which talks about this in detail. _density_ the density of water is close to that of water depending on the percent alcohol and sugar content. Similarly in ML Interpretability there is the Accuracy Interpretability trade off rule of thumb which states that generally the more accurate is a model or a model of models like ensemble s bagging and boosting the more complex it is and so the more difficult it is to interpret it s outputs. SHAP Dependence PlotsSHAP dependence plots show the effect of a single or two feature across the whole dataset. and to come back here after if you wish to upvote it. Installation Feature ImportancesWith XGBoost ELI5 just use the same native feature importances computation methods with the default gain parameter which we have just seen earlier and give us an easy and ergonomic way of displaying it with the eli5. SHAP gives a nice reasoning below showing which features were the most influential in the model taking the correct decision of predicting that this wine will be scored as Low Quality. 012 NB the effect is much more visible with an unique definition class ie target_names in the preceding InMemoryModel call Local Interpretations with SkaterLocal Interpretation could possibly be achieved in two ways Firstly one could possibly approximate the behavior of a complex predictive model in the vicinity of a single input using a simple interpretable auxiliary or surrogate model e. io interpretable ml book pdp. _citric acid_ found in small quantities citric acid can add freshness and flavor to wines. The vertical dispersion of SHAP values at a single feature value is driven by interaction effects and another feature can be chosen for coloring to highlight possible interactions. 575 and total sulfur dioxide 90. to have a practical grasp on some of the main frameworks actually available for machine learning interpretability like __ELI5 LIME SKATER or SHAP__ and also PDPbox and FairML. What are the differences What can you infer about the reliability of these features Partial Dependence Plots PD plot The partial dependence plot PD plot shows the marginal effect one or two features have on the predicted outcome of a machine learning model J. KernelExplainer Kernel SHAP Applying to any models by using LIME and Shapley values. The basic idea behind FairML and many other attempts to audit or interpret model behavior is to measure a model s dependence on its inputs by changing them. They help us in identifying key features and meaningful representations even with large datasets with for exemple dimensionality reduction techniques PCA t SNE etc. html by the author for an exemple of use case. Sorry for the inconvenience. audit_model returns an overloaded dictionary where keys are the column names of input pandas dataframe and values are lists containing model dependence on that particular feature. Building Train and Test DatasetsNow for sake of simplicity let s transform our target data to a binary classification problem Low vs Hight quality wine and build our train and test datasets on a classical 70 30 ratio. SHAP dependence plots are similar to partial dependence plots but account for the interaction effects present in the features and are only defined in regions of the input space supported by data. edu ml datasets wine quality __. html for testing the installation python c from skater. The output formatter takes a requests. The intend of this tutorial is twofold to give a basic introduction to the subject looking at different strategies for tackling the potential black box problem in a model agnostic fashion. Target distribution Multivariate Analysis Correlation matrixWe can see that the alcohol level has the strongest postive correlation 0. I have chosen a small and clean dataset 1599 rows x 11 1 variables for quick and easy pre processing. _pH_ describes how acidic or basic a wine is on a scale from 0 very acidic to 14 very basic most wines are between 3 4 on the pH scale. _fixed acidity_ most acids involved with wine or fixed or nonvolatile do not evaporate readily. It should be obvious now that human must stay in the loop as machines don t have in fact any real semantic skills unless they are assigned by a human consensus and may be AI should better stand for Additive Intelligence. there is no data about grape types wine brand wine selling price etc. Out of the box skater allows models return numpy arrays and pandas dataframes. Notice how a feature like sulphates play an opposite influential role in explaining model prediction between the two classes and acts like a flag in these two particular examples. for pH we can confirm a behavioral shift of the model after 3. Proofs from game theory show this is the only possible consistent approach. PD plot of pH and influence of fixed acidity affecting model predictionWe can use the dependence_plot method for ploting the effect of a feature on the prediction with or without the influence of another feature. com questions 49402701 eli5 explaining prediction xgboost model. 3 that you can highlight by choosing in the left menu alcohol effects If you display the alcohol feature with the top drop down menu you can confirm that the alcohol level begin to play an important role in pushing upward the prediction at a level around 11. pdf summarizes this in a nice way https i. _ models are opinions embedded in mathematics Cathy O Neil mathematician data scientist and author of the famous Weapons of Math Destruction _ _ Science without conscience is but the ruin of the Soul Rabelais who first quoted the word automaton in french during the 16th century _Machine Learning Interpretability of is a hot and crucial subject machine learning algorithms are everywhere becoming more and more ubiquitous complex and efficient and sometimes treated like __black boxes__. Of course feel free to try other features. Building Interpretable Models with Surrogate Tree based Models SKATER skater_tree 6. Surrogate Model Performance Evaluation As expected the model performance drops but still we get an overall F1 score of 71 as compared to our preceding boosted model s score of 75. See FairML Auditing Black Box Predictive Models https blog. The implementation also generates a fidelity score to quantify tree based surrogate model s approximation to the Oracle. And that s a nice and interesting cybernetics and cognitive feedback loop. So we need more sophisticated metrics and analytics than just plain accuracy of the results. explore by yourself important features like sulphates and total sulfur dioxide. from PDPbox documentation. This intuition underlies the feature dependence measure. ICE algorithm gives the user insight into the several variants of conditional relationships estimated by the black box. They plot a feature s value vs. Further readings and conclusion further_readings We will work on a real world dataset about Red Wine Quality after all ethics can and should also be convivial with an XGBoost model which is a complex optimized distributed gradient boosting library providing a parallel tree boosting algorithm and the learning task will be a classical supervised classification with binarized classes. Installation Univariate PD PlotsLet s see how the feature alcohol behave with confidence interval This PD plot show us that the alcohol level seems to have an increasing positive influence on the prediction of Hight Quality wines for values between 9. We then learn locally weighted linear surrogate models on this neighborhood data to explain each of the classes in an interpretable way. On another end the AUC shows a more significant drop from 0. All feature values in the room participate in the game contribute to the prediction. com TeamHG Memex eli5 __ Explain Like I m 5 is a Python library which helps to debug machine learning classifiers and regressors and explain their predictions in an easy to understand an intuitive way. If you want more explanation about his meaning in Eli5 and in this context XGBoost model you can check out this Stackoverflow thread https stackoverflow. com intrepretable machine learning nfl combine. com uciml red wine quality cortez et al 2009 __ or via the __ UCI Machine Learning repository https archive. This is a good step in direction of model agnostic interpretation but not entirely model agnostic like we will see it later for LIME. io interpretable ml book __ by Christoph Molnar An article by qualified experts of the field Patrick Hall Al. Tree Surrogates with SKATERWe have see various ways to interpret machine learning models with features dependence plots and even LIME. Cap tain Obvious tip red color indicates that the factor highly contributes to the model prediction yeah i know what about the blue You can play with the different parameters like the include_interactions flag or with direct_input_pertubation_strategy referring to how to zero out a single variable with three different options constant zero replace with a random constant value constant median replace with median constant value random sample replace all values with a random permutation of the column Well is FairML itself really fair What do you think Last but not least going further about MLI. Reminder the last attribute ie quality is our target independant variable Quick EDA and VisualizationExploration data analysis and visualization are the first tools for interpreting and getting latent insights from data. ai __ 04 2019 v1 Christophe Rigon datacog free. We won t perform here any cross validation on our model as one should probably do in a real context. Model Interpretation with SHAP SHAP SHapley Additive exPlanations is a unified approach to explain the output of any machine learning model. An intuitive way to understand the Shapley value is the following The feature values enter a room in random order. Surrogate models are interpretable models like a linear model or decision tree that are learned on the predictions of the original black box model. Parametrics models ex logistic regression offers a contrario a first level of interpretation by the way of their coefficients but in fact even this is not completely trivial. response as an input and returns a numpy ndarray or pandas DataFrame. Features are sorted by the sum of the SHAP value magnitudes across all samples. ai course https mlcourse. Partial Dependence plots with SKATERPD plots can also be generated with the SKATER library PD plot of pH affecting model prediction There is a clear gap for pH 3. Predicting when a particular wine quality will be 5 Hight Quality We can again see that SKATER LIME gives a nice display showing which features were the most influential in the model taking the correct decision of predicting the wine quality score here for a Hight Quality wine. 2 chance that the wine will be classified as Hight Quality one. Here we will be using the dedicated __ PDPbox library https christophm. What about sulphates and total sulfur dioxide that we saw before Does it confirm the first hypotheses you made with the PD plot I think you would agree that it s definitely interesting to see how we can find out patterns in SHAP displays which lead us to have a better understanding why the model is making specific decisions and help us being able to provide explanations for them. If you don t give any interaction_index parameter SHAP will decide by itself and propose automatically an interaction feature for you. What can your a priori infer from their PD Plot Univariate ICE plotICE plots are similar to PD plots but offer a more detailled view about the behavior of near similar clusters around the PD plot average curve. A user can optionally pass data samples to the examples keyword argument. Linear Regressor Secondly one could use the base estimator to understand the behavior of a single prediction using intuitive approximate functions based on inputs and outputs. In that case regularization techniques like L1 regularization LASSO can be usefull to reduce the feature space and improving the interpretability of the model. You can compare this result with the basic feature classification ie without permutation. Feature Importances with SHAPThis basically takes the average of the SHAP value magnitudes across the dataset and plots it as a simple bar chart. 45 and sulphates 0. png Let s now dive into SHAP and leverage it for interpreting our model Installation Explain predictions with SHAPShapley value is the average contribution of features which are predicting in different situation. Following is a standard high level workflow for this Choose your instance of interest for which you want to have an explanation of the predictions of your black box model. Now we import the two key methods from fairml. For categorical features it perturbs by sampling according to the training distribution and making a binary feature that is 1 when the value is the same as the instance being explained. But what if the input attributes are correlated The trick used here to counter this multicollinearity is orthogonal projection. ELI5 does it by showing weights for each feature depicting how influential it might have been in contributing to the final prediction decision across all trees. SKATER Model InterpretationSKATER is a relative new MLI framework and documentation is not easily accessible for now. fr loading the csv dataset in a dataframe Let s visually check the first lines of our wine collection types of data Synthetic descriptive statistics make for security a copy of the original dataframe before further processing Extracting our target variable and creating a usefull feature list of dependant variables for visualizing correlations create our separate target vector mapping the target to a binary class at quality 5 quickly check that we have a balanced target partition building train test datasets on a 70 30 ratio ML in two lines make predictions for test data We design a simple classification evaluative function Evaluate predictions calculate the FPR and TPR for all thresholds of the classification ploting XGBoost default feature importances pip install eli5 we need to retrain a new model with arrays as eli5 has a bug with Dataframes and XGBoost cf. It is an open source python library designed to demystify the learned structures of a black box model both globally inference on the basis of a complete data set and locally inference about an individual prediction. Using the interpreter instance invoke call to the TreeSurrogate Using the surrogate model to learn the decision boundaries learned by the base estimator Reports the fidelity value when compared to the base estimator closer to 0 is better Learner uses F1 score as the default metric of choice for classification. i ve tried hard but couldn t fix it. It maintains a queue of leaves which are expanded into subtrees as they are removed from the queue. Predicting when a particular wine quality will be 5 Hight Quality Visualizing and explaining multiple predictionsSHAP can build beautiful interactive plots which can visualize and explain multiple predictions at once. 1 relative to the effect of this feature on the prediction of the probability of the wine being of Hight Quality Bivariate PD Plot showing interactions between features pH and fixed acidity and their effect on the quality classification Here if we look carefully we can see again the same effect but this time with an additional inhibitory negative effect the canyon in the middle of the feature sub space of the fixed acidy for values around 0. Explaining Model Prediction Decisions with ELI5To make random forest predictions more interpretable every prediction of the model can be presented as a sum of feature contributions plus the bias showing how the features lead to a particular prediction. Skater originally started off as a fork of LIME but then broke out as an independent framework of it s own with a wide variety of feature and capabilities for model agnostic interpretation for any black box models. fr __IMPORTANT NOTE__ I have a serious problem installing et running properlly here one of the main librariy of this tutorial SKATER. Each child is either made a leaf of the tree or put into the queue for future expansion. Univariate Analysis features and target quality Features distributions We can notice that some features like sulfur dioxide or sulphates for exemple have a right skewed distribution and should in a real context probably be engineered into their log forms for better results. Explain prediction by interpreting the local model. SHAP SHapley Additive exPlanations assigns each feature an importance value for a particular prediction. _residual suga_ the amount of sugar remaining after fermentation stops it s rare to find wines with less than 1 gram liter and wines with greater than 45 grams liter are considered sweet. _total sulfur dioxide_ amount of free and bound forms of S02 in low concentrations SO2 is mostly undetectable in wine but at free SO2 concentrations over 50 ppm SO2 becomes evident in the nose and taste of wine. 48 with the quality notation. Local Models to create a skater model based on a local function or method pass in the predict function to an InMemoryModel. So I invite you to look at it directly on my Github at __https github. com ideas ideas on interpreting machine learning __ For the more greedy the __ awesome machine learning interpretability repository https github. com SauceCat PDPbox issues 40 installation of skater can be tricky try pip install skater check out skater installation instructions at https oracle. Check out the __ the GitHub repository of SKATER https github. For Skater s implementation for building explainable surrogate models the base estimator Oracle could be any form of a supervised learning predictive model our black box model. So you d better use your _validation dataset_ if you have one for computing your features permutation importances. Due to privacy and logistic issues only physicochemical inputs and sensory the output variables are available e. Let s check this out in action __NOTE __ The implementation is currently experimental and might change in future. The intuition is that the more a model s decision criteria depend on a feature the more we ll see predictions change as a function of perturbing a feature. as indicated in the data description most wines are acidic and have a pH of 3 4 Bivariate Features vs Target exemple alcohol vs quality plot We can visualy confirm here the global positve correlation seen precedently between the alcohol level 14 and the final quality score. com ideas interpreting predictive models with skater unboxing model opacity and by an extensive and really great tutorial on MLI made by Dipanjan Sarkar https towardsdatascience. As future Data Scientist Machine Learning Engineers even if we clearly don t need interpretability all the time I think we have a social and professional ethical duty trying to design models as fair accountable and transparent as possible. Craven 1996 EXTRACTING COMPREHENSIBLE MODELS FROM TRAINED NEURAL NETWORKS_ http ftp. The SHAP values result from averaging over all possible orderings. Notice that when the scatter points don t fit on a line they pile up to show density and the color of each point represents the feature value of that individual. Further more interpreting our machine learning processes could give us valuable insights for debugging informing feature engineering model comparisons driving future data collection informing human decision making and generally for better communication and trust building. Orthogonal projection of vectors is important because it allows us to completely remove the linear dependence between attributes. The above explanation shows features each contributing to push the model output from the base value the average model output over the training dataset we passed to the actual model output. SHAP connects game theory with local explanations uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on what they claim do check out the SHAP NIPS paper http papers. Training our classification modelWe will now instanciate and train an out of the box XGBoost classification model on our train data Model Performance EvaluationNow let s test our model and evaluate how it has performed with its predictions on the test data. The constraint set describes the conditions that instances must satisfy in order to reach the node this information is used when drawing a set of query instances for a newly created node. It is interesting to confirm again that hight values of alcohol play positive important role in the final prediction and hight values of total sulfur dioxide seems a contrario to have a negative impact on predicting hight quality wines. com explainable artificial intelligence part 3 hands on machine learning model interpretation e8ebe5afc608 __ __Part 4 not published yet. Data Description Input variables based on physicochemical tests 1. Dependence and Interaction Plots SHAP shap_plots 8. For non parametric models fortunately many of them like XGBoost give an access to interpretation methods like feature importance for helping us to understand the inner evaluation of the model for making his predictions. This part is heavily inspired by this article https www. Typically an Interpretation consumes a dataset and optionally some metadata like feature names and row ids. This datasets is related to red variants of the Portuguese Vinho Verde wine. _quality_ score between 0 and 10 given by human wine tasters. A partial dependence plot can show whether the relationship between the target and a feature is linear monotonous or more complex. The project was started as a research idea to find ways to enable better interpretability preferably human interpretability to predictive black boxes both for researchers and practioners. __Tip__ Notice the drop down menus at the top of the graph with multiple display and ordering options and also on the left for isolating and highlighting multiple or individual features effects. Features importanceTo interpret a model we basically need to know which features are the most important in the model the effect of each feature on a particular prediction the effects of each feature over a large number of predictions Native global feature importance calculations that come with XGBoostare are based on the following parameters Feature Weights based on the number of times a feature appears in a tree across the ensemble of trees Coverage the average coverage number of samples affected of splits which use the feature Gain the average gain of splits which use the feature As you can see features are ordered in differents ways by these different criterions. Importantly for non linear functions the order in which features are introduced matters. show_prediction method. This is more reliable but this technique is computationally slow with a big number of features. There is also an understandable negative correlation between pH and the fixed acidity of the wine the pH is the mesure of acidity basicity with a scale between 0 very acid and 14 very basic with a midscale at 7 neutral the definition of pH is the negative log of H ion hydrogen ion so the more acidic is a solution the more the concentration of H is hight the more pH tends toward 0. 83 with the initial XGB model to now a weak 0. j cup x_ ij val text features. SHAP also enables us to use a density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output for individuals in the dataset. com learn machine learning explainability __ by Dan Becker with interactives exercices permutation importance partial plots Shap values A detailed and really great tutorial about MLI in 4 parts by Dipanjan DJ Sarkar __ The Importance of Human Interpretable Machine Learning https towardsdatascience. com explainable artificial intelligence part 2 model interpretation strategies 75d4afa6b739 __ __ Hands on Machine Learning Model Interpretation https towardsdatascience. zip call audit model print feature importance generate feature dependence plot Print it in a file. Craven described as the TREPAN algorithm. ELI5 is a good starting point and support tree based and parametric linear models and also text processing and HashingVectorizer utilities from scikit learn but doesn t support true model agnostic interpretations. __Tip __ As in our case If you don t have a separate held out dataset you can fit PermutationImportance on the same data as used for training this still allows to inspect the model but doesn t show which features are important for generalization. One advantage of FairML is that it can audit any classifier or regressor. __Congratulations to those who have made it so far and also to the others ___I hope that this tutorial has interested you and that it will be usefull and inspiring as an introduction to this important complex and very interesting topic. com adebayoj fairml archive master. 71 which cast a legitimate doubt on the validity of the preceding rules we extracted from the tree. SKATER model interpretation skater 5. SHAP has c implementations supporting XGBoost LightGBM CatBoost and scikit learn tree models. Here we visualize model prediction decisions for the first 1000 test data samples. They have already and will have in the future more and more impact on our society and our everyday s life. Interpreting your Machine Learning Model Why and How _A tutorial made during the_ __ MLcourse. Output variable based on sensory data 12. html So let s play with the Tree SHAP implementation integrated into XGBoost to explain the test dataset. After loading and providing a description of the data descrip_data a quick EDA eda and training evaluating our model model we will be exploring more in depth differents interpretation techniques like 1. SHAP values can explain the output of any machine learning model but for complex ensemble models it can be slow. The input formatter takes a pandas DataFrame or a numpy ndarray and returns an object such as a dict that can be converted to JSON to be posted. The process of expanding a node in Trepan is much like it is in conventional decision tree algorithms a splitting test is selected for the node and a child is created for each outcome of the test. Explaining Model Predictions with Skater using LIMESkater can leverage LIME to explain model predictions. We need a more consistent way of doing so. The explanations are approximated using Decision Trees both for Classification Regression by learning decision boundaries similar to that learned by the Oracle predictions from the base model are used for learning the Decision Tree representation. all_tests import run_tests run_tests beware this process is computationally slow heavy A reminder for referencing the originals feature names since these names are not kept in the surrogate tree Low Quality score 5 in red using our evaluation_scores function calculate the ROC AUC score for the tree surrogated model pip install shap load JS visualization code to notebook explain the model s predictions using SHAP values same syntax works for LightGBM CatBoost and scikit learn models pip install https github. For more details consult the reference __ Cortez et al. Here s some curated ressources about Machine Learning Interpretability A __ practical and knowledgeable tutorial on Kaggle Learn https www. The recent European GDRP General Data Protection Regulation resolution testifies to a citizen right to explanation of algorithmic decisions that significantly affect any individual. com explainable artificial intelligence part 2 model interpretation strategies 75d4afa6b739 which will also be referenced in the last Going further about MLI section at the end of this tutorial. _free sulfur dioxide_ the free form of SO2 exists in equilibrium between molecular SO2 as a dissolved gas and bisulfite ion it prevents microbial growth and the oxidation of wine. The skater feature importance implementation is based on an information theoretic criteria measuring the entropy in the change of predictions given a perturbation of a given feature. Basically LIME explanations are based on local surrogate models. But instead of trying to fit a global surrogate model LIME focuses on fitting local surrogate models to explain why single predictions were made. The query instances are used along with the training examples to select the splitting test if the node is an internal node or to determine the classlabel if it is a leaf. other optional parameters that control the mechanics of the auditing process for example number_of_runs number of iterations to perform interactions flag to enable checking model dependence on interactions. cc paper 1152 extracting tree structured representations of trained networks. NB you can understand BIAS here as the expected average score output by the model based on the distribution of the training set. linear classifier regressor. io interpretable ml book shapley. Predicting when a particular wine quality will be 5 Low Quality Reminder features pushing the prediction higher than the base value are shown in red those pushing the prediction lower are in blue. FairML orthogonally projects the input to measure the dependence of the predictive model on each attribute. Now let s dive into the interpretation of our model The trade off Accuracy InterpretabilityIn general machine learning design there is well known trade offs between the bias and the variance of a model or between precision and recall in classification algorithms for exemple. Meaning unbiasedness non discriminative giving reliable results and being able to be queried to validate predictive decisions. But can we build an approximation or a surrogate model which is more interpretable from a really complex black box model like our XGBoost model having hundreds of decision trees Here in we introduce the idea of using __ TreeSurrogates __ as means for explaining a model s learned decision policies for inductive learning tasks which is inspired by the work of Mark W. Let s examine individual data point predictions one for each class 0 quality wine scored 5 and a label of Hight Quality with the eli5. Visualizing the Surrogate TreeLet s visualize our surrogate tree with the Low Quality score 5 in red but before let s display a reminder of the features names Interesting rules from the surrogate treeHere are exemples of some interesting rules you can observe from the above tree If alcohol 10. Perturb your dataset and get the black box predictions for these new points. Weight the new samples by their proximity to the instance of interest. We recommend checking out the following excellent papers on the TREPAN algorithm to build surrogate trees. These lists of size number_of_runs. __ Another nice tutorial __ Interpretable Machine Learning with Python http savvastjortjoglou. audit_model takes required black box function which is the model to be audited required sample_data to be perturbed for querying the function. so who will explain the explainer __ A votre sant\u00e9 Cheers _ Christophe Rigon datacog free. Simply put while feature importance shows WHAT variables most affect predictions partial dependence plots show HOW a feature affects predictions. Typically SHAP values try to explain the output of a model function as a sum of the effects of each feature being introduced into a conditional expectation. Each row sums to the difference between the model output for that sample and the expected value of the model output which is stored as expected_value attribute of the explainer. Bonus auditing our black box predictive model with FairML fairml 9. Again you can play also with total sulfur dioxide and sulphates for exemple Hands on FairML Bonus __ FairML https github. 83 well not too bad without any tuning or feature engineering. DeployedModels require two functions an input formatter and an output formatter which speak to the requests library for posting and parsing. The above visualization can be interacted with in multiple ways. ", "id": "datacog314/tutorial-machine-learning-interpretability", "size": "43109", "language": "python", "html_url": "https://www.kaggle.com/code/datacog314/tutorial-machine-learning-interpretability", "git_url": "https://www.kaggle.com/code/datacog314/tutorial-machine-learning-interpretability", "script": "sklearn.metrics pdp train_test_split LimeTabularExplainer audit_model plot_dependencies Interpretation fairml PermutationImportance plot_pdp skater.tests.all_tests seaborn numpy auc XGBClassifier InMemoryModel info_plots show_in_notebook get_dataset sklearn.model_selection sklearn metrics matplotlib.pyplot pandas skater.util.dataops accuracy_score skater.core.explanations skater.model plot_importance skater.core.local_interpretation.lime.lime_tabular roc_curve evaluation_scores xgboost eli5.sklearn pdpbox run_tests; run_tests()\" ", "entities": "(('even this', 'fact'), 'offer') (('SHAP dependence Dependence plots', 'whole dataset'), 'show') (('Ideally score', 'truthful explanation'), 'be') (('Features', 'samples'), 'sort') (('data point individual predictions', 'eli5'), 'let') (('we', '3'), 'confirm') (('pdf', 'https i.'), 'summarize') (('where keys', 'particular feature'), 'return') (('More accuracy', 'data'), 'stand') (('eli5', 'Dataframes'), 'make') (('SO2', 'wine'), 'be') (('how surrogate model', 'test dataset'), 'feel') (('They', 't SNE etc'), 'help') (('we', 'ordering'), 'have') (('s', 'classical 70 30 ratio'), 'let') (('you', 'least further MLI'), 'indicate') (('you', 'Stackoverflow thread https stackoverflow'), 'check') (('one features', 'J.'), 'be') (('7 wine', 'Hight also Quality'), 'chance') (('LimeTabularExplainer Typically class', 'tabular i.'), 'help') (('which', 'different situation'), 'let') (('us', 'attributes'), 'be') (('plots partial Shap', '_ Human Machine Learning https Sarkar Interpretable towardsdatascience'), 'learn') (('we', 'tree'), '71') (('predictions', 'feature'), 'be') (('FairML', 'attribute'), 'project') (('SHAP dependence plots', 'data'), 'be') (('they', 'SHAP'), 'theory') (('model', 'feature'), 'be') (('infer', 'PD plot average curve'), 'be') (('which', 'detail'), 'book') (('it', 'still progressively that'), 'be') (('which', 'locally global behavior'), 'help') (('datasets', 'Vinho Verde Portuguese wine'), 'relate') (('Global Local InterpretationsPredictive models', 'output space'), 'map') (('Bivariate PD plotLet', 'acidity plots https fixed further PDPbox towardsdatascience'), 'explore') (('implementation', 'Oracle'), 'generate') (('one', 'model agnostic approach'), 'be') (('novel components', 'desirable properties'), 'include') (('Creating', 'interpretation algorithms'), 'be') (('Here curated ressources', 'Kaggle Learn https www'), 's') (('it', 'important complex very topic'), 'Congratulations') (('wine when particular quality', 'Hight sulfur 5 one sulfate total dioxide'), 'predict') (('X_test', 'original'), 'get') (('features', 'different criterions'), 'interpret') (('it', 'grams greater than 45 liter'), 'consider') (('we', 'model actual output'), 'feature') (('2 wine', 'Hight Quality'), 'chance') (('et', 'tutorial SKATER'), 'fr') (('ROC Curve', 'F1'), 'be') (('how features', 'particular prediction'), 'make') (('DeepExplainer DEEP', 'DeepLIFT values'), 'shap') (('Tree Surrogates', 'features dependence plots'), 'see') (('com ideas', 'https Dipanjan Sarkar towardsdatascience'), 'interpret') (('You', 'permutation'), 'compare') (('pdf Briefly Trepan', 'best first manner'), 'construct') (('It', 'locally individual prediction'), 'be') (('most acids', 'wine'), 'evaporate') (('Python new that', 'black box predictive models'), 'be') (('relationship', 'target'), 'show') (('SHAP', 'tree models'), 'have') (('why single predictions', 'fitting local surrogate models'), 'focus') (('We', 'probably real context'), 'win') (('They', 'society'), 'have') (('non discriminative', 'predictive decisions'), 'mean') (('how feature', 'two particular examples'), 'notice') (('alcohol level', '11'), '3') (('which', 'feature generally space'), 'be') (('machine learning Further more processes', 'generally better communication'), 'give') (('instance', 'binary feature'), 'perturb') (('only it', 'predict function'), 'require') (('that', 'node'), 'consist') (('visualization', 'model prediction pattern interesting decisions'), 'show') (('more technique', 'features'), 'be') (('Basically LIME explanations', 'local surrogate models'), 'base') (('you', 'alcohol'), 'visualize') (('intelligence model interpretation com explainable artificial part 2 75d4afa6b739 which', 'tutorial'), 'strategy') (('I', 'pre quick processing'), 'choose') (('that', 'significantly individual'), 'testify') (('it', 'internal classlabel'), 'use') (('features', 'better results'), 'feature') (('us', 'predictions'), 'give') (('it', 'wine'), '_') (('implementation', 'currently future'), 'let') (('output which', 'posting'), 'require') (('predictionWe', 'feature'), 'use') (('top 3 influential features', 'bias'), 'predict') (('it', 'bagging'), 'be') (('visualy', 'alcohol precedently level'), 'be') (('which', 'explainer'), 'sum') (('child', 'test'), 'be') (('it', 'classifier'), 'be') (('Here we', 'test data first 1000 samples'), 'visualize') (('Learner', 'classification'), 'invoke') (('AI', 'Additive better Intelligence'), 'be') (('_ _ ELI5 https', 'ELI5 Model documentation'), 'InterpretationAccording') (('density', 'percent alcohol'), 'be') (('_ chlorides', 'wine'), '_') (('Model Interpretation', 'machine learning model'), 'be') (('subject hot learning algorithms', '_ everywhere more ubiquitous sometimes _ black boxes'), 'be') (('which', 'requests'), 'Model') (('ICE algorithm', 'black box'), 'give') (('_ nice tutorial', 'savvastjortjoglou'), '_') (('each', 'particular prediction'), 'feature') (('zip call audit model print feature importance', 'file'), 'generate') (('target independant variable data Quick EDA analysis', 'data'), 'be') (('base estimator', 'supervised learning predictive model'), 'be') (('explain_instance function', 'prediction'), 'generate') (('acidity volatile we', 'associated values'), 'predict') (('decision that', 'box original black model'), 'be') (('child', 'future expansion'), 'make') (('So I', '_ _ https github'), 'invite') (('Model Prediction Explanations', 'Local Interpretation LIME'), 'lime') (('more pH', '0'), 'be') (('carefully we', '0'), 'relative') (('Feature Importances', 'bar simple chart'), 'take') (('still we', '75'), 'Evaluation') (('So s', 'test dataset'), 'html') (('trade', 'exemple'), 'let') (('explanations', 'Decision Tree representation'), 'approximate') (('Firstly one', 'simple interpretable auxiliary'), 'achieve') (('We', 'more consistent way'), 'need') (('We', 'surrogate trees'), 'recommend') (('we', 'eli5'), 'ImportancesWith') (('give', 'you'), 'decide') (('that', 'JSON'), 'take') (('mean absolute value', 'data'), 'be') (('when we', 'feature'), 'represent') (('learning task', 'classical supervised binarized classes'), 'reading') (('doesn t', 'model true agnostic interpretations'), 'be') (('Linear Secondly one', 'inputs'), 'Regressor') (('SHAP values', 'possible orderings'), 'result') (('feature values', 'random order'), 'be') (('wine', 'Low Quality'), 'give') (('Interpretable Model Agnostic Explanations LIME Local LIME', 'models interpretable surrogate e.'), 'be') (('_ _ _ _ Part', 'intelligence machine learning model com explainable artificial 3 interpretation'), 'part') (('feature values', 'prediction'), 'participate') (('then linear transformation', 'other'), 'be') (('Now we', 'fairml'), 'import') (('Typically values', 'conditional expectation'), 'SHAP') (('this', 'game theory'), 'show') (('pip install skater', 'https oracle'), 'com') (('We', 'interpretable way'), 'learn') (('set', 'constraints'), 'with') (('Red Wine Quality DatasetThe Red Wine Quality dataset', '_ Kaggle page https _ www'), 'Load') (('interpretation Local which', 'feature'), 'algorithm') (('KernelExplainer Kernel', 'LIME values'), 'shap') (('AUC', '0'), 'show') (('us', 'them'), 'confirm') (('above visualization', 'multiple ways'), 'interact') (('why model', 'class specific outcome'), 'help') (('Dependence Partial plots', 'clear pH'), 'be') (('information', 'newly created node'), 'describe') (('features', 'generalization'), '_') (('method', 'permutation also importance'), 'provide') (('alcohol level', '9'), 'see') (('_', 'ftp'), 'comprehensible') (('it', 'complex ensemble models'), 'explain') (('that', 'alcohol hight degree'), 'classify') (('feature', 'possible interactions'), 'drive') (('alcohol level', 'strongest postive correlation'), 'see') (('basic idea', 'them'), 'be') (('which', 'Hight Quality here wine'), 'see') (('you', 'features permutation importances'), 'use') (('feature importance skater implementation', 'given feature'), 'base') (('Internally Interpretation', 'data requests'), 'generate') (('Shapley later values', 'consistent way'), 'offer') (('_ citric acid _', 'wines'), 'find') (('feature', 'dataset'), 'enable') (('_ _ _ ij _ features', 'i.'), 'be') (('This', 'pandas missing data'), 'have') (('models', 'numpy arrays'), 'allow') (('again hight values', 'quality hight wines'), 'be') (('project', 'researchers'), 'start') (('part', 'article https heavily www'), 'inspire') (('it', 'training data'), 'perturb') (('we', 'later LIME'), 'be') (('So we', 'results'), 'need') (('which', 'multiple predictions'), 'build') (('_ votre sant\u00e9 Cheers _ Christophe Rigon datacog', 'explainer'), 'explain') (('more total number', 'training'), '_') (('Model Prediction Explanation', 'SHAP values'), 'shap') (('we', 'fair accountable'), 'think') (('which', 'option better bug'), 'pull') (('We', 'LIME chapter https christophm'), 'recommend') (('XGBoost CatBoost', 'Tree SHAP'), 'Support') (('NB you', 'training set'), 'understand') (('_ Tip _ _ Notice', 'isolating effects'), '_') (('regularization techniques', 'model'), 'be') (('pip', 'https github'), 'import') (('predictive model', 'particular feature'), 'be') (('user', 'examples'), 'pass') (('which', 'Mark W.'), 'build') (('they', 'individual'), 'notice') (('other optional that', 'interactions'), 'parameter') (('This', 'output only types'), 'use') (('Python 5 which', 'intuitive way'), 'com') (('which', 'function'), 'take') (('First we', 'instance'), 'generate') (('Craven', 'TREPAN algorithm'), 'describe') (('back here you', 'it'), 'come') (('you', 'box black model'), 'be') (('how it', 'test data'), 'instanciate') (('Explaining', 'model predictions'), 'leverage') (('HOW feature', 'predictions'), 'put') (('SHAP', 'models'), 'provide') (('Again you', 'FairML Bonus _ _ FairML https github'), 'play') (('we', '1'), 'explore') (('wich', 'antimicrobial'), 'sulphate') (('those', 'blue'), 'be') (('only physicochemical inputs', 'output variables'), 'be') (('which', 'unpleasant vinegar taste'), '_') (('how data', 'exploration data now quick analysis'), 'let') (('how wine', 'pH between 3 scale'), 'describe') (('they', 'queue'), 'maintain') (('trick', 'here multicollinearity'), 'be') (('intuition', 'feature dependence measure'), 'underlie') (('how it', 'trees'), 'do') (('Skater', 'box black models'), 'start') (('Typically Interpretation', 'feature optionally names'), 'consume') (('Here we', '_ _ PDPbox library https dedicated christophm'), 'use') (('we', 'ML AI very same models'), 'show') (('intend', 'model agnostic fashion'), 'be') (('features', 'matters'), 'order') ", "extra": "['biopsy of the greater curvature', 'outcome', 'test', 'bag']", "label": "Perfect_files", "potential_description_queries": ["absolute", "account", "accuracy", "advantage", "algorithm", "approach", "article", "associated", "attribute", "author", "average", "bagging", "basic", "behavior", "best", "binary", "blob", "book", "boosting", "box", "build", "calculate", "call", "case", "categorical", "check", "checking", "child", "choice", "classification", "classifier", "clean", "clear", "close", "code", "collection", "color", "column", "compare", "computation", "compute", "conclusion", "confidence", "content", "context", "contour", "control", "copy", "correct", "correlation", "correlations", "could", "course", "create", "criteria", "csv", "current", "data", "dataframe", "dataset", "decision", "default", "degree", "depend", "depth", "derive", "description", "dict", "dictionary", "difference", "dimensionality", "direction", "directly", "display", "distributed", "distribution", "domain", "drop", "eda", "effect", "eli5", "enable", "end", "engineering", "ensemble", "entropy", "estimator", "evaluate", "evaluation", "even", "every", "ex", "expected", "experimental", "explore", "fact", "factor", "feature", "feedback", "field", "figure", "final", "find", "fit", "fitting", "fix", "fixed", "following", "forest", "form", "forward", "found", "framework", "function", "future", "game", "gap", "general", "generate", "generated", "generic", "gradient", "graph", "grid", "group", "grow", "growth", "handle", "help", "high", "highlight", "hope", "hot", "http", "human", "idea", "implementation", "import", "importance", "include", "individual", "inference", "influence", "inner", "input", "instance", "interactive", "interest", "interpretability", "interpretation", "interval", "intuition", "io", "itself", "kept", "key", "label", "lead", "leaf", "learn", "learning", "least", "left", "let", "level", "library", "line", "linear", "list", "load", "local", "log", "look", "looking", "loop", "lower", "main", "mapping", "matrix", "mean", "meaning", "measure", "median", "memory", "metadata", "method", "metric", "middle", "might", "missing", "ml", "model", "most", "multicollinearity", "multiple", "my", "name", "near", "need", "negative", "neighborhood", "new", "no", "node", "noise", "non", "not", "notebook", "number", "numerical", "numpy", "object", "opacity", "open", "operation", "option", "order", "ordered", "out", "outcome", "output", "overall", "overfitting", "package", "page", "parallel", "parameter", "part", "partial", "pattern", "pdf", "pdpbox", "percent", "perform", "performance", "permutation", "plot", "png", "point", "positive", "potential", "pre", "precision", "predict", "prediction", "present", "price", "print", "priori", "probability", "problem", "processing", "project", "projection", "provide", "python", "query", "random", "rare", "ratio", "re", "read", "recall", "recommend", "reduce", "reference", "regression", "regularization", "relationship", "relative", "remove", "replace", "repository", "research", "resolution", "response", "result", "return", "right", "role", "room", "row", "run", "running", "sample", "sampling", "scale", "scaling", "scatter", "scikit", "score", "section", "select", "selected", "separate", "set", "several", "shape", "shift", "similar", "similarity", "simplicity", "single", "size", "skater", "society", "solution", "source", "space", "splitting", "standard", "step", "sub", "subject", "subset", "sum", "supervised", "support", "system", "tabular", "target", "task", "technique", "term", "test", "testing", "text", "theory", "think", "those", "through", "thumb", "time", "total", "train", "training", "transform", "transformation", "tree", "try", "tuning", "tutorial", "understanding", "unique", "up", "user", "val", "validate", "validation", "value", "variable", "variance", "vector", "vertical", "view", "visualization", "visualize", "volume", "while", "who", "wine", "word", "work", "workflow", "world", "xgboost", "zip"], "potential_description_queries_len": 367, "potential_script_queries": ["auc", "core", "pdp", "seaborn", "sklearn", "util"], "potential_script_queries_len": 6, "potential_entities_queries": ["average", "call", "ensemble", "fixed", "importance", "interpretation", "library", "local", "model", "page", "partial", "permutation", "point", "print", "skater", "total"], "potential_entities_queries_len": 16, "potential_extra_queries": ["bag", "biopsy"], "potential_extra_queries_len": 2, "all_components_potential_queries_len": 373}