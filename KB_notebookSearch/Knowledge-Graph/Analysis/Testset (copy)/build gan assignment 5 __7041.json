{"name": "build gan assignment 5 ", "full_name": " h1 Controllable Generation h3 Goals h3 Learning Objectives h2 Getting started h4 CelebA h4 Packages and Visualization h4 Generator and Noise h4 Classifier h2 Specifying Parameters h2 Train a Classifier Optional h2 Loading the Pretrained Models h2 Training h4 Update Noise h4 Generation h2 Entanglement and Regularization ", "stargazers_count": 0, "forks_count": 0, "description": "Feel free to skip this code block and if you do want to train your own classifier it is recommended that you initially go through the assignment with the provided classifier Loading the Pretrained ModelsYou will then load the pretrained generator and classifier using the following code. This is because some features are entangled. norm https pytorch. Alternatively even if the generator can produce an image with the intended features it might require many intermediate changes to get there and may get stuck in a local minimum. This process may change features which the classifier was not trained to recognize since there is no way to penalize them with this method. The classifier has the same archicture as the earlier critic remember that the discriminator critic is simply a classifier used to classify real and fake. If you trained your own classifier you can load that one here instead. Remember the equation for gradient ascent new old old weight. 3 Multiply the mean of the example norms by the penalty weight. alt text https drive. However in case you would like to train your own classifier the code for that has been provided as well. Make sure to negate the value since it s a penalty 4 Take the mean of the current classifications for the target feature over all the examples. Calculating the magnitude of the change requires you to take the norm of the difference between the classifications not the difference of the norms. However in this assignment you are interested in maximize your feature using gradient ascent since many features in the dataset are not present much more often than they re present and you are trying to add a feature to the images not remove. It may fail more often at producing the target feature when compared to the original approach. In the code given to you here you can generate smiling faces. This makes sense For example it may not be able to generate a face that s smiling but whose mouth is NOT slightly open. If you d like to use this just run train_classifier filename to train and save a classifier on the label indices to that filename. Their formulas are essentially the same however instead of subtracting the weighted value stochastic gradient ascent adds it it can be calculated by new old old weight where is the gradient of old. This mean will be your target_score. CelebA is a dataset of annotated celebrity images. One way you can implement this is by penalizing the differences from the original class with L2 regularization. The greater this value is the more features outside the target have changed. The higher the score the better 2. Whether it s possible to train models to avoid changing unsupervised features is an open question. If you d like to do this you ll have to download it and run it ideally using a GPU train_classifier filename Downlaod the prtrained models from Google Drive UNQ_C1 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION calculate_updated_noise UNIT TEST Check that the basic function works Check that it works for generated images First generate a bunch of images with the generator Number of gradient steps to take Number of gradient steps to skip in the visualization Feel free to change this value to any string from feature_names UNQ_C2 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_score Steps 1 Calculate the change between the original and current classifications as a tensor by indexing into the other_indices you re trying to preserve like in x features. Calculate the norm magnitude of changes per example and multiply by penalty weight Take the mean of the current classifications for the target feature UNIT TEST Feel free to change this value to any string from feature_names from earlier. CelebAFor this notebook instead of the MNIST dataset you will be using CelebA http mmlab. Finally add this penalty to the target score. Feel free to change the target index and control some of the other features in the list You will notice that some features are easier to detect and control than others. This L2 regularization would apply a penalty for this difference using the L2 norm and this would just be an additional term on the loss function. You have also been provided with the generator noise and classifier code from earlier assignments. Since they are colored not black and white the images have three channels for red green and blue RGB. Getting started You will start off by importing useful libraries and packages and defining a visualization function. By training a classifier to recognize a relevant feature you can use it to change the generator s inputs z vectors to make it generate images with more or less of that feature. You do this by performing stochastic gradient ascent. If you wanted to reduce the amount of the feature you would perform gradient descent. GenerationNow you can use the classifier along with stochastic gradient ascent to make noise that generates more of a certain feature. The target score is the mean of the target class in the current noise. Gradient ascent is gradient descent over the negative of the value being optimized. This may also expose a limitation of the generator. Set for our testing purposes please do not change Build the neural network You can run this code to train your own classifier but there is a provided pretrained one. Entanglement and RegularizationYou may also notice that sometimes more features than just the target feature change. You perform stochastic gradient ascent to try and maximize the amount of the feature you want. com uc id 1xn6LXNdQHia4Av31qx_TDP3r5QkC_cTe Packages and Visualization Generator and Noise Classifier Specifying ParametersBefore you begin training you need to specify a few parameters z_dim the dimension of the noise vector batch_size the number of images per forward backward pass device the device type Train a Classifier Optional You re welcome to train your own classifier with this code but you are provided with a pretrained one later in the code. You will calculate the magnitude of the change take the mean and negate it. This suggests that the model may not be able to generate an image that has the target feature without changing the other features. Here you ll have to implement the score function the higher the better. Optional hint for calculate_updated_noise1. TrainingNow you can start implementing a method for controlling your GAN Update NoiseFor training you need to write the code to update the noise to produce more of your desired feature. This will be your other_class_penalty. You will be started you off with a pre trained generator and classifier so that you can focus on the controllability aspects. Controllable Generation GoalsIn this notebook you re going to implement a GAN controllability method using gradients from a classifier. In the following block of code you will run the gradient ascent with this new score function. Target all the classes so that s how many the classifier will learn classifier_val_losses Dataloader returns the batches Calculate the gradients Update the weights Keep track of the average classifier loss Uncomment the last line to train your own classfier this line will not work in Coursera. You might notice a few things after running it 1. You want to calculate the loss per image so you ll need to pass a dim argument to torch. For every non target class take the difference between the current noise and the old noise. org docs stable generated torch. 2 Calculate the norm magnitude of changes per example. You use stochastic gradient ascent to find the local maxima as opposed to stochastic gradient descent which finds the local minima. If you wanted to control another feature you would need to get data that is labeled with that feature and train a classifier on that feature. Optional hints for get_score1. Observe how controllability can change a generator s output. To fix this you can try to isolate the target feature more by holding the classes outside of the target class constant. Resolve some of the challenges that entangled features pose to controllability. The score is calculated by adding the target score and a penalty note that the penalty is meant to lower the score so it should have a negative value. Learning Objectives1. The list you have here are the features labeled in CelebA which you used to train your classifier. Given the noise with its gradient already calculated through the classifier you want to return the new noise vector. ", "id": "amoghjrules/build-gan-assignment-5", "size": "7041", "language": "python", "html_url": "https://www.kaggle.com/code/amoghjrules/build-gan-assignment-5", "git_url": "https://www.kaggle.com/code/amoghjrules/build-gan-assignment-5", "script": "torch.utils.data __init__ calculate_updated_noise torch make_grid tqdm.auto show_tensor_images DataLoader forward train_classifier make_classifier_block seaborn CelebA torchvision nn matplotlib.pyplot torchvision.datasets get_score get_noise tqdm Generator(nn.Module) transforms torchvision.utils make_gen_block Classifier(nn.Module) ", "entities": "(('it', 'feature'), 'input') (('that', 'other features'), 'suggest') (('target score', 'current noise'), 'be') (('You', 'it'), 'notice') (('mouth', 'face'), 'make') (('you', 'target class'), 'try') (('how controllability', 'generator s output'), 'observe') (('you', 'torch'), 'want') (('it', 'examples'), 'make') (('that', 'controllability'), 'resolve') (('s', 'unsupervised features'), 'be') (('you', 'one'), 'load') (('which', 'local minima'), 'use') (('it', 'where old'), 'be') (('you', 'following code'), 'load') (('You', 'gradient stochastic ascent'), 'do') (('code', 'that'), 'like') (('more features', 'target'), 'be') (('that', 'certain feature'), 'use') (('black images', 'red green'), 'have') (('you', 'gradient descent'), 'want') (('here you', 'smiling faces'), 'generate') (('you', 'controllability aspects'), 'start') (('features', 'others'), 'notice') (('Gradient ascent', 'value'), 'be') (('you', 'x features'), 'have') (('discriminator critic', 'simply real'), 'remember') (('You', 'classifier earlier assignments'), 'provide') (('you', 'noise new vector'), 'want') (('you', 'mmlab'), 'dataset') (('you', 'filename'), 'run') (('It', 'when original approach'), 'fail') (('CelebA', 'celebrity annotated images'), 'be') (('this', 'loss just additional function'), 'apply') (('You', 'it'), 'calculate') (('classifier', 'method'), 'change') (('TEST', 'feature_names'), 'calculate') (('you', 'later code'), 'com') (('Entanglement', 'target feature also sometimes more just change'), 'notice') (('it', 'there local minimum'), 'require') (('that', 'feature'), 'want') (('You', 'visualization function'), 'start') (('it', 'negative value'), 'calculate') (('you', 'desired feature'), 'start') (('you', 'classifier'), 'be') (('you', 'images'), 'be') (('Calculating', 'norms'), 'require') (('Controllable Generation you', 'classifier'), 'GoalsIn') (('you', 'feature'), 'perform') (('You', 'own classifier'), 'change') (('you', 'L2 regularization'), 'be') (('Here you', 'score function'), 'have') (('you', 'score new function'), 'run') (('line', 'Coursera'), 'target') (('This', 'generator'), 'expose') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["apply", "argument", "assignment", "average", "backward", "basic", "batch_size", "block", "calculate", "case", "classifier", "classify", "code", "colored", "control", "current", "data", "dataset", "detect", "device", "difference", "dim", "dimension", "download", "equation", "even", "every", "face", "fail", "feature", "filename", "find", "fix", "following", "forward", "function", "generate", "generated", "generator", "gradient", "green", "http", "https pytorch", "id", "image", "implement", "index", "initially", "label", "labeled", "learn", "line", "list", "load", "local", "lower", "magnitude", "mean", "method", "might", "model", "mouth", "need", "negative", "network", "neural", "new", "no", "noise", "non", "norm", "not", "notebook", "number", "open", "per", "perform", "performing", "pre", "present", "pretrained", "re", "reduce", "regularization", "return", "run", "running", "save", "score", "sense", "start", "string", "target", "tensor", "term", "testing", "text", "through", "track", "train", "training", "try", "type", "update", "value", "vector", "visualization", "weight", "work", "write"], "potential_description_queries_len": 110, "potential_script_queries": ["nn", "seaborn", "torch", "torchvision", "tqdm"], "potential_script_queries_len": 5, "potential_entities_queries": ["local"], "potential_entities_queries_len": 1, "potential_extra_queries": ["test"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 115}