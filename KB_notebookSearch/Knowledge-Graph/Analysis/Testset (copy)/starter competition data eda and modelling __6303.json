{"name": "starter competition data eda and modelling ", "full_name": " h2 1 Understanding the Comptetion h3 Competition Objective h2 2 Understanding the Data h4 Columns h2 3 EDA and Data Prepataion h3 3 1 EDA h3 Observations h3 3 1 1 Duplicate Id s and dataset labels h4 Note As we can see this 170113f9 399c 489e ab53 2faf5c64c5bc Id is mentioning multiple datasets So for each id in test we ll need to predict all posible datasets used h3 3 1 2 Duplicate pub title and dataset label h4 Note As we observed in the above artifact there are publication titles using multiple datasets h3 3 1 3 Multiple publications having same title h3 3 1 4 Dataset titles and labels h3 3 2 Data Preparation h3 3 2 1 WordCloud of publication titles h3 3 2 2 WordCloud of most frequent words in the texts h2 4 Baseline Model h3 Hypothesis building h3 4 1 Preparing test set h3 4 2 Let s check this hypothesis on training data To check if it s even worth to use this h3 4 3 Making submission file h4 We can see that string matching gives 100 accuracy on train set On submission as well it will probably give a good score This model can definetely serve as a baseline h4 Note Accuracy isn t the actual evaluation metric The actual evaluation metric is Jaccard similatity base FBeta 0 5 score I have prepared this Notebook that implements the evaluation metric and it also evaluates the baseline on actual metric h4 If you found it useful please consider appreciating it by an UPVOTE Thanks ", "stargazers_count": 0, "forks_count": 0, "description": "Description In this competition we need to develop an algorithm to automate the discovery of how scientific data are referenced in publications. Understanding the DataList of data file provided as inputWe have a train. Meaning that there are some datasets that has multiple labels. In any publication the authors mentions the names of the datasets that are used in their work. Well interesting 4 There 45 dataset titles but 130 dataet labels. We can use these details to generate some features for model building. 2 WordCloud of most frequent words in the texts 4. Meaning that there are cases when two different publications from two different authors have same title. For this we need a list of possible datasets and we can get it from the training set. This is just a baseline hypothesis. Thanks print print nDescription of numerical variables desc_df_num df list num_df column_name. BUT this isn t what the competition demands. 2 Let s check this hypothesis on training data To check if it s even worth to use this Let s check the accuracySuperb This hypothesis gives 100 accuracy on training set. I have prepared this Notebook https www. head row_limit A text cleaning function. EDA and Data Prepataion 3. So using this id column we can have any information about the publication. So by simple string matching we can find out whether a dataset is mentioned in a publication or not. If you found it useful please consider appreciating it by an UPVOTE. This type of automation will be very useful in showing what datasets are used in a particular type of publications or the reverse what are the potential usages of a datset. Now let s look at train. Duplicate pub_title and dataset label Note As we observed in the above artifact there are publication titles using multiple datasets. Look into this Evaluation Process Jaccard FBeta https www. The unlabelled dataset test set will be used for evaluation of the algorithm. Baseline Model Hypothesis building Instead of directly jumping into models like BERT XLNet GPT 3 let s think simple here. com pashupatigupta ci how score is calculated jaccard fbeta notebook for a detailed explanation of evaluation process. Let s look into the folders first then we ll look into files. csv file that I believe has the labels information. So for each id in test we ll need to predict all posible datasets used. 1 Preparing test set 4. Let s what are the sections in a paper Woah So we have each and every detail of a publication available in a json format. edu sites oerc themes oerc images projects coleridge. Let s look into the data to understand the data and the comptetion better. Competition ObjectiveOne liner We are required to build an algorithm that can find our what are the datasets that a publications uses. We have some json files in both the directories. 3 Making submission file We can see that string matching gives 100 accuracy on train set. On submission as well it will probably give a good score. So instead of inferering from the publication which datasets are used we ll be finding out if a particular dataset is used in publication or not. Note Accuracy isn t the actual evaluation metric. We have with us the full text of scientific publications from numerous research areas we ll identify data sets that the publications authors used in their work. Columns id publication id note that there are multiple rows for some training documents indicating multiple mentioned datasets pub_title title of the publication a small number of publications have the same title dataset_title the title of the dataset that is mentioned within the publication dataset_label a portion of the text that indicates the dataset cleaned_label the dataset_label as passed through the clean_text function from the Evaluation pageSo we have id publication_title and cleaned_label columns. That s why that id is repeating. 2 Same is the case with pub_title. We have a labelled dataset train set that we ll use to develop our algorithm. The id column is same as the json filenames. A single publication is using mutiple datasets. 3 There is NO one to one mapping of id and pub_title. 2 Data PreparationWe ll read the text of a publication from the json file and put it in the train dataframeNow that we have the text content of each publication let s do some wordcloud analysis. It s also important to understand the evaluation process of this competition because it is little different. This model can definetely serve as a baseline. The actual evaluation metric is Jaccard similatity base FBeta 0. We ll look into how these two are related. 1 EDA Observations 1 There are duplicate id s meaning that there are some pulications that are using mutiple datasets. Multiple publications having same titleThere is NO one to one mapping of id and pub_title. Dataset titles and labelsA single dataset can have multiple labels. Duplicate Id s and dataset labels Note As we can see this 170113f9 399c 489e ab53 2faf5c64c5bc Id is mentioning multiple datasets. com pashupatigupta ci how score is calculated jaccard fbeta that implements the evaluation metric and it also evaluates the baseline on actual metric. rename columns index column_name display desc_df_num. Let s look into the files to find out what are they Well these json files are full text version of publication. We also have train and test folders. This is about the json files. png Background The Coleridge Initiative is a not for profit organization originally established at New York University that is working with governments to ensure that data are more effectively used for public decision making. Understanding the Comptetion ci https oerc. 1 WordCloud of publication titles 3. ", "id": "pashupatigupta/starter-competition-data-eda-and-modelling", "size": "6303", "language": "python", "html_url": "https://www.kaggle.com/code/pashupatigupta/starter-competition-data-eda-and-modelling", "git_url": "https://www.kaggle.com/code/pashupatigupta/starter-competition-data-eda-and-modelling", "script": "plotly.graph_objects basic_eda get_text seaborn numpy matplotlib.pyplot WordCloud pandas wordcloud clean_text STOPWORDS ", "entities": "(('hypothesis', 'training set'), 'let') (('json files', 'text full publication'), 'let') (('datasets', 'potential datset'), 'be') (('we', 'posible datasets'), 'need') (('that', 'work'), 'mention') (('themes oerc images', 'coleridge'), 'oerc') (('particular dataset', 'publication'), 'use') (('Multiple publications', 'd'), 'be') (('we', 'algorithm'), 'have') (('when two different publications', 'same title'), 'mean') (('we', 'publication multiple datasets'), 'pub_title') (('s', 'wordcloud analysis'), 'read') (('it', 'actual metric'), 'ci') (('as well it', 'probably good score'), 'give') (('it', 'competition'), 's') (('we', 'Evaluation pageSo'), '-PRON-') (('dataset', 'publication'), 'find') (('So we', 'json available format'), 'let') (('I 2faf5c64c5bc d', 'multiple datasets'), 's') (('We', 'directories'), 'have') (('d we', 'publication'), 'have') (('it', 'UPVOTE'), 'find') (('Thanks print', 'print numerical variables'), 'nDescription') (('string matching', 'train set'), 'see') (('competition', 'isn what'), 't') (('XLNet 3 s', 'BERT'), 'let') (('Understanding', 'train'), 'have') (('we', 'training set'), 'need') (('Dataset', 'labelsA single multiple labels'), 'have') (('com pashupatigupta how score', 'evaluation process'), 'ci') (('first then we', 'files'), 'look') (('publications authors', 'work'), 'have') (('data', 'decision more effectively public making'), 'Background') (('that', 'multiple labels'), 'mean') (('model', 'definetely baseline'), 'serve') (('I', 'labels information'), 'file') (('single publication', 'mutiple datasets'), 'use') (('EDA 1 1 that', 'mutiple datasets'), 'observation') (('We', 'model building'), 'use') (('s', 'data'), 'let') (('dataset test unlabelled set', 'algorithm'), 'use') (('how scientific data', 'publications'), 'description') (('publications', 'that'), 'require') ", "extra": "['organization', 'test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "algorithm", "artifact", "baseline", "build", "case", "check", "cleaning", "column", "competition", "consider", "content", "csv", "data", "dataset", "decision", "detail", "develop", "df", "directly", "display", "duplicate", "ensure", "evaluation", "even", "every", "file", "find", "found", "frequent", "function", "generate", "head", "id", "index", "jaccard", "json", "label", "labelled", "let", "list", "little", "look", "mapping", "matching", "meaning", "metric", "model", "most", "multiple", "need", "not", "notebook", "number", "numerical", "organization", "out", "png", "potential", "predict", "print", "public", "publication", "read", "research", "reverse", "scientific", "score", "set", "single", "string", "submission", "test", "text", "think", "through", "title", "train", "training", "type", "version"], "potential_description_queries_len": 81, "potential_script_queries": ["numpy", "plotly", "seaborn"], "potential_script_queries_len": 3, "potential_entities_queries": ["numerical", "scientific", "test"], "potential_entities_queries_len": 3, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 84}