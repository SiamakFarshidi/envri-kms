{"name": "breast cancer prediction 99 ", "full_name": " h1 Data Set Information h2 Attribute Information h1 1 import library h1 2 load and analys data h4 from the first look in our data description we can see that h2 A finding missing values h1 3 feautres selection h4 Diagnosis h2 A correlation map h4 this feautres had a corralation valus 0 07 with the target columns h1 4 data vizualisation h2 Plotting many distributions h2 A diagnosis h2 B concave points worst h2 B concavity mean h2 3 perimeter worst h1 5 machine learning application h2 A split data h2 B Feature Scaling h3 a Logistic Regression h3 b KNN h3 c Support Vector Machines h3 d DecisionTreeClassifier h3 e RANDOM FOREST CLASSIFCATION h3 f ANN neural network h3 g xgboost h3 h catboost ", "stargazers_count": 0, "forks_count": 0, "description": "Unnamed 32 columns is an empty column A finding missing values like we c our data is clean exept the last columns that is empty so we gonna drop it 3. This database is also available through the UW CS ftp server ftp ftp. data vizualisation Plotting many distributions A diagnosis B concave points_worst B concavity_meanWe can see there are some outliers. The actual linear program used to obtain the separating plane in the 3 dimensional space is that described in K. Relevant features were selected using an exhaustive search in the space of 1 4 features and 1 3 separating planes. They describe characteristics of the cell nuclei present in the image. 07 with the target columns fractal_dimension_mean texture_se smoothness_se symmetry_se fractal_dimension_se 4. A few of the images can be found at Web Link Separating plane described above was obtained using Multisurface Method Tree MSM T K. educd math prog cpo dataset machine learn WDBC Attribute Information 1. B benign 0 A correlation map this feautres had a corralation valus 0. com products 9781783980284 graphics 3a298fcc 54fb 42c2 a212 52823e709e30. B benign is the most frequent value in our target columns2. Lets remove them 120 5 machine learning application A split data B Feature Scaling a Logistic Regression b KNN c Support Vector Machines d DecisionTreeClassifier e RANDOM FOREST CLASSIFCATION f ANN neural network g xgboost h catboost manipulation data visualiation data default theme drop the id columns transformation of type of the target value to numerical drop this columns Feature Selection Making Confusion Matrix and calculating accuracy score Fit the model Confusion Matrix accuracy score Finding the optimum number of neighbors Training the K Nearest Neighbor Classifier on the Training set Predicting the Test set results Making the confusion matrix and calculating accuracy score Training the Support Vector Classifier on the Training set Predicting the test set results Making the confusion matrix and calculating accuracy score Finding the optimum number of max_leaf_nodes Training the Decision Tree Classifier on the Training set Predicting the test set results Making the confusion matrix and calculating accuracy score Finding the optimum number of n_estimators Training the RandomForest Classifier on the Training set Predicting the test set results Making the confusion matrix and calculating the accuracy score Initialising the ANN Adding the input layer and the first hidden layer Adding the second hidden layer Adding the third hidden layer Adding the fourth hidden layer Adding the output layer Compiling the ANN Training the ANN on the training set Predicting the test set results Making the confusion matrix calculating accuracy_score confusion matrix accuracy Making the confusion matrix and calculating the accuracy score Making the confusion matrix and calculating the accuracy score. Data Set Information Features are computed from a digitized image of a fine needle aspirate FNA of a breast mass. M malignant 12. feautres selection Diagnosis 1. 97 101 1992 a classification method which uses linear programming to construct a decision tree. Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society pp. Bennett Decision Tree Construction Via Linear Programming. 3 32 Ten real valued features are computed for each cell nucleus radius mean of distances from center to points on the perimeter texture standard deviation of gray scale values perimeter area smoothness local variation in radius lengths compactness perimeter 2 area 1. 0 concavity severity of concave portions of the contour concave points number of concave portions of the contour symmetry fractal dimension coastline approximation 1 https static. Diagnosis M malignant B benign 3. load and analys data like we see all our feautres are numirical values exept the target value diagnosis M malignant B benign we had 569 Rows and 33 columns small data from the first look in our data description we can see that 1. 5 3 perimeter_worstWe can see there are 2 outliers. Mangasarian Robust Linear Programming Discrimination of Two Linearly Inseparable Sets Optimization Methods and Software 1 1992 23 34. ", "id": "midouazerty/breast-cancer-prediction-99", "size": "3103", "language": "python", "html_url": "https://www.kaggle.com/code/midouazerty/breast-cancer-prediction-99", "git_url": "https://www.kaggle.com/code/midouazerty/breast-cancer-prediction-99", "script": "sklearn.metrics sklearn.tree KNeighborsClassifier plotly.graph_objects DecisionTreeClassifier seaborn numpy CatBoostClassifier XGBClassifier ExtraTreesClassifier plotly.express sklearn.ensemble sklearn sklearn.model_selection metrics confusion_matrix RandomForestClassifier matplotlib.pyplot tensorflow pandas StandardScaler LogisticRegression accuracy_score sklearn.neighbors SVC sklearn.linear_model sklearn.preprocessing catboost sklearn.svm xgboost train_test_split preprocessing ", "entities": "(('database', 'UW CS ftp server ftp also ftp'), 'be') (('confusion matrix', 'accuracy score'), 'remove') (('Relevant features', '1 4 features'), 'select') (('we', '1'), 'load') (('diagnosis B', 'data many distributions'), 'vizualisation') (('so we', 'it'), 'be') (('They', 'present image'), 'describe') (('Data Set Information Features', 'breast mass'), 'compute') (('Web Link Separating plane', 'Multisurface Method Tree MSM T above K.'), 'find') (('B benign', 'most frequent target'), 'be') (('that', 'K.'), 'use') (('3 32 Ten real valued features', 'radius lengths'), 'compute') (('B correlation feautres', 'corralation'), 'benign') (('classification 97 101 method which', 'decision tree'), '1992') (('educd math prog cpo dataset machine', 'WDBC Attribute Information'), 'learn') ", "extra": "['biopsy of the greater curvature', 'test', 'diagnosis']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "application", "area", "breast", "catboost", "cell", "center", "classification", "clean", "column", "confusion", "contour", "correlation", "data", "database", "dataset", "decision", "default", "describe", "description", "diagnosis", "dimension", "drop", "empty", "found", "frequent", "gray", "id", "image", "input", "layer", "learn", "learning", "linear", "load", "local", "look", "malignant", "map", "math", "matrix", "mean", "method", "missing", "model", "most", "network", "neural", "nuclei", "number", "numerical", "output", "plane", "present", "remove", "scale", "score", "search", "second", "selected", "selection", "set", "space", "split", "standard", "target", "test", "through", "training", "transformation", "type", "value", "variation", "xgboost"], "potential_description_queries_len": 74, "potential_script_queries": ["numpy", "preprocessing", "seaborn", "sklearn", "tensorflow"], "potential_script_queries_len": 5, "potential_entities_queries": ["dataset", "math"], "potential_entities_queries_len": 2, "potential_extra_queries": ["biopsy"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 79}