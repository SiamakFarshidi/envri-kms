{"name": "tutorial dcgan with tensorflow create fake images ", "full_name": " h3 Special Note Due to Out of memory limits in Kaggle I could NOT run this Notebook inside Kaggle with any larger dataset than 1000 images Hence the resultant fake images generated at the end will almost be like noise But this Notebook is mostly written as a Tutorial to show the flow of DCGAN h2 Link to my Youtube Video Explaining this whole Notebook h1 Facial Attribute prediction h3 Getting to know CELEB A Dataset h4 Data Files h2 The Facial Attributes of CELEB A Dataset h3 A quick tip in this regard if you are using local machine to handle this dataset challenge h1 Fundamental way GAN Works h3 The basic objective function of a vanilla GAN model is the following h1 Regular Neural Network vs CNN h4 Convolutional Neural Networks have three important architectural features h1 Deep Convolutional GANs DCGANs h2 Generator Architecture h2 The architecture of a discriminator h3 Several tricks had to be applied to make DCGANs stable and useful h1 How Reverse ConvNets works in DCGAN h3 Scaling images h1 Transposed Convolutions h1 UpSampling2D vs Conv2DTranspose in Keras h1 Generator h4 I use Leaky relu activations in the hidden layer neurons and sigmoids for the output layers Originally ReLU was recommend for use in the generator model and LeakyReLU was recommended for use in the discriminator model although more recently the LeakyReLU is recommended in both models h2 Use of Batch Normalization h3 For understanding parameters look at this blog h3 Lets see how our generator is creating images from noise Lets generate a sample image with the below code h1 Discriminator h1 DCGAN Combining Generator and Discriminator h2 Setting discriminator trainable to False h1 Final DCGAN Training h3 Final Training Architecture of a DCGAN h2 DCGAN is super sensitive h3 Since we are training two models at once the discriminator and the generator we can t rely on Keras fit function Instead we have to manually loop through each epoch and fit the models on batches ", "stargazers_count": 0, "forks_count": 0, "description": "trainable to False will only affect the copy of the discriminator in the combined model. Batch normalization is used after the activation of convolution in the discriminator modeland after transpose convolutional layers in generator model. Batch normalization limits the amount by which updating theparameters in the previous layers can affect the distribution ofinputs received by the current layer. On the contrary the discriminator tries to become better at distinguishing samples coming from G from samples coming from the real distribution. The assumption made here is that the input data has spatial significance or in the example of computer vision the relationship between two distant pixels is probably less significant than two close neighbors. Since we are training two models at once the discriminator and the generator we can t rely on Keras. The standard process to train a DCGAN network is to first train the discriminator on the batch of samples. To load an image from a file we use the open function in the Image module passing it the path to the image. The generator network will generate fake samplesthat we use to train our discriminator network. Discriminator the discriminator evaluate the authenticity of provided images it classifies the images from the generator and the original image. Originally ReLU was recommend for use in the generator model and LeakyReLU was recommended for use in the discriminator model although more recently the LeakyReLU is recommended in both models. As training progresses the generated images will look increasingly real just like original celeb a images. We do this by first sampling some random noise from a random uniform distribution then getting the generator s predictions on the noise. png The basic objective function of a vanilla GAN model is the following Imgur https imgur. glob full_image_train_path image_path_50k img_list 0 500 print len image_path_50k For the Image. csv Recommended partitioning of images into training validation testing sets. Convolutional Neural Networks CNNs are neural networks with architectural constraints to reduce computational complexity and ensure translational invariance the network interprets input patterns the same regardless of translation in terms of image recognition a banana is a banana regardless of where it is in the image. For example either a generator or a discriminator could overfit if one or the otherdoes not converge. Using Conv2DTranspose will also upsample its input but the key difference is the model should learn what is the best upsampling for the job. Conv2DTranspose is a convolution operation whose kernel is learnt just like normal conv2d operation while training your model. crop 30 55 150 175 croping size for the image so that only the face at centre is obtainedtraining_images np. The output layer has a stride value of 1 and apadding value of 0. The loss is calculated for each of these modelsIn the process defined below I am training the generator and discriminator simultaneously. Array to collect loss for the discriminator model Array to collect loss for generator model Loop over each batch in the dataset Benefits of Double Division Operator over Single Division Operator in Python The Double Division operator in Python returns the floor value for both integer and floating point arguments after division. Setting discriminator. The discriminator is then used to classify real images drawn from the training set and fakes images produced by the generator. Generator Architecture Imgur https imgur. When G does a good enough job to fool D the output probability should be close to 1. It s important that the generator and discriminator do not overpower each other e. Deep learning based facial attribute analysis consists of two basic sub issues facial attribute estimation FAE which recognizes whether facial attributes are present in given images andfacial attribute manipulation FAM which synthesizes or removes desired facial attributes. All hidden layers have a stride value of 2 and a paddingvalue of 1 so that their output image sizes will be half the input images. But this Notebook is mostly written as a Tutorial to show the flow of DCGAN. There are 5 landmarks left eye right eye nose left mouth right mouth list_attr_celeba. Why would we want to do this Well we aren t going to be training the generator model directly we are going to be combining the generator and discriminator into a single model then training that. Could not work at all with the full set of 200k images. png The discriminator network of a DCGAN consists of 4 hidden layers again we treat theinput layer as the 1st hidden layer and 1 output layer. csv Attribute labels for each image. Sometimes thespeed up of SSD reading samples at 50 MB s over the traditional harddrive 5 MB s can save you a big chunk of training time. It maps 4 x 4 feature maps to single values so that the Sigmoid functioncan transform the value into prediction confidence. Generator takes random latent vector and output a fake image of the same size as our reshaped celebA image. This introduces a similar effect to tanh x or sigmoid x as non linearities to increase the model s expressive power. Transposed convolution layers areused in hidden layers which are followed by batch normalization layers and ReLUactivation functions. linear algebra First I am creating an array containing all the full path names for each of the. All kernels inconvolution layers are of a size of 4 x 4. The output will be a 3x64x64 RGB image. After about 300 epochs they resemble almost the original. x_1 and y_1 represent the upper left point coordinate of bounding box. The output layer is also a transposed convolution layer and Tanh isused as the activation function. The architecture of the discriminator is shown inthe following The input channel can be either 1 or 3 depending on whether you are dealing withgrayscale images or color images. Initially both of the networks are naive and have random weights. csv Image landmarks and their respective coordinates. array training_images Scaling images it is also recommended that real images used to train the discriminator are scaled so that their pixel values are in the range 1 1. max training_images axis 0 Generator will upsample our seed using convolutional transpose layers upsampling layers Random noise to 4x4x512 image Next add a reshape layer to the network to reshape the tensor from the last layer to a tensor of a shape of 4 4 512 BatchNormalization is added to the model after the hidden layer but before the activation such as LeakyReLU. But the fact that it can create an image from a random noise array proves the model s power. DCGAN is super sensitiveHere even when we only train a GAN to manipulate 1D data we have to use multipletechniques to ensure a stable training. Use of Batch NormalizationBatch normalization standardizes the activations from a prior layer to have a zero mean and unit variance. At the beginning of the training the generated images will look like random noise. Final DCGAN TrainingTraining GANs is an art form itself as incorrect hyperparameter settings lead to mode collapse. Example attributes are the color of hair hairstyle age gender etc. Hence the resultant fake images generated at the end will almost be like noise. Sometimes the generator only generates a handful of sample varieties. input img_align_celeba img_align_celeba. The 1st layerhas a padding value of 0 and the other layers have a padding value of 1. Instead we have to manually loop through each epoch and fit the models on batches. Pooling and ReLU CNNs have two non linearities pooling layers and ReLU functions. As theimage feature map sizes increase by two in deeper layers the numbers of channels aredecreasing by half. min training_images axis 0 print np. This is so that the discriminator will always receive images as input real and fake that have pixel values in the same range. Becausewhen you are training neural networks on a very large dataset whichcannot fit in the GPU memory the reading speed from physical drivescould be the bottleneck of your training performance. 1 represents positive while 1 represents negative The Facial Attributes of CELEB A Dataset Imgur https imgur. The 2nd 3rd and 4th hidden layers and the output layer have a stride value of 2. A lot of things could go wrong in the training ofGANs. png The 2 2 kernel produces a 2 2 output when convolving over a 3 3 image. We also need tocreate an array of labels label 1 for real images and label 0 for fake images. The training of CNNs then becomes the task of learning filters deciding what features you should look for in the data. to use a smaller input and to learn its larger representation being the following Imgur https imgur. The Conv2DTranspose layer learns a number of filters similar to the regular Conv2D layer. Concatenate the real images and the fake images to create a new set of sample images. The generator consists of convolutional transpose layers batch normalization layers and ReLU activations. Lets generate a sample image with the below code. input img_align_celeba img_align_celeba When running in Local Machine full_image_train_path content gdrive MyDrive celeba dataset img_align_celeba img_align_celeba When running in Colab with data in G Drive full_image_train_path content img_align_celeba img_align_celeba For reading from Colab sesstion storage as g drive is failing for large 200k files of CelebA Now from this array print image_path_50k Model Constants Alternative way to build the numpy array for all the training images For getting the list of training_images I COULD REPLACE THE ABOVE os. Local Connectivity Neurons in one layer are only connected to neurons in the next layer that are spatially close to them. This has the effect of stabilizing the training process. Convolution layers are used in alllayers which are followed by batch normalization layers except that the first layer does nothave batch normalization. This decreases anyunwanted interdependence between parameters across layers which helps speed up the network training process andincrease its robustness especially when it comes to networkparameter initialization. All kernel sizes of transposed convolution layers are set to 4 x 4. First create a random noise print generated_image It is just plain noise. Feedthis latent vector to the untrained generator network. Several tricks had to be applied to make DCGANs stable and useful Batch normalization was applied to both the generator and the discriminator networkDropout is used as a regularization techniqueThe generator needed a way to upsample the random input vector to an output image. However this was not simply a matter of adding convolutional layers to the model since training became even more unstable. min 255 training_images i. This is a common convention in the architecture design of neuralnetworks. For understanding parameters look at this bloghttps towardsdatascience. By forcing the neurons of one layer to share weights the forward pass feeding data through the network becomes the equivalent of convolving a filter over the image to produce a new image. png Thats where the Transposed convolutions in the Keras API comes to help. To combine the generator and discriminator we will be calling the discriminator on the output of the generator. So play with different hyperparameters to obtain better results. The architecture of a discriminator Imgur https imgur. This is good If the copy of the discriminator in the combined model were trainable it would update itself to be worse at classifying images. The crop method from the Image module takes four coordinates as input. Imgur https imgur. Each filter has a relatively small receptive field width height but always extends through theentire depth of the input volume. As image sizesincrease in deeper layers the numbers of channels are increasing by twice. zip All the face images cropped and aligned list_eval_partition. That seed is used to produce an image. min training_images i training_images i 2 1 uncomment this if activation is tanh for generator last layer training_images np. Special Note Due to Out of memory limits in Kaggle I could NOT run this Notebook inside Kaggle with any larger dataset than 1000 images. jpg file The purpose to crete this array is to be able to select only limited number of training_images to train from this array When running in Kaggle full_image_train_path. When we train this network we don t want to train the discriminator network so make it non trainable before we add it to the adversarial model. img_list glob. DiscriminatorHere I am creating a Sequential model incrementally via the add method DCGAN Combining Generator and DiscriminatorThe combined model is stacked generator and discriminator Setting discriminator. So the training loop begins with generator receiving a random seed as input. As the formula shows the generator optimizes for maximally confusing the discriminator by trying to make it output high probabilities for fake data samples. com understand transposed convolutions and build your own transposed convolution layer from scratch 4f5d97b2967 Lets see how our generator is creating images from noise. Shared Weights This is the concept that makes CNNs convolutional. Convolutions are performed by sliding one or more filters the smaller Matrix which is the Kernel of the Current Layer over the input layer. So overall the core to the DCGAN architecture uses a standard CNN architecture on the discriminative model. In DCGAN under the Generator part to generate an image by using the ConvNet architecture we reverse the process instead of taking an image and processing it into a vector we take avector and up size it to an image. Remember that the transpose layer simply swaps the backwards and forward pass keeping the rest of the operations the same As the transposed convolution will also slide over the input we must specify a kernel_size as with the normal convolution. Start by sampling a batch of noise vectors from a uniform distribution generator receives a random seed as input which is used to produce an image. To generate fake samples create a latent vector of a shape of 100 over a uniform distribution. resize 64 64 for path in image_path_50k for i in range len training_images training_images i training_images i training_images i. Regular Neural Network vs CNNUnlike a regular feed forward neural network whose neuronsare arranged in flat fully connected layers layers in aConvNet are arranged in three dimensions width height depth. LeakyReLU activation functions are used in the hidden layersand Sigmoid is used for the output layer. It is added to the model after the hidden layer but before the activation such as LeakyReLU. crop function argument box which is the crop rectangle as a left upper right lower tuple. Generator I use Leaky relu activations in the hidden layer neurons and sigmoids for the output layers. But I want to work in the opposite direction i. png The Dataset is considerably large to manage with even Google Colab s Pro version. Discriminator takes true of fake images and output the probability estimate ranging between 0 and 1. Final Training Architecture of a DCGAN1. Images 1 162770 are training 162771 182637 are validation 182638 202599 are testing list_bbox_celeba. Transposing convolutional layers is employed hereLeakyRelu and TanH activations are used throughout both networks How Reverse ConvNets works in DCGANConvNets have traditionally been used for image classificationtasks in which the network takes in an image with thedimensions height width number of color channels asinput and through a series of convolutional layers outputs asingle vector of class scores with the dimensions 1 n wheren is the number of class labels. The right can also be represented as left width and lower can be represented as upper height. Doing this reduces the size of the output and requires no added parameters to learn so pooling layers are often used to regulate the size of the network and keep the system below a computational limit. Advantage is it s cheap. If you have a Solid State Drive SSD with enough space plugged in yourmachine I highly recommend you move all of your training samples tothe SSD especially when you have a powerful graphics card. Pooling layers consider a block of input data and simply pass on the maximum value. There are 40 attributes. png The generator network of a DCGAN contains 4 hidden layers we treat the input layer asthe 1st hidden layer for simplicity and 1 output layer. Deep Convolutional GANs DCGANs Deep Convolutional GANs DCGANs introduced convolutions to the generator and discriminator networks. Link to my Youtube Video Explaining this whole Notebook https youtu. The noise variable is the code equivalent of the variable z which we discussed earlier. Convolutional Neural Networks have three important architectural features. We already have the real samples so we nowneed to generate the fake samples. But for the generator convolutions are replaced with up convolutions so the representation at each layer of the generator is actually successively larger as it maps from a low dimensional latent vector onto a high dimensional image. ReLU x argmax x 0. be csQj1e6Oj38 Imgur https imgur. At every step as it slides across the input each filter outputs a single activation value the dot product between the input values and the filter entries. A quick tip in this regard if you are using local machine to handle this dataset challenge. png Here D refers to the discriminator network while G obviously refers to the generator. Transposed Convolutions Imgur https imgur. This is called mode collapse. To do this we need fake samples as well as real samples. The outputchannel can be either 1 or 3 depending on whether you want to generate grayscale imagesor color images. Similarly we got to specify strides output_padding UpSampling2D vs Conv2DTranspose in KerasUpSampling2D is just a simple scaling up of the image by using nearest neighbour or bilinear upsampling so nothing smart. width and height represent the width and height of bounding box list_landmarks_align_celeba. be csQj1e6Oj38 Facial Attribute predictionFacial Attribute prediction is a Computer Vision CV task about deducing the set of attributes belonging to a face. print training_images training_images is a 4 D array print training_images 500 64 64 3 print np. Fundamental way GAN WorksGAN contains two networks which has two competing objectives Generator the generator generates new data instances that are similar to the training data in our case celebA images. csv Bounding box information for each image. Typically image data is loaded as a NumPy array such that pixel values are 8 bit unsigned integer uint8 values in the range 0 255. Getting to know CELEB A Dataset202 599 number of face images of various celebrities10 177 unique identities but names of identities are not given40 binary attribute annotations per image5 landmark locations Data Files img_align_celeba. The GAN training process consists of a two player minimax game in which D is adapted to minimize the discrimination error between real and generated samples and G is adapted to maximize the probability of D making a mistake. that they train at a similar rate. Facial attribute analysis has received considerable attention when deep learning techniques maderemarkable breakthroughs in this field over the past fewyears. The ReLU function takes one input x and returns the maximum of 0 x. listdir with glob like below full_image_train_path. I got out of memory quite a few times. This allows the generator to understand the discriminator so it can update itself more effectively. This design trims the vast majority of connections between consecutive layers but keeps the ones that carry the most useful information. Now I am taking real x_train data by sampling a batch of real images from the set of all image Create Labels First training on real image Next train the discriminator network on real images and real labels Now training on fake image Creating variables to make ready the whole adversarial network Image Label vector that has all the values equal to 1 To fool the Discriminator Network. ", "id": "paulrohan2020/tutorial-dcgan-with-tensorflow-create-fake-images", "size": "17974", "language": "python", "html_url": "https://www.kaggle.com/code/paulrohan2020/tutorial-dcgan-with-tensorflow-create-fake-images", "git_url": "https://www.kaggle.com/code/paulrohan2020/tutorial-dcgan-with-tensorflow-create-fake-images", "script": "Flatten tensorflow.keras.utils Reshape LeakyReLU tensorflow.keras.layers Dropout Sequential tensorflow.keras.models Conv2D numpy PIL Conv2DTranspose Image matplotlib.pyplot Dense tensorflow generator_model pandas discriminator_model tensorflow.keras BatchNormalization plot_model preprocessing ", "entities": "(('1', 'CELEB A Dataset Imgur https imgur'), 'represent') (('that', 'case celebA images'), 'contain') (('kernels inconvolution layers', '4 4'), 'be') (('we', 'generator'), 'call') (('CELEB Dataset202 599 number', 'given40 attribute image5 landmark locations Data binary Files'), 'be') (('directly we', 'then that'), 'want') (('Lets', 'below code'), 'generate') (('2nd 3rd hidden layers', '2'), 'have') (('x_1', 'box'), 'represent') (('linear First I', 'the'), 'algebra') (('reading speed', 'training performance'), 'be') (('you', 'grayscale imagesor color images'), 'be') (('We', '0 fake images'), 'need') (('you', 'dataset challenge'), 'tip') (('it', 'classifying images'), 'be') (('output probability', '1'), 'do') (('we', 'normal convolution'), 'remember') (('Instead we', 'batches'), 'have') (('numbers', 'channels'), 'increase') (('lot', 'training wrong ofGANs'), 'go') (('we', 'stable training'), 'be') (('kernel sizes', '4 4'), 'set') (('that', 'spatially them'), 'connect') (('output layer', '0'), 'have') (('Notebook', 'DCGAN'), 'write') (('This', 'training process'), 'have') (('It', 'noise print First random generated_image'), 'create') (('Generator', 'celebA reshaped image'), 'take') (('Convolutional GANs DCGANs Deep Convolutional GANs Deep DCGANs', 'generator networks'), 'introduce') (('theimage feature map', 'half'), 'size') (('Discriminator', '0'), 'take') (('especially when you', 'graphics powerful card'), 'have') (('convolution kernel', 'model'), 'be') (('other layers', '1'), 'layerha') (('filter', 'input volume'), 'have') (('discriminator', 'real distribution'), 'try') (('relationship', 'probably less two close neighbors'), 'be') (('regardless where it', 'image'), 'be') (('we', 'fake samples'), 'need') (('face images', 'list_eval_partition'), 'zip') (('eye right nose', 'mouth'), 'be') (('where Transposed convolutions', 'Keras API'), 'png') (('It', 'such LeakyReLU'), 'add') (('Facial predictionFacial Attribute csQj1e6Oj38 Attribute prediction', 'face'), 'be') (('which', 'image'), 'receive') (('generator', 'other e.'), 's') (('pooling layers', 'computational limit'), 'reduce') (('amount', 'current layer'), 'limit') (('This', 'expressive power'), 'introduce') (('Images', '1 162770 162771 182637 182638 202599 list_bbox_celeba'), 'be') (('it', 'generator'), 'evaluate') (('feeding forward pass data', 'new image'), 'become') (('real that', 'same range'), 'be') (('Sigmoid functioncan', 'prediction confidence'), 'map') (('Example attributes', 'hair hairstyle age gender etc'), 'be') (('Hence resultant fake images', 'almost noise'), 'be') (('We', 'noise'), 'do') (('trainable', 'combined model'), 'affect') (('best upsampling', 'job'), 'upsample') (('training', 'model'), 'be') (('Batch normalization', 'generator model'), 'use') (('we', 'image'), 'load') (('I', 'i.'), 'want') (('ABOVE', 'training_images'), 'input') (('we', 't Keras'), 'rely') (('we', 'discriminator network'), 'generate') (('when deep learning', 'past fewyears'), 'receive') (('which', 'batch normalization layers'), 'areuse') (('we', 'image'), 'reverse') (('i', 'i.'), 'resize') (('Sigmoid', 'output layer'), 'use') (('D', 'mistake'), 'consist') (('you', 'data'), 'become') (('it', 'data fake samples'), 'show') (('actually successively it', 'high dimensional image'), 'replace') (('we', 'simplicity'), 'contain') (('Similarly we', 'nearest neighbour'), 'get') (('generated images', 'random noise'), 'look') (('first layer', 'nothave batch normalization'), 'use') (('width', 'box list_landmarks_align_celeba'), 'represent') (('standard process', 'samples'), 'be') (('G', 'obviously generator'), 'refer') (('I', '1000 images'), 'note') (('neuronsare', 'dimensions height three width depth'), 'arrange') (('crop method', 'input'), 'take') (('Next add', 'such LeakyReLU'), 'axis') (('Use', 'unit zero mean variance'), 'standardize') (('2 png 2 kernel', '3 when over a 3 image'), 'produce') (('network', '1 class n labels'), 'employ') (('Conv2DTranspose layer', 'Conv2D similar regular layer'), 'learn') (('pixel such values', '8 bit unsigned integer uint8 range'), 'load') (('it', 'power'), 'prove') (('techniqueThe generator', 'output image'), 'have') (('purpose', 'Kaggle When full_image_train_path'), 'file') (('parameters', 'bloghttps towardsdatascience'), 'look') (('Generator I', 'output layers'), 'use') (('we', 'adversarial model'), 'want') (('pixel values', 'range'), 'training_images') (('you', 'withgrayscale images'), 'show') (('more recently LeakyReLU', 'models'), 'recommend') (('So overall core', 'discriminative model'), 'use') (('last layer', 'np'), 'training_images') (('generated images', 'increasingly just original celeb'), 'look') (('only face', 'centre'), 'crop') (('I', 'generator'), 'calculate') (('that', 'most useful information'), 'trim') (('that', 'Discriminator Network'), 'take') (('generator', 'normalization layers'), 'consist') (('convolution also transposed Tanh', 'activation function'), 'be') (('smaller which', 'input layer'), 'perform') (('basic objective function', 'vanilla GAN model'), 'png') (('ReLU function', '0 x.'), 'take') (('one', 'example'), 'overfit') (('they', 'almost original'), 'resemble') (('output image sizes', '1'), 'have') (('synthesizes', 'facial attributes'), 'consist') (('crop', 'left upper right lower tuple'), 'box') (('Pooling layers', 'simply maximum value'), 'consider') (('Initially both', 'random weights'), 'be') (('especially when it', 'networkparameter initialization'), 'decrease') (('right', 'upper height'), 'represent') (('we', 'which'), 'be') (('filter', 'input values'), 'output') (('how generator', 'noise'), 'transpose') (('print training_images training_images', 'print 64 64 3 np'), 'be') (('Sometimes generator', 'sample varieties'), 'generate') (('This', 'neuralnetworks'), 'be') (('we', 'fake samples'), 'have') (('it', 'itself'), 'allow') (('generate', 'uniform distribution'), 'create') (('Dataset', 'Google Pro considerably even version'), 'png') (('generator', 'input'), 'begin') (('art hyperparameter incorrect settings', 'mode collapse'), 'be') (('again we', '1st hidden layer'), 'png') (('Convolutional Neural Networks', 'three important architectural features'), 'have') (('discriminator', 'generator'), 'use') (('DiscriminatorHere I', 'discriminator'), 'create') (('Double Division operator', 'point floating division'), 'Array') (('Pooling CNNs', 'layers functions'), 'have') ", "extra": "['annotation', 'biopsy of the greater curvature', 'gender', 'test']", "label": "Perfect_files", "potential_description_queries": ["age", "architecture", "argument", "array", "art", "attribute", "basic", "batch", "become", "best", "binary", "bit", "block", "bounding", "box", "build", "case", "channel", "chunk", "classify", "close", "code", "color", "combine", "combined", "computer", "concept", "consider", "content", "convolution", "convolutional", "copy", "core", "could", "create", "crop", "csv", "current", "data", "dataset", "depth", "difference", "direction", "directly", "distribution", "dot", "drive", "effect", "end", "ensure", "epoch", "equal", "error", "estimation", "evaluate", "even", "every", "eye", "face", "fact", "feature", "feed", "field", "file", "filter", "fit", "flat", "floor", "flow", "following", "form", "formula", "forward", "function", "game", "gender", "generate", "generated", "generator", "glob", "grayscale", "half", "handle", "height", "high", "hyperparameter", "image", "increase", "input", "integer", "itself", "job", "kernel", "key", "label", "layer", "lead", "learn", "learning", "left", "len", "linear", "list", "listdir", "load", "local", "look", "loop", "lot", "lower", "majority", "manage", "map", "max", "maximum", "mean", "memory", "method", "min", "minimize", "mode", "model", "module", "most", "mouth", "move", "my", "naive", "nearest", "need", "negative", "network", "neural", "new", "next", "no", "noise", "non", "normal", "normalization", "not", "number", "numpy", "objective", "open", "operation", "operator", "out", "output", "overall", "overfit", "padding", "part", "past", "path", "per", "pixel", "player", "png", "point", "pooling", "positive", "prediction", "present", "print", "probability", "processing", "product", "purpose", "random", "range", "reading", "recommend", "reduce", "regularization", "relationship", "representation", "reshape", "resize", "rest", "reverse", "right", "run", "running", "sample", "sampling", "save", "scaled", "scaling", "scratch", "select", "set", "shape", "sigmoid", "similar", "simplicity", "single", "size", "slide", "space", "spatial", "speed", "standard", "step", "storage", "stride", "sub", "system", "tanh", "task", "tensor", "testing", "through", "train", "training", "transform", "transpose", "under", "understanding", "uniform", "unique", "unit", "up", "update", "upper", "upsampling", "validation", "value", "vanilla", "variable", "vector", "vision", "while", "width", "work", "zip"], "potential_description_queries_len": 236, "potential_script_queries": ["preprocessing", "tensorflow"], "potential_script_queries_len": 2, "potential_entities_queries": ["attribute", "binary", "close", "even", "forward", "height", "image", "lower", "mean", "objective", "png", "random", "similar", "width"], "potential_entities_queries_len": 14, "potential_extra_queries": ["biopsy", "test"], "potential_extra_queries_len": 2, "all_components_potential_queries_len": 239}