{"name": "oversampling with smote and adasyn ", "full_name": " h1 Oversampling with SMOTE and ADASYN h2 Introduction h2 Oversampling with SMOTE h2 Oversampling with ADASYN ", "stargazers_count": 0, "forks_count": 0, "description": "Otherwise the tradeoffs are the same. The SMOTE algorithm is parameterized with k_neighbors the number of nearest neighbors it will consider and the number of new points you wish to create. png This is conceptually a very simple algorithm to quote something a graph theory professor I once had liked to say you could probably explain how it works to your grandmother. imlearn documentation is very vague as to how this works simply stating that it uses an SVM classifier to find support vectors and generate samples considering them SVM will again focus sampling on points near the boundaries of the clusters. Randomly select a minority point. But you should always be careful about checking the effect of your transformations net net. Here is what either algorithm looks like in practice The borderline SMOTE algorithms are so named because they will only sample points on the border. Oversampling with ADASYNThe other oversampling technique implemented in imlearn is adaptive synthetic sampling or ADASYN. its ability to create innerpoint outerpoint bridges. Because the algorithm doesn t have any jitter for minority class sample clouds with few enough points it tends to result in long data lines. com residentmario undersampling and oversampling imbalanced data I discussed random oversampling and undersampling what they are how they work and why they re useful. Oversampling with SMOTE and ADASYN IntroductionIn a previous notebook https www. Each step of the algorithm will 1. Funny looking results are fine as long as they actually result in improved classifiers when used as input down the line SMOTE has also done something here that I am less comfortable with it s constructed a bridge between the main red point cloud and a handful of outlier points located in the blue cluster. We can verify this if we increase the incidence rate for this rare class by 10x in the underlying dataset This tendancy of SMOTE to connect inliers and outliers is the algorithm s primary weakness in practice. Most of these work by generate or subsetting new synthetic points. Only points in danger will be sampled in step one of the algorithm. it will bias the sample space that is the likelihood that any particular point will be chosen for duping towards points which are located not in homogenous neighborhoods. Applied to our sample data this results in the following ADASYN uses 1 the kind normal SMOTE algorithm 2 on point not in homogenous neighborhoods. The result is a kind of hybrid between regular SMOTE and borderline1 SMOTE. Then on step two of the algorithm instead of selecting a point from n_neighbors belonging to the same class borderline1 will select a point from the five nearest points not belonging to the given point s class while borderline2 will select a point from the five nearest points of any class. The imbalanced learn documentation includes the following illustration https i. Similarly the noisy outlier points located inside of the blue cluster are ignored solving the innerpoint outerpoint join problem. But when boundary points are small in number and distant from the rest of their neighborhood you once again get somewhat odd looking results look at those crazy clusters near the decision boundary in the red group In this example the clusters overall still look linearly separable for example so we would not expect this to significantly effect the performance of e. This technique inherits the primary weakness of SMOTE e. Let s see what this looks like in practice. Generate and place a new point on the vector between the two points located lambda percent of the way from the original point. Randomly specify a lambda value in the range 0 1. org media 953 live 953 2037 jair. The imbalanced learn module in sklearn includes a number of more advanced sampling algorithms and I ll discuss the oversampling related ones here. This results in some very funky looking resultant datasets. There are in total four modes in imlearn. Oversampling with SMOTEThe SMOTE algorithm is one of the first and still the most popular algorithmic approach to generating new dataset samples. The algorithm introduced and accessibly enough described in a 2002 paper https www. The last option is kind svm. The remaining three are adaptations. However as the red cluster demonstrates as a result SMOTE borderline will tend to focus extremely heavily on the same relatively small number of points. The one demonstrated thus far the classic SMOTE algorithm corresponds with kind regular. We have replaced extensions to outliers with extensions from borderline points into their neighborhood. Randomly select any of its k_neighbors nearest neighbors belonging to the same class. But remember we re just building a new dataset sample. ADASYN is similar to SMOTE and derived from it featuring just one important difference. For a broad class of data it s actually reasonably easy to tell when someone has used SMOTE on it. We can see that these new points are quite dense if we zoom in on the structure Why does this happen This surprising new structure shows up prominently because in the line constructor nature of SMOTE a few of the outlier points inside of the blue point cloud will match up sometimes with points in the main body of the class cluster. This is an artifact of the SMOTE algorithm and is problematic because it introduces a feature into the dataset this point bridge which doesn t actually exist in the underlying dataset. points inside of the class clusters are never sampled. Recall the following synthetic sample dataset from the previous notebook When we apply SMOTE to this we get The weakness of SMOTE is readily apparent from this quick test. It limits the algorithm s applicability to datasets with sufficiently few samples and or sufficiently sparse point clouds. Points that are located in a neighborhood of n_neighbors points all of their class are left untouched so e. Somewhat funny looking results are actually a trademark of these sampling techniques. New points created in dense but not totally homogenous neighborhoods which are essentially jittered versions of existing ones they are displaced just a small amount from their progenitors. kind borderline1 and kind borderline2 are one class of adaptations. Now borderline2 Because borderline2 allows extending to any point class wise it will retain a bit of the stringiness of the regular SMOTE which may make the result less separable. Whether or not the heavy focus on the outlier points is a good thing or not is application dependent but overall ADASYN feels like a very heavy transformation algorithm and e. These will then get interpolated into long spaghetti lines. When applying SMOTE to your own data make sure to take a good hard look at whether or not it s doing what you expect it to be doing. The literature on sampling imbalanced datasets extends past these naive approaches. pdf works by oversampling the underlying dataset with new synthetic points. imlearn includes several adaptations of the naive SMOTE algorithm which attempt to address this weakness in various ways. These will classify points are being noise all nearest neighbors are of a different class in danger half or more nearest neighbors are a different class or safe all nearest neighbors are of the same class. one requiring that the underlying point cluster be sufficiently large as imblearn doesn t provide any modifications to this algorithm for modulating its tendency to create them as it does for SMOTE. ", "id": "residentmario/oversampling-with-smote-and-adasyn", "size": "7739", "language": "python", "html_url": "https://www.kaggle.com/code/residentmario/oversampling-with-smote-and-adasyn", "git_url": "https://www.kaggle.com/code/residentmario/oversampling-with-smote-and-adasyn", "script": "SMOTE imblearn.over_sampling seaborn ADASYN matplotlib.pyplot make_classification sklearn.datasets ", "entities": "(('probably how it', 'grandmother'), 'png') (('good application overall ADASYN', 'transformation very heavy algorithm'), 'be') (('actually reasonably when someone', 'it'), 's') (('we', 'dataset just new sample'), 'remember') (('ADASYN', 'just one important difference'), 'be') (('Somewhat funny looking results', 'sampling actually techniques'), 'be') (('so they', 'border'), 'be') (('it', 'data long lines'), 'have') (('few', 'class cluster'), 'see') (('It', 'sufficiently few samples'), 'limit') (('algorithm', 'paper https accessibly enough 2002 www'), 'introduce') (('Randomly', 'range'), 'specify') (('you', 'transformations net net'), 'be') (('you', 'new points'), 'parameterize') (('why they', 'imbalanced data'), 'undersampling') (('literature', 'naive approaches'), 'extend') (('tendancy', 'primary practice'), 'verify') (('I', 'oversampling related ones'), 'include') (('different nearest neighbors', 'same class'), 'classify') (('this', 'e.'), 'get') (('all', 'e.'), 'leave') (('We', 'neighborhood'), 'replace') (('borderline2', 'class'), 'select') (('simply it', 'clusters'), 'be') (('These', 'spaghetti then long lines'), 'interpolate') (('technique', 'SMOTE e.'), 'inherit') (('it', 'SMOTE'), 'one') (('weakness', 'readily quick test'), 'recall') (('Randomly', 'nearest same class'), 'select') (('it', 'what'), 'make') (('which', 'various ways'), 'include') (('less it', 'blue cluster'), 'be') (('points', 'class inside clusters'), 'sample') (('doesn', 'actually underlying dataset'), 'be') (('Oversampling', 'imlearn'), 'be') (('learn imbalanced documentation', 'illustration https following i.'), 'include') (('Only points', 'one algorithm'), 'sample') (('this', 'practice'), 'let') (('kind borderline1', 'kind one adaptations'), 'be') (('Similarly noisy outlier points', 'innerpoint outerpoint join problem'), 'ignore') (('they', 'progenitors'), 'displace') (('results', 'homogenous neighborhoods'), 'use') (('result', 'regular SMOTE'), 'borderline2') (('which', 'homogenous neighborhoods'), 'bias') (('result', 'regular SMOTE'), 'be') (('SMOTE thus far classic algorithm', 'kind regular'), 'demonstrate') (('SMOTE borderline', 'points'), 'demonstrate') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["advanced", "algorithm", "application", "apply", "approach", "artifact", "bit", "body", "boundary", "checking", "classifier", "classify", "cloud", "cluster", "consider", "could", "create", "data", "dataset", "decision", "dependent", "effect", "feature", "find", "following", "generate", "graph", "group", "half", "imblearn", "increase", "input", "jitter", "join", "learn", "left", "likelihood", "line", "look", "looking", "main", "match", "module", "most", "naive", "nature", "near", "nearest", "neighborhood", "new", "noise", "normal", "not", "notebook", "number", "option", "outlier", "overall", "past", "pdf", "percent", "performance", "place", "png", "point", "practice", "provide", "quote", "random", "range", "rare", "re", "rest", "result", "sample", "sampling", "select", "several", "similar", "sklearn", "something", "space", "sparse", "step", "structure", "support", "technique", "theory", "those", "total", "transformation", "undersampling", "up", "value", "vector", "verify", "while", "wise", "work", "zoom"], "potential_description_queries_len": 100, "potential_script_queries": ["seaborn"], "potential_script_queries_len": 1, "potential_entities_queries": ["following", "looking", "new", "overall"], "potential_entities_queries_len": 4, "potential_extra_queries": ["test"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 102}