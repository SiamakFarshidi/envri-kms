{"name": "introduction to sound event detection ", "full_name": " h2 Update h3 Version note h2 About h2 Model for SED task h3 torchlibrosa h4 LICENSE h3 audioset tagging cnn h4 LICENSE h3 Building blocks h2 Train SED model with only weak supervision h3 Dataset h3 Criterion h3 Callbacks h3 Train h2 Prediction with SED model h2 Postprocess h2 EOF ", "stargazers_count": 0, "forks_count": 0, "description": "sum norm_att cla dim 2 return x norm_att cla def nonlinear_transform self x if self. Assume we use 2D CNN based model which takes log melspectrogram as input and extract features using CNN feature extractor and do classification with the feature map which is the output of CNN. net 508a62f305652e6d9af853c65ab33ae9900ff38e 17a88 images tasks challenge2016 task3_overview. IN NO EVENT SHALL THEAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHERLIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS INTHE SOFTWARE. Then we train it normally by using BCE loss with clip level prediction and clip level annotation. Train SED model with only weak supervision weak label vs strong label https www. cnn_feature_extractor method will take this as input and output feature map. Update Version notev3 Training procedure didn t use pretrained weight. In PANNsCNN14Att input raw waveform will be converted into log melspectrogram using torchlibrosa s utilities. Let s put a chunk into the CNN feature extractor of the model above. clip_id onset offset event audio_001 0. Chunk level prediction can be treated as audio tagging task if we treat each chunk as short audio clip but we can also use SED approach. Since several concerns https www. I put this functionality in PANNsCNN14Att. This will be used to aggregate the classification result for segment. In this way we can get both clip level prediction and segment level prediction if the time resolution is high it can be treated as event level prediction. Therefore we need to train our SED model in weakly supervised manner. For this competition we only have weak annotation clip level annotation. How can we output segment wise prediction The idea is simple. temperature temperature self. The model here is pretrained with AudioSet https research. init_weights def init_weights self init_layer self. Let s check the output of the feature extractor. Thanks ttahara Seems it s learning something. AboutIn this notebook I will introduce Sound Event Detection SED task and model fit for that task and I will show how to train SED model with only weak annotation. bn_att def forward self x x n_samples n_in n_time norm_att torch. Module def __init__ self in_features int out_features int activation linear temperature 1. activation activation self. com c freesound audio tagging is Audio Tagging which we ll need to provide clip level prediction and the task in TensorFlow Speech Recognition Challenge https www. com c birdsong recognition discussion 172356 are expressed about over sharing of top solutions during competition and since I do respect those people who have worked hard to improve their scores I would not make trained weight in common and would not share how I trained this model. cla x calculates segment wise classification result. activation sigmoid return torch. att x 10 10 dim 1 cla self. Each element of this dimension is segment. sigmoid x In the forward method it at first calculate self attention map in the first line norm_att torch. com qiuqiangkong torchlibrosa LICENSE ISC LicenseCopyright c 2013 2017 librosa development team. Let s check how this is implemented in the PANNs model above. BatchNorm1d out_features self. Prediction with SED model Postprocess EOF type ignore type ignore type ignore By default use the entire frame Set the default hop if it s not already specified Pad the window out to n_fft size DFT IDFT matrix n_fft 2 1 1 n_fft n_fft 2 1 1 n_fft batch_size channels_num data_length batch_size n_fft 2 1 time_steps batch_size 1 time_steps n_fft 2 1 batch_size n_fft 2 1 time_steps n_fft 2 1 mel_bins Mel spectrogram Logmel spectrogram dim 2 time dim 3 frequency x n_samples n_in n_time Downsampled ratio Spectrogram extractor Logmel feature extractor Spec augmenter t1 time. com ttahara training birdsong baseline resnest50 fast. cla init_bn self. I ll use trained model of this which I trained by myself using the data of this competition in my local environment. 75 speech audio_001 5. cla x x torch. 10211 torchlibrosaIn PANNs torchlibrosa a PyTorch based implementation are used to replace some of the librosa s functions. In weakly supervised setting we only have clip level annotation therefore we also need to aggregate that in time axis. Here I use some functions of torchlibrosa. com c freesound audio tagging 2019 or Freesound General Purpose Audio Tagging Challenge https www. Therefore our prediction for SED task should look like this. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL DIRECT INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE DATA OR PROFITS WHETHER IN AN ACTION OF CONTRACT NEGLIGENCE OR OTHER TORTIOUS ACTION ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. com c tensorflow speech recognition challenge is Speech Recognition so what we need to predict is which speech command is in that audio clip which is in a sense similar to Audio Tagging task because we only need to provide clip level prediction. Now that I ve introduced the basic idea let s look into a SED model with some code. att init_layer self. Then in the third line attention aggregation is performed to get clip wise prediction. In SED model we provide prediction for each of this. Building blocksWhat is good in PANNs models is that they accept raw audio clip as input. Hense we at first put classifier that outputs class existence probability for each time step just after the feature extractor and then aggregate the output of the classifier result in time axis. 28 alarm SED task is different from the tasks in past audio competitions in kaggle. SED overview http d33wubrfki0l68. com qiuqiangkong audioset_tagging_cnn. In this competition what we need to provide is 5sec chunk level prediction for site_1 and site_2 data and clip level prediction for site_3 data. In this notebook I ll use Weakly supervised SED model provided by PANNs repository https github. That feature map has information about which time segment has what sound event. PANNs paper https arxiv. THE SOFTWARE IS PROVIDED AS IS AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. LICENSE The MIT License Copyright c 2010 2017 Google Inc. __init__ self. Let s check the output. Changed to use pretrained weight. time batch_size 1 time_steps freq_bins batch_size 1 time_steps mel_bins Mixup on spectrogram Output shape batch size channels time frequency Aggregate in frequency axis Get framewise output list of list file_path ebird_code loaders model Fixed in V3 Optimizer Scheduler Loss callbacks Overlap deletion this part may not be necessary I deleted this part in other model and found there s no difference on the public LB score. Model for SED taskHow can we provide prediction with onset and offset time information To do this models for SED task output segment wise prediction instead of outputting aggregated prediction for a clip which is usually used for Audio Tagging model. ppm This figure gives us an intuitive explanation what is weak annotation and what is strong annotation in terms of sound event detection. Now I ll show how this model works in the inference phase. Although it s downsized through several convolution and pooling layers the size of it s third dimension is 15 and it still contains time information. The task in Freesound Audio Tagging 2019 https www. THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS ORIMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. py which is a SED model. att x 10 10 dim 1. Now let s try to train this model in weakly supervised manner. com audioset which is an ImageNet counterpart in audio field. In the second line cla self. Permission to use copy modify and or distribute this software for any purpose with or without fee is hereby granted provided that the above copyright notice and this permission notice appear in all copies. activation linear return x elif self. com qiuqiangkong audioset_tagging_cnn blob master pytorch models. The output of CNN feature extractor still contains information about frequency and time it should be 4 dimensional batch size channels frequency time so if we aggregate it only in frequency axis we can preserve time information on that feature map. orgPermission is hereby granted free of charge to any person obtaining a copyof this software and associated documentation files the Software to dealin the Software without restriction including without limitation the rightsto use copy modify merge publish distribute sublicense and or sellcopies of the Software and to permit persons to whom the Software isfurnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included inall copies or substantial portions of the Software. net profile Anurag_Kumar10 publication 329239818 figure fig5 AS 743089203322880 1554177680169 Weakly Labeled vs Strongly Labeled Strongly labeled data contains time stamps of the. Add link to original paper. Conv1d in_channels in_features out_channels out_features kernel_size 1 stride 1 padding 0 bias True self. audioset_tagging_cnnI also use Cnn14_DecisionLevelAtt model from PANNs models https github. Dataset Criterion Callbacks TrainSome code are taken from https www. nonlinear_transform self. segment wise prediction and clip wise prediction is actually calculated in AttBlock of the model. png In SED task we need to detect sound events from continuous long audio clip and provide prediction of what sound event exists from when to when. ", "id": "hidehisaarai1213/introduction-to-sound-event-detection", "size": "10055", "language": "python", "html_url": "https://www.kaggle.com/code/hidehisaarai1213/introduction-to-sound-event-detection", "git_url": "https://www.kaggle.com/code/hidehisaarai1213/introduction-to-sound-event-detection", "script": "torch.optim torch.utils.data sklearn.metrics __init__ F1Callback(Callback) STFT(DFTBase) get_model idft_matrix mAPCallback(Callback) List dft_matrix timer progress_bar Optional fastprogress on_loader_start Path average_precision_score preprocess PANNsLoss(nn.Module) on_batch_end forward torch.nn numpy init_weight __getitem__ pathlib ConvBlock(nn.Module) cnn_feature_extractor LogmelFilterBank(nn.Module) init_bn transform_slice interpolate PANNsDataset(data.Dataset) init_layer PANNsCNN14Att(nn.Module) prediction_for_clip SupervisedRunner Spectrogram(nn.Module) typing SpecAugmentation(nn.Module) AttBlock(nn.Module) sklearn.model_selection contextmanager Audio f1_score librosa.display pandas State CallbackOrder set_seed power_to_db CheckpointCallback DropStripes(nn.Module) nonlinear_transform __len__ DFTBase(nn.Module) torch.nn.functional prediction Callback catalyst.dl init_weights soundfile pad_framewise_output on_loader_end StratifiedKFold get_logger IPython.display contextlib ", "entities": "(('permission notice', 'substantial Software'), 'be') (('PyTorch based implementation', 'functions'), 'torchlibrosa') (('Update notev3 Training procedure didn Version t', 'pretrained weight'), 'use') (('DFT IDFT batch_size data_length 2 2 2 2 mel_bins Mel spectrogram 2 1 2 1 1 batch_size 1 1 n_fft time_steps n_fft 1 Logmel', 'n_fft size'), 'prediction') (('that', 'time'), 'existence') (('This', 'segment'), 'use') (('we', 'this'), 'provide') (('Dataset Criterion Callbacks TrainSome code', 'https www'), 'take') (('s', 'model'), 'let') (('we', 'SED also approach'), 'treat') (('therefore we', 'also time'), 'need') (('they', 'input'), 'be') (('Here I', 'torchlibrosa'), 'use') (('COPYRIGHT HOLDERS', 'DEALINGS INTHE OTHER SOFTWARE'), 'theauthor') (('Therefore prediction', 'this'), 'look') (('it', 'something'), 'seem') (('line attention Then third aggregation', 'clip wise prediction'), 'perform') (('idea', 'segment wise prediction'), 'output') (('weak what', 'event sound detection'), 'ppm') (('com which', 'ImageNet audio field'), 'audioset') (('Therefore we', 'weakly supervised manner'), 'need') (('we', 'TensorFlow Speech Recognition Challenge https www'), 'be') (('it', 'time still information'), 'be') (('AUTHOR', 'MERCHANTABILITY'), 'be') (('segment wise prediction', 'model'), 'calculate') (('I', 'PANNs repository https github'), 'use') (('s', 'feature extractor'), 'let') (('PROVIDED AS', 'PARTICULAR PURPOSE'), 'be') (('sound event', 'what'), 'png') (('Then we', 'clip level prediction'), 'train') (('alarm SED 28 task', 'kaggle'), 'be') (('how model', 'inference phase'), 'show') (('element', 'dimension'), 'be') (('model', 'AudioSet https here research'), 'pretraine') (('Strongly labeled data', 'the'), 'figure') (('s', 'code'), 'let') (('which', 'CNN'), 'assume') (('I', 'PANNsCNN14Att'), 'put') (('I', 'local environment'), 'use') (('I', 'LB public score'), 'batch_size') (('copyright above notice', 'permission copies'), 'be') (('method', 'output feature input map'), 'take') (('I', 'only weak annotation'), 'aboutin') (('time segment', 'information'), 'have') (('how this', 'PANNs model'), 'let') (('input raw waveform', 'utilities'), 'convert') (('it', 'event level prediction'), 'get') (('we', 'annotation clip level only weak annotation'), 'have') (('DAMAGES WHATSOEVER', 'OR SOFTWARE'), 'LIABLE') (('we', 'feature map'), 'contain') (('which', 'Audio Tagging usually model'), 'model') (('we', 'clip level only prediction'), 'be') (('audioset_tagging_cnnI', 'PANNs models https github'), 'use') (('need', 'site_3 data'), 'be') (('how I', 'model'), 'express') (('Now s', 'weakly supervised manner'), 'let') ", "extra": "['annotation', 'onset', 'procedure']", "label": "Perfect_files", "potential_description_queries": ["aggregate", "annotation", "appear", "associated", "audio", "baseline", "basic", "batch", "batch_size", "blob", "calculate", "challenge", "check", "chunk", "classification", "classifier", "clip", "code", "command", "competition", "convolution", "copy", "data", "def", "default", "detect", "development", "difference", "dim", "dimension", "event", "extract", "feature", "figure", "fit", "following", "forward", "found", "frame", "frequency", "high", "http", "idea", "ignore", "implementation", "improve", "including", "inference", "input", "int", "label", "labeled", "learning", "let", "level", "librosa", "line", "linear", "link", "list", "local", "log", "look", "map", "matrix", "melspectrogram", "merge", "method", "model", "my", "need", "no", "not", "notebook", "offset", "onset", "out", "output", "overview", "padding", "paper https arxiv", "part", "past", "people", "person", "png", "pooling", "predict", "prediction", "pretrained", "probability", "procedure", "profile", "provide", "public", "publication", "purpose", "py", "pytorch", "ratio", "raw", "replace", "repository", "resolution", "result", "return", "second", "segment", "sense", "several", "shape", "short", "sigmoid", "similar", "size", "sound", "step", "stride", "subject", "sum", "supervised", "task", "tensorflow", "those", "through", "time", "train", "training", "try", "type", "weight", "who", "window", "wise"], "potential_description_queries_len": 134, "potential_script_queries": ["contextlib", "contextmanager", "interpolate", "nn", "numpy", "pathlib", "timer", "torch"], "potential_script_queries_len": 8, "potential_entities_queries": ["batch_size", "feature", "labeled", "procedure", "raw", "sound", "wise"], "potential_entities_queries_len": 7, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 140}