{"name": "optimize your algorithms ", "full_name": " h1 Optimize Your Algorithms h2 Index h2 1 Importing Libraries and Data Cleaning h1 2 Data Analysis h1 3 Testing different models with default parameters h1 4 Support Vector Machines SVM h3 How does it work h3 What parameters can we tune h3 Default parameters h2 C Penalty Parameter h2 Gamma How exact should I fit the data h2 Kernel How do I fit the data h2 Degree How sharp are the Poly Kernel s lines h2 C Again h1 5 Ridge Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 Alpha Forcing coefficients toward zero h1 6 Decision Tree Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 min samples leaf Minimum number of samples h2 max depth h2 Depth Samples h1 7 K Neighbors Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 n neighbors How many points are around me h2 Algorithm How do I find my neighbors h2 Weight Are all my neighbors equal h1 8 Extra Trees Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 n estimators How many h1 9 Random Forest Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h1 10 Gradient Boosting Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 learning rate How much does a single Decision Tree contribute h1 11 Ada Boost Classifier h3 How does it work h3 What parameters can we tune h3 Default parameters h2 base estimator What kind of model do I use h1 12 Final Score h1 13 Conclusion h1 14 Sources ", "stargazers_count": 0, "forks_count": 0, "description": "These leaves are the ones that could give use incorrect predictions. 7 but to be sure we ll graph a large range of alphas. What parameters can we tune The Extra Tree Classifier parameters that we ll tune are n_estimators max_depth and min_samples_leaf. Sources Introduction To Machine Learning by A. Let s take a look at the data we ve just imported. All of the models that we ve tuned have higher outcomes than the default models. The mean of the concavity and concave points in a malignant cell nucleus are more than triple the mean of a benign cell nucleus When we take a closer look at the fractal dimension of a cell nucleus we can see that while the mean doesn t differ much between benign and malignant cell nuclei the worst values of malignant cell nuclei are higher than those of benign cell nuclei. 001 to 1000 we ll try to find the optimal alpha. Weight Are all my neighbors equal We have all these different neighbors but they aren t all the same. com all things ai in depth parameter tuning for svc 758215394769 GBC Tuning https medium. So these are clear indications of cell nuclei being malignant. K Neighbors Classifier8. We are going to start by importing the libraries we are going to use and turning our csv into a pandas DataFrame. What parameters can we tune SVM s are able to use different kernels to classify the given data. These values can later be used to compare the optimised models against them. Here is where the parameters come in. Final ScoreSo now we can see the results of our hard work. The AdaBoostClassifier is the best model however with almost 98 accuracy. This should be more than enough to train our models. Depth SamplesLet s try both max_depth and min_samples_leaf at once to make sure we have the optimal values. read_csv Gaussian Process Naive Bayes. Another clear indication. 001 to 1000This range doesn t give us any different scores so we ll try a more extreme range. The plots show that the optimal n_estimators are mostly around 35 and the optimal maximum depth is mostly above 17. Extra Trees Classifier9. It controls the trade off between smooth decision boundary and classifying the training points correctly. 0 gamma auto 1 n_features degree 3 C Penalty ParameterC is the penalty parameter of the error term. Gamma How exact should I fit the data The higher the gamma value the harder it tries to exactly fit the training data setLet s start by using a range of 0. The most surprising one is the SVM who went up by about 0. If alpha is higher than it will force the coefficients more towards zero. These values don t seem to change our predictions so we ll wait untill we have some more parameters figured out and then we ll come back to the C parameter. The leaves show multiple pieces of information Condition of the leaf Gini or chance of incorrect measurement of a random training sample at that point The number of samples that passed during fitting Class or prediction of the sample at that pointAs we can see in the image not all bottom leaves have a gini equal to 0. The standard deviation of the fractal dimensions of a malignant cell nucleus are also higher though not as significantly as some other values. It turns out that the Poly Kernel is the best type of kernel for our data Degree How sharp are the Poly Kernel s lines Degree determines the degrees of the polynomial used to split the data on the hyperplane. Default parameters learning_rate 0. Using the default parameters of a model can be useful if you need a quick indication of its effectiveness but it won t always give you the accuracy you need. Let s start by creating a List with some basic values to check the average score they give our model. 1 n_estimators 10 max_depth None learning_rate How much does a single Decision Tree contribute The learning_rate parameter shrinks the contribution of each tree by learning_rate. Hyperplanes are subspaces that consist of one less dimension than the original space. Importing Libraries and Data Cleaning2. Default parameters n_estimators 10 max_depth None min_samples_leaf 1 n_estimators How many The value for this parameter determines how many Decision Trees are made by the model. We can drop it without affecting our data. This function fits and tests the model multiple times and then returns a list of scores. Let s start by trying N 1 100 Depth 1 30 and Samples 0. Starting with a simple list of values from 0. We are going to test the models using cross validation score. These kernels can all be tuned using different parameters. Using the sort_values function on the DataFrame gives us a nice overview of the performance of all the tested models. K Neighbors Classifier How does it work The K Neighbors Classifier searches the correct answer based on the data it already has. Support Vector Machines SVM 5. html SVM Tuning https medium. However this does require a lot more work and time than just using the default parameters. Ada Boost Classifier 12. It tries to predict the classification by looking at which data points are near it and what they are classified as. The closer these coefficients are to 0 the better the predictions. The other plots show that the optimal number of trees is somewhere between 10 and 40. Testing different models with default parameters4. We have already discussed max_depth and min_samples_leaf in Decision Trees so we will skip their explanation in this part. The data contains 357 benign and 212 malignant cell nuclei. There seems to be a column named Unnamed 32 that is full of NaN values. Before we take a deeper look at these correlations we have to edit our data so the diagnosis becomes a boolean instead of a character. Not every neighbor is equally far away from our data point. What parameters can we tune The parameters we ll tune are min_samples_leaf and max_depth Default parametersBy default Decision Tree Classifiers use these default values min_samples_leaf 2 max_depth None min_samples_leaf Minimum number of samplesThe min_samples_leaf parameter decides if a Decision Tree is allowed to split a node. This impacts our predictions. The fractal dimension mean is the only mean that isn t effective when trying to identify a malignant cell nucleus. It will only be allowed if there are at least X training samples in both the left and right branch of the node where X is the integer you gave the parameter. This time we ll use all numbers between 0. These could be improved by gathering more data that involves these leaves though this is not always possible. We have already discussed max_depth in Decision Tree and n_estimators in Extra Trees. C AgainNow that we have some more parameters tuned let s try our C parameter again. Just as we concluded earlier a high value of the radius texture perimeter and smoothness are all indicators of malignant cell nucleusAgain these correlations match up with our earlier conclusions. Support Vector Machines SVM How does it work An SVM uses hyperplanes to classify the data. As shown in the table and plot the optimal number of neighbors is 14. What parameters can we tune This algorithm uses 3 parameters that we can tune n_neighbors weight and algorithm. The mean radius and texture of malignant cell nuclei are significantly higher than those of the benign cell nuclei. As expected we don t see any spikes in accuracy above 0. This sounds really similar to the Extra Trees Classifier model because it is. Let s start by looking at the distribution of benign and malignant cell nuclei. Ridge Classifier How does it work The Ridge Classifier creates a formula that outputs the prediction. This then sends the user over to another leaf etcetera. Knowing this we can try the other parameters a bit more in depth. Let s start by trying N 1 100 Depth 1 30 and learning_rate 0. 1 1Looking at these results and plots we can see that n_estimators should be somewhere above 60 a max_depth of 1 is optimal and the learning_rate should be somewhere between 0. Every leaf has a condition that is either True or False. And finally the fractional dimension worst and standard deviation are also both useful in predicting malignant cell nuclei. 3 points higher Kernel How do I fit the data The kernel decides how the data gets seperated on the hyperplane. For each point of data except diagnosis we get 3 numbers the mean the standard deviation and the worst. The mean of the perimeter also shows a significant increase when a cell nucleus is malignant. By tuning your parameters you can increase or decrease the accuracy of your model. What parameters can we tune The only parameter in this model that we can tune is alpha Default parametersBy default Ridge Classifiers uses these default values alpha 1. The best depths are above 10 according to the plot. It s clear that lower alphas are better in this case so we ll create a longer list with some lower valuesIt s clear in this graph that we won t get a much better prediction than around 0. It seems that the algorithm doesn t matter much this time but it never hurts to have checked. The models are going to predict whether the cell nucleus is malignant or not. max_depthThe integer you set here determines the maximum number of layers your Decision Tree makes. ConclusionBy tuning your models you can squeeze a bit more performance out of them. Now let s take a look at some of the differences in the data between benign and malignant cell nuclei starting with radius and texture. Kernels Parameters Linear C RBF C gamma Poly C gamma degree Default parametersBy default SVC uses these default values kernel rbf C 1. We have 2 types of weight to give to data points uniform All data points are equal distance The closer a data point is the more influential it is to the predictionThe uniform weight apparently suits our data better though it doesn t differ much from the distance weight. Decision Tree Classifier How does it work Decision Trees create a network of leaves or nodes and branches between these leaves. The worst of a data point is the highest largest number of the data for this cell nucleus. Random Forest Classifier10. To view the model s performance we ll make a new DataFrame in which we ll store the Algorithm it s scores and the standard deviation. What parameters can we tune We ll use the following parameters base_estimator n_estimators learning_rateWe have already discussed n_estimators in Extra Trees and learning_rate in Gradient Boosting. In this kernel I ll show you some predictive models and I ll also show you how to tune them. Using the mean of this list we get an average score of the model. Data AnalysisNow that our data has been cleaned we can take a look at what it contains. Random Forest Classifier How does it work The Random Forest Classifier fits a number of randomized Decision Trees on sub samples of the dataset and then uses averaging to get a more accurate prediction. Importing Libraries and Data CleaningBefore we can start trying different algorithms we will have to clean the data we re going to use. Default parameters n_neighbors 5 algorithm auto weight uniform n_neighbors How many points are around me The value of this parameter determines how much points around the target data point are measured to determine the outcome. Default parameters base_estimator DecisionTreeClassifier max_depth 1 n_estimators 50 learning_rate 1 base_estimator What kind of model do I use The base estimator is the model on which the boost is built. Using the graphviz library we ll visualize the end result of the Decision Tree. So a 3D space becomes 2D a 2D space becomes 1D etcetera. It chooses coefficients in this formula based on the data given. 0 Alpha Forcing coefficients toward zeroRidge Classifiers use alpha to force coefficients more less toward zero. Looking at the remaining data we can see this trend continuing for every data point except fractal dimension. What parameters can we tune The Gradient Boosting Classifier that we ll tune are learning_rate n_estimators max_depth. Gradient Boosting Classifier11. This means that approximately 1 out of 3 cell nuclei in the dataset is malignant. When possible we use random_state 0 to ensure fair results. Testing different models with default parametersBefore we start optimising our models we ll first run them with the default parameters. What parameters can we tune The Random Forest parameters that we ll tune are n_estimators max_depth and min_samples_leaf. Ada Boost Classifier How does it work The Ada Boost Classifier builds the same model multiple times but the data gets assigned different weights every time. Max_depth doesn t matter much after 4 layers. com all things ai in depth parameter tuning for gradient boosting 3363992e9bae linear algebra data processing CSV file I O e. We get a lot of data about the cell nucleus radius perimeter area smoothness compactness concavity concave points symmetry fractal dimension and diagnosis. We have already discussed all of these in previous models max_depth min_samples_leaf in Decision Tree and n_estimators in Extra Trees. The big difference is that the Random Forest Classifier chooses the features it will use by the most discriminative thresholds instead of randomly. Gradient Boosting Classifier How does it work The Gradient Boosting Classifier builds multiple Decision Trees just like the Random Forest Classifier but does this in different stages while trying to optimize the loss function. Any values above this will only lead to really slow models. Decision Tree Classifier7. Default parameters n_estimators 10 max_depth None min_samples_leaf 1Let s start by trying N 1 100 Depth 1 30 and Samples 1 5Looking at the plots we can clearly see that n_estimators should be somewhere between 40 and 80 max_depth should be above 5 and samples can vary wildly. The accuracy became around 0. We ll try values between 1 and 3. 1 1Looking at the results and at the heatmap we see that samples isn t really affecting our average score. Optimize Your AlgorithmsDo you ever wonder if you can squeeze some more performance out of your models I certainly do. Guido Scikit learn https scikit learn. 001Now we get some good results out of the SVM. So if you have the time I definitely recommend optimizing your models. Extra Trees Classifier How does it work The Extra Trees Classifier fits a number of randomized Decision Trees on sub samples of the dataset and then uses averaging to get a more accurate prediction. This will ensure more optimal results later on. Algorithm How do I find my neighbors There are 3 types of algorithms to use for finding neighbors ball_tree kd_tree brute force If you give the parameter auto the model will try to find the best model himself. ", "id": "veleon/optimize-your-algorithms", "size": "15335", "language": "python", "html_url": "https://www.kaggle.com/code/veleon/optimize-your-algorithms", "git_url": "https://www.kaggle.com/code/veleon/optimize-your-algorithms", "script": "cross_val_score sklearn naive_bayes sklearn.model_selection seaborn numpy matplotlib.pyplot neighbors gaussian_process pandas tree ensemble linear_model svm ", "entities": "(('Degree', 'hyperplane'), 'turn') (('that', 'condition'), 'have') (('we', 'that'), 'tune') (('we', '0'), 'see') (('it', 'Extra Trees Classifier really model'), 'sound') (('mean doesn', 'cell benign nuclei'), 'see') (('you', 'them'), 'tune') (('s', 'C parameter'), 'AgainNow') (('isn t', 'really average score'), '1looking') (('This', 'more optimal results'), 'ensure') (('001Now we', 'SVM'), 'get') (('Default default Ridge alpha parametersBy Classifiers', 'alpha'), 'tune') (('we', 'default models'), 'have') (('Decision single Tree', 'learning_rate'), '10') (('we', 'Decision Tree'), 'visualize') (('Gradient Boosting Classifier', 'loss function'), 'Classifier') (('we', 'n_neighbors weight'), 'tune') (('Kernels Parameters Linear C RBF C gamma Poly C gamma degree', 'default default Default parametersBy values'), 'use') (('best depths', 'plot'), 'be') (('I', 'models'), 'optimize') (('we', 'data'), 'let') (('SVM', 'data'), 'SVM') (('who', 'about 0'), 'be') (('isn only t', 'cell when malignant nucleus'), 'be') (('boost', 'which'), 'parameter') (('diagnosis', 'data'), 'have') (('at once we', 'optimal values'), 'SamplesLet') (('we', 'Extra Tree Classifier parameters'), 'tune') (('how data', 'hyperplane'), 'Kernel') (('we', 'Random Forest parameters'), 'tune') (('Decision Tree', 'node'), 'tune') (('we', 'around 0'), 's') (('s', 'cell benign nuclei'), 'let') (('mostly around optimal maximum depth', 'mostly 17'), 'show') (('cell approximately 1 out of 3 nuclei', 'dataset'), 'mean') (('we', 'data'), 'have') (('cell when nucleus', 'also significant increase'), 'show') (('I', 'definitely models'), 'so') (('This', 'enough models'), 'be') (('gamma C Penalty 0 auto 1 degree 3 ParameterC', 'penalty error term'), 'be') (('you', 'parameter'), 'allow') (('we', 'more extreme range'), '001') (('So these', 'cell clear nuclei'), 'be') (('The closer coefficients', '0'), 'be') (('it', 'what'), 'AnalysisNow') (('It', 'data'), 'choose') (('Alpha Forcing 0 coefficients', 'more less zero'), 'use') (('time we', '0'), 'use') (('values', 'only really slow models'), 'lead') (('worst', 'cell nucleus'), 'be') (('Max_depth doesn', 't much after 4 layers'), 'matter') (('Decision Trees', 'leaves'), 'Classifier') (('We', 'Gradient Boosting'), 'tune') (('Random Forest Classifier', 'then more accurate prediction'), 'classifier') (('When we', '0 fair results'), 'use') (('Decision how many Trees', 'model'), 'parameter') (('you', 'always accuracy'), 'be') (('we', 'alphas'), 'be') (('finally fractional dimension worst deviation', 'cell also both malignant nuclei'), 'be') (('it', 'data'), 'Classifier') (('multiple times data', 'different weights'), 'Classifier') (('that', 'incorrect predictions'), 'be') (('It', 'training points'), 'control') (('data', 'cell 357 benign 212 malignant nuclei'), 'contain') (('Using', 'tested models'), 'give') (('AdaBoostClassifier', 'best however almost 98 accuracy'), 'be') (('it', 'more zero'), 'be') (('neighbor', 'data equally far away point'), 'be') (('they', 'different neighbors'), 'be') (('we', 'given data'), 'tune') (('we', 'pandas'), 'go') (('We', 'perimeter area smoothness compactness concavity concave points symmetry fractal dimension'), 'get') (('how much points', 'outcome'), 'parameter') (('that', 'original space'), 'be') (('it', 'most discriminative thresholds'), 'be') (('bottom leaves', 'equal 0'), 'show') (('samples', '5'), 'parameter') (('that', 'prediction'), 'Classifier') (('I', 'how them'), 'show') (('we', 'model'), 'get') (('function', 'scores'), 'fit') (('This', 'leaf'), 'send') (('optimal number', 'trees'), 'show') (('However this', 'default just parameters'), 'require') (('We', 'data'), 'drop') (('trend', 'fractal dimension'), 'see') (('correlations', 'earlier conclusions'), 'be') (('then we', 'C back parameter'), 'seem') (('we', 'bit more depth'), 'try') (('Now s', 'radius'), 'let') (('it', 'Algorithm'), 'make') (('values', 'them'), 'use') (('learning_rate', 'somewhere 0'), 'see') (('t', 'distance much weight'), 'have') (('so we', 'part'), 'discuss') (('we', 'default parameters'), 'start') (('Decision Tree', 'layers'), 'determine') (('they', 'model'), 'let') (('they', 'what'), 'try') (('model', 'best model'), 'Algorithm') (('this', 'leaves'), 'improve') (('training data exactly setLet', '0'), 'gamma') (('mean radius', 'cell benign nuclei'), 'be') (('We', 'Extra Trees'), 'discuss') (('kernels', 'different parameters'), 'tune') (('Final now we', 'hard work'), 'ScoreSo') (('32 that', 'NaN values'), 'seem') (('Extra Trees Classifier', 'then more accurate prediction'), 'classifier') (('001 to 1000 we', 'optimal alpha'), 'try') (('standard deviation', 'cell malignant nucleus'), 'be') (('you', 'model'), 'increase') (('We', 'cross validation score'), 'go') (('we', 'standard deviation'), 'get') ", "extra": "['outcome', 'test', 'diagnosis']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "algorithm", "answer", "area", "auto", "average", "basic", "best", "bit", "boolean", "boosting", "bottom", "boundary", "branch", "case", "cell", "check", "classification", "classify", "clean", "clear", "column", "compare", "condition", "correct", "correlations", "could", "create", "csv", "data", "dataset", "decision", "default", "degree", "depth", "diagnosis", "difference", "dimension", "distance", "distribution", "drop", "end", "ensure", "equal", "error", "estimator", "every", "expected", "file", "find", "fit", "fitting", "following", "formula", "function", "gamma", "gini", "gradient", "graph", "graphviz", "heatmap", "high", "image", "increase", "integer", "kernel", "largest", "lead", "leaf", "learn", "learning_rate", "least", "left", "let", "library", "linear", "list", "look", "looking", "lot", "lower", "malignant", "match", "max_depth", "maximum", "mean", "measurement", "model", "most", "multiple", "my", "near", "need", "network", "new", "node", "not", "nuclei", "number", "optimize", "out", "overview", "parameter", "performance", "plot", "point", "predict", "prediction", "processing", "random", "range", "re", "recommend", "result", "right", "run", "sample", "scikit", "score", "set", "similar", "single", "smooth", "space", "split", "standard", "start", "store", "sub", "table", "target", "test", "those", "time", "train", "training", "tree", "trend", "try", "tune", "tuning", "type", "uniform", "up", "user", "validation", "value", "view", "visualize", "weight", "while", "who", "work", "worst"], "potential_description_queries_len": 154, "potential_script_queries": ["ensemble", "numpy", "seaborn", "sklearn", "svm"], "potential_script_queries_len": 5, "potential_entities_queries": ["auto", "clear", "default", "dimension", "gamma", "maximum", "single", "worst"], "potential_entities_queries_len": 8, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 159}