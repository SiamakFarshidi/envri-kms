{"name": "shap values ", "full_name": " h1 Introduction h1 How They Work h1 Code to Calculate SHAP Values h1 Your Turn h2 SHAP values are awesome Have fun applying them alongside the other tools you ve learned to solve a full data science scenario ", "stargazers_count": 0, "forks_count": 0, "description": "The results aren t identical because KernelExplainer gives an approximate result. SHAP values do this in a way that guarantees a nice property. It s cumbersome to review raw arrays but the shap package has a nice way to visualize the results. Where could you use this A model says a bank shouldn t loan someone money and the bank is legally required to explain the basis for each loan rejection A healthcare provider wants to identify what factors are driving each patient s risk of some disease so they can directly address those risk factors with targeted health interventionsYou ll use SHAP Values to explain individual predictions in this lesson. IntroductionYou ve seen and used techniques to extract general insights from a machine learning model. Here is an example using KernelExplainer to get similar results. For this example we ll reuse the model you ve already seen with the Soccer data. This blog post https towardsdatascience. The shap_values object above is a list with two arrays. But the results tell the same story. com kernels fork 1637226. This allows us to decompose a prediction in a graph like this Imgur https i. We won t go into that detail here since it isn t critical for using the technique. Have fun applying them alongside the other tools you ve learned to solve a full data science scenario https www. Feature values causing increased predictions are in pink and their visual size shows the magnitude of the feature s effect. If you look carefully at the code where we created the SHAP values you ll notice we reference Trees in shap. com dansbecker partial plots lessons. Now we ll move onto the code to get SHAP values for that single prediction. There is some complexity to the technique to ensure that the baseline plus the sum of individual effects adds up to the prediction which isn t as straightforward as it sounds. Feature values decreasing the prediction are in blue. Your TurnSHAP values are awesome. In the next lesson you ll see how these can be aggregated into powerful model level insights. Specifically you decompose a prediction with the following equation sum SHAP values for all features pred_for_team pred_for_baseline_values That is the SHAP values of all features sum up to explain why my prediction was different from the baseline. We typically think about predictions in terms of the prediction of a positive outcome so we ll pull out SHAP values for positive outcomes pulling out shap_values 1. png How do you interpret this We predicted 0. How They WorkSHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we d make if that feature took some baseline value. Could use multiple rows if desired package used to calculate Shap values Create object that can calculate shap values Calculate Shap values use Kernel SHAP to explain test set predictions. The team is 70 likely to have a player win the award. com one feature attribution method to supposedly rule them all shapley values f3e04534983d has a longer theoretical explanation. Though the ball possession value has a meaningful effect decreasing the prediction. But the SHAP package has explainers for every type of model. DeepExplainer works with Deep Learning models. So if we answer this question for number of goals we could repeat the process for all other features. Code to Calculate SHAP ValuesWe calculate SHAP values using the wonderful Shap https github. For context we ll look at the raw predictions before looking at the SHAP values. The first array is the SHAP values for a negative outcome don t win the award and the second array is the list of SHAP values for the positive outcome wins the award. png If you want a larger view of this graph here is a link https i. If you subtract the length of the blue bars from the length of the pink bars it equals the distance from the base value to the output. Have questions or comments Visit the course discussion forum https www. com learn machine learning explainability discussion to chat with other learners. An example is helpful and we ll continue the soccer football example from the permutation importance https www. KernelExplainer works with all models though it is slower than other Explainers and it offers an approximation rather than exact Shap values. Convert from string Yes No to binary use 1 row of data here. 7 whereas the base_value is 0. We could ask How much was a prediction driven by the fact that the team scored 3 goals But it s easier to give a concrete numeric answer if we restate this as How much was a prediction driven by the fact that the team scored 3 goals instead of some baseline number of goals. We will look at SHAP values for a single row of the dataset we arbitrarily chose row 5. In these tutorials we predicted whether a team would have a player win the Man of the Match award. com slundberg shap library. But what if you want to break down how the model works for an individual prediction SHAP Values an acronym from SHapley Additive exPlanations break down a prediction to show the impact of each feature. The biggest impact comes from Goal Scored being 2. TreeExplainer my_model. com dansbecker permutation importance and partial dependence plots https www. Of course each team has many features. ", "id": "dansbecker/shap-values", "size": "5284", "language": "python", "html_url": "https://www.kaggle.com/code/dansbecker/shap-values", "git_url": "https://www.kaggle.com/code/dansbecker/shap-values", "script": "sklearn.ensemble sklearn.model_selection RandomForestClassifier numpy pandas train_test_split ", "entities": "(('Calculate Shap values', 'test set predictions'), 'use') (('we', 'arbitrarily row'), 'look') (('that', 'nice property'), 'do') (('how model', 'feature'), 'want') (('we', 'permutation importance https www'), 'be') (('t KernelExplainer', 'approximate result'), 'aren') (('feature', 'baseline value'), 'interpret') (('visual size', 'effect'), 'be') (('player', 'Match award'), 'predict') (('it', 'approximation'), 'work') (('questions', 'course discussion forum https www'), 'have') (('you', 'graph'), 'png') (('as straightforward it', 'which'), 'be') (('Now we', 'single prediction'), 'move') (('health targeted interventionsYou', 'lesson'), 'say') (('how these', 'model level powerful insights'), 'see') (('we', 'other features'), 'repeat') (('Feature values', 'blue'), 'be') (('list', 'award'), 'be') (('We', '0'), 'png') (('it', 'output'), 'subtract') (('likely player', 'award'), 'be') (('biggest impact', 'Goal'), 'come') (('SHAP package', 'model'), 'have') (('us', 'Imgur https i.'), 'allow') (('you', 'Soccer already data'), 'reuse') (('DeepExplainer', 'Deep Learning models'), 'work') (('shap package', 'results'), 's') (('IntroductionYou', 'machine learning model'), 'see') (('you', 'data science scenario https full www'), 'have') (('we', 'SHAP values'), 'look') (('why prediction', 'baseline'), 'decompose') (('team', 'instead baseline goals'), 'ask') (('we', 'shap'), 'look') (('Here example', 'similar results'), 'be') (('here it', 'isn t technique'), 'win') (('ball possession value', 'prediction'), 'have') (('shap_values object', 'above two arrays'), 'be') (('we', 'shap_values'), 'think') ", "extra": "['disease', 'outcome', 'patient', 'positive outcome', 'test']", "label": "Perfect_files", "potential_description_queries": ["answer", "array", "award", "bank", "baseline", "binary", "blog", "calculate", "code", "comparison", "context", "could", "course", "data", "dataset", "decompose", "detail", "directly", "disease", "distance", "effect", "ensure", "equation", "every", "extract", "fact", "feature", "following", "football", "fun", "general", "graph", "importance", "individual", "learn", "learning", "length", "lesson", "level", "link", "list", "loan", "look", "looking", "magnitude", "method", "model", "move", "multiple", "my", "negative", "next", "number", "numeric", "object", "out", "outcome", "package", "partial", "patient", "permutation", "pink", "player", "png", "positive", "post", "prediction", "provider", "question", "raw", "reference", "review", "risk", "row", "scenario", "science", "second", "set", "similar", "single", "size", "string", "subtract", "sum", "team", "technique", "test", "think", "those", "type", "up", "value", "view", "visualize"], "potential_description_queries_len": 94, "potential_script_queries": ["numpy", "sklearn"], "potential_script_queries_len": 2, "potential_entities_queries": ["importance", "level", "science"], "potential_entities_queries_len": 3, "potential_extra_queries": ["test"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 96}