{"name": "build gan assignment 1 ", "full_name": " h1 Your First GAN h3 Goal h3 Learning Objectives h2 Getting Started h4 MNIST Dataset h4 Tensor h4 Batches h2 Generator h2 Noise h2 Discriminator h2 Training ", "stargazers_count": 0, "forks_count": 0, "description": "GeneratorThe first step is to build the generator component. Now you can initialize your generator discriminator and optimizers. Additionally these images are also in black and white so only one dimension or color channel is needed to represent them more on this later in the course. html will be useful here. This output classifies whether an image is fake or real. Don t forget to detach the generator Remember the loss function you set earlier criterion. You will start by creating a function to make a single layer block for the generator s neural network. Finally you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network. It will start with the image tensor and transform it until it returns a single number 1 dimension tensor output. Note that whenever you create a new tensor using torch. Note that each optimizer only takes the parameters of one particular model since we want each optimizer to optimize only one of the models. NoiseTo be able to use your generator you will need to be able to create noise vectors. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. The documentation may be useful if you re less familiar with PyTorch https pytorch. Since multiple images will be processed per pass you will generate all the noise vectors at once. For every batch you will need to update the discriminator and generator using their loss. You need a ground truth tensor in order to calculate the loss. org the machine learning library you will be using. Before you train your GAN you will need to create functions to calculate the discriminator s loss and the generator s loss. In addition be warned that this runs very slowly on a CPU. For example a ground truth tensor for a fake image is all zeros. org docs stable index. One way to run this more quickly is to use Google Colab 1. In general use torch. You will probably find torch. html to map to another shape a batch normalization https pytorch. The same goes for the discriminator it will calculate its loss on the entire batch of generated images as well as on the reals before the model is updated. You should roughly expect to see this progression. png You may notice that the images are quite pixelated this is because they are all only 28 x 28 The small size of its images makes MNIST ideal for simple training. 2 UNQ_C5 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION Discriminator Hint You want to transform the final output into a single value so add one more linear map. The noise vector z has the important role of making sure the images generated from the same class don t all look the same think of it as a random seed. Tensors are a generalization of matrices for example a stack of three matrices with the amounts of red green and blue at different locations in a 64 x 64 pixel image is a tensor with the shape 3 x 64 x 64. For example consider changing the size of the hidden dimension or making the networks shallower or deeper by changing the number of layers. Replace device cpu with device cuda 5. Optional hint for get_noise1. Create generator and discriminator loss functions. You will learn more about this in the following lectures REctified Linear Unit ReLU Leaky ReLU https drive. TensorYou will represent the data using tensors https pytorch. Beginning with the noise vector the generator will apply non linear transformations via the block function until the tensor is mapped to the size of the image to be outputted the same size as the real images from MNIST. Make sure your get_noise function uses the right device But remember don t expect anything spectacular this is only the first lesson. Linear https pytorch. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments. Make the runtime type GPU under Runtime Change runtime type Select GPU from the dropdown 4. zeros where possible. Your First GAN GoalIn this notebook you re going to create your first generative adversarial network GAN for this course Specifically you will build and train a GAN that can generate hand written images of digits 0 9. You are also provided with a visualizer function to help you investigate the images your GAN will create. Batches are sets of images that will be predicted on before the loss functions are calculated instead of calculating the loss function after each image. Sigmoid https pytorch. ones 3 3 device device or move it onto the target device using torch. randn https pytorch. Now you can build the generator class. org docs stable tensors. The final layer does not need a normalization or activation function but does need to be scaled with a sigmoid function https pytorch. com uc export view id 1Qp0aiob39MKPGVw6OecYXBhhlx c MpB Now you can use these blocks to make a discriminator The discriminator class holds 2 values The image dimension The hidden dimensionThe discriminator will build a neural network with 4 layers. Upload it to Google Drive and open it with Google Colab3. org docs stable generated torch. 2 Calculate the generator loss assigning it to gen_loss. After you ve submitted a working version with the original architecture feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. 4 Calculate the discriminator s loss by averaging the real and fake loss and set it to disc_loss. org docs master generated torch. zeros you ll need to pass device device to them. Verify the generator block function Check the three parts Check the output shape UNQ_C2 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION Generator Build the neural network There is a dropdown with hints if you need them Needed for grading Verify the generator class Check there are six modules in the sequential part Check that the output shape is correct UNQ_C3 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_noise NOTE To use this on GPU with device cuda make sure to pass the device argument to the function you use to generate the noise. In GANs and in machine learning in general you will process multiple images per training step. As with the generator component you will start by creating a function that builds a neural network block for the discriminator. You will be using PyTorch in this specialization so if you re not familiar with this framework you may find the PyTorch documentation https pytorch. Each block should include a linear transformation https pytorch. com uc export view id 1ePf0x1EkkK0Ll0V5lxsZQl5aJ1cEOYOr https drive. Verify the noise vector function Make sure a normal distribution was used UNQ_C4 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_discriminator_block Verify the discriminator block function Check there are two parts Check that the shape is right Check that the LeakyReLU slope is about 0. For testing purposes to check that your code changes the generator weights Keep track of the average discriminator loss Keep track of the average generator loss. html so the output can be transformed in complex ways. Note that you do not need a sigmoid after the output layer since it is included in the loss function. The results will get better with later lessons as you learn methods to help keep your generator and discriminator at similar levels. You will learn more about activations and batch normalization later in the course. The output size of the final linear transformation should be im_dim but remember you need to scale the outputs between 0 and 1 using the sigmoid function. Tensors are easy to manipulate and supported by PyTorch https pytorch. TrainingNow you can put it all together First you will set your parameters criterion the loss function n_epochs the number of times you iterate through the entire dataset when training z_dim the dimension of the noise vector display_step how often to display visualize the images batch_size the number of images per forward backward pass lr the learning rate device the device type here using a GPU which runs CUDA not CPUNext you will load the MNIST dataset as tensors using a dataloader. 3 Backprop through the generator update the gradients and optimizer. You will need to fill in the code for final layer since it is different than the others. You do not need to do this if you re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input such as torch. Getting StartedYou will begin by importing some useful packages and the dataset you will use to build and train your GAN. zeros_like instead of torch. It will take 3 values The noise vector dimension The image dimension The initial hidden dimensionUsing these values the generator will build a neural network with 5 layers blocks. DiscriminatorThe second component that you need to construct is the discriminator. Feel free to explore them more but you can imagine these as multi dimensional matrices or vectors BatchesWhile you could train your model after generating one image it is extremely inefficient and leads to less stable training. Important You should NOT write your own loss function here use criterion pred true Multiply Multiply Zero out the gradient before backpropagation Calculate discriminator loss Update gradients Check that they detached correctly Update optimizer Check that some discriminator weights changed UNQ_C7 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_gen_loss These are the steps you will need to complete 1 Create noise vectors and generate a batch of fake images. randn you either need to create it on the target device e. Remember the generator wants the discriminator to think that its fake images are real Important You should NOT write your own loss function here use criterion pred true Multiply Multiply Check that the loss is reasonable UNQ_C8 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION Whether the generator should be tested Dataloader returns the batches Flatten the batch of real images from the dataset Zero out the gradients before backpropagation Calculate discriminator loss Update gradients Update optimizer For testing purposes to keep track of the generator weights Hint This code will look a lot like the discriminator updates These are the steps you will need to complete 1 Zero out the gradients. 2 Get the discriminator s prediction of the fake image and calculate the loss. Build the generator and discriminator components of a GAN from scratch. Make sure to pass the device argument to the noise. These are called batches. detach on the generator result to ensure that only the discriminator is updated Remember that you have already defined a loss function earlier criterion and you are encouraged to use torch. The hints will also often include links to relevant documentation. com uc export view id 1BlfFNZACaieFrOjMv_o2kGqwAR6eiLmN Training dataset Set for testing purposes please do not change UNQ_C1 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_generator_block Hint Replace all of the None with the appropriate dimensions. This means that your generator will generate an entire batch of images and receive the discriminator s feedback on each before updating the model. It contains 60 000 images of handwritten digits from 0 to 9 like these MNIST Digits https upload. Since the generator is needed when calculating the discriminator s loss you will need to call. ones_like and torch. This is how the discriminator and generator will know how they are doing and improve themselves. MNIST DatasetThe training images your discriminator will be using is from a dataset called MNIST http yann. Note that you may see a loss to be greater than 1 this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess. 2 Get the discriminator s prediction of the fake image. html for stabilization and finally a non linear activation function you use a ReLU here https pytorch. 3 Get the discriminator s prediction of the real image and calculate the loss. Remember to pass the device argument to the get_noise function. Note You use leaky ReLUs to prevent the dying ReLU problem which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU which result in a zero gradient. Finally to use your discrimator s neural network you are given a forward pass function that takes in an image tensor to be classified. 3 Calculate the generator s loss. Optional hints for Generator1. 5 minutes MNIST Digits https drive. Train your GAN and visualize the generated images. Finally you can put everything together For each epoch you will process the entire dataset in batches. It s also often the case that the discriminator will outperform the generator especially at the start because its job is easier. On a GPU this should take about 15 seconds per 500 steps on average while on CPU it will take roughly 1. org wikipedia commons 2 27 MnistExamples. It s important that neither one gets too good that is near perfect accuracy which would cause the entire model to stop learning. Needed for grading Verify the discriminator class Check there are three parts Check the linear layer is correct Set your parameters Load MNIST dataset as tensors UNQ_C6 UNIQUE CELL IDENTIFIER DO NOT EDIT GRADED FUNCTION get_disc_loss These are the steps you will need to complete 1 Create noise vectors and generate a batch num_images of fake images. Learning Objectives1. ", "id": "amoghjrules/build-gan-assignment-1", "size": "11022", "language": "python", "html_url": "https://www.kaggle.com/code/amoghjrules/build-gan-assignment-1", "git_url": "https://www.kaggle.com/code/amoghjrules/build-gan-assignment-1", "script": "torch.utils.data __init__ test_get_noise torch make_grid Discriminator(nn.Module) tqdm.auto get_discriminator_block show_tensor_images get_gen DataLoader test_gen_reasonable forward test_disc_reasonable test_disc_loss torchvision test_discriminator nn get_disc matplotlib.pyplot torchvision.datasets test_gen_block get_generator_block get_noise test_disc_block get_gen_loss tqdm Generator(nn.Module) transforms test_generator test_gen_loss torchvision.utils MNIST # Training dataset get_disc_loss ", "entities": "(('you', 'target device e.'), 'randn') (('loss functions', 'image'), 'be') (('that', 'discriminator'), 'start') (('that', 'image tensor'), 'give') (('block', 'transformation https linear pytorch'), 'include') (('so output', 'complex ways'), 'transform') (('it', 'dimension tensor single number 1 output'), 'start') (('One way', 'Google more quickly Colab'), 'be') (('optimizer', 'models'), 'note') (('you', 'torch'), 'detach') (('you', 'noise vectors'), 'be') (('You', 'following lectures'), 'learn') (('It', 'MNIST'), 'contain') (('ground truth tensor', 'fake image'), 'be') (('you', 'batches'), 'put') (('general you', 'training step'), 'process') (('generator', 'layers 5 blocks'), 'take') (('you', 'gradients'), 'remember') (('first step', 'generator component'), 'be') (('job', 'especially start'), 's') (('this', 'very slowly CPU'), 'warn') (('it', 'roughly 1'), 'take') (('right LeakyReLU slope', 'discriminator block function'), 'make') (('model', 'as well reals'), 'go') (('you', 'loss'), 'need') (('spectacular this', 'anything'), 'make') (('you', 'PyTorch documentation https pytorch'), 'use') (('you', 'that'), 'be') (('You', 'roughly progression'), 'expect') (('it', 'others'), 'need') (('it', 'extremely less stable training'), 'be') (('you', 'machine learning library'), 'org') (('you', 'later lectures'), 'be') (('how they', 'themselves'), 'be') (('entire model', 'perfect accuracy'), 's') (('final layer', 'function https sigmoid pytorch'), 'need') (('tensors', 'dataloader'), 'put') (('Tensors', 'PyTorch https pytorch'), 'be') (('you', 'similar levels'), 'get') (('You', 'normal distribution'), 'generate') (('code', 'generator average loss'), 'check') (('you', 'them'), 'zero') (('x all only 28 small size', 'simple training'), 'notice') (('GAN', 'images'), 'provide') (('UNQ_C5 UNIQUE CELL GRADED Discriminator 2 You', 'so one more linear map'), 'IDENTIFIER') (('tensor', 'MNIST'), 'apply') (('also black only one dimension', 'later course'), 'be') (('you', 'fake images'), 'need') (('dimensionThe hidden discriminator', '4 layers'), 'com') (('generator', 'model'), 'mean') (('you', 'earlier criterion'), 'forget') (('you', 'sigmoid function'), 'be') (('TensorYou', 'tensors https pytorch'), 'represent') (('that', '0 9'), 'GoalIn') (('single layer', 'neural network'), 'start') (('you', 'fake images'), 'write') (('you', 'GAN'), 'get') (('Now you', 'generator discriminator'), 'initialize') (('discriminator', 'dataset'), 'be') (('which', 'zero gradient'), 'note') (('you', 'noise'), 'verify') (('networks', 'layers'), 'consider') (('that', 'such torch'), 'need') (('you', 'when loss'), 'need') (('how different architectural choices', 'better GANs'), 'submit') (('that', 'neural network'), 'give') (('you', 'noise vectors'), 'generate') (('whenever you', 'torch'), 'note') (('it', 'loss function'), 'note') (('look', 'random seed'), 'have') (('stack', 'shape'), 'be') (('You', 'batch later course'), 'learn') (('you', 'PyTorch https less pytorch'), 'be') (('3', 'loss'), 'get') (('You', 'loss'), 'need') (('EDIT GRADED DO FUNCTION', 'appropriate dimensions'), 'com') (('cross entropy loss', 'positive sufficiently confident wrong guess'), 'note') (('hints', 'relevant documentation'), 'include') (('linear activation finally non you', 'https ReLU here pytorch'), 'html') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "apply", "architecture", "argument", "average", "backpropagation", "backward", "batch", "batch_size", "binary", "block", "build", "calculate", "case", "cause", "channel", "check", "code", "color", "consider", "correct", "could", "course", "create", "criterion", "cuda", "data", "dataset", "device", "dimension", "display", "distribution", "ensure", "entropy", "epoch", "every", "everything", "explore", "export", "feedback", "fill", "final", "find", "following", "forward", "framework", "function", "general", "generalization", "generate", "generated", "generator", "gradient", "grading", "green", "ground", "hand", "help", "http", "https pytorch", "id", "image", "improve", "include", "initialize", "input", "job", "layer", "lead", "leaky", "learn", "learning", "library", "linear", "load", "look", "lot", "lr", "manipulating", "map", "model", "move", "multiple", "near", "need", "negative", "network", "neural", "new", "noise", "non", "normal", "normalization", "not", "notebook", "number", "open", "optimize", "optimizer", "order", "out", "output", "part", "per", "phenomenon", "pixel", "png", "positive", "pred", "prediction", "prevent", "problem", "random", "re", "result", "right", "role", "run", "runtime", "sampling", "scale", "scaled", "second", "set", "shape", "sigmoid", "similar", "single", "six", "size", "something", "stack", "standard", "start", "step", "target", "tensor", "testing", "think", "through", "track", "train", "training", "transform", "transformation", "type", "under", "until", "update", "value", "variation", "vector", "version", "view", "visualize", "while", "write"], "potential_description_queries_len": 157, "potential_script_queries": ["forward", "nn", "torch", "torchvision", "tqdm"], "potential_script_queries_len": 5, "potential_entities_queries": ["block", "entropy", "linear", "number", "single"], "potential_entities_queries_len": 5, "potential_extra_queries": ["test"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 162}