{"name": "understanding cross lingual models ", "full_name": " h1 Introduction h1 Setup TPU configuration h1 Part 1 Understanding cross lingual models h1 XLM is based on several key concepts h1 Training a Masked Language Model MLM for BERT h1 Unsupervised Cross lingual Representation Learning at Scale h1 Part 2 Implementation using TPU Multiprocessing h1 More Text Cleaning h1 Tokenize encode comments h1 Focal Loss h1 Build the model and check summary h1 Visualize model architecture h1 Learning rate schedule h1 Training h1 Make Submission ", "stargazers_count": 0, "forks_count": 0, "description": "Clean the text remove usernames and links More Text Cleaning applying text cleaning techniques like clean_text replace_typical_misspell handle_contractions fix_quote on train test and validation set we can see from above 2 cells that text cleaning for train validation and test set takes 8 minutes that means we are losing some of our vital times for training on tpu which is 3 hours max. com wp content uploads 2019 03 transformercomparison. png The encoding component is a stack of encoders the paper stacks six of them on top of each other there s nothing magical about the number six one can definitely experiment with other arrangements. com maroberti fastai with transformers bert roberta Jigsaw Multilingual Toxicity EDA Models https www. Introduction In this kernel i will try to share my understanding and findings of cross lingual models. Feel free to correct me if I made any mistakes in this kernel. These are the language the XLM model supports en es fr de zh ru pt it ar ja id tr nl pl simple fa vi sv ko he ro no hi uk cs fi hu th da ca el bg sr ms bn hr sl zh_yue az sk eo ta sh lt et ml la bs sq arz af ka mr eu tl ang gl nn ur kk be hy te lv mk zh_classical als is wuu my sco mn ceb ast cy kn br an gu bar uz lb ne si war jv ga zh_min_nan oc ku sw nds ckb ia yi fy scn gan tt am. 2 for Urdu over the previous XLM model. We also present a detailed empirical evaluation of the key factors that are required to achieve these gains including the trade offs between 1 positive transfer and capacity dilution and 2 the performance of high and low resource languages at scale. png The next step would be to convert this into a sequence of integers and create PyTorch tensors of them so that we can use them directly for computation Notice that we have set MASK at the 8th index in the sentence which is the word Hensen. The model outperforms other models in a cross lingual classification task sentence entailment in 15 languages and significantly improves machine translation when a pre trained model is used for initialization of the translation model. net profile Jeremy_Barnes5 publication 309312650 figure fig1 AS 669424235323406 1536614583578 The process of cross lingual sentiment classification We assume that the opinion units_W640. com blog 2019 07 pytorch transformers nlp python First let s prepare a tokenized input from a text string using BertTokenizer This is how our text looks like after tokenization https cdn. A Transformer includes two parts an encoder that reads the text input and generates a lateral representation of it e. 07291 by Facebook AI named XLM presents an improved version of BERT to achieve state of the art results in both classification and translation tasks. io images t Transformer_decoder. The exact same feed forward network is independently applied to each position. com wp content uploads 2019 07 Screenshot from 2019 07 18 15 18 42. com max 1400 0 lBYVNRe1esIXn1qE. net publication 309312650_Exploring_Distributional_Representations_and_Machine_Translation_for_Aspect based_Cross lingual_Sentiment_Classification Fastai with Transformers BERT RoBERTa. png The encoder s inputs first flow through a self attention layer a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. com slide 12311059 73 images 13 Cross lingual Document Classification. 02116 shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross lingual transfer tasks. 25 version 8 More text cleaning everything else is left same as version 7 so that we can compare version 7 with version 8 s result for additional text cleaning clean_text replace_typical_misspell handle_contractions fix_quote version 9 as you can see now that in version 7 lb 0. Let s now use BertForMaskedLM to predict a masked token This was a small demo of training a Masked Language Model on a single input sequence. Each training sample consists of the same text in two languages whereas in BERT each sample is built from a single language. We will make XLM R code data and models publicly available. com tarunpaparaju jigsaw multilingual toxicity eda models Flower Classification with TPUs EDA and Baseline https www. As in BERT the goal of the model is to predict the masked tokens however with the new architecture the model can use the context from one language to predict tokens in the other as different words are masked words in each language they are chosen randomly. The model then should predict the original value of the masked words based on the context provided by the other non masked words in the sequence. The new metadata helps the model learn the relationship between related tokens in different languages. In a machine translation application it would take a sentence in one language and output its translation in another. We assume that the opinion units have already been determined. com facebookresearch XLM The Illustrated Transformer https jalammar. The upgraded BERT is denoted as Translation Language Modeling TLM while the vanilla BERT with BPE inputs is denoted as Masked Language Modeling MLM. check the below pictures https www. Make Submission References Jigsaw TPU XLM Roberta https www. io images t the_transformer_3. 9151 where the only difference in version 9 was to add extra text cleaning techniques now in version 9 i will move to xlmr large model with maxlen 192 BATCH_SIZE 16 strategy. How XLM works The paper Cross lingual Language Model Pretraining https arxiv. Our model dubbed XLM R significantly outperforms multilingual BERT mBERT on a variety of cross lingual benchmarks including 13. the Positional Encoding separately. validation and testing dataset Focal Loss Build the model and check summary Define Define ReduceLROnPlateau callback Visualize model architecture Learning rate schedule Trainingwe train it for 2 more epochs on the validation set which is significantly smaller but contains a mixture of different languages. png The below animation wonderfully illustrates how Transformer works on a machine translation task https cdn. io images t The_transformer_encoder_decoder_stack. Part 2 Implementation using TPU MultiprocessingEven though i am a pytorch lover but not sure if the video below is true for 2020 also or not. 3 average F1 score on MLQA and 2. com tarunpaparaju jigsaw multilingual toxicity eda models XLM R handles the following 100 languages Afrikaans Albanian Amharic Arabic Armenian Assamese Azerbaijani Basque Belarusian Bengali Bengali Romanized Bosnian Breton Bulgarian Burmese Burmese Catalan Chinese Simplified Chinese Traditional Croatian Czech Danish Dutch English Esperanto Estonian Filipino Finnish French Galician Georgian German Greek Gujarati Hausa Hebrew Hindi Hindi Romanized Hungarian Icelandic Indonesian Irish Italian Japanese Javanese Kannada Kazakh Khmer Korean Kurdish Kurmanji Kyrgyz Lao Latin Latvian Lithuanian Macedonian Malagasy Malay Malayalam Marathi Mongolian Nepali Norwegian Oriya Oromo Pashto Persian Polish Portuguese Punjabi Romanian Russian Sanskri Scottish Gaelic Serbian Sindhi Sinhala Slovak Slovenian Somali Spanish Sundanese Swahili Swedish Tamil Tamil Romanized Telugu Telugu Romanized Thai Turkish Ukrainian Urdu Urdu Romanized Uyghur Uzbek Vietnamese Welsh Western Frisian Xhosa Yiddish. io illustrated transformer Exploring Distributional Representations and Machine Translation for Aspect based Cross lingual Sentiment Classification https www. jpg here L1 means language 1 and L2 means language 2 https slideplayer. io images t Transformer_encoder. A win win for everyone in NLP. only the predecessors of each word in 2018 updated BERT used the Transformer s encoder to learn a language model by masking dropping some of the words and then trying to predict them allowing it to uses the entire context i. Now that our data is rightly pre processed for BERT we will create a Masked Language Model. io images t The_transformer_encoders_decoders. a vector for each word and a decoder that produces the translated text from that representation. The model also receives the language ID and the order of the tokens in each language i. Check out the below comparison https cdn. The decoder has both those layers but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence similar what attention does in seq2seq models. The process of cross lingual sentiment classification. XLM is based on several key concepts Transformers The Transformer architecture is at the core of almost all the recent major developments in NLP. 07291 presents two innovative ideas a new training technique of BERT for multilingual classification tasks and the use of BERT as initialization of machine translation models. A High Level Look Let s begin by looking at the model as a single black box. 1 average F1 score on NER. com blog 2019 03 pretrained models get started nlp Introduction to PyTorch Transformers An Incredible Library for State of the Art NLP with Python code https www. We ll look closer at self attention later in the post. com wp content uploads 2019 03 transform20fps. We train a Transformer based masked language model on one hundred languages using more than two terabytes of filtered CommonCrawl data. num_replicas_in_sync and keeping everything else as it was before version 10 as we see version 9 model diverged and it seems like loss focal_loss gamma 1. 0 for focal loss reducing train non toxic comment samples removing translated validation data from train set patience 1 and epoch 6 bug version 4 same as version 3 but trying to fix the problem for 3 epoch i was not monitoring val auc correctly in version 3 version 5 error version 6 patience 2 extra 2 epochs using validation set version 7 using translated validation data focal_loss gamma 1. 25 worked just fine for xlmr base but not with large so will just try binary_crossentropy instead just to make sure whether or not my assumption is correct leaving everything else as it is saving model based on maximum validation accuracyand using EarlyStopping ModelCheckpoint LearningRateScheduler training for 4 epochs version 11 trying to solve error of version 10 version 12 trying to solve bugs of version 11 version 13 adding translated spanish data in training set version 14 oversampling validation english data reducing spanish non english sample a bit for training for 5 epochs within 3 hours tpu limit failed couldn t finish commit within 3 hours tpu limit version 15 trying for 4 epoch and oversampling positive samples of translated validation set version 16 more data for training 958870 total and using 2 epochs because it will take time Note one thing i found in this competition is large subset of trainset helps the learning algorithm perform better here instead of choosing small subset for long training version 17 in version 16 you can see i got NotImplementedError after first epoch and i am unable to save best checkpoint in previous versions i also tried monitor val_acc and monitor val_accuracy but none of them saving best checkpoint for me where am i making mistakes in version 17 i will try to get rid of the error and will train again if you know why i am unable to save best checkpoint please help me in the comment box thanks in advance Imports Setup TPU configuration Part 1 Understanding cross lingual modelsthe paper titled Cross lingual Language Model Pretraining https arxiv. com blog 2019 07 pytorch transformers nlp python facebookresearch XLM https github. This is what our model will try to predict. ChangeLog version 1 training xlm roberta base for 1 epoch including validation english data in training set version 2 training for 4 epoch poor lb close to 0. 8 average accuracy on XNLI 12. so it would be a good idea if we create another kernel and save above 2 cells newly updated train val and test_data as kernels output then using those files we can quickly import our new train test and validation data here which will save time for training model on TPU Roc Auc Evaluation metric Tokenize encode comments Load bert tokenizer Encode comments previously we lost 8 minutes and here 12 minutes sum them and we have lost 20 minutes which is almost 1 epoch training time here. jpg First instead of using word or characters as the input of the model it uses Byte Pair Encoding BPE that splits the input into the most common sub words across all languages thereby increasing the shared vocabulary between languages. implementation is adapted from tarunpaparaju s kernel Jigsaw Multilingual Toxicity EDA Models https www. com miklgr500 jigsaw tpu bert with huggingface and keras 8 Excellent Pretrained Models to get you Started with Natural Language Processing NLP https www. The model significantly outperforms other prominent models Unsupervised Cross lingual Representation Learning at ScaleAbstract This paper Unsupervised Cross lingual Representation Learning at Scale https arxiv. The Spanish test set is mapped accordingly and the classifier is tested on this cross lingual test set. gif The vanilla Transformer has only limited context of each word i. png Popping open that Optimus Prime goodness we see an encoding component a decoding component and connections between them. png The encoders are all identical in structure yet they do not share weights. The complete XLM model was trained by training both MLM and TLM and alternating between them. com dimitreoliveira flower classification with tpus eda and baseline notebook. words to the left and right of a masked word. The outputs of the self attention layer are fed to a feed forward neural network. Second it upgrades the BERT architecture in two manners 1. png This Transformer architecture outperformed both RNNs and CNNs convolutional neural networks. png To assess the contribution of the model the paper presents its results on sentence entailment task classify relationship between sentences using XNLI dataset that includes sentences in 15 languages. com xhlulu jigsaw tpu distilbert with huggingface and keras Jigsaw TPU BERT with Huggingface and Keras https www. 8987 and in version 8 we got lb 0. 8 in XNLI accuracy for Swahili and 9. XLM R performs particularly well on low resource languages improving 11. The computational resources required to train models were reduced as well. Each one is broken down into two sub layers https jalammar. It introduced an attention mechanism that processes the entire text input simultaneously to learn contextual relations between words or sub words. XLM uses a known pre processing technique BPE and a dual language training mechanism with BERT in order to learn relations between words in different languages. The English train set is used to train a classifier. The decoding component is a stack of decoders of the same number. For quick demonstration purpose i will use code from analyticsvidhya s article Introduction to PyTorch Transformers An Incredible Library for State of the Art NLP with Python code https www. 89 version 3 gamma 2. Finally we show for the first time the possibility of multilingual modeling without sacrificing per language performance XLM Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. Training a Masked Language Model MLM for BERTlet s say the problem statement is Given an input sequence we will randomly mask some words. com xhlulu jigsaw tpu xlm roberta Jigsaw TPU DistilBERT with Huggingface and Keras https www. ", "id": "mobassir/understanding-cross-lingual-models", "size": "16537", "language": "python", "html_url": "https://www.kaggle.com/code/mobassir/understanding-cross-lingual-models", "git_url": "https://www.kaggle.com/code/mobassir/understanding-cross-lingual-models", "script": "sklearn.metrics Translator PCA clean_text tensorflow.keras.regularizers tensorflow.keras.callbacks Embedding callback EarlyStopping Dropout tensorflow.keras.models torch.nn make_subplots optimizers ReduceLROnPlateau sklearn.decomposition plotly.express textblob Image kaggle_datasets wordcloud tensorflow.keras.backend Timestamp tensorflow.keras Callback Conv1D backend as K RocAucEvaluation(Callback) SVG googletrans nltk.sentiment.vader tensorflow.keras.optimizers nltk.stem.wordnet WordNetLemmatizer tensorflow.keras.layers KMeans focal_loss_fixed TfidfVectorizer replace_typical_misspell on_epoch_end transformers regular_encode seaborn numpy gensim.models Input model_to_dot replace networkx ModelCheckpoint constraints metrics IPython.core.display tqdm.notebook stats keras.utils pandas tensorflow word_tokenize nltk.stem WordCloud nltk.corpus colorama Model plotly.figure_factory HTML matplotlib.cm TweetTokenizer tensorflow.keras.initializers stopwords tensorflow.keras.activations Detector TextBlob BertForMaskedLM build_model pytorch_transformers lrfn Adam shuffle sklearn.feature_extraction.text Back Word2Vec GRU build_lrfn PIL BertWordPieceTokenizer tensorflow.keras.constraints accuracy_score CSVLogger clean_data scipy Style focal_loss roc_auc_score backend LearningRateScheduler clean Fore BertTokenizer IPython.display _get_mispell datetime nltk sklearn.utils __init__ wordnet sklearn.cluster LSTM plotly.graph_objects polyglot.detect init BertModel date initializers clean_numbers handle_contractions sklearn matplotlib.pyplot Dense SpatialDropout1D fix_quote activations plotly.subplots tqdm SentimentIntensityAnalyzer torch.nn.functional regularizers nltk.tokenize tokenizers layers KaggleDatasets TreebankWordTokenizer nltk.tokenize.treebank STOPWORDS ", "entities": "(('We', 'later post'), 'look') (('Second it', 'two manners'), 'upgrade') (('XLM R', '11'), 'perform') (('it', 'specific word'), 'png') (('we', 'Masked Language Model'), 'create') (('com blog', 'Python code https www'), 'start') (('which', '20 minutes'), 'be') (('that', 'languages'), 'first') (('model', 'Unsupervised Representation Scale https Cross lingual arxiv'), 'outperform') (('Finally we', 'GLUE benchmarks'), 'show') (('This', 'input single sequence'), 'let') (('vanilla Transformer', 'word'), 'gif') (('Khmer Korean Kurdish Kurmanji Kyrgyz Macedonian Malagasy Malay Malayalam Marathi Mongolian Nepali Norwegian Oriya Oromo Scottish Gaelic Serbian Sindhi Sinhala Slovak Slovenian Somali Sundanese Swahili Swedish Tamil Tamil Telugu Latin Latvian Lithuanian Pashto Persian Polish Portuguese Punjabi Romanian Russian Telugu', 'Hungarian Icelandic Japanese Javanese Kannada Indonesian Irish Italian Kazakh'), 'com') (('exact same feed forward network', 'independently position'), 'apply') (('i', 'validation 7 translated data'), '0') (('computational resources', 'models'), 'reduce') (('i', 'cross lingual models'), 'introduction') (('they', 'weights'), 'png') (('com xhlulu jigsaw', 'Huggingface'), 'tpu') (('sample', 'single language'), 'consist') (('We', 'CommonCrawl filtered data'), 'train') (('XLM', 'different languages'), 'use') (('model', '13'), 'dub') (('we', 'randomly words'), 'train') (('train English set', 'classifier'), 'use') (('attention', 'seq2seq models'), 'have') (('com pytorch 2019 07 transformers', 'python facebookresearch XLM https github'), 'blog') (('9 i', 'maxlen 192 BATCH_SIZE 16 strategy'), '9151') (('Submission References Jigsaw', 'XLM Roberta https www'), 'make') (('that', 'words'), 'introduce') (('accordingly classifier', 'cross test lingual set'), 'map') (('language', 'https 1 language 2 slideplayer'), 'mean') (('it', 'entire context'), 'use') (('07291', 'machine translation models'), 'present') (('that', '15 languages'), 'png') (('when pre trained model', 'translation model'), 'outperform') (('that', 'scale'), 'present') (('how text', 'tokenization https cdn'), 'blog') (('encoding component', 'decoding them'), 'png') (('which', 'sentence'), 'png') (('model', 'sequence'), 'predict') (('io', 'Machine Sentiment Classification https Aspect based Cross lingual www'), 'illustrate') (('it', 'loss'), 'num_replicas_in_sync') (('which', 'different languages'), 'Build') (('BERT', 'Masked Language Modeling MLM'), 'denote') (('07291', 'classification tasks'), 'present') (('9 you', '7 lb 0'), 'leave') (('which', 'tpu'), 'clean') (('that', 'representation'), 'vector') (('s', 'single black box'), 'let') (('Transformer architecture', 'NLP'), 'base') (('ChangeLog training xlm roberta version 1 base', 'close 0'), 'set') (('model', 'what'), 'be') (('Transformer png architecture', 'RNNs convolutional neural networks'), 'outperform') (('decoding component', 'same number'), 'be') (('outputs', 'feed forward neural network'), 'feed') (('model', 'different languages'), 'help') (('implementation', 'kernel'), 'adapt') (('pretraining', 'cross transfer lingual tasks'), 'show') (('Excellent Pretrained 8 you', 'Natural Language Processing NLP https www'), 'bert') (('it', 'another'), 'take') (('s', 'six definitely other arrangements'), 'png') (('one', 'sub layers https two jalammar'), 'break') (('Imports Setup', 'Understanding modelsthe Part 1 cross lingual paper'), 'work') (('I', 'kernel'), 'feel') (('model', 'language'), 'receive') (('gan tt', 'en de it'), 'be') (('opinion', 'sentiment cross lingual classification'), 'figure') (('How XLM', 'paper lingual Language Model Pretraining https Cross arxiv'), 'work') (('that', 'it'), 'include') (('XLM complete model', 'them'), 'train') (('i', 'Python code https www'), 'use') (('wonderfully how Transformer', 'machine translation task https cdn'), 'png') (('they', 'language'), 'be') (('pytorch video', 'below 2020'), 'implementation') (('com xhlulu jigsaw', 'keras Huggingface'), 'tpu') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "advance", "algorithm", "animation", "application", "architecture", "art", "article", "ast", "auc", "average", "baseline", "bert", "best", "bg", "bit", "blog", "bn", "box", "check", "checkpoint", "classification", "classifier", "classify", "cleaning", "close", "code", "comment", "compare", "comparison", "competition", "computation", "content", "context", "convert", "convolutional", "core", "correct", "create", "data", "dataset", "decoder", "demonstration", "difference", "directly", "eda", "en", "encode", "encoder", "encoding", "epoch", "error", "evaluation", "everyone", "everything", "experiment", "fastai", "feed", "figure", "filtered", "fix", "flow", "flower", "focal", "following", "forward", "found", "gamma", "gan", "help", "high", "hr", "hu", "id", "idea", "implementation", "import", "including", "index", "input", "io", "kernel", "key", "ko", "language", "layer", "learn", "learning", "left", "let", "look", "looking", "lost", "major", "mask", "masked", "masking", "max", "maximum", "metadata", "metric", "mixture", "ml", "model", "most", "move", "my", "network", "neural", "new", "next", "nlp", "nn", "no", "non", "none", "not", "number", "open", "opinion", "order", "out", "output", "per", "perform", "performance", "png", "positive", "possibility", "pre", "predict", "prepare", "present", "pretrained", "problem", "processing", "profile", "publication", "purpose", "python", "pytorch", "range", "relationship", "remove", "representation", "result", "right", "sample", "save", "saving", "scale", "schedule", "score", "sentence", "sentiment", "sequence", "set", "several", "similar", "single", "six", "sk", "slide", "sq", "stack", "state", "step", "string", "structure", "sub", "subset", "sum", "summary", "task", "technique", "test", "testing", "text", "those", "through", "time", "token", "tokenization", "total", "train", "training", "transfer", "transformer", "try", "understanding", "val", "validation", "value", "vanilla", "vector", "version", "video", "while", "word", "xhlulu"], "potential_description_queries_len": 200, "potential_script_queries": ["backend", "clean", "colorama", "date", "datetime", "init", "lrfn", "networkx", "nltk", "numpy", "replace", "scipy", "seaborn", "shuffle", "sklearn", "stem", "tensorflow", "tokenize", "tqdm", "wordnet"], "potential_script_queries_len": 20, "potential_entities_queries": ["convolutional", "filtered", "language", "png", "pytorch", "task", "training", "version"], "potential_entities_queries_len": 8, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 217}