{"name": "getting started image processing basics ", "full_name": " h2 Basic Solution ", "stargazers_count": 0, "forks_count": 0, "description": "where function to encode all pixels with an intensity value the threshold value as 1 and all other pixels as 0. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. The nuclei that are closer to one another get clustered to form one nuclei. In that respect KMeans may work better on images that aren t particularly grayscale or on images where there is no sharp contrast in the intensity values between objects of interest and the background What are some useful edge detection algorithms to create boundaries between nuclei that are extremely close to one another How do we as humans identify objects in an image We indeed take it for granted but if we think of objects as anything that has a fixed shape and size and is prominent with respect to the background what is the technical or computer definition of these terms This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. The optimal seperation value is somewhere around 20 but rather than relying on such descriptive statistics we should take a more formal approach such as using Otsu s method. Transforming the colorspace removes all color information leaving only the luminance of each pixel. com rakhlin fast run length encoding python for developing this function that has been used by many Kagglers for the purpose of this competition. label function example of 1 1 1 0 0 1 1 running RLE would give us 0 3 5 2 which means 3 pixels from the zeroth pixel inclusive and 2 pixels from the 5th pixel we see a series of 1sCredit to Kaggle user rahlkin https www. label encodes every object starting from number 1 To see one such mask Function for rle encoding. T sets Fortran order down then right Running RLE on the last label_mask in label_array gives us Now defining a function that does is applicable to all images Reading the image To extract the image ID Reading it in OpenCV Converting everything to grayscale from BGR To remove the background Using Otsu s method to seperate the foreground objects from the background Coding objects with intensity values higher than background as 1 Extracting connected objects labels gives us the label of the different objects in every image starting from 1 and nlabels gives us the total number of objects in every image Iterating through every object label Individual masks for every nucleus RLE for every mask Return the dataframe Defining a function that takes a list of image paths pathlib. Therefore these three values must be combined into a single value when removing color from an image. We reduced a dimension when we transformed from the BGR colorspace to grayscale. Our job here is to seperate the two that is seperate the nuclei from the background. Since digital images are displayed using a combination of red green and blue RGB colors each pixel has three separate luminance values. However a more careful look suggests that the mask hasn t found out all the nuclei especially the two in the top right corner. e for one nucleus in the other column. All in all the two major problems in this image are Insignificant spots dots being labelled as nuclei. This is the default colorpsace in which images are read in OpenCV. Otsu s method named after Nobuyuki Otsu is used to automatically perform clustering based image thresholding or the reduction of a graylevel image to a binary image. So for example if 1 1 1 0 0 1 1 was our row vector using ndimage. For a deeper explanantion as to why this could have happened one may read D Liu s paper http ieeexplore. We can try out something different i. label on this would give us 1 1 1 0 0 2 2 signifying the fact that there are 2 distinct objects in the row vector. What is RLE RLE or Run Length Encoding converts a matrix into a vector and returns the position starting point of the first pixel from where we observe an object identified by a 1 and gives us a count of how many pixels from that pixel we see the series of 1s. Why did this happen This is because grayscale is a range of monochromatic shades from black to white. Therefore a grayscale image contains only shades of gray and no color i. The result of this function will be stored in a variable called maskWe see that the mask has done a decent job. 201 which gives us our baseline accuracy. The darker coloured nuclei are causing a problem as the pixels that represent these nuclei have intensity values lesser than Otsu s threshold value. org document 5254345 reload true where he has compared K Means with Otsu s method Object identification To get a count of the total number of nuclei we can use the ndimage. Luminance can also be described as brightness or intensity which can be measured on a scale from black zero intensity to white full intensity Thus this displays one entire row of the image matrix with the corresponding luminance or intensities of every pixel Removing the background We can see that there are 2 prominent peaks. For more information https en. Some important questions to ask are Will we achieve a satisfactory performance by converting all pictures to grayscale What are the different types of pictures in the dataset What are the numerous ways to seperate the background from objects of interest Otsu s method requires computing a graylevel histogram for us to find the optimum seperation value. Otsu s method exhibits relatively good performance if the histogram can be assumed to have bimodal distribution and assumed to possess a deep and sharp valley between two peaks source https en. org wiki RGB_color_space Basic SolutionIt is important to understand the structure of the image here. org wiki Otsu 27s_method Now we ll use the np. It s extrememly hard to tell if there s a difference. So we need to seperate them using some edge detection algorithm like convolution with a sobel filter or canny edge detector as suggested by Ramsu Now for this competition we need to have a seperate mask for every nucleus. Run Length Encoding Every mask for every nucleus requires an RLE vector. Any layer of complexity that we add onto this should better this score failing which it is absolutely useless. Now there could be more nuclei than that as some nuclei have been combined into one and our mask hasn t been able to identify all the nuclei especially the ones in the top right corner. read_csv Input data files are available in the. e a different picture in future work. This is the format required by the competition. Reading the image We will be using the same example. In simpler terms An RGB color can be understood by thinking of it as all possible colors that can be made from three colored lights for red green and blue. These spots should have their labels KMeans Otsu set to 0 if their sizes are too small. The 1s represent 1 such object nucleus in the entire picture. However the 2 seperate spots in the top right corner get labelled as 2 different objects. If the result is 1 it means there is no difference at allThere is no difference at all. The algorithm assumes that the image contains two classes of pixels following bi modal histogram foreground pixels and background pixels it then calculates the optimum threshold separating the two classes so that their combined spread intra class variance is minimal or equivalently so that their inter class variance is maximal. com stkbailey teaching notebook for total imaging newbies so as to make the concepts simpler to understand. Around the 500 400 mark the three nuclei have been all combined together to form one cluster. In the file named stage1_train_labels. zip we have the image IDs in one column and the Run Length Encoded RLE vector for one such mask i. The function returns the labeled array and the number of distinct objects it found in the array. Importing the other necessary libraries Importing OpenCV the computer vision library Glob the training data and load a single image path To read the image To get rid of the x ticks and y ticks on the image axis To see the structure of the image let s display one row of the image matrix To transfrom the colorspace from BGR to grayscale so as to make things simpler To plot the image cmap has been used as matplotlib uses some default colormap to plot grayscale images To get rid of the x ticks and y ticks on the image axis To understand this further let s display one entire row of the image matrix Okay let s look at the distribution of the intensity values of all the pixels This is to flatten the matrix and put the intensity values of all the pixels in one single row vector To zoom in on the distribution and see if there is more than one prominent peak To plot the original image and mask side by side Let s see if K Means does a good job on this data 2 as we re still trying to seperate the lighter coloured nuclei from the darker coloured background To check if there s any difference Since we need to create a seperate mask for every nucelus let s store the masks in an iterable like a list We need to iterate from 1 as ndimage. Path objects analyzes each and returns a submission ready DataFrame We ll use this for the test images Creating one dataframe for every image Appending all these dataframes Returing the submission ready dataframe Final submission. The count of pixels with intensity values around 0 is extrememly high 250000. We have a third dimension as every pixel is represented by it s B G and R components. Putting everything together The submission scored 0. If these images were to appear in a newspaper column titled Spot the difference between except the obvious colour difference it would have had people scratch their heads in frustration. We would expect this to occur as the nuclei cover a smaller portion of the picture as compared to the background which is primarily black. A particular BGR RGB color space is defined by the three chromaticities of the red green and blue additive primaries and can produce any chromaticity that is the triangle defined by those primary colors. This problem has been caused by some nuclei that have pixels where the intensity values are lesser than Otsu s threshold value thus causing only some pixels to have their label encoded as 1. For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output. label function which labels features pixels in an array based on their interconnectedness. Let s see if there is any difference by comparing the labels of Otsu and K Means at a pixel level summing over the booleans and dividing them by the total number of pixels in the image. e it primarily contains only black and white. The image has been read in the BGR colorspace. In this notebook I ve tried to break down Stephen Bailey s fantastic notebook https www. ", "id": "akshayt19nayak/getting-started-image-processing-basics", "size": "8237", "language": "python", "html_url": "https://www.kaggle.com/code/akshayt19nayak/getting-started-image-processing-basics", "git_url": "https://www.kaggle.com/code/akshayt19nayak/getting-started-image-processing-basics", "script": "scipy basic ndimage seaborn numpy matplotlib.pyplot skimage.filters sklearn.cluster rle KMeans pandas list_of_images subprocess check_output threshold_otsu ", "entities": "(('that', 'red green'), 'understand') (('We', 'same example'), 'read') (('it', 'which'), 'well') (('together submission', 'everything'), 'put') (('s', 'image'), 'let') (('1s', 'entire picture'), 'represent') (('mask', 'decent job'), 'store') (('that', 'threshold lesser value'), 'cause') (('label', '1'), 'cause') (('We', 'ndimage'), 'import') (('grayscale Therefore image', 'gray'), 'contain') (('we', 'nucleus'), 'need') (('grayscale', 'white'), 'happen') (('labels', 'interconnectedness'), 'function') (('images', 'OpenCV'), 'be') (('that', 'background'), 'be') (('pixel', 'B G components'), 'have') (('which', 'baseline accuracy'), '201') (('count', '0'), 'be') (('us', 'seperation optimum value'), 'be') (('that', 'primary colors'), 'define') (('one', 'D paper ieeexplore'), 'read') (('We', 'background'), 'describe') (('It', 'python docker image https kaggle github'), 'work') (('that', 'image paths pathlib'), 'set') (('concepts', 'imaging total newbies'), 'com') (('org wiki Now we', 'np'), 'Otsu') (('ready DataFrame We', 'ready dataframe Final submission'), 'analyze') (('Insignificant spots dots', 'nuclei'), 'be') (('However 2 seperate spots', '2 different objects'), 'label') (('Transforming', 'pixel'), 'remove') (('Therefore three values', 'image'), 'combine') (('mask hasn one t', 'especially top right corner'), 'be') (('we', 'ndimage'), 'reload') (('histogram', 'peaks source https two en'), 'exhibit') (('mask hasn t', 'especially top right corner'), 'suggest') (('you', 'output'), 'list') (('it', 'primarily only black'), 'contain') (('which', 'background'), 'expect') (('Basic RGB_color_space SolutionIt', 'image'), 'be') (('I', 'Stephen notebook https ve fantastic www'), 'try') (('pixel', 'luminance three separate values'), 'have') (('inter class variance', 'two classes'), 'assume') (('that', 'one nuclei'), 'cluster') (('when we', 'grayscale'), 'reduce') (('sizes', '0'), 'have') (('three nuclei', 'together one cluster'), 'combine') (('we', 'Kaggle user rahlkin https www'), 'give') (('that', 'competition'), 'run') (('read_csv Input data files', 'the'), 'be') (('we', '1s'), 'convert') (('This', 'competition'), 'be') (('zip we', 'Run Length Encoded RLE mask one such i.'), 'have') (('it', 'array'), 'return') (('Nobuyuki Otsu', 'binary image'), 'method') (('Run Length mask', 'RLE vector'), 'encoding') (('we', 'such method'), 'be') (('label', 'row 2 distinct vector'), 'give') (('people', 'frustration'), 'be') (('image', 'BGR colorspace'), 'read') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["algorithm", "appear", "approach", "array", "background", "baseline", "binary", "canny", "check", "close", "clustering", "color", "colored", "column", "combined", "competition", "computer", "contrast", "convolution", "could", "count", "create", "current", "data", "dataframe", "dataset", "default", "detection", "detector", "difference", "dimension", "directory", "display", "distribution", "document", "edge", "encode", "encoding", "environment", "every", "everything", "extract", "fact", "file", "filter", "find", "fixed", "following", "foreground", "form", "format", "found", "function", "future", "gray", "grayscale", "green", "high", "histogram", "http", "image", "input", "intensity", "inter", "interest", "job", "kaggle", "label", "labeled", "labelled", "layer", "length", "let", "level", "library", "linear", "list", "load", "look", "major", "mask", "matplotlib", "matrix", "method", "need", "no", "notebook", "nuclei", "number", "object", "order", "out", "path", "peak", "people", "perform", "performance", "picture", "pixel", "plot", "point", "position", "problem", "processing", "purpose", "python", "range", "re", "read", "remove", "result", "right", "rle", "row", "run", "running", "scale", "score", "scratch", "separate", "set", "several", "shape", "side", "single", "size", "sobel", "something", "source", "space", "spread", "store", "structure", "submission", "test", "think", "those", "threshold", "thresholding", "through", "total", "training", "try", "user", "value", "variable", "variance", "vector", "vision", "work", "write", "zip", "zoom"], "potential_description_queries_len": 153, "potential_script_queries": ["basic", "ndimage", "numpy", "scipy", "seaborn"], "potential_script_queries_len": 5, "potential_entities_queries": ["dataframe", "kaggle", "notebook", "separate", "total"], "potential_entities_queries_len": 5, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 157}