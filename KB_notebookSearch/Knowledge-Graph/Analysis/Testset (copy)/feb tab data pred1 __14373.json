{"name": "feb tab data pred1 ", "full_name": " h2 1 Import relevant libraries h2 2 TPU Check h2 3 Read in The Data Files h2 4 Checking for missing Values h2 5 Let s see the summary info of the data h2 6 Check Cardinality of Categorical Variables h2 7 Pinpointing rare categories in categorical variables h2 8 Visualize the relationship between Categorical variables and Target h2 9 One Hot Encoding Categorical Variables h2 10 Drop categorical columns with 10 or less value contribution per variable h2 11 Visualizing The Relationship between Numerical Variables and Target h2 12 Correlation Strength h2 13 Checking for Multi collinearity h2 14 Applying Variance Inflation Factor h2 15 Dropping Target Variable h2 16 Applying Box Cox Transformation to Original Dataset h2 17 Visualizing Variable Normality using a Q Q Plot h2 18 Exploring Outliers h2 19 Applying Standardization to Train Copy dataset h4 train stdize data has less than 3 outliers to dataset ratio Let s try mean normalization h2 20 Applying Mean Normalization to Original Dataset h4 The decision is between Box Cox and Mean Norm They both have a perfect range across all features between 0 and 1 But Mean Norm balances the data better with zero deviation from the mean of 0 and slightly lower deviation from the std of 1 than Box Cox Also Box cox is just slightly better on outliers as compared to Mean norm But since I d treat outliers soon I so far prefer the centralised distribution of Mean norm h4 Standardization is not an option because even though it perfectly balances the distribution of all variables at a MEAN of 0 and STD of 1 it causes a chaotic range of distribution and an outlier ratio worse than Box cox and similar to Mean norm Let s try a couple more h2 21 Applying Robust Scaling to Train copy h3 X scaled X X median X quantile 0 75 X quantile 0 25 h2 22 Applying Scaling to Vector Unit Length L1 Norm h2 23 Applying Scaling to Vector Unit Length L2 Norm h2 24 Summarizing Feature Scaling Normalization Choices h2 25 Winsorization to Address Outliers h2 26 Kurtosis AKA The 4th Statistical Moment h2 27 Performing Data Transformations on Test set h4 We also need to transform the test set on values learnt on the training set The functions below perform all the transformations we ve been doing for specific activation functions in addition to reducing the train test and target datasets and returning these fit for machine learning h3 Note that each transformation function must only contain one key word amongst other words Key words are h3 Instantiate an instance of the FinalPrep class passing if we want One Hot Encoding or Not and the type of transformation for the final test and train data sets h2 28 Saving the pre processed train target and test sets ", "stargazers_count": 0, "forks_count": 0, "description": "We can see the general bell curve shape of the distribution from the histogram. According to the IQR proximity rule a value is an outlier if it falls outside these boundaries Upper boundary 75th quantile IQR 1. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory import os for dirname _ filenames in os. 5 Here IQR is given by the following equation IQR 75th quantile 25th quantile Let s randomly plot two variables we might see some outliers. Dropping Target VariableMost machine learning models work better with a normalized data set. A typical strategy involves setting outliers to a specified percentile. Plus the MEAN dist and STD dist are worse than Mean norm but better than Boxcox. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. 25 Let s see how far away values are from the MEAN and STD We Can see that the distribution range of pure numerical values for Robust norm is uneven and chaotic like Z Score norm going from 0 up to almost 5. Here we want to grab the rare cat column names just as they would appear after the O H E so that we can drop them off as individual columns from cat_vars dataframe immediately after applying O H E. This is the result of the mean norm we did earlier. 0 to capture them all We can see that by removing outliers certain categorical variables basically have no more distribution. walk kaggle input for filename in filenames print os. We can identify these by the much lighter colors in the matrix. Finally we can see the thick dotted ouliers above point 4on the y axis of the Box plot indicating the presence of outliers. While those of categorical values are within 0 to 1. L1 for vector unit length L1 5. Let s see the summary info of the data 6. The higher the Kurtosuis measure is the more outliers are present and the longer the tails in the distribution of the histogram are Let s return to the mean_norm_df data and apply kurtosis Now let s apply the winsorizer First let s calculate the kurtosis score and see if it s gone down. In this case all VIF scores are just between 1 and 2. We d apply the inter quartile range IQR proximity rule. So let s continue by keeping these variables. Let s consider the Z score or Standardization method The MEAN distance and STD distance above show the sum total of how far the numeric features in the DataFrame in this case train boxcox are far away from a MEAN of 0 and STD of 1 Let s see the distribution range of values after box cox The range of distribution above from the box cox transformation is just awesomely perfect Even though the MEAN and STD are not around 0 or 1 17. Though the data is more central around the mean the range is uneven and chaotic. Applying Scaling to Vector Unit Length L1 Norm. Performing Data Transformations on Test set We also need to transform the test set on values learnt on the training set. When scaling variables to the median and quantiles the median value is deducted from the observations and the result is divided by the inter quartile range IQR. Let s re check the distance of means from 0 and stds from 1 in mean norm df The percentage of outliers to data size has also reduced from 3. Just as suspected all have a min equal min values and max values. 5 the IQR Well it turns out that the percent of outliers to total data size is 3 for train boxcox. Visualize the relationship between Categorical variables and Target Let s investigate the relationship between cat0 and Target As much as possible we want to have categorical variables in a column that seem to have distinct relationship with target not too similar hence they lose their predictive power. Let s see the outliers train_stdize data has less than 3 outliers to dataset ratio. On occasions outliers are very informative for example when looking for credit card transactions an outlier may be an indication of fraud. ROBUST for robust scaling 4. Applying Robust Scaling to Train copy This is also called scaling with median and quantiles. This procedure involves subtracting the MEAN from each observation and then dividing the result by the difference between the minimum and maximum values Let s see how far the numeric values are from the MEAN and STD The decision is between Box Cox and Mean Norm. join dirname filename You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session Just incase we need it for median range or Robust Scaling for vector unit length norm for the Q Q plots to display the total number columns present in the dataset detect TPUs TPU detection detect GPUs for GPU or multi GPU machines For the 1st and 2nd cat vars For the 3rd and 4th cat vars For the 5th and 6th cat vars For the 7th and 8th cat vars For the 9th and 10th cat vars Average target for cat0 Average target for cat1 Average target for cat2 Average target for cat3 Average target for cat4 Average target for cat5 The original column names of Categorical columns drop former cat column names since O H E Double check for NAN values after join Let s delete cat_vars from memory plot both together to compare For cont0 and cont1 For cont2 and cont3 For cont4 and cont5 For cont6 and cont7 For cont8 and cont9 For cont10 and cont11 For cont12 and cont13 Let s remove the correlation of same to same columns 1. Let s try mean normalization. BOX for boxcox activation 2. Read in The Data Files. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. 5 and this is not enough to drop the columns so we continue. But Mean Norm balances the data better with zero deviation from the mean of 0 and slightly lower deviation from the std of 1 than Box Cox. Applying Mean Normalization to Original Dataset In mean normalization we center the variable at zero and rescale the distribution to the value range. Kurtosis indicates the outlier content within the data. While normal outliers are outliers 1. Summarizing Feature Scaling Normalization Choices Looking at the above charts for choice of Feature Norm Scaling let s create a function that basically summarizes the key similarities or differences of each choice in a table. Link https github. Typically we consider a label to be rare when it appears in less than 5 or 1 of the population 8. Standardization is not an option because even though it perfectly balances the distribution of all variables at a MEAN of 0 and STD of 1 it causes a chaotic range of distribution and an outlier ratio worse than Box cox and similar to Mean norm. read_csv Input data files are available in the read only. In other cases outliers are rare observations that do not add any additional value. Let s check the cardinality of each Cat variable and delete those with high cardinality. Let s try to reposition the data to have mean 0 and Std 1. Checking for Multi collinearity 14. 5 Lower boundary 25th quantile IQR 1. Drop categorical columns with 10 or less value contribution per variable Let s first make a copy of the original train set and use this for the transformations Merge both dataframes. Also Box cox is just slightly better on outliers as compared to Mean norm. Count the cardinality values per categorical column Let s see one of the variables Let s make a plot with the cardinality of each categorical variable Let s define a method that plots each Cat feature and a threshold of percentage importance per Cat feature value to each Cat feature. In a Q Q plot we plot the quantiles of the variable in the y axis and the expected quantiles of the normal distribution in the x axis. Note that this scaling technique scales the feature vector as opposed to each individual variable. Checking for missing Values 5. They both have a perfect range across all features between 0 and 1. concat will return NAN values. If the outcome is 1 it s okay. Winsorization to Address Outliers. Finally reduce the target size and save it. When scaling to vector unit length we divide each feature vector by its norm using either l1 manhattan dist or l2 euclidean dist norm. Pinpointing rare categories in categorical variablesCategories that appear in a tiny proportion of the observations are rare. So let s use a simple join since they both have the same index col. Let s investigate further with a correlation matrix Clearly there is no linear relationship between each numerical variable and TargetThe only way to learn any meaningful representation is to use a non linear style regression One more important thing we can learn from the corr matrix is that some columns may be highly correlated or multi collinearity issues. A feature vector contains the values of each variable for a single observation. Applying Variance Inflation Factor We shall use VIF to determine the overall columns with high multi collinearity and seive them out. Here we use the Power Transform Function to Normalize the data First let s visualize the current shapesBox_cox can t work with negative values so let s confirm if we have negative values in the training set So we shall use standard scaler with the box cox transformation Let s see the normalized features Box Cox has done a good transformation with the shape of the data but looking at the numeric variables they are not centred around zero. Robust scaling produces more robust estimates for the center and value range of the variable and is recommended if the data contains outliers just like our present data X_scaled X X_median X. Let s look at variables cont8 and cont2 of mean_norm_df. Yep it s gone down from 245 to a mere 41 because the winsorizer has fixed a lot of the outliers in the data the only outlies left may warrant us to be more strict with our IQR rate down to 1. Double check for possible NAN values after join 11. I d create a function that takes a dataframe and the factor default is 1. Let s print their min and max values to be certain. We observe this through the boxplot as well as the average comparison to the target variable done above for each cat column. Applying Scaling to Vector Unit Length L2 Norm. Let s see the average target score per categorical variable per column 9. Visualizing Variable Normality using a Q Q Plot Normality can be also assessed by Q Q plots. 5 but can be changed to use in the IQR calculation and returns the IQR proximity rule boundaries Let s find the extreme outliers for train boxcox these are outliers 3 times the IQR rate. Applying Standardization to Train Copy dataset. Let s create a Winsorizer object to Cap outliers based on the same Inter Quantile Range we specified earlier Next we d fit_transform the training data using the winsorizer and then transform the test data soon with the learned parameters from the training data using the transform function of the Winsorizer But first let s sample a few variables and see the outliers before and after applying winsorization. This makes Robust norm not yet an ideal choice over Mean Norm. 0 Let s remove the target column from numcols Select only Numeric cols first scale values to range 1 2 Then apply boxcox transform After boxcox scale back to range 0 1 convert the array back to a dataframe convert the array back to a dataframe confirm no missing values from transformation For example let s see the cont10 variable if it s normally distributed The blue dots should adjust to the 45 degree line For example let s see the cont10 variable if it s normally distributed The blue dots should adjust to the 45 degree line Select only numeric cols Select only numeric cols confirm no missing values from transformation For example let s see the cont10 variable if it s normally distributed The blue dots should adjust to the 45 degree line Select only numeric cols Let s learn the means Let s learn the ranges Fit the learned means and ranges to the train set If test also fit it on the test set confirm no missing values from transformation set up the scaler fit the scaler to the train set it will learn the parameters Fit to only numerical columns convert back to df transform testset convert back to df confirm no missing values from transformation norm takes a str value of l1 or l2 set up the scaler fit transform the scaler to the train set it will learn the parameters fit on only numeric columns convert back to df transform testset convert back to df confirm no missing values from transformation confirm no missing values from transformation function to create histogram Q Q plot and boxplot of specific variables function takes a dataframe df and the variable of interest as arguments define figure size histogram Q Q plot boxplot Let s look at variable cont8 before applying winsorization Let s look at variable cont8 before applying winsorization Let s see the data Let s see the normalized data ensure activation is all lower case Reducing Num_cols to Float 32 Reducing Cat_cols to Int 32 if OHE. com Lawrence Krukrubo Understanding_Multiple_Linear_Regression blob master coefficients_of_multiple_linear_regression. Applying Box Cox Transformation to Original Dataset. 33 refers to the few original numerical variables that have moderate outliers around 1. Standardization is also called Z Score Norm Let s see how far away each numeric feature s MEAN and STD is from 0 and 1 We can see that there is virtually no distance between the features MEAN and STD of train_stdize as impress as this is let s see the range of distribution. The functions below perform all the transformations we ve been doing for specific activation functions in addition to reducing the train test and target datasets and returning these fit for machine learning. Import relevant libraries 2. Kurtosis AKA The 4th Statistical Moment Now just before we fit the Winsorizer on the dataset let s compute the Kurtosis score of the data which is a score that computes how much outliers are in the dataset. An outlier is a data point that is significantly different from the remaining data. L2 for vector unit length L2 6. We need to treat all such columns 13. But since I d treat outliers soon I so far prefer the centralised distribution of Mean norm. Since both dataframes have different columns using pd. Visualizing The Relationship between Numerical Variables and Target 12. We shall use the box cox normalization for the numerical columns 16. Note that each transformation function must only contain one key word amongst other words. STANDARD for standardization Instantiate an instance of the FinalPrep class passing if we want One Hot Encoding or Not and the type of transformation for the final test and train data sets. If the variable follows a normal distribution the dots in the Q Q plot should fall in a 45 degree diagonal line. Let s try a couple more. MEAN for mean norm activation 3. One Hot Encoding Categorical Variables Each column of values seem too alike and it makes no sense to keep all. ipynb First we make the unique high corr data a dataframe Next we standardize the data Next we apply the VIF The VIF is a measure of colinearity among predictor variables within a multiple regression. This data does not have a general MEAN of 0 and STD of 1. We can see that the dotted outliers have disappeared after applying winsorization to the vectorl1_df data 26. When scaling to vector unit length we transform the components of a feature vector so that the transformed vector has a length of 1 or in other words a norm of 1. We can also see the 45 deg line and the blue dots that roughly keep to the red line of the Q Q Plot Probability Plot this also indicates presence of a normal distribution. we d drop Cat O H E columns that don t have significance up to a certain threshold we choose. Winsorization or winsorizing is the process of transforming the data by limiting the extreme values that is the outliers to a certain arbitrary value closer to the mean of the distribution. It seems these variables are just around 0 all through. Check Cardinality of Categorical Variables. Correlation Strength Looking at the regplot for each numerical variable and Target there is a general weak linear relationship. Saving the pre processed train target and test sets. These are the values whose relative values were dropped for being smaller than the threshold we set earlier. Winsorizing is different from trimming because the extreme values are not removed but are instead replaced by other values. If it s between 1 and 5 it shows low to average colinearity and above 5 generally means highly redundant and variable should be dropped. Running the cell below repeatedly plots different pairs of variables. ", "id": "black9t/feb-tab-data-pred1", "size": "14373", "language": "python", "html_url": "https://www.kaggle.com/code/black9t/feb-tab-data-pred1", "git_url": "https://www.kaggle.com/code/black9t/feb-tab-data-pred1", "script": "sklearn.metrics __init__ category_freq RobustScaler  # for median/range or Robust Scaling deepcopy plot_range train_test Path diagnostic_plots _keep_same_cols MinMaxScaler drop_rare_cat_cols _apply_transform variance_inflation_factor statsmodels.stats.outliers_influence apply_boxcox_scaler mean_squared_error seaborn numpy sklearn.pipeline mean_std_distance pathlib outliers_toDF box_plots scipy.stats copy _reduce_datasets reg_plots find_boundaries sklearn.model_selection plot_hist matplotlib.pyplot SparkSession tensorflow pandas Pipeline Normalizer  # for vector unit-length-norm feature_engine.outliers standardize norm_scale_summary calc_kurtosis PowerTransformer robust_scaler mean_norm sklearn.preprocessing FinalPrep(object) clean_winsorizer _winsorization vector_unit_scaler train_test_split pyspark.sql standardize_numCols _copy_test Winsorizer ", "entities": "(('certain categorical variables', 'basically more distribution'), '0') (('dots', '45 degree diagonal line'), 'follow') (('we', 'H immediately E.'), 'want') (('We', 'them'), 'apply') (('FinalPrep class Instantiate we', 'data sets'), 'STANDARD') (('Box Also cox', 'Mean norm'), 'be') (('we', 'x axis'), 'plot') (('that', '1'), 'refer') (('rare that', 'additional value'), 'be') (('we', 'l1 manhattan dist'), 'divide') (('transformed vector', 'norm 1'), 'transform') (('we', 'machine learning'), 'perform') (('percent', 'train boxcox'), '5') (('how much outliers', 'dataset'), 'AKA') (('result', 'inter quartile range IQR'), 'deduct') (('Visualizing', 'Q Q also plots'), 'assess') (('VIF scores', 'case'), 'be') (('Running', 'variables'), 'plot') (('s', 'data'), 'let') (('We', 'training set'), 'need') (('it', 'kurtosis score'), 'be') (('So s', 'variables'), 'let') (('we', 'threshold'), 'be') (('This', 'Mean Robust yet ideal Norm'), 'make') (('We d', 'inter quartile range IQR proximity rule'), 'apply') (('input directory', '_ os'), 'list') (('data', 'just present data'), 'produce') (('percentage', 'also 3'), 'let') (('only outlies', 'down 1'), 'go') (('This', 'also median'), 'call') (('we', 'mean norm'), 'be') (('VIF', 'multiple regression'), 'make') (('we', 'outliers'), 'give') (('Kurtosis', 'data'), 'indicate') (('these', 'train boxcox'), 'change') (('they', 'index same col'), 'let') (('first s', 'before winsorization'), 'let') (('activation', 'Int'), '0') (('decision', 'Box Cox'), 'involve') (('data that', 'significantly remaining data'), 'be') (('too similar they', 'predictive power'), 'visualize') (('data', '1'), 'have') (('we', 'certain threshold'), 'd') (('They', '0'), 'have') (('s', 'column'), 'let') (('we', 'columns'), 'be') (('We', 'matrix'), 'identify') (('columns', 'corr matrix'), 'let') (('that', 'observations'), 'be') (('transformation function', 'other words'), 'note') (('Dropping', 'data better normalized set'), 'work') (('s', 'mean_norm_df'), 'let') (('s', 'high cardinality'), 'let') (('soon I', 'Mean norm'), 'prefer') (('extreme values', 'instead other values'), 'be') (('that', 'table'), 'let') (('s', '0'), 'let') (('too alike it', 'all'), 'variable') (('scaling technique', 'individual variable'), 'note') (('Just all', 'min min equal values'), 'have') (('those', '1'), 'be') (('outlier', 'fraud'), 'be') (('just awesomely Even MEAN', 'box cox above transformation'), 'let') (('typical strategy', 'specified percentile'), 'involve') (('Z Score norm', 'up almost 5'), 'let') (('s', 'same columns'), 'join') (('dotted outliers', 'data'), 'see') (('feature vector', 'single observation'), 'contain') (('this', 'normal distribution'), 'see') (('it', 'Upper boundary IQR'), 'be') (('that', 'distribution'), 'be') (('it', 'Mean norm'), 'be') (('We', 'numerical columns'), 'use') (('Mean Norm', 'Box Cox'), 'balance') (('s', 'min values'), 'let') (('We', 'cat above column'), 'observe') (('data', 'ratio'), 'let') (('between it', '5'), 'show') (('Finally we', 'outliers'), 'see') (('dataframes', 'pd'), 'have') (('they', 'zero'), 'use') (('We', 'histogram'), 'see') (('s', 'Merge dataframes'), 'drop') (('range', 'more mean'), 'be') (('we', 'value range'), 'apply') (('factor default', 'dataframe'), 'create') (('read_csv Input data files', 'read'), 'be') (('It', 'kaggle python Docker image https github'), 'come') (('MEAN dist', 'Boxcox'), 'be') (('s', 'distribution'), 'call') (('when it', 'population'), 'consider') (('that', 'Cat feature'), 'count') ", "extra": "['outcome', 'test', 'procedure']", "label": "Perfect_files", "potential_description_queries": ["adjust", "appear", "apply", "array", "average", "blob", "boundary", "box", "boxplot", "calculate", "calculation", "case", "cat", "categorical", "cell", "center", "check", "choice", "column", "compare", "comparison", "compute", "consider", "contain", "content", "convert", "copy", "corr", "correlation", "create", "credit", "current", "curve", "data", "dataframe", "dataset", "decision", "default", "define", "degree", "detect", "detection", "df", "diagonal", "difference", "directory", "display", "dist", "distance", "distributed", "distribution", "drop", "ensure", "environment", "equal", "equation", "even", "expected", "factor", "feature", "figure", "file", "filename", "final", "find", "fit", "fixed", "following", "function", "general", "grab", "high", "histogram", "https github", "image", "import", "importance", "index", "individual", "info", "input", "instance", "inter", "interest", "join", "kaggle", "key", "l1", "l2", "label", "learn", "learning", "left", "length", "let", "line", "linear", "list", "load", "look", "looking", "lot", "lower", "matrix", "max", "maximum", "mean", "measure", "median", "memory", "method", "might", "min", "minimum", "missing", "multiple", "need", "negative", "no", "non", "norm", "normal", "normalization", "normalized", "not", "number", "numeric", "numerical", "object", "observation", "option", "out", "outcome", "outlier", "output", "overall", "per", "percent", "percentage", "perform", "plot", "point", "population", "pre", "predictor", "present", "print", "procedure", "processing", "python", "range", "rare", "ratio", "re", "read", "reduce", "regression", "relationship", "relative", "remove", "representation", "rescale", "result", "return", "robust", "run", "running", "sample", "save", "scale", "scaler", "scaling", "score", "sense", "session", "set", "several", "shape", "similar", "single", "size", "standard", "standardization", "std", "str", "strategy", "style", "sum", "summary", "target", "technique", "test", "those", "threshold", "through", "total", "train", "training", "transform", "transformation", "try", "type", "under", "unique", "unit", "up", "value", "variable", "vector", "version", "visualize", "walk", "word", "work", "write"], "potential_description_queries_len": 215, "potential_script_queries": ["deepcopy", "numpy", "pathlib", "seaborn", "sklearn", "tensorflow"], "potential_script_queries_len": 6, "potential_entities_queries": ["categorical", "degree", "equal", "min", "present", "range"], "potential_entities_queries_len": 6, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 220}