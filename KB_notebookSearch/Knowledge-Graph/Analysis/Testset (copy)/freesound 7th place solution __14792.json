{"name": "freesound 7th place solution ", "full_name": " h1 Freesound Audio Tagging 2019 h2 Contents h2 Data Augmentation h3 pitch shift h3 Fade h3 Reverb h3 Treble and Bass h3 Equalizing h2 Crop Policy h3 Uniform Crop h3 Strength Adaptive Crop h2 Model Structure h3 Custom CNN h4 kernel size h4 The nuber of convolution blocks h4 Cutting output feature map to arbitrary size h3 Inception v3 h2 Augmentation in Batch h3 blend image augmentation h2 Training Strategy h3 Pretrain with Noisy Data h3 Training with Curated Data h3 TTA for Validation h2 Ensemble h2 What didn t work ", "stargazers_count": 0, "forks_count": 0, "description": "The reason is simple the size was to large. Specific CNN structures CoordConv https arxiv. Ensemble Ensemble Data AugmentationWe created augmented training dataset with sox http sox. htmlSo We got mixed Image dataWe define below function to apply to above strategy in our training Training StrategyThis is our pipline. io en latest source augmenters. Fourth Convolution Block channel 128 256 4 1 pooling. Sox is an open source synthesis tool which is easy to use from command line interface and is installed to Kaggle kernel by default. io en stable reference Image. We used normal KFold to get cross validation and split the dataset into 5 subsets. The cycle was 10 epochs. We should not allow patterns translate along the frequency axis. Cutting the feature map to arbitrary size. We started from improving CVSSP Baseline https www. pdf to catch the local features. Then We introduce Strength Adaptive Crop. com omalleyt s 2nd place solution for Tensorflow Speech Recognition Competition very helpful. Split noisy data with train_test_split. With this trick CV lwlrap score improved 0. In this block we ll get the primitive pattens connected in the frequency axis. We altered the model structure like the model below. pdf We thought that the position in the image was very important in this competition and tried CoordConv because it says it can handle the position but we found it not useful. We also applied pitch shift to the noisy data. As explained in this blog post https t. We borrowed nice implementation from https github. Crop Policy Crop_Policy Uniform Crop Uniform_Crop Strength Adaptive Crop Strength_Adaptive_Crop 3. We took careful not to aggregate the feature map along to the time axis and to the frequency axis simultaneously. Treble and BassThis augmentation emphasize or weaken treble and bass range of the sound. The original audio data and its melspectrogram representation are below. One thing to note is that we didn t used augmented dataset for validation. com daisukelab fat2019_prep_mels1 We used noisy data for warm up just like pre training the model and curated data for fine tuning the model which was trained in earlier stage with noisy data. get mix data blend_arr_x blend_arr_y idx_ex _get_blend_arr x_trn y_trn lam np. Spatial and Channel Squeeze and Exitation Added both pool. com c freesound audio tagging 2019 discussion 89382 latest 534349. Cutting output feature map to arbitrary sizeWe found that the position in the feature map is important when treating spectrogram. com kaggle competitions kaggle 10700 logos header. com knjcode pytorch finetuner GaussianNoise https imgaug. When we train the model with curated data we used all of these augmented features so the size of the train dataset was 8 times larger 7 augmentations 1 original than the original curated dataset. Strength Adaptive CropMany sound crip has important information at the first few seconds. Here s one of the kernel which created some of the augmented features with train_curated dataset. At first it wa cropped uniformly as mhiro s kernel. TTA for ValidationIt is very effective to use tta because we use random crop. We used mhiro2 s way https www. com mhiro2 simple 2d cnn classifier with pytorch https www. Clearly spectrogram has different axis and we need to treat them separately. None of us had experience with audio data and we also didn t get used to use Nerual Networks. Fifth Convolution Block channel 256 512 1 3 pooling. FadeThis augmentation alter the volume of the sound. With pre training local lwlrap jumped from 0. Validation Stratified K fold for Multilabel Models DenseNet XceptionNet SE Inception ResNet ResNeXt SE ResNeXt MobileNet ConvLSTM CapsuleNet We ve tried many models but we didn t find them better than Custom CNN or Inception v3 model. In short our solution is 3 models ensemble one is Custom CNN and the other two are Inception v3 models. Here s the table which summarize how much each augmentation worked on the score. pitch shiftThis augmentation changes the pitch of the sound. ReverbThis augmentation add reverberation to the original sound. com phalanx for these implementations. com hidehisaarai1213 freesound sox pitch fade reverb treble basWe created 7 augmented datasets whose size are 4970 each which means the same size as curated one. We prepare strength_list in advance and apply strength adaptive crop in FATTrainDataset. Training with Curated DataWe used curated data to finetune the models which were pre trained with noisy data. We added some modification in the ConvBlock and the model structure. In this block we ll get the primitive patterns connected to the patterns nearby in the time axis. Augmentation in Batch blend image augmentationWe used below augmentation strategy in batchhttps pillow. Data Augmentation Data_Augmentation pitch shift pitch_shift fade Fade reverb Reverb treble and bass Treble_and_Bass equalizing Equalizing 2. html additivegaussiannoise SpecAugment http ai. Some sample has it in the middle of crip. We used Log Melspectrogram features made with the manner introduced by daisukelab in his great dataset. We simply used the same indices for splitting original dataset and augmented datasets. com sailorwei fat2019 2d cnn with mixup lb 0 673We ve created a kernel to explain team kaggler ja Shirogane s solution. EqualizingThis augmentation adjust custom frequency range. We tried both policy and found it surprising that policy 2 was slightly better than policy 1. 03247 OctaveConv https arxiv. We used those augmentation which certainly alter what it sounds like but still can be recoginized as the same class. But that isn t the case when the degree of the translation is not so big. optimize BCEWithLogitLossWe also used augmented data in this stage. We applied both pitch up and pitch down. We applied these augmentations to the whole curated data. When RandomResizedCrop is used val score fluctuate so if val tta is not used an appropriate epoch can not be selected. We were helped a lot with public kernels below shared by them. We can use sox like this. com c tensorflow speech recognition challenge forums t 47715 2nd place solution forumMessageId 270205 post270205The design philosophy behind the placement of convolution blocks are as follows. Since the size of each augmented dataset is all the same as the original one which means 4970 and the order of them are all the same it was easy to avoid data leak. score augmentation no aug no warm up pitch down fade reverb treble and bass down no warm up pitch down fade reverb treble and bass down all CV we don t remember 0. Third Convolution Block channel 64 128 1 3 pooling. Training Strategy Training_Strategy Pretrain with Noisy Data Pretrain_with_Noisy_Data Training with Curated Data Training_with_Curated_Data TTA for Validation TTA_for_Validation 6. We believe it was just a seed magic but the model trained with policy12 were at least as better as the model trained with policy1. The score drops about 0. html All the above had no impact on CV. Pretrain with Noisy DataWe used noisy data to pre train our model. 725 Crop Policy Uniform CropWe use random crop because we use fixed image size 128 128. We only used train_noisy dataset for validation. com daisukelab for his clear instruction with great kernels and datasets mhiro2 https www. 88 with Custom CNN and 0. Finaly we decided to use 20 TTA for each model and blend 5 fold 3models 15model with equally weighted manner. We created a pitch augmented dataset from train_noisy dataset and used it only for training. In the pre training of Inception models we applied RandomResizedCrop to the data and it required us to train longer so we introduced stage 2 pre training. co IgUKfFOYaa moving the pattern in the frequency can be problematic. Note that this score was calcurated with Custom CNN model and the effect of the augmentation varies between models. Also we faced the problem of Blend Many Models vs Increasing the number of TTA. Model Structure Custom CNNAt first we worked a lot on our custom CNN model. We also added spatial and channel attention. To treat the position in the feature map we used spatial pyramid pooling https arxiv. 04 when learning only the last few seconds. One is all the same as the original Inception v3 architecture so we simply used the one in torchvision. All the works for this competition were done with kaggle kernels and we d like to thank a lot to kaggle for giving us great opportunities and computing resources. This method is very effective because most samples contain important information in places where the sound is loud. Train with all the noisy data validate with curated data. EnsembleWe end up with simply blending the output with the same weight which means CustomCNN Inception v3 3ch Inception v3 1ch 3 but we tried a lot to use many models with comparatively low scores around 0. Especially since we use random resized crop tta is very important. png Thank you for all the competitors kaggle teams and the host of this competition We enjoyed a lot during the competition and learned many things. So we used tta for validation to ensure that validation can be properly evaluated. com 2019 04 specaugment new data augmentation. We put this block to collect primitive patterns of the log melspectrogram. com mhiro2 simple 2d cnn classifier with pytorch to train which means change learning rate with CosineAnnealingLR cyclickally in the range between 1e 3 and 1e 5. Freesound Audio Tagging 2019 Logo https storage. The score of the 1ch version was not as stable as 3ch version but we found that model a little bit different from the original one. Ensemblemany models Inception CustomCNN v1 CustomCNN v2 Se ResNeXt DenseNet sec to make time steps 128 500 cent pitch down 500 cent pitch up fade in and out cyclically emphasize weaken Stack X as X X X Standardize Normalize to 0 255 Just zero all 0 batch_size feature_maps time_stpes batch_size feature_maps Attention Layers changed kernel size we don t know why but this is better than 3 3 kernels. We used augmented dataset with original train_curated dataset which made the whole training dataset 8 times larger than the original one. Xception model may have been better but we couldn t train it properly as we didn t have enough time to try it in the last period of the competition. Both train and validate are done with only noisy data. Augmentation in Batch Augmentation_in_Batch blend image augmentation blend_image_augmentation 5. Inception v3We used two Inception v3 models. com mhiro2 for sharing excellent training framework and sailorwei https www. com hidehisaarai1213 convnet noisy all pitch pretraining. 825 CV warm up 0. First Convolution Block channel 1 32 no pooling. Second Convolution Block channel 32 64 4 1 pooling. However due to the nature of recording it is rare to have important information at th end of sounds. This block allows patterns to move in the time axis. What didn t work Augmentation CutMix https github. 87 with Inception v3. The degree of pitch shift was fixed to 500 cent. The same pattern at the bottom of the image and at the top of the image are totally different. That is a little bit different from treating natural images. com hidehisaarai1213 freesound dameoshi aug fold5 s one of the kernel to conduct learning with curated data. The difference from the original model is. You can see how we did this in this kernel https www. 5 ConvBlocks Changed pooling policy. Here s the types of augmentation. This block allows patterns to move in the frequency axis a little. We d like to appreciate to phalanx https www. Random crop got a little better cv than the first 2 seconds cropped. com ybabakhin kaggle_salt_bes_phalanx. With careful experiment we found that Inception model was comparatively unstable and requires comparatively large number of TTA which was around 20 25 times. We used sox from Python using subprocess package and created augmented features with kaggle kernels. The other one was 1ch version of the original Inception v3 so we just simply altered the first layer. We tried to crop the place where the total of db is high preferentially. We especially want to thank daisukelab https www. And the transformed audio data is like this. The nuber of convolution blocksWe used 5 blocks to get 512 channels feature map. In the stage2 pre training we didn t changed any settings except for the initial weight of the network. We had two policy to train and validate with noisy data. com sailorwei for showing his Inception v3 model in his public kernel. The original CVSSP Baseline model structure is below. kernel sizeWe don t know why but the size 2 2 was the best. Now check the result of audio transformation. We found omalleyt https www. 720 We didn t checked all the score improvement of Inception v3 model with these augmentation but between no aug and with aug there exist around 0. com daisukelab cnn 2d basic solution powered by fast ai https www. 005 upThis sample has important information earlier but there is a little information from the end of sound. Unfortunately we don t know how much the LB lwlrap score improved with Custom CNN but the LB lwlrap score of Inception v3 improved 0. We changed kernel size number of Convolution Blocks Pooling policy and the structure of the fully connected layer. score augmentation no aug pitch down pitch up fade reverb treble and bass down treble and bass up equalize pitch down fade reverb treble and bass down all CV no warm up 0. Model Structure Model_Structure Custom CNN Custom_CNN Inception v3 Inception_v3 4. This is because we designed our model considering the meaning of each blocks although we re not sure whether it is working as we imaged originally. We also tried OctaveConv but couldn t find it useful for this competition. ", "id": "hidehisaarai1213/freesound-7th-place-solution", "size": "14792", "language": "python", "html_url": "https://www.kaggle.com/code/hidehisaarai1213/freesound-7th-place-solution", "git_url": "https://www.kaggle.com/code/hidehisaarai1213/freesound-7th-place-solution", "script": "torch.utils.data __init__ audio_to_melspectrogram progress_bar tqdm_notebook fastprogress array2img read_audio master_bar Path _get_strength_list read_as_melspectrogram show_before_and_after forward torch.nn GAB(nn.Module) numpy transform_and_compare init_weight __getitem__ pathlib Cnn_9layers_AvgPooling(nn.Module) ConvBlock(nn.Module) PIL SCse(nn.Module) FATValidDataset(Dataset) Image matplotlib.pyplot ImageOps show_melspectrogram pandas subprocess Dataset conf __len__ tqdm FATTrainDataset(Dataset) ConvNet(nn.Module) SpatialAttention2d(nn.Module) torch.nn.functional _get_strength init_weights _get_blend_arr mono_to_color convert_wav_to_image IPython.display ", "entities": "(('We', 'noisy data'), 'apply') (('how we', 'kernel https www'), 'see') (('We', 'axis frequency'), 'take') (('we', 'network'), 'change') (('where total', 'db'), 'try') (('we', 'nearby time'), 'get') (('Third Convolution Block', '64 128 1 3 pooling'), 'channel') (('sizeWe position', 'when spectrogram'), 'find') (('so size', '8 times 7 augmentations 1 original original curated dataset'), 'be') (('Ensemble Data Ensemble AugmentationWe', 'sox http sox'), 'create') (('We', 'whole curated data'), 'apply') (('We', '5 subsets'), 'use') (('same pattern', 'image'), 'be') (('We', 'kaggle kernels'), 'use') (('We', 'noisy data'), 'have') (('We', 'great dataset'), 'use') (('dameoshi aug hidehisaarai1213 fold5', 'curated data'), 'com') (('effect', 'models'), 'note') (('we', 'torchvision'), 'be') (('which', 'one'), 'com') (('seed just model', 'policy1'), 'believe') (('We', 'original dataset'), 'use') (('We', 'https github'), 'borrow') (('com', 'public kernel'), 'sailorwei') (('we', 'don 0'), 'augmentation') (('We', 'below them'), 'help') (('We', 'model'), 'alter') (('validation', 'validation'), 'use') (('We', 'fully connected layer'), 'change') (('Fourth Convolution Block', '128'), 'pooling') (('We', 'many things'), 'thank') (('which', '1e'), 'com') (('why this', 'kernel size'), 'model') (('nuber', 'channels feature 512 map'), 'use') (('degree', '500 cent'), 'fix') (('Finaly we', '5 3models 15model equally weighted manner'), 'decide') (('we', 'random crop'), 'be') (('wa', 'uniformly kernel'), 'crop') (('we', 'image fixed size'), 'use') (('com kaggle competitions', 'logos 10700 header'), 'kaggle') (('Spatial Squeeze', 'pool'), 'add') (('patterns', 'time'), 'allow') (('That', 'little bit natural images'), 'be') (('Pretrain', 'model'), 'use') (('we', 'Nerual Networks'), 'have') (('we', 'little bit different original one'), 'be') (('We', 'FATTrainDataset'), 'prepare') (('when degree', 'translation'), 't') (('whole training', '8 times original one'), 'use') (('patterns', 'frequency'), 'allow') (('couldn it', 'competition'), 'try') (('Strength Adaptive CropMany sound crip', 'first few seconds'), 'have') (('0 ve', 'solution'), 'sailorwei') (('we', 'them'), 'have') (('didn t work Augmentation', 'What'), 'github') (('moving', 'frequency'), 'be') (('v3 1ch 3 we', '0'), 'end') (('Then We', 'Strength Adaptive Crop'), 'introduce') (('We', 'log melspectrogram'), 'put') (('optimize BCEWithLogitLossWe', 'stage'), 'use') (('we', 'just simply first layer'), 'be') (('it', 'sounds'), 'be') (('Data Augmentation Data_Augmentation pitch shift', 'fade Fade reverb Reverb pitch_shift treble'), 'equalizing') (('EqualizingThis augmentation', 'custom frequency range'), 'adjust') (('where sound', 'places'), 'be') (('LB lwlrap score', 'Inception v3'), 'know') (('Also we', 'TTA'), 'face') (('first we', 'custom CNN lot model'), 'work') (('We', 'also spatial attention'), 'add') (('transformed audio data', 'this'), 'be') (('FadeThis augmentation', 'sound'), 'alter') (('sample', 'crip'), 'have') (('properly we', 'competition'), 'be') (('source synthesis open which', 'default'), 'be') (('ReverbThis augmentation', 'original sound'), 'add') (('which', 'noisy data'), 'daisukelab') (('train', 'only noisy data'), 'do') (('which', 'train_curated dataset'), 's') (('Random crop', 'seconds first 2 cropped'), 'get') (('We', 'CVSSP Baseline https www'), 'start') (('We', 'validation'), 'use') (('use', 'crop random resized tta'), 'be') (('We', 'only training'), 'create') (('we', 'frequency axis'), 'get') (('pitch', 'CV'), 'augmentation') (('Treble', 'sound'), 'emphasize') (('policy', '2 slightly policy'), 'try') (('it', 'still same class'), 'use') (('com speech recognition challenge place design c tensorflow t 47715 2nd forumMessageId 270205 philosophy', 'convolution blocks'), 'forum') (('Fifth Convolution Block', '256'), 'channel') (('we', 'validation'), 'be') (('didn them', 'Custom CNN'), 'fold') (('We', 'daisukelab https especially www'), 'want') (('Training StrategyThis', 'training'), 'htmlSo') (('difference', 'original model'), 'be') (('we', 'blocks'), 'be') (('We', 'ConvBlock'), 'add') (('upThis sample', 'sound'), '005') (('augmentation', 'sound'), 'pitch') (('longer so we', 'pre stage 2 training'), 'in') (('all it', 'data leak'), 'be') (('Inception v3We', 'Inception v3 two models'), 'use') (('it', 'position'), 'pdf') (('com place 2nd solution', 'Tensorflow Speech Recognition Competition'), 'helpful') (('We', 'https www'), 'd') (('noisy data', 'curated data'), 'train') (('which', 'TTA'), 'find') (('didn 720 We t', 'there around 0'), 'check') (('we', 'great opportunities'), 'do') (('how much augmentation', 'score'), 's') (('we', 'pooling https spatial pyramid arxiv'), 'use') (('which', 'noisy data'), 'use') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["adjust", "advance", "aggregate", "apply", "architecture", "audio", "aug", "augmentation", "basic", "batch_size", "bit", "blend", "block", "blog", "bottom", "case", "challenge", "channel", "check", "classifier", "clear", "cnn", "command", "competition", "contain", "convolution", "crop", "custom", "cv", "cycle", "data", "dataset", "define", "degree", "difference", "effect", "en", "end", "ensemble", "ensure", "epoch", "equalize", "experience", "experiment", "explained", "feature", "find", "fixed", "fold", "found", "framework", "frequency", "function", "handle", "high", "http", "image", "implementation", "improvement", "io", "kaggle", "kernel", "learning", "least", "line", "little", "local", "log", "lot", "magic", "manner", "map", "meaning", "melspectrogram", "method", "middle", "mixed", "mixup", "model", "most", "move", "nature", "need", "new", "no", "normal", "not", "number", "open", "optimize", "order", "out", "output", "package", "pattern", "pdf", "period", "place", "png", "pooling", "position", "post", "pre", "prepare", "problem", "public", "pytorch", "random", "range", "rare", "re", "reason", "recording", "reference", "representation", "result", "sample", "score", "shift", "short", "size", "solution", "sound", "source", "spatial", "split", "splitting", "stage", "strategy", "strength", "structure", "summarize", "table", "team", "tensorflow", "those", "thought", "time", "tool", "total", "train", "training", "translate", "try", "tta", "tuning", "up", "v2", "v3", "val", "validate", "validation", "version", "volume", "weight", "work"], "potential_description_queries_len": 156, "potential_script_queries": ["conf", "display", "forward", "nn", "numpy", "pathlib", "torch", "tqdm"], "potential_script_queries_len": 8, "potential_entities_queries": ["aug", "challenge", "fixed", "http", "lot", "place", "sound", "stage", "v3"], "potential_entities_queries_len": 9, "potential_extra_queries": ["test"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 165}