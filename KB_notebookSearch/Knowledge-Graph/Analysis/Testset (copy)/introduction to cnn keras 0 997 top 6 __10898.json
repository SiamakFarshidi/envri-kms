{"name": "introduction to cnn keras 0 997 top 6 ", "full_name": " h1 Introduction to CNN Keras Acc 0 997 top 8 h3 Yassine Ghouzam PhD h4 18 07 2017 h1 1 Introduction h1 2 Data preparation h2 2 1 Load data h2 2 2 Check for null and missing values h2 2 3 Normalization h2 2 3 Reshape h2 2 5 Label encoding h2 2 6 Split training and valdiation set h1 3 CNN h2 3 1 Define the model h2 3 2 Set the optimizer and annealer h2 3 3 Data augmentation h1 4 Evaluate the model h2 4 1 Training and validation curves h2 4 2 Confusion matrix ", "stargazers_count": 0, "forks_count": 0, "description": "e the area size pooled each time more the pooling dimension is high more the downsampling is important. We have to choose the pooling size i. The first is the convolutional Conv2D layer. This metric function is similar to the loss function except that the results from the metric evaluation are not used when training the model only for evaluation. To keep the advantage of the fast computation time with a high LR i decreased the LR dynamically every X steps epochs depending if it is necessary when accuracy is not improved. 17 sklearn versions. Sometime it is very difficult to catch the difference between 4 and 9 when curves are smooth. Moreover the CNN converg faster on 0. 3 Data augmentation In order to avoid overfitting problem we need to expand artificially our handwritten digit dataset. Evaluate the model 4. 114 With data augmentation i achieved 99. 671 of accuracy with this CNN trained in 2h30 on a single CPU i5 2500k. 3 Normalization 2. I choosed to set 32 filters for the two firsts conv2D layers and 64 filters for the two last ones. The LR is the step by which the optimizer walks through the loss landscape. 1 Training and validation curves 4. Keras requires an extra dimension in the end which correspond to channels. I want to see the most important errors. There is no missing values in the train and test dataset. The Flatten layer is use to convert the final feature maps into a one single 1D vector. 67 of accuracyFor the data augmentation i choosed to Randomly rotate some training images by 10 degrees Randomly Zoom by 10 some training images Randomly shift images horizontally by 10 of the width Randomly shift images vertically by 10 of the height I did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9. For those six case the model is not ridiculous. I ll show you the training and validation curves i obtained from the model i build with 30 epochs 2h30 The model reaches almost 99 98. Be carefull with some unbalanced dataset a simple random split could cause inaccurate evaluation during the validation. Each filter transforms a part of the image defined by the kernel size using the kernel filter. These are used to reduce computational cost and to some extent also reduce overfitting. Data preparation 2. It is the error rate between the oberved labels and the predicted ones. This function will iteratively improve parameters filters kernel values weights and bias of neurons. 1 Load data a random split of the train set doesn t cause some labels to be over represented in the validation set. 2 Confusion matrix 5. However it seems that our CNN has some little troubles with the 4 digits hey are misclassified as 9. The metric function accuracy is used is to evaluate the performance our model. 5 Label encodingLabels are 10 digits numbers from 0 to 9. For RGB images there is 3 channels we would have reshaped 784px vectors to 28x28x3 3D matrices. The rectifier activation function is used to add non linearity to the network. Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. 1 Define the model 3. callbacks i choose to reduce the LR by half if the accuracy is not improved after 3 epochs. The improvement is important Without data augmentation i obtained an accuracy of 98. Firstly I will prepare the data handwritten digits images then i will focus on the CNN modeling and evaluation. 5 Label encoding 2. I choosed to build it with keras API Tensorflow backend which is very intuitive. The last 9 is also very misleading it seems for me that is a 0. 997 top 8 Yassine Ghouzam PhD 18 07 2017 1. Introduction to CNN Keras Acc 0. Once our model is ready we fit the training dataset. The validation accuracy is greater than the training accuracy almost evry time during the training. Computation will be much much faster For computational reasons i set the number of steps epochs to 2 if you want to achieve 99 of accuracy set it to 30. So we can safely go ahead. In the last layer Dense 10 activation softmax the net outputs distribution of probability of each class. 2 Confusion matrixConfusion matrix can be very helpfull to see your model drawbacks. relu is the rectifier activation function max 0 x. in order to minimise the loss. 2 Set the optimizer and annealerOnce our layers are added to the model we need to set up a score function a loss function and an optimisation algorithm. fit X_train Y_train batch_size batch_size epochs epochs validation_data X_val Y_val verbose 2 With data augmentation to prevent overfitting accuracy 0. This technique also improves generalization and reduces the overfitting. 1 Load dataWe have similar counts for the 10 digits. It is like a set of learnable filters. 6 Split training and valdiation set I choosed to split the train set in two parts a small fraction 10 became the validation set which the model is evaluated and the rest 90 is used to train the model. Its better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function. 0 GPU capabilites from GTX 650 to recent GPUs you can use tensorflow gpu with keras. We need to encode these lables to one hot vectors ex 2 0 0 1 0 0 0 0 0 0 0. IntroductionThis is a 5 layers Sequential Convolutional Neural Network for digits recognition trained on MNIST dataset. Dataframe as 1D vectors of 784 values. you found this notebook helpful or you just liked it some upvotes would be very much appreciated That will keep me motivated convert to one hot encoding Load the data Drop label column free some space Check the data Normalize the data Reshape image in 3 dimensions height 28px width 28px canal 1 Encode labels to one hot vectors ex 2 0 0 1 0 0 0 0 0 0 0 Set the random seed Split the train and the validation set for the fitting Some examples Set the CNN model my CNN architechture is In Conv2D relu 2 MaxPool2D Dropout 2 Flatten Dense Dropout Out Define the optimizer Compile the model Set a learning rate annealer Turn epochs to 30 to get 0. Combining convolutional and pooling layers CNN are able to combine local features and learn more global features of the image. For those who have a 3. 6 Split training and valdiation set 3. The most important function is the optimizer. Since i set epochs 2 on this notebook. That means that our model dosen t not overfit the training set. Some popular augmentations people use are grayscales horizontal flips vertical flips random crops color jitters translations rotations and much more. 1 Predict and Submit results 1. Let s investigate for errors. Some of these errors can also be made by humans especially for one the 9 that is very close to a 4. It looks at the 2 neighboring pixels and picks the maximal value. I choosed RMSprop with default values it is a very effective optimizer. We define the loss function to measure how poorly our model performs on images with known labels. 3 Data augmentation 4. By applying just a couple of these transformations to our training data we can easily double or triple the number of training examples and create a very robust model. In order to make the optimizer converge faster and closest to the global minimum of the loss function i used an annealing method of the learning rate LR. The CNN can isolate features that are useful everywhere from these transformed images feature maps. We could also have used Stochastic Gradient Descent sgd optimizer but it is slower than RMSprop. With the ReduceLROnPlateau function from Keras. 3 ReshapeTrain and test images 28px x 28px has been stock into pandas. Dropout is a regularization method where a proportion of nodes in the layer are randomly ignored setting their wieghts to zero for each training sample. 3 NormalizationWe perform a grayscale normalization to reduce the effect of illumination s differences. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. The most important errors are also the most intrigous. This layer simply acts as a downsampling filter. For example the number is not centered The scale is not the same some who write with big small numbers The image is rotated. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive monotonically decreasing learning rate. 2 Set the optimizer and annealer 3. Prediction and submition 5. 1 Training and validation curvesThe code below is for plotting loss and accuracy curves for training and validation. Since we have 42 000 training images of balanced labels see 2. The second important layer in CNN is the pooling MaxPool2D layer. MNIST images are gray scaled so it use only one channel. 7 accuracy on the validation dataset after 2 epochs. 99286 set input mean to 0 over the dataset set each sample mean to 0 divide inputs by std of the dataset divide each input by its std apply ZCA whitening randomly rotate images in the range degrees 0 to 180 Randomly zoom image randomly shift images horizontally fraction of total width randomly shift images vertically fraction of total height randomly flip images randomly flip images Fit the model Plot the loss and accuracy curves for training and validation Look at confusion matrix Predict the values from the validation dataset Convert predictions classes to one hot vectors Convert validation observations to one hot vectors compute the confusion matrix plot the confusion matrix Display some error results Errors are difference between predicted labels and true labels Probabilities of the wrong predicted numbers Predicted probabilities of the true values in the error set Difference between the probability of the predicted label and the true label Sorted list of the delta prob errors Top 6 errors Show the top 6 errors predict results select the indix with the maximum probability. 98114 history model. We reshape all data to 28x28x1 3D matrices. In the end i used the features in two fully connected Dense layers which is just artificial an neural networks ANN classifier. Filters can be seen as a transformation of the image. 1 Define the modelI used the Keras Sequential API where you have just to add one layer at a time starting from the input. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima. Our model is very well trained 4. 2 Check for null and missing values 2. This Notebook follows three main parts The data preparation The CNN modeling and evaluation The results prediction and submission 2. We can make your existing dataset even larger. For that purpose i need to get the difference between the probabilities of real value and the predicted ones in the results. The higher LR the bigger are the steps and the quicker is the convergence. We use a specific form for categorical classifications 2 classes called the categorical_crossentropy. The kernel filter matrix is applied on the whole image. To avoid that you could use stratify True option in train_test_split function Only for 0. 2 Check for null and missing valuesI check for corrupted images missing values inside. We can get a better sense for one of these examples by visualising the image and looking at the label. I plot the confusion matrix of the validation results. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit. Here we can see that our CNN performs very well on all digits with few errors considering the size of the validation set 4 200 images. It combines all the found local features of the previous convolutional layers. 9967 accuracy Without data augmentation i obtained an accuracy of 0. This flattening step is needed so that you can make use of fully connected layers after some convolutional maxpool layers. ", "id": "yassineghouzam/introduction-to-cnn-keras-0-997-top-6", "size": "10898", "language": "python", "html_url": "https://www.kaggle.com/code/yassineghouzam/introduction-to-cnn-keras-0-997-top-6", "git_url": "https://www.kaggle.com/code/yassineghouzam/introduction-to-cnn-keras-0-997-top-6", "script": "Flatten sklearn.metrics keras.preprocessing.image keras.layers keras.callbacks train_test_split Dropout Sequential MaxPool2D to_categorical # convert to one-hot-encoding keras.utils.np_utils Conv2D seaborn numpy plot_confusion_matrix ReduceLROnPlateau sklearn.model_selection confusion_matrix ImageDataGenerator matplotlib.pyplot RMSprop Dense pandas keras.optimizers display_errors keras.models matplotlib.image ", "entities": "(('you', 'maxpool convolutional layers'), 'need') (('90', 'model'), 'set') (('regularization where proportion', 'training sample'), 'be') (('validation accuracy', 'evry almost time training'), 'be') (('which', 'keras API Tensorflow backend'), 'choose') (('We', 'one hot vectors'), 'need') (('test 3 ReshapeTrain 28px 28px', 'pandas'), 'image') (('Confusion matrixConfusion 2 matrix', 'model very drawbacks'), 'be') (('Filters', 'image'), 'see') (('how poorly model', 'known labels'), 'define') (('Label 5 encodingLabels', '9'), 'be') (('when curves', '4'), 'be') (('7 accuracy', '2 epochs'), 'dataset') (('you', 'keras'), '0') (('CNN', '4 200 images'), 'see') (('We', 'categorical classifications 2 classes'), 'use') (('curvesThe 1 Training code', 'training'), 'be') (('It', 'error oberved labels'), 'be') (('It', 'previous convolutional layers'), 'combine') (('We', 'label'), 'get') (('technique', 'overfitting'), 'improve') (('when someone', 'digit'), 'be') (('model dosen', 'training set'), 'mean') (('RMSProp update', 'learning aggressive monotonically decreasing rate'), 'adjust') (('activation rectifier function', 'network'), 'use') (('We', '28x28x1 3D matrices'), 'reshape') (('function', 'neurons'), 'improve') (('model', 'almost 99 98'), 'show') (('42 training 000 images', '2'), 'see') (('we', 'training dataset'), 'be') (('labels', 'validation set'), 'datum') (('optimizer', 'probably local minima'), 'be') (('gray it', 'only one channel'), 'be') (('hey', '9'), 'seem') (('it', 'such 6'), 'rotate') (('image', 'big small numbers'), 'center') (('where you', 'input'), 'use') (('model', '0'), 'find') (('that', 'data augmentation techniques'), 'approach') (('then i', 'CNN modeling'), 'focus') (('Combining', 'image'), 'be') (('I', '64 two last ones'), 'choose') (('results', 'only evaluation'), 'be') (('layer', 'downsampling simply filter'), 'act') (('results', 'maximum probability'), 'mean') (('optimizer', 'loss landscape'), 'be') (('simple random split', 'validation'), 'cause') (('you', '30'), 'be') (('that', 'images feature everywhere transformed maps'), 'isolate') (('second important layer', 'CNN'), 'be') (('These', 'also overfitting'), 'use') (('671', 'CPU i5 single 2500k'), 'train') (('you', 'Only 0'), 'avoid') (('3 NormalizationWe', 'differences'), 'perform') (('i', '98'), 'be') (('i', 'predicted results'), 'need') (('when accuracy', 'X steps dynamically epochs'), 'decrease') (('that', 'very 4'), 'make') (('This', 'distributed way'), 'drop') (('IntroductionThis', 'MNIST dataset'), 'be') (('grayscales horizontal vertical flips', 'crops color jitters translations rotations'), 'be') (('Load', '10 digits'), 'have') (('it', 'RMSprop'), 'use') (('which', 'channels'), 'require') (('Flatten layer', '1D one single vector'), 'be') (('i', 'learning rate LR'), 'use') (('we', 'digit artificially handwritten dataset'), 'augmentation') (('model', 'six case'), 'be') (('i', '0'), 'accuracy') (('function metric accuracy', 'performance'), 'use') (('Notebook', 'data CNN results prediction'), 'follow') (('3 we', '28x28x3 3D matrices'), 'be') (('kernel filter matrix', 'whole image'), 'apply') (('accuracy', '3 epochs'), 'callback') (('It', 'maximal value'), 'look') (('fit X_train batch_size epochs epochs validation_data X_val batch_size Y_val', 'overfitting accuracy'), 'Y_train') (('it', 'default values'), 'choose') (('we', 'loss function'), 'set') (('which', 'Dense two fully connected layers'), 'use') (('i', '99'), '114') (('that', 'me'), 'seem') (('filter', 'kernel filter'), 'transform') (('I', 'validation results'), 'plot') (('we', 'very robust model'), 'double') ", "extra": "['biopsy of the greater curvature', 'test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "advantage", "apply", "area", "array", "augmentation", "backend", "batch_size", "build", "case", "categorical", "cause", "check", "choose", "close", "code", "color", "column", "combine", "computation", "compute", "confusion", "convert", "convolutional", "cost", "could", "create", "data", "dataset", "default", "define", "difference", "digit", "dimension", "distributed", "distribution", "double", "effect", "encode", "encoding", "end", "error", "evaluate", "evaluation", "even", "every", "ex", "expand", "extent", "faster", "feature", "filter", "final", "fit", "fitting", "flip", "form", "found", "function", "generalization", "gpu", "gray", "grayscale", "half", "height", "high", "history", "hot", "idea", "image", "improve", "improvement", "input", "kernel", "label", "layer", "lead", "learn", "learning", "list", "little", "local", "looking", "main", "matrix", "max", "maximum", "mean", "measure", "method", "metric", "minimum", "missing", "model", "most", "my", "need", "network", "neural", "no", "non", "normalization", "not", "notebook", "null", "number", "optimizer", "option", "order", "overfit", "overfitting", "part", "people", "perform", "performance", "plot", "plotting", "pooling", "predict", "prediction", "prepare", "prevent", "prob", "probability", "problem", "purpose", "random", "range", "reduce", "regularization", "representation", "reshape", "rest", "robust", "rotate", "sample", "sampling", "scale", "scaled", "score", "second", "select", "sense", "set", "shift", "similar", "single", "six", "size", "sklearn", "softmax", "space", "split", "std", "step", "submission", "technique", "tensorflow", "test", "those", "through", "time", "total", "train", "training", "transformation", "up", "update", "validation", "value", "vertical", "while", "who", "width", "write", "zoom"], "potential_description_queries_len": 176, "potential_script_queries": ["image", "numpy", "preprocessing", "seaborn"], "potential_script_queries_len": 4, "potential_entities_queries": ["batch_size", "convolutional", "filter", "local", "random", "single", "training"], "potential_entities_queries_len": 7, "potential_extra_queries": ["biopsy of the greater curvature", "biopsy"], "potential_extra_queries_len": 2, "all_components_potential_queries_len": 181}