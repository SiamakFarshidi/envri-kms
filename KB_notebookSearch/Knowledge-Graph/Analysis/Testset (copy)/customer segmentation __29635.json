{"name": "customer segmentation ", "full_name": " h1 Customer segmentation h2 1 Data preparation h2 2 Exploring the content of variables h3 2 1 Countries h3 2 2 Customers and products h4 2 2 1 Cancelling orders h4 2 2 2 StockCode h4 2 2 3 Basket Price h2 3 Insight on product categories h3 3 1 Products Description h3 3 2 Defining product categories h4 3 2 1 Data encoding h4 3 2 2 Creating clusters of products h4 3 2 3 Characterizing the content of clusters h2 4 Customer categories h3 4 1 Formatting data h4 4 1 1 Grouping products h4 4 1 2 Separation of data over time h4 4 1 3 Consumer Order Combinations h3 4 2 Creation of customers categories h4 4 2 1 Data encoding h4 4 2 2 Creation of customer categories h2 5 Classification of customers h3 5 1 Support Vector Machine Classifier SVC h4 5 1 1 Confusion matrix h4 5 1 2 Learning curve h3 5 2 Logistic Regression h3 5 3 k Nearest Neighbors h3 5 4 Decision Tree h3 5 5 Random Forest h3 5 6 AdaBoost Classifier h3 5 7 Gradient Boosting Classifier h3 5 8 Let s vote h2 6 Testing predictions h2 7 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "2 Creation of customers categories 4. 1 Cancelling ordersFirst of all I count the number of transactions corresponding to canceled orders We note that the number of cancellations is quite large sim 16 of the total number of transactions. It is therefore interesting to look at how the predictions and real values compare to the breasts of the different classes. Once done I also give some basic informations on the content of the dataframe the type of the various variables the number of null values and their percentage with respect to the total number of entries While looking at the number of null values in the dataframe it is interesting to note that sim 25 of the entries are not assigned to a particular customer. Therefore I discard these words from the analysis that follows and also I decide to consider only the words that appear more than 13 times. To do this I create the categorical variable categ_product where I indicate the cluster of each product ___ 4. It remains to understand the habits of the customers in each cluster. a_ i j. Daniel September 2017 _____This notebook aims at analyzing the content of an E commerce database that lists purchases made by sim 4000 customers over a period of one year from 2010 12 01 to 2011 12 09. 3 Characterizing the content of clusters 4. Nominal a 5 digit integral number uniquely assigned to each distinct product. 1 Formatting dataIn the previous section the different products were grouped in five clusters. 2 Clusters of products 3. Firstly I define the X matrix as mot 1. 1 Support Vector Machine Classifier SVC The first classifier I use is the SVC classifier. org stable auto_examples model_selection plot_learning_curve. Testing predictionsIn the previous section a few classifiers were trained in order to categorize customers. Nominal a 5 digit integral number uniquely assigned to each customer. 1 Support Vector Machine Classifier SVC 5. In order to obtain a global view of their contents I determine which keywords are the most frequent in each of themand I output the result as wordclouds From this representation we can see that for example one of the clusters contains objects that could be associated with gifts keywords Christmas packaging card. . The X matrix indicates the words contained in the description of the products using the one hot encoding principle. 3 Basket PriceI create a new variable that indicates the total price of every purchase Each entry of the dataframe indicates prizes for a single kind of product. The first stage of this work consisted in describing the different products sold by the site which was the subject of a first classification. To do this I use the following function This function takes as input the dataframe and analyzes the content of the Description column by performing the following operations extract the names proper common appearing in the products description for each name I extract the root of the word and aggregate the set of names associated with this particular root count the number of times each root appears in the dataframe when several words are listed for the same root I consider that the keyword associated with this root is the shortest name this systematically selects the singular when there are singular plural variants The first step of the analysis is to retrieve the list of products Once this list is created I use the function I previously defined in order to analyze the description of the various products The execution of this function returns three variables keywords the list of extracted keywords keywords_roots a dictionary where the keys are the keywords roots and the values are the lists of words associated with those roots count_keywords dictionary listing the number of times every word is usedAt this point I convert the count_keywords dictionary into a list to sort the keywords according to their occurences Using it I create a representation of the most common keywords ___ 3. 2 Time spliting of the dataset 4. 1 Cancelling orders 2. Numeric Product price per unit in sterling. In order to ensure a good classification at every run of the notebook I iterate untill we obtain the best possible silhouette score which is in the present case around 0. html sphx glr self examples model selection pad learning curve py from which I represent the leanring curve of the SVC classifier On this curve we can see that the train and cross validation curves converge towards the same limit when the sample size increases. read the datafile gives some infos on columns types and numer of null values show first lines gives some infos on columns types and numer of null values Cancelation WITHOUT counterpart Cancelation WITH a counterpart Various counterparts exist in orders we delete the last one somme des achats utilisateur commande date de la commande selection des entr\u00e9es significatives D\u00e9compte des achats Repr\u00e9sentation du nombre d achats montant collect the words root association root keyword Aggregate the silhouette scores for samples belonging to cluster i and sort them Label the silhouette plots with their cluster numbers at the middle Compute the new y_lower for next plot define individual silouhette scores and do the graph define the color of the words I set the legend abreviation airline name somme des achats utilisateur commande pourcentage du prix de la commande categorie de produit date de la commande selection des entr\u00e9es significatives nb de visites et stats sur le montant du panier utilisateurs I set the legend abreviation airline name define individual silouhette scores and do the graph Correcting time range. 2 Learning curveA typical way to test the quality of a fit is to draw a learning curve. Later one of the objectives will be to develop a model capable of characterizing and anticipating the habits of the customers visiting the site and this from their first visit. Based on this analysis I develop a model that allows to anticipate the purchases that will be made by a new customer during the following year and this from its first purchase. More generally we see that there is always a representation in which two clusters will appear to be distinct. 8 Let s vote Finally the results of the different classifiers presented in the previous sections can be combined to improve the classification model. Below I make a census of the cancel orders and check for the existence of counterparts In the above function I checked the two cases 1. I then found that 75 of clients are awarded the right classes. To do this I use the VotingClassifier method of the sklearn package. I decide to collect the information related to a particular order and put in in a single entry. In practice this seasonal effect may cause the categories defined over a 10 month period to be quite different from those extrapolated from the last two months. mot N produit 1 a_ 1 1 a_ 1 N. Insight on product categoriesIn the dataframe products are uniquely identified through the StockCode variable. In particular this type of curves allow to detect possible drawbacks in the model linked for example to over or under fitting. 2 of the dataframe entries. Insight on product categories 3. InvoiceDate Invice Date and time. In order to define the category to which the clients belong I recall the instance of the kmeans method used in section 4. produit M a_ M 1 a_ M N where the a_ i j coefficient is 1 if the description of the product i contains the word j and 0 otherwise. Testing the predictions 7. Exploring the content of variablesThis dataframe contains 8 variables that correspond to InvoiceNo Invoice number. For the cancellations without counterparts a few of them are probably due to the fact that the buy orders were performed before December 2010 the point of entry of the database. However this definition uses data obtained over a period of 2 months via the variables count min max and sum. org stable auto_examples cluster plot_kmeans_silhouette_analysis. Now I check the number of entries that correspond to cancellations and that have not been deleted with the previous filter If one looks for example at the purchases of the consumer of one of the above entries and corresponding to the same product as that of the cancellation one observes We see that the quantity canceled is greater than the sum of the previous purchases. In order to be able to test the model in a realistic way I split the data set by retaining the first 10 months to develop the model and the following two months to test it ____ 4. Once these categories established I finally trained several classifiers whose objective is to be able to classify consumers in one of these 11 categories and this from their first purchase. 1 Data encodingNow I will use these keywords to create groups of product. In practice before creating these clusters it is interesting to define a base of smaller dimension allowing to describe the scaled_matrix matrix. To do so I start by adding to the selected_customers dataframe a variable that defines the cluster to which each client belongs Then I average the contents of this dataframe by first selecting the different groups of clients. Nominal the name of the country where each customer resides. One of the objectives may be for example to target these customers in order to retain them. org stable auto_examples model_selection plot_confusion_matrix. 2 Logistic RegressionI now consider the logistic regression classifier. Given the available information I decided to develop a classifier that allows to anticipate the type of purchase that a customer will make as well as the number of visits that he will make during a year and this from its first visit to the E commerce site. 3 Characterizing the content of clustersI check the number of elements in every class a _Silhouette intra cluster score_ In order to have an insight on the quality of the classification we can represent the silhouette scores of each element of the different clusters. Each entry in the dataset describes the purchase of a product by a particular customer and at a given date. The total number of transactions carried out is of the order of sim 22 000. In practice this choice has been done with respect to the performance of the classification carried out in the next section. produit i. To do this I decide to locate the entries that indicate a negative quantity and check if there is systematically an order indicating the same quantity but positive with the same description CustomerID Description and UnitPrice We see that the initial hypothesis is not fulfilled because of the existence of a _Discount_ entry. 2 Separation of data over timeThe dataframe basket_price contains information for a period of 12 months. 2 Creation of customer categoriesAt this point I define clusters of clients from the standardized matrix that was defined earlier and using the k means algorithm from scikit learn. UnitPrice Unit price. Now let s look at the first lines of the dataframe On these few lines we see that when an order is canceled we have another transactions in the dataframe mostly identical except for the Quantity and InvoiceDate variables. At this point I decide to create a new variable in the dataframe that indicate if part of the command has been canceled. The performance of the classifier therefore seems correct given the potential shortcomings of the current model. to port charges or bank charges. 2 Creating customer categories 4. The objective is to make this classification possible at the first visit. Classification of customersIn this part the objective will be to adjust a classifier that will classify consumers in the different client categories that were established in the previous section. A shrort description of the products is given in the Description variable. To fulfill this objective I will test several classifiers implemented in scikit learn. html b _Word Cloud_ Now we can have a look at the type of objects that each cluster represents. 1 Confusion matrix 5. In total approximately sim 4000 clients appear in the database. 2 Customers and productsThe dataframe contains sim 400 000 entries. Then the classifier can be tested by comparing its predictions with these categories. This is the purpose of the next figure which is taken from the sklearn documentation http scikit learn. 1 Product description 3. Quantity The quantities of each product item per transaction. 3 Grouping orders 4. 2 Customers and products 2. In order to correct such bias it would be beneficial to have data that would cover a longer period of time. Customer categories 4. In this section I intend to use the content of this latter variable in order to group the products into different categories. org wiki Distance_de_Hamming. 2 Defining product categories 3. Data Preparation 2. So I delete them from the dataframe OK therefore by removing these entries we end up with a dataframe filled at 100 for all variables Finally I check for duplicate entries and delete them ___ 2. In this case I will use this base in order to create a representation of the different clusters and thus verify the quality of the separation of the different groups. Note that the kmeans method of sklearn uses a Euclidean distance that can be used but it is not to the best choice in the case of categorical variables. As before I create an instance of the Class_Fit class adjust the model on the training data and see how the predictions compare to the real values Then I plot the learning curve to have a feeling of the quality of the model 5. If this code starts with letter c it indicates a cancellation. com judithabk6 for the advices and help provided during the writing of this notebook ___ 1. However in order to use the Hamming s metric we need to use the kmodes https pypi. What are the number of users and products in these entries It can be seen that the data concern 4372 users and that they bought 3684 different products. 2 Logistic regression 5. 1 Products DescriptionAs a first step I extract from the Description variable the information that will prove useful. These categories have been established in Section 4. Numeric the day and time when each transaction was generated. 3 k Nearest Neighbors 5. At this level I chose to mix Random Forest Gradient Boosting and k Nearest Neighbors predictions because this leads to a slight improvement in predictions ___ 7. com yassineghouzam don t know why employees leave read this This allows to have a global view of the content of each cluster It can be seen for example that the first 5 clusters correspond to a strong preponderance of purchases in a particular category of products. To do this I define a class to create Radar Charts which has been adapted from this kernel https www. Ab\u00e9cassis https www. I decide to check if this is true for all the entries. Hence I add 6 extra columns to this matrix where I indicate the price range of the products and to choose the appropriate ranges I check the number of products in the different groups ____ 3. In part I find that this type of customer represents 1 3 of the customers listed ___ 4. The data were then processed in two steps first all the data was considered ober the 2 months to define the category to which each client belongs and then the classifier predictions were compared with this category assignment. 7 Gradient Boosting Classifier___ 5. 1 Grouping products 4. I therefore choose to separate the dataset into 5 clusters. Hence I will now try to understand the content of these clusters in order to validate or not this particular separation. As a first step I adjust the parameters of the various classifiers using the best parameters previously found Then I define a classifier that merges the results of the various classifiers and train it Finally we can create a prediction for this model Note that when defining the votingC classifier I only used a sub sample of the whole set of classifiers defined above and only retained the Random Forest the k Nearest Neighbors and the Gradient Boosting classifiers. 2 Creating categories 5. 7 Gradient Boosting Classifier 5. In practice I decide to delete all of these entries which count respectively for sim 1. Until that point the whole analysis was based on the data of the first 10 months. 1 Confusion matrixThe accuracy of the results seems to be correct. In order to define approximately the number of clusters that best represents the data I use the silhouette score In practice the scores obtained above can be considered equivalent since depending on the run scores of 0. At this stage it is a question of using these habits in order to define the category to which the consumer belongs. 1 Data encoding 3. I therefore perform a PCA beforehand and I represent the amount of variance explained by each of the components ___ 4. Nevertheless let us remember that when the different classes were defined there was an imbalance in size between the classes obtained. In particular one class contains around 40 of the clients. 5 Random Forest 5. Nominal a 6 digit integral number uniquely assigned to each transaction. 2 StockCode 2. I collect all the purchases made during a single order to recover the total order prize In order to have a global view of the type of order performed in this dataset I determine how the purchases are divided according to total prizes It can be seen that the vast majority of orders concern relatively large purchases given that sim 65 of purchases give prizes in excess of 200. 1 Formating data 4. Others are do not carry information like colors. 2 Leraning curves 5. org pypi kmodes package which is not available on the current plateform. Also we can see that the accuracy of the training curve is correct which is synonymous of a low bias. This is the subject of the confusion matrices and to represent them I use the code of the sklearn documentation http scikit learn. Each entry in this dataframe corresponds to a particular client. 3 Consumer Order CombinationsIn a second step I group together the different entries that correspond to the same user. Customer segmentation_F. In order to draw this curve I use the scikit learn documentation code again http scikit learn. This is typical of modeling with low variance and proves that the model does not suffer from overfitting. 1 Data encodingThe dataframe transactions_per_user contains a summary of all the commands that were made. This can be achieved by selecting the customer category as the one indicated by the majority of classifiers. html from which I create the following representation ___ 5. a cancel order exists without counterpart2. I check the contents of this variable by looking for the set of codes that would contain only letters We see that there are several types of peculiar transactions connected e. In the case of matrices with binary encoding the most suitable metric for the calculation of distances is the Hamming s metric https en. Given the large number of variables of the initial matrix I first perform a PCA and then check for the amount of variance explained by each component We see that the number of components required to explain the data is extremely important we need more than 100 components to explain 90 of the variance of the data. I use this information to characterize the different types of customers and only keep a subset of variables In practice the different variables I selected have quite different ranges of variation and before continuing the analysis I create a matrix where these data are standardized In the following I will create clusters of customers. This also shows to which extent the mode could benefit from a larger data sample. Hence cancellations do not necessarily correspond to orders that would have been made beforehand. However I am correcting the data to take into account the difference in time between the two datasets and weights the variables count and sum to obtain an equivalence with the training set Then I convert the dataframe into a matrix and retain only variables that define the category to which consumers belong. I thus determine the number of purchases made by the user as well as the minimum maximum average amounts and the total amount spent during all the visits Finally I define two additional variables that give the number of days elapsed since the first purchase FirstPurchase and the number of days since the last purchase LastPurchase A customer category of particular interest is that of customers who make only one purchase. Hence orders are split on several lines. 2 Defining product categories The list that was obtained contains more than 1400 keywords and the most frequent ones appear in more than 200 products. On the other hand I found that beyond 5 clusters some clusters contained very few elements. In practice I decide to keep only a limited number of components since this decomposition is only performed to visualize the data ___ 4. 6 AdaBoost Classifier 5. Data preparationAs a first step I load all the modules that will be used in this notebook Then I load the data. Here we prepare the test data by defining the category to which the customers belong. Now I will determine the number of products purchased in every transaction The first lines of this list shows several things worthy of interest the existence of entries with the prefix C for the InvoiceNo variable this indicates transactions that have been canceled the existence of users who only came once and only purchased one product e. Other clusters will differ from basket averages mean the total sum spent by the clients sum or the total number of visits made count. The predict method of this instance calculates the distance of the consumers from the centroids of the 11 client classes and the smallest distance will define the belonging to the different categories Finally in order to prepare the execution of the classifier it is sufficient to select the variables on which it acts It remains only to examine the predictions of the different classifiers that have been trained in section 5 Finally as anticipated in Section 5. CustomerID Customer number. ConclusionThe work described in this notebook is based on a database providing details on purchases made on an E commerce platform over a period of one year. Country Country name. Here it is a question of using the available data over a period of two months and using this data to define the category to which the customers belong. 1 CountriesHere I quickly look at the countries from which orders were made and show the result on a chloropleth map We see that the dataset is largely dominated by orders made from the UK. At this level I recall the method of normalization that had been used on the training set Each line in this matrix contains a consumer s buying habits. b _Score de silhouette intra cluster_ As with product categories another way to look at the quality of the separation is to look at silouhette scores within different clusters c _Customers morphotype_ At this stage I have verified that the different clusters are indeed disjoint at least in a global way. Exploring the content of variables 2. However while examinating the content of the list I note that some names are useless. Hence the model does not underfit the data. In particular a bias that has not been dealt with concerns the seasonality of purchases and the fact that purchasing habits will potentially depend on the time of year for example Christmas. StockCode Product item code. 8 it is possible to improve the quality of the classifier by combining their respective predictions. 1 Grouping productsIn a second step I decide to create the categ_N variables with N in 0 4 that contains the amount spent in each product category Up to now the information related to a single order was split over several lines of the dataframe one line per product. ___ Acknowledgement many thanks to J. 2 StockCodeAbove it has been seen that some values of the StockCode variable indicate a particular transaction i. Classifying customers 5. I check again the hypothesis but this time discarding the _Discount_ entries Once more we find that the initial hypothesis is not verified. 2 Creating clusters of productsIn this section I will group the products into different classes. In order to use it I create an instance of the Class_Fit class and then call grid_search. At this stage it is important to bear in mind that this step does not correspond to the classification stage itself. The classifier defined in Section 5 uses a more restricted set of variables that will be defined from the first purchase of a client. Another cluster would rather contain luxury items and jewelry keywords necklace bracelet lace silver. n\u00ba12346 the existence of frequent users that buy a large number of items at each order___ 2. At first I use the result of the PCA in order to create a representation of the various clusters From this representation it can be seen for example that the first principal component allow to separate the tiniest clusters from the rest. For this the classifier is based on 5 variables which are mean amount of the basket of the current purchase categ_N with N in 0 4 percentage spent in product category with index N Finally the quality of the predictions of the different classifiers was tested over the last two months of the dataset. 05 will be obtained for all clusters with n_clusters 3 we obtain slightly lower scores for the first cluster. 4 Decision Tree 5. In this section I test the model the last two months of the dataset that has been stored in the set_test dataframe In a first step I regroup reformattes these data according to the same procedure as used on the training set. With the data available it is impossible to impute values for the user and these entries are thus useless for the current exercise. This gives access to for example the average baskets price the number of visits or the total sums spent by the clients of the different clusters. c _Principal Component Analysis_ In order to ensure that these clusters are truly distinct I look at their composition. First in order to simplify their use I define a class that allows to interface several of the functionalities common to these different classifiers Since the goal is to define the class to which a client belongs and this as soon as its first visit I only keep the variables that describe the content of the basket and do not take into account the variables related to the frequency of visits or variations of the basket price over time Finally I split the dataset in train and test sets ___ 5. In practice I have found that introducing the price range results in more balanced groups in terms of element numbers. There I grouped the different products into 5 main categories of goods. I also determine the number of clients in each group variable size Finally I re organize the content of the dataframe by ordering the different clusters first in relation to the amount wpsent in each product category and then according to the total amount spent d _Customers morphology_ Finally I created a representation of the different morphotypes. When calling this method I provide as parameters the hyperparameters for which I will seek an optimal value the number of folds to be used for cross validationOnce this instance is created I adjust the classifier to the training data then I can test the quality of the prediction with respect to the test data ___ 5. Nevertheless it can also be observed that many words appear in various clusters and it is therefore difficult to clearly distinguish them. I have classified clients into 11 major categories based on the type of products they usually buy the number of visits they make and the amount they spent during the 10 months. In a second step I performed a classification of the customers by analyzing their consumption habits over a period of 10 months. I choose the number of clusters based on the silhouette score and I find that the best score is obtained with 11 clusters At first I look at the number of customers in each cluster a _Report via the PCA_ There is a certain disparity in the sizes of different groups that have been created. Hence I use the kmeans method even if this is not the best choice. In order to prepare the rest of the analysis a first step consists in introducing this information into the dataframe. 1 Data enconding 4. there s at least one counterpart with the exact same quantityThe index of the corresponding cancel order are respectively kept in the doubtfull_entry and entry_to_remove lists whose sizes are Among these entries the lines listed in the doubtfull_entry list correspond to the entries indicating a cancellation but for which there is no command beforehand. I therefore create a new dataframe that contains for each order the amount of the basket as well as the way it is distributed over the 5 categories of products 4. Description Product item name. ", "id": "fabiendaniel/customer-segmentation", "size": "29635", "language": "python", "html_url": "https://www.kaggle.com/code/fabiendaniel/customer-segmentation", "git_url": "https://www.kaggle.com/code/fabiendaniel/customer-segmentation", "script": "matplotlib.cm sklearn.metrics __init__ PCA display legend plotly.offline sklearn.cluster KMeans _scale_data predict AdaBoostClassifier plot train Path plot_learning_curve matplotlib.patches keywords_inventory feature_selection iplot seaborn numpy plotly.graph_objs pathlib learning_curve fill title plot_confusion_matrix sklearn.decomposition linear_model HTML sklearn.ensemble sklearn metrics sklearn.model_selection confusion_matrix Class_Fit(object) grid_search neighbors matplotlib.pyplot WordCloud pandas silhouette_samples wordcloud StandardScaler random_color_func make_wordcloud RadarChart() svm grid_predict GridSearchCV init_notebook_mode model_selection SVC matplotlib sklearn.preprocessing sklearn.svm graph_component_silhouette tree grid_fit silhouette_score IPython.display ensemble STOPWORDS preprocessing ", "entities": "(('org pypi kmodes which', 'current plateform'), 'package') (('that', 'buying habits'), 'recall') (('I', 'customers'), 'use') (('i', 'word'), 'M') (('first principal component', 'rest'), 'use') (('where customer', 'country'), 'Nominal') (('I', 'scikit'), 'test') (('model', 'overfitting'), 'be') (('more than 1400 keywords', 'most frequent more than 200 products'), 'category') (('who', 'product only once only one e.'), 'determine') (('we', 'kmodes https pypi'), 'need') (('I', 'element numbers'), 'find') (('However definition', 'variables count'), 'use') (('Finally I', 'them'), 'delete') (('clusters', 'very few elements'), 'find') (('We', 'connected e.'), 'check') (('Data 1 encodingNow I', 'product'), 'use') (('classification', 'first visit'), 'be') (('objective', 'first purchase'), 'train') (('cluster', 'that'), '_') (('buy orders', 'database'), 'be') (('type', 'customers'), 'find') (('Others', 'colors'), 'carry') (('vast majority', '200'), 'collect') (('two clusters', 'always which'), 'see') (('that', 'only words'), 'discard') (('categoriesIn dataframe products', 'StockCode uniquely variable'), 'insight') (('first 5 clusters', 'products'), 'know') (('2 Separation', '12 months'), 'contain') (('I', 'following representation'), 'html') (('that', 'client'), 'use') (('then 75', 'right classes'), 'find') (('which', 'kernel https www'), 'define') (('scores', '0'), 'in') (('part', 'command'), 'decide') (('therefore how predictions', 'different classes'), 'be') (('Finally I', 'train'), 'define') (('approximately sim 4000 clients', 'database'), 'appear') (('I', 'most common keywords'), 'use') (('I', 'single entry'), 'decide') (('customers', 'which'), 'prepare') (('entries', 'thus current exercise'), 'be') (('performance', 'current model'), 'seem') (('particular one class', 'clients'), 'contain') (('sklearn documentation http scikit', 'code'), 'be') (('first step', 'dataframe'), 'consist') (('that', 'keywords Christmas packaging card'), 'determine') (('shrort description', 'Description variable'), 'give') (('I', 'section'), 'recall') (('that', 'same user'), 'combinationsin') (('dataset', 'UK'), 'look') (('_ Customers morphology Finally I', 'different morphotypes'), 'determine') (('There I', 'goods'), 'group') (('3 we', 'first cluster'), 'obtain') (('that', '09'), 'September') (('sample when size', 'same limit'), 'py') (('choice', 'next section'), 'do') (('different products', 'five clusters'), 'datain') (('I', '10 months'), 'perform') (('Logistic 2 RegressionI', 'regression now logistic classifier'), 'consider') (('that', 'scikit learn'), 'creation') (('I', 'Random above only Forest'), 'define') (('mode', 'data larger sample'), 'show') (('which', '0'), 'in') (('I', 'components'), 'perform') (('jewelry keywords', 'bracelet lace silver'), 'contain') (('this', 'entries'), 'decide') (('I', 'training set'), 'test') (('cancel order', 'counterpart2'), 'exist') (('when different classes', 'classes'), 'let') (('I', '5 clusters'), 'choose') (('it', 'therefore clearly them'), 'observe') (('Nearest Neighbors this', 'predictions'), 'choose') (('Later one', 'first visit'), 'be') (('decomposition', 'only data'), 'decide') (('I', 'different groups'), 'add') (('that', 'necessarily orders'), 'correspond') (('that', 'previous section'), 'be') (('initial hypothesis', '_ Discount _ time entries'), 'check') (('where I', 'product'), 'create') (('entry', 'particular client'), 'correspond') (('that', 'time'), 'be') (('customers', 'which'), 'be') (('initial hypothesis', '_ Discount _ entry'), 'decide') (('entry', 'product'), 'create') (('that', 'InvoiceNo Invoice number'), 'contain') (('values', 'transaction particular i.'), 'see') (('it', 'categorical variables'), 'note') (('Confusion matrixThe 1 accuracy', 'results'), 'seem') (('Then I', 'data'), 'preparationas') (('I', 'different categories'), 'intend') (('I', 'then grid_search'), 'create') (('they', '10 months'), 'classify') (('One', 'them'), 'be') (('I', 'two cases'), 'make') (('ConclusionThe work', 'one year'), 'base') (('number', 'transactions'), '1') (('information', 'product'), 'productsin') (('average baskets', 'different clusters'), 'give') (('lines', 'which'), 's') (('even this', 'kmeans method'), 'use') (('Hence I', 'order'), 'try') (('it', 'respective predictions'), '8') (('I', 'following two it'), 'in') (('who', 'only one purchase'), 'determine') (('I', 'different classes'), 'group') (('Then I', 'clients'), 'start') (('4372 they', '3684 different products'), 'be') (('25', 'particular customer'), 'give') (('N Finally quality', 'dataset'), 'base') (('sklearn documentation http scikit', 'next figure'), 'be') (('consumers', 'which'), 'correct') (('step', 'classification stage'), 'be') (('then I', 'test data'), 'call') (('which', 'low bias'), 'see') (('again scikit', 'documentation code'), 'http') (('few classifiers', 'customers'), 'train') (('It', 'cluster'), 'remain') (('that', 'commands'), 'dataframe') (('Then classifier', 'categories'), 'test') (('as well it', 'products'), 'create') (('2 Customers', 'productsThe 400 000 entries'), 'contain') (('which', 'respectively sim'), 'decide') (('consumer', 'which'), 'be') (('classifier then predictions', 'category assignment'), 'process') (('that', '5 Finally Section'), 'calculate') (('that', 'different groups'), 'choose') (('Then I', 'model'), 'adjust') (('that', 'order'), 'n\u00ba12346') (('truly I', 'composition'), '_') (('total number', 'sim'), 'be') (('whole analysis', 'first 10 months'), 'base') (('I', 'different groups'), 'use') (('that', 'first purchase'), 'develop') (('particular type', 'fitting'), 'allow') (('purchasing habits', 'example'), 'bias') (('we', 'different clusters'), 'check') (('categories', 'Section'), 'establish') (('Learning 2 curveA typical way', 'learning curve'), 'be') (('names', 'list'), 'note') (('legend abreviation airline name', 'time range'), 'show') (('different clusters', 'indeed at least global way'), 'cluster') (('it', 'scaled_matrix matrix'), 'be') (('Other clusters', 'total visits'), 'differ') (('it', 'cancellation'), 'start') (('extremely we', 'data'), 'perform') (('that', 'information'), 'variable') (('Hence model', 'data'), 'underfit') (('categories', 'last two months'), 'cause') (('I', 'sklearn package'), 'use') (('This', 'classifiers'), 'achieve') (('X matrix', 'encoding one hot principle'), 'indicate') (('Finally results', 'classification model'), 'let') (('we', 'mostly identical Quantity variables'), 'let') (('he', 'E commerce site'), 'give') (('which', 'first classification'), 'consist') (('quantity', 'previous purchases'), 'check') (('entry', 'given date'), 'describe') ", "extra": "['test', 'procedure']", "label": "Perfect_files", "potential_description_queries": ["account", "accuracy", "adjust", "aggregate", "algorithm", "analyze", "appear", "associated", "association", "average", "bank", "basic", "best", "binary", "calculation", "call", "cancellation", "case", "categorical", "category", "cause", "check", "choice", "choose", "classification", "classifier", "classify", "client", "cluster", "code", "coefficient", "color", "column", "combined", "command", "compare", "confusion", "consider", "contain", "content", "convert", "correct", "could", "count", "country", "create", "current", "curve", "customer", "data", "database", "dataframe", "dataset", "date", "day", "decomposition", "define", "depend", "describe", "description", "detect", "develop", "dictionary", "difference", "digit", "dimension", "distance", "distributed", "draw", "duplicate", "effect", "elapsed", "encoding", "end", "ensure", "even", "every", "execution", "explained", "extent", "extract", "fact", "figure", "filter", "find", "fit", "following", "found", "frequency", "frequent", "function", "graph", "group", "grouped", "hand", "help", "hot", "http", "imbalance", "improve", "improvement", "impute", "index", "indicate", "individual", "input", "instance", "interest", "item", "kept", "kernel", "latter", "learn", "learning", "least", "leave", "let", "letter", "level", "line", "list", "load", "look", "looking", "lower", "main", "major", "majority", "map", "matrix", "max", "maximum", "mean", "method", "metric", "middle", "min", "mind", "minimum", "mode", "model", "month", "most", "name", "nb", "need", "negative", "new", "next", "no", "normalization", "not", "notebook", "null", "number", "objective", "order", "organize", "out", "output", "package", "pad", "part", "per", "percentage", "perform", "performance", "performing", "period", "plot", "point", "positive", "potential", "practice", "predict", "prediction", "prefix", "prepare", "present", "price", "principal", "procedure", "product", "provide", "purpose", "py", "quantity", "question", "range", "re", "read", "recall", "regression", "relation", "representation", "rest", "result", "right", "run", "sample", "scikit", "score", "seasonal", "seasonality", "second", "section", "select", "selected", "selection", "separate", "separation", "set", "several", "shortest", "silhouette", "sim", "single", "site", "size", "sklearn", "sort", "split", "stage", "start", "step", "sub", "subject", "subset", "sum", "summary", "target", "test", "those", "through", "time", "total", "train", "training", "transaction", "try", "type", "under", "unit", "up", "user", "validate", "validation", "value", "variable", "variance", "variation", "verify", "view", "visit", "visualize", "vote", "while", "who", "word", "work", "year"], "potential_description_queries_len": 261, "potential_script_queries": ["display", "ensemble", "fill", "iplot", "matplotlib", "numpy", "pathlib", "predict", "preprocessing", "seaborn", "svm", "title", "tree"], "potential_script_queries_len": 13, "potential_entities_queries": ["dataframe", "hot", "least", "morphology", "sim"], "potential_entities_queries_len": 5, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 272}