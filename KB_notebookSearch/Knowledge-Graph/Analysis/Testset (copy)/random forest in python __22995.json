{"name": "random forest in python ", "full_name": " h1 Wisconsin Breast Cancer Machine Learning h1 Abstract h2 Origins h3 Steps Taken h2 Introduction to Random Forest h2 Decision Trees h3 Limitations to Decision Trees h2 Bootstrap Aggregating Trees h3 Limitations to Bagging Trees h2 Random Forest h1 Load Data h2 Cleaning h2 Missing Values h2 Class Imbalance h1 Creating Training and Test Sets h1 Fitting Random Forest h1 Hyperparameters Optimization h1 Out of Bag Error Rate h1 Traditional Training and Test Set Split h1 Training Algorithm h1 Variable Importance h1 Cross Validation h2 K Fold Cross Validation h1 Test Set Metrics h2 Confusion Matrix h1 ROC Curve Metrics h1 Classification Report h2 Metrics for Random Forest h1 Conclusions ", "stargazers_count": 0, "forks_count": 0, "description": "Printing the column name and total missing values for that column iteratively. Decision TreesDecision trees are simple but intuitive models that utilize a top down approach in which the root node creates binary splits until a certain criteria is met. Typically a value closer to 1 means that our model was able to differentiate correctly from a random sample of the two target classes of two patients with and without the disease. metrics this report gives many important classification metrics including Precision also the positive predictive value is the number of correct predictions divided by the number of correct predictions plus false positives so tp tp fp Recall also known as the sensitivity is the number of correct predictions divided by the total number of instances so tp tp fn where fn is the number of false negatives f1 score this is defined as the weighted harmonic mean of both the precision and recall where the f1 score at 1 is the best value and worst value at 0 as defined by the documentation http scikit learn. Class imbalance is a term used to describe when a target class within a data set is outnumbered by another target class or classes. Since OOB can be calculated with the model estimation it s helpful when cross validating and or optimizing hyperparameters prove to be too computationally expensive. If our curve is located in the top left corner of the plot that indicates an ideal model i. edu olvi uwmp cancer. This will ensure the results are the same for anyone who uses this generator and therefore that they will be able to replicate this project. To demonstrate how this works in practice specifically in a classification context I ll be walking you through an example using a famous data set from the University of California Irvine UCI Machine Learning Repository. I will source them at the end of the project as well but I found these to be interesting reads especially since I ve seen the data set used heavily without a lot of context as to how the data was actually extracted. In this case we will create 10 sets within our data set that calculate the estimations we have done already then average the prediction error to give us a more accurate representation of our model s prediction power. Limitations to Decision TreesDecision trees tend to have high variance when they utilize different training and test sets of the same data since they tend to overfit on training data. The AUC is used as a metric to differentiate the prediction power of the model for patients with cancer and those without it. Ensemble methods use multiple learning models to gain better predictive results in the case of a random forest the model creates an entire forest of random uncorrelated decision trees to arrive at the best possible answer. If you have any suggestions recommendations or corrections please reach out to me. Test Set MetricsUsing the test set that was created earlier let s examine another metric for the evaluation of our model. Fitting Random ForestNow let s create the model starting with parameter tuning. As you can see we got a very similar error rate for our test set that we did for our OOB which is a good sign for our model. org stable modules generated sklearn. Users then chose areas of the FNA with minimal overlap between nuclei they then took scans utilizing a digital camera. Random ForestRandom forest aims to reduce the previously mentioned correlation issue by choosing only a subsample of the feature space at each split. Fortunately random forest does not require any pre processing. This approach helps reduce variance by averaging the ensemble s results creating a majority votes model. Since we are utilizing the gini impurity the impurity measure reaches 0 when all target class labels are the same. Using a software called Xcyt the team created approximate boundaries which would then used a process called snakes https en. We will also calculate the area under the curve AUC. Model OOB Error Rate Test Error Rate Cross Validation Score AUC Random Forest 0. It can also give us insight into the mind of the practitioner by showing what variables played an important part in the predictions generated by the model. Using the pandas series object I can easily find the OOB error rate for the estimator as follows Utilizing the OOB error rate that was created with the model gives us an unbiased error rate. Data leakage is a common problem that can result in overfitting. Import modules Setting id_number as our index Converted to binary to help later on with models and plots For later use in CART models Stores value counts Resets index to make index a column in data frame If the number of unique instances in column exceeds 20 print warning Else it calculates prints percentage for each unique value in column Create a function to output the percentage try except block goes here if it can t find the column in data frame Cleaning test sets to avoid future warning messages Set the random state for reproducibility Set best parameters given by grid search Convert dictionary to a pandas series for easy plotting Captures whether the model has been trained Captures whether first parameter is a model Captures whether the model has been trained Set Metrics Changed for Future deprecation of as_matrix Here we calculate the test error rate We grab the second array from the output which corresponds to to the predicted probabilites of positive classes Ordered wrt fit. Fortunately ensemble methods that rely on CART models use a metric to evaluate the homogeneity of splits. Creating Training and Test SetsLet s split the data set into our training and test sets which will be pseudo randomly selected to create a 80 20 split. net profile Adriao_Duarte_Neto OriginsThis data set originated in early 1990 s when Dr. Training AlgorithmNext we train the algorithm utilizing the training and target class set we had made earlier. net profile Cicilia_Leite Ana Mg Guerreiro https www. Metrics for Random ForestHere I ve accumulated the various metrics we used through this tutorial in a simple table Showcasing the power and effectiveness of Random Forest Modeling. You can see in the maximum row of the chart that our data varies in distribution this will be important as we consider classification models. Using cross validation we can create multiple training and test sets and average the scores to give us a less biased metric. Along with including the bootstrap parameter. Traditional Training and Test Set SplitIn order for this methodology to work we will set the number of trees calculated using the OOB error rate and removing the warm_start and oob_score parameters. Steps Taken FNA s were done on a total of 569 patients once done the samples were then stained to help differentiate distinguished cell nuclei Samples were classified as cancer based through biopsy and historical confirmation. However a downside to this process is that the utilization of the entire feature space creates a risk of correlation between trees increasing bias in the model. There have been advancements with image classification in the past decade that make it possible to use images instead of extracted features from those images but this data set is a great resource for making use of machine learning processes and concepts. Bagging trees allow the trees to grow without pruning reducing the tree depth sizes and resulting in high variance but lower bias which can help improve predictive power. Next we will employ a function that gives us standard descriptive statistics for each feature including mean standard deviation minimum value maximum value and range intervals. Some models like neural networks https www. I will provide a brief overview of different CART methodologies that are relevant to random forest beginning with decision trees. More information here http citeseerx. max_features The maximum number of features that will be used in node splitting the main difference I previously mentioned between bagging trees and random forest. The OOB error rate starts to oscilate at around 400 trees so I will go ahead and use my judgement to use 400 trees in my forest. We print the data types of our data set this is important because this will often be an indicator of missing data as well as giving us context to anymore data cleanage. Let s graph these calculations. Feature engineering would be a powerful tool for extracting information and moving forward into the research phase and would help define key metrics to utilize when optimizing model parameters. Next we ll give the dimensions of the data set where the first value is the number of patients and the second value is the number of features. But for the purposes of this tutorial I included it to show the different accuracy metrics available. Missing ValuesGiven context of the data set I know that there is no missing data but I ran an apply method utilizing a lambda expression that checks to see if there was any missing values through each column. For use of categorical data see sklearn s Encoding Categorical Data http scikit learn. Class ImbalanceThe distribution of diagnoses is important because it speaks to class imbalance within machine learning and data mining applications. Wolberg was curious if he could find a way to accurately predict breast cancer diagnosis based on FNA s. com rasbt python machine learning book blob master faq decision tree binary. You will use the test set which will act as unseen data to assess model performance. Source by Glaucia Rma Sizilio https www. To follow this tutorial you will need some familiarity with classification and regression tree CART modeling. This following code will be used for the random forest model where the id_number won t be relevant. We have to delete this extra column since it doesn t contain any data. Notice how we didn t utilize the bootstrap True parameter this will make sense in the following section. Many people favor gini impurity because it has a lower computational cost than entropy which requires calculating the logarithmic function. ROC Curve MetricsA receiver operating characteristic ROC curve calculates the false positive rates and true positive rates across different thresholds. Classification ReportThe classification report is available through sklearn. 6745 rep rep1 type pdf. Note If your data set suffers from class imbalance I suggest reading up on upsampling and downsampling. org stable auto_examples ensemble plot_ensemble_oob. We have a total of 29 features that were computed for each cell nucleus with an ID Number and the Diagnosis Later converted to binary representations Malignant 1 Benign 0. Here I have created a confusion matrix showcasing the following metrics n Sample Size Predicted Benign Predicted Malignant Actual Benign True Negative False Positive Actual Malignant False Negative True Positive Confusion MatrixHere we create a confusion matrix visual with seaborn and transposing the matrix when creating the heatmap. Fortunately this data set does not suffer from class imbalance. The data set called the Breast Cancer Wisconsin Diagnostic Data Set deals with binary classification and includes features computed from digitized images of biopsies. Two such metrics are gini impurity and entropy. com projects 95 Also contributed to Data Science Inc. This binary splitting of nodes provides a predicted value based on the interior nodes leading to the terminal final nodes. Within a random forest context if your data set is significantly large you can choose to not do cross validation and instead use the OOB error rate as an unbiased metric for computational costs. Setting the n_jobs to 3 tells the grid search to run three jobs in parallel reducing the time the function will take to compute the best parameters. Hyperparameters Optimization Utilizing the GridSearchCV functionality let s create a dictionary with parameters we are looking to optimize to create the best model for our data. We are creating 10 subsets of our data on which to employ the training and test set methodology then we will average the accuracy for all folds to give us our estimation. Once we are given the best parameter combination we set the parameters to our model. Although intuitive decision trees have limitations that prevent them from being useful in machine learning applications. Here we re deleting the extra columnLet s preview the data set utilizing the head function which will give the first 5 values of our data frame. A single tree can outline for us important node splits as well as variables that were important at each split. The visual above helps drive home the point since you can clearly see the difference in the importance of variables for the ensemble method. Cross ValidationCross validation is a powerful tool that is used for estimating the predictive power of your model and it performs better than the conventional training and test set. This will help set the parameters we will use to tune one final parameter the number of trees in our forest. Let s access the feature importance of the model and use a helper function to output the importance of our variables in descending order. org stable modules preprocessing. As outlined below when calculating OOB two parameters have to be changed. On the other hand a ROC curve that is at 45 degrees is indicative of a model that is essentially randomly guessing. When using this method for machine learning always be wary of utilizing your test set to create models. criterion This is the metric used to asses the stopping criteria for the decision trees. Here are the parameters we will be tuning in this tutorial max_depth The maximum splits for all trees in the forest. Out of Bag Error RateAnother useful feature of random forest is the concept of an out of bag OOB error rate. Load DataFor this section I ll load the data into a Pandas dataframe CleaningWe do some minor cleanage like setting the id_number to be the data frame index along with converting the diagnosis to the standard binary 1 0 representation using the map function. However a simple tweak of the bagging trees methodology can prove advantageous to the model s predictive power. Other important metrics to consider would be the false positive rate since within this context it would be bad for the model to tell someone that they are cancer free when in reality they have cancer. com wiki Leakage NOTE What I mean when I say that we will pseudo randomly select data is that we will use a random seed generator and set it equal to a number of our choosing. In the case of our test data set knowing this information would help practitioners in the medical field focus on the top variables and their relationship with breast cancer. Because only two thirds of the data are used to train each tree when building the forest one third of unseen data can be used in a way that is advantageous to our accuracy metrics without being as computationally expensive as something like cross validation for instance. For more information I recommend reading this article https github. Thus when creating ensembles these metrics can be utilized to give insight into the important variables used in the training of the model. Image of a malignant solitary fibrous tumor using FNA. Certain cutoff points can be made to reduce the inclusion of features and can help in the accuracy of the model since we ll be removing what is considered noise within our feature space. com resources webinars introduction to neural nets with the data incubator can perform poorly if pre processing isn t considered so the describe function is a good indicator for standardization. I included the timer to see how long different jobs took that led me to ultimately decide to use three parallel jobs. Typically you want a value that is less than p where p is all features in your data set. org wiki Active_contour_model which converged to give the exact nuclei boundary. Essentially it aims to make the trees de correlated and prune the trees by setting a stopping criteria for node splits which I will cover in more detail later. classes_ in our case 0 1 where 1 is our positive class Add Diagonal line. html For the original analysis I compared Kth Nearest Neighbor Random Forest and Neural Networks so most of the analysis was done to compare across different models. Also by utilizing a for loop across a multitude of forest sizes we can calculate the OOB error rate and use it to asses how many trees are appropriate for our model NOTE When calculating the oob score setting bootstrap True will produce errors but is necessary for oob_score calculation as stated on this example http scikit learn. Our model did exceptional with an AUC over. The data set can be downloaded here https archive. Another important feature of bagging trees is that the resulting model uses the entire feature space when considering node splits. net scientific contributions 2069488816_Ana_Mg_Guerreiro and Adriao D Doria Neto https www. edu gareth ISL specifically chapter 5. net profile Glaucia_Sizilio Cicilia Rm Leite https www. Here we define each metric Gini Impurity 1 sum_i p_i and Entropy sum_i p_i log_2 p_i where p_i is defined as the proportion of subsamples that belong to a certain target class. 967 ConclusionsWe ve now gone through a number of metrics to assess the capabilities of our random forest but there s still much that can be done using background information from the data set. 90 now we do a zoomed in view to showcase the closeness our ROC Curve is relative to the ideal ROC Curve. precision_recall_fscore_support. edu ml datasets Breast Cancer Wisconsin 28Diagnostic 29. precision_recall_fscore_support support number of instances that are the correct target valuesAcross the board we can see that our model provided great insight into classifying patients based on FNA scans. Non cancer samples were confirmed by biopsy or follow ups. edu viewdoc download doi 10. a false positive rate of 0 and true positive rate of 1. The two metrics vary and from reading documentation online many people favor gini impurity due to the computational cost of entropy since it requires calculating the logarithmic function. Bootstrap Aggregating TreesThrough a process known as bootstrap aggregating or bagging it s possible to create an ensemble forest of trees where multiple training sets are generated with replacement meaning data instances or in the case of this tutorial patients can be repeated. Standardization is an important requirement for many classification models that should be handled when implementing pre processing. The research was broken down into two parts the extraction of the data which we will go over and the classficattion if you want to read more find information on this section here http citeseerx. If you d like to brush up on your knowledge of CART modeling before beginning the tutorial I highly recommend reading Chapter 8 of the book An Introduction to Statistical Learning with Applications in R which can be downloaded here http www bcf. This can create misleading accuracy metrics known as an accuracy paradox. Wisconsin Breast Cancer Machine Learning Contributor Raul Eulogio Edits by Brittany Marie Swanson Originally posted on inertia7 https www. If some variables within the feature space are indicative of certain predictions you run the risk of having a forest of correlated trees thereby increasing bias and reducing variance. In a classification context a decision tree will output a predicted target class for each terminal node produced. Variable ImportanceOnce we have trained the model we can assess variable importance. 6745 rep rep1 type pdf Introduction to Random ForestRandom forests also known as random decision forests are a popular ensemble method that can be used to build predictive models for both classification and regression problems. You can learn more about implementing a decision tree here http scikit learn. You will use the training set to train the model and perform some optimization. More on data leakage can be found in this Kaggle article https www. Limitations to Bagging TreesThe main limitation of bagging trees is that it uses the entire feature space when creating splits in the trees. To make sure our target classes aren t imbalanced create a function that will output the distribution of the target classes. This leads to poor performance on unseen data. However using ensemble methods we can create models that utilize underlying decision trees as a foundation for producing powerful results. Finally once the boundaries for the nuclei were set calculations were made resulting in 29 features creating this data set More information regarding the process can be found here http pages. Once the training sets are created a CART model can be trained on each subsample. 1 K Fold Cross ValidationHere we are employing K fold cross validation more specifically 10 folds. bootstrap An indicator of whether or not we want to use bootstrap samples when building trees. org stable modules tree. For more discussion I recommend reading this article https github. html encoding categorical features section. You ll recall that that we didn t touch the test set until now after we had completed hyperparamter optimization to avoid the problem of data leakage. For the sake of this tutorial I will go over other traditional methods for optimizing machine learning models including the training and test error route and cross validation metrics. com resources notebooks random forest intro Abstract For this project I implemented a Random Forest Model on a data set containing descriptive attributes of digitized images of a process known as fine needle aspirate FNA of breast mass. Unfortunately this limits the usage of decision trees in predictive modeling. Once we ve instantiated our model we will go ahead and tune our parameters. One downside to using ensemble methods with decision trees is that you lose the interpretability a single tree gives. This gives us great insight for further analyses like feature engineering although I won t get into that topic during this tutorial. The model s performance can vary significantly when utilizing different training and test sets. We can see here that our top 5 variables were area_worst perimeter_worst concave_points_worst concave_points_mean radius_worst. Suggested Reading For a more concise explanation of Cross Validation I recommend reading An Introduction to Statistical Learnings with Applications in R http www bcf. ", "id": "raviolli77/random-forest-in-python", "size": "22995", "language": "python", "html_url": "https://www.kaggle.com/code/raviolli77/random-forest-in-python", "git_url": "https://www.kaggle.com/code/raviolli77/random-forest-in-python", "script": "sklearn.metrics cross_val_score plot_roc_curve cross_val_metrics urlopen print_var_importance seaborn numpy auc print_dx_perc sklearn.ensemble urllib.request confusion_matrix sklearn.model_selection create_conf_mat KFold RandomForestClassifier matplotlib.pyplot pandas classification_report print_class_report = breast_cancer.iloc[ variable_importance GridSearchCV roc_curve train_test_split variable_importance_plot ", "entities": "(('model', 'disease'), 'mean') (('net profile Adriao_Duarte_Neto OriginsThis data set', 'Dr.'), 'originate') (('it', 'logarithmic function'), 'vary') (('Edits', 'https Originally inertia7 www'), 'post') (('CleaningWe', 'map function'), 'DataFor') (('data set', 'machine learning processes'), 'be') (('forest', 'variance'), 'be') (('it', 'better conventional training'), 'be') (('You', 'optimization'), 'use') (('org wiki which', 'nuclei exact boundary'), 'Active_contour_model') (('I', 'tutorial'), 'give') (('cell nuclei then distinguished Samples', 'biopsy'), 'stain') (('which', 'model performance'), 'use') (('that', 'mean standard deviation minimum value maximum value'), 'employ') (('they', 'training data'), 'tend') (('http here scikit', 'decision tree'), 'learn') (('utilization', 'increasing model'), 'be') (('still that', 'data set'), 'go') (('second value', 'features'), 'give') (('what', 'feature space'), 'make') (('that', 'ultimately three parallel jobs'), 'include') (('this', 'data cleanage'), 'print') (('We', 'curve AUC'), 'calculate') (('we', 'less biased metric'), 'create') (('which', 'wrt fit'), 'count') (('you', 'me'), 'reach') (('training where multiple sets', 'tutorial patients'), 'Bootstrap') (('I', 'forest'), 'start') (('we', 'data'), 'let') (('describe function', 'good standardization'), 'webinar') (('model', 'FNA scans'), 'number') (('target class when labels', '0'), 'utilize') (('cancer samples', 'biopsy'), 'confirm') (('metrics', 'model'), 'utilize') (('decision tree', 'terminal node'), 'output') (('that', 'target certain class'), 'define') (('I', 'Irvine UCI Machine Learning Repository'), 'demonstrate') (('that', 'machine learning applications'), 'have') (('which', 'www here bcf'), 'http') (('we', 'prediction power'), 'create') (('I', 'breast mass'), 'notebook') (('I', 'www bcf'), 'http') (('max_depth', 'forest'), 'be') (('I', 'more detail'), 'aim') (('won t', 'forest random model'), 'use') (('http scikit', 'example'), 'by') (('we', 'when trees'), 'bootstrap') (('we', 'when heatmap'), 'create') (('Unfortunately this', 'predictive modeling'), 'limit') (('binary splitting', 'terminal final nodes'), 'provide') (('ROC Curve MetricsA receiver ROC operating characteristic curve', 'true positive different thresholds'), 'calculate') (('which', 'good model'), 'get') (('we', 'Random Forest Modeling'), 'Metrics') (('that', 'powerful results'), 'create') (('you', 'tree CART classification modeling'), 'need') (('I', 'article https github'), 'recommend') (('therefore they', 'project'), 'ensure') (('This', 'decision trees'), 'criterion') (('Fortunately random forest', 'pre processing'), 'require') (('significantly you', 'computational costs'), 'choose') (('Classification ReportThe classification report', 'sklearn'), 'be') (('that', 'decision trees'), 'provide') (('entropy which', 'logarithmic function'), 'favor') (('we', 'model'), 'give') (('above home point you', 'ensemble method'), 'help') (('how data', 'context'), 'source') (('we', 'forest'), 'help') (('that', 'target classes'), 'create') (('we', 'more specifically 10 folds'), 'fold') (('when hyperparameters', 'model estimation'), 's') (('it', 'trees'), 'be') (('accuracy different metrics', 'it'), 'include') (('this', 'following section'), 'notice') (('it', 'data'), 'have') (('we', 'ahead parameters'), 'go') (('When using', 'models'), 'be') (('performance', 'significantly when different training'), 'vary') (('where 1', 'classes case'), '_') (('model', 'AUC'), 'do') (('Random ForestRandom forest', 'split'), 'aim') (('scikit', 'categorical data'), 'see') (('that', 'error unbiased rate'), 'object') (('More', 'Kaggle article https www'), 'find') (('we', 'class training set'), 'AlgorithmNext') (('approach', 'majority votes model'), 'help') (('I', 'bagging previously trees'), 'max_features') (('single tree', 'interpretability'), 'be') (('common that', 'overfitting'), 'be') (('where p', 'p'), 'want') (('over you', 'section'), 'break') (('I', 'validation metrics'), 'go') (('two parameters', 'below when OOB'), 'outline') (('popular ensemble that', 'classification problems'), 'be') (('we', 'variable importance'), 'train') (('data Fortunately set', 'class imbalance'), 'suffer') (('which', 'data frame'), 'delete') (('that', 'Diagnosis Later binary representations'), 'have') (('we', 'choosing'), 'com') (('This', 'unseen data'), 'lead') (('ROC Curve', 'ROC ideal Curve'), '90') (('resulting model', 'when node splits'), 'be') (('which', 'randomly 80 20 split'), 'create') (('More information', 'here http pages'), 'make') (('that', 'model ideal i.'), 'locate') (('earlier s', 'model'), 'let') (('model', 'best possible answer'), 'use') (('we', 'data leakage'), 'recall') (('com projects', 'Data Science 95 Also Inc.'), 'contribute') (('he', 'FNA s.'), 'be') (('they', 'cancer'), 'be') (('lower which', 'predictive power'), 'allow') (('variables', 'model'), 'give') (('it', 'machine data mining learning applications'), 'be') (('I', 'upsampling'), 'note') (('Feature engineering', 'model when parameters'), 'be') (('certain criteria', 'binary splits'), 'be') (('s', 'descending order'), 'let') (('we', 'classification models'), 'see') (('they', 'digital camera'), 'choose') (('which', 'then process'), 'create') (('parameter', 'tuning'), 'let') (('edu ml', 'Breast Cancer Wisconsin'), 'dataset') (('f1 where score', 'documentation http scikit'), 'give') (('that', 'missing column'), 'know') (('that', 'pre when processing'), 'be') (('CART model', 'subsample'), 'train') (('we', 'warm_start parameters'), 'order') (('However simple tweak', 'predictive power'), 'prove') (('AUC', 'it'), 'use') (('that', 'instance'), 'use') (('This', 'accuracy paradox'), 'create') (('Neural so most', 'different models'), 'html') (('then we', 'estimation'), 'average') (('target when class', 'target class'), 'be') (('function', 'best parameters'), 'tell') (('as well that', 'split'), 'outline') (('that', 'splits'), 'use') (('that', 'model'), 'be') ", "extra": "['biopsy', 'disease', 'patient', 'test', 'bag', 'diagnosis']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "algorithm", "apply", "approach", "area", "array", "article", "average", "background", "bag", "bagging", "best", "binary", "biopsy", "blob", "block", "board", "book", "breast", "build", "calculate", "calculation", "cancer", "case", "categorical", "cell", "chart", "choose", "classification", "code", "column", "compare", "compute", "concept", "confusion", "consider", "contain", "context", "correct", "correlation", "cost", "could", "create", "criteria", "criterion", "curve", "data", "dataframe", "decision", "define", "depth", "describe", "detail", "diagnosis", "dictionary", "difference", "distribution", "download", "drive", "encoding", "end", "engineering", "ensemble", "ensure", "entropy", "equal", "error", "estimation", "estimator", "evaluate", "evaluation", "expression", "extraction", "f1", "feature", "field", "final", "find", "fn", "fold", "following", "forest", "forward", "found", "frame", "function", "future", "generated", "generator", "gini", "grab", "graph", "grid", "grow", "hand", "head", "help", "helper", "high", "http", "image", "imbalance", "importance", "improve", "including", "index", "interpretability", "issue", "key", "knowledge", "learn", "learning", "left", "let", "load", "looking", "loop", "lot", "lower", "main", "majority", "malignant", "map", "matrix", "max_depth", "max_features", "maximum", "mean", "meaning", "measure", "medical", "method", "metric", "mind", "minimum", "missing", "ml", "model", "most", "multiple", "my", "name", "need", "neural", "no", "node", "noise", "not", "nuclei", "number", "object", "optimization", "optimize", "order", "out", "output", "overfit", "overlap", "overview", "parallel", "parameter", "part", "past", "pdf", "people", "percentage", "perform", "performance", "plot", "plotting", "point", "positive", "power", "practice", "pre", "precision", "predict", "prediction", "prevent", "print", "problem", "processing", "profile", "project", "provide", "python", "random", "range", "re", "read", "reading", "recall", "recommend", "reduce", "regression", "relationship", "relative", "replacement", "report", "representation", "reproducibility", "research", "result", "risk", "row", "run", "sample", "scientific", "scikit", "score", "seaborn", "search", "second", "section", "select", "selected", "sense", "set", "sign", "similar", "single", "sklearn", "something", "source", "space", "split", "splitting", "standard", "state", "support", "table", "target", "team", "term", "test", "those", "through", "time", "timer", "tool", "topic", "total", "train", "training", "tree", "try", "tumor", "tune", "tuning", "tutorial", "type", "under", "unique", "until", "up", "upsampling", "usage", "validation", "value", "variable", "variance", "view", "warning", "who", "work", "worst", "zoomed"], "potential_description_queries_len": 267, "potential_script_queries": ["auc", "numpy", "sklearn"], "potential_script_queries_len": 3, "potential_entities_queries": ["binary", "data", "final", "learning", "minimum", "node", "parallel", "positive", "random", "training"], "potential_entities_queries_len": 10, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 269}