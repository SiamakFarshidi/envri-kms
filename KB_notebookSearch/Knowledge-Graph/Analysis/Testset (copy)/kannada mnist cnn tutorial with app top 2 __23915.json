{"name": "kannada mnist cnn tutorial with app top 2 ", "full_name": " h1 A Simple CNN Tutorial with App h1 1 Introduction h4 If you will find it useful I would appreciate it if you upvote h4 Then let s begin h1 2 Convolutional Neural Networks h2 Convolution Operation h4 Steps of convulation operation h4 Stride h3 An Example of Convolution Operation h3 Why do we use Filters h2 Padding h3 Why do we use Padding h3 Equation of calculating output dimension changes with using padding process as below h3 Types of Padding in Keras h2 Pooling h3 Hyper Parameters of Pooling Operation h3 Why do we use Pooling h3 Types of Pooling in Keras h2 Batch Normalization h4 What is Batch Norm h4 Why do we use Batch Norm h4 Some notes about Batch Norm h2 Drop Out h1 3 An Image Classification Application with CNN h2 Import Modules h2 Understanding the Data h4 Image of Handwritten Character h2 Data Preprocessing h3 Normalizing Data h3 Cross Validation Training Validation Set Split h3 Reshape Data to Fit Model h2 Building and Training a CNN Model h4 Model Build h3 Model Compile h4 Compile Loss Function Categorical Crossentropy h4 Compile Optimizer h4 On the Fly Data Augmentation h3 Model Fit h2 Evaluation of the Model h3 Accuracy and Loss Curves h3 Test Set Accuracy Score h3 Confusion Matrix h3 F1 Score Calculation h3 Evaluate with Another Dataset h3 Submit for Competition h1 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "A feature map can be considered as the new picture we have for the next CNN layer. This kernel was prepared within the scope of handwriting recognition contest using the Kannada MNIST data set and in the competition it managed to be among the __top 2 __. jpg You can think of left one as the input signal or image and the other called the kernel as a filter on the input image producing an output image so convolution takes two matrix as input and produces a third as output. Result is the upper corner cell s value of your new output matrix. In addition we determine the final performance of the model with the test set. jpg attachment rsz_1rsz_conv_sample. This data set is __homogeneously__ distributed as you see below. png Why do we use Batch Norm Return Contents 0 At artificial neural networks distribution of the inputs changes for each layer during training since the weight parameters of the previous layers change. Here it can be observed in which regions of the matrix in the previous layer how much data is available for the filter pattern. In order to normalize training set data we need to convert x to float type. Types of Pooling in Keras Return Contents 0 Max Pooling The operation of selecting the maximum value from each mini group pool as the picture above Average Pooling The operation of selecting the average value from each mini group. In effect each update to a layer during training is performed with a different view of the configured layer. w and h is calculated as above equation and the channel dimension of the matrix is the same as filter number used on convolution. Model Fit Return Contents 0 Evaluation of the Model Accuracy and Loss Curves Return Contents 0 On classification accuracy metric is calculated as below begin equation classification accuracy frac correct predictions total predictions 100 end equation Test Set Accuracy Score Return Contents 0 Confusion Matrix Return Contents 0 If 90 of the data set is cat image and 10 is dog image your accuracy will be 90 even if you estimate the entire test set as a cat. Then sum all row and column to obtain a single value3. This has the effect of making the layer look like and be treated like a layer with a different number of nodes and connectivity to the prior layer. It can be used with most types of layers such as dense fully connected layers convolutional layers and recurrent layers such as the long short term memory network layer. png Compile Optimizer Return Contents 0 To optimize the weight values of the network we should choose an optimizer algorithm. __Data Preprocessing__ 230 Normalizing the Data 231 Train and Test Splitting 232 Reshape Data to Appropriate Sizes 233 4. With this observation you can roughly specify the range you should choose. On image processing applications generally we normalize data to 0 1 scale with dividing data to 255. According to problem stride content may vary change and this will directly effect on output dimensions. png Hyper Parameters of Pooling Operation 1. In the Conv layer the filters are shifted over the image by the number of strides and convolution is performed. Practicing with an Application Handwriting recognition with the Kannada handwritten digits dataset which is different than the MNIST handwritten digits dataset. This results in the loss of information on the edges. For example the first layer of the network can detect just __horizontal lines__ while deeper layers of the network __can detect the nose or eye__ and again ongoing deeper layers __can detect even the human face. Our brain recognizes some parts of the face separately and combines them to give a final decision. This study consists of two main parts. Filter Matrices containing a specific pattern 2. Model Build On building stage you specify the architecture of the model mainly. jpg attachment rsz_images. In the application part I also gave some theoretical information with visual details according to the necessity. Convolutional Neural Networks Return Contents 0 In Deep Learning Convolutional Neural Network CNN or ConvNet is a class of deep neural networks most commonly applied to analyzing visual imagery. The output matrix dimension getting smaller whenever we use convolution operation. 0 quiet 1 update messages. png attachment batch_norm. f is the horizontal vertical dimension of filter kernel matrix. Therefore you just initialize them and with backpropagation or other optimization algorithm they will be optimized like weight parameters. It is about only selecting one value from small groups. Calculation of output matrix dimension for l th layer begin equation n l frac n l 1 f s 1 text output n l x n l end equation Where l demonstrates the layer number. Steps of convulation operation 1. threshold for measuring the new optimum to only focus on significant changes. To evaluate the model we need to split our training set into training and validation set. This is the 2nd layer of CNN. png Why do we use Filters Return Contents 0 To answer this question first we should ask another and more general question What is our main purpose In this kernel our main purpose is as you know recognition handwrite patterns but for simplicity let us think about cats. png attachment padding. rsz_convolution with multiple filters2. To use much higher learning rates and be less careful about initialization 3. To increase the learning speed of the model. The matrix obtained as a result of the 3rd process is called a feature map. The reason for using the test set on the final evaluation is the model would have a bias on the validation set because we developed the model according to the validation set performance. __ To sum up filters are used to extract the features of the image. __Convolution Layer__ 110 Steps of Convolution Operation 111 Stride 112 An Example of Convolution Operation 113 Why Do We Use Filters 114 2. There are 2 kinds of false predictions __Predicted as 1 but Ground Truth is 0 False Positive __ __Predicted as 0 but Ground Truth is 1 False Negative __ The F1 score creates a success performance metric taking into account both of these incorrect prediction performances as well as the true positive predictions Since our problem has more than 2 classes we calculated the F1 score for each class on a one to all basis. png attachment cross_entropy. __By changing__ some of the properties of the image from the code below __you can observe what changes are happening in the dataset__. To speed up the computation Reducing the size of the representation for increasing the speed of the computation. In min mode lr will be reduced when the quantity monitored has stopped decreasing in the max mode it will be reduced when the quantity monitored has stopped increasing in auto mode the direction is automatically inferred from the name of the monitored quantity. We can now detect the patterns in this picture with a new filter. jpg attachment rsz_1smile. Convolution is a specialized kind of linear operation. The algorithm of the batch norm is shown below batch_norm. We can increase the number of layers with this logic. The hidden layers of a CNN typically consist of convolutional layers pooling layers fully connected layers and normalization layers. png As you see above we have 5x5 image matrix and 3x3 filter matrix If we choose the stride S parameter as 1 then our output matrix would be 3x3 matrix If we choose the stride S parameter as 2 then our output matrix would be 2x2 matrix begin equation n l frac 5 3 1 1 3 text output 3 x 3 text n l frac 5 3 2 1 2 text output 2 x 2 end equation If the entrance picture is colored RGB as below the 2nd and 3rd dimensions will also be available. Dataset has been already flattened and has 784 pixel values for each pic. s is the stride constant. Plot training validation loss values. You can read the original paper written by Sergey Ioffe and Christian Szegedy from here https arxiv. Padding mostly used in designing the CNN layers when the dimensions of the input volume need to be preserved in the output volume Same Padding. If you found it useful I would appreciate it if you upvote rsz_1smile. com 2019 07 08 keras imagedatagenerator and data augmentation who is the author of PyImageSearch a very instructive web site about computer vision. Without further ado I would like to give you brief information about the content of the study. Stride Sliding amount is declared by stride S constant. Model Compile Return Contents 0 On Compile Section we specify the loss function optimizer algorithm and metric to use for evaluating the model. n l is the horizontal vertical dimension of output matrix. __Understanding the Data__ 220 3. Basic Theoretical Knowledge Causal approaches Visual expression Answers of the questions that may arise in your mind about CNN. You can imagine that after the convolution operation each filter creates a different layer on the matrix as below. __If the data wasn t homogeneously__ distributed what would we do 1. After that slide the filter over S column on main image and do the same thing. Changing the distribution of each layer s inputs during training because of the enhancement of parameters of the previous layers calling covariate shifting at the original paper. Introduction Return Contents 0 Hello everyone first of all I am pleased to present this work to you. The loss is calculated using the following formula begin equation Loss frac 1 m sum_ i 1 m sum_ k 1 K Y i_klog hat Y i_k 1 Y i_k log 1 hat Y i_k end equation where k demonstrates class i demonstrates sample number hat Y_c is the predicted value Y_c is the ground truth value m is the sample number in a batch and K is the total number of classes. Drop Out Return Contents 0 Drop out is an effective regularization method. png __Why do we use Padding __ Return Contents 0 1. Why do we use log Because cross entropy function penalize bigger difference more and smaller difference less as mentioned below. The task of the filter becomes more complex in the deep layers of the evolutionary network. Numerically high values are indicative of pattern compliance. __Building and Training a CNN Model__ 250 Model Build 251 Model Compile 252 Categorical Cross Entropy 253 Optimizer 255 On the Fly Data Augmentation 257 Model Fit 259 5. They give us a training set for training the model and a test set without labels for prediction. So a kind of overfitting on the validation set is formed. Data Preprocessing Normalizing Data Return Contents 0 What is normalizing Normalization means that adjusting values measured on different scales to a notionally common scale. With the ImageDataGenerator on Keras we can handle this objective easily. You can leave one unknown dimension as 1. Whenever numpy has 1 value on reshape method it will calculate the dimension which denoted with 1 automatically. png attachment rsz_convolution with multiple filters2. F1 Score Calculation Return Contents 0 As mentioned above accuracy 271 gives a general idea about the performance of the model but does not provide any information about the model s trends. jpg attachment brain. gif On the Fly Data Augmentation Return Contents 0 On classification tasks on image datasets data augmentation is a common way to increase the generalization of the model. So output images are a matrix in w x h x c dimension. Despite the convolution operation Pooling has not any weight because you don t use any filter to operation. But in another aspect the model s success in predicting dogs is 0. __Evaluation of the Model__ 270 Accuracy and Loss Curves 271 Test Set Accuracy Score 272 Confusion Matrix 273 F1 Score Calculation 274 Evaluate with Another Dataset 275 Submit for Competition 275 6. __Padding__ 130 Why do we use Padding 131 Equation of Calculating Output Dimension 132 Types of Padding in Keras 133 3. In this context accuracy may not always give us realistic information about the actual performance of the model. Factor by which the learning rate will be reduced. If you will find it useful I would appreciate it if you upvote. In short terms we use Batch Norm 1. Equation of calculating output dimension changes with using padding process as below begin equation n l frac n l 1 2p f s 1 text output n l x n l end equation Types of Padding in Keras Return Contents 0 Valid No padding Same Choosing a p value on above equation to obtain output matrix in same dimension as like input matrix. The confusion matrix shows how confused your classification model is for which classes by detailing the relationship between actual class and predicted class. io content images 2016 09 contours_evaluation_optimizers. png attachment pooling. Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers A convolutional neural network consists of one or several convolutional layers followed by some fully connected layers of neurons like in classical multilayer feedforward neural networks. To regularize the network side positive effect Some notes about Batch Norm Return Contents 0 The parameters gamma and beta are learnable parameters. The information at the edges is convoluted only once while the information in the middle is convoluted 3 times with a 3x3 filter. It is important to know the distribution of data according to the labels they have. io optimizing gradient descent is a very nice overview article written by Sebastian Ruder https ruder. read_csv data visualization data visualization First and last values of dataset labels An observation code for our dataset fit parameters from data configure batch size and retrieve one batch of images create a grid of 3x3 images show the plot Quantity to be monitored. If you want to take a more detailed look at Gradient Descent algorithms here https ruder. __Why do we use Drop Out __ The answer is to prevent overfitting. Some of them are ADAGRAD ADAM ADAMAX NADAM and RMSPROP. Using for determine how many columns will kernel shift to right side in order to calculate output matrixes next cell. __Drop Out Layer__ 180 3. png attachment conv2. __CONCLUSION__ 290 1. The important factor in here is filter1 has the same values for all three channel demonstrated as different colors he has. io content images 2016 09 saddle_point_evaluation_optimizers. Or if we have enough data we can discard some high quantity labels Image of Handwritten Character Return Contents 0 An overview of a picture You can change the num variable to see other numbers. The method consists of performing normalization on each neuron of a specific layer on each training mini batch. Totaly we have 60000 pics in training set. As we don t have the labels we don t know the final performance of the test set until we submit our predictions. auto min max. These algorithms use techniques such as adaptive learning rate and momentum to achieve the global minimum. On the example above we use only one filter but in real applications there will be more filters used at the same time as below. __INTRODUCTION__ 1 2. jpg Evaluate with Another Dataset Return Contents 0 Submit for Competition Return Contents 0 Conclusion Return Contents 0 Please do not hesitate to comment and ask questions. jpg linear algebra data processing CSV file I O e. number of epochs to wait before resuming normal operation after learning rate lr has been reduced. You have your output 0 1 value. Padding Return Contents 0 Padding refers to the process of symmetrically adding values to the input matrix. After training the data Kaggle will evaluate the final performance of our data with test set predictions. If there is an anomaly something like above mentioned you can specify the problem with confusion matrix and improve accuracy by various methods like adding more data for a specific class etc. In classification algorithms it is important to analyze the false predictions of the model. jpg attachment rsz_1form C3 BCl. there is 2 x 2 pooling size and stride 2 sliding on matrix by 2 cell Why do we use Pooling Return Contents 0 1. Each filter has a job some of them detect the edges Sobel filter as on the example some of them detect just the horizontal lines. Here https towardsdatascience. Let s read csv files We have 28 x 28 dimension handwritten pics. Take the element wise production of the upper left corner sub matrix and filter2. Over time the gradient descent algorithm was developed and the algorithms that achieved faster and more accurate results were obtained. Then let s begin. Because each pixel in every sample of training set has integer values from 0 to 255. An Example of Convolution Operation Return Contents 0 conv2. Compile Loss Function Categorical Crossentropy Return Contents 0 The Categorical Crossentropy Loss Function computes between network predictions and target values for multiclass classification problems. This slows down training by requiring lower learning rates and careful parameter initialization making it difficult to train models with non linear saturation. Batch Normalization Return Contents 0 What is Batch Norm Return Contents 0 Batch Normalization is a method used to normalize the input of a hidden layer by adjusting and scaling the activations. Normalized neurons of l th layer will have a specific mean and variance for the problem according to trained gamma and beta parameters. In the first lessons of artificial learning the gradient descent algorithm is taught as the optimization algorithm used in backpropagation. This Kernel is prepared on a Kaggle competition dataset. com multi class metrics made simple part ii the f1 score ebe8b2c2ca1 is a nice article on how to calculate the F1 score in multiple classes supported by examples. After feature extraction we use a fully connected layer for classification according to extracted features. new_lr lr factor The number of epochs with no improvement after which learning rate will be reduced. com 2019 07 08 keras imagedatagenerator and data augmentation is a comprehensive and inspiring article about data augmentation and ImageDataGenerator written by Adrian Rosebrock https www. co Qcb9y35 Types of Learning in Machine Learning. Why should you normalize the data With a normalized data weight values reach optimum value faster. __Batch Normalization Layer__ 160 5. __Pooling Layer__ 150 Hyper Parameters of Pooling Operation 151 Why do we use Pooling 152 Types of Pooling in Keras 153 4. To gain robustness on feature extraction Pooling prevents the model from over training by discarding the unnecessary data relative to the selected value. Convolution Operation Return Contents 0 rsz_1rsz_conv_sample. Benan AKCA brain. Building and Training a CNN Model Return Contents 0 On Keras Sequential Networks there is three stage for training building compiling and fitting the model. __As you see below__ while Stochastic Gradient Descent SGD which is a basic gradient descent algorithm cannot escape the saddle point more advanced algorithms escape the saddle point __at different speeds. __Import Modules__ 210 2. When you see a cat face picture how do you understand that is a cat It should have solid patterns on its face right Solid patterns like eye shape nose curve mustaches etc. Edge information can be convoluted multiple times thanks to padding. Stride Same as convolution operation for example in the above pic. For I prefer to split Training set 80 Validation set 20 Reshape Data to Fit Model Return Contents 0 In order to feed the CNN model we need to reshape our 54000 x 784 flatten image data to 54000 x 28 x 28 x 1 dimensions. __CONVOLUTIONAL NEURAL NETWORKS__ 100 1. During training some number of layer outputs are randomly ignored or dropped out. The output matrix width w and height h sizes are generally the same in applications but it can be different also. A Simple CNN Tutorial with App. __APPLICATION WITH CNN__ 200 1. Pooling Return Contents 0 Pooling refers to the process of dividing a matrix into pools and select from every pool only one value as representing that pool. io optimizing gradient descent. Understanding the Data Return Contents 0 We have training and test set CSV files In order to evaluate the generalization skill of the model we will split our training set into training and validation sets. You can decide the Filter 110 size and Padding 150 type you will use on Convolution 110 operations and add Pooling 150 Batch Normalization Dropout activation function layers with build section. Cross Validation Training Validation Set Split Return Contents 0 In order to measure the generalization ability of the model we train the data with the training set and make the model arrangement according to the error value in the validation set. lower bound on the learning rate. 6x6x3 In this case we will convolute the filter 1 to have three different values from 3 channels and will sum the results to obtain a single number. n l 1 is the horizontal vertical dimension of input matrix. Then we could use data augmentation techniques to generate new data for low quantity labels 2. An Image Classification Application with CNN Return Contents 0 Import Modules With this code below you can check if the kernel use GPU or not. It has applications in image and video recognition recommender systems image classification medical image analysis and natural language processing The name Convolutional Neural Network indicates that the network employs a mathematical operation called convolution. The same explanation is acceptable for CNN filters. ", "id": "benanakca/kannada-mnist-cnn-tutorial-with-app-top-2", "size": "23915", "language": "python", "html_url": "https://www.kaggle.com/code/benanakca/kannada-mnist-cnn-tutorial-with-app-top-2", "git_url": "https://www.kaggle.com/code/benanakca/kannada-mnist-cnn-tutorial-with-app-top-2", "script": "Flatten sklearn.metrics Nadam tensorflow.keras.utils Adadelta tensorflow.keras.optimizers LeakyReLU tensorflow.keras.layers tensorflow.keras.regularizers tensorflow.keras.preprocessing.image tensorflow.keras.callbacks EarlyStopping Dropout Sequential keras.utils.np_utils Conv2D seaborn numpy ReduceLROnPlateau sklearn.model_selection ImageDataGenerator confusion_matrix matplotlib.pyplot RMSprop calc_F1 Dense tensorflow l2 pandas tensorflow.keras to_categorical BatchNormalization MaxPooling2D train_test_split ", "entities": "(('Dataset', 'pic'), 'flatten') (('you', 'it'), 'find') (('we', 'training set'), 'evaluate') (('io optimizing', 'overview Sebastian Ruder https very nice ruder'), 'be') (('he', 'different colors'), 'be') (('I', 'study'), 'like') (('h', 'convolution'), 'calculate') (('how many columns', 'output matrixes'), 'kernel') (('filters', 'strides'), 'shift') (('you', 'rsz_1smile'), 'find') (('direction', 'monitored quantity'), 'reduce') (('entropy function', 'bigger difference'), 'use') (('anomaly above you', 'class specific etc'), 'be') (('parameter careful it', 'linear non saturation'), 'slow') (('pixel', '255'), 'have') (('We', 'logic'), 'increase') (('Padding Return Contents 0 Padding', 'input matrix'), 'refer') (('we', 'basis'), 'be') (('generally we', '255'), 'normalize') (('algorithm', 'batch_norm'), 'show') (('who', 'web computer very instructive vision'), 'com') (('we', 'single number'), '6x6x3') (('update', 'configured layer'), 'perform') (('whenever we', 'convolution operation'), 'get') (('113 Why We', '114 2'), 'Layer') (('only once information', '3 times 3x3 filter'), 'convolute') (('Pooling Return Contents 0 Pooling', 'pool'), 'refer') (('text output l end 1 f 1 Where l', 'layer number'), 'begin') (('data augmentation', 'model'), 'gif') (('It', 'eye shape nose curve right Solid mustaches'), 'see') (('task', 'evolutionary network'), 'become') (('obtained', '3rd process'), 'call') (('you', 'model'), 'build') (('faster more accurate results', 'time'), 'develop') (('optimization other they', 'weight parameters'), 'initialize') (('weight parameters', 'previous layers'), 'use') (('number', 'layer outputs'), 'ignore') (('neural convolutional network', 'classical multilayer'), 'be') (('MNIST handwritten digits', 'Kannada digits handwritten dataset'), 'practice') (('even you', 'cat'), 'evaluation') (('Padding 150 you', 'build section'), 'decide') (('it', '_ _'), 'prepare') (('algorithms', 'global minimum'), 'use') (('layer', 'prior layer'), 'have') (('Totaly we', 'training set'), 'have') (('normalizing', 'notionally common scale'), '0') (('this', 'output directly dimensions'), 'vary') (('Why you', 'optimum value'), 'normalize') (('you', 'Gradient Descent https algorithms here ruder'), 'want') (('Kernel', 'Kaggle competition dataset'), 'be') (('parameters 0 gamma', 'Batch Norm Return Contents'), 'regularize') (('how much data', 'filter pattern'), 'observe') (('kernel', 'GPU'), 'check') (('simple part', 'examples'), 'make') (('png _ Why we', 'Padding _ _ Return Contents'), '_') (('1 text end entrance 5 3 2 2 output 2 2 picture', 'RGB 2nd dimensions'), 'png') (('hidden layers', 'layers pooling typically convolutional layers'), 'consist') (('we', '1'), '_') (('Edge information', 'multiple times thanks padding'), 'convolute') (('we', 'float type'), 'in') (('we', 'optimizer algorithm'), 'content') (('I', 'you'), 'content') (('training three building', 'model'), 'building') (('Normalized neurons', 'trained gamma'), 'have') (('learning rate', 'which'), 'factor') (('_ _ again ongoing deeper _', 'even human face'), 'detect') (('same explanation', 'CNN filters'), 'be') (('it', 'generally applications'), 'be') (('more advanced algorithms', '_ different speeds'), '_') (('we', 'training sets'), 'understand') (('don t', 'operation'), 'have') (('Batch Normalization Return Batch Norm Return Batch 0 Contents 0 Normalization', 'activations'), 'content') (('keras 2019 07 08 imagedatagenerator', 'Rosebrock https Adrian www'), 'com') (('method', 'training mini batch'), 'consist') (('Then we', 'quantity low labels'), 'use') (('130 Why we', '133 3'), 'Padding') (('I', 'necessity'), 'give') (('f', 'filter kernel horizontal vertical matrix'), 'be') (('Kaggle', 'test set predictions'), 'evaluate') (('Convolutional Neural Networks Return Deep Learning Convolutional Neural Network 0 CNN', 'most commonly visual imagery'), 'content') (('success', 'dogs'), 'be') (('K', 'total classes'), 'calculate') (('Numerically high values', 'pattern compliance'), 'be') (('we', '54000 28 28 1 dimensions'), 'prefer') (('classification how model', 'actual class'), 'show') (('You', '1'), 'leave') (('answer', 'overfitting'), '_') (('they', 'labels'), 'be') (('filter', 'matrix'), 'imagine') (('Stride Sliding amount', 'stride S'), 'declare') (('which', '1'), 'calculate') (('gain', 'relative selected value'), 'prevent') (('so convolution', 'output'), 'think') (('Example', 'Convolution Operation Return Contents'), 'conv2') (('You', 'other numbers'), '0') (('we', 'test set'), 'determine') (('brain', 'final decision'), 'recognize') (('So kind', 'validation set'), 'form') (('when dimensions', 'output volume'), 'use') (('n l', 'output horizontal vertical matrix'), 'be') (('it', 'model'), 'be') (('Why we', 'Pooling Return Contents'), 'be') (('us', 'cats'), 'use') (('we', 'validation set performance'), 'be') (('we', 'more same time'), 'use') (('Quantity', '3x3 images'), 'label') (('we', 'extracted features'), 'use') (('we', 'objective'), 'handle') (('We', 'new filter'), 'detect') (('Some', 'them'), 'be') (('n', 'input 1 horizontal vertical matrix'), 'be') (('we', 'predictions'), 'have') (('151 Why we', 'Keras'), 'Layer') (('changes', '_ dataset _'), '_') (('You', 'Christian https arxiv'), 'read') (('rate lr', 'normal operation'), 'reduce') (('you', 'roughly range'), 'specify') (('we', 'model'), 'content') (('that', 'CNN'), 'approach') (('It', 'term memory network recurrent such long short layer'), 'use') (('we', 'validation set'), 'Set') (('Convolution', 'specialized linear operation'), 'be') (('We', '28 28 dimension handwritten pics'), 'let') (('output So images', 'h c dimension'), 'be') (('They', 'prediction'), 'give') (('network', 'mathematical operation'), 'have') (('It', 'small groups'), 'select') (('Result', 'corner output upper new matrix'), 'be') (('we', 'CNN next layer'), 'consider') (('some', 'just horizontal lines'), 'have') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["account", "accuracy", "advanced", "algorithm", "analyze", "anomaly", "answer", "application", "architecture", "article", "augmentation", "author", "auto", "average", "backpropagation", "basic", "batch", "beta", "brain", "build", "calculate", "case", "cat", "cell", "channel", "check", "choose", "classification", "code", "colored", "column", "comment", "competition", "computation", "computer", "confusion", "content", "context", "convert", "convolution", "convolutional", "correct", "could", "create", "csv", "curve", "data", "dataset", "detect", "difference", "dimension", "direction", "directly", "distributed", "distribution", "effect", "end", "entropy", "equation", "error", "evaluate", "evaluation", "even", "every", "everyone", "expression", "extract", "extraction", "eye", "f1", "face", "factor", "faster", "feature", "feed", "file", "filter", "final", "find", "fit", "fitting", "float", "following", "formula", "found", "frac", "function", "gamma", "general", "generalization", "generate", "gradient", "grid", "ground", "group", "handle", "height", "high", "https towardsdatascience", "human", "idea", "image", "improve", "improvement", "increase", "initialize", "input", "integer", "io", "job", "kernel", "language", "layer", "learning", "least", "leave", "left", "let", "linear", "log", "look", "lower", "lr", "main", "map", "matrix", "max", "maximum", "mean", "measure", "medical", "memory", "method", "metric", "middle", "min", "mind", "mini", "mode", "model", "momentum", "most", "multiple", "name", "need", "network", "neural", "neuron", "new", "next", "no", "non", "norm", "normal", "normalization", "normalize", "normalized", "not", "num", "number", "numpy", "objective", "observation", "operation", "optimization", "optimize", "optimizer", "order", "out", "output", "overfitting", "overview", "padding", "parameter", "part", "pattern", "performance", "performing", "picture", "pixel", "place", "plot", "png", "point", "pooling", "positive", "prediction", "present", "prevent", "problem", "processing", "production", "provide", "purpose", "quantity", "question", "range", "read", "reason", "recommender", "recurrent", "regularization", "relationship", "relative", "representation", "reshape", "result", "right", "row", "sample", "scale", "scaling", "scope", "score", "select", "selected", "set", "several", "shape", "shift", "short", "side", "signal", "simplicity", "single", "site", "size", "slide", "something", "speed", "split", "stage", "stride", "sub", "sum", "target", "task", "term", "test", "text", "think", "threshold", "time", "total", "train", "training", "type", "until", "up", "update", "upper", "validation", "value", "variable", "variance", "vertical", "video", "view", "visualization", "volume", "web", "weight", "while", "who", "width", "wise", "work"], "potential_description_queries_len": 267, "potential_script_queries": ["l2", "preprocessing", "seaborn", "tensorflow"], "potential_script_queries_len": 4, "potential_entities_queries": ["competition", "convolutional", "dataset", "directly", "high", "kernel", "memory", "network", "next", "output", "recurrent", "selected", "shape", "text", "vertical"], "potential_entities_queries_len": 15, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 270}