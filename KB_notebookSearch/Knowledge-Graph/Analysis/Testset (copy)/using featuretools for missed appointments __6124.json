{"name": "using featuretools for missed appointments ", "full_name": " h1 Using Featuretools to Predict Missed Appointments h2 Structuring the Data h2 Generating Features with Deep Feature Synthesis h2 Machine Learning h1 Some Plots ", "stargazers_count": 0, "forks_count": 0, "description": "In order to make predictions at the time the appointment is scheduled we set the cutoff_time for each row to be the scheduled_time. This feature matrix has features like MONTH and WEEKDAY of the scheduled time and also more complicated features like how often do patients not show up to this location locations. We quickly reconstruct the features that were made by hand in the most popular kernel https www. From that table we use Featuretools to automatically generate the features below. This is where the time indices get used. Using Featuretools to Predict Missed AppointmentsIn this notebook we use Featuretools https github. The full list can be found by running ft. Even though ages are numbers it s can be useful to think of them as Categorical or Ordinal. That means that DFS while building features will only use the data that is known as the appointment is made. com loading_data using_entitysets. Note For convenience the next cell has all of the code necessary to create the feature matrix. Here s a short overview of the keywords used target_entity is the entity for which we re building features. The time index and secondary time index notate when certain columns are valid for use. Some PlotsAn interesting workflow with this dataset is to plot generated features to learn about the data. While many types can be detected automatically some are necessarily tricky. com automated_feature_engineering afe. Feature engineering requires that we use what we understand about the data to build numeric rows feature vectors which we can use as input into machine learning algorithms. patients and what the index should be e. The primary benefit of Featuretools is that it does not require you to make those features by hand. Machine LearningWe can put the created feature matrix directly into sklearn. EntitySets are a collection of tables with information about relationships between tables and semantic typing for every column. To do that we only need to specify where we start appointments the name of the new entity e. Structuring the DataWe are given a single table of data. We have applied and stacked simple functions called primitives such as MONTH WEEKDAY and PERCENT_TRUE to build features across all the Entities in our EntitySet. Each row represents a scheduled appointment and our goal is to predict if the patient actually shows up for that appointment. Changing the variable type will change which functions are automatically applied to generate features. PERCENT_TRUE no_show will calculate percentage of patients of at this location that haven t shown up in the past. In particular because of our secondary time index it won t use the label to create features. 5 with similar scores for F1 and K first. We ll go through the content step by step in the next section. As an example the feature locations. com somrikbanerjee predicting show up no show and make some other interesting features automatically. PERCENT_TRUE appointments. approximate 3h rounds cutoff times into blocks that are 3 hours long for faster computation cutoff_time is a dataframe that says when to calculate each row verbose True makes the progress barFor more information see the documentation https docs. Instead the requirement is that you pass in what you know about the data. As an example computers tend to read age as numeric. We re going to show how to pass in information about semantic types of columns load in a dataframe to an EntitySet and tell the EntitySet about reasonable new Entities to make from that dataframe. We take unique values from patient age neighborhood and gender and make a new Entity for each whose rows are the unique values. Finally we build new entities from our existing one using normalize_entity. Having those additional Entities and Relationships tells the algorithm about reasonable groupings which allows for some neat aggregations. Data Wrangling After loading the data with pandas we have to fix typos in some column names but we change others as well to suit personal preference. Show the size of the data in a print statement This is all of the code from the notebook No need to run read this cell if you re running everything else List the semantic type for each column Use those variable types to make an EntitySet and Entity from that table Add a patients entity with patient specific variables Make locations ages and genders Take the index and the appointment time to use as a cutoff time Rename cutoff time columns to avoid confusion Make feature matrix from entityset cutoff time pair List the semantic type for each column Make an entity named appointments which stores dataset metadata with the dataframe Make a patients entity with patient specific variables Make locations ages and genders Show the patients entity Take the index and the appointment time to use as a cutoff time Rename columns to avoid confusion Generate features using the constructed entityset. By explicitly marking those we can avoid using data from the future while creating features. com Featuretools featuretools to automatically generate features relating to when patients don t show up for doctor appointments. list_primitives max_depth 3 says to stack up to 3 primitives deep. Notice that we specified an index a time index a secondary time index and the variable_types from the last cell as keyword arguments. By doing so you can get very different feature matrices. It takes roughly 20 minutes of work to structure any data and make your first feature matrix using Featuretools. Feel free to fork this kernel and modify the parameters. Here we ll show the number of visits by neighborhood and the likelihood to show up by neighborhood and age as created by DFS. The variable_types dictionary is a place to store information about the semantic type of each column. Generating Features with Deep Feature SynthesisWith our data structued in an EntitySet we can immediately build features across our entity and relationships with Deep Feature Synthesis DFS https docs. We ll walk through the steps now. Next we make an entity appointments We have turned the dataframe into an entity by calling the function entity_from_dataframe. Supposing that we don t want to use our label to predict itself we either need to specify that it is valid for use after the other columns or drop it out of the dataframe entirely. It would be equally easy to make a feature matrix for the locations entity agg_primitives and trans_primitives are lists of which primitives will be used while constructing features. Similar to the other kernels we do not do a good job predicting no shows. This dataset is a single table of appointments with more than sixty thousand unique patients. That knowledge is stored in a Featuretools EntitySet https docs. With one unshuffled train test split our roc_auc_score is roughly. ", "id": "sjrothsc/using-featuretools-for-missed-appointments", "size": "6124", "language": "python", "html_url": "https://www.kaggle.com/code/sjrothsc/using-featuretools-for-missed-appointments", "git_url": "https://www.kaggle.com/code/sjrothsc/using-featuretools-for-missed-appointments", "script": "sklearn.metrics gridplot recall_score figure precision_score show bokeh.io numpy output_notebook ColumnDataSource bokeh.layouts bokeh.models sklearn.ensemble sklearn.model_selection f1_score RandomForestClassifier featuretools.variable_types pandas plot_f1 accuracy_score plot_roc_auc bokeh.models.sources featuretools HoverTool roc_auc_score roc_curve train_test_split bokeh.plotting plot_kfirst ", "entities": "(('primitives', 'features'), 'be') (('more information', 'documentation https docs'), 'approximate') (('Finally we', 'normalize_entity'), 'build') (('we', 'shows'), 'do') (('Here we', 'DFS'), 'show') (('haven t', 'past'), 'calculate') (('we', 'row'), 'in') (('we', 'machine learning algorithms'), 'require') (('we', 'Deep Feature Synthesis DFS https docs'), 'structue') (('which', 'neat aggregations'), 'tell') (('We', 'EntitySet'), 'apply') (('We', 'entity_from_dataframe'), 'make') (('knowledge', 'Featuretools EntitySet https docs'), 'store') (('full list', 'ft'), 'find') (('Structuring', 'data'), 'give') (('we', 'automatically features'), 'use') (('you', 'data'), 'be') (('EntitySets', 'semantic column'), 'be') (('variable_types dictionary', 'column'), 'be') (('We', 'dataframe'), 'go') (('next cell', 'feature necessary matrix'), 'note') (('appointment', 'only data'), 'mean') (('patient', 'actually appointment'), 'represent') (('also more complicated how often patients', 'location locations'), 'feature') (('rows', 'each'), 'take') (('PlotsAn interesting workflow', 'data'), 'be') (('index secondary when certain columns', 'use'), 'notate') (('we', 'as well personal preference'), 'wrangle') (('it', 'dataframe'), 'need') (('it', 'Categorical'), 'be') (('that', 'kernel https most popular www'), 'reconstruct') (('example computers', 'numeric'), 'tend') (('Machine LearningWe', 'directly sklearn'), 'put') (('it', 'hand'), 'be') (('feature first matrix', 'Featuretools'), 'take') (('We', 'next section'), 'go') (('we', 'features'), 'by') (('train one unshuffled test', 'roc_auc_score'), 'be') (('so you', 'feature very different matrices'), 'get') (('entity', 'constructed entityset'), 'show') (('only where we', 'entity new e.'), 'need') (('we', 'keyword arguments'), 'notice') (('functions', 'automatically features'), 'change') (('patients don when t', 'doctor appointments'), 'featuretool') (('we', 'building features'), 's') (('dataset', 'sixty more than thousand unique patients'), 'be') (('we', 'Featuretools https github'), 'use') (('it', 'features'), 'in') ", "extra": "['gender', 'patient', 'test']", "label": "Perfect_files", "potential_description_queries": ["age", "algorithm", "build", "calculate", "cell", "code", "collection", "column", "computation", "confusion", "content", "create", "data", "dataframe", "dataset", "detected", "dictionary", "directly", "doctor", "drop", "engineering", "entity", "every", "everything", "faster", "feature", "fix", "found", "function", "future", "gender", "generate", "generated", "hand", "index", "input", "itself", "job", "kernel", "knowledge", "label", "learn", "learning", "likelihood", "list", "load", "matrix", "max_depth", "metadata", "most", "name", "need", "neighborhood", "new", "next", "no", "not", "notebook", "number", "numeric", "order", "out", "overview", "pair", "patient", "percentage", "place", "plot", "predict", "print", "re", "read", "reconstruct", "row", "run", "running", "set", "short", "similar", "single", "size", "split", "stack", "start", "step", "store", "structure", "table", "test", "think", "those", "through", "time", "train", "type", "unique", "up", "valid", "variable", "walk", "while", "work", "workflow"], "potential_description_queries_len": 103, "potential_script_queries": ["figure", "gridplot", "numpy", "sklearn"], "potential_script_queries_len": 4, "potential_entities_queries": ["most", "unique"], "potential_entities_queries_len": 2, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 107}