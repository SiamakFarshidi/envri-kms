{"name": "nba salaries prediction in 2017 2018 season ", "full_name": " h1 Contents h1 1 Introduction Questions h1 2 Methods Results h2 2 1 Data Cleaning h2 2 2 Exploratory Analysis h3 2 2 1 Descriptive Statistics h3 2 2 2 Statistical Inference h3 2 2 3 Probability Distributions h2 2 3 Modelling h3 2 3 1 Feature Engineering h4 1 Creating Features h4 2 Pearson s R Square Correlation h4 3 Multicollinearity Analysis h3 2 3 2 Regression h4 1 Measure of Goodness RMSE h4 2 Selection of Model Multivariate Cross Validation and Bias Variance Trade off h4 3 Regularization Ridge Lasso and ElasticNet h3 2 3 3 Classification h4 1 Measure of Goodness Accuracy Score and Confusion Matrix h4 2 Selection of Model KNN SVM Na\u00efve Bayes Decision Tree Logistic Regression h4 3 Comparison Model Tuning Learning Curve and Curse of Dimensionality h1 3 Recommendations Discussions h2 Recommendations h2 Discussions h2 Reference ", "stargazers_count": 0, "forks_count": 0, "description": "H1 There is significant difference of salaries between players from USA and not. In this case the t statistics 0. The first way is to increase the size of dataset the second way is to choose a suitable model complexity and the third way is to use regularization to reduce the value of coefficient. Setup a knn classifier with k neighbors https medium. The gamma parameter defines how far the influence of a single training example reaches with low values meaning far and high values meaning close. using median value of each column to fill the N A values because it will not be influened by outliers. Learning Curve is the process to see the change of correctness within the quantity of data set. In order to accept or reject our hypothesis we use cross validation to separate our data into training set and validation set 8 2. Naive BayesIn Naive Bayes we assume that the features are independent from each other. In this report the 3 key questions are What are the most important 4 features that influence the salary What are the most suitable regression and classification models to predict players salaries And how do the models work What recommendations can be made In order to answer the questions data cleaning exploratory analysis and data modelling methods will be applied. 3 Classification 1 Measure of Goodness Accuracy Score and Confusion MatrixAccuracy Score is straight forward for it tells us the probability of the right answers that your model can predict. to 1 2 3 4 0 x_train_values_list np. 3 Regularization Ridge Lasso and ElasticNetThere are 3 ways to solve overfiting. Here we use KNN SVM to fit the Binary target variables and use others to fit the Nominal variables. The most suitable point is not the highest point in training set but a balanced point which performs not so bad in both training and validation sets. make copy to avoid changing original data when imputing. And maybe that is the reason why my model s RMSE reachs over million. Although I cannot apply these models this time I believe I will understand them in the nearing future. Decision TreeThere are three ways to build a decision tree. Therefore we use imputation method to fill in the missing values. It is also a good way to overcome the adverse impact of overfitting for a big size dataset can make a complex model performs well than a small size dataset. The higher the gamma value it tries to exactly fit the training data set. 2 Selection of Model Multivariate Cross Validation and Bias Variance Trade offAt first we use multivariate linear regression to build our initial model. sample variance Why does Bessel s correction use N 1 https en. loading pandas and numpy for data cleaning and exploratory anaysis. Lastly Confusion Matrix tells us the performance on different target groups. 3 Recommendations Discussions Recommendations What are the most important 4 features that influence the salary Through our modelling process the most important 4 features are draft number age WS and BPM. Selecting Features Creating Features buiding binary categories in order to make classifications prediction normal 0 star 1 copy_data. DiscussionsIn this mini project there are some technical limitations such as normalization methods model selections and stacking skills. 1 Feature Engineering 1 Creating Features 2 Pearson s R Square Correlation 3 Multicollinearity Analysis 2. As it is a discrete value we can build a probability mass function about age. net li8zi8fa article details 76176597 GaussianNB MultinomialNB BernoulliNB http www. array x_train_values. For large values of C the optimization will choose a smaller margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Also the identity information is captured such as age draftnumber and so on. So we introduce the l to make the coefficients smaller than before. At the beggining of this report we planned to apply other advanced models such as random forests ADBoost and so on. If it is larger than 10 we think that the multicollinearity is very strong and the feature should not be included. com sflender comparing lin regression ridge lasso ElasticNet https www. edu jcrouser SDS293 labs lab10 py. Firstly the dataset should be cleaned in order to make it easier to build model. Therefore we choose NBA_DraftNumber Age WS BPM as our features for modelling. Apart from that the histogram of salaries can make it clear to see the distribution of NBA salaries. So if we want to do the feature selection we can choose Lasso. Compared with MSE and MAE RMSE can provide the same dimensionality with target variables and the sqaure function can make the measurement more precise than MAE when comparing different models. 2 extract columns convert 1 2 3. Secondly in order to get familiar with the dataset this report will use descriptive statistics statistical inference and concepts of probability distributions to make a description of the potential variables that we need to choose for moddelling. In other words with low gamma points far away from plausible seperation line are considered in calculation for the seperation line. 2 Statistical Inference 2. The Regularization parameter tells the SVM optimization how much you want to avoid misclassifying each training example. Continuing the Country problem a Bernoulli Distribution can be applied to see the difference between these two groups. com joparga3 2 tuning parameters for logistic regression Transform to df for easier plotting Learning Curve Number of folds in cross validation Evaluation metric Use all computer cores 50 different sizes of the training set Create means and standard deviations of training set scores Create means and standard deviations of validation set scores Draw lines Draw bands Create plot Curse of Dimensionality. Conversely we reject the null. 0 using shrinking heuristics stopping criterion tolerance no need to enable probability estimates 200 MB cache size all classes are treated equally print the logs no limit let it run will use one vs rest explicitly NB assumes that the features themselves are not correlated to each other. As it is shown on the Figure 5 when degree 1 both of training and validation set s RMSE are quite low. CART is for binary target variables ID3 is for nomial attributes and C4. And the performance of different models is shown above where 2nd order polynomial regression performs the best. KNN SVMThe use of gamma is similar to k in KNN. Secondly the correlations between players and their stats are not so strong which means that there are the situations that players are overpaid or underpaid. In our models the curse of dimensionality is obvious in SVM Naive Bayes and Logistic Regression where the high dimensional features cause overfiting. more features means more evidence in different dimensions but it could cause overfitting. Therefore a model which can predict players salaries according to their performance data is necessary in the league. Similarly within my ability stacking skills is not approachable yet. Methods Results 2. This also means that the salary problem needs to be paid attention by managers. 1 Feature Engineering 1 Creating FeaturesIn order to make classification prediction it is neccessary to create discrete target variables according to players salaries. com select rows columns by name or index in dataframe using loc iloc python pandas Regularization parameter kernel type rbf working fine here default value kernel coefficient change to 1 from default value of 0. Here we choose Country as our feature. when alpha level. And when it comes to 4nd order polynomial regression it causes overfiting. When the size of training size is small the score of training set is very high but the score of vaidation set is very low which causes overfitting. com all things ai in depth parameter tuning for svc 758215394769 https medium. Where as high gamma means the points close to plausible line are considered in calculation. In this part we focus on regularization and select degree 4 to test the effectiveness of these three methods. 1 Data Cleaning 2. Also the Age BPM USG VORP MP and PER also share the similar high VIF so some of them should be discarded. 05 two tailed test If t statistics t critical we retain the null hypothesis. Then we assume that our model does not cause overfitting or underfitting. 2 Regression 1 Measure of Goodness RMSERoot Mean Squrare Error is a measure of how far the predicted points away from the real points. 3 Classification 1 Measure of Goodness Accuracy Score and Confusion Matrix 2 Selection of Model KNN SVM Na\u00efve Bayes Decision Tree Logistic Regression 3 Comparison Model Tuning Learning Curve and Curse of Dimensionality3. 1 Data CleaningMissing values and outliers would make the modelling process difficult. com lesliexong p 6907642. arange 1 20 https www. Here we retain the null hypothesis that the 1nd order polynomial model does not cause high bias. While in other models maybe it is because of the number of features are not enough the curse does not appear. Model selection is important. H0 There is no significant difference of salaries between players from USA and not. drop USA NOT axis 1 inplace True Then we build nomial categories 0 edge players 1 normal players 2 all stars 3 superstars number of variables for heatmap Using scatter plots to detect the correlation value https etav. It is oriented from Kaggle the largest online machine learning and data science community Narayanan Shi Rubinstein 2011. edu stat501 node 347 RMSE for testing data Cross Validation Spliting dataset into three parts for training validation and testing respectively. Firstly model tuning is quite silimar with Bias Variance Trade off. We can try non binary target variables. In Neural Networks IJCNN The 2011 International Joint Conference on pp. com sflender comparing lin regression ridge lasso https www. net guomutian911 article details 43317019 Bayes Theorem P a b P b a P a P b Bernoulli distribution Define the dataset Create bars Create names on the x axis Show graphic probability mass function S total 1 When variables are continous it becomes Probability Denstiy Function. The meaning of regularization can be considered as punishiment. Also players age is one of the most important issues in NBA for a player can make more profits if he can play longer. However if we want to know the Accuracy Score of each target group it is more suitable to use Confusion Matrix which will show the comparison of predicted values and real values in each group. Thridly different models have different sensitivities to dimensionality. But I have not applied it because I think this will affect the real data s meaning and also I think it is meaningless because our RMSE is a relative value not an absolute one. 2 Exploratory Analysis 2. The methods here include mean median mode standard deviation interquatile range. com jack89roberts top 7 using elasticnet with interactions Comparison http www. Also multiple regression is the one that I cannot solve in programming language because I cannot find any example in others work. This should be noticed by the teams managers. As expected the OWS DWS and WS have a high variance inflation factor because they explain the same meaning. 0 using shrinking heuristics stopping criterion tolerance no need to enable probability estimates 200 MB cache size all classes are treated equally print the logs no limit let it run will use one vs rest explicitly Confusion Matrix Learning Curve Number of folds in cross validation Evaluation metric Use all computer cores 50 different sizes of the training set Create means and standard deviations of training set scores Create means and standard deviations of validation set scores Draw lines Draw bands Create plot curse of dimensionality one or two features are simple but it cannot recognize and divide our categories. 1 Descriptive StatisticsIn order to understand our data descriptive statistics can be used to get a well understand of the dataset. In this case we choose median number of the whole column to fill in the missing variables because a median value will not be influened by outliers. If the number of dimensions is similar to the amount of data each or several samples may form one class which may make the traing model performs well in the training set but losing its ability to predict the testing set at the same time. Bias Variance Trade off training RMSE validation RMSE RMSE for testing data At first we calculate the RMSE before regularization. Decision Tree model performs the best in Nominal target classification with 0. What are the most suitable regression and classification models to predict players salaries And how do the models work The most suitable regression model is 2nd order polynomial regression which has the RMSE of about 4. Therefore the VIF value can be chosen to detect the multicollinearity. org wiki Bessel 27s_correction Proof_of_correctness_ _Alternate_3 covariance https blog. io python vif_factor_python. Defining the dataset s new name in this project. Welcome to my Mini Project. 3 Probability Distributions 2. This means that the managers and coaches of all 30 teams have to focus on finding those who are ability to put their teams to another level within their budgets. com ff8040 Number of folds in cross validation Evaluation metric Use all computer cores 50 different sizes of the training set Create means and standard deviations of training set scores Create means and standard deviations of validation set scores Draw lines Draw bands Create plot curse of dimensionality one or two features are simple but it cannot recognize and divide our categories. Focusing on coefficients and we can find that Ridge regularization drives parameters to smaller values. net app_12062011 article details 52136117 Model Tuning https www. 3 Comparison Model Tuning Learning Curve and Curse of DimensionalityThe process of Model Tuning is similar to Bias Variance Trade off which is to find the balance that provide not only the high score of the training set but also good ability to predict the testing set. read the data Matplotlib package for visualisation. When the model is too complex the values of coefficients are very large. For example SVM does better in predicting label 0 while KNN performs better in predicting label 1. The similar phenomenon happens in another comparison which can be touted as an important way to see the details of our models prediction. From Andrew Ng s Open Course Normalization can change the big value features into a small value one which may make the lost function more accurate. But if the multicollinearity exits Lasso will turn its coefficients to 0 while Ridge will not erase any feature value. 2 Methods Results 2. In all although the effect of regularization is significant it is much better to choose the correct parameters and features. Value Creation Comparative Netnographic Study of Two NBA Online Communities. If you think this is helpful or it has any problem please upvote it and discuss your idea with me. 3 Probability DistributionsBayes Theorem is the fundamental concept of probability. Thirdly we will use regression and classification methods to select build and evaluate our models. Here we can apply it to answer the question such as what is the probability that players salaries are higher than 10 million dollars given that the player is from USA Probability Distribution can make it clear to realize the feature of our variables. 1 Descriptive Statistics 2. Ridge https blog. Determining NBA Free Agent Salary from Player Performance. com diegosch classifier evaluation using confusion matrix 0 edge players 1 normal players 2 all stars 3 superstars Transform to df for easier plotting Learning Curve Number of folds in cross validation Evaluation metric Use all computer cores 50 different sizes of the training set Create means and standard deviations of training set scores Create means and standard deviations of validation set scores Draw lines Draw bands Create plot https blog. Introduction Questions2. After applying our models it is essential to use the model tuning techniques to find the best parameters that fit our dataset. 5 can be applied for continous features whcih is the most suitable in our case. Then we apply the bias variance trade off graph to see whether the assumption is true or not. However it is the mathematical concepts that let me realize that it is meaningless to apply them if I cannot understand the basic algorithm behind them. What recommendations can be made Firstly through our exploratory analysis and modelling process the difference between salaries of overseas and USA players are not significant but the number of USA players are almost 3 times more than that of overseas players. 2 Statistical InferenceIn order to select appropriate features for prediction an independent t test can be applied to calculate whether a feature is significant enough. Different regularization methods perform differently. The most suitable binary classification model is SVM taking the accuracy score of 0. Therefore if the collinearity of our features are low the model will perform better. As the increase of data size the score of training set becomes lower and the validation set s score becomes higher which means that the distance between these two groups are narrowing. This report chooses a dataset named NBA 2017 18 season players salaries. 3 Multicollinearity AnalysisAs we have 8 features now which may contain multicollinearity that make the model inaccurate and cause overfitting. com machine learning 101 chapter 1 supervised learning and naive bayes classification part 1 theory 8b9e361897d5 https blog. 2 Regression 1 Measure of Goodness RMSE 2 Selection of Model Multivariate Cross Validation and Bias Variance Trade off 3 Regularization Ridge Lasso and ElasticNet 2. Because the missing value will have an adverse impact on the building of regression model. However the values of some models parameters are very large while others are quite small such as Naive Bayes which should be paid attention to. com pablovargas naive bayes svm spam filtering for binary target variables Model Tuning 5 fold cross validation How to find K 5 fold cross validation Learning Curve How KNN algorithm performs in both small size data and big size data choose an acceptable color https www. Ending Importing all pakages that necessary. This difference can be an opportunity to recruit more overseas players from other countries for the promotion. However as the size of this dataset only reaches 300 it cannot make sure that if the size is absolutly large more than 10 thousands how well will the curves perform. And if the gamma value is too high it will cause overfitting. Recommendations Discussions 1. In fact this phenomenon is quite popular in the real NBA market. com junyingzhang2018 ridge regression score 0 119 Adjust alpha based on previous result Adjust alpha based on previous result Use alpha 40. Secondly all models share the similar trends in the Learning Curve. Logistic RegressionThrough Model Tuning Learning Curve Curse of Dimensionality and Confusion Matrix we can get some knowledge about models characteristics. Here we create two columns named Binary and Nominal as below 2 Pearson s R Square CorrelationIn order to choose features that are correlated to our target variables the Pearson s R Square Correlation can be applied to choose top 8 features that are most correlated to the salaries. Conversely a very small value of C will cause the optimizer to look for a larger margin separating hyperplane even if that hyperplane misclassifies more points. https thispointer. 7033 which is smaller than t critical then we prefer to think that there is no significant difference of salaries between players from USA and overseas. Extracting two columns Salary and NBA_Country. Therefore I select Decision Tree by reading the slides and select Logistic Regression by watching Andrew Ng s videos. 2 Selection of Model KNN SVM Na\u00efve Bayes Decision Tree Logistic RegressionFive Models are selected to fit our dataset. Curse of Dimensionality is one of the main reasons of overfitting. 4 to predict the test data Lasso https www. Introduction QuestionsIn 2017 18 season the salary cap and the luxury tax of NBA reached 99 million and 119 million dolars respectively Di 2018. html https onlinecourses. We believe that the dataset is suitable because it focuses on not only on ball stats such as scores rebounds and assists but also off ball stats which can capture those who have a big impact on the game without ball on the hand. The first three methods are to read the central tendency of data and the rest are to describe the dispersion of datasets. Ridge Lasso ElasticNet As it is shown on the Figure 5 when degree 1 both of training and validation set s RMSE are quite low. Link prediction by de anonymization How we won the kaggle social network challenge. Firstly we should have a look whether the data is completed or not. This is NBA Salaries Prediction version 3. com question 38121173 https www. But I believe that I will get them done in the future. com joparga3 2 tuning parameters for logistic regression https www. But if we want to keep all features on the list we prefer Ridge. But when degree 4 the difference between training set s RMSE and validation set s RMSE is obvious. com machine learning 101 chapter 2 svm support vector machine theory f0812effc72 Regularization parameter kernel type rbf working fine here default value kernel coefficient change to 1 from default value of 0. I hope everybody can learn from it. tolist convert dataframe The best result is captured at k 5 hence it is used for the final model. html Gaussian is for continous features \u503c\u5f97\u6ce8\u610f\u7684\u662f \u5728\u79bb\u6563\u6837\u672c\u4e5f\u5c31\u662f\u57fa\u4e8e\u9891\u7387\u7684\u4f30\u8ba1\u4e2d \u5982\u679c\u67d0\u4e2a\u7279\u5f81fn\u672a\u5728\u8bad\u7ec3\u96c6\u7684\u7c7b\u522bci\u4e2d\u51fa\u73b0\u8fc7 \u90a3\u4e48P fn ci \u9879\u4e3a0\u4f1a\u5bfc\u81f4\u6574\u4e2a\u4f30\u8ba1\u4e3a0\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u7684\u7279\u5f81\u4fe1\u606f \u8fd9\u6837\u7684\u4f30\u8ba1\u663e\u7136\u662f\u4e0d\u51c6\u786e\u7684 \u6240\u4ee5\u901a\u5e38\u9700\u8981\u5bf9\u4e8e\u6837\u672c\u8fdb\u884c\u6837\u672c\u4fee\u6b63\u4fdd\u8bc1\u4e0d\u4f1a\u67090\u6982\u7387\u51fa\u73b0 \u6bd4\u5982\u91c7\u7528laplace\u6821\u51c6 \u5bf9\u6ca1\u7c7b\u522b\u4e0b\u6240\u6709\u5212\u5206\u7684\u8ba1\u6570\u52a01 \u8fd9\u6837\u5982\u679c\u8bad\u7ec3\u6837\u672c\u96c6\u6570\u91cf\u5145\u5206\u5927\u65f6 \u5e76\u4e0d\u4f1a\u5bf9\u7ed3\u679c\u4ea7\u751f\u5f71\u54cd listone\u4fee\u6b63\u5219\u662f\u52a0\u4e00\u4e2a0 1\u4e4b\u95f4\u7684\u6570 \u548c\u591a\u5143\u6734\u7d20\u8d1d\u53f6\u65af\u4e2d\u901a\u8fc7\u7279\u5f81\u51fa\u73b0\u9891\u7387\u6765\u8ba1\u7b97P fn ci \u4e0d\u540c \u4f2f\u52aa\u5229\u6a21\u578b\u53ea\u8003\u8651\u51fa\u73b0\u4e0d\u51fa\u73b0\u7684\u4e8c\u503c\u95ee\u9898 for i in np. com drgilermo stephen curry s decision tree Transform to df for easier plotting Learning Curve Number of folds in cross validation Evaluation metric Use all computer cores 50 different sizes of the training set Create means and standard deviations of training set scores Create means and standard deviations of validation set scores Draw lines Draw bands Create plot Curse of Dimensionality logistic regression LR https www. net hzw19920329 article details 77200475 https www. From the positive skewed histogram we can read that more than 33 of NBA players salaries are less than 3 million dollars while only no more than 40 players salaries are more than 25 million dollars. ", "id": "aishjun/nba-salaries-prediction-in-2017-2018-season", "size": "17270", "language": "python", "html_url": "https://www.kaggle.com/code/aishjun/nba-salaries-prediction-in-2017-2018-season", "git_url": "https://www.kaggle.com/code/aishjun/nba-salaries-prediction-in-2017-2018-season", "script": "sklearn.metrics DataFrame cross_val_score sklearn.naive_bayes sklearn.tree Ridge KNeighborsClassifier Lasso DecisionTreeClassifier variance_inflation_factor mean_squared_error seaborn numpy sklearn.pipeline learning_curve make_pipeline precision_recall_fscore_support PolynomialFeatures rmse_model rmse_cv OneVsRestClassifier sklearn.model_selection sklearn metrics confusion_matrix KFold matplotlib.pyplot pandas fillWithMedian ElasticNet LogisticRegression accuracy_score ElasticNetCV GridSearchCV sklearn.neighbors SVC sklearn.linear_model sklearn.preprocessing GaussianNB sklearn.svm tree sklearn.multiclass train_test_split LinearRegression statsmodels.stats.outliers_influence ", "entities": "(('I', 'them'), 'be') (('phenomenon', 'NBA quite real market'), 'be') (('where 2nd order polynomial regression', 'best'), 'show') (('5 hence it', 'final model'), 'dataframe') (('some', 'them'), 'share') (('USA number', 'overseas players'), 'make') (('players', 'players'), 'be') (('classifications prediction', 'normal 0 star 1 copy_data'), 'select') (('median value', 'outliers'), 'choose') (('Use', 'Draw Create plot https blog'), 'star') (('absolutly large more than 10 thousands how well curves', 'only 300'), 'make') (('1nd order polynomial model', 'high bias'), 'retain') (('salary also problem', 'managers'), 'mean') (('season 2017 18 players', 'dataset'), 'choose') (('features', 'other'), 'estimate') (('which', 'attention'), 'be') (('that', 'most salaries'), 'create') (('model', 'that'), 'be') (('we', 'three methods'), 'focus') (('performs', 'size color https big acceptable www'), 'filter') (('Lastly Confusion Matrix', 'target different groups'), 'tell') (('hyperplane', 'training points'), 'choose') (('traing model', 'same time'), 'be') (('which', 'models prediction'), 'happen') (('VIF Therefore value', 'multicollinearity'), 'choose') (('copy', 'original data'), 'make') (('coefficients', 'l'), 'introduce') (('that', 'testing also good set'), 'be') (('data', 'look'), 'have') (('difference', 'promotion'), 'be') (('3 players less than million only no more than 40 salaries', 'NBA players salaries'), 'read') (('Regularization Ridge 3 Lasso', '3 overfiting'), 'be') (('curse', 'features'), 'appear') (('which', 'league'), 'be') (('model', 'overfitting'), 'AnalysisAs') (('05 t statistics two tailed t we', 'null hypothesis'), 'test') (('distance', 'two groups'), 'become') (('features', 'other'), 'Bayes') (('Curse', 'overfitting'), 'be') (('them', 'future'), 'believe') (('lost function', 'small value'), 's') (('feature', 't independent test'), 'apply') (('model Firstly tuning', 'Bias Variance quite Trade'), 'be') (('1 normal players', 'correlation value https etav'), 'drop') (('ID3', 'nomial attributes'), 'be') (('joparga3', 'regression https logistic www'), 'com') (('RMSE set', 'training set'), 'but') (('How we', 'network kaggle social challenge'), 'prediction') (('it', '1'), 'detail') (('KNN SVMThe use', 'KNN'), 'be') (('it', 'variables'), 'apply') (('assumption', 'graph'), 'apply') (('Secondly models', 'Learning Curve'), 'share') (('Here we', 'Nominal variables'), 'use') (('It', 'largest online machine learning'), 'orient') (('very feature', '10'), 'think') (('he', 'more profits'), 'be') (('it', 'me'), 'have') (('Therefore I', 'videos'), 'select') (('Logistic where high dimensional features', 'SVM Naive Bayes'), 'be') (('it', 'polynomial regression'), 'cause') (('sample Why Bessel', 'correction N'), 'variance') (('data modelling methods', 'exploratory analysis'), 'be') (('third way', 'coefficient'), 'be') (('2 Selection', 'dataset'), 'select') (('Use', 'Draw bands Create Dimensionality'), 'Transform') (('it', 'overfitting'), 'mean') (('hyperplane even misclassifies', 'hyperplane'), 'cause') (('you', 'training example'), 'tell') (('Probability DistributionsBayes 3 Theorem', 'fundamental probability'), 'be') (('we', 'regularization'), 'Trade') (('net app_12062011 article', 'Model Tuning https 52136117 www'), 'detail') (('This', 'teams managers'), 'notice') (('salary Introduction QuestionsIn 2017 18 cap', '99 119 million dolars'), 'reach') (('too values', 'coefficients'), 'be') (('far values', 'low values'), 'define') (('points', 'calculation'), 'consider') (('edu stat501', 'validation'), 'node') (('it', 'model'), 'clean') (('it', 'categories'), 'estimate') (('which', 'real group'), 'be') (('who', 'budgets'), 'mean') (('discrete we', 'age'), 'build') (('very which', 'overfitting'), 'be') (('example', 'others'), 'be') (('SVM', '0'), 'be') (('set', '1 training'), 'show') (('GaussianNB MultinomialNB BernoulliNB', '76176597'), 'detail') (('we', 'moddelling'), 'in') (('model', 'overfitting'), 'assume') (('Country Bernoulli Continuing Distribution', 'two groups'), 'problem') (('methods', 'deviation interquatile here mean median mode standard range'), 'include') (('Use', 'Draw Create plot Dimensionality'), 'com') (('they', 'same meaning'), 'have') (('Descriptive StatisticsIn data descriptive 1 statistics', 'dataset'), 'order') (('it', 'categories'), 'use') (('validation', '8 2'), 'in') (('we', 'such random forests'), 'plan') (('that', 'most important 4 features'), 'Recommendations') (('Confusion we', 'models characteristics'), 'Curse') (('Decision Tree model', '0'), 'perform') (('balanced which', 'so bad training sets'), 'be') (('Therefore we', 'missing values'), 'use') (('too it', 'overfitting'), 'cause') (('missing value', 'regression model'), 'have') (('RMSE', 'real meaning'), 'apply') (('it', 'NBA salaries'), 'make') (('Therefore we', 'modelling'), 'choose') (('model', 'features'), 'perform') (('we', 'smaller values'), 'focus') (('Regularization parameter kernel type rbf working fine', '0'), 'f0812effc72') (('I', 'nearing future'), 'believe') (('who', 'hand'), 'believe') (('complex model', 'size well small dataset'), 'be') (('Comparison', 'www'), 'com') (('why model', 'million'), 'be') (('0 KNN', 'better label'), 'do') (('Mean Squrare Error', 'how far predicted away real points'), 'measure') (('it', 'players salaries'), '1') (('2nd order polynomial which', 'about 4'), 'be') (('\u5728\u79bb\u6563\u6837\u672c\u4e5f\u5c31\u662f\u57fa\u4e8e\u9891\u7387\u7684\u4f30\u8ba1\u4e2d \u90a3\u4e48P fn \u6240\u4ee5\u901a\u5e38\u9700\u8981\u5bf9\u4e8e\u6837\u672c\u8fdb\u884c\u6837\u672c\u4fee\u6b63\u4fdd\u8bc1\u4e0d\u4f1a\u67090\u6982\u7387\u51fa\u73b0 \u5bf9\u6ca1\u7c7b\u522b\u4e0b\u6240\u6709\u5212\u5206\u7684\u8ba1\u6570\u52a01 \u8fd9\u6837\u5982\u679c\u8bad\u7ec3\u6837\u672c\u96c6\u6570\u91cf\u5145\u5206\u5927\u65f6 \u548c\u591a\u5143\u6734\u7d20\u8d1d\u53f6\u65af\u4e2d\u901a\u8fc7\u7279\u5f81\u51fa\u73b0\u9891\u7387\u6765\u8ba1\u7b97P fn \u5982\u679c\u67d0\u4e2a\u7279\u5f81fn\u672a\u5728\u8bad\u7ec3\u96c6\u7684\u7c7b\u522bci\u4e2d\u51fa\u73b0\u8fc7 ci \u9879\u4e3a0\u4f1a\u5bfc\u81f4\u6574\u4e2a\u4f30\u8ba1\u4e3a0\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u7684\u7279\u5f81\u4fe1\u606f listone\u4fee\u6b63\u5219\u662f\u52a0\u4e00\u4e2a0 1\u4e4b\u95f4\u7684\u6570 ci', 'np'), 'be') (('set', '1 training'), 'ElasticNet') (('measurement', 'when different models'), 'provide') (('critical then we', 'USA'), '7033') (('it', 'outliers'), 'use') (('we', 'Ridge'), 'prefer') (('Learning Curve', 'data'), 'be') (('identity Also information', 'age such draftnumber'), 'capture') (('Thridly different models', 'dimensionality'), 'have') (('Multivariate Cross Bias Variance first we', 'initial model'), 'Selection') (('Decision TreeThere', 'decision three tree'), 'be') (('rest', 'datasets'), 'be') (('applied', 'most case'), 'be') (('we', 'Lasso'), 'so') (('it', 'much correct parameters'), 'be') (('that', 'dataset'), 'be') (('Thirdly we', 'models'), 'use') (('Ridge', 'feature value'), 'turn') (('meaning', 'punishiment'), 'consider') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["absolute", "accuracy", "advanced", "age", "algorithm", "answer", "apply", "array", "article", "balance", "basic", "best", "binary", "build", "cache", "calculate", "calculation", "case", "cause", "choose", "classification", "classifier", "cleaning", "clear", "close", "coefficient", "color", "column", "community", "comparison", "computer", "concept", "confusion", "contain", "convert", "copy", "correct", "correction", "correlation", "correlations", "could", "covariance", "create", "criterion", "data", "dataframe", "dataset", "decision", "default", "degree", "depth", "describe", "description", "detect", "df", "difference", "dimensionality", "discrete", "distance", "distribution", "drop", "edge", "effect", "enable", "evaluate", "evaluation", "even", "evidence", "expected", "extract", "fact", "factor", "feature", "fill", "final", "find", "fit", "fn", "fold", "form", "forward", "function", "game", "gamma", "graph", "group", "heatmap", "high", "histogram", "hope", "http", "https 1 www", "hyperplane", "idea", "identity", "include", "increase", "index", "inference", "inflation", "influence", "io", "job", "kaggle", "kernel", "key", "knowledge", "label", "language", "largest", "learn", "learning", "let", "level", "line", "linear", "list", "look", "lost", "lower", "main", "margin", "matrix", "mean", "meaning", "measure", "measurement", "median", "method", "metric", "mini", "missing", "mode", "model", "modelling", "most", "multicollinearity", "multiple", "my", "naive", "name", "need", "network", "new", "no", "node", "non", "normal", "normalization", "not", "null", "number", "numpy", "optimization", "optimizer", "order", "overfitting", "package", "parameter", "part", "perform", "performance", "phenomenon", "player", "plot", "plotting", "point", "positive", "potential", "predict", "prediction", "print", "probability", "problem", "project", "provide", "python", "quantity", "question", "random", "read", "reading", "reason", "reduce", "regression", "regularization", "relative", "report", "rest", "result", "right", "run", "salary", "sample", "scatter", "science", "score", "second", "select", "selected", "selection", "separate", "set", "several", "similar", "single", "size", "standard", "supervised", "support", "svm", "target", "test", "testing", "theory", "think", "those", "through", "time", "total", "training", "tree", "try", "tuning", "turn", "type", "validation", "value", "variance", "vector", "version", "while", "who", "work"], "potential_description_queries_len": 234, "potential_script_queries": ["seaborn", "sklearn"], "potential_script_queries_len": 2, "potential_entities_queries": ["correct", "even", "high", "kernel", "median", "modelling", "normal", "parameter", "random", "training", "value"], "potential_entities_queries_len": 11, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 235}