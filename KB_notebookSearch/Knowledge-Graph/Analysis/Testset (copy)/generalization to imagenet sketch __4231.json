{"name": "generalization to imagenet sketch ", "full_name": " h1 Can pretrained ImageNet models generalize to sketches h1 The Images h1 The Models h1 Results h1 Top 1 Results by Model h1 Top 1 Differences h1 Top 5 Differences h1 Text Summary of Top 1 and Top 5 Differences h1 Best and Worst Predictions h1 The Best Predictions h1 The Worst Predictions ", "stargazers_count": 0, "forks_count": 0, "description": "com github rwightman pytorch image models blob master notebooks GeneralizationToImageNetV2. From working with adversarial attacks and defense in the past Jigsaw is one of those classes that s easy to confuse NN. Again the IG ResNeXts stand apart. The Worst PredictionsLooking at these samples I actually feel the model could be doing even better. Top 5 DifferencesThe same as above for the Top 5 validation scores. Can pretrained ImageNet models generalize to sketches I ve been exploring generalization capabilities of models recently after poking around with the ImageNetV2 https colab. Top 1 results by model2. This is definitely very different from ImageNet samples. Compare differences for Top 1 accuracy between each model on ImageNet 1K and ImageNet Sketch3. We ll be using models and helpers from my PyTorch Image Models TIMM at https github. We ll collect per example losses and top 5 predictions and then display the results. The InceptionResnetV2 is one of the less impacted non IG models while the EfficientNet B2 and MobileNet V3 are hit the hardest. First thing that stood out for me as that they are lacking in color not completely but almost completely void of color. I wanted to explore this further on other datasets that had OOTB compatibility with ImageNet pretrained models. Same as above but for Top 5 differences Top 1 Results by ModelThe figure and text output below display the Top 1 accuracy of the models on the ImageNetSketch dataset. Several models especially Facebook s weakly supervised Instagram tag pretrained ResNeXt models https pytorch. In terms of quality and detail there is a wide range highly detailed sketches with lots of texture to cartoonish or child like drawings. Text Summary of Top 1 and Top 5 DifferencesIf you prefer text an absolute and relative diff ranking of all models by their top 1 and top 5 Best and Worst PredictionsWe re going to re run inference on one of our better models a ResNext101 32x16 pretrained on Instagram tags. You can basically apply any image on top of a jigsaw puzzle and this is indeed the case in examples for this class. There are clearly issues with the dataset in terms of mislabled examples and confusing images that have multiple valid answers. The ModelsThe pretrained models that will be run cover a wide range of performance on ImageNet 1k from mobile optimized MobileNet V3 through the workhorse range in the the ResNet50 SE ResNext50 to some larger higher performing models like PNASNet5 Large and EfficientNet B5 and finally ending up with the full set of released Instagram pretrained ResNeXt101 models. The Best PredictionsWhat does the model predict best Nothing too suprising here aside from upside down washing machine perhaps all of these samples have distinct shapes for their class or good levels of detail. The ImageNet Sketch dataset here fits the bill the same classes as ImageNet 1k but a very different distribution in terms of the images themselves. read_csv a basic validation routine and runner that configures each model and loader metrics for collecting per sample prediction loss details only bother collecting top5 we re also collecting per example loss cleanup checkpoint cache to avoid running out of disk space add some non metric values for charting comparisons create key to identify model in charts load the dataset Run all the models through validation Setup the common charting elements download ImageNet 1k resuls run on my model collection for top 1 top 5 comparisons some helpers for the dumbbell plots draw the ImageNet Sketch dots we re sorted on this draw the original ImageNet 1k validation dots draw the ImageNet Sketch top 5 dots we re sorted on this draw the original ImageNet 1k validation dots create mappings of label id to text and synset re run validation on just one model this time collecting per example losses and predictions a function to display images in a grid and ground truth vs predictions for specified indices. Also visible are other model to model variations in how well they do on Sketch vs original validation. Let s see how the models perform without training them on this dataset. com rwightman pytorch image models The ImagesLooking at a random sampling from the dataset we can get a taste of what the images look like. ResultsWe re going to walk through the results as follows 1. I am not expecting reasonable performance. ipynb dataset and comparing models for runtime performance https colab. You can make a similar case for the shower curtains a tighter crop of those illustrations would have helped. com github rwightman pytorch image models blob master notebooks EffResNetComparison. Right away we see that the Instragram pretrained ResNeXt models are in class of their own and suprisingly doing not too shabbily Top 1 Differences Looking a bit closer we plot the differences between the ImageNet Sketch Top 1 validation scores and the original ImageNet 1k validation scores. Import packages setup our logger and check if we have CUDA linear algebra data processing CSV file I O e. org hub facebookresearch_WSL Images_resnext stood out as having a comparatively lower drop in accuracy when applied to different datasets without retraining them. ", "id": "rwightman/generalization-to-imagenet-sketch", "size": "4231", "language": "python", "html_url": "https://www.kaggle.com/code/rwightman/generalization-to-imagenet-sketch", "git_url": "https://www.kaggle.com/code/rwightman/generalization-to-imagenet-sketch", "script": "make_grid draw_line_horiz OrderedDict collections numpy timm.utils PIL torchvision Image matplotlib.pyplot draw_line_vert runner pandas show_summary label_line_vert show_img transforms matplotlib.lines validate torchvision.utils label_line_horiz ", "entities": "(('Same', 'ImageNetSketch dataset'), 'display') (('tighter crop', 'illustrations'), 'make') (('how models', 'dataset'), 'let') (('I', 'reasonable performance'), 'expect') (('they', 'completely almost completely color'), 'void') (('We', 'https github'), 'use') (('perhaps all', 'detail'), 'predict') (('bit closer we', 'ImageNet validation Sketch Top 1 scores'), 'see') (('Several models', 'Instagram models https especially s weakly tag ResNeXt pytorch'), 'supervise') (('how well they', 'original validation'), 'be') (('images', 'what'), 'model') (('org hub facebookresearch_WSL Images_resnext', 'them'), 'stand') (('I', 'actually model'), 'do') (('This', 'ImageNet definitely very samples'), 'be') (('EfficientNet B2', 'MobileNet hardest'), 'be') (('ImageNet Sketch dataset', 'images'), 'fit') (('that', 'ImageNet pretrained models'), 'want') (('that', 'NN'), 'be') (('CUDA', 'CSV file'), 'setup') (('I', 'ImageNetV2 https recently around colab'), 'pretraine') (('1 Top 5 you', 'Instagram 32x16 tags'), 'prefer') (('We', 'top 5 then results'), 'collect') (('ResultsWe', '1'), 'go') (('confusing that', 'multiple valid answers'), 'be') (('ImageNet 1k validation original dots', 'specified indices'), 'routine') (('this', 'class'), 'apply') (('that', 'released Instagram'), 'cover') ", "extra": "", "label": "No_extra_files", "potential_description_queries": ["absolute", "accuracy", "apply", "basic", "best", "bit", "blob", "cache", "case", "check", "checkpoint", "child", "collection", "color", "could", "create", "crop", "data", "dataset", "detail", "diff", "disk", "display", "distribution", "download", "draw", "drop", "even", "explore", "figure", "file", "function", "generalization", "grid", "ground", "hub", "id", "image", "inference", "key", "label", "linear", "load", "loader", "look", "lower", "metric", "model", "multiple", "my", "non", "not", "out", "output", "past", "per", "perform", "performance", "performing", "plot", "predict", "prediction", "pretrained", "processing", "pytorch", "random", "range", "ranking", "re", "relative", "routine", "run", "runner", "running", "runtime", "sample", "sampling", "set", "setup", "similar", "space", "supervised", "tag", "text", "those", "through", "time", "training", "up", "valid", "validation", "walk", "while"], "potential_description_queries_len": 93, "potential_script_queries": ["numpy", "torchvision", "validate"], "potential_script_queries_len": 3, "potential_entities_queries": ["pretrained", "valid"], "potential_entities_queries_len": 2, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 95}