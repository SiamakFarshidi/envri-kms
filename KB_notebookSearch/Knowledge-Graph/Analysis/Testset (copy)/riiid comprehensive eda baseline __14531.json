{"name": "riiid comprehensive eda baseline ", "full_name": " h1 About the Riiid AIEd Challenge 2020 h1 Table of Contents h1 1 EDA h1 1 1 Exploring Train h1 The target answered correctly h1 1 2 Exploring Questions h1 1 3 Exploring Lectures h1 Example test h1 2 Baseline model ", "stargazers_count": 0, "forks_count": 0, "description": "The goal is to accurately predict how students will perform on future interactions. com rohanrao tutorial on reading large datasets Thanks Rohan. The reading section consists of Part 5 7 Reading Section 75 minutes 100 questions. It remains possible that some questions weren t logged due to other issues that all datasets of mobile users are susceptible to such as if a user lost their connection mid question. The meaning of the tags will not be provided but these codes are sufficient for clustering the questions together. However as the feature works with regards to the CV see Baseline model I also wanted to find out if there is a trend. timestamp int64 the time in milliseconds between this user interaction and the first event completion from that user. However that would make this baseline model a lot more complicated. user_answer int8 the user s answer to the question if any. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedbackThe train dataset is ordered by ascending user_id and ascending timestamp. Below I am plotting the number of answers per user_id against the percentage of questions answered correctly sample of 200. 1 Exploring Train 1. Please note that there is double counting of questions for instance if a question has 5 tags its answers are aggregated in the totals of each of the 5 tags. Your challenge in this competition is a version of that overall task you will predict whether students are able to answer their next questions correctly. To find out I have made 5 bins of timestamp. Baseline modelBelow I am adding user features. Does it help if the prior_question_had_explanation Yes as you can see the percent answered correctly is about 17 higher when there was an explanation. As you can see Part 5 has a lot more question_id s and is also the most difficult. The other files don t take very long to load and I am importing the CSVs directly. task_container_id int16 Id code for the batch of questions or lectures. prior_question_elapsed_time. If anything the percent_correct actually seems to go down slightly. As we can see there is a new user in example_test indeed. If you want still check what the numbers look like with those features you can do that by simply hashing in and out the features line. The test data follows chronologically after the train data. As the train dataset is huge I am gladly using the pickle that Rohan Rao prepared in this kernel https www. Kaggle says that there are new users in the test set but let s check this anyway with example_test. Although it is probably better to treat not having an explanation as a disadvantage as there was an explanation before the vast majority of questions. For example a user might see three questions in a row before seeing the explanations for any of them. As you can see below table sorted on descending Percent_lecture the percent of lectures of the task_container_id s is never high. You ll be provided with the same sorts of information a complete education app would have that student s historic performance the performance of other students on the same question metadata about the question itself and more. 3 Exploring LecturesMetadata for the lectures watched by users as they progress in their education. Therefore we do not have the option to compare timestamps across users and the average based content features are as good as it gets for this dataset. correct_answer the answer to the question. mean see https www. As you can see there is s slightly downward trend. Content_type_id False means that a question was asked. However timestamp 0 for user x is not the same as timestamp 0 for user y. prior_question_had_explanation bool Whether or not the user saw an explanation and the correct response s after answering the previous question bundle ignoring any lectures in between. Please be sure to review the Time series API Details section closely. Do we have the full history of all user_id s Yes if we filter train on timestamp 0 we get a time 0 for all users. Example testThis file is a very small file and only good to check what s in there. part top level category code for the lecture. For content features the story is different. I seemed as if a copy of train was kept in RAM at least temporarily and I ran into an out of memory error. As you can see most interactions are from users that were not active very long on the platform yet. I also want to find out if there is a relationship between timestamp and answered_correctly. type_of brief description of the core purpose of the lectureLet s have a look at the type_of. 001 and feature importance is very low for both features. This is a time series code competition you will receive test set data and make predictions with Kaggle s time series API. The value is shared across a single question bundle and is null for a user s first question bundle or lecture. task_container_id int16 Id code for the batch of questions or lectures. Content_id is a code for the user interaction. Basically these are the questions if content_type is question question_id foreign key for the train test content_id column when the content type is question. We can also see the highest percentages of lectures are around 2. This year the company released EdNet the world s largest open database for AI education containing more than 100 million student interactions. As you can see below I am using only 5 features for this baseline. First let s check if there are any question_id s without tags. when looking at train we see that this question was just asked once. bundle_id code for which questions are served together. 45 minutes 100 questions. com c riiid test answer prediction discussion 195032 add thousands separator this clears everything loaded in RAM including the libraries adding user features adding content features using one of the validation sets composed by tito save mean before splitting please be aware that there is an issues with train. The last thing that I want to check is if having a lecture in a batch helps. Let s also check out what the distribution of answered_correctly looks like if we groupby the 10 000 unique task_container_id s. Below I am doing the same thing by content_id is question_id for content_type is question. Tailoring education to a student s ability level is one of the many valuable things an AI tutor can do. Below I have taken a sample of 200 rows. Note that the time is the average time a user took to solve each question in the previous bundle. Below I am displaying the count and percent correct by part. we can see that prior_question_had_explanation is object and taking a lot of memory while it is supposed to be boolean. answered_correctly int8 if the user responded correctly. prior_question_elapsed_time float32 The average time in milliseconds it took a user to answer each question in the previous question bundle ignoring any lectures in between. The test iterations give interactions of users chronologically. 3 Exploring Lectures 2. 3 Exploring Lectures 1. 1 Exploring TrainThe columns in the train file are described as row_id int64 ID code for the row. content_id int16 ID code for the user interaction content_type_id int8 0 if the event was a question being posed to the user 1 if the event was the user watching a lecture. You can find the correct way to compose user features in all high scoring public notebooks which use loops to calculate values for each individual question. Let s start by checking how much memory this dataframe is using. Seems that the questions are multiple choice answers 0 3. Dislaimer This a quick and dirty way. csv really is just an example. Can be compared with the train user_answer column to check if the user was right. The listening section consists of Part 1 4 Listening Section approx. text v i round v 2 color white fontweight bold fontsize 14 ha right va center please be aware that there is an issues with train. timestamp int64 the time in milliseconds between this user interaction and the first event completion from that user. For both wrong and correct answers the mean is about 25 seconds. True means that the user was watching a lecture. tags one or more detailed tag codes for the question. As we can see we have over 101 million rows the the train set. Future information should not be used. At first glance this does not seem very interesting regarding our target. Is there a correlation between the percent_lecture and the percent_correct No I don t really see it. This a low number compared to the tags with most answers. Therefore I unfortunately went back to an ugly version of code repetition for those steps same code for validation and test_df. About the Riiid AIEd Challenge 2020Riiid Labs an AI solutions provider delivering creative disruption to the education market empowers global education players to rethink traditional ways of learning leveraging AI. With those two features CV barely goes up less than 0. As you can see there is exactly one question_id without at least one tag. 8 which means one lecture on about 36 questions. Batches without lectures have about 8 more correct answers than batches with lectures. As some users have answered huge amounts of questions I have taken out the outliers user_ids with 1000 questions answered. Let s fix this before continuing. Important In the Updates corrections and clarifications topic is said that the hidden test set contains new users but not new questions The train test data is complete in the sense that there are no missing interactions in the union of train and test data. With a strong belief in equal opportunity in education Riiid launched an AI tutor based on deep learning algorithms in 2017 that attracted more than one million South Korean students. The TOEIC L R uses an optically scanned answer sheet. As you can see it helps indeed Batches task_container_id may also contain lectures and I want to find out if there are any batches with high numbers of lectures. Not a big deal but we need to keep in mind that we have to impute something here if we make features based on tags. As you can see I have also changed the tags column into lists of tags. As you can see the trend is upward but there is also a lot of variation among users that have answered few questions. 001 the public score goes down 0. Tito rightfully argues that just taking the last couple of questions from each user as the validation set leads to much on light users in this kernel Thanks Tito https www. If we had real points in time we would have had the option to track changes in difficulty over time. Baseline model 2. There are 200 questions to answer in two hours in Listening approximately 45 minutes 100 questions and Reading 75 minutes 100 questions. I am again taking a sample of 200 and have taken out the content_ids with more than 25 000 questions asked. 000 rows for training. Table of Contents 1. tag one tag codes for the lecture. I actually do this at work all the time and in this case it reduces the time to load the dataset with the data types specified in the file description from close to 9 minutes to about 16 seconds. For instance the percent answered_correctly for a user s 10th question should be the average of his first 9 answers only. prior_question_elapsed_time float32 The average time in milliseconds it took a user to answer each question in the previous question bundle ignoring any lectures in between. So we should realize that example_test. As you can see the only noticable thing is that users who have registered relatively recently perform a little worse than users who are active longer. The submission happens via the API. Is null for a user s first question bundle or lecture. The tags seem valuable to me. com its7171 cv strategyIn the previous version I made a function for all the merges fillna s and label encoding below. As you can see there is a slight downward trend. Let s find out how many answers were Right and Wrong per question_id so per content_id in train. 2 Exploring QuestionsMetadata for the questions posed to users. As you can see it does not. Since there are not that many lectures I want to check if it helps if a user watches lectures at all. com c riiid test answer prediction discussion 195032 features user_questions user_mean content_questions content_mean watches_lecture prior_question_elapsed_time prior_question_had_explanation for now just taking 10. Those three would all share a task_container_id. user_id int32 ID code for the user. Read 1 as null for lectures. 2 Exploring Questions 1. Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. However after adding a few features in this version I ran into memory issues. Now I can add up all Wrong and Right answers for all questions that are labeled with a particular tag and calculate the percent correct for each tag. The meaning of the tags will not be provided but these codes are sufficient for clustering the lectures together. In addition it is also interesting to see that the percent answered correctly for the missing values is closer to True than to False. As you can see the differences are significant However we should also realize that the tag with the worst percent_correct only has about 250 000 answers. part the relevant section of the TOEIC test. EDAAltogether we are given 7 files. lecture_id foreign key for the train test content_id column when the content type is lecture 1. add thousands separator 1 year 31536000000 ms make function that can also be used for other fields for i v in zip pq. You will pair your machine learning skills using Riiid s EdNet data. Initially I also included watches_lecture and prior_question_had_explanation but these two features are very questionable. As mentioned in the data description 1 is actually no answer as the interaction was a lecture instead of a question. question_id foreign key for the train test content_id column when the content type is question 0. In this competition your challenge is to create algorithms for Knowledge Tracing the modeling of student knowledge over time. The target answered_correctlyAnswered_correctly is our target and we have to predict to probability for an answer to be correct. What are the so called Parts When following the link provided in the data description we find out that this relates to a test. Therefore I believe that the simpler model is preferred. Without looking at the lecture interactions 1 we see about 1 3 of the questions was answered incorrectly. ", "id": "erikbruin/riiid-comprehensive-eda-baseline", "size": "14531", "language": "python", "html_url": "https://www.kaggle.com/code/erikbruin/riiid-comprehensive-eda-baseline", "git_url": "https://www.kaggle.com/code/erikbruin/riiid-comprehensive-eda-baseline", "script": "correct sklearn.metrics LabelEncoder matplotlib.style seaborn numpy matplotlib.pyplot lightgbm roc_auc_score sklearn.preprocessing pandas FuncFormatter matplotlib.ticker ", "entities": "(('everything', 'train'), 'add') (('test data', 'train chronologically data'), 'follow') (('it', 'about 16 seconds'), 'do') (('time series code you', 'time series API'), 'be') (('they', 'education'), 'LecturesMetadata') (('Batches', 'lectures'), 'have') (('about 1 3', 'questions'), 'see') (('value', 'question first bundle'), 'share') (('mean', 'wrong answers'), 'be') (('1 actually interaction', 'data description'), 'mention') (('saw', 'lectures'), 'bool') (('story', 'content'), 'feature') (('dataframe', 'how much memory'), 'let') (('content_id content when type', 'train foreign test'), 'key') (('that', 'very long platform'), 'be') (('questions', 'which'), 'code') (('you', 'features simply line'), 'check') (('which', 'individual question'), 'find') (('we', 'time'), 'have') (('Rohan Rao', 'kernel https www'), 'be') (('user', 'lecture'), 'mean') (('I', '25 more than 000 questions'), 'take') (('user', 'them'), 'see') (('three', 'all task_container_id'), 'share') (('Submissions', 'predicted probability'), 'evaluate') (('I', 'really it'), 'be') (('it', 'memory'), 'see') (('Content_id', 'user interaction'), 'be') (('that', 'one more than million South Korean students'), 'launch') (('I', 'Baseline model'), 'see') (('question', 'when train'), 'see') (('don very long I', 'CSVs'), 'take') (('content average based as it', 'dataset'), 'have') (('1000 questions', 'user_ids'), 'answer') (('we', 'new example_test'), 'be') (('this', 'very target'), 'seem') (('I', 'also timestamp'), 'want') (('I', 'batch'), 'be') (('Below I', '200'), 'plot') (('average user', 'previous bundle'), 'note') (('here we', 'tags'), 'deal') (('accurately how students', 'future interactions'), 'be') (('TOEIC L R', 'answer optically scanned sheet'), 'use') (('codes', 'lectures'), 'provide') (('CV', 'two features'), 'go') (('how many answers', 'train'), 'let') (('percent', 'prior_question_had_explanation'), 'help') (('i', 'zip pq'), 'add') (('Therefore I', 'validation'), 'go') (('com c riiid test answer prediction discussion', 'now just 10'), 'feature') (('answers', '5 tags'), 'note') (('where they', 'timestamp'), 'order') (('Below I', 'content_type'), 'do') (('0 we', '0 users'), 'have') (('I', 'lectures'), 'see') (('1 event', 'lecture'), 'code') (('validation', 'kernel'), 'argue') (('You', 'EdNet data'), 'pair') (('two features', 'also watches_lecture'), 'include') (('complete app', 'question'), 'provide') (('you', 'task_container_id'), 'sort') (('First s', 'question_id tags'), 'let') (('codes', 'questions'), 'provide') (('that', 'few questions'), 'be') (('test iterations', 'users'), 'give') (('user', 'train user_answer column'), 'compare') (('year company', '100 student more than million interactions'), 'release') (('I', 'tags'), 'change') (('I', 'baseline'), 'use') (('answered_correctly we', '10 task_container_id 000 unique s.'), 'let') (('challenge', 'time'), 'be') (('content_id content when type', 'train foreign test'), 'be') (('percent', 'first 9 answers'), 'be') (('train test new data', 'train'), 'say') (('Below I', 'part'), 'display') (('it', 'questions'), 'be') (('we', '101 over million rows'), 'see') (('that', 'tag'), 'add') (('such user', 'mid question'), 'remain') (('Baseline modelBelow I', 'user features'), 'add') (('content_id content when type', 'question_id train foreign test'), 'key') (('at least temporarily I', 'memory error'), 'seem') (('education global players', 'AI'), 'empower') (('round v 2 color white fontweight', 'train'), 'v') (('which', 'about 36 questions'), '8') (('type_of brief description', 'type_of'), 'have') (('Part', 'question_id 5 lot more s'), 'have') (('I', 'memory issues'), 'run') (('listening section', 'Part'), 'consist') (('students', 'next questions'), 'be') (('it', 'lectures'), 'float32') (('AI tutor', 'many valuable things'), 'be') (('TrainThe columns', 'row'), 'explore') (('I', 'timestamp'), 'make') (('this', 'test'), 'find') (('previous I', 'encoding'), 'cv') (('also percent', 'False'), 'be') (('reading section', 'Part 5 7 Reading Section 75 100 questions'), 'consist') (('answer', 'probability'), 'be') (('also tag', '250 only about 000 answers'), 'realize') (('feature importance', 'very features'), 'be') (('user', 'lectures'), 'be') (('s', 'anyway example_test'), 'say') (('you', 'exactly one at least one tag'), 'be') (('also highest percentages', 'lectures'), 'see') (('who', 'relatively recently little worse users'), 'be') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["active", "answer", "area", "average", "baseline", "batch", "bool", "calculate", "case", "category", "center", "challenge", "check", "checking", "choice", "close", "clustering", "code", "color", "column", "company", "compare", "competition", "connection", "contain", "content", "copy", "core", "correct", "correlation", "count", "create", "csv", "curve", "cv", "data", "database", "dataframe", "dataset", "description", "distribution", "double", "education", "encoding", "equal", "event", "everything", "feature", "file", "filter", "find", "fix", "float32", "following", "function", "future", "groupby", "help", "high", "history", "importance", "impute", "including", "individual", "instance", "int64", "itself", "kept", "kernel", "key", "knowledge", "label", "labeled", "largest", "learning", "least", "lecture", "let", "level", "light", "link", "little", "load", "look", "looking", "lost", "lot", "majority", "market", "mean", "meaning", "memory", "metadata", "might", "mind", "missing", "model", "most", "multiple", "need", "new", "next", "no", "not", "null", "number", "object", "open", "option", "ordered", "out", "overall", "pair", "part", "per", "percent", "percentage", "perform", "performance", "pickle", "plotting", "predict", "prediction", "probability", "provider", "public", "purpose", "question", "reading", "relationship", "response", "review", "right", "row", "sample", "save", "score", "scoring", "section", "sense", "set", "several", "single", "something", "splitting", "start", "student", "submission", "table", "tag", "target", "task", "test", "text", "those", "time", "timestamp", "topic", "track", "train", "trend", "tutorial", "type", "under", "unique", "up", "user", "validation", "value", "variation", "version", "while", "who", "work", "world", "worst", "year", "zip"], "potential_description_queries_len": 178, "potential_script_queries": ["lightgbm", "numpy", "seaborn", "ticker"], "potential_script_queries_len": 4, "potential_entities_queries": ["average", "color", "least", "lot", "prediction", "test", "unique"], "potential_entities_queries_len": 7, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 181}