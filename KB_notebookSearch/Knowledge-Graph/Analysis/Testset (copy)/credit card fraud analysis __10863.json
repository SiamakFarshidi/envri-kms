{"name": "credit card fraud analysis ", "full_name": " h1 Introduction Anomaly Detection h2 What Are Anomalies h2 1 Anomaly Detection Techniques h4 Simple Statistical Methods h4 Challenges with Simple Statistical Methods h2 2 Machine Learning Based Approaches h4 a Density Based Anomaly Detection h4 b Clustering Based Anomaly Detection h4 c Support Vector Machine Based Anomaly Detection h4 Isolation Forest Anomaly Detection Algorithm h4 Density Based Anomaly Detection Local Outlier Factor Algorithm h4 Support Vector Machine Anomaly Detection Algorithm h2 Credit Card Fraud Detection h2 Problem Statement h4 DataSet h4 Observations h2 Preprocessing h3 Import Libraries h2 Exploratory Data Analysis h2 Model Prediction h4 Observations h2 Autoencoders h2 If you like this kernel greatly appreciate an UPVOTE ", "stargazers_count": 0, "forks_count": 0, "description": "Data instances that fall outside of these groups could potentially be marked as anomalies. Contextual anomalies The abnormality is context specific. Collective anomalies A set of data instances collectively helps in detecting anomalies. This concept is based on a distance metric called reachability distance. Our aim here is to detect 100 of the fraudulent transactions while minimizing the incorrect fraud classifications. SVM detecting 8516 errors Isolation Forest has a 99. So overall Isolation Forest Method performed much better in determining the fraud cases which is around 30. An autoencoder learns to compress data from the input layer into a short code and then uncompress that code into something that closely matches the original data. Get all the columns from the dataframe Model PredictionDefine the outlier detection methodsFit the model Observations Isolation Forest detected 73 errors versus Local Outlier Factor detecting 97 errors vs. Density Based Anomaly Detection Density based anomaly detection is based on the k nearest neighbors algorithm. This resulted in only 0. com mlg ulb creditcardfraud Observations The data set is highly skewed consisting of 492 frauds in a total of 284 807 observations. Here are a few The data contains noise which might be similar to abnormal behavior because the boundary between normal and abnormal behavior is often not precise. Machine Learning Based ApproachesBelow is a brief overview of popular machine learning based techniques for anomaly detection. You would need a rolling window to compute the average across the data points. The dataset consists of numerical values from the 28 Principal Component Analysis PCA transformed features namely V1 to V28. The first autoencoder might learn to encode easy features like corners the second to analyze the first layer s output and then encode less local features like the tip of a nose the third might encode a whole nose etc. If you like this kernel greatly appreciate an UPVOTE Create a trace Filter the columns to remove data we do not want Store the variable we are predicting Define a random state Print the shapes of X Y Fit the data and tag outliers Reshape the prediction values to 0 for Valid transactions 1 for Fraud transactions Run Classification Metrics. Challenges with Simple Statistical MethodsThe low pass filter allows you to identify anomalies in simple use cases but there are certain situations where this technique won t work. The nearest set of data points are evaluated using a score which could be Eucledian distance or a similar measure dependent on the type of the data categorical or numerical. Depending on the use case the output of an anomaly detector could be numeric scalar values for filtering on domain specific thresholds or textual labels such as binary multi labels. We can also improve on this accuracy by increasing the sample size or use deep learning algorithms however at the cost of computational expense. We can also use complex anomaly detection models to get better accuracy in determining more fraudulent casesNow let us look at one particular Deep Learning Algorithm called Autoencoders AutoencodersAn autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The pattern is based on seasonality. Some architectures use stacked sparse autoencoder layers for image recognition. Business use case Detecting credit card fraud based on amount spent. Therefore the threshold based on moving average may not always apply. 74 more accurate than LOF of 99. Assumption Normal data points occur around a dense neighborhood and abnormalities are far away. This involves more sophisticated methods such as decomposing the data into multiple trends in order to identify the change in seasonality. Introduction Anomaly Detection Anomaly detection is a technique used to identify unusual patterns that do not conform to expected behavior called outliers. This type of anomaly is common in time series data. Technically this is called a rolling average or a moving average and it s intended to smooth short term fluctuations and highlight long term ones. Assumption Data points that are similar tend to belong to similar groups or clusters as determined by their distance from local centroids. Furthermore there is no metadata about the original features provided so pre analysis or feature study could not be done. A SVM is typically associated with supervised learning but there are extensions OneClassCVM for instance that can be used to identify anomalies as an unsupervised problems in which training data are not labeled. Doesn t seem like the time of transaction really matters here as per above observation. They could be broadly classified into two algorithms K nearest neighbor k NN is a simple non parametric lazy learning technique used to classify data based on similarities in distance metrics such as Eucledian Manhattan Minkowski or Hamming distance. Let s say the definition of an anomalous data point is one that deviates by a certain standard deviation from the mean. In this jupyter notebook we are going to take the credit card fraud detection as the case study for understanding this concept in detail. Preprocessing Import Libraries Exploratory Data AnalysisLet us now check the missing values in the datasetDetermine the number of fraud and valid transactions in the entire dataset. How different are the amount of money used in different transaction classes Let s have a more graphical representation of the dataDo fraudulent transactions occur more often during certain time frame Let us find out with a visual representation. Anomaly Detection Techniques Simple Statistical MethodsThe simplest approach to identifying irregularities in data is to flag the data points that deviate from common statistical properties of a distribution including mean median mode and quantiles. Support Vector Machine Based Anomaly Detection A support vector machine is another effective technique for detecting anomalies. The aim of an autoencoder is to learn a representation encoding for a set of data typically for the purpose of dimensionality reduction. Mathematically an n period simple moving average can also be defined as a low pass filter. Therefore autoencoders are unsupervised learning models. This skewed set is justified by the low number of fraudulent transactions. Business use case Spending 100 on food every day during the holiday season is normal but may be odd otherwise. Now let us print the outlier fraction and no of Fraud and Valid Transaction cases Correlation MatrixThe above correlation matrix shows that none of the V1 to V28 PCA components have any correlation to each other however if we observe Class has some form positive and negative correlations with the V components but has no correlation with Time and Amount. until the final autoencoder encodes the whole image into a code that matches for example the concept of cat. The Time and Amount features are not transformed data. Relative density of data This is better known as local outlier factor LOF. What Are Anomalies Anomalies can be broadly categorized as Point anomalies A single instance of data is anomalous if it s too far off from the rest. It has many applications in business from intrusion detection identifying strange patterns in network traffic that could signal a hack to system health monitoring spotting a malignant tumor in an MRI scan and from fraud detection in credit card transactions to fault detection in operating environments. Anomaly detection is similar to but not entirely the same as noise removal and novelty detection. Business use case Someone is trying to copy data form a remote machine to a local host unexpectedly an anomaly that would be flagged as a potential cyber attack. K means is a widely used clustering algorithm. It creates k similar clusters of data points. The simplest form of an autoencoder is a feedforward non recurrent neural network very similar to the many single layer perceptrons which makes a multilayer perceptron MLP having an input layer an output layer and one or more hidden layers connecting them but with the output layer having the same number of nodes as the input layer and with the purpose of reconstructing its own inputs instead of predicting the target value Y given inputs X. This model is then used to identify whether a new transaction is fraudulent or not. Noise removal NR is the process of removing noise from an otherwise meaningful signal. There is no missing value in the dataset. 09 When comparing error precision recall for 3 models the Isolation Forest performed much better than the LOF as we can see that the detection of fraud cases is around 27 versus LOF detection rate of just 2 and SVM of 0. This forces the autoencoder to engage in dimensionality reduction for example by learning how to ignore noise. The algorithm learns a soft boundary in order to cluster the normal data instances using the training set and then using the testing instance it tunes itself to identify the abnormalities that fall outside the learned region. Clustering Based Anomaly Detection Clustering is one of the most popular concepts in the domain of unsupervised learning. Novelty detection is concerned with identifying an unobserved pattern in new observations not included in training data like a sudden interest in a new channel on YouTube during Christmas for instance. In this jupyter notebook we are going to take the credit card fraud detection as the case study for understanding this concept in detail using the following Anomaly Detection Techniques namely Isolation Forest Anomaly Detection Algorithm Density Based Anomaly Detection Local Outlier Factor Algorithm Support Vector Machine Anomaly Detection Algorithm Credit Card Fraud Detection Problem Statement The Credit Card Fraud Detection Problem includes modeling past credit card transactions with the knowledge of the ones that turned out to be fraud. DataSet The dataset that is used for credit card fraud detection is derived from the following Kaggle URL https www. An alternative use is as a generative model for example if a system is manually fed the codes it has learned for cat and flying it may attempt to generate an image of a flying cat even if it has never seen a flying cat before. Traversing mean over time series data isn t exactly trivial as it s not static. The definition of abnormal or normal may frequently change as malicious adversaries constantly adapt themselves. Now let us take a sample of the dataset for out modelling and predictionPlot histogram of each parameterDetermine the number of fraud and valid transactions in the dataset. ", "id": "rajshree07/credit-card-fraud-analysis", "size": "10863", "language": "python", "html_url": "https://www.kaggle.com/code/rajshree07/credit-card-fraud-analysis", "git_url": "https://www.kaggle.com/code/rajshree07/credit-card-fraud-analysis", "script": "sklearn.metrics plotly.offline LocalOutlierFactor rcParams IsolationForest iplot seaborn numpy plotly.graph_objs pylab sklearn.ensemble matplotlib.pyplot pandas classification_report accuracy_score OneClassSVM init_notebook_mode sklearn.neighbors sklearn.svm plotly.plotly plotly.figure_factory ", "entities": "(('Collective set', 'collectively anomalies'), 'anomaly') (('output', 'textual such binary multi labels'), 'be') (('so analysis', 'Furthermore original features'), 'be') (('Y', 'target instead value'), 'be') (('that', 'Kaggle URL https following www'), 'DataSet') (('skewed set', 'fraudulent transactions'), 'justify') (('This', 'how noise'), 'force') (('that', 'potentially anomalies'), 'mark') (('that', 'cyber potential attack'), 'case') (('it', 'too far off rest'), 'categorize') (('errors Isolation 8516 Forest', '99'), 'SVM') (('Clustering', 'unsupervised learning'), 'be') (('Business use case', 'holiday day season'), 'be') (('exactly it', 'time'), 'traverse') (('frequently malicious adversaries', 'constantly themselves'), 'change') (('architectures use', 'image recognition'), 'stack') (('type', 'series data'), 'be') (('k nearest NN', 'Manhattan such Eucledian Minkowski'), 'classify') (('Novelty detection', 'instance'), 'concern') (('Observations Isolation Forest', '97 errors'), 'get') (('Now us', 'valid dataset'), 'let') (('us', 'valid entire dataset'), 'check') (('This', 'factor better local outlier LOF'), 'density') (('us', 'visual representation'), 'be') (('Machine Learning Based ApproachesBelow', 'anomaly detection'), 'be') (('X Y data outliers', 'Fraud 1 transactions'), 'appreciate') (('training data', 'which'), 'associate') (('that', 'closely original data'), 'learn') (('that', 'operating environments'), 'have') (('This', 'seasonality'), 'involve') (('rolling moving it', 'term short fluctuations'), 'call') (('we', 'detail'), 'go') (('aim', 'dimensionality reduction'), 'be') (('that', 'cat'), 'encode') (('which', 'data'), 'evaluate') (('Autoencoders AutoencodersAn autoencoder', 'unsupervised manner'), 'use') (('that', 'learned region'), 'learn') (('aim', 'fraud incorrect classifications'), 'be') (('that', 'mean'), 'let') (('Based Anomaly Detection support vector machine', 'effective anomalies'), 'be') (('Assumption data Normal points', 'dense neighborhood'), 'occur') (('Noise removal NR', 'otherwise meaningful signal'), 'be') (('time', 'really here per observation'), 'seem') (('that', 'ones'), 'go') (('detection', '0'), '09') (('moving Mathematically n period simple average', 'pass also low filter'), 'define') (('detection', 'neighbors k nearest algorithm'), 'base') (('We', 'computational expense'), 'improve') (('even it', 'flying cat'), 'attempt') (('that', 'expected behavior'), 'introduction') (('com mlg ulb creditcardfraud data set', '284 807 observations'), 'observation') (('You', 'data points'), 'need') (('certain where technique', 't work'), 'allow') (('that', 'local centroids'), 'tend') (('features', 'namely V28'), 'consist') (('K', 'widely used algorithm'), 'mean') (('Therefore autoencoders', 'learning models'), 'unsupervise') (('It', 'data points'), 'create') (('that', 'median mean mode'), 'be') (('which', 'fraud much better cases'), 'perform') (('third', 'whole nose'), 'learn') (('Anomaly detection', 'entirely noise removal detection'), 'be') (('concept', 'distance metric'), 'base') (('boundary', 'normal behavior'), 'be') (('however we', 'Time'), 'let') ", "extra": "['biopsy of the greater curvature', 'test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "algorithm", "analyze", "anomaly", "approach", "associated", "autoencoder", "average", "behavior", "binary", "boundary", "case", "cat", "categorical", "channel", "check", "classify", "cluster", "clustering", "code", "compute", "concept", "context", "copy", "correlation", "correlations", "cost", "could", "credit", "data", "dataframe", "dataset", "day", "dependent", "detail", "detect", "detected", "detection", "detector", "dimensionality", "distance", "distribution", "domain", "encode", "encoding", "error", "even", "every", "expected", "factor", "feature", "filter", "final", "find", "following", "form", "frame", "generate", "highlight", "histogram", "ignore", "image", "improve", "including", "input", "instance", "interest", "itself", "kernel", "knowledge", "layer", "lazy", "learn", "learning", "let", "local", "look", "malignant", "matrix", "mean", "measure", "median", "metadata", "metric", "might", "missing", "mode", "model", "modelling", "most", "multiple", "nearest", "need", "negative", "neighborhood", "network", "neural", "new", "no", "noise", "non", "none", "normal", "not", "notebook", "number", "numeric", "numerical", "order", "out", "outlier", "output", "overall", "overview", "past", "pattern", "per", "period", "point", "positive", "potential", "pre", "precision", "prediction", "print", "purpose", "random", "recall", "recurrent", "remove", "representation", "sample", "scan", "score", "second", "set", "short", "signal", "similar", "single", "size", "smooth", "soft", "something", "sparse", "standard", "state", "supervised", "support", "system", "tag", "target", "technique", "term", "testing", "threshold", "time", "total", "training", "transaction", "tumor", "type", "understanding", "until", "valid", "value", "variable", "vector", "while", "window"], "potential_description_queries_len": 170, "potential_script_queries": ["iplot", "numpy", "plotly", "pylab", "seaborn", "sklearn"], "potential_script_queries_len": 6, "potential_entities_queries": ["data", "following", "mean", "noise", "per", "potential", "support"], "potential_entities_queries_len": 7, "potential_extra_queries": ["biopsy of the greater curvature", "biopsy", "test"], "potential_extra_queries_len": 3, "all_components_potential_queries_len": 179}