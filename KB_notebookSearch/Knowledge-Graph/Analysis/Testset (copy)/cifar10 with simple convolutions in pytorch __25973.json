{"name": "cifar10 with simple convolutions in pytorch ", "full_name": " h3 Working through chapter 8 of Deep Learning with Pytorch book h3 About the size of the kernel h3 Lets get the cifar triaining and validation set now h3 why should we be using Convolutional neural networks anyway h3 output shape input shape h3 Padding the boundary h3 Detecting features with convolution h3 what weights and bias h3 Looking further with depth and pooling h3 Downsampling h3 Maxpooling h3 Combining convolutions and downsampling h3 Putting it together for our network h3 looking into the convolutions h3 The error of our ways h3 Subclassing nn Module h3 Our network as an nn Module h3 Discussion on the model h3 How pytorch keeps track of parameters and submodules h3 can we have arbitrary functions h3 how parameters are found h3 The functional API h3 To use or not to use functional API h3 Training our Convnet h2 Measuring accuracy h3 how to save your model h3 Note about training on the GPU h3 Amending the training loop to cater for GPU h3 Model design h3 Helping model to converge and regularize h3 using Dropout without relying on a single input h3 Batch Normalisation for keeping activations in check h2 Going deeper and learning more complex structures Depth h3 Skip connections h3 slowed down convergence h3 Initialisation h2 Conclusion h2 extra bits Exercises ", "stargazers_count": 0, "forks_count": 0, "description": "We will be recognising between airoplane and a bird. The derivatives of the loss function with respect tothe parameters especially those in early layers need to be multiplied by a lot of othernumbers originating from the chain of derivative operations between the loss and theparameter. We got two numbers Information flows correctly. bias is false because Batch norm will cancel the effect of bias Uses custom initializations. The net result is that the output hasnow the exact same size as the input Doing padding helps us separate the matters of convolution and changing image sizes so we have one less thing toremember. By accessing their grad attribute which has been populated by autograd theoptimizer will know how to change parameters to minimize the loss The functional APIPyTorch has functional counterparts for every nn module. Sequential we re inthe modular realm. For instance for our first convolutional module we ll have 3 input features per pixel the RGB channels and an arbitrary number ofchannels in the output say 16. By functional here we mean having no internal state in other words whose output value is solely and fully determined by the value input arguments. We ve heard stories of famous practitioners putting inarbitrary numbers and then relying on error messages from PyTorch to backtrack thecorrect sizes for their linear layers. Again we apply a Tanh activation and then pool to an8 channel 8 8 output. MaxPool2d but unfortunately wedon t have any explicit visibility of the output of each module when we use nn. Module at a minimum we need to define a forward function that takes the inputs to the module and returns the output. linear which is a function that has signature linear input weight bias None. This is exactly howit is done in PyTorch. then in forwardwe traverse it 100 times slowed down convergenceIn the implementation we parameterize the actual number of layers which is important for experimentation and reuse. Since the aim for batch normalization is to rescale the inputs of the activations the natural location is after the linear transformation convolution in this case and the activation as shown here similar to dropout batch normalisation also behaves differnetly Going deeper and learning more complex structures DepthThere s another way to think about depth increasing depth is related to increasingthe length of the sequence of operations that the network will be able to performwhen processing inputDepth comes with some additional challenges which prevented deep learning modelsfrom reaching 20 or more layers until late 2015. com max 1200 1 1okwhewf5KCtIPaFib4XaA. The resulting 16 channel 32 32 image is pooled to a 16 channel 16 16 image by the first MaxPool3d. org glossary terms 5361a0e82d3941e58903b90f64e71491 download. At a minimum the arguments we provide to nn. At this point the downsampled image undergoes another convolution that generates an 8 channel 16 16 output. We found that our model did not converge and lookedat what people commonly choose as initialization a smaller variance in weights andzero mean and unit variance outputs for batch norm and then we halved the outputvariance in the batch norm when the network would not converge. It will also ensure that allthe parameters in the block are visibele to Net. To include these submodules we typically define them in the constructor__init__ and assign them to self for use in the forward function. This could be achieved by calling view on the output of the last nn. This term is crafted so that the weights of the model tend to be small on their own limiting how much training makes themgrow. we need to convert that into 0 and 1now lets get the datasetThe data is ready to go. eval on any of the nn. In order to increase the capacity of the model we could increase the number of output channels for theconvolution layers that is the number of features each convolution layer generates which would lead the linear layer to increase its size as well. png ehk 25wY39EtQMRbUWi0278TX3wGQjI11w6Uxr732 2fd0C 2bA 3d risl pid ImgRaw Here we will be using a simple Convolutional Neural Network to work with CIFAR 10 data. Module can access the list of all child parameters. But instead of working on the input arguments and stored parameters like the module counterparts they take inputs and parameters as arguments to the function call. kaiming_normal_ initializes with normal random elements with standard deviation as computed in the ResNet paper. What about the big picture How do we know that all structures in our images are 3 pixels or 5 pixels wide Well we don t because they aren t. We also explored ways in which we can make our models wider anddeeper while controlling effects like overfitting. Module to an attribute in an nn. Lets work on our model. Let s add onelayer to our simple convolutionalmodel and let s use ReLU as the activation for a change. This is also referred to as identity mapping. Conclusion In this chapter we have built reasonable working models in PyTorch that canlearn from images. Unsurprisingly the network is quite a bit slower to converge. This explains why we re missing two pixels in each dimension. we can increase te number of channels starting from first convolutionWe can easily pass a parameter to init and parameterize the width taking care to also paramterize the call to view in the forward functionLets look at the number of parameters of the modelgreater the capacity more variability but also might lwad to overfitting. com questions 59013109 runtimeerror input type torch floattensor and weight type torch cuda floatte Model designWe can add the width of the network or the number of neurons per layer. This is where we define ourmodule s computation. It is matching closely with val accuracy. gif It is very common to have kernel sizes that are the same in all directions soPyTorch has a shortcut for this whenever kernel_size 3 is specified for a 2D convolution it means 3 3 provided as a tuple 3 3 in Python About the size of the kernelWhat do we expect to be the shape of the weight tensor The kernel is of size 3 3 so we want the weight to consist of 3 3 parts. Finally we have as many of those as we have output channels here out_ch 16 so the complete weight tensor is out_ch in_ch 3 3 in our case16 3 3 3. Batch normalization in PyTorch is provided through the nn. To keep things simple we would be using a subset of two classes in cifar dataset. And if they aren t how are our networks going to be equipped tosee those patterns with larger scope This is something we ll really need if we want to solve our birds versus airplanes problem effectively since although CIFAR 10 imagesare small the objects still have a wing span several pixels across. Well sure at the limit wecould get a 32 32 kernel for a 32 32 image but we would converge to the old fullyconnected affine transformation and lose all the nice properties of convolution. Where does this end After the input image has been reduced to a set of 8 8 features we expect to be able to output some probabilities from the network that we canfeed to our negative log likelihood. using Dropout without relying on a single inputZero out a random fraction of outputs from neurons accross network where randomisation happens at each iteration. We will work on a limited dataset recognising between an aeroplane and a bird. Lame eh Nah it s all legit Training our ConvnetThe core of our convnet is two nested loops an outer one over theepochs and an inner one of the DataLoader that produces batches from our Dataset. Helping model to converge and regularize1. Skip connectionsA skip connection is nothing butthe addition of the input to the outputof a block of layers. The model has zero chance of running without complaining The error of our waysAdmittedly the error message is a bit obscure but not too much so. The vanilla module with an extra layer looks like thisIn other words we re using the output of the first activations as inputs to the last inaddition to the standard feed forward path. The bias will have size 16 we haven t talked about bias for a while forsimplicity but just as in the linear module case it s a constant value we add to eachchannel of the output image. We are making some ad hoc choices about what information to present where. This makes their contribution to the gradient of the loss moredirect as partial derivatives of the loss with respect to those parameters have a chancenot to be multiplied by a long chain of other operations. Be aware that calling such methods will be similar tocalling forward instead of the module itself they will be ignorant of hooks and theJIT does not see the module structure when using them because we are missing theequivalent of the __call__ bits shown in section how parameters are found What happens here is that the parameters call delves into all submodules assignedas attributes in the constructor and recursively calls parameters on them. if you see the original dataset has labels for aeroplane and bird as 0 and 2. This is why we used more detailed initializations and trainedour NetRes with a learning rate of 3e 3 instead of the 1e 2 we used for the othernetworks. to is out of place in some ways computation just like Tensor. Also we collect and print some information Measuring accuracyIn order to have an accuracy more interpretable thatn the loss we can take alookat our accuracies on the training and validation datasets how to save your modelIts pretty easy to do we will store it in birds_vs_airplanes. Here it is something very simple like reshaping. Our network as an nn. Feed the inputs through the model the forward pass. In each loop we then have to1. Typically our computation will use other modules premade like convolutions orcustomized. Theweight and bias parameters are arguments to the function. a What impact does this change have on the number of parameters in themodel Parameters increase with changing the kernel_size of convolutionsb Does the change improve or degrade overfitting Maybe. Module Discussion on the modelThe Net class is equivalent to the nn. Sequentil will ensure that the output of one block is used as the input to the next. Have the optimizer take a step in toward lower loss. We put the Warning note in the code for a reason. Also because we are randomly initializing them some of the features we llget even after training will turn out to be useless. The first convolution takes us from 3 RGB channels to 16 thereby giving the network a chance to generate 16 independent features that operate to hopefully discriminate low level features of birds and airplanes. This convolution is supposed to enhance the vertical edges of images. They will at thesame time hold their parameters throughout the lifetime of our module. Note that youneed to call super. With any luck this output will consist of higher level features. Oneimplication is that it is good practice to create the Optimizer after moving the parameters to the appropriate device. In otherwords it is a penalty on larger weight values. Wenow have a solid basis for facing the challenges we ll encounter when working ondeep learning projects. Finding an image and manually feeding it into the model. No matter how nested the submodule any nn. converts 32 by 32 to 16 by 16 something missing to be filled later we need a reshaping block here plt. one_ that would result in each pixel in theoutput being the sum of the pixels in the neighborhood. Zero any old gradients. However we can play with convolution by setting weights by hand and see what happens. Conv2dexpects a B C H W shaped tensor as input B is batch sizeC is colorH is HeightW is Widthnote the output shape here is different to input shape. Let s first zero out bias just to remove any confounding factors and then set weights to a constant value so that each pixel in the output gets the mean of its neighbors. For instance the functional counterpart of nn. png Detecting features with convolutionWe said earlier that weight and bias are parameters that are learned through backpropagation exactly as it happens for weight and bias in nn. In order to subclass nn. Not a big difference except that the values in the output image would have been nine times larger. Thishas been the last bit missing to understand how the code organization works in justabout any neural network implemented in PyTorch. This is controlled through the train property of the dropout this is achieved through model. So it punishes having more weights in the model. Then we recommended the use of small kernels like 3 3 or 5 5 that s peak locality all right. Linear 512 32 the first linear module afterthe last convolution block. Let s take our previous fullyconnected model as a starting point and introduce nn. When part of a network is so simple that we want to use nn. Note about training on the GPUThere is a somewhat subtle difference between Module. Padding the boundaryHowever PyTorch gives us the possibility of padding the image by creating ghost pixels around the border that have value zero as far as the convolution is concerned. However probabilities are a pair of numbers in a1D vector one for airplane one for bird but here we re still dealing with multichannel 2D features. is_available Amending the training loop to cater for GPUhttps stackoverflow. Let s verify our assumptions Lets get the cifar triaining and validation set nowRemember our final aim for getting the dataset is to recognise between a plane and a bird. Applying a convolution kernel as a weighted sumof pixels in a 3 3 neighborhood requires that there are neighbors in all directions. There are again 2 L1 regularisation which is the sum of absolute values of all weights in the model L2 regularisation which is the sum of squared of all the weights in the model. By taking the highest value we ensure that features are found to survive the downsampling Combining convolutions and downsamplingLets see how to use it on larger structures. weight are initialized randomly so the output image will not be particularly meaningful. Looking further with depth and poolingThis is all well and good but conceptually there s an elephant in the room. How pytorch keeps track of parameters and submodulesInterestingly assigning an instance of nn. For example for a modelwhere training is substantially different than its use say for prediction it may makesense to have a predict method. We start by applying a set of 3 3 kernels on our 8 8image obtaining a multichannel output image of the same size. Note that we leave the batch dimension as 1 in the call to view since in principle wedon t know how many samples will be in the batch. We got all excited because by moving from fully connected layers to convolutions we achievelocality and translation invariance. We did it in a way that helped us build our intuition around convolutional networks. backward to compute the gradients of the loss with respect to allparameters the backward pass. The submodules must be top level attributes not buried inside list ordict instances Otherwise the optimizer will not be able to locate the submodules and hence their parameters. InitialisationInitialisation is a running problem with training neural network in pytorch. Working through chapter 8 of Deep Learning with Pytorch book. When we want to build models that do more complex things than just applyingone layer after another we need to leave nn. extra bits ExercisesChange our model to use a 5 5 kernel with kernel_size 5 passed to thenn. Back to our model it makes sense to keep using nn modules for nn. Taking the sum gives the number of items in the batch where the prediction and ground truth agree. Take the maximim of th efout pixels3. Thinking back to the beginning of this chapter we already know what we need todo turn the 8 channel 8 8 image into a 1D vector and complete our network with aset of fully connected layers That s very reasonable for a limited dataset of such small images. ModuleAt some point in developing neural networks we will find ourselves in a situation wherewe want to compute something that the premade modules do not cover. Conv2d so that Net will be able to manage their Parameters during training. Perform a strided convolution where every nth pixel is calculated MaxpoolingIntuitively the output images form a convolution layer especially since they are followed by an activation just like any other linear layer they tend to have a high magnitude where certain features corresponding to the estimated kernel are detected such as vertical lines. ModuleDict can we have arbitrary functions Yes We can call arbitrary methods of an nn. Then we apply the Tanh activation function. Note that dropout is only active during training and it is taken as zero in all subsequent run. For situations where your modelrequires a list or dict of submodules PyTorch provides nn. Conv2d are the number of input features or channels since we re dealingwith multichannel images that is more than one value per pixel the number of output features and the size of the kernel. Batch Normalisation for keeping activations in checkMain idea is to rescale the inputs to the activations of the network so that minibatches have a certain desirable distibution. In our case both the kernel weights and the bias conv. Another option which is used in convolutional neural networks is stacking one convolution after the other and at the same time downsampling the image between successive convolutions. We will be using convolutional neural network here. com th id R2dcc41f9079d1abf5883a113c0d1ca31 rik i3uC2F68Rj8a6w riu http 3a 2f 2fpytorch. sequential containing a list of RsBlock instances. show the loss we wish to minimise getting rid of gradients from last round performs the backward step we computer the gradients of all parameters we want the network to learn updates the model change it to 100 for better values since we dont want to update the parameters index of highest value as output counts the number of examples so total is increased by the batch size Comparing the predicted class that had the maximum probability and the ground truth labels we first get a Boolean array. Average the four pixels2. Although we still only scratched thesurface we have taken another significant step ahead from the previous chapter. In our case specifying padding 1 when kernel_size 3 means i00 has an extra setof neighbors above it and to its left so that an output of the convolution can be computed even in the corner of our original image. A 2D convolution pass produces a 2D image as output whose pixels are a weighted sum over neighborhoods of the input image. Module if we need to and we also have the functional API for cases when instantiating and then calling an nn. So how does this alleviate the issues with vanishing gradients we were mentioningearlier Thinking about backpropagation we can appreciate that a skip connection or asequence of skip connections in a deep network creates a direct path from the deeperparameters to the loss. BatchNorm3d modules depending on the dimensionality ofthe input. d Can you describe what kernel_size 1 3 will do model does not work with this kernel. It is considered good style to move things to the GPU if one is available. For creating we ll resort to nn. __init__ before you can do that or PyTorch will remind you. As usual we need to add the zeroth batch dimension withunsqueeze if we want to call the conv module with one input image since nn. What s missing there is the reshaping step from an 8 channel 8 8 image to a 512 element 1D vector 1D if we ignore the batch dimension that is. However we can safely switch to the functional counterparts of pooling and activation since they have no parameters To use or not to use functional APIWhether to use the functional or the modular API is a decision based on style andtaste. We trained none of the networks to convergence but we would not havegotten anywhere without these tweaks. why should we be using Convolutional neural networks anywayWe can see how convolutions are a convenient choice for learning from images. A goodpattern is to set the a variable device depending on torch. If we are at i00 we only have pixels to the right of and below us. Module never comes with a backward. With PyTorch if we use standard torch operations autograd will take care of the backward pass automatically and indeed an nn. For odd sized kernels this results in images that are one half the convolution kernel s width in our case 3 2 1 smaller on each side. org 2ftutorials 2f_images 2fcifar10. If you follow through this notebook it will present you with basic of CNN as in the nitty gritty detail of shapes of layers why maxpooling creating your own model from scratch etc. One possibility could be to use large convolution kernels. The bottom line is that a long chain of multiplications will tend to make the contribution of the parameter to the gradient vanish leading to ineffective training of that layersince that parameter and others like it won t be properly updated. sequential containing a list of nn. Those numbers being multiplied could be small generating ever smallernumbers or large swallowing smaller numbers due to floating point approximation. So now we can make our own nn. We might not realize it right now but in more complex models getting the size of the first linear layer right is sometimes a source of frustration. By default PyTorch willslide the convolution kernel within the input picture getting width kernel_width 1horizontal and vertical positions. Can you guess how output shape input shapeThe fact that our output image is smaller than the input is a side effect of deciding whatto do at the boundary of the image. Putting it together for our networkWith these building blocks in our hands we can now proceed to build our convolutional neural network for detecting birds and airplanes. So on one hand the first set of kernels operates on small neighborhoods on firstorder low level features while the second set of kernels effectively operates on widerneighborhoods producing features that are compositions of the previous features. For each 3 3 neighborhood what weights and biasWe could have gone with conv. This is a very powerful mechanism that provides convolutional neural networks withthe ability to see into very complex scenes much more complex than our 32 32images from the CIFAR 10 dataset. PyTorch allows us to use any computation in our model by subclassing nn. Adding depth to a model generallymakes training harder to converge. Anyway let s see the effect on our CIFAR image So we need to call detach on the outputIt looks like a blob of a skeleton rising from a swamp. The batch norm is initialized to produce output distributions that initially have 0 mean and 0. We have smaller models looking for local patterns whose weights are optimized across the entire image. Also needless to say backpropagation will work asexpected. Recall that the goal of classification networks typically is to compress information in the sense that westart with an image with a sizable number of pixels and compress it into a vector of probabilities of classes. Compute the loss also part of the forward pass. to is in place the module instance is modified. Then we scale down the output image by half obtaining a 4 4 image and apply another set of 3 3 kernels to it. 5 varianc aeroplane bird. Sequential for something that gives usadded flexibility. Module aswe did in the earlier constructor automatically registers the module as a submodule. We find references to linear in the traceback looking back at the model we see that only modulethat has to have a 512 32 tensor is nn. functional provides many functions that work like the modules we find in nn. keeping the parameters in checkAdding a regularisation termto the loss. But even for that first we download the entire dataset Now we need a subset of the database. regularisation is added by using a term in the lossit is worth noting that the SGD optimizer in PyTorch already has a weight_decay parameter thatcorresponds to 2 lambda and it directly performs weight decay during the updateas described previously. When we are writing our own forwards it may be more natural touse the functional interface for things that do not need state in the form of parameters. tanh returning a new tensor. MaxPool2d asdescribed previously looking into the convolutionsYou dont need to read the explanation in this para just scroll down where I define my own model and look at the model output to inspect the intermediate sizes. This second set of kernels operates on a 3 3 neighborhood of something that has been scaled down by half so it effectively maps back to 8 8 neighborhoodsof the input. DownsamplingScaling the image by half is equivalent of taking four neighboring pixels and producing one pixel. In practice it shifts and scales an intermediate input using the mean and standard deviation collected at that intermediate location over the samples of minibatch. Let s stick to a kernel size of 3 3. Sequential modelwe built earlier in terms of submodules but by writingthe forward function explicitly we can manipulate theoutput of self. Also called weight decay. The thing isonyl weight is storedWhile loading we need to give the structure of the model. pool3 directly and call view on it to turnit into a B N vector. In addition the second set of kernels takes the output of the first set ofkernels features like averages edges and so on and extracts additional features ontop of those. For a single output pixel value our kernelwould consider say in_ch 3 input channels so the weight component for a singleoutput pixel value and by translation the invariance for the entire output channel isof shape in_ch 3 3. https cdn images 1. ", "id": "fanbyprinciple/cifar10-with-simple-convolutions-in-pytorch", "size": "25973", "language": "python", "html_url": "https://www.kaggle.com/code/fanbyprinciple/cifar10-with-simple-convolutions-in-pytorch", "git_url": "https://www.kaggle.com/code/fanbyprinciple/cifar10-with-simple-convolutions-in-pytorch", "script": "torch.optim __init__ Net(nn.Module) NetDropoutKernelMod(nn.Module) training forward torch.nn NetDropoutMod(nn.Module) torchvision matplotlib.pyplot NetResDeep(nn.Module) training_loop ResBlock(nn.Module) torch.nn.functional transforms NetDropout(nn.Module) NetBatchNorm(nn.Module) NetRes(nn.Module) datasets training_loop_l2reg validate Netwidth(nn.Module) NetWidth(nn.Module) ", "entities": "(('We', 'convolutional neural network'), 'use') (('that', 'previous features'), 'operate') (('constant we', 'output image'), 'have') (('big values', 'output image'), 'difference') (('change', 'convolutionsb'), 'have') (('s', 'change'), 'let') (('We', 'nn'), 'have') (('it', 'scratch etc'), 'present') (('module instance', 'place'), 'modify') (('we', 'nn'), 'at') (('optimizer', 'lower loss'), 'take') (('we', 'also overfitting'), 'increase') (('pixels', 'input image'), 'produce') (('pretty easy we', 'birds_vs_airplanes'), 'collect') (('weights', 'conv'), 'go') (('RGB channels', '16'), 'have') (('output value', 'value input solely arguments'), 'mean') (('functional APIPyTorch', 'nn module'), 'know') (('Batch norm', 'custom initializations'), 'be') (('That', 'such small images'), 'know') (('exactly it', 'nn'), 'say') (('This', 'last nn'), 'achieve') (('datasetThe data', '0'), 'need') (('very powerful that', 'CIFAR'), 'be') (('model', 'error waysAdmittedly message'), 'have') (('weight component', '3 3'), 'say') (('that', 'birds'), 'take') (('how convolutions', 'convenient images'), 'use') (('We', 'we'), 'get') (('We', 'same size'), 'start') (('which', 'size'), 'increase') (('we', 'standard feed'), 'look') (('so it', '8 effectively back to 8 neighborhoodsof'), 'operate') (('effectively CIFAR 10 imagesare small objects', 'still wing span'), 'go') (('com', 'layer'), 'add') (('we', 'cifar dataset'), 'use') (('what', 'hand'), 'play') (('Typically computation', 'convolutions'), 'use') (('just where I', 'intermediate sizes'), 'need') (('isonyl storedWhile we', 'model'), 'be') (('so we', 'nn'), 'be') (('we', 'convolution'), 'get') (('why we', 'dimension'), 'explain') (('Note', 'somewhat subtle Module'), 'be') (('they', 't.'), 'about') (('premade modules', 'that'), 'moduleat') (('which', 'successive convolutions'), 'stack') (('kaiming_normal _', 'ResNet paper'), 'initialize') (('It', 'closely val accuracy'), 'match') (('DownsamplingScaling', 'one pixel'), 'be') (('Again we', 'an8 then channel'), 'apply') (('output', 'next'), 'ensure') (('output', 'original image'), 'specify') (('explicitly we', 'self'), 'manipulate') (('1 3 do', 'kernel'), 'd') (('3 that', 'locality'), 'recommend') (('Now we', 'database'), 'download') (('extra bits', '5 thenn'), 'pass') (('One possibility', 'convolution large kernels'), 'be') (('that', '8 channel 16 16 output'), 'undergo') (('png ehk 2fd0C 2bA 3d risl 25wY39EtQMRbUWi0278TX3wGQjI11w6Uxr732 Here we', 'CIFAR 10 data'), 'pid') (('theoutput', 'neighborhood'), '_') (('how many samples', 'batch'), 'note') (('here parameters', 'them'), 'be') (('where randomisation', 'iteration'), 'use') (('later we', 'reshaping block'), 'convert') (('where we', 'computation'), 'be') (('convolution kernel', 'width kernel_width 1horizontal positions'), 'by') (('we', 'nn'), 'want') (('which', 'model'), 'be') (('parameters', 'loss'), 'need') (('models', 'overfitting'), 'explore') (('This', 'identity also mapping'), 'refer') (('weight so complete tensor', 'out_ch 3 3 case16'), 'have') (('numbers', 'point small ever large swallowing smaller floating approximation'), 'be') (('that', 'classes'), 'recall') (('How pytorch', 'nn'), 'keep') (('We', 'linear layers'), 'hear') (('that', '2 3 1 side'), 'result') (('when we', 'nn'), 'have') (('32 resulting 16 channel 32 image', '16 16 first MaxPool3d'), 'pool') (('here we', 'multichannel 2D still features'), 'be') (('when network', 'batch norm'), 'find') (('we', 'ahead previous chapter'), 'take') (('poolingThis', 'all well conceptually room'), 'look') (('We', 'aeroplane'), 'work') (('we', 'us'), 'have') (('it', 'subsequent run'), 'note') (('Module Discussion', 'nn'), 'be') (('convolution', 'images'), 'suppose') (('goodpattern', 'torch'), 'be') (('code last bit how organization', 'PyTorch'), 'be') (('s', 'nn'), 'let') (('we', 'when then nn'), 'module') (('keeping', 'loss'), 'termto') (('We', 'airoplane'), 'recognise') (('Applying', 'directions'), 'require') (('PyTorch', 'nn'), 'provide') (('one', 'GPU'), 'consider') (('So we', 'swamp'), 'let') (('also allthe parameters', 'Net'), 'ensure') (('rik R2dcc41f9079d1abf5883a113c0d1ca31 riu', '3a'), 'com') (('right', 'sometimes frustration'), 'realize') (('that', 'images'), 'conclusion') (('2 we', 'othernetworks'), 'be') (('Batch normalization', 'nn'), 'provide') (('contribution', 'other operations'), 'make') (('Information', 'two numbers'), 'get') (('We', 'information'), 'make') (('we', 'nn'), 'need') (('how much training', 'themgrow'), 'craft') (('they', 'function call'), 'of') (('ground we', 'first Boolean array'), 'show') (('it', 'updateas'), 'add') (('features', 'larger structures'), 'ensure') (('Skip connectionsA skip connection', 'block layers'), 'be') (('zero as far convolution', 'value'), 'give') (('pixel', 'neighbors'), 'let') (('InitialisationInitialisation', 'pytorch'), 'be') (('parameter', 't'), 'be') (('output', 'level higher features'), 'consist') (('they', 'style modular andtaste'), 'switch') (('Otherwise optimizer', 'submodules'), 'be') (('that', 'batch dimension'), 'be') (('we', 'anywhere tweaks'), 'train') (('multichannel that', 'kernel'), 'be') (('features', 'such vertical lines'), 'detect') (('autograd', 'pass'), 'take') (('PyTorch', 'you'), '_') (('how submodule', 'nn'), 'matter') (('that', 'output'), 'module') (('original dataset', '0'), 'see') (('us', 'convolutional networks'), 'do') (('us', 'nn'), 'allow') (('that', 'parameters'), 'be') (('it', 'appropriate device'), 'be') (('minibatches', 'certain desirable distibution'), 'be') (('exactly howit', 'PyTorch'), 'be') (('side whatto', 'image'), 'guess') (('output shape', 'input here shape'), 'Conv2dexpects') (('computation', 'just Tensor'), 'be') (('Module', 'child parameters'), 'access') (('weights', 'entire image'), 'have') (('we', 'learning when ondeep projects'), 'have') (('image changing we', 'thing one less toremember'), 'be') (('They', 'module'), 'hold') (('it', 'minibatch'), 'shift') (('it', 'nn'), 'make') (('we', 'nn'), 'provide') (('we', 'forward function'), 'include') (('that', 'usadded flexibility'), 'sequential') (('this', 'model'), 'achieve') (('Then we', 'Tanh activation function'), 'apply') (('So it', 'model'), 'punish') (('only modulethat', '512 32 tensor'), 'find') (('it', 'predict method'), 'say') (('that', 'Dataset'), 'lame') (('even training', 'features'), 'initialize') (('which', 'late 2015'), 'be') (('we', 'birds'), 'proceed') (('that', 'initially 0 mean'), 'initialize') (('Module aswe', 'submodule'), 'do') (('it', 'weight larger values'), 'be') (('that', 'signature linear input weight None'), 'linear') (('skip connection', 'loss'), 'alleviate') (('we', 'log negative likelihood'), 'end') (('which', 'experimentation'), 'slow') (('Net', 'training'), 'conv2d') (('Theweight', 'bias function'), 'be') (('second set', 'those'), 'take') (('where prediction', 'batch'), 'give') (('We', 'reason'), 'put') (('Then we', 'it'), 'scale') (('dataset', 'plane'), 'let') (('weight', '3 3 parts'), 'gif') ", "extra": "['organization']", "label": "Perfect_files", "potential_description_queries": ["absolute", "accuracy", "active", "affine", "apply", "assign", "attribute", "autograd", "backpropagation", "backward", "basic", "batch", "bit", "blob", "block", "border", "bottom", "boundary", "build", "call", "care", "case", "chain", "channel", "child", "choice", "choose", "classification", "code", "computation", "compute", "computer", "connection", "consider", "conv", "convergence", "convert", "convolution", "convolutional", "core", "could", "create", "cuda", "custom", "data", "dataset", "decay", "decision", "default", "define", "depth", "derivative", "describe", "detail", "detected", "device", "dict", "difference", "dimension", "dimensionality", "directly", "download", "effect", "end", "enhance", "ensure", "error", "eval", "even", "every", "fact", "feed", "final", "find", "form", "forward", "found", "function", "functional", "generate", "grad", "gradient", "ground", "half", "hand", "high", "http", "id", "idea", "identity", "ignore", "image", "implementation", "improve", "include", "increase", "index", "init", "initially", "inner", "input", "instance", "intuition", "itself", "kernel", "layer", "lead", "learn", "learning", "leave", "left", "length", "let", "level", "lifetime", "line", "linear", "list", "local", "log", "look", "looking", "loop", "lot", "lower", "magnitude", "manage", "matching", "max", "maximum", "mean", "message", "might", "minimize", "minimum", "missing", "model", "module", "move", "my", "need", "negative", "neighborhood", "network", "neural", "new", "nn", "no", "none", "norm", "normal", "normalization", "not", "notebook", "number", "optimizer", "option", "order", "organization", "out", "outer", "output", "overfitting", "padding", "pair", "para", "parameter", "part", "partial", "path", "peak", "people", "per", "picture", "pixel", "place", "plane", "png", "point", "pooling", "possibility", "practice", "predict", "prediction", "present", "print", "probability", "problem", "processing", "property", "provide", "pytorch", "random", "re", "read", "remove", "rescale", "result", "right", "running", "save", "scale", "scaled", "scope", "scratch", "second", "section", "sense", "separate", "sequence", "set", "several", "shape", "side", "similar", "single", "situation", "size", "something", "source", "squared", "standard", "start", "state", "step", "store", "structure", "style", "subset", "sum", "tanh", "tensor", "term", "think", "those", "through", "time", "torch", "total", "track", "train", "training", "transformation", "tuple", "turn", "type", "unit", "until", "update", "val", "validation", "value", "vanilla", "variability", "variable", "variance", "vector", "verify", "vertical", "view", "weight", "while", "width", "work"], "potential_description_queries_len": 264, "potential_script_queries": ["nn", "torchvision", "validate"], "potential_script_queries_len": 3, "potential_entities_queries": ["channel", "linear", "neural", "vertical"], "potential_entities_queries_len": 4, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 264}