{"name": "kernel tricks to build efficient classifier model ", "full_name": " h1 Author Pradeep Sathyamurthy h1 Date Started Oct 15 2017 h1 Last Modified Date Oct 28 2017 h1 Topic Focussed SVM Kernals h1 Dataset Voice Dataset for Gender Regonition from Kaggle h1 Introduction h3 1 SVM is a kernal trick which can be used for both supervised and unsupervised learning h3 2 As part of this case study I am going to apply SVM for a supervised learning as I am aware of the class labels to be classified h3 3 Thus in this notebook I will be using the voice dataset obtained from URL sighted below to classify if the parameters for a particular instances is a male or a female h1 Objective of case study h3 1 My main objective is to apply SVM and its different kernals and observe how the margin defined helps in improving the classification accuracy h3 2 I will try to tune different parameters in Kernal and choose the best tuning parameter wrt SVM to classify the dataset h3 3 I will also apply different classification techniques and compare the results obtained from these with result obtained from SVM classifier h1 Steps involved in this case study h3 1 Data Manipulation h3 2 Setting a benchmark accuracy for classifiers using Raw Data Naive Bayes h3 3 Exploratory Data Analysis h3 4 Data Munging and Partition h3 5 Validating the cleaned dataset with benchmark accuracy obtained h3 6 Core Model Building Applying Different Kernals for SVM h4 6 1 Linear Kernal SVM h4 6 2 RBF Kernal SVM h4 6 3 Polynomial Kernal SVM h4 6 4 Sigmoidal Kernal SVM h3 7 Perfomance Evaluation on Different Kernals for SVM with 10 fold cross validation h4 7 1 Evaluation on Linear Kernal SVM h4 7 2 Evaluation on RBF Kernal SVM h4 7 3 Evaluation on Polynomial Kernal SVM h4 7 4 Evaluation on Sigmoidal Kernal SVM h3 8 Parameter tuning on Different Kernals for SVM with 10 fold cross validation h4 8 1 Tuning on Linear Kernal SVM h4 8 2 Tuning on RBF Kernal SVM h4 8 3 Tuning on Polynomial Kernal SVM h3 9 Choosing best Kernals Parameters with grid search h3 10 Visualization of kernal Margin and boundries considereing on two columns meanfun sp ent h3 11 Building a Decision Tree h3 12 Building a KNN model h3 13 Comparing individual classifier results h3 14 Ensemble Learning h3 15 Reporting and Discussing final results h3 16 Final Model h1 Dataset URL h1 Importing Packages h1 Step 1 Data Manipulation h3 Reading Data h3 Data Types of Features h3 Checking for Missing Values h3 Seperating Independent and Target Variables h3 Target Variable Encoding h3 Inference h4 1 All independent variables are continuous in nature h4 2 While the target variables seems binary in nature of typr str h4 3 There are totally 3168 rows with 21 columns h4 4 There are no missing values in any of the record h1 Step 2 Setting a benchmark accuracy for classifiers using Raw Data Naive Bayes h3 Inference h4 1 Naive Bayes is a naive method which uses the probablistic theory to classify a target table h4 2 Since it has a fast computation power in training a data and testing it we can use it as a base method to validate our dataset h4 3 Accuracy obtained from this can be set as a bench mark for any classifier that we will start to work going forward h4 4 Using the raw data and classifying the dataset with Naive implementation with cross validation i obtained an accuracy of 0 85671 h4 5 Thus any data clean up we do further or any classifier model we build should not decrease the accuracy that we obtained here and it must always yeald a high or atleast an accuracy equal to 0 85671 else we will discard the data cleaning done or classifier built to classify the target variable h1 Step 3 Exploratory Data Analysis EDA h4 1 Variables meanfreq sd median Q25 are normally distributed h4 1 From above visualization and summary stats we can say Q75 is normally distributed h4 2 While IQR skew and kurt are skewed to right h4 1 sp ent s fm centroid are normally distributed h4 2 While mode is skewed h4 1 Variables meanfun is normally distributed h4 2 While variables minfun maxfun meandom are skewed h4 1 Variables modindx is normally distributed h4 2 While variables mindom maxdom and dfrange are skewed h3 Inference h4 1 Lets explain the skeweness in data from above visualization and summary stats h4 2 Irrespectve to viz of histogram we can also infer those attributes with mean and median values almost equal have gaussian distribution h4 3 Thus variables meanfreq sd median Q25 Q75 sp ent sfm centroid meanfun are Normally distributed h4 4 Variables skew kurt minfun maxfun meandom mindom maxdom dfrange midindex IQR mode are skewed h4 5 Exceptable range of voice freq for a human as per wiki is between 0 085 and 0 255KHz and hence we will remove any values from the dataset below 0 085 and above 0 255 assuming it to be a outlier based on domain knowledge h4 6 Our target variables 1 Male and 0 Female are symmetrical in nature with equal count of 1584 records for both Male and Female h1 Step 4 Data Munging and Partition h3 Data Cleaning h4 1 Exceptable range of voice freq for a human as per wiki is between 0 085 and 0 255KHz and hence we will identify the variable which has this frequncy information and remove them assuming it to be a outlier based on domain knowledge h4 2 In our data set meanfun is the variable which have the value of Fundamental frequency h4 3 As per the sitation given in wiki we can say that typical adult male will have a fundamental frequency from 85 to 180 xa0Hz and typical adult female from 165 to 255 xa0Hz h4 4 Thus from given dataset we will filter values based on meanfun whose values less than 0 085 and greater than 0 18 for male and values less than 0 165 and greater than 0 255 for female and consider them as outliers and remove them h3 Normalization h4 1 In this dataset meanfreq median Q25 Q75 IQR are the only variables associated with unit kHz h4 2 let us normalize these variables to make them unit free h4 3 we will apply the z score normalization for meanfreq median Q25 Q75 h4 4 we will apply min max normalization for IQR h3 Creating Partially Normalized Data h3 Handling Multicollinearity h3 Creating Completely Normalized Dataset All columns are normalized h3 Data Partition h3 Inference h4 1 I treated the variables with units making them unit free by standardizing them h4 2 z score normalization for meanfreq median Q25 Q75 was done h4 3 min max normalization was done for IQR variable h4 4 correlation between independent variables was checked to handle the multicollinearity issues h4 5 correlation between two variables greater than 0 9 are considered to be heavily coreelated and with respective VIF factor h4 6 Variables kurt Centroid dfrange z meanfreq was removed from dataset and this was maintained as a whole new dataset h4 7 Target variable was converted to numeric male as 1 and female as 0 using sklearn preprocessing pack n labelencoder object h4 8 Data partition was done based on sklearns model selection package using train test split object h4 9 Thus I have 4 dataset treated from raw data h4 10 I have 4 dataset treated from raw data and dimentionality reduced h4 11 I have 4 dataset treated from raw data with all independent variables normalized h1 Step 5 Validating the cleaned dataset with benchmark accuracy obtained h4 1 NB Cross Validation on Treated raw dataset h4 2 NB Cross Validation on Treated partially normalized and dimension reduced dataset This can at times help in building best SVM h4 3 NB Cross Validation on Treated and Completely Normalized dataset h3 Inference h4 1 Naive bayes classifier after data tretment produce an avg accuracy of 0 95 being the data is normalized or not normalized h4 2 we see a significant increase in accuracy from 0 85671 to 0 952 after we clean the data h4 3 We see the data with dimention reduced and data which are completely normalized works better than raw treated dataset h4 4 However this can be considered as a base classifier at this point and above result makes sure that our data clean up holds good and we havent removed any influential datas from dataset h4 5 This also set a new benchmark for any complex classifier that will be built further h4 6 Thus accuracy of 0 95 can be set as a bench mark accuracy value for this dataset which is cleaned and processed h4 7 Any model which produce accuracy less than 0 95 can be consodired as a non efficient model for this dataset from now on h1 Step 6 Core Model Building Applying Different Kernals for SVM h3 6 1 Linear Kernal SVM h3 Inference h4 1 I subjected 3 different dataset as explained above to a linear SVM model and I can observe that dataset which is completely normalize is performing well h4 2 As part of this kernal trick we have our hyperplane to be linear in a 20 dimentional space h4 3 This model exhibit a classification accuracy of 0 993902 h4 4 Since the data is 20 dimentional we cannot visualize if the data pocesses a linear or curved relation in feature space we can take a domain level expertise here h4 5 However since we have none for individual analysis purpose we will try to build a model with other kernal tricks types too and see how the model behaves in classifying the gender h3 6 2 RBF Kernal SVM h3 Inference h4 1 RBF or Gaussian is the default kernal which SVM uses in sklearn h4 2 Performance of RBF kernal trick is also same as linear kernal SVM h4 3 I obtained a accuracy of 0 993902 for RBF Kernal using SVM for normalized dataset h4 4 This shows that our voice dataset are both linearly and gaussian seperable h3 6 3 Polynomial Kernal SVM h3 Inference h4 1 To acheive much more high accuracy i tried using polynomial kernal too h4 2 I obtained an accuracy of 0 985 for polynomial kernal on normalized dataset h4 3 This is comparitively much less than the linear and rbf kernals h4 4 However we cannot conclude this result at this stage as our training dataset is just one single sample on which we obtained this result h3 6 4 Sigmoidal Kernal SVM h3 Inference h4 1 When a dataset is behaving well linearly it is explicitly known that it doesn t work well in a sigmoidal space h4 2 Above result obtained is the evident for this h4 3 I obtained accuracy of just 0 831 with sigmoidal kernal h3 4 5 Consolidated model accuracy h3 Inference h4 1 From above table it is clear that a completely normalized dataset behaves well compare to un normalized dataset h4 2 I obtain a maximum accuracy due to the data treatment done that is treating the meanfun attribute based on biological fact h4 3 Maximum accuracy i could acheive is 0 9939 whcih is from Linear and Gaussian Kernal using SVM h4 4 While the polinomial and Sigmoidal kernal doesn t seems to classify the target variable accurately and giving a low accuracy of 0 95 and 0 83 for Polynomial and Sigmoidal keransl respectively h4 5 However I cannot blindly accept this accuracy result because this is derived from one sample of training set and validated with a sample test set In order to evaluate this model to be more robust and to ensure data doesnt overfit I wanted to subject these model and dataset to a 10 fold cross validation and observe its result as part of next session h1 Step 7 Perfomance Evaluation on Different Kernals for SVM with 10 fold cross validation h3 7 1 Evaluation on Linear Kernal SVM h3 Inference h4 1 I see even with 10 fold cross validation our linear kernal SVM is providing a high accuracy of 0 9939 h4 2 Thus I can consider linear Kernal SVM as one of the serious model to subject for further tuning and see if it increases the accuracy h4 3 From abov table it is still evident that the completely normalized dataset behaves well comparitively h3 7 2 Evaluation on RBF Kernal SVM h3 Inference h4 1 From above table I see a slight decrease in accuracy when I subject Gaussian kernal to 10 fold cross validation h4 2 With out 80 20 split test set we saw an accuracy of 0 9939 however with 10 fold CV we obtain accuracy of 0 986 h4 3 Thus so far we see linear kernal is behaving well consistently and there is a slight decrese with gaussian kernal h3 7 4 Evaluation on Sigmoidal Kernal SVM h2 Inference h4 1 Like Gaussian kernal even polynomial and sigmoidal kernals yeald less accuracy with 10 fold CV h4 2 I did not include the results of polynomial kernal subjected to 10 fold CV because it was consuming more time to compute h4 3 However results of sigmoidal kernal is shown above and we see accuracy is dropped from 0 81 to 0 79 h3 7 5 Consolidated SVM Kernal Model s Evaluation Result h3 Inference h4 1 From above table it is clearly evident that Linear SVM Kernal on a completely normalized datset behaves really well h4 2 Even with 10 fold cross validation I obtaned an accuracy of 0 9933927 which seems consistent when compare to other kernals h4 3 After linear kernal it is the Gaussian and Polynomial kernal which gives high accuracy h4 4 So as part of next session we will drop Sigmoidal kernal from our further analyis as it doens t even satisfy the bench mark accuracy h4 5 I will take up other 3 SVM models for performance tuning and see how the accuracychanges when we tradeoff between kernal parameters like penalty C and gamma in order to obtain a soft margin h1 Step 8 Parameter tuning on Different Kernals for SVM with 5 fold cross validation experimenting with margins h4 From above experimentation we see dataset which was normalized yeald a good result h4 Thus for further experimentation we will use the dataset whose independent variables are normalized i e h4 data x3 and data y3 h3 8 1 Tuning on Linear Kernal SVM h3 Inference h4 1 Ultimate aim in building a kernal is to find an optimum hyper plane in feature space which has maximum margin in classifying our target variable h4 2 Kernal which I have built above so far in order to check the performance are those with hard margins this is not good to be generalized as it may cause overfitting h4 3 So in this session we will trade off between margin and Support vectors to choose an optimum boundry which will not overfit the model and at the same time deliver a high accuracy in classifying the target variable h4 4 With linear kernal it is the penalty measure through which we can do some trade off h4 5 Above table shows the accuracy model performance for different values of C h4 6 Both from graph and above table we see 0 6 and 1 1 to be the optimum penalty measure or C value which we can treade off with in classifying the target variable h4 7 Even with such trade off we obtain almost 0 9939 accuracy for linear kernal h3 8 2 Tuning on RBF Kernal SVM h3 Inference h4 1 In Gaussian kernal tradeoff is done with penalty C along with gamma parameter h4 2 I first experimented with wider Gamma values ranging between 1 and 10 and obsevred Kernal started to behave bad with gamma greater than 1 h4 3 So I tried to find the most optimum value with in 0 and 1 and as show in above table i obtained a maximum accuracy of 0 991 when gammal was equal to 0 03 and 0 05 h4 4 However when compare to Linear kernal we see rbf produce an accuracy of 0 002 times less h4 5 Thus it is quite evident again that linear kernal acts well on this dataset in classification of target variable h3 8 3 Tuning on Polynomial Kernal SVM h3 Inference h4 1 Along with penalty and gamma parameter with polynomial kernal we can trade off with degree h4 2 I experimented with various degree as shown above and obtained degree 1 1 produce a high accuracy h4 3 Accuracy obtained by polynomial is almost same as Linear which is 0 993 h4 4 So to produce a final inference in choosing the best kernal we will apply a grid search in our next session and see which model and which parameter produce a high accuracy h1 Step 9 Choosing best Kernals Parameters with grid search h3 9 1 Choosing the best parameter h3 Inference h4 1 I did a grid search whcih is a structure way to obtain an optimized kernal and its parameter measures h4 2 From above result I see it is the polynomial kernal with penalty measure of C 1 6 and gamma 0 005 and with degree 1 produce a high accuracy of 0 9939 in classifying the target variable h4 3 In this next session i have tried to visualize my margin and kernal behaviour by subjecting only 2 columns for analysis as it becomes a 2 dimentional space for visualization h1 Step 10 Visualization of kernal Margin and boundries considereing only two columns meanfun sp ent to represent a 2D space h3 10 1 Choosing the best attribute to represent dataset in 2D space h3 Inference h4 1 After doing necessary data cleanup and model building I was able to infer that a polinomial kernal SVM with parameters C 1 6 gamma 0 005 and degree 1 plots a perfect margin in a high dimentional space to classify gender label which is our target variable h4 2 However vizualizing more than two dimention is complex to represnt h4 3 So I would like to choose any 2 variables from dataset through which i can represnt my margin and kernal boundries in a 2 dimentional space h4 4 For this i used the correlation matrix and above scatter plot obtained above and choose two variable which is moderately correlated As neither the strong nor the weak correlation variables might not be well represented in ourder to show the decision boundries h4 5 meanfun being the most important variable for the dataset I decided to choose it and match it with another variable which has moderate correlation with it with 0 52 as correlation value between i choose sp ent and meanfun to be my choise of 2 dimentional feature space h3 10 2 Visualizing the margin modeled h3 Inference h4 1 meanfun being the most important variable for the dataset I decided to choose it and match it with another variable which has moderate correlation with it with 0 52 as correlation value between i choose sp ent and meanfun to be my choise of 2 dimentional feature space h4 2 I modeled polynomial kernal with penalty measure of C 1 6 gamma 0 05 and degree 1 to obtain the above scatter plot h4 3 When did SVM projected my data in a 2 dimentional space and obtained an optimal margin that classifies my gender being male and female h4 4 From the above figure we can infer h4 1 Orage points Instance which are Male h4 2 Blue Points Instance which are Female h4 3 Circled Points Support Vectors used to obtain margin h4 4 Straingh Line Hard Margin h4 5 Dotted Lines Soft Margin with trade off being C 1 6 gamma 0 05 and degree 1 h4 5 With respective to only these two variables meanfun and sp ent It is so evident that our model is not being overfit as it gives a clear distinction between two classes Male and Female with no complications in margins Thus accuracy of 0 99 can be considered to be valid enough at this point However this is just the visualization about margins we will not visualize how the SVM boundy is placed in a for all our parameters in a 2D space h3 10 3 Visualizing the Kernal boundaries h3 Inference h4 1 I still consider meanfun and sp ent to be my favorite variables to visualize my kernal boundries in a 2D space h4 2 I modeled polynomial kernal with same parameters penalty measure of C 1 6 gamma 0 05 and degree 1 to obtain the above scatter plot h4 3 When did SVM projected my data in a 2 dimentional space and obtained above feature space with boundries that classifies gender being male and female h4 4 From the above figure we can infer h4 1 Linear kernal with c 1 6 have a strict boundry h4 2 While in RBF kernal the boundry is strict and also have some points misclassified h4 3 Polynomial kernal have a lineant boundry which are discriminative h4 4 From above figure we dont see any complex boundries for polynomial and hence we need not worry about the model being over fitting h4 5 With respective to only these two variables meanfun and sp ent It is so evident that our model is not being overfit as it gives a clear distinction between two classes Male and Female with no complications in margins in a feature space h4 6 Thus accuracy of 0 993 produced by Polynomial kernal can be considered to be valid enough this means 7 out of 1000 times ther could be a misclassification Let is see if we can minimize this error occurence by increasing the accuracy further using few ensemble learnings h1 Step 11 Building a Decision Tree Classifier with grid search h3 Inference h4 1 I see accuracy yealded by decision tree is 0 9894 which is less when compare to SVM classifier which was 0 993 h4 2 We can say compare to decision tree SVM model seems more efficient h4 3 So if scrutability is the requirement based on which a model needs to be built we can go ahead with decision tree model h1 Step 12 Building a KNN with 5 nearest neighbors h3 Inference h4 1 KNN yealds an accuracy of 0 977 which is comparitive less to SVM h4 2 However its accuracy touches the benchmark of 0 95 which we decided based on Naive Bayes we can have this model for any ensemble building etc and it not advisable to just discard it h4 2 Though KNN perform better than Naive Bayes its accuracy is less compare to SVM h1 13 Comparing individual classifier results h3 Inference h4 1 From above table and graph it seems very clear that SVM with polynomial kernal behaves best h4 2 Accuracy produces by Polynomial kernal equal to 0 993 is the highest of all cross validation results obtained from other classifiers h4 3 Thus with individual classifiers we can infer that as a individual classifier SVM with Polynomial Kernal does a best classification wrt his voice dataset in classifying an instance as Male or Female h4 4 This SVM polynomial kernal tend to miss classify only 7 out of 1000 times when subjected to such dataset which is pretty good h4 5 However we will yet try to improve the accuracy further using some ensemble techniques h1 14 Ensemble Learning h3 14 1 Bagging with Random Forest h3 14 2 Boosting with Random Forest h1 15 Reporting and Discussing the final results h1 16 Final Model h3 Inference h4 1 From above table and graph it seems very clear that SVM with polynomial kernal behaves best h4 2 Accuracy produces by Polynomial kernal equal to 0 993 is the highest of all cross validation results obtained from other classifiers h4 3 Thus with individual classifiers we can infer that as a individual classifier SVM with Polynomial Kernal does a best classification wrt his voice dataset in classifying an instance as Male or Female h4 4 This SVM polynomial kernal tend to miss classify only 7 out of 1000 times when subjected to such dataset which is pretty good h4 5 However we will yet try to improve the accuracy further using some ensemble techniques h2 End of the Book ", "stargazers_count": 0, "forks_count": 0, "description": "I experimented with various degree as shown above and obtained degree 1. Visualizing the Kernal boundaries Inference 1. 991 when gammal was equal to 0. Data Munging and Partition 5. Naive Bayes is a naive method which uses the probablistic theory to classify a target table 2. So in this session we will trade off between margin and Support vectors to choose an optimum boundry which will not overfit the model and at the same time deliver a high accuracy in classifying the target variable. I first experimented with wider Gamma values ranging between 1 and 10 and obsevred Kernal started to behave bad with gamma greater than 1 3. Exploratory Data Analysis 4. Sigmoidal Kernal SVM 7. 95 which we decided based on Naive Bayes we can have this model for any ensemble building etc. Thus I have 4 dataset treated from raw data a. Naive bayes classifier after data tretment produce an avg accuracy of 0. 9933927 which seems consistent when compare to other kernals. data_y2_train d. However I cannot blindly accept this accuracy result because this is derived from one sample of training set and validated with a sample test set. Polynomial kernal have a lineant boundry which are discriminative 4. Step 2 Setting a benchmark accuracy for classifiers using Raw Data Naive Bayes Inference 1. Performance of RBF kernal trick is also same as linear kernal SVM 3. 985 for polynomial kernal on normalized dataset 3. Tuning on RBF Kernal SVM 8. Evaluation on Polynomial Kernal SVM 7. Evaluation on Linear Kernal SVM Inference 1. Core Model Building Applying Different Kernals for SVM 6. Let is see if we can minimize this error occurence by increasing the accuracy further using few ensemble learnings. Step 9 Choosing best Kernals Parameters with grid search 9. 1 to be the optimum penalty measure or C value which we can treade off with in classifying the target variable. I see accuracy yealded by decision tree is 0. When did SVM projected my data in a 2 dimentional space and obtained an optimal margin that classifies my gender being male and female. Step 8 Parameter tuning on Different Kernals for SVM with 5 fold cross validation experimenting with margins From above experimentation we see dataset which was normalized yeald a good result Thus for further experimentation we will use the dataset whose independent variables are normalized i. From abov table it is still evident that the completely normalized dataset behaves well comparitively 7. data_y3_train d. Linear Kernal SVM Inference 1. Since it has a fast computation power in training a data and testing it we can use it as a base method to validate our dataset 3. Using the raw data and classifying the dataset with Naive implementation with cross validation i obtained an accuracy of 0. NB Cross Validation on Treated partially normalized and dimension reduced dataset This can at times help in building best SVM 3. Along with penalty and gamma parameter with polynomial kernal we can trade off with degree 2. 99 can be considered to be valid enough at this point. Thus any data clean up we do further or any classifier model we build should not decrease the accuracy that we obtained here and it must always yeald a high or atleast an accuracy equal to 0. I see even with 10 fold cross validation our linear kernal SVM is providing a high accuracy of 0. Linear Kernal SVM 6. So as part of next session we will drop Sigmoidal kernal from our further analyis as it doens t even satisfy the bench mark accuracy. 01 performing grid search with different tuning parameters Scatter plot with strong correlation not useful much to represnt the distribution wrt kernal boundries Scatter plot with weak correlation not useful much to represnt the distribution wrt kernal boundries Scatter plot with moderate correlation useful much to represnt the distribution wrt kernal boundries Scatter plot with moderate negative correlation useful much to represnt the distribution wrt kernal boundries import some data to play with fit the model don t regularize for illustration purposes title for the plots plot the decision function create grid to evaluate model plot decision boundary and margins plot support vectors import some data to play with SVM regularization parameter title for the plots Set up 2x2 grid for plotting. With out 80 20 split test set we saw an accuracy of 0. Step 11 Building a Decision Tree Classifier with grid search Inference 1. Consolidated SVM Kernal Model s Evaluation Result Inference 1. When did SVM projected my data in a 2 dimentional space and obtained above feature space with boundries that classifies gender being male and female. Accuracy produces by Polynomial kernal equal to 0. With respective to only these two variables meanfun and sp. While mode is skewed 1. 005 and degree 1 plots a perfect margin in a high dimentional space to classify gender label which is our target variable 2. linspace 2 20 10 lets do a 10 fold Cross validation to make sure the accuracy obtained above lets do a 10 fold Cross validation to make sure the accuracy obtained above Applying Random forest to improve the decision tree model lets do a 10 fold Cross validation to make sure the accuracy obtained above adaboost lets do a 10 fold Cross validation to make sure the accuracy obtained above Building the ROC Curve for the final SVM Kernal model CV Accuracy ROC measure. In this dataset meanfreq median Q25 Q75 IQR are the only variables associated with unit kHz 2. Any model which produce accuracy less than 0. 255 assuming it to be a outlier based on domain knowledge 6. However since we have none for individual analysis purpose we will try to build a model with other kernal tricks types too and see how the model behaves in classifying the gender. To acheive much more high accuracy i tried using polynomial kernal too 2. Parameter tuning on Different Kernals for SVM with 10 fold cross validation 8. Validating the cleaned dataset with benchmark accuracy obtained 6. While variables mindom maxdom and dfrange are skewed Inference 1. Normalization 1. 9939 however with 10 fold CV we obtain accuracy of 0. I will take up other 3 SVM models for performance tuning and see how the accuracychanges when we tradeoff between kernal parameters like penalty C and gamma in order to obtain a soft margin. and it not advisable to just discard it. 1 produce a high accuracy 3. ent to represent a 2D space 10. 9939 in classifying the target variable. I treated the variables with units making them unit free by standardizing them 2. Step 10 Visualization of kernal Margin and boundries considereing only two columns meanfun sp. Thus in this notebook I will be using the voice dataset obtained from URL sighted below to classify if the parameters for a particular instances is a male or a female Objective of case study 1. 05 and degree 1 5. I obtained accuracy of just 0. We can say compare to decision tree SVM model seems more efficient 3. data_x3_train b. SVM is a kernal trick which can be used for both supervised and unsupervised learning. 9 are considered to be heavily coreelated and with respective VIF factor 6. However this is just the visualization about margins we will not visualize how the SVM boundy is placed in a for all our parameters in a 2D space. End of the Book for data handling for data manipulation for plotting For encoding class variables for train and test split to built svm model inherits other SVM objects to calculate classifiers accuracy to perform cross validation to perform standardization to perform grid search for all classifiers to perform decision tree classification to perform knn to perform Naive Bayes produce classifier reports to perform ensemble bagging random forest to perform ensemble boosting to plot ROC Curve Reding the data as pandas dataframe Verifying if all records are read having the headers handy Data type Checking for any missing values in data and other junk values if any let us seperate the independent and dependent variables seperately encoding the target variable from categorical values to binary form Let us do a 80 20 split let us do a descriptive statistics Distribution of target variables Actual Raw Data size Filtering ouliers from male category Filtering ouliers from female category Thus we need to remove 710 rows from both data_x and data_y using the index obtained from above filters Preparing final dataset for model building Target dataset Distribution of target variables after cleanup Z score Normalization Lets now drop the original column from data_x as we have these as backup in data_raw dataframe Plotting the normalized columns we could see that z score norm variables have mean 0 and standard deviation 1 And the min max norm varibales value are confined between 0 1 and stays positive let us see the correlation in data Thus we see high correlation exist between above variables thus let us create a dataset by removing variables that create high Variance Inflation Factor Thus removing kurt Centroid dfrange z_meanfreq let me not do any dimentionality reduction and do z score normalization on all independent variables Let us do a 80 20 split on raw dataset let us do a 80 20 split on dimention reduced dataset too let us do a 80 20 split on raw dataset which was only normalized let us check the size let is cross check the size of dimention reduced data set too let is cross check the size of normalized raw data set too defining the Naive Bayes object lets do a 10 fold Cross validation to make sure the accuracy obtained above lets do a 10 fold Cross validation to make sure the accuracy obtained above lets do a 10 fold Cross validation to make sure the accuracy obtained above Partially normlized dataset Dimention reduced dataset Completely normalized dataset Partially normlized dataset Dimention reduced dataset Completely normalized dataset Partially normlized dataset Dimentione reduced dataset Completely normalized dataset Partially normlized dataset Dimentione reduced dataset Completely normalized dataset Partially normlized dataset Dimentione reduced dataset Completely normalized dataset Partially normlized dataset Dimentione reduced dataset Completely normalized dataset Partially normlized dataset Dimentione reduced dataset Completely normalized dataset penality parameter C is 1. Polynomial Kernal SVM Inference 1. ent sfm centroid meanfun are Normally distributed 4. Variables skew kurt minfun maxfun meandom mindom maxdom dfrange midindex IQR mode are skewed 5. 255KHz and hence we will identify the variable which has this frequncy information and remove them assuming it to be a outlier based on domain knowledge 2. I did not include the results of polynomial kernal subjected to 10 fold CV because it was consuming more time to compute 3. Tuning on Linear Kernal SVM 8. I have 4 dataset treated from raw data and dimentionality reduced a. Lets explain the skeweness in data from above visualization and summary stats 2. Maximum accuracy i could acheive is 0. Straingh Line Hard Margin 5. 831 with sigmoidal kernal 4. In our data set meanfun is the variable which have the value of Fundamental frequency 3. As part of this case study I am going to apply SVM for a supervised learning as I am aware of the class labels to be classified. Ultimate aim in building a kernal is to find an optimum hyper plane in feature space which has maximum margin in classifying our target variable. For this i used the correlation matrix and above scatter plot obtained above and choose two variable which is moderately correlated. Dotted Lines Soft Margin with trade off being C 1. Choosing the best attribute to represent dataset in 2D space Inference 1. From above visualization and summary stats we can say Q75 is normally distributed 2. Ensemble Learning 14. We see the data with dimention reduced and data which are completely normalized works better than raw treated dataset. Even with such trade off we obtain almost 0. I have 4 dataset treated from raw data with all independent variables normalized a. 83 for Polynomial and Sigmoidal keransl respectively. Evaluation on Sigmoidal Kernal SVM Inference 1. 85671 else we will discard the data cleaning done or classifier built to classify the target variable. Though KNN perform better than Naive Bayes its accuracy is less compare to SVM 13. Tuning on Linear Kernal SVM Inference 1. Comparing individual classifier results 14. I will try to tune different parameters in Kernal and choose the best tuning parameter wrt SVM to classify the dataset 3. meanfun being the most important variable for the dataset I decided to choose it and match it with another variable which has moderate correlation with it. Accuracy obtained by polynomial is almost same as Linear which is 0. I subjected 3 different dataset as explained above to a linear SVM model and I can observe that dataset which is completely normalize is performing well. Circled Points Support Vectors used to obtain margin 4. Accuracy obtained from this can be set as a bench mark for any classifier that we will start to work going forward 4. ent and meanfun to be my choise of 2 dimentional feature space. Above table shows the accuracy model performance for different values of C 6. 05 and degree 1 to obtain the above scatter plot. Tuning on Polynomial Kernal SVM 9. I modeled polynomial kernal with penalty measure of C 1. 18 for male and values less than 0. From above table and graph it seems very clear that SVM with polynomial kernal behaves best. 95 being the data is normalized or not normalized 2. Author Pradeep Sathyamurthy Date Started Oct 15 2017 Last Modified Date Oct 28 2017 Topic Focussed SVM Kernals Dataset Voice Dataset for Gender Regonition from Kaggle Introduction 1. correlation between two variables greater than 0. Variables modindx is normally distributed 2. As per the sitation given in wiki we can say that typical adult male will have a fundamental frequency from 85 to 180 Hz and typical adult female from 165 to 255 Hz 4. Above result obtained is the evident for this 3. Visualizing the margin modeled Inference 1. ent It is so evident that our model is not being overfit as it gives a clear distinction between two classes Male and Female with no complications in margins. Building a Decision Tree 12. There are totally 3168 rows with 21 columns 4. Target variable was converted to numeric male as 1 and female as 0 using sklearn preprocessing pack n labelencoder object 8. Final Model Dataset URL http www. RBF or Gaussian is the default kernal which SVM uses in sklearn 2. Orage points Instance which are Male 2. Kernal which I have built above so far in order to check the performance are those with hard margins this is not good to be generalized as it may cause overfitting. fm centroid are normally distributed 2. correlation between independent variables was checked to handle the multicollinearity issues 5. min max normalization was done for IQR variable 4. Since the data is 20 dimentional we cannot visualize if the data pocesses a linear or curved relation in feature space we can take a domain level expertise here. This model exhibit a classification accuracy of 0. I did a grid search whcih is a structure way to obtain an optimized kernal and its parameter measures 2. Setting a benchmark accuracy for classifiers using Raw Data Naive Bayes 3. Reporting and Discussing final results 16. Evaluation on RBF Kernal SVM Inference 1. ent to be my favorite variables to visualize my kernal boundries in a 2D space. However its accuracy touches the benchmark of 0. Bagging with Random Forest 14. As neither the strong nor the weak correlation variables might not be well represented in ourder to show the decision boundries. After linear kernal it is the Gaussian and Polynomial kernal which gives high accuracy 4. Both from graph and above table we see 0. However we cannot conclude this result at this stage as our training dataset is just one single sample on which we obtained this result. Ensemble Learning 15. Exceptable range of voice freq for a human as per wiki is between 0. Our target variables 1 Male and 0 Female are symmetrical in nature with equal count of 1584 records for both Male and Female Step 4 Data Munging and Partition Data Cleaning 1. Exceptable range of voice freq for a human as per wiki is between 0. data_y3_test Step 5 Validating the cleaned dataset with benchmark accuracy obtained 1. 9939 whcih is from Linear and Gaussian Kernal using SVM 4. However this can be considered as a base classifier at this point and above result makes sure that our data clean up holds good and we havent removed any influential datas from dataset. This shows that our voice dataset are both linearly and gaussian seperable 6. z score normalization for meanfreq median Q25 Q75 was done 3. linspace 1 30 15 min_samples_split np. Evaluation on RBF Kernal SVM 7. When a dataset is behaving well linearly it is explicitly known that it doesn t work well in a sigmoidal space 2. Blue Points Instance which are Female 3. However results of sigmoidal kernal is shown above and we see accuracy is dropped from 0. From above table I see a slight decrease in accuracy when I subject Gaussian kernal to 10 fold cross validation 2. data_x3 and data_y3 8. Tuning on RBF Kernal SVM Inference 1. Choosing the best parameter Inference 1. I obtained an accuracy of 0. So I tried to find the most optimum value with in 0 and 1 and as show in above table i obtained a maximum accuracy of 0. NB Cross Validation on Treated and Completely Normalized dataset Inference 1. 977 which is comparitive less to SVM 2. Step 12 Building a KNN with 5 nearest neighbors Inference 1. Evaluation on Linear Kernal SVM 7. While in RBF kernal the boundry is strict and also have some points misclassified 3. Consolidated model accuracy Inference 1. NB Cross Validation on Treated raw dataset 2. 9939 accuracy for linear kernal 8. While the target variables seems binary in nature of typr str 3. we will apply min max normalization for IQR Creating Partially Normalized Data Handling Multicollinearity Creating Completely Normalized Dataset All columns are normalized Data Partition Inference 1. Boosting with Random Forest 15. Data Manipulation 2. My main objective is to apply SVM and its different kernals and observe how the margin defined helps in improving the classification accuracy 2. Comparing individual classifier results Inference 1. 9894 which is less when compare to SVM classifier which was 0. However vizualizing more than two dimention is complex to represnt 3. 993 is the highest of all cross validation results obtained from other classifiers. 1 and kernel as linear With rbf gamma value 0. 993 produced by Polynomial kernal can be considered to be valid enough this means 7 out of 1000 times ther could be a misclassification. Choosing best Kernals Parameters with grid search 10. Sigmoidal Kernal SVM Inference 1. This SVM polynomial kernal tend to miss classify only 7 out of 1000 times when subjected to such dataset which is pretty good. In this next session i have tried to visualize my margin and kernal behaviour by subjecting only 2 columns for analysis as it becomes a 2 dimentional space for visualization. This also set a new benchmark for any complex classifier that will be built further 6. Variables meanfun is normally distributed 2. I obtain a maximum accuracy due to the data treatment done that is treating the meanfun attribute based on biological fact 3. From above result I see it is the polynomial kernal with penalty measure of C 1. Final Model Inference 1. While the polinomial and Sigmoidal kernal doesn t seems to classify the target variable accurately and giving a low accuracy of 0. From the above figure we can infer 1. So to produce a final inference in choosing the best kernal we will apply a grid search in our next session and see which model and which parameter produce a high accuracy. Evaluation on Sigmoidal Kernal SVM 8. I will also apply different classification techniques and compare the results obtained from these with result obtained from SVM classifier Steps involved in this case study 1. Thus from given dataset we will filter values based on meanfun whose values less than 0. Thus so far we see linear kernal is behaving well consistently and there is a slight decrese with gaussian kernal 7. All independent variables are continuous in nature 2. Step 3 Exploratory Data Analysis EDA 1. Thus I can consider linear Kernal SVM as one of the serious model to subject for further tuning and see if it increases the accuracy 3. Like Gaussian kernal even polynomial and sigmoidal kernals yeald less accuracy with 10 fold CV 2. Linear kernal with c 1. 005 and with degree 1 produce a high accuracy of 0. Building a KNN model 13. Data partition was done based on sklearns model_selection package using train_test_split object 9. 95 can be consodired as a non efficient model for this dataset from now on Step 6 Core Model Building Applying Different Kernals for SVM 6. 95 can be set as a bench mark accuracy value for this dataset which is cleaned and processed. However when compare to Linear kernal we see rbf produce an accuracy of 0. So if scrutability is the requirement based on which a model needs to be built we can go ahead with decision tree model. min_samples_leaf np. As part of this kernal trick we have our hyperplane to be linear in a 20 dimentional space 3. we see a significant increase in accuracy from 0. 165 and greater than 0. From above table it is clear that a completely normalized dataset behaves well compare to un normalized dataset 2. From above table it is clearly evident that Linear SVM Kernal on a completely normalized datset behaves really well 2. KNN yealds an accuracy of 0. However we will yet try to improve the accuracy further using some ensemble techniques. While IQR skew and kurt are skewed to right 1. Variables kurt Centroid dfrange z_meanfreq was removed from dataset and this was maintained as a whole new dataset 7. While variables minfun maxfun meandom are skewed 1. Irrespectve to viz of histogram we can also infer those attributes with mean and median values almost equal have gaussian distribution. Reporting and Discussing the final results 16. data_x2_train b. RBF Kernal SVM 6. 52 as correlation value between i choose sp. 085 and greater than 0. I obtained a accuracy of 0. we will apply the z score normalization for meanfreq median Q25 Q75 4. Polynomial Kernal SVM 6. 6 have a strict boundry 2. Thus variables meanfreq sd median Q25 Q75 sp. 0 by default in sklearn I would like to experiment it with multiple margins in range of c from 1 to 10 Completely normlized dataset plot the value of C for SVM x axis versus the cross validated accuracy y axis Completely normlized dataset plot the value of C for SVM x axis versus the cross validated accuracy y axis Doing further tradeoff plot the value of C for SVM x axis versus the cross validated accuracy y axis Completely normlized dataset plot the value of C for SVM x axis versus the cross validated accuracy y axis Now performing SVM by taking hyperparameter C 0. Variables meanfreq sd median Q25 are normally distributed 1. Even with 10 fold cross validation I obtaned an accuracy of 0. In order to evaluate this model to be more robust and to ensure data doesnt overfit I wanted to subject these model and dataset to a 10 fold cross validation and observe its result as part of next session Step 7 Perfomance Evaluation on Different Kernals for SVM with 10 fold cross validation 7. Thus it is quite evident again that linear kernal acts well on this dataset in classification of target variable. 952 after we clean the data 3. With linear kernal it is the penalty measure through which we can do some trade off 5. I still consider meanfun and sp. ent It is so evident that our model is not being overfit as it gives a clear distinction between two classes Male and Female with no complications in margins in a feature space. 993902 for RBF Kernal using SVM for normalized dataset 4. In Gaussian kernal tradeoff is done with penalty C along with gamma parameter 2. com 2016 06 22 identifying the gender of a voice using machine learning Importing Packages Step 1 Data Manipulation Reading Data Data Types of Features Checking for Missing Values Seperating Independent and Target Variables Target Variable Encoding Inference 1. There are no missing values in any of the record. I modeled polynomial kernal with same parameters penalty measure of C 1. 255 for female and consider them as outliers and remove them. Perfomance Evaluation on Different Kernals for SVM with 10 fold cross validation 7. Visualization of kernal Margin and boundries considereing on two columns meanfun sp. So I would like to choose any 2 variables from dataset through which i can represnt my margin and kernal boundries in a 2 dimentional space 4. 255KHz and hence we will remove any values from the dataset below 0. Tuning on Polynomial Kernal SVM Inference 1. From above figure we dont see any complex boundries for polynomial and hence we need not worry about the model being over fitting 5. Thus with individual classifiers we can infer that as a individual classifier SVM with Polynomial Kernal does a best classification wrt his voice dataset in classifying an instance as Male or Female 4. let us normalize these variables to make them unit free 3. RBF Kernal SVM Inference 1. This is comparitively much less than the linear and rbf kernals 4. After doing necessary data cleanup and model building I was able to infer that a polinomial kernal SVM with parameters C 1. ", "id": "pradeepsathyamurthy/kernel-tricks-to-build-efficient-classifier-model", "size": "27466", "language": "python", "html_url": "https://www.kaggle.com/code/pradeepsathyamurthy/kernel-tricks-to-build-efficient-classifier-model", "git_url": "https://www.kaggle.com/code/pradeepsathyamurthy/kernel-tricks-to-build-efficient-classifier-model", "script": "sklearn.metrics cross_val_score # to perform cross validation funct_svm SVC # to built svm model measure_performance make_meshgrid LabelEncoder # For encoding class variables pyplot as plt # for plotting plot_contours pyplot numpy naive_bayes # to perform Naive Bayes metrics # to calculate classifiers accuracy sklearn.ensemble sklearn sklearn.model_selection RandomForestClassifier RandomForestClassifier # to perform ensemble bagging - random forest classification_report # produce classifier reports pandas auc # to plot ROC Curve svm # inherits other SVM objects neighbors # to perform knn tree # to perform decision tree classification StandardScaler # to perform standardization GridSearchCV # to perform grid search for all classifiers train_test_split # for train and test split matplotlib sklearn.preprocessing sklearn.svm roc_curve funct_svm_cv funct_tune_svm AdaBoostClassifier # to perform ensemble boosting ", "entities": "(('I', 'fold cross 10 validation'), 'in') (('even polynomial kernals', '10 fold'), 'yeald') (('it', 'domain knowledge'), 'assume') (('I', 'dataset'), 'try') (('we', 'building ensemble etc'), '95') (('parameter', 'high accuracy'), 'produce') (('we', '0'), 'see') (('I', '0'), 'with') (('we', 'degree'), 'trade') (('grid search whcih', 'structure optimized kernal'), 'do') (('when I', 'cross validation'), 'see') (('we', 'target variable'), '1') (('accuracy', '0'), 'show') (('This', 'comparitively much linear'), 'be') (('Accuracy', 'equal 0'), 'produce') (('Data partition', 'train_test_split object'), 'do') (('9939 whcih', 'Gaussian SVM'), 'be') (('95', 'SVM'), 'consodire') (('doesn polinomial kernal t', '0'), 'seem') (('kernal which', 'supervised learning'), 'be') (('variables', 'mindom maxdom'), 'be') (('margins plot support vectors', '2x2 grid'), 'useful') (('we', 'decision tree ahead model'), 'go') (('I', 'reduced a.'), 'have') (('this', 'whole new dataset'), 'dfrange') (('99', 'enough point'), 'consider') (('it', 'C'), 'see') (('Points Support Circled Vectors', 'margin'), 'use') (('Lines Soft Dotted Margin', 'trade'), 'be') (('too how model', 'gender'), 'try') (('which', 'only 7 out of times when such dataset'), 'tend') (('which', 'Fundamental frequency'), 'be') (('hyperplane', '20 dimentional space'), 'have') (('Step', 'columns meanfun only two sp'), 'consideree') (('table', 'C'), 'show') (('score normalization', 'meanfreq median Q25 Q75'), 'do') (('which', 'when other kernals'), '9933927') (('correlation', 'multicollinearity issues'), 'check') (('model', 'fitting 5'), 'see') (('which', 'almost Linear'), 'be') (('We', 'tree SVM model'), 'say') (('Polynomial which', 'linear kernal'), 'be') (('how when we', 'soft margin'), 'take') (('Modified 15 Last Oct 28 2017 Topic', 'Kaggle Introduction'), 'Started') (('which', 'lineant boundry'), 'have') (('I', 'class labels'), 'go') (('t', 'bench mark even accuracy'), 'drop') (('I', '1'), 'apply') (('explicitly it', 't well sigmoidal space'), 'know') (('we', 'dataset'), 'consider') (('dataset completely normalized behaves', 'un well normalized dataset'), 'be') (('we', 'classifier'), 'set') (('which', 'accuracy'), 'model') (('accuracy', 'less SVM'), 'be') (('5 Validating', '1'), 'data_y3_t') (('Q25 Q75 IQR', 'unit only kHz'), 'be') (('Exceptable range', '0'), 'be') (('model I', 'parameters C'), 'be') (('we', '0'), 'obtain') (('that', 'complex classifier'), 'set') (('we', 'further few ensemble learnings'), 'let') (('we', 'almost 0'), 'obtain') (('very SVM', 'polynomial kernal behaves'), 'seem') (('Gaussian kernal tradeoff', 'gamma parameter'), 'do') (('we', '5'), 'be') (('we', 'meanfreq median'), 'apply') (('which', 'it'), 'meanfun') (('Performance', 'also linear kernal SVM'), 'be') (('naive which', 'target table'), 'be') (('ent sfm centroid meanfun', 'Normally 4'), 'distribute') (('it', 'abov table'), 'be') (('i', 'sp'), '52') (('that', 'gender'), 'project') (('This', 'times'), 'normalize') (('times ther', '1000'), 'mean') (('columns', 'Data Partition Inference'), 'apply') (('quite again that', 'target variable'), 'be') (('Step', 'Raw Data Naive Bayes Inference'), 'set') (('linear kernal SVM', '0'), 'see') (('unit', '3'), 'let') (('also points', 'RBF kernal'), 'be') (('them', '2'), 'treat') (('IQR skew', 'right 1'), 'be') (('I', 'hyperparameter'), 'like') (('Variables sd median Q25', 'normally 1'), 'meanfreq') (('target variables', 'typr str'), 'seem') (('SVM', 'sklearn'), 'be') (('we', 'result'), 'conclude') (('Comparing', 'classifier individual Inference'), 'result') (('which', 'SVM when classifier'), '9894') (('Target variable', 'pack n labelencoder object'), 'convert') (('bayes data Naive tretment', '0'), 'classifier') (('linear kernal', 'well consistently slight gaussian kernal'), 'see') (('accuracy', 'decision tree'), 'see') (('993', 'other classifiers'), 'be') (('below parameters', 'female case'), 'use') (('it', 'accuracy'), 'consider') (('it', 'overfitting'), 'be') (('which', 'target variable'), 'trade') (('which', 'SVM above linear model'), 'subject') (('which', 'completely normalized better raw treated dataset'), 'see') (('I', 'C'), 'model') (('we', 'domain level expertise'), 'visualize') (('it', '2 dimentional visualization'), 'try') (('hence we', '0'), '255khz') (('I', 'independent variables'), 'have') (('it', '3'), 'include') (('adult typical male', '165'), 'say') (('dataset penality parameter Completely normalized C', 'dataset'), 'inherit') (('I', 'gamma'), 'experiment') (('Lets', 'visualization'), 'explain') (('model', '0'), 'exhibit') (('accuracy', 'Kernal model CV Accuracy ROC final SVM measure'), 'linspace') (('Linear SVM clearly Kernal', 'datset completely normalized behaves'), 'be') (('However accuracy', '0'), 'touch') (('9', 'VIF heavily respective factor'), 'consider') (('which', 'above two variable'), 'use') (('best classification', 'Male'), 'infer') (('classifier', 'data cleaning'), 'discard') (('strong', 'decision boundries'), 'represent') (('i', '2 dimentional space'), 'like') (('independent variables', 'dataset'), 'see') (('that', 'biological fact'), 'obtain') (('we', 'dataset'), 'use') (('rbf', '0'), 'see') (('min max normalization', 'IQR variable'), 'do') (('different how margin', 'classification accuracy'), 'be') (('evident', '3'), 'be') (('i', '0'), 'obtain') (('values', 'meanfun'), 'filter') (('Q75', 'normally 2'), 'say') (('Thus I', 'data raw a.'), 'have') (('here it', 'equal 0'), 'clean') (('it', 'domain knowledge'), '255khz') (('Step', 'grid search Inference'), 'build') (('it', 'margins'), 'ent') (('i', '0'), 'try') (('which', 'dataset'), 'set') (('variables', 'maxfun meandom'), 'skewed') (('However vizualizing', 'more than two dimention'), 'be') (('which', 'maximum margin'), 'be') (('i', 'polynomial kernal'), 'acheive') (('this', 'sample test set'), 'accept') (('However we', 'further ensemble techniques'), 'try') (('SVM how boundy', '2D space'), 'be') (('independent variables', 'nature'), 'be') (('Step', '5 nearest neighbors'), 'build') (('it', 'feature space'), 'ent') (('mean values', 'almost gaussian distribution'), 'have') (('which', 'gender label'), 'plot') (('variables', 'Male Step'), 'be') (('I', 'above degree'), 'experiment') ", "extra": "['gender', 'test', 'bag']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "apply", "associated", "attribute", "avg", "bagging", "benchmark", "best", "binary", "boosting", "boundary", "build", "calculate", "case", "categorical", "category", "cause", "centroid", "check", "choose", "classification", "classifier", "classify", "clean", "cleaning", "clear", "column", "compare", "computation", "compute", "consider", "correlation", "could", "count", "create", "data", "dataframe", "dataset", "decision", "default", "degree", "dependent", "dimension", "distributed", "distribution", "domain", "drop", "encoding", "ensemble", "ensure", "equal", "error", "evaluate", "even", "experiment", "explained", "fact", "factor", "feature", "figure", "filter", "final", "find", "fit", "fitting", "fold", "forest", "form", "forward", "freq", "frequency", "function", "gamma", "gaussian", "gender", "graph", "grid", "handle", "help", "high", "histogram", "http", "human", "hyperparameter", "hyperplane", "implementation", "import", "improve", "include", "increase", "index", "individual", "inference", "instance", "kernel", "knowledge", "label", "learning", "let", "level", "linear", "main", "male", "margin", "match", "matrix", "max", "maximum", "mean", "measure", "median", "method", "might", "min", "minimize", "missing", "mode", "model", "most", "multicollinearity", "multiple", "my", "naive", "nature", "nearest", "need", "negative", "new", "next", "no", "non", "none", "norm", "normalization", "normalize", "normalized", "not", "notebook", "numeric", "object", "objective", "order", "out", "outlier", "overfit", "pack", "package", "parameter", "part", "per", "perform", "performance", "performing", "plane", "plot", "plotting", "point", "positive", "power", "preprocessing", "purpose", "random", "range", "raw", "read", "regularization", "relation", "remove", "result", "right", "robust", "sample", "scatter", "score", "search", "session", "set", "single", "size", "skew", "sklearn", "soft", "space", "split", "stage", "standard", "standardization", "start", "str", "structure", "subject", "summary", "supervised", "support", "svm", "table", "target", "test", "testing", "theory", "those", "through", "time", "title", "train", "training", "treatment", "tree", "try", "tune", "tuning", "type", "unit", "up", "valid", "validate", "validation", "value", "variable", "visualization", "visualize", "viz", "work"], "potential_description_queries_len": 223, "potential_script_queries": ["auc", "boosting", "classification", "decision", "ensemble", "grid", "matplotlib", "numpy", "plot", "plt", "pyplot", "search", "split", "svm", "train", "validation"], "potential_script_queries_len": 16, "potential_entities_queries": ["data", "even", "gaussian", "level", "max", "model", "new", "out", "plot", "search", "test"], "potential_entities_queries_len": 11, "potential_extra_queries": ["bag"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 226}