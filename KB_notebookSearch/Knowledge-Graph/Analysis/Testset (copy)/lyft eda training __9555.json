{"name": "lyft eda training ", "full_name": " h1 Overview h1 Data Format h1 l5kit h2 Data h1 Visualization h3 Raw Data h3 Data Abstraction h1 PyTorch Training h3 Model h3 Training Loop h1 Inference h1 Training Parameter EDA h3 Raster Size h3 Pixel Size h3 Changing Raster Size and Pixel Size h1 Augmentation h1 To be continued ", "stargazers_count": 0, "forks_count": 0, "description": "We can then use the EgoDataset and AgentDataset objects in l5kit to iterate over the rasterizer object we just defined to return RGB images. This folder should contain subfolders for the aerial and semantic maps as well as the. zarr file into a ChunkedDataset object2. This is yet another import training parameter to experiment with especially when we are feeding these rasterized images to a CNN If pixel_size. 5 we ought to expect a mean error of. zarr file and store it as a ChunkedDataset object. com tuckerarrants lyft inference resnet50 edit but I included the general procedure here for completeness Here you upload your freshly trained model Now we get our predictions and save them as a csv file with write_pred_csv Training Parameter EDA Let s see which parameters in our configuration file are relevant to training Let s explore two important training specific parameters raster_size and pixel_size Raster Size We can see that different raster sizes affect how much of a scene is rasterized meaning that our model sees different agents depending on the raster size. Note that only 4 of the 17 possible agent label_probabilities are present in this dataset. com lyft l5kit tree master examples visualisation Raw Data. The EgoDataset iterates only over the AV annotations and the AgentDataset iterates everything but the AV annotations. We can combat this by using a smaller pixel_size Changing Raster Size and Pixel Size If you want the model to see more of a scene you can change either raster_size or pixel_size. 2 Forward pass from https www. Luckily this is exactly how our data is set up in this Kaggle kernel so all we need to do is this Now we can iteract with l5kit via the below configuration. zarr file contains a set of scenes driving episodes acquired from a given vehicle. npz masks out any test object for which predictions are NOT required. Pass AgentDataset into a torch Dataloader Let s do this now Model We can create a simple model to feed this DataLoader to. com c lyft motion prediction autonomous vehicles discussion 178323 Augmentation It seems there is a large gap between training loss and validation. com nxrprime lyft understanding the data and eda Pretty cool right We are able to follow our AV as it traverses threw this scene PyTorch Training Main source https github. zarr files is fine but l5kit also provides abstract data classes to generate inputs and targets with ease. Among other things this library allows us to predict the future movement of cars based on historical observations. zarr files of interest are in train_data_loader key Visualization Main source https github. So this is the value to tweak if you want to change the resolution of the image. Overview Most of the contents of this notebook can be found on the l5kit GitHub repository in the examples folder here https github. This is a powerful hyper parameter should our model focus on agents far away Or should it focus on agents closer to it Pixel Size From the l5kit GitHub repository the pixel size of the raster image is the raster s spatial resolution meters per pixel the size in the real world one pixel corresponds to. com corochann lyft training with multi mode confidence convert to batch_size num_modes future_len num_coords add modes add modes and cords error batch_size num_modes future_len reduce coords and use availability when confidence is 0 log goes to inf but we re fine with it error batch_size num_modes reduce time use max aggregator on modes for numerical stability error batch_size num_modes error are negative at this point so max gives the minimum one reduce modes print error error pred bs x time x 2D coords bs x mode 1 x time x 2D coords create confidence bs x mode 1 Backward pass save model during training display training progress Rasterizer Test dataset dataloader Build Rasterizer reset to default reset to default reset to default for augmentations for better visualization get a random sample. target_position into pixel coordinates and then call draw_trajectory which can be used to draw predicted trajectories as well. In particular l5kit allows us to Load driving scenes from zarr files Read aerial and semantic maps Create birds eye view images that repesent a scene around an automonous vehicle other vehicles And most importantly train neural networks and visualize their results Let s add l5kit now and start to explore how we can use it for the task at hand. l5kit uses PyTorch DataLoaders so we will use albumentations to compose these augmentations. zarr file for training we follow the below procedure 1. zarr file to create a scatter plot of all the AV henceforth AV locations like so I have this d out because it takes around 40 minutes to complete and is not nearly as insightful as the below visualizations will be Data Abstraction Working with raw. This is largely because some images are very similar e. We also need to change the output size to X Y number of future states Update see discussion post here https www. The below is a ResNet18 pre trained on ImagNet taken from Peter https www. agents_mask a mask that for train and validation masks out objects that aren t useful for training. The following code is again taken from Peter https www. com lyft l5kit blob master examples. It will become clear how to manipulate this cfg as we go through the notebook This configuration file is used to load the data by using a LocalDataManager object which resolves relative paths from the configuration file using the L5KIT_DATA_FOLDER env that we set earlier to extract the. com pestipeti pytorch baseline inference Since I have internet enabled in this notebook which is not allowed for submission notebooks in this comp I carried out the prediction part in this notebook https www. We can add some weather augmentations which doesn t make much sense to do with the py_semantic raster images but I guess one could use it for py_satellite raster images. We will define our optimizer and metric here as well Training Loop Once you are happy with your training you can save the model state to your disk and upload the pre trained weights offline for an inference submission using torch. the frames of the agent vehicle at a red light are much the same in comparison to frames in which it is moving at full speed. Now this probably isn t that useful but it is cool so I will demo it anyway. When looking at the above configuration dictionary we see that the. For a more thorough explanation see Peter s comment here https www. Let s first visualize the AV using an EgoDataset Once in an EgoDataset we can plot the ground truth trajectory by converting the EgoDataset. zarr files and processes them into rectangular grid of pixels a raster image so that we can view them regularly on a computer screen. com ilu000 expected error inaccuracy from rasterization you ll see that there is some inherent innacuracy from the rasterization process defined by pixel_size. 5 4 for each direction of each predicted position. Note that as per the competition rules internet is not allowed in this competition so we cannot use pip to install packages. zarr files the scenes. Taken from the competition Data description each. We need to change the input channels of the ResNet to match the rasterizer output. It is a great place to start for this competition. Data Format The data in this competition is packed into. This is why Lyft has provided us the l5kit module as a utility script called kaggle_l5kit. traffic_light_faces traffic light information. com pestipeti s notebook here https www. com pestipeti pytorch baseline train. For example we can iterate over the frames in our. More importantly if you want the same scene in a higher resolution you need to change both raster_size and pixel_size. We can change the rasterizer by building a new one and an accomodating dataset for it like so And now we can do the exact same thing to view the agents by switching from an EgoDataset to an AgentDataset The below animation code is taken from here https www. Wrap ChunkedDataset object into an AgentDataset which inhereits from the PyTorch Dataset class3. frames snapshots in time of the pose of the vehicle. zarr files which can be loaded with the Python zarr module. com c lyft motion prediction autonomous vehicles discussion 186492 for explanation of new architecture and training loop changes We can speed up our training by placing our computations on the GPU with the torch. Most importantly we can use a rasterizer which takes these chunked. l5kit l5kit is a Python library developed by Lyft Level 5 with functionality for developing and training learned prediction planning and simulation models for autonomous driving applications. com lyft l5kit tree master examples agent_motion_prediction And now we are ready to explore how to train a neural network with this. 5 this means that we are at most able to capture objects no less than half a meter in size if the object is smaller than half a meter it will not be detected as a pixel so to view these objects we need a higher resolution lower pixel_size As you can read about in this notebook https www. We can reduce this similarity between frames by introducing augmentations like CutOut CourseDropout and more. Data Data is expected to live in the folder that we set using the L5KIT_DATA_FOLD env variable. save Inference And now after all that work we can predict with this baseline model. In test the mask provided in files as mask. agents a generic entity captured by the vehicle s sensors. Each history position lane and other agents are encoded into the pixels and if the raster has pixel_size. zarr files support most traditional NumPy array operations. python basics for deep learning for scene visualization set env variable for data let s see what one of the objects looks like for idx_coord idx_data in enumerate tqdm range len frames desc getting centroid to plot trajectory X Y coords for the future positions output shape batch_sizex50x2 You can add more layers here. ", "id": "tuckerarrants/lyft-eda-training", "size": "9555", "language": "python", "html_url": "https://www.kaggle.com/code/tuckerarrants/lyft-eda-training", "git_url": "https://www.kaggle.com/code/tuckerarrants/lyft-eda-training", "script": "torch.utils.data animation __init__ display albumentations rc torch ChunkedDataset LyftModel(nn.Module) l5kit.visualization optim pyplot as plt torchvision.models.resnet TARGET_POINTS_COLOR pytorch_neg_multi_log_likelihood_single l5kit.evaluation DataLoader Dict forward draw_trajectory pyplot numpy AgentDataset animate_solution write_pred_csv show_images typing nn sample_dataset clear_output Tensor l5kit.dataset run_prediction build_rasterizer EgoDataset tqdm animate LocalDataManager l5kit.geometry resnet18 pytorch_neg_multi_log_likelihood_batch matplotlib transform_points l5kit.data l5kit.rasterization IPython.display HTML ", "entities": "(('which', 'predicted trajectories'), 'target_position') (('which', 'chunked'), 'use') (('lyft l5kit tree master examples', 'Raw Data'), 'com') (('us', 'historical observations'), 'allow') (('zarr files', 'Visualization Main source https train_data_loader key github'), 'be') (('so I', 'it'), 't') (('Update', 'discussion post'), 'need') (('you', 'pixel_size'), 'com') (('com lyft motion vehicles prediction autonomous 178323 It', 'training large loss'), 'c') (('Rasterizer Test dataset dataloader Build Rasterizer', 'random sample'), 'training') (('you', 'torch'), 'define') (('import training yet especially when we', 'CNN'), 'be') (('l5kit', 'ease'), 'be') (('EgoDataset', 'AV annotations'), 'iterate') (('now how we', 'hand'), 'allow') (('We', 'CutOut CourseDropout'), 'reduce') (('we', 'packages'), 'note') (('we', 'our'), 'iterate') (('folder', 'aerial maps'), 'contain') (('Most', 'examples https folder here github'), 'overview') (('following code', 'Peter https again www'), 'take') (('it', 'full speed'), 'be') (('mask', 'mask'), 'provide') (('It', 'great competition'), 'be') (('Now we', 'below configuration'), 'be') (('You', 'more layers'), 'set') (('one pixel', 'real world'), 'be') (('now after we', 'baseline model'), 'save') (('we', 'earlier the'), 'become') (('you', 'image'), 'be') (('one', 'raster images'), 'add') (('you', 'raster_size'), 'combat') (('model', 'raster size'), 'edit') (('below', 'Peter https www'), 'be') (('we', 'L5KIT_DATA_FOLD env variable'), 'expect') (('why Lyft', 'utility script'), 'be') (('lyft l5kit tree master now we', 'this'), 'com') (('you', 'notebook https www'), '5') (('predictions', 'which'), 'mask') (('we', 'augmentations'), 'use') (('zarr file', 'given vehicle'), 'contain') (('only 4', 'dataset'), 'note') (('I', 'notebook https www'), 'enable') (('Data data', 'competition'), 'format') (('we', 'the'), 'dictionary') (('you', 'raster_size'), 'need') (('animation below code', 'https www'), 'change') (('raster', 'other pixels'), 'encode') (('zarr which', 'Python zarr module'), 'file') (('we', 'EgoDataset'), 'let') (('zarr files', 'NumPy array most traditional operations'), 'support') (('which', 'PyTorch Dataset'), 'object') (('we', 'computer regularly screen'), 'file') (('training loop We', 'torch'), 'discussion') (('l5kit l5kit', 'driving autonomous applications'), 'be') (('now Model We', 'DataLoader'), 'pass') (('it', 'Data raw'), 'file') (('We', 'rasterizer output'), 'need') (('we', 'RGB just images'), 'use') (('that', 'useful training'), 'agents_mask') (('it', 'PyTorch Training source https Main github'), 'com') (('we', 'below procedure'), 'file') ", "extra": "['annotation', 'test', 'procedure']", "label": "Perfect_files", "potential_description_queries": ["abstract", "agent", "animation", "architecture", "array", "baseline", "batch_size", "become", "blob", "call", "centroid", "cfg", "clear", "code", "comment", "comp", "comparison", "competition", "computer", "confidence", "contain", "convert", "could", "create", "csv", "data", "dataset", "default", "define", "description", "detected", "dictionary", "direction", "disk", "display", "draw", "eda", "entity", "enumerate", "error", "everything", "expected", "experiment", "explore", "extract", "eye", "feed", "file", "folder", "following", "found", "future", "gap", "general", "generate", "generic", "grid", "ground", "half", "history", "image", "import", "inference", "input", "interest", "kernel", "key", "l5kit", "learning", "len", "let", "library", "light", "load", "log", "looking", "loop", "lower", "lyft", "mask", "match", "max", "mean", "meaning", "metric", "minimum", "mode", "model", "module", "most", "motion", "need", "negative", "network", "neural", "new", "no", "not", "notebook", "npz", "number", "numerical", "object", "offline", "optimizer", "out", "output", "parameter", "part", "per", "pixel", "place", "plot", "point", "position", "post", "pre", "pred", "predict", "prediction", "present", "print", "procedure", "python", "pytorch", "random", "range", "rasterization", "rasterizer", "re", "read", "reduce", "relative", "repository", "reset", "resolution", "return", "right", "save", "scatter", "scene", "script", "sense", "set", "shape", "similar", "similarity", "simulation", "size", "source", "spatial", "speed", "start", "state", "store", "submission", "support", "task", "test", "through", "time", "torch", "tqdm", "train", "training", "trajectory", "tree", "understanding", "up", "validation", "value", "variable", "view", "visualization", "visualize", "work", "world"], "potential_description_queries_len": 177, "potential_script_queries": ["animate", "display", "forward", "matplotlib", "nn", "numpy", "optim", "plt", "pyplot", "rc", "resnet18", "tqdm"], "potential_script_queries_len": 12, "potential_entities_queries": ["dataset", "key", "loop", "most", "prediction", "training"], "potential_entities_queries_len": 6, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 187}