{"name": "complete titanic tutorial with ml nn ensembling ", "full_name": " h1 Complete Titanic tutorial with ML NN Ensembling h3 Welcome on this Titanic tutorial It is aimed for beginners but whatever your level you could read it and if you find a way to improve it I encourage you to fork this notebook and contribute by adding a better solution h3 In this notebook we are going to predict wether a passenger of the famous boat will survive or not By doing this we will go through several topics and fundamental techniques of machine learning Here is a list of these techniques and some additional resources that you can consult to find out more h2 Table of Contents h2 Imports and useful functions h2 1 Data exploration h3 There is 77 of missing data in the cabin column it s usually way too much for this column to be exploitable but as we have a small amount of data we will still try to use it in feature engineering h3 For the age we will either interpolate missing values or we will fill it with the mean for the corresponding category in term of class age sex of passenger h3 There is only two missing values for the embarked column let s try to replace it Below is the distribution of Embarked according to Fare and sex and the two observations with missing Embarked value Let s look at there two observations and choose the best matching embarked value according to their fare value and sex h3 Both passengers are female who paid 80 dollars as fare for their tickets Moreover they have the same ticket and cabin so they probably had to board at the same place According to the distribution above the more probable embarked value for them is Cherbourg C We ll replace these two missing values later during features engineering part h3 Finally let s plot some histograms to visualise the distributions of our variables h3 With this first exploration we can see that h2 2 Features engineering h3 My advice is to group all the transformations to be done on the dataset in a single function This way you can apply the same changes to the training dataset and the test dataset easily Moreover if you want to add a modification you ll have to do it only in the function h2 3 Try several models h3 Introduction to metrics h3 To evaluate our models on the test set for this classification problem we are going to use several metrics which will be displayed into a confusion matrix to easily see the false positive and the false negative predicted by the model i e respectivey type I II errors From those two types of error some metrics can be computed the F1 score the Recall the accuracy You can find on the image below a quick summary of what is a confusion matrix how to read it and what are those metrics h3 3 1 Logistic regression h3 Logistic regression is the hello world of machine learning algorithms It is very simple to understand how it works here is a good article which cover theory of this algorithm h3 3 2 Decision tree h3 Decision tree is a quite intuitive model easy to vizualize and interpret Here we are even going to display our tree to improve our understanding on how the algorithm manage to classify our samples h3 To find out more about decision trees DT h3 3 3 SVM h3 SVMs aim at solving classification problems by finding good decision boundaries between two sets of points belonging to two different categories To understand how it works you can refer to this webpage h3 3 4 Random forest h3 Random forest is a robust practical algorithm based on decision trees It outperforms almost always the two previous algorithm we saw If you want to find out more about this model here is a good start h3 3 5 Artificial neural network h3 Neural networks are more complex and more powerful algorithm than standars machine learning it belongs to deep learning models To build a neural network we are going to use Keras Keras is a high level API for tensorflow which is a tensor manipulation framework made by google Keras allows you to build neural networks by assembling blocks which are the layers of our neural network For more details here is a great keras tutorial h3 To understand better the above implemented architecture and dive deeply that s the case to say in this exciting field I highly recommend you to read the great book Deep Learning with Python link at the beginning of the kernel written by Fran\u00e7ois Chollet the creator of the keras framework h2 4 Finding the best model using k folds cross validation h3 The precision we calculated above for those 4 different models does not mean anything In fact if we execute each cell again we could have sightly different accuracy because we trained again our models We need to verify which model has the best accuracy over several training steps We can do it using cross validation method which consists of dividing out training set in k parts folds and evaluating k times using successively each part as the test set and the 9 other parts as the training set Therefore we can compute a mean error over the 10 trainings of our model h3 Let s check which one of our previously implemented model is the best one with this method We will not only compute the mean but also the variance because a good model needs to have the lowest possible variance in addition to have a low bias h3 All models seems to have a good accuracy and nearly the same variance it seems that there is no best model Indeed there is no one model which seems truly better than the other In fact if we make submissions with all of these models we will obtain approximately the same score Moreover the variance is a little bit too high for saying that these models are reliable 0 05 variance means that the same model can score 0 75 and 0 8 which is not very convenient h3 To obtain a better score we will in the next part build our own classifier which will combine predictions from a random forest an svm classifier and a keras neural networks The diversity from these 3 very different models will increase the quality of our predictions and reduce the variance h2 5 Ensembling creating a homemade classifier h3 Ensembling is the science of combining classifiers to improve the accuracy of a models Moreover it diminushes the variance of our model making it more reliable You can start learning about ensembling here h3 We are going to make our own classifier To do that we ll create a class with two methods fit predict just as the other classifiers we used from sckitlearn and keras In the fit method we just train our 3 classifiers on training data In the predict method we make a prediction with each of the 3 classifier and combine it if two or more classifiers classified the passenger as a survivor our homemade EsembleClassifier classify it as survivor Else it ll predict that the passenger did not survived h3 Our new model seems to be quite performing You can try to train and validate it several time on train test split you ll see that the variance is not high so our model is also quite constant in its performances h3 Let s try this new model on the test dataset now h2 6 Apply our homemade model on test dataset and submit on kaggle h2 7 Results h3 We are not done yet What about the results I ve tried to make 30 submissions with this classifier here are the results h3 Although I know that we can do much better the 0 79904 still places us in the top 16 On the other hand we see that our homemade classifier has reduced the variance so it is reliable and constant in its performance something that is not tested by the leaderboard but still important for a data sceintist Moreover our solution remains simple and accessible even for beginners h3 Thank you for your reading feel free to fork this kernel and improve it and enjoy datascience D ", "stargazers_count": 0, "forks_count": 0, "description": "Moreover the variance is a little bit too high for saying that these models are reliable 0. Therefore we can compute a mean error over the 10 trainings of our model https www. 8 which is not very convenient. 77511 2 0. In the predict method we make a prediction with each of the 3 classifier and combine it if two or more classifiers classified the passenger as a survivor our homemade EsembleClassifier classify it as survivor. Here we are even going to display our tree to improve our understanding on how the algorithm manage to classify our samples To find out more about decision trees DT https medium. There is only two missing values for the embarked column let s try to replace it. We will not only compute the mean but also the variance because a good model needs to have the lowest possible variance in addition to have a low bias https www. Here is a list of these techniques and some additional resources that you can consult to find out more EDA Data exploration https medium. Try several models trymodels 4. In the fit method we just train our 3 classifiers on training data. On the other hand we see that our homemade classifier has reduced the variance so it is reliable and constant in its performance something that is not tested by the leaderboard but still important for a data sceintist. 79425 2 0. To build a neural network we are going to use Keras. net profile Kiret_Dhindsa publication 323969239 figure fig10 AS 607404244873216 1521827865007 The K fold cross validation scheme 133 Each of the K partitions is used as a test. The diversity from these 3 very different models will increase the quality of our predictions and reduce the variance 5. com keras tutorial deep learning in python Deep Learning with python https www. It is very simple to understand how it works here https towardsdatascience. We ll replace these two missing values later during features engineering part. Scaling means that each columns as a 0 mean and a 1 variance Split dataset for model testing Create and train model on train data sample Predict for test data sample Compute error between predicted data and true response and display it in confusion matrix Initializing our ANN Adding the input layer and the first hidden layer of our ANN with dropout Add other layers it is not necessary to pass the shape because there is a layer before Adding the output layer Compiling the ANN Training the ANN Predicting the Test set results convert probabilities to binary output Compute error between predicted data and true response and display it in confusion matrix Create and train model on train data sample Predict for test data sample. Our new model seems to be quite performing You can try to train and validate it several time on train_test_split you ll see that the variance is not high so our model is also quite constant in its performances. jpg cb 1440698161 3. Indeed there is no one model which seems truly better than the other. Results results Imports and useful functions 1. machinelearningtutorial. You can start learning about ensembling here https towardsdatascience. Keras allows you to build neural networks by assembling blocks which are the layers of our neural network. Ensembling creating a homemade classifier Ensembling is the science of combining classifiers to improve the accuracy of a models. Finally let s plot some histograms to visualise the distributions of our variables With this first exploration we can see that Only aproximately 35 of passengers survived. respectivey type I II errors. By doing this we will go through several topics and fundamental techniques of machine learning. Moreover if you want to add a modification you ll have to do it only in the function 3. com williamkoehrsen random forest simple explanation 377895a60d2d is a good start. More than the half of passengers are in the lowest class pclass 3 Most of the fare tickets are below 50 Majority of passengers are alone sibsp and parch Note this EDA is not complete at all since it is not the purpose of this kernel to make a deep exploration of data. Data exploration There is 77 of missing data in the cabin column it s usually way too much for this column to be exploitable but as we have a small amount of data we will still try to use it in feature engineering. 3 SVM SVMs aim at solving classification problems by finding good decision boundaries between two sets of points belonging to two different categories. To understand better the above implemented architecture and dive deeply that s the case to say in this exciting field I highly recommend you to read the great book Deep Learning with Python link at the beginning of the kernel written by Fran\u00e7ois Chollet the creator of the keras framework. If you want to find out more about this model here https medium. Finding the best model using k folds cross validation The precision we calculated above for those 4 different models does not mean anything. 05 variance means that the same model can score 0. Below is the distribution of Embarked according to Fare and sex and the two observations with missing Embarked value. Let s look at there two observations and choose the best matching embarked value according to their fare value and sex Both passengers are female who paid 80 dollars as fare for their tickets. com books deep learning with python Ensembling https mlwave. To obtain a better score we will in the next part build our own classifier which will combine predictions from a random forest an svm classifier and a keras neural networks. Choosing the best model choose 5. 4 Random forest Random forest is a robust practical algorithm based on decision trees. 5 Artificial neural network Neural networks are more complex and more powerful algorithm than standars machine learning it belongs to deep learning models. Moreover our solution remains simple and accessible even for beginners. Moreover it diminushes the variance of our model making it more reliable. net wp content uploads 2017 01 bias variance tradeoff. 78468 10 0. jpg In this notebook we are going to predict wether a passenger of the famous boat will survive or not. Else it ll predict that the passenger did not survived. Features engineering My advice is to group all the transformations to be done on the dataset in a single function. com qconrio machinelearningforeveryone 150826200704 lva1 app6892 95 qcon rio machine learning for everyone 51 638. com chiragsehra42 decision trees explained easily 28f23241248 title dt. As we saw before the two missing values for embarked columns can be replaced by C Cherbourg We replace missing ages by the mean age of passengers who belong to the same group of class sex family We replace the only missing fare value for test dataset and the missing values of the cabin column Create a Title column from name column Filling Age missing values with mean age of passengers who have the same title Transform categorical variables to numeric variables Create a Family Size Is Alone Child and Mother columns Modification of cabin column to keep only the letter contained corresponding to the deck of the boat Create a ticket survivor column which is set to 1 if an other passenger with the same ticket survived and 0 else Note this implementation is ugly and unefficient if sombody found a way to do it easily with pandas it must be a way please comment the kernel with your solution These two columns are not useful anymore Let s divide the train dataset in two datasets to evaluate perfomance of the machine learning models we ll use We scale our data it is essential for a smooth working of the models. Try several models Introduction to metrics To evaluate our models on the test set for this classification problem we are going to use several metrics which will be displayed into a confusion matrix to easily see the false positive and the false negative predicted by the model i. com metrics evaluate machine learning algorithms python Evaluating a model over several trainings k fold cross validation https towardsdatascience. In fact if we execute each cell again we could have sightly different accuracy because we trained again our models We need to verify which model has the best accuracy over several training steps We can do it using cross validation method which consists of dividing out training set in k parts folds and evaluating k times using successively each part as the test set and the 9 other parts as the training set. Apply our homemade model on test dataset and submit on kaggle 7. com machine learning comprehensive guide feature engineering Evaluating a model over one training metrics https machinelearningmastery. 77990 3 0. 78947 11 0. Data exploration data_exploration 2. 79904 2 Although I know that we can do much better the 0. 1 Logistic regression Logistic regression is the hello world of machine learning algorithms. To understand how it works you can refer to this webpage https www. com kaggle ensembling guide Table of Contents 1. For more details here https elitedatascience. com nhlr21 titanic colorful eda for this competition if this interests you. You can find on the image below a quick summary of what is a confusion matrix how to read it and what are those metrics https image. Submission submission 7. svg All models seems to have a good accuracy and nearly the same variance it seems that there is no best model. This way you can apply the same changes to the training dataset and the test dataset easily. png Let s check which one of our previously implemented model is the best one with this method. com keras tutorial deep learning in python is a great keras tutorial. From those two types of error some metrics can be computed the F1 score the Recall the accuracy. 2 Decision tree Decision tree is a quite intuitive model easy to vizualize and interpret. However you can look at my EDA kernel https www. Keras is a high level API for tensorflow which is a tensor manipulation framework made by google. com train test split and cross validation in python 80b61beca4b6 Neural network with keras https elitedatascience. Thank you for your reading feel free to fork this kernel and improve it and enjoy datascience D Check out this beautiful distribution of kaggle scoring for our homemade classifier Ignore warnings Some useful functions we ll use in this notebook Path of datasets Create dataframe for training dataset and print five first rows as preview Compute some basical statistics on the dataset Let s plot some histograms to have a previzualisation of some of the data. Let s try this new model on the test dataset now 6. In fact if we make submissions with all of these models we will obtain approximately the same score. com python pandemonium introduction to exploratory data analysis in python 8b6bcb55c190 Features engineering https adataanalyst. Feature egineering fe 3. For the age we will either interpolate missing values or we will fill it with the mean for the corresponding category in term of class age sex of passenger. com two is better than one ensembling models 611ee4fa9bd8 We are going to make our own classifier. Moreover they have the same ticket and cabin so they probably had to board at the same place According to the distribution above the more probable embarked value for them is Cherbourg C. Esembling Homemade classifier ensembling 6. 79904 still places us in the top 16. com vi yuMNWt6S0ZA maxresdefault. Results We are not done yet What about the results I ve tried to make 30 submissions with this classifier here are the results Score Nb of occurrences 0. com understanding logistic regression 9b02c2aec102 is a good article which cover theory of this algorithm. com 2014 11 svm understanding math part 1. To do that we ll create a class with two methods fit predict just as the other classifiers we used from sckitlearn and keras. Complete Titanic tutorial with ML NN Ensembling Welcome on this Titanic tutorial It is aimed for beginners but whatever your level you could read it and if you find a way to improve it I encourage you to fork this notebook and contribute by adding a better solution https i. It outperforms almost always the two previous algorithm we saw. ", "id": "nhlr21/complete-titanic-tutorial-with-ml-nn-ensembling", "size": "11098", "language": "python", "html_url": "https://www.kaggle.com/code/nhlr21/complete-titanic-tutorial-with-ml-nn-ensembling", "git_url": "https://www.kaggle.com/code/nhlr21/complete-titanic-tutorial-with-ml-nn-ensembling", "script": "__init__ cross_val_score display sklearn.tree keras.layers predict Dropout Sequential DecisionTreeClassifier seaborn numpy display_confusion_matrix draw_missing_data_table scipy.stats optimizers EsemblingClassifier sklearn.ensemble sklearn metrics sklearn.model_selection build_ann RandomForestClassifier matplotlib.pyplot Dense pandas StandardScaler LogisticRegression fit NotFittedError KerasClassifier export_graphviz SVC sklearn.linear_model sklearn.preprocessing visualize_tree sklearn.svm keras sklearn.exceptions keras.models preprocess_data keras.wrappers.scikit_learn train_test_split IPython.display ", "entities": "(('However you', 'EDA kernel https www'), 'look') (('you', 'only function'), 'moreover') (('way you', 'training dataset'), 'apply') (('Ensembling', 'models'), 'be') (('which', 'svm classifier'), 'build') (('you', 'EDA Data exploration https more medium'), 'be') (('advice', 'single function'), 'feature') (('this', 'you'), 'com') (('s', 'data'), 'thank') (('metrics https image', 'confusion how it'), 'find') (('s', 'test dataset'), 'let') (('133 Each', 'test'), 'profile') (('You', 'https here towardsdatascience'), 'start') (('we', 'anything'), 'find') (('which', 'training 9 other set'), 'have') (('Results', 'Imports functions'), 'result') (('we', 'almost always two previous algorithm'), 'outperform') (('nearly same it', 'good accuracy'), 'seem') (('we', 'training data'), 'train') (('passenger', 'famous boat'), 'jpg') (('I', 'solution https better i.'), 'aim') (('same model', '0'), 'mean') (('which', 'neural network'), 'allow') (('Moreover solution', 'even beginners'), 'remain') (('you', 'webpage https www'), 'refer') (('homemade EsembleClassifier', 'survivor'), 'make') (('I', 'keras framework'), 's') (('how algorithm', 'decision trees DT https medium'), 'go') (('SVM 3 SVMs', 'two different categories'), 'aim') (('they', 'them'), 'have') (('which', 'tensor manipulation google'), 'be') (('we', 'passenger'), 'interpolate') (('we', 'sckitlearn'), 'predict') (('also good model', 'bias https low www'), 'compute') (('Logistic regression Logistic 1 regression', 'hello machine learning algorithms'), 'be') (('Therefore we', 'model https www'), 'compute') (('metrics', 'Recall'), 'compute') (('model', 'also quite performances'), 'seem') (('s', 'it'), 'be') (('com books', 'python Ensembling https deep mlwave'), 'learning') (('com machine', 'training metrics https over one machinelearningmastery'), 'feature') (('it', 'model'), 'diminushe') (('which', 'model false i.'), 'go') (('Below distribution', 'two Embarked value'), 'be') (('who', 'tickets'), 'let') (('you', 'model'), 'want') (('we', 'feature engineering'), 'exploration') (('diversity', 'variance'), 'increase') (('we', 'machine fundamental learning'), 'go') (('forest Random 4 Random forest', 'decision robust practical trees'), 'be') (('it', 'models'), 'see') (('wp net content', 'bias variance 2017 01 tradeoff'), 'upload') (('com train test', 'Neural keras https 80b61beca4b6 elitedatascience'), 'split') (('it', 'learning deep models'), 'be') (('Indeed one which', 'truly other'), 'be') (('ANN', 'test data sample'), 'mean') (('We', 'features engineering later part'), 'replace') (('at all it', 'data'), 'be') (('very how it', 'https here towardsdatascience'), 'be') (('Only aproximately 35', 'passengers'), 'let') (('com chiragsehra42 decision trees', 'title easily 28f23241248 dt'), 'explain') (('one', 'best method'), 'let') (('we', 'approximately same score'), 'obtain') (('We', 'own classifier'), 'be') (('good which', 'algorithm'), 'be') (('here results', 'Score occurrences'), 'result') (('com keras tutorial', 'deep python'), 'be') (('k', 'cross validation https towardsdatascience'), 'evaluate') (('we', 'Keras'), 'go') (('that', 'data still sceintist'), 'see') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "age", "algorithm", "apply", "architecture", "article", "best", "binary", "bit", "board", "boat", "book", "build", "cabin", "case", "categorical", "category", "cb", "cell", "check", "choose", "classification", "classifier", "classify", "column", "combine", "comment", "competition", "compute", "confusion", "content", "convert", "could", "create", "data", "dataframe", "dataset", "decision", "display", "distribution", "diversity", "eda", "engineering", "ensembling", "error", "evaluate", "even", "everyone", "execute", "explained", "fact", "family", "fare", "feature", "field", "figure", "fill", "find", "fit", "fold", "forest", "found", "framework", "function", "group", "half", "hand", "high", "image", "implementation", "improve", "increase", "input", "interpolate", "kaggle", "kernel", "layer", "leaderboard", "learning", "let", "letter", "level", "link", "list", "little", "look", "manage", "matching", "math", "matrix", "mean", "method", "missing", "model", "my", "name", "need", "negative", "network", "neural", "new", "next", "no", "not", "notebook", "numeric", "out", "output", "part", "passenger", "performance", "performing", "place", "plot", "png", "positive", "precision", "predict", "prediction", "print", "problem", "profile", "publication", "purpose", "python", "random", "read", "reading", "recommend", "reduce", "regression", "replace", "response", "robust", "sample", "scale", "science", "score", "scoring", "set", "several", "sex", "shape", "single", "smooth", "solution", "something", "split", "start", "submission", "summary", "survived", "svm", "tensor", "tensorflow", "term", "test", "testing", "theory", "those", "through", "ticket", "time", "titanic", "title", "train", "training", "tree", "try", "tutorial", "type", "understanding", "validate", "validation", "value", "variance", "verify", "visualise", "who", "world"], "potential_description_queries_len": 180, "potential_script_queries": ["fit", "numpy", "seaborn", "sklearn"], "potential_script_queries_len": 4, "potential_entities_queries": ["decision", "kernel", "regression", "train", "variance"], "potential_entities_queries_len": 5, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 182}