{"name": "learning from the disaster 99 accuracy ", "full_name": " h1 Table of Contents h1 How a Data Scientist Beat the Odds h1 A Data Science Framework h1 Step 1 Define the Problem h1 Step 2 Gather the Data h1 Step 3 Prepare Data for Consumption h2 3 1 Import Libraries h2 3 11 Load Data Modelling Libraries h2 3 2 Meet and Greet Data h2 3 21 The 4 C s of Data Cleaning Correcting Completing Creating and Converting h2 3 22 Clean Data h2 3 23 Convert Formats h2 3 24 Da Double Check Cleaned Data h2 3 25 Split Training and Testing Data h1 Step 4 Perform Exploratory Analysis with Statistics h1 Step 5 Model Data h3 Data Science 101 How to Choose a Machine Learning Algorithm MLA h2 5 1 Evaluate Model Performance h3 Data Science 101 Determine a Baseline Accuracy h3 Data Science 101 How to Create Your Own Model h2 5 11 Model Performance with Cross Validation CV h1 5 12 Tune Model with Hyper Parameters h2 5 13 Tune Model with Feature Selection h1 Step 6 Validate and Implement h1 Step 7 Optimize and Strategize h2 Conclusion h2 Change Log h1 Credits ", "stargazers_count": 0, "forks_count": 0, "description": "12 31 17 Completed data science framework iteration 1 and added section 7 with conclusion. It may be external or internal structured or unstructured static or streamed objective or subjective etc. The goal is to not just learn the whats but the whys. neighbors Support Vector Machines SVM http scikit learn. Most data scientist come from one of the three fields so they tend to lean towards that discipline. However just like autocorrect spellcheck technology sometimes we humans can be too smart for our own good and actually underperform a coin flip. qualitative vs quantitative is also important to understand and select the correct hypothesis test or data model. Possible to validate a model using statistical tests. Also the dead subgroup is less than 10 so we will stop. Please Note This Kernel is still being improved. Other techniques often require data normalization dummy variables need to be created and blank values to be removed. Data Science 101 How to Create Your Own Model Our accuracy is increasing but can we do better Are there any signals in our data To illustrate this we re going to build our own decision tree model because it is the easiest to conceptualize and requires simple addition and multiplication calculations. In laymen terms this means it either occurred or did not occur. html classification Some advantages of decision trees are Simple to understand and to interpret. Optimize and Strategize This is the bionic man step where you iterate back through the process to make it better. So chances are the dataset s already exist somewhere in some format. CV http blog test. So problems that once required graduate degrees in mathematics or statistics now only take a few lines of code. Otherwise our model will be overfitted. Our rule of thumb will be the majority rules. 25 Split Training and Testing DataAs mentioned previously the test file provided is really validation data for competition submission. com comparing classifiers and there is also a school of thought that says more data beats a better algorithm https www. Since our problem is predicting if a passenger survived or did not survive this is a discrete target variable. 11 25 17 Added enhancements to published notebook and started step 4. If a given situation is observable in a model the explanation for the condition is easily explained by Boolean logic. The cost of using the tree i. In short NFLT states there is no super algorithm that works best in all situations for all datasets. It should be noted that if they were unreasonable values for example age 800 instead of 80 then it s probably a safe decision to fix now. In sklearn algorithms are called Estimators and implemented in their own classes. We ll save clustering and dimension reduction for another day and focus on classification and regression. We will use sklearn s train_test_split function http scikit learn. So check the Change Logs below for updates. pdf of Machine Learning. Since these variables already exist we ll make use of it to see if title like master makes a difference. They are basically different implementations of a decision tree which is the easiest concept to learn and understand. Sklearn http scikit learn. While others like decision trees can handle null values. The author will theorize that for small datasets a manmade algorithm is the bar to beat. Giving us an accuracy of 79. jpg A Data Science Framework1. html multimetric cross validation function to train test and score our model performance. A common business application is churn or customer retention. Define the Problem If data science big data machine learning predictive analytics business intelligence or any other buzzword is the solution then what is the problem As the saying goes don t put the cart before the horse. Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. org Classes ENGR5358 Papers NFL_4_Dummies. model_selection to allow a little more randomness in our test scoring. Question 3B going down the male branch with count 577 Going back to question 2 we know the majority of males died. Data Science 101 Determine a Baseline Accuracy Before we decide how to make our model better let s determine if our model is even worth keeping. Think of MLA like a TI 89 calculator on a Calculus Exam. 12 3 17 Update section 4 with improved graphical statistics. Both are discrete quantitative datatypes. and y dependent target outcome response etc. com 2015 06 machine learning more data better algorithms. For example you won or did not win you passed the test or did not pass the test you were accepted or not accepted and you get the point. Also thanks to computer science a lot of the heavy lifting is done for you. An intermediate methodology is to use the basic methodology based on specific criteria like the average age by class or embark port by fare and SES. Correcting Reviewing the data there does not appear to be any aberrant or non acceptable data inputs. Thus they will be excluded from analysis. Accuracy with very simple data cleaning and logistic regression is 82. html Choosing Estimator Mind Map http scikit learn. The disadvantages of decision trees include Decision tree learners can create over complex trees that do not generalize the data well. DecisionTreeClassifier we accepted all the function defaults. com gp product 0470944889 ref as_li_tl ie UTF8 tag kaggle 20 camp 1789 creative 9325 linkCode as2 creativeASIN 0470944889 linkId f797da48813ed5cfc762ce5df8ef957f Learn the art and science of data visualization Machine Learning for Dummies by John Mueller and Luca Massaron https www. Step 5 Model DataData Science is a multi disciplinary field between mathematics i. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. ETL and web scraping and data cleaning to identify aberrant missing or outlier data points. With that being said some good research has been done to compare algorithms such as Caruana Niculescu Mizil 2006 https www. At worst it makes completing the project impossible. Validate and Implement Data Model After you ve trained your model based on a subset of your data it s time to test your model. We can narrow our list of choices. org stable modules cross_validation. Also we will stop if the subgroup is less than 10 and or our model accuracy plateaus or decreases. html Indexing and Selecting Data https pandas. This is important so we don t overfit our model https www. We will then visualize our tree with graphviz http scikit learn. Able to handle multi output problems. 21 The 4 C s of Data Cleaning Correcting Completing Creating and ConvertingIn this stage we will clean our data by 1 correcting aberrant values and outliers 2 completing missing information 3 creating new features for analysis and 4 converting fields to the correct format for calculations and presentation. When creating a decision tree you want to ask questions that segment your target response placing the survived 1 and dead 0 into homogeneous subgroups. This tutorial will give you a 1 2 year head start over your peers by providing a framework that teaches you how to think like a data scientist vs what to think code. Now let s learn a little bit more about our DT algorithm. The wrong model can lead to poor performance at best and the wrong conclusion that s used as actionable intelligence at worst. In coding it s okay to try fail and try again. com blog_assets Scikit_Learn_Cheat_Sheet_Python. This can be used for feature engineering to create a family size and is alone variable. Chapter 8 Evaluate Model Performance ch8 1. com c titanic data Step 3 Prepare Data for ConsumptionSince step 2 was provided to us on a golden plater so is step 3. There are more complex methodologies however before deploying it should be compared to the base model to determine if complexity truly adds value. org pandas docs stable generated pandas. programming languages computer systems etc. Your dataset and expected results will determine the algorithms available for use. Chapter 2 A Data Science Framework ch2 1. S the majority 63 died. Last we ll need some business acumen to think through the problem. Thus it s important to fix before we start modeling because we will compare and contrast several models. Guess and checking other features none seem to push us past 82. 0 we used sklearn cross_validate http scikit learn. If you do run into problems Google is your second best friend because 99. Next we use the info and sample function to get a quick and dirty overview of variable datatypes i. linear_model Naive Bayes http scikit learn. Reference Cross Validation and Decision Tree Tutorial http www. So we will change females class 3 embarked S from assuming they survived to assuming they died. Our categorical data imported as objects which makes it difficult for mathematical calculations. feature_selection has several options we will use recursive feature elimination RFE with cross validation CV http scikit learn. 12 9 17 Updated section 5 with model optimization how tos. Remember it s important we use a different subset for train data to build our model and test data to evaluate our model. But the question we always ask is can we do better and more importantly get an ROI return on investment for our time invested For example if we re only going to increase our accuracy by 1 10th of a percent is it really worth 3 months of development. The problem of learning an optimal decision tree is known to be NP complete under several aspects of optimality and even for simple concepts. Such algorithms cannot guarantee to return the globally optimal decision tree. Perform Exploratory Analysis Anybody who has ever worked with data knows garbage in garbage out GIGO. net solomon_caruana_wslmw of MLA comparisons Ogutu et al. in an artificial neural network results may be more difficult to interpret. Unsupervised learning is where you train the model using a training dataset that does not include the correct answer. For this dataset age will be imputed with the median the cabin attribute will be dropped and embark will be imputed with mode. Able to handle both numerical and categorical data. This is helpful in a Kaggle Competition or any use case where consistency matters and surprises should be avoided. This created wide margins between the cross validation CV accuracy score and Kaggle submission accuracy score. Please note this is a manual process created by hand. Chapter 1 How a Data Scientist Beat the Odds ch1 1. So study the next section wisely. It s good because these algorithms are now accessible to more people that can solve more problems in the real world. Although there was some element of luck involved in surviving the sinking some groups of people were more likely to survive than others such as women children and the upper class. Change Log ch90 1. Started working on section 6 for super model. ensemble Generalized Linear Models GLM http scikit learn. In order to better align the CV score and Kaggle score and improve the overall accuracy. But the question still remains can we do better than our handmade model Before we do let s code what we just wrote above. CV is basically a shortcut to split and score our model multiple times so we can get an idea of how well it will perform on unseen data. So another step in data modeling is feature selection. io roc curves and auc explained. That s why I took the time in the previous sections to show you how predictions work. DecisionTreeClassifier class sklearn. org stable modules generated sklearn. Introduction to Machine Learning with Python A Guide for Data Scientists by Andreas M\u00fcller and Sarah Guido https www. Below are common classes to load. Also please be sure to upvote fork and comment and I ll continue to develop. Nonetheless if we assumed everybody died our sample accuracy is 62. This subgroup 55 died and 33 survived since majority died we need to find a signal to identify the 33 or a subgroup to change them from dead to survived and improve our model accuracy. If you ve exhausted all your resources the Kaggle Community via forums and comments can help too. Below are available hyper parameters and defintions http scikit learn. Create a pivot table with survival in the columns count and of row count in the values and the features described below in the rows. As the saying goes you don t have to reinvent the wheel you just have to know where to find it. But if you don t know what you re doing on the exam a calculator even a TI 89 is not going to help you pass. com wp content uploads 2016 03 hilarious autocorrect fails 20x. value_counts https pandas. It s important to remember algorithms are tools and not magical wands or silver bullets. First you must understand that the purpose of machine learning is to solve human problems. 0 max_features None random_state None max_leaf_nodes None min_impurity_decrease 0. Our model accuracy increases to 81. The SibSp represents number of related siblings spouse aboard and Parch represents number of related parents children aboard. We know this is a binary problem because there are only two possible outcomes passengers survived or died. ca fidler teaching 2015 slides CSC411 tutorial3_CrossVal DTs. DataFrame https pandas. The Survived variable is our outcome or dependent variable. qualitative vs quantitative. Therefore it is important to deploy descriptive and graphical statistics to look for potential problems patterns classifications correlations and comparisons in the dataset. A basic methodology for qualitative data is impute using mode. html cross validation that splits our dataset into train and test for data modeling comparison. Adding another level does not seem to gain much more information. This is part science and part art so let s just play the 21 question game to show you how it works. Chapter 7 Step 5 Model Data ch7 1. Chapter 10 Tune Model with Feature Selection ch10 1. At worst it makes completing the project impossible or even worst implements incorrect actionable intelligence. Gather the Data John Naisbitt wrote in his 1984 yes 1984 book Megatrends we are drowning in data yet staving for knowledge. You won t have to do this but it s important to understand it before you start working with MLA. 12 12 17 Cleaned section 5 to prep for hyper parameter tuning. com categorical encoding. We can generalize that a continuous target variable requires a regression algorithm and a discrete target variable requires a classification algorithm. html Choosing Estimator Cheat Sheet https s3. That makes it possible to account for the reliability of the model. 0 min_impurity_split None class_weight None presort False We will tune our model using ParameterGrid http scikit learn. com machine learning latest dg model fit underfitting vs overfitting. Trees can be visualized. pdf Now that we identified our solution as a supervised learning classification algorithm. Meaning the algorithm is so specific to a given subset it cannot accurately generalize another subset from the same dataset. Surprisingly class or even embarked didn t matter like it did for females but title does and gets us to 82. In this stage you will find yourself classifying features and determining their correlation with the target variable and each other. We then hope to present it a new subset from the same dataset and have similar results in prediction accuracy. 12 13 17 Updated section 5 to focus on learning data modeling via decision tree. You did it with very little information we get to 82 accuracy. So keep that in mind when improving your model. com gp product 1449369413 ref as_li_tl ie UTF8 tag kaggle 20 camp 1789 creative 9325 linkCode as2 creativeASIN 1449369413 linkId 740510c3199892cca1632fe738fb8d08 Machine Learning 101 written by a core developer of sklearn Visualize This The Flowing Data Guide to Design Visualization and Statistics by Nathan Yau https www. In the next step we will worry about transforming dirty data to clean data. There are concepts that are hard to learn because decision trees do not express them easily such as XOR parity or multiplexer problems. In addition data categorization i. This problem is mitigated by using decision trees within an ensemble. In this challenge we ask you to complete the analysis of what sorts of people were likely to survive. So now that I ve hammered no pun intended my point I ll show you what to do and most importantly WHY you do it. Other techniques are usually specialized in analyzing datasets that have only one type of variable. In particular we ask you to apply the tools of machine learning to predict which passengers survived the tragedy. Creating Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. The same is true with machine learning. variables for data modeling. The train dataset has a different distribution than the test validation dataset and population. Question 4A going down the female class 3 branch with count 144 Did you embark from port C Q or S We gain a little information. If you want to follow along on your own download the train dataset and import into Excel. Click here to learn more about parameters vs hyper parameters. On April 15 1912 during her maiden voyage the Titanic sank after colliding with an iceberg killing 1502 out of 2224 passengers and crew. It s a small sample size 11 9 but one often used in statistics. Meaning if the majority or 50 or more survived then everybody in our subgroup survived 1 but if 50 or less survived then if everybody in our subgroup died 0. C and Q the majority still survived so no change. Another popular use case is healthcare s mortality rate or survival analysis. pdf watch video lecture here http videolectures. 99 of the time someone else had the same question problem and already asked the coding community. org pandas docs stable indexing. Project Summary The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. 1 Import LibrariesThe following code is written in Python 3. The Sex and Embarked variables are a nominal datatype. communication subject matter knowledge etc. com wp content uploads 2015 04 B fig 1. org stable modules tree. Thanks and may you have statistically significant luck Table of Contents1. There are multiple ways to encode categorical variables we will use the sklearn and pandas functions. Therefore if we just predict the most frequent occurrence that 100 of people died then we would be right 67. If you work in research maybe the answer is yes but if you work in business mostly the answer is no. 2 Meet and Greet DataThis is the meet and greet step. fillna https pandas. describe https pandas. By contrast in a black box model e. pdf comparing 179 classifiers from 17 families Thoma 2016 sklearn comparison https martin thoma. It s bad because a lower barrier to entry means more people will not know the tools they are using and can come to incorrect conclusions. predicting data is logarithmic in the number of data points used to train the tree. 24 Da Double Check Cleaned DataNow that we ve cleaned our data let s do a discount da double check 3. statistics linear algebra etc. Consequently practical decision tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. We will use cross validation and scoring metrics discussed in later sections to rank and compare our algorithms performance. Chapter 3 Step 1 Define the Problem and Step 2 Gather the Data ch3 1. The idea is why write ten lines of code when you can write one line. It is a binary nominal datatype of 1 for survived and 0 for did not survive. One side note logistic regression while it has regression in the name is really a classification algorithm. Instead it s best to impute missing values. In later sections we will also use sklearn s cross validation functions http scikit learn. ParameterGrid GridSearchCV http scikit learn. 23 Convert FormatsWe will convert categorical data to dummy variables for mathematical analysis. naive_bayes Nearest Neighbors http scikit learn. Data wrangling includes implementing data architectures for storage and processing developing data governance standards for quality and control data extraction i. html Sklearn OneHotEncoder http scikit learn. Below is an image of the default CV split. 12 24 17 Updated section 5 with random_state and score for more consistent results. It s important to note more predictor variables do not make a better model but the right variables. It could be used in feature engineering to derive the gender from title family size from surname and SES from titles like doctor or master. But we have information about the dataset so we should be able to do better. Decision tree learners create biased trees if some classes dominate. However since there are many null values it does not add value and thus is excluded from analysis. It s a little more expensive in computer processing but it s important so we don t gain false confidence. Thanks 12 2 17 Updated section 4 with exploratory analysis and section 5 with more classifiers. There are two common methods either delete the record or populate the missing value using a reasonable input. Binary events create an interesting dynamic because we know statistically a random guess should achieve a 50 accuracy rate without creating one single algorithm or writing one single line of code. org attachments 10778 10778 boost. 12 7 17 Updated section 5 with Data Science 101 Lesson. And reinforced learning is a hybrid of the previous two where the model is not given the correct answer immediately but later after a sequence of events to reinforce learning. They will be converted to dummy variables for mathematical calculations. Hello and Welcome to Kaggle the online Data Science Community to learn share and compete. How a Data Scientist Beat the OddsIt s the classical problem predict the outcome of a binary event. 22 Clean DataNow that we know what to clean let s execute our code. 11 Model Performance with Cross Validation CV In step 5. 12 8 17 Reorganized section 3 4 with cleaner code. org pandas docs stable categorical. It s very powerful and helps you with a lot of the grunt work. Nonetheless I want to give credit where credit is due. So we ll stop here for now. We will use a classification algorithm from the sklearn library to begin our analysis. org stable modules model_evaluation. Class 3 is even at a 50 50 split. Female majority 74 survived. org stable tutorial machine_learning_map index. We know that 1 502 2 224 or 67. So with all this information where is a beginner to start I recommend starting with Trees Bagging Random Forests and Boosting http jessica2. But don t worry we only need a high level overview which we ll cover in this Kernel. Think of it like a first date before you jump in and start poking it in the bedroom. html Sklearn Estimator Detail http scikit learn. Developer Documentation Categorical Encoding http pbpython. This can be mitigated by training multiple trees in an ensemble learner where the features and samples are randomly sampled with replacement. The same is true in data modelling. DecisionTreeClassifier criterion gini splitter best max_depth None min_samples_split 2 min_samples_leaf 1 min_weight_fraction_leaf 0. Requires little data preparation. Thanks 11 23 17 Cleaned up published notebook and updated through step 3. Question 5A going down the female class 3 embarked S branch with count 88 So far it looks like we made good decisions. What does it look like datatype and values what makes it tick independent feature variables s what s its goals in life dependent target variable s. Improved model to 85 accuracy. However data science is like a three legged stool with no one leg being more important than the other. In this step we will also define our x independent features explanatory predictor etc. 12 10 17 Updated section 3 4 with cleaner code and better datasets. Step 4 Perform Exploratory Analysis with StatisticsNow that our data is cleaned we will explore our data with descriptive and graphical statistics to describe and summarize our variables. 1 Evaluate Model PerformanceLet s recap with some basic data cleaning analysis and machine learning algorithms MLA we are able to predict passenger survival with 82 accuracy. While no general conclusions can be made from testing a handful of algorithms on a single dataset there are several observations on the mentioned dataset. It is not recommended to delete the record especially a large percentage of records unless it truly represents an incomplete record. get_dummies https pandas. discriminant_analysis Data Science 101 How to Choose a Machine Learning Algorithm MLA IMPORTANT When it comes to data modeling the beginner s question is always what is the best machine learning algorithm To this the beginner must learn the No Free Lunch Theorem NFLT http robertmarks. Missing values can be bad because some algorithms don t know how to handle null values and will fail. Click here for the Source Data Dictionary https www. The Name variable is a nominal datatype. Developer Documentation pandas. If you don t understand something in the code the print function is your best friend. Previously I used the analogy of asking someone to hand you a Philip screwdriver and they hand you a flathead screwdriver or worst a hammer. A basic methodology for quantitative data is impute using mean median or mean randomized standard deviation. Chapter 12 Conclusion and Step 7 Optimize and Strategize ch12 1. gov pmc articles PMC3103196 done by the NIH for genomic selection Fernandez Delgado et al. Given the same dataset decision tree based algorithms seemed to converge on the same accuracy score after proper tuning. Problems before requirements requirements before solutions solutions before design and design before technology. Click here to learn more about ROC_AUC scores http www. In addition we see we may have potential outliers in age and fare. So we will use sklearn function to split the training data in two datasets 75 25 split. That s why I focus on teaching you not just what to do but why you re doing it. It s like cheating on a school quiz to get 100 but then when you go to take the exam you fail because you never truly learned anything. We can play with our features. 12 20 17 Updated section 4 Thanks Daniel M. This sensational tragedy shocked the international community and led to better safety regulations for ships. Despite tuning no machine learning algorithm exceeded the homemade algorithm. Uses a white box model. GridSearchCV and customized sklearn scoring http scikit learn. Model Data Like descriptive and inferential statistics data modeling can either summarize the data or predict future outcomes. Interesting for this dataset the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score. The Pclass variable is an ordinal datatype for the ticket class a proxy for socio economic status SES representing 1 upper class 2 middle class and 3 lower class. Credit sklearn http scikit learn. svm Decision Trees http scikit learn. Step 6 Validate and ImplementThe next step is to prepare for submission using the validation data. For this dataset we will convert object datatypes to categorical dummy variables. Machine Learning ML as the name suggest is teaching the machine how to think and not what to think. Below I ll give an overview of how to run and compare several MLAs but the rest of this Kernel will focus on learning data modeling via decision trees and its derivatives. html Sklearn LabelEncoder http scikit learn. We are doing supervised machine learning because we are training our algorithm by presenting it with a set of features and their corresponding target. Change Log 11 22 17 Please note this kernel is currently in progress but open to feedback. Performs well even if its assumptions are somewhat violated by the true model from which the data were generated. On a worst bad good better and best scale we ll set 82 to good since it s a simple model that yields us decent results. html click here to learn more about ROC_AUC scores http www. Chapter 9 Tune Model with Hyper Parameters ch9 1. It is therefore recommended to balance the dataset prior to fitting with the decision tree. At best it shows a complete lack of understanding. See algorithms for more information. There are no date or currency formats but datatype formats. CreditsProgramming is all about borrowing code because knife sharpens knife. As a data scientist your strategy should be to outsource developer operations and application plumbing so you have more time to focus on recommendations and design. Got it Let s go Question 1 Were you on the Titanic If Yes then majority 62 died. DecisionTreeClassifier. In addition to CV we used a customized sklearn train test splitter http scikit learn. com gp product 1119245516 ref as_li_tl ie UTF8 tag kaggle 20 camp 1789 creative 9325 linkCode as2 creativeASIN 1119245516 linkId 5b4ac9a6fd1da198d82f9ca841d1af9f Easy to understand for a beginner book but detailed to actually learn the fundamentals of the topic. The PassengerID and Ticket variables are assumed to be random unique identifiers that have no impact on the outcome variable. This helps ensure you haven t overfit your model or made it so specific to the selected subset that it does not accurately fit another subset from the same dataset. Supervised learning is where you train the model by presenting it a training dataset that includes the correct answer. For data visualization we will use the matplotlib and seaborn library. An analogy would be asking someone to hand you a Philip screwdriver and they hand you a flathead screwdriver or worst a hammer. for suggestion to split up visualization code. Question 2 Are you male or female Male majority 81 died. Using the same dataset and different implementation of a decision tree adaboost random forest gradient boost xgboost etc. So let s set 50 as the worst model performance because anything lower than that then why do I need you when I can just flip a coin Okay so with no information about the dataset we can always get 50 with a binary problem. Note our sample survival is different than our population of 68. 77990 submission accuracy. So think of it like a coin flip problem. 12 23 17 Edited section 1 5 for clarity and more concise code. Practice Skills Binary classification Python and R basics Step 2 Gather the DataThe dataset is also given to us on a golden plater with test and train data at Kaggle s Titanic Machine Learning from Disaster https www. What happens when technology is too smart for its own good Funny Autocorrect http 15858 presscdn 0 65. isnull https pandas. with tuning does not exceed the 0. Machine Learning Classification Algorithms Ensemble Methods http scikit learn. Not only will you be able to submit your first competition but you ll be able to solve any problem thrown your way. In this kernel I use Kaggle s Getting Started Competition Titanic Machine Learning from Disaster to walk the reader through how to use the data science framework to beat the odds. Note however that this module does not support missing values. So the best approach is to try multiple MLAs tune them and compare them for your specific scenario. With that in mind for iteration two I would spend more time on preprocessing and feature engineering. Not bad for a few lines of code. com watch v EJtTNboTsm8 However in order to tune a model we need to actually understand it. The Age and Fare variable are continuous quantitative datatypes. After all like training a sight seeing dog it s learning from us and not the other way around. The Cabin variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the incident occurred and SES from deck levels. If you have a fair coin and you guessed heads or tail then you have a 50 50 chance of guessing correct. org stable modules classes. Subsequent model iterations may modify this decision to determine if it improves the model s accuracy. Initial competition submission with Decision Tree will update with better algorithm later. Get to know your data by first name and learn a little bit about it. Too often we are quick to jump on the new shiny technology tool or algorithm before determining the actual problem we are trying to solve. Therefore normal processes in data wrangling such as data architecture governance and extraction are out of scope. 11 Load Data Modelling LibrariesWe will use the popular scikit learn library to develop our machine learning algorithms. 13 Tune Model with Feature SelectionAs stated in the beginning more predictor variables do not make a better model but the right predictors do. There are many machine learning algorithms however they can be reduced to four categories classification regression clustering or dimensionality reduction depending on your target variable and data modeling goals. You must still be the master craft wo man that knows how to select the right tool for the job. 12 Tune Model with Hyper ParametersWhen we used sklearn Decision Tree DT Classifier http scikit learn. This leaves opportunity to see how various hyper parameter settings will change the model accuracy. faster than it was before. Step 7 Optimize and Strategize ConclusionIteration one of the Data Science Framework seems to converge on 0. For this dataset we will create a title feature to determine if it played a role in survival. org learn python machine learning lecture fVStr overfitting and underfitting. Prepare Data for Consumption This step is often referred to as data wrangling a required process to turn wild data into manageable data. html Pandas Categorical dtype https pandas. To begin this step we first import our data. Continue to up vote and I will continue to develop this notebook. It s important our algorithm has not seen the subset we will use to test so it doesn t cheat by memorizing the answers. While this topic and big data has been around for decades it is becoming more popular than ever because the barrier to entry is lower for businesses and professionals alike. computer science i. This is both good and bad. 12 11 17 Updated section 5 with better how tos. One I found was fare 0 8 majority survived. Chapter 5 The 4 C s of Data Cleaning Correcting Completing Creating and Converting ch5 1. Meaning it s great at predicting data it s already seen but terrible at predicting data it has not seen which is not prediction at all. Once you re able to package your ideas this becomes your currency exchange rate. Chapter 11 Step 6 Validate and Implement ch11 1. and business management i. Step 1 Define the ProblemFor this project the problem statement is given to us on a golden plater develop an algorithm to predict the survival outcome of passengers on the Titanic. All other variables are potential predictor or independent variables. They are also easier to tune discussed in the next section than something like SVC. Libraries provide pre written functionality to perform necessary tasks. So let s set 68 as bad model performance because again anything lower than that then why do I need you when I can just predict using the most frequent occurrence. Machine Learning Selection Sklearn Estimator Overview http scikit learn. However we want to use caution when we modify data from its original value because it may be necessary to create an accurate model. Thus only data cleaning is in scope. Completing There are null values or missing data in the age cabin and embarked field. Chapter 4 Step 3 Prepare Data for Consumption ch4 1. Converting Last but certainly not least we ll deal with formatting. tree Discriminant Analysis http scikit learn. Since the dead subgroup is less than 10 we will stop going down this branch. Question 3A going down the female branch with count 314 Are you in class 1 2 or 3 Class 1 majority 97 survived and Class 2 majority 92 survived. org papers volume15 delgado14a delgado14a. Credits ch91 How to Use this Tutorial Read the explanations provided in this Kernel and the links to developer documentation. However since they are reasonable values we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. I provide clear explanations clean code and plenty of links to resources. Most beginners get lost in the field because they fall into the black box approach using libraries and algorithms they don t understand. In this step we determine if our model overfit generalize or underfit our dataset http docs. So we are looking for a feature that identifies a subgroup that majority survived. org stable user_guide. To do that we have to go back to the basics of data science 101. Chapter 6 Step 4 Perform Exploratory Analysis with Statistics ch6 1. So this step will require advanced knowledge in mathematics. Machine learning can be categorized as supervised learning unsupervised learning and reinforced learning. This is called overfitting. No new information to improve our model is gained. Remember the name of the game is to create subgroups using a decision tree model to get survived 1 in one bucket and dead 0 in another bucket. 11 26 17 Skipped ahead to data model since this is a published notebook. Mechanisms such as pruning not currently supported setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem. We slightly improve our accuracy but not much to move us past 82. ", "id": "shivampanchal/learning-from-the-disaster-99-accuracy", "size": "42382", "language": "python", "html_url": "https://www.kaggle.com/code/shivampanchal/learning-from-the-disaster-99-accuracy", "git_url": "https://www.kaggle.com/code/shivampanchal/learning-from-the-disaster-99-accuracy", "script": "display #pretty printing of dataframes in Jupyter notebook discriminant_analysis scatter_matrix IPython mytree naive_bayes feature_selection seaborn numpy plot_confusion_matrix matplotlib.pylab XGBClassifier linear_model sklearn metrics LabelEncoder neighbors matplotlib.pyplot gaussian_process correlation_heatmap pandas subprocess OneHotEncoder check_output pandas.tools.plotting svm scipy model_selection sklearn.preprocessing matplotlib tree xgboost ensemble ", "entities": "(('it', 'model'), 'make') (('basic methodology', 'mode'), 'be') (('algorithms', 'don how null values'), 'be') (('explanation', 'easily Boolean logic'), 'explain') (('we', 'categorical dummy variables'), 'convert') (('2016 03 hilarious autocorrect', '20x'), 'upload') (('it', 'really 3 months development'), 'be') (('when you', 'one line'), 'be') (('at best wrong that', 'actionable intelligence'), 'lead') (('so they', 'discipline'), 'come') (('embark', 'mode'), 'impute') (('we', 'dataset'), 'wait') (('I', 'resources'), 'provide') (('here more about ROC_AUC scores', 'www'), 'click') (('model overfit', 'http dataset docs'), 'determine') (('So step', 'mathematics'), 'require') (('Thus they', 'analysis'), 'exclude') (('they', 'outcome'), 'be') (('that', 'homogeneous subgroups'), 'create') (('data', 'which'), 'perform') (('com gp ie product 1119245516 as_li_tl UTF8', 'topic'), 'ref') (('competition Initial submission', 'better algorithm'), 'update') (('then we', 'people'), 'be') (('we', 'features'), 'do') (('predicting', 'tree'), 'be') (('cross validation CV http scikit', 'learn'), 'have') (('you', 'just where it'), 'have') (('Step 7 Optimize', '0'), 'seem') (('you', 'point'), 'win') (('we', 'sklearn'), 'be') (('kernel', 'currently progress'), 'Log') (('Reference Cross Validation', 'Decision Tree www'), 'http') (('classical problem', 'binary event'), 'predict') (('general conclusions', 'several mentioned dataset'), 'be') (('Such algorithms', 'decision globally optimal tree'), 'guarantee') (('book 1984 we', 'yet knowledge'), 'gather') (('da', 'double 3'), 'clean') (('it', 'accuracy'), 'modify') (('less than we', 'branch'), 'be') (('we', 'age'), 'see') (('Google', 'problems'), 'be') (('I', 'odds'), 'use') (('it', 'simple addition calculations'), 'Science') (('shipwreck', 'enough passengers'), 'be') (('exhausted', 'forums'), 'help') (('train dataset', 'test validation dataset'), 'have') (('title', '82'), 'class') (('sample small 11 9 one', 'often statistics'), 's') (('com c Prepare titanic data 3 Data', 'golden plater'), 'provide') (('This', 'cross validation CV accuracy score'), 'create') (('Therefore it', 'dataset'), 'be') (('Instead it', 'missing values'), 's') (('Model DataData Step 5 Science', 'mathematics multi disciplinary i.'), 'be') (('it', 't answers'), 's') (('aboard Parch', 'parents related children'), 'represent') (('They', 'mathematical calculations'), 'convert') (('how it', 'you'), 'be') (('Guess', '82'), 'seem') (('It', 'decision tree'), 'recommend') (('Convert 23 FormatsWe', 'mathematical analysis'), 'convert') (('that', 'variable'), 'be') (('Machine learning', 'supervised learning unsupervised learning'), 'categorize') (('good research', 'Niculescu such Caruana Mizil'), 'say') (('what', 'code'), 'give') (('new information', 'model'), 'gain') (('Load Data Modelling 11 LibrariesWe', 'machine learning algorithms'), 'use') (('dataset', 'already somewhere format'), 'be') (('we', 'learning classification supervised algorithm'), 'pdf') (('data However science', 'more other'), 'be') (('everybody', 'sample accuracy'), 'be') (('print function', 'code'), 'understand') (('name', '0 bucket'), 'remember') (('Mechanisms', 'problem'), 'support') (('Next we', 'variable datatypes'), 'use') (('we', 'good decisions'), '5a') (('Adding', 'much more information'), 'seem') (('this', 'ideas'), 'become') (('then majority', 'Titanic'), 'let') (('we', 'predictor explanatory etc'), 'define') (('Data wrangling', 'data extraction quality i.'), 'include') (('It', 'doctor'), 'use') (('Testing test previously file', 'validation competition really submission'), 'mention') (('We', 'little information'), 'go') (('others', 'null values'), 'handle') (('Data 1 How Scientist', 'Odds ch1'), 'chapter') (('algorithms', 'own classes'), 'call') (('sensational tragedy', 'ships'), 'shock') (('more data', 'algorithm https better www'), 'classifier') (('GridSearchCV', 'http scikit'), 'learn') (('we', 'model'), 'remember') (('that', 'correct answer'), 'be') (('basic methodology', 'mean median randomized standard deviation'), 'be') (('completely different tree', 'data'), 'be') (('1 2 3 Class 1 97 Class 2 majority', 'class'), 'go') (('we', '82 accuracy'), '1') (('that', 'real world'), 's') (('we', 'several models'), 's') (('they', 'conclusions'), 's') (('majority', 'that'), 'look') (('often data', 'manageable data'), 'prepare') (('where locally optimal decisions', 'node'), 'base') (('title', 'difference'), 'exist') (('majority', 'males'), '3b') (('certainly we', 'formatting'), 'deal') (('many null it', 'thus analysis'), 'however') (('model', 'Baseline Data 101 Accuracy'), 'Science') (('even worst implements', 'actionable intelligence'), 'make') (('problem statement', 'Titanic'), 'Define') (('you', 'MLA'), 'win') (('it', 'accurate model'), 'want') (('status socio economic SES', '1 upper class 2 middle class'), 'be') (('when incident', 'deck levels'), 'be') (('scikit', 'validation also sklearn functions'), 'use') (('where I', 'Trees Bagging Random Forests'), 'with') (('passengers', 'tragedy'), 'ask') (('don t', 'horse'), 'define') (('fidler', 'tutorial3_CrossVal DTs'), 'slide') (('it', 'same dataset'), 'be') (('hyper parameter how various settings', 'model accuracy'), 'leave') (('they', 'libraries'), 'lose') (('Thanks', 'step'), 'clean') (('goal', 'just the'), 'be') (('where credit', 'credit'), 'want') (('more barrier', 'businesses'), 'become') (('business acumen', 'problem'), 'need') (('manmade algorithm', 'small datasets'), 'theorize') (('you', 'bedroom'), 'think') (('lot', 'you'), 'do') (('ParameterGrid http scikit', 'model'), '0') (('name', 'how what'), 'teach') (('problem', 'even simple concepts'), 'know') (('qualitative', 'hypothesis also correct test'), 'be') (('Now s', 'DT algorithm'), 'let') (('forest adaboost random gradient', 'etc'), 'use') (('learning algorithm', 'homemade algorithm'), 'exceed') (('based algorithms', 'proper tuning'), 'seem') (('graphviz http scikit', 'learn'), 'visualize') (('decision trees', 'XOR easily such parity'), 'be') (('it', 'fail'), 'in') (('random unique that', 'outcome variable'), 'assume') (('two I', 'more time'), 'with') (('we', 'first data'), 'import') (('stage we', 'calculations'), '21') (('most importantly WHY you', 'it'), 'intend') (('It', 'grunt work'), 's') (('you', 'truly anything'), 's') (('predictor more variables', 'better model'), 's') (('you', 'Contents1'), 'thanks') (('It', 'survived'), 'be') (('This', 'family size'), 'use') (('statistically random guess', 'code'), 'create') (('they', 'flathead screwdriver'), 'use') (('At best it', 'understanding'), 'show') (('so we', 'model https www'), 'be') (('Practice Skills Binary R DataThe Step 2 dataset', 'Disaster https www'), 'classification') (('We', 'algorithms performance'), 'use') (('when technology', 'Funny too own good Autocorrect'), 'happen') (('we', 'actual problem'), 'be') (('this', 'manual hand'), 'note') (('org', 'lecture fVStr overfitting'), 'learn') (('intermediate methodology', 'fare'), 'be') (('dataset results', 'available use'), 'determine') (('Import 1 LibrariesThe following code', 'Python'), 'write') (('decision tree simple algorithm', 'accuracy same best score'), 'have') (('use consistency where matters', 'Kaggle Competition'), 'be') (('it', 'back process'), 'optimize') (('mostly answer', 'business'), 'be') (('that', 'data modeling comparison'), 'cross') (('we', 'binary problem'), 'let') (('sklearn train test splitter http customized scikit', 'CV'), 'use') (('why you', 'it'), 's') (('we', 'variables'), 'Analysis') (('where model', 'learning'), 'be') (('Step 6 next step', 'validation data'), 'validate') (('super that', 'datasets'), 'be') (('complexity', 'truly value'), 'be') (('sometimes we humans', 'coin actually flip'), 'be') (('where features', 'randomly replacement'), 'mitigate') (('we', 'clean data'), 'worry') (('it', 'terms'), 'mean') (('rest', 'decision trees'), 'give') (('We', 'classification'), 'save') (('right predictors', 'better model'), 'state') (('which', 'decision basically different tree'), 'be') (('we', 'what'), 'remain') (('we', 'actually it'), 'v') (('many machine learning however they', 'dimensionality data target variable goals'), 'be') (('sklearn Decision Tree DT Classifier http scikit', 'Tune Hyper 12 ParametersWhen'), 'Model') (('it', 'model'), 'Model') (('simple that', 'decent results'), 'set') (('you', 'way'), 'be') (('it', 'same dataset'), 'help') (('Therefore normal processes', 'scope'), 'be') (('classes', 'biased trees'), 'create') (('html advantages', 'decision trees'), 'classification') (('someone', 'already coding community'), 'have') (('C majority', 'still so change'), 'survive') (('sample survival', '68'), 'note') (('yourself', 'target variable'), 'find') (('So best approach', 'specific scenario'), 'be') (('then you', 'correct'), 'have') (('Accuracy', 'data very simple cleaning'), 'be') (('problem', 'ensemble'), 'mitigate') (('when I', 'just most frequent occurrence'), 'let') (('At worst it', 'project'), 'make') (('linkId Machine Learning', 'Nathan Yau https www'), 'ref') (('Project sinking', 'history'), 'Summary') (('it', 'name'), 'be') (('that', 'code'), 'take') (('regression algorithm', 'target classification discrete algorithm'), 'generalize') (('we', 'dataset'), 'have') (('application so you', 'recommendations'), 'be') (('So we', '75 25 split'), 'use') (('linkId f797da48813ed5cfc762ce5df8ef957f', 'John Mueller'), 'product') (('who', 'GIGO'), 'know') (('how well it', 'unseen data'), 'be') (('you', 'train Excel'), 'want') (('we', 'matplotlib library'), 'use') (('They', 'SVC'), 'be') (('Libraries', 'necessary tasks'), 'provide') (('it', 'us'), 'learn') (('We', '82'), 'improve') (('it', 'survival'), 'create') (('rule', 'thumb'), 'be') (('So step', 'data modeling'), 'be') (('how predictions', 'you'), 's') (('then everybody', 'subgroup'), 'mean') (('age 800 instead 80 then it', 'unreasonable example'), 'note') (('this', 'data ahead model'), 'skip') (('We', 'prediction accuracy'), 'hope') (('which', 'already data'), 'mean') (('We', 'function http sklearn train_test_split scikit'), 'use') (('t', 'false confidence'), 's') (('sorts', 'people'), 'ask') (('NFLT', 'robertmarks'), 'Science') (('s', 'code'), 'DataNow') (('we', 'Kernel'), 'worry') (('knife', 'knife'), 'be') (('We', 'analysis'), 'use') (('that', 'data'), 'include') (('however module', 'missing values'), 'note') (('it', 's.'), 'look') (('purpose', 'human problems'), 'understand') (('we', 'data science'), 'have') (('they', 'flathead screwdriver'), 'ask') (('we', '82 accuracy'), 'do') (('Correcting', 'data'), 'appear') (('it', 'truly incomplete record'), 'recommend') (('they', '3 S'), 'change') (('I', 'Also fork'), 'be') (('max_features None None', 'None'), '0') (('that', 'job'), 'be') (('we', 'model dead accuracy'), 'die') (('I', 'notebook'), 'continue') (('you', 'calculator'), 'know') (('DecisionTreeClassifier we', 'function defaults'), 'accept') (('it', 'mathematical calculations'), 'datum') ", "extra": "['gender', 'outcome', 'test', 'bag']", "label": "Perfect_files", "potential_description_queries": ["account", "accuracy", "advanced", "age", "algorithm", "answer", "appear", "application", "apply", "approach", "architecture", "art", "attribute", "auc", "author", "average", "balance", "basic", "best", "binary", "bit", "blog", "book", "box", "branch", "build", "cabin", "case", "categorical", "challenge", "check", "checking", "children", "clarity", "classification", "clean", "cleaning", "clear", "clustering", "code", "coding", "comment", "community", "compare", "comparison", "competition", "computer", "concept", "conclusion", "condition", "consistency", "content", "contrast", "control", "convert", "core", "correct", "correlation", "correlations", "cost", "could", "count", "create", "credit", "criteria", "criterion", "currency", "customer", "data", "dataset", "date", "day", "decision", "default", "define", "dependent", "depth", "derive", "describe", "develop", "dimension", "dimensionality", "discrete", "distribution", "doctor", "double", "download", "dummy", "encode", "engineering", "ensemble", "ensure", "evaluate", "even", "execute", "expected", "explained", "explore", "external", "extraction", "fail", "family", "fare", "faster", "feature", "field", "fig", "file", "find", "fit", "fitting", "fix", "flip", "following", "forest", "format", "found", "framework", "frequent", "function", "future", "game", "gender", "general", "generated", "gini", "gradient", "graphviz", "hand", "handle", "head", "help", "high", "hope", "http", "human", "idea", "image", "implementation", "import", "improve", "impute", "include", "increase", "info", "investment", "io", "iteration", "kaggle", "kernel", "knowledge", "lead", "leaf", "learn", "learner", "learning", "least", "lecture", "let", "level", "library", "life", "line", "linear", "list", "little", "look", "looking", "lost", "lot", "lower", "majority", "male", "manual", "matplotlib", "max_depth", "max_features", "maximum", "mean", "median", "middle", "might", "mind", "minimum", "missing", "model", "module", "mortality", "most", "move", "multiple", "my", "name", "need", "network", "neural", "new", "next", "no", "node", "non", "none", "normal", "normalization", "not", "notebook", "null", "number", "numerical", "object", "objective", "open", "optimization", "order", "out", "outcome", "outlier", "output", "overall", "overfit", "overfitting", "overview", "package", "parameter", "part", "passenger", "past", "pdf", "people", "percent", "percentage", "perform", "performance", "point", "population", "position", "potential", "pre", "predict", "prediction", "predictor", "prep", "prepare", "preprocessing", "present", "print", "problem", "processing", "product", "project", "provide", "purpose", "python", "question", "random", "rank", "re", "reader", "recommend", "record", "regression", "research", "response", "rest", "result", "return", "right", "roc", "role", "row", "run", "sample", "save", "scale", "school", "science", "scikit", "score", "scoring", "seaborn", "second", "section", "segment", "select", "selected", "selection", "sequence", "set", "several", "ship", "short", "side", "signal", "similar", "single", "situation", "size", "sklearn", "solution", "something", "split", "stage", "standard", "start", "step", "storage", "strategy", "subgroup", "subject", "submission", "subset", "summarize", "supervised", "support", "survival", "survived", "svm", "table", "tag", "target", "technology", "test", "testing", "think", "thought", "through", "thumb", "ticket", "time", "titanic", "title", "tool", "topic", "tragedy", "train", "training", "tree", "try", "tune", "tuning", "turn", "tutorial", "type", "under", "unique", "until", "up", "update", "upper", "validate", "validation", "value", "variable", "video", "visualization", "visualize", "vote", "walk", "web", "while", "who", "work", "worst", "write", "xgboost", "year"], "potential_description_queries_len": 366, "potential_script_queries": ["display", "matplotlib", "numpy", "printing", "scipy", "sklearn", "tree"], "potential_script_queries_len": 7, "potential_entities_queries": ["best", "coding", "consistency", "data", "discrete", "extraction", "frequent", "learning", "middle", "next", "null", "product", "sklearn", "supervised", "test", "train", "unique", "validation", "worst"], "potential_entities_queries_len": 19, "potential_extra_queries": ["bag"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 370}