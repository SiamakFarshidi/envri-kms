{"name": "titanic ml or blatt ", "full_name": " h1 Part 1 Introduction h2 1 1 Background h2 1 2 Goal h2 1 3 Strategy h2 1 4 What to expect h3 Feature Engineering sneak peek h1 Table of Contents h1 Part 2 Question Problem Definition h1 Part 3 About This Dataset h2 3 1 Pre Processing Facts h2 3 2 Features breakdown h3 3 2 1 Text h3 3 2 2 Numerical h4 Discrete Ordinal h4 Continous h3 3 2 3 Categorical h4 Nominal h4 Binary h3 3 2 4 Time Series Data h3 3 2 5 Useless h3 3 2 6 Summary h2 3 3 Insights and Observations h3 3 3 1 Observations in a nutshell for all features h3 3 3 2 What is the distribution of numerical feature values across the samples h4 Outliers h3 3 3 3 What is the distribution of categorical features h3 3 3 4 All Observations h1 Install Libraries h1 Part 4 Acquire training and testing data h2 Dabl Pandas profiling for quick insights h2 Import Libraries h1 Part 5 Data Wrangling h2 Feature Completing h3 Train Dataset missing values X 891 Passengers h3 Test Dataset missing values X 418 Passengers h3 Steps h3 Completing techniques WIP h2 Feature Correcting h3 Detect outliers h3 Reduce skewness h3 Visualize distribution of missing values using missingno h2 5 1 Completing Train Embarked h2 5 2 Complete Test Fare h2 5 3 Complete Cabin Cabin Prefix h3 Cabin Outlier Correcting h3 Describe datasets h2 5 3 1 Cabin Prefix Train h3 5 3 2 Complete Cabin Prefix Test h2 5 4 Complete Age Train Test DataFrames h1 Part 6 Feature Engineering h2 6 1 Family Size Ordinal Numeric Feature h2 6 2 Calculated Fare based on Family Size h2 6 3 Ticket Frequency Ordinal Numeric Feature h2 6 4 Family Title h2 6 5 Survival Rate Survival Rate NA h4 Considering that passengers from the same family and or the same ticket are found in train and test datasets we can calculate the family survival rate and ticket survival rate h3 Title Correcting and Grouping Rare h4 Title Correcting typos h4 Title Grouping Rare h2 6 6 Is Married Feature h2 6 7 Features Correcting Power Transforms h3 Correct outliers and skewness h3 Deal with Fare Skewness by log h3 Shapiro Wilks test h4 Power Transforms h3 Box Cox Transform h3 Yeo Johnson Transform h3 Reduce skewness in Test Data h1 Part 7 Feature Transformation h2 Recap h2 7 1 Feature Binning h3 1 Continious Features Binning h2 Feature Binning Bayesian Blocks h1 Cleaning things up pre feature selection h2 7 2 Label Encoding h2 7 3 OneHotEncoding h2 Part 8 Feature Selection h2 8 1 Multicollinearity Variance Infalction Factor VIF h3 Summary of Multicollinary Changes based on VIF decrease and feature removal h2 8 2 Predictive Power Score WIP h1 Part 9 Modelling h2 9 1 Define X and y features h2 9 2 Split Training data in two datasets anti overfitting h3 Dabl h2 9 3 Hyperparameters Optimization Grid Search h3 9 3 1 Models without Hyperparameters h3 9 3 2 Tune Model with Hyper Parameters h2 Hyperparameters Tuning Impact h2 9 4 Voting Classifier h1 Part 10 Submission h1 Change Log h1 Part 11 Credits and Resources h1 Interesting Resources ", "stargazers_count": 0, "forks_count": 0, "description": "com questions 13610074 is there a rule of thumb for how to divide a dataset into training and validatio 13623707 13623707Feature ranking with recursive feature elimination and cross validated selection of the best number of features. Reduce skewness in Test Data Part 7 Feature Transformation 1 https www. Observations in a nutshell for all features 4. Complete Test Fare Instead of imputing the missing value from the fare median for all passengers in train test datasets I decided to narrow down the list via correlation of independant variables of the sole passenger who has NaN Fare. apply function_to_apply return list_of_dataframes train_df test_df combined_df apply_to_train_test_combined_dataframes mask_boolean_indexing_of_sex https stackoverflow. Part 1 Introduction 1. Observations in a nutshell for all featuresMainly taken from here https www. items mode_of_categorical_variable get_mode_of_categorical_variable df final_df_cabin_prefix_features_description cabin_prefix cabin_prefix df_column passengers_without_cabin_column is_correlated_feature mode_of_categorical_variable passengers_without_cabin_value temp_boolean_list. Describe datasetsfor each passenger with no cabin check if his independant variables match multiple values correlated with those passengers with cabin per cabin_prefixfor passenger_no_cabin in dataframe_of_passengers_with_no_cabin function function input series output Cabin_Prefix estimation check if the passenger has independant variables that fit each cabin prefix for each independant variable execute function input pd. Calculated_Fare based on Family_Size Resource https www. 05 max_depth 2 3 4 6 8 10 min_samples_split 2 5 10. com data types from a machine learning perspective with examples 111ac679e8bc Most data can be categorized into 4 basic types from a Machine Learning perspective numerical data categorical data time series data and text. com orblat titanic ml or blatt Part 5 Data Wrangling 5. com gunesevitan titanic advanced feature engineering tutorial 2. 4 Ticket Resource 3 https www. append is_correlated_feature print Mode mode_of_categorical_variable print Passenger s value passengers_without_cabin_value print is_correlated_feature return temp_boolean_list def get_boolean_list_for_numerical_correlation_of_passenger_value_mean_and_std one_passenger_without_cabin row_series cabin_prefix_dictionary boolean_dictionary_per_cabin_prefix final_df_cabin_prefix_features_description final_df_cabin_prefix_features_description print get_boolean_list_for_numerical_correlation_of_passenger_value_mean_and_std n print 40 temp_boolean_list numerical_columns Age SibSp Parch Fare for cabin_prefix in cabin_prefix_dictionary. fit_transform X_train y_train from sklearn. columns dupli column dupli column 1 if column in dupli. profile_report profile. Focus on sophisticated feature engineering to create the most optimal dataset. 49 train_df Calculated_Fare. 05 max_features auto sqrt log2 ExtraTreeClassifier criterion gini entropy splitter best random min_samples_split 2 5 10. contains B na False temp_df. 2 One Hot Encoding the Categorical Features number of categories for a categorical feature rename columns assign changes to input dataframe and delete the original non one hot encoded feature remove cols 0 to avoid multicollinarity df. Feel free to suggest how I can fix this. T cabin passenger has the closest resemblance to A Cabin_Prefix passengers so he is grouped with A Cabin. cut train_df Age bins Calculated_Age_bins retbins True labels False train_df Age bins_temp_with_inf pd. com headsortails pytanic https www. CategoricalEmbarked and Sex will be encoded using OneHotEncoding Nominal Embarked character. nunique print n from sklearn. They weren t counted as family but they used the same ticket. html This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. astype str df df. keys if in j i str i for k in range dupli i 0 1 df i 0 df i 0. Feature Correcting Detect outliers Fare Reduce skewness Box Cox Yeo Johnson Power Transforms Visualize distribution of missing values using missingnoFound in an amazing notebook https www. some titles can indicate a certain age group. Survival_Rate Survival_Rate_NA Considering that passengers from the same family and or the same ticket are found in train and test datasets we can calculate the family survival rate and ticket survival rate. 05 max_features auto sqrt log2 RandomForestClassifier n_estimators 50 70 100 120 criterion gini entropy max_depth 2 3 4 6 8 10 None min_samples_split 2 5 10. contains B na False train_df. com vikasmalhotra08 feature binning using bayesian blocks Use of Bayesian Block in Santander Customer Transaction Prediction Challenge scott s rule Calculated_Age_bins int round train_df Age. Pclass3 lowest chances of survival most passaengers. Correct outliers and skewnessIn several notebooks I saw some that detect and remove outliers whereas others reduce skewness. Feature Correcting Power Transforms https www. com masumrumi a statistical analysis ml workflow of titanic Embarked feature Resource2 https www. Group rare title names. concat data_other_cols encoded_data axis 1 return concatenated_df train_df_copy onehotencode_dataframe train_df_copy nominal_categorical_features train_df_copy. Ticket_Frequency Ordinal Numeric Feature Resource1 https www. cut train_df Calculated_Fare bins Calculated_Fare_bins retbins True labels False bins_temp train_df Calculated_Fare bins_temp_with_inf pd. SibSp is the count of siblings and spouse and Parch is the count of parents and children. Predictive Power Score WIP 9. the survival for families with 5 8 members is 0 all the large families in Pclass3 3 died. This rational is derived from here https www. Hence we ll remove Is_Married featureAlthough Sex Feature still has high VIF score I d preserve it. org en stable visualization histogram. com orblat titanic ml or blatt Part 11 Credits and Resources Part 2 Question Problem Definition Find out who survived the titanic incident. com orblat titanic ml or blatt Hyperparameters Tuning Impact 9. Age AgeSurvival Infant Toddlers age 4 were saved in large numbers irrespective of the class The Women and Child First Policy. All Observations Sex SexSurvival The chance of survival for women is high as compared to men. describe def get_mean_and_std_of_numerical_variable df final_df_cabin_prefix_features_description cabin_prefix cabin_prefix df_column passengers_without_cabin_column slice a pd. std Calculated_Fare_bins _ bins_temp pd. 371 Passengers share these commonalities checks that changes were saved correctly method 1 method 2 df Cabin_Prefix. Split Training data in two datasets anti overfitting https www. Feature Binning https www. Pclass Sex Survival Sex Female Pclass 1 Survival 1 Sex Female Pclass 2 high chances of survival. Data Wrangling https www. com orblat titanic ml or blatt Part 7 Feature Transformation 7. com questions 10212445 python map list item to function with arguments condition for each independant variable originally 80 I used 60 100 range for variance. Test dataset range 0 6 Continous Fare Float Meaning how much each passenger paid for their journey. html https stackoverflow. Family_Name Title https www. cut train_df Family_Size bins np. com max 700 1 n QnKwkk_hxNihfAYxfAzg. 05 min_samples_leaf 1 5 10. Techniques for completing of 1 values vs. Calculated_Fare based on Family_Size 6. inf labels False bins_temp_with_inf using Bayesian_blocks train_df Calculated_Fare train_calculated_fare_bins_array pd. Hoping that I haven t fallen to this pit this is the summary The Standard Error of Mean SEM is a measure of precision for an estimated population mean. 77 Test Dataset missing values X 418 Passengers Cabin Age Fare 1 Fare missing. Note that Grid Search is a slow method. Pre Processing Facts https www. The survival rate is calculated for families with more than 1 members in that list and stored in Family_Survival_Rate feature. Parch and SibSp have slightly more than 5 VIF. cut train_df Calculated_Fare bins Calculated_Fare_bins retbins True labels False train_calculated_fare_bins_array scotts binned histogram blocks binned histogram bins bayesian_blocks train_df Age https www. Test dataset range 0 512. 21 The 4 C s of Data Cleaning Correcting Completing Creating and Converting Feature Completing Resource https www. Sex female boolean indexing error def apply_to_train_test_combined_dataframes function_to_apply list_of_dataframes train_df test_df combined_df for df in list_of_dataframes df df. Submission https www. Embarked S majority of the rich people boarded and majority of Pclass3 passengers boarded. Complete Cabin Cabin PrefixAccording to A Statistical Analysis ML workflow of Titanic https www. Family_Survival_Rate Ticket_Survival_Rate https www. Features Breakdown https www. fit_transform df nominal_categorical_features. com articles pandas find rows where columnfield is null importing the library don t forget to install pip install missingno train_df train_df. reshape len integer_encoded 1 onehot_encoded onehot_encoder. drop columns feature cols 0 inplace True use this to avoid multicollinarity without checking VIF as I am about to do. keys return df train_df_copy concat_duplicate_columns train_df_copy test_df_copy concat_duplicate_columns test_df_copy combined_df_copy pd. Label Encoding https www. Since all available information 1 http users. Complete missing value in Test Fare. Features breakdown 3. org stable _static ml_map. com max 351 1 D8B5_HlEfwSQURgQkymoBA. Voting Classifier Part 10 SubmissionUnfortunately I discovered that I created a complex model overfitting the dataset. From dataset of two passengers the dataset grew to 128 passengers with correlated independent variables. 05 indicates significant skewness. com blog 2020 03 one hot encoding vs label encoding using scikit learn Feature Selectionhttps www. loc axis 1 df_column mean_of_numerical_variable sliced_description_df_by_cabin_prefix_and_feature mean 0 std_of_numerical_variable sliced_description_df_by_cabin_prefix_and_feature std 0 return mean_of_numerical_variable std_of_numerical_variable def get_mean_plus_minus_std mean mean_of_numerical_variable std std_of_numerical_variable return round mean std round mean std round for Parch and SibSp def get_boolean_list_for_categorical_correlation_of_passenger_value_and_mode one_passenger_without_cabin row_series cabin_prefix_dictionary boolean_dictionary_per_cabin_prefix final_df_cabin_prefix_features_description final_df_cabin_prefix_features_description print get_boolean_list_for_categorical_correlation_of_passenger_value_and_mode n print 40 temp_boolean_list categorical_columns Sex Embarked Pclass for cabin_prefix in cabin_prefix_dictionary. Some persons might share the same surname indicating family relations. Then we could identify how much the hyperparameters helped to increase the model score. C passengers_representing_those_with_missing_embarked1 temp_df temp_df. Hyperparameters Optimization Grid Search We ll first fit each basic algorithm then iterate through various parameters for each algorithm and compare the results. Continuous data can assume any value within a range whereas discrete data has distinct values. S port used by most passengers top S Cabin values have several dupicates across samples. com orblat titanic ml or blatt Part 10 Submission 11. Ticket feature has high ratio 22 of duplicate values unique 681. Money Wins PclassAge Survival Infant Toddlers age 4 passengers in Pclass 2 and Pclass 3 mostly survived. randint with mean std range for each DataFrame to impute missing Age values. fillna missing_embarked_value inplace True train_df train_df Embarked. SexPclassSurvival Sex Female Pclass 1 2 Survival 1 SexAge Age distribution seems to be the same in Male and Female subpopulations so Sex is not informative to predict Age. get_dummies train_df_copy columns nominal_categorical_features drop_first True dummies from category_encoders import TargetEncoder enc TargetEncoder cols Name_of_col Another_name training_set enc. 7 1 kernel linear poly rbf sigmoid degree range 1 5 shrinking True False probability True False gamma scale auto decision_function_shape ovo ovr break_ties True False DecisionTreeClassifier criterion gini entropy splitter best random min_samples_split 2 5 10. com sinakhorami titanic best working classifier Classifier Comparisonhttps www. SibSp Integer Meaning the number of siblings or spouses travelling with each passenger. org pandas docs stable generated pandas. com questions 45003806 python pandas use slice with describe versions greater than 0 20 train_df train_df Cabin_Prefix. keys else 1 for key val in dict dupli. 21 327 Cabin values are missing only 91 Cabin numbers were preserved. We can use a Pipeline object to apply both transforms in sequenceBox Cox Transform managed to deal with Age but not with Fare due to zero values. com gunesevitan titanic advanced feature engineering tutorial 1. One way to solve this problem is to use a MixMaxScaler transform first to scale the data to positive values then apply the transform. preprocessing import OneHotEncoder LabelEncoder onehot_encoder OneHotEncoder sparse False temp_train_df train_df_copy. 9 1 n_components range len train_df. shape 0 1 3 3. figure figsize 16 8 f. Is_Married https www. std 100 3 best_model_for_submission vote_hard test_survived best_model_for_submission. com orblat titanic ml or blatt Part 6 Creating Feature Engineering 6. 4 of the total training set survived the crash. find out which passengers share the same group puchase by ticket. 05 max_depth 2 3 4 6 8 10 None min_samples_split 2 5 10. Define X and y features 9. 55 while it is lowest for S. This trend is conserved when we look at both male and female passengers. preprocessing import OneHotEncoder nominal_categorical_features Cabin_Prefix Pclass Embarked Sex train_df_copy train_df. Completing Fare https www. descriptive statistics resource https towardsdatascience. com power transforms with scikit learn resource2 https www. 20 687 Cabin values are missing large majority of Cabin numbers are missing. columns use ppscore Predictive Power Score PPS TO DO https www. gov pmc articles PMC2959222 2 http www. com masumrumi a statistical analysis ml workflow of titanic calculated_fare Some people have travelled in groups like family or friends. Most passengers 75 did not travel with parents or children. max train_df Calculated_Fare. com orblat titanic ml or blatt 3. completing 80 values. We may be able to solve these two missing values by looking at other independent variables of the two rows. com yassineghouzam titanic top 4 with ensemble modeling Fare Fare distribution is very skewedThis can lead to overweigth very high values in the model even if it is scaled. cut train_df Calculated_Fare bins np. Binary Sex string Meaning an indicator whether the passenger was female or male. Insights and Observations https www. 1 Continious Features BinningFeature Binning Conversion of a continuous variable to categorical Types of Binning Unsupervised Binning Equal width binning Equal frequency binning adaptive binning fixed binning Supervised Binning Entropy based binningLet s use bayesian_blocks to determine bin size and then bin age Calculated_Fare and Family_Size. Test dataset range 0 10 Parch Integer Meaning the number of parents or children travelling with each passenger. Family_Size Ordinal Numeric Feature Resource1 https www. Predictive Power Score WIP Predictive Power Score PPS. Pre Processing Facts 3. astype str df i str k 1. Family Title Title Correcting Grouping https www. org stable modules generated sklearn. Insights and Observations 3. Alternatively several passengers shared a cabin. PclassAge 1st class passengers are older than 2nd class passengers who are also older than 3rd class passengers. sort_values https stackoverflow. 22 Clean Data Title Correcting typos Title Grouping Rare 6. copy deep True combined_df_copy test_df. com orblat titanic ml or blatt 6. Part 3 About This Dataset 3. condition_alone train_df. fit_transform reshaped_column label_encoder LabelEncoder integer_encoded label_encoder. This is often described as removing a skew in the distribution. com max 501 1 lheLiN7y4sSD2JKvow clw. 2 Frequency Encoding Resource 2 https www. 7 of the subsets let s know the worst that can happen save MLA predictions see section 6 for usage print and sort table https pandas. com helgejo an interactive data science tutorial 2. Pclass Integer Meaning represents social economic status SES. Completing Train Embarked 5. join count_df_of_passengers_with_cabin_per_prefix final_df_cabin_prefix_features_description pd. fit_transform values print integer_encoded dummies pd. Hence not considered. com arthurtok introduction to ensembling stacking in python Interesting Resourceshttps www. 4 Cabin there is one person on the boat deck in T cabin and he is a 1st class passenger. EmbarkedSex Port Q looks looks to be unlukiest for Men as almost all were from Pclass 3 which is known for low chances of survival for as compared to other Pclasses. com a 56780852 prettify using pyplot https matplotlib. 05 max_features auto sqrt log2 NuSVC nu 0. png Feature Binning Bayesian Blocks pd. com orblat titanic ml or blatt Part 9 Modelling 9. describe by Cabin_Prefix and by feature to get the mode of the categorical feature for a specific Cabin_Prefix sliced_description_df_by_cabin_prefix df. fit_transform X_train y_train from category_encoders import WOEEncoder enc WOEEncoder cols Name_of_col Another_name training_set enc. Features Correcting Power Transforms Shapiro Wilks test https www. 3292 USD Age Float Test dataset range 0. Survival_Rate Survival_Rate_NA 6. Parch SibSp Parch SibSp Survival SibSp 1 2 U Parch 1 3 alone U large family. 1 Label Encoding Non Numerical FeaturesEmbarked Sex Deck Title and Family_Size_Grouped are object type and Age and Fare features are category type. com ldfreeman3 a data science framework to achieve 99 accuracy 3. 1 177 Age values are missing. columns sorted set re. 1 kernel linear poly rbf sigmoid precomputed degree range 1 5 shrinking True False probability True False gamma scale auto decision_function_shape ovo ovr break_ties True False LinearSVC penalty l1 l2 loss hinge squared_hinge dual True False DecisionTreeClassifier criterion gini entropy splitter best random max_depth 2 4 None min_samples_split 2 5 min_samples_leaf 1 2 max_features auto sqrt log2 ExtraTreeClassifier criterion gini entropy splitter best random max_depth 2 4 None min_samples_split 2 5 min_samples_leaf 1 2 max_features auto sqrt log2 LinearDiscriminantAnalysis solver svd lsqr eigen shrinkage None auto 0. 23 Convert Formats 9. iterrows pass def get_estimated_letter_bundle pass def get_mode_of_categorical_variable df final_df_cabin_prefix_features_description cabin_prefix cabin_prefix df_column passengers_without_cabin_column slice a pd. copy deep True test_df_copy test_df. groupby Cabin_Prefix. com masumrumi a statistical analysis ml workflow of titanic comments all_data. ca epidemiology hanley IntMedResidents SD SE. 5 Feature Transformation 2. preprocessing import OneHotEncoder LabelEncoder onehotencoder OneHotEncoder nominal_categorical_features Cabin_Prefix Pclass Embarked Sex train_df_copy train_df. com orblat titanic ml or blatt Yeo Johnson Transform 7. 2 Outlier detection is an excellent code snippet for that based on Tukey method 1977 using IQR. Pre Processing Facts How Many Survived https www. 12 Tune Model with Hyper Parameters Hyperparameters Tuning Impact 9. com orblat titanic ml or blatt Part 1 Introduction 2. To tackle this I decided to plot the Fare range for each Cabin_prefix but Fare is too Skewed and no insights could be concluded. com orblat titanic ml or blatt 8. inf retbins True labels False test_df Calculated_Fare pd. What is the distribution of numerical feature values across the samples This helps us determine among other early insights how representative is the training dataset of the actual problem domain. Ticket_Frequency Ticket groupby transform count 3. Sometimes a lift in performance can be achieved by first standardizing the raw dataset prior to performing a Yeo Johnson transform. Hyperparameters Optimization Grid Search https www. com headsortails pytanic Train Dataset missing values X 891 Passengers Cabin Age Embarked 2 Embarked values are missing. Total samples are 891 or 40 of the actual number of passengers on board the Titanic 2 224. edu gmeeden papers hist. Voting Classifier https www. com yassineghouzam titanic top 4 with ensemble modeling 5. com 2013 09 08 basic feature engineering with the titanic data Family_Size is created by adding SibSp Parch and 1. A list of family names that are occuring in both training and test set non_unique_families is created. Sex and Family_Size have much higher VIF than 5. com orblat titanic ml or blatt 5. png revision latest cb 20180322183733 Cabin Outlier CorrectingAccording to Titanic Advanced Feature Engineering Tutorial https www. fit_transform col concatenated_df df transformed_data onehotencoder. Feature Transformation https www. Family feature is created with the extracted surname. There are two popular approaches for automatic power transforms they are Box Cox Transform Yeo Johnson Transform Box Cox Transformassumes positive values 0 and negative values are not supported. At this stage I decided to re evaluate whether mean SD SEM is the right way to complete the Cabin feature. Embarked The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S. Few elderly passengers 1 within age range 65 80. Oldest passengers Age 80 survived. com blog 2020 03 one hot encoding vs label encoding using scikit learn Dummy Variable Trap is a scenario in which variables are highly correlated to each other. Modelling https www. com yassineghouzam titanic top 4 with ensemble modeling Fare Reduce Fare and Age skewness using Shapiro s law and power transforms Multicollinearity Variance Infalction Factor VIF https www. These passengers better represent the passenger with missing Fare because they share same characteristics which are depicted by the correlated independant variables. Source https www. Age 38 temp_df. com vikasmalhotra08 feature binning using bayesian blocks Use of Bayesian Block in Santander Customer Transaction Prediction Challenge color black _ bins_temp pd. Hence I ll convert this binary feature to a nominal feature adding child as an option. The score ranges from 0 no predictive power to 1 perfect predictive power. However for those who still want to detect and remove outliers here https www. https stackoverflow. It s important our algorithm has not seen the subset we will use to test so it doesn t cheat by memorizing the https miro. read_csv list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session ignore warnings to be used for submission import pandas_profiling profile train_df. Sex The chance of survival for women is high as compared to men. For each missing Cabin the value will be imputed by a cabin_estimator function based on a Fare range of each Cabin_Prefix. This is a categorical nominal feature that was already mapped to numerical values. com ResidentMario missingno 5. Cabin string object Meaning an alphanumeric representation of the cabin number of each passenger. preprocessing import OneHotEncoder low_dimensional_nominal_categorical_columns Pclass Embarked high_dimensional_nominal_categorical_columns Cabin_Prefix Title nominal_categorical_columns low_dimensional_nominal_categorical_columns high_dimensional_nominal_categorical_columns onehot_encoder OneHotEncoder sparse False for df in train_df_copy for column in nominal_categorical_columns reshaped_column df column. com orblat titanic ml or blatt Shapiro Wilks test Box Cox Transform https www. Completing Age https www. We can explore this by adding a StandardScaler as a first step in the pipeline. keys for i in set re. com ash316 eda to prediction dietanic Observations in a Nutshell for all features. Is_Married Feature 6. SummaryIn summary we have 11 12 test train features 4 numerical features Fare Age SibSp Parch 3 text features Name Cabin Ticket 4 catrgorical features Embarked Pclass Sex Survived and 1 useless feature PassengerID 2 Numerical Continous features as float datatype Fare Age 2 Numerical Discrete Ordinal features as integer datatype SibSp Parch 3 Text features as string object dataype Name Cabin Ticket 1 Categorical Nominal features as character datatype Embarked 1 Categorical ordinal features as integer datatype Pclass. com gunesevitan titanic advanced feature engineering tutorialhttps www. Yeo Johnson Transform handled skewness better hence we ll use it. Multicollinearity occurs where there is a dependency between the independent features. Hence a cabin_estimator function will mislead us and will most likely impute false values to complete Cabin feature. Embarked C highest survival chances Embarked S lowest survival chances EmbarkedPclass Embarked C Pclass 1 better survival chances even though the majority of Pclass1 passengers got up at S. Useless PassengerID Integer A running index for the dataset no true numerical or categorical meaning. Complete Age Train Test DataFrames 6. org wiki Power_transform 2 https machinelearningmastery. net titanic images f f9 Titanic_side_plan. Survived integer Target Found in train dataset only. org writing gotchas mutable default arguments https stackoverflow. com orblat titanic ml or blatt Box Cox Transform Yeo Johnson Transform https www. to_file train_data_pandas_profiling. Pclass Pclass Survival Pclass1 highest chances of survival most survivals. 3 Ticket classes 1 First 2 Second 3 Third. Tune Model with Hyper Parameters Source https www. cut test_df Calculated_Fare bins np. reshape 1 1 n temp_train_df Pclass. copy deep True le LabelEncoder def onehotencode_dataframe df nominal_categorical_features nominal_categorical_features le le df nominal_categorical_features df nominal_categorical_features. columns store_covariance True False QuadraticDiscriminantAnalysis store_covariance True False XGBClassifier booster gbtree gblinear dart Ensemble Methods Gaussian Processes Navies Bayes Nearest Neighbor Discriminant Analysis xgboost http xgboost. Feature Binning Bayesian Blocks https www. 1 86 Age values are missing. from_frame final_df_cabin_prefix_features_description for row_index row_series in passengers_without_cabin. Age Children less than 5 10 years do have a high chance of survival In particular infants Age 4 had high survival rate. Ticket_Survival_Rate and Family_Survival_Rate are averaged and become Survival_Rate and Ticket_Survival_Rate_NA and Family_Survival_Rate_NA are also averaged and become Survival_Rate_NA. Nearly 30 of the passengers had siblings and or spouse aboard. com helgejo an interactive data science tutorialhttps www. 15 died a lot. More Parch Older Age More SibSp Younger Age Embarked The chances for survival for Port C is highest around 0. concat test_df_copy encoded_df len nominal_categorical_features axis 1 import re def concat_duplicate_columns df dupli for column in df. It is a binary nominal datatype of 1 for survived and 0 for did not survive. For women the chance of survival from Pclass1 is almost 1 and is high too for those from Pclass2. astype int result pd. Features breakdown Resource https towardsdatascience. 2 Frequency Encoding Resource2 https triangleinequality. The oldest Passengers were saved 80 years. PclassParch The number of children increases with Pclass. Insights and Observations Most observations were taken from here https www. keys print nCabin Prefix cabin_prefix for passengers_without_cabin_column passengers_without_cabin_value in one_passenger_without_cabin numerical_columns. They are converted to numerical type with LabelEncoder. Those columns are added in order to find the total size of families. html remember to install it first pip install dabl data analysis and wrangling visualization machine learning https dzone. 05 max_features auto sqrt log2 parameters_dict AdaBoostClassifier n_estimators range 50 100 10 learning_rate list float_range 0 1. R random_state None 42 BaggingClassifier n_estimators range 10 20 bootstrap True False bootstrap_features True False oob_score True False warm_start True False random_state None 42 ExtraTreesClassifier n_estimators 50 70 100 120 criterion gini entropy max_depth 2 4 None min_samples_split 2 5 min_samples_leaf 1 2 max_features auto sqrt log2 GradientBoostingClassifier loss deviance exponential n_estimators 50 70 100 120 criterion friedman_mse mse mae max_depth 2 4 None min_samples_split 2 5 min_samples_leaf 1 2 max_features auto sqrt log2 RandomForestClassifier n_estimators 50 70 100 120 criterion gini entropy max_depth 2 4 None min_samples_split 2 5 min_samples_leaf 1 2 max_features auto sqrt log2 GaussianProcessClassifier multi_class one_vs_rest one_vs_one warm_start True False LogisticRegressionCV fit_intercept True False cv 3 5 7 10 dual True False penalty l1 l2 elasticnet solver newton cg lbfgs liblinear sag saga multi_class auto ovr multinomial PassiveAggressiveClassifier fit_intercept True False warm_start True False RidgeClassifierCV SGDClassifier penalty l2 l1 elasticnet average True False Perceptron penalty l2 l1 elasticnet warm_start True False BernoulliNB GaussianNB KNeighborsClassifier weights uniform distance algorithm auto ball_tree kd_tree brute leaf_size range 20 40 5 SVC kernel linear poly rbf sigmoid precomputed degree range 1 5 gamma scale auto shrinking True False probability True False NuSVC nu list float_range 0 1. It s important to note more predictor variables do not make a better model but the right variables. cut train_df Age bins np. com ash316 eda to prediction dietanic How many Survived Out of 891 passengers in training set only around 350 survived i. index data_other_cols df. j for j in dupli. Acquire training and testing data https www. com masumrumi a statistical analysis ml workflow of titanic Part 7 Modeling the Datahttps www. Already mapped from categories to numerical values. apply changes on all dataframes Function to calculate VIF same as from statsmodels. com analytics vidhya data scientists stop randomly binning histograms 1069d7380c3a I could find is mostly theoretical and not implemented python wise I d appreciate if anyone can enlighten me on this https miro. com questions 40111161 pandas sort column by maximum values sort correlation dataframe by maximum access the last column the one with the lowest maximum vif and choose the feature to drop to achieve this vif scores pip install ppscore f plt. html Passenger in the T deck is changed to A https stackoverflow. com questions 10665889 how to take column slices of dataframe in pandas 44736467 filter frequency of features based on mean std for numerical features mode freq for categorical features passengers_with_cabin final_df_cabin_prefix_features_description pd. Completing Embarked https www. symmetric_difference train_df. pivot columns x index y values ppscore sns. com power transforms with scikit learn will make the probability distribution of a variable more Gaussian. Introduction https www. Completing Cabin Cabin Prefix https www. Age Parch SibSp Moreover the more a passenger has parents children the older he is and the more a passenger has siblings spouses the younger he is. com orblat titanic ml or blatt 9. Complete missing values in Train Embarked. com orblat titanic ml or blatt Title Correcting and Grouping Rare 6. inf labels False bins_temp_with_inf scotts binned histogram blocks binned histogram https www. GradientBoostingClassifier criterion mae loss deviance max_depth 4 max_features sqrt min_samples_leaf 2 min_samples_split 5 n_estimators 100 best_model_for_submission. Title Correcting and Grouping Rare1. com What is seed in random number generation pip install dabl Machine Learning Algorithm MLA Selection and Initialization Ensemble Methods Gaussian Processes Navies Bayes Nearest Neighbor Discriminant Analysis xgboost http xgboost. Passengers at Q were all from Pclass3. Pclass 1 temp_df. The PPS is an asymmetric data type agnostic score that can detect linear or non linear relationships between two columns. Define X and y features x independent features explanatory predictor etc. html split dataset in cross validation with this splitter class http scikit learn. com questions 25140998 pandas compute mean or std standard deviation over entire dataframe train_df. 1 algorithm SAMME SAMME. 4 Cabin Titanic_Deck https vignette. resource https machinelearningmastery. columns axis 1 change reindex_axis to reindex for kaggle compatibility df. To understand what s the best or more percise way to deal with outliers skewness I got inspiration from Jochen Wilhelm here https www. Predictive Power Score WIP https www. 9 the real prediction score turned to be 0. Power TransformsA power transform 1 https en. The Survived variable is our outcome or dependent variable https www. Being alone also proves to be fatal and the chances for survival decreases when somebody has 4 parents on the ship. y dependent target outcome response etc Source https www. DataFrame encoded_feat columns cols print encoded_df. Hyperparameters Optimization Grid Search Hyperparameters Tuning Impact https www. html removed models w o attribute predict_proba required for vote classifier and models with a 1. nunique cols _. Aside from ML being a buzzword I discovered a whole world thanks to Kaggle. cut test_df Age bins np. 4 Target Encoding There are too many unique Ticket values to analyze so grouping them up by their frequencies makes things easier. Completing Train Embarked Resource1 https www. 3 Strategy No exploratory data analysis. com a 56780852 Ensemble Methods Gaussian Processes Navies Bayes Nearest Neighbor Discriminant Analysis xgboost http xgboost. Maximum number of deaths were in the age group of 30 40. org examples scatterplot_matrix. com masumrumi a statistical analysis ml workflow of titanic Embarked feature Completing missing features and creating calculated_fare A Journey through Titanic https www. Cabin Prefix Train 5. Since I used Power Transforms Yeo Johnsons on age and Calculated_Fare I assume that there s a big difference between Scott s rule bin size and Bayesian blocks bin size. add_subplot 1 2 2 matrix_df pps. A Journey through Titanic https www. fit train_df train_df_x_calc train_df Target print base_results train_score. Learn from others implement others code to form a whole that is greater that the sum of its parts. Feature Selection https www. ShuffleSplit note this is an alternative to train_test_split run model 10x with 60 30 split intentionally leaving out 10 create table to compare MLA metrics create table to compare MLA predictions index through MLA and save performance to table set name and parameters score model with cross validation http scikit learn. 05 max_features auto sqrt log2 bootstrap True False oob_score True False warm_start True False GradientBoostingClassifier loss deviance exponential n_estimators range 10 150 30 criterion friedman_mse mse mae min_samples_split 2 5 10. com omarelgabry a journey through titanic Part 6 Feature Engineering1. loc axis 1 df_column return sliced_description_df_by_cabin_prefix_and_feature top 0 top equalls mode for pd. keys print nCabin Prefix cabin_prefix for passengers_without_cabin_column passengers_without_cabin_value in one_passenger_without_cabin categorical_columns. Please comment this notebook and submit your suggestions Change Log21 02 21 Kernel is open to feedback. Numerical Definition Numerical data is any data where data points are exact numbers. Question Problem Definition https www. inf retbins True labels False test_df Age pd. Complete Age Train Test DataFrames 20 missing values use np. Maximum passengers boarded from S. com orblat titanic ml or blatt Part 8 Feature Selection 8. cross_validate if this is a non bias random sample then 3 standard deviations std from the mean should statistically capture 99. Sex_is_Female 1 temp_df. com arthurtok introduction to ensembling stacking in pythonhttps www. Fare Higher fare paying passengers had better survival chances Money Matters. org generated seaborn. html why choose one model when you can pick them all with voting classifier http scikit learn. Needs to be predicted in test dataset. loc axis 0 slice cabin_prefix cabin_prefix sliced_description_df_by_cabin_prefix_and_feature sliced_description_df_by_cabin_prefix. Adding 1 at the end is the current passenger. Pclass The upper class passengers Pclass 1 were more likely to have survived compared to second third classes Pclass 2 OR Pclass 3. 2 Meet and Greet Data. This is important so we don t overfit our model. inf labels False bins_temp_with_inf remove Text Features remove Family Ticket Survival Rate features that were usued to create Survival_Rate and Survival_Rate_NA remove the features which were used to build other features we have Calculated_Fare instead https stackoverflow. Ticket_Frequency Ordinal Numeric Feature 6. max train_df Age. 3 5 independent variables are correlated. Fare 80 passengers_representing_those_with_missing_embarked1 passengers_representing_those_with_missing_embarked2 temp_df condition_female condition_pclass1 condition_age_38_to_62 condition_fare condition_pclass1 condition_female condition_age_38_to_62 condition_fare condition_age_38_to_62 condition_female condition_pclass1 condition_fare condition_fare condition_female condition_pclass1 condition_age_38_to_62 fillna inplace doesn t work https stackoverflow. It seems like Fare column kept a record of the total fare rather than the fare of individual passenger therefore calculated fare will be much handy in this situation. com parulpandey useful python libraries for data science Single Predictive Power Score Part 9 Modelling ChooseModel https scikit learn. The mode operation will be used to impute the missing embarked values based on a larger dataset. Resource https www. png Resource https towardsdatascience. 4 Target Encoding Creating survival_rate and Survival_Rate_NA Titanic Top 4 with ensemble modeling https www. DataFrame data result columns Survived 1st row as the column names submit pd. net post Outlier_and_skewness_effect_on_Normality_and_homogeneity_of_variance_testing The bottom line Removing outliers is only sensible if these values are bad values that is when they are extremely implausible Since the outliers of Age and Fare values are plausible and since we want to provide certain models with features represented by normal distribution I decided to opt for skewness reduction. Credits and Resources https www. com orblat titanic ml or blatt Feature Binning Bayesian Blocks 7. toarray encoded_data pd. Decision find out which passengers share the same cabin. Split Training data in two datasets anti overfitting 9. Label Encoding 7. com ldfreeman3 a data science framework to achieve 99 accuracy Huge inspiration for wrangling data completing missing features and modeling A Statistical Analysis ML workflow of Titanic https www. Complete missing values in Train Test Cabin. com parulpandey useful python libraries for data science use chi squared statistic and the mutual information statistic https machinelearningmastery. com startupsci titanic data science solutionshttps www. This is necessary for grouping passengers in the same family. com ldfreeman3 a data science framework to achieve 99 accuracy 5. merge filtered_df_of_passengers_with_cabin_mean_std_numeric description_of_passengers_with_cabin_categorical_features on Cabin_Prefix how inner. Models without Hyperparameters 9. Around 38 samples survived representative of the actual survival rate at 32. Thanks Part 11 Credits and Resources A Data Science Framework To Achieve 99 Accuracy https www. What is the distribution of categorical features Names are unique across the dataset count unique 891 Sex variable as two possible values with 65 male top male freq 577 count 891. 5 4 5 independent variables are correlated. html Hard Vote or majority rules Soft Vote or weighted probabilities best_model_for_submission ensemble. An extra binary feature Family_Survival_Rate_NA is created for families that are unique to the test set. This data has meaning as a measurement Numerical data can be characterized into continuous or discrete data. 1309 371 passengers. com questions 17114904 python pandas replacing strings in dataframe with numbers https stackoverflow. cut train_df Family_Size bins Family_Size_bins retbins True labels False train_df Family_Size bins_temp_with_inf pd. LabelEncoder basically labels the classes from 0 to n. This process is necessary for models to learn from those features. shape 0 encoded_df. 2 Setup helper FunctionsClassifiersEnsamblingVotingConfusion matrixcross_validation avoid overfitting underfitting CV use cross validation for small datasets. com questions 21998354 pandas wont fillna inplace train_df. com questions 42718870 defining a default argument as a global variable https stackoverflow. 3 Title Is Married 6. com vikasmalhotra08 feature binning using bayesian blocks Use of Bayesian Block in Santander Customer Transaction Prediction Challenge using Scott s rule Calculated_Fare_bins int round train_df Calculated_Fare. mean 100 print base_results test_score. Family_Size Ordinal Numeric Feature 6. com yassineghouzam titanic top 4 with ensemble modeling 2. com jeffd23 scikit learn ml from start to finishhttps www. Complete Test Fare 5. To be more precise The chances of survival is good for somebody who has 1 3 parents on the ship. com ash316 eda to prediction dietanic and here https www. 1 Catrgorical Binary feature as string object datatype Sex To be converted to 3 options nominal feature male female child 1 running index with no meaning PassengerID 1 Catrgorical Binary feature as integer datatype Survived 3. com feature engineering deep dive into encoding and binning techniques 5618d55a6b38 Continious features need to be binned and then encoded in order to be used by ML algorithms. There are other methods to explore such as Bayesian Optimization Particle based methods Convex optimizers and Simulated annealing 9. com orblat titanic ml or blatt 7. Observations in a nutshell https www. concat objs train_df_copy test_df_copy axis 0. According to the titanic tale women and children were evacuated first. Complete missing values in Train Test Age. min train_df. Install Libraries Part 4 Acquire training and testing data Dabl Pandas_profiling for quick insights Import Libraries Part 5 Data Wrangling Great resource for the 4 Cs https www. copy deep True for df in test_df_copy train_df_copy combined_df_copy for feature in nominal_categorical_features encoded_feat OneHotEncoder. Those groups consist of friends nannies maids and etc. Parch 0 790 values it s too frequent. com helgejo an interactive data science tutorial 5. com questions 24458645 label encoding across multiple columns in scikit learn https www. copy deep True a onehot_encoder. Data Analysis Insights https www. heatmap matrix_df cmap BrBG annot True from sklearn. Ticket_Survival_Rate and Ticket_Survival_Rate_NA features are also created with the same method. DataFrame transformed_data index df. Then the dataset of similar passengers will better represent the passengers with missing Embarked values. com questions 17995024 how to assign a name to the a size column https seaborn. items del dupli key if val 1 for i in range val dupli key str i val else dupli key 1 df. fit_transform X_train y_train from category_encoders import LeaveOneOutEncoder enc LeaveOneOutEncoder cols Name_of_col Another_name training_set enc. concat PassengerId result axis 1 submit 1st row as the column names. Huge Family 8 12 people Cleaning things up pre feature selection 7. Family_Survival_Rate is calculated from families in training set since there is no Survived feature in test set. outliers_influence import variance_inflation_factor https stackoverflow. For those who have seen the Titanic movie 1997 I am sure we all remember this sentence during the evacuation Women and children first. OneHotEncoding https www. com feature selection with categorical data define y variable aka target outcome define x variables for original features aka feature selection data1_x Sex Age SibSp Parch Family_Size Calculated_Fare Ticket_Frequency Survival_Rate Survival_Rate_NA Pclass Embarked Cabin_Prefix Title pretty name values for charts data1_x_calc list coded for algorithm calculation random_state seed or control random number generator https www. Spoiler C it is 5. I d love to fix this. com kaggle docker python linear algebra data processing CSV file I O e. Standard Deviation SD is a measure of data variability around mean of a sample of population. com max 1050 1 MAr4rWj6zw0Rdo01ecZu1A. We ll find the best one to remove and then check VIF again2. Calculated_Fare https www. Complete Cabin Prefix Test 5. com masumrumi a statistical analysis ml workflow of titanic Embarked feature The passengers with missing Embarked values have the following independent variables 1. Define X and y features https www. com python10pm pandas 100 tricks https www. Is_Married FeatureCalculate Is_Married boolean value based on Title of Mrs. Voting Classifier 10. Correct typos in Mlle Miss Ms Miss Mme Mrs 2. fit_transform temp_train_df Pclass. pdf except astropy https docs. drop i str k 1 1 df df. com orblat titanic ml or blatt Part 2 Question Problem Definition 3. Most Passengers are between age group 15 to 35 and died a lot. groupby Cabin_Prefix Fare. com parulpandey useful python libraries for data science Github https github. Small Family 3 people4. The dummy variables which are created using one hot encoding have infinite VIF. In this notebook I will include techniques for Feature Engineering ML twicking Hyperparamaters Ensambling etc. Cabin i 0 for i in all_data. Text Name The names also contain titles. Apparantly Standard Deviation SD and Standard Error of Mean SE SEM are used interchangeably by mistake in various medical journals 1 https www. However when I tried to do so I discovered a big standard deviation of Fare for each Cabin_Prefix same for calculating median instead of mean to overcome skewness. Embarked takes three possible values. Pclass Age Sex Survival Survival chances for Passenegers aged 20 50 from Pclass1 is high and is even better for Women. How is this feature different than Family_Size Many passengers travelled along with groups. com 10 essential numerical summaries in statistics for data science theory python and r f3ee5e0eca32 normal distribution mean standard deviation SD no normal distribution median and range. items print passengers_without_cabin_column passengers_without_cabin_value mean_of_numerical_variable std_of_numerical_variable get_mean_and_std_of_numerical_variable df final_df_cabin_prefix_features_description cabin_prefix cabin_prefix df_column passengers_without_cabin_column mean_plus_std mean_minus_std get_mean_plus_minus_std mean mean_of_numerical_variable std std_of_numerical_variable is_correlated_feature mean_plus_std passengers_without_cabin_value mean_minus_std 0 else 0 retrieve just the numeric input values perform a box cox transform of the dataset unknown why feature_range 1 2 instead of the default feature_range 0 1 convert the array back to a dataframe histograms of the variables retrieve just the numeric input values perform a yeo johnson transform of the dataset define the pipeline convert the array back to a dataframe histograms of the variables retrieve just the numeric input values perform a yeo johnson transform of the dataset define the pipeline convert the array back to a dataframe histograms of the variables scotts binned histogram blocks binned histogram bins bayesian_blocks train_df Calculated_Fare https www. 128 passengers compared to two passengers at the beginning. Although I got a high accuracy score when it comes to training the model 0. fit_transform df feature. html barplot using https seaborn. Time Series Datanone. This feature is also necessary because there is no way to calculate those families survival rate. 25 Split Training and Testing Data use sklearn function to split the training data in two datasets 75 25 split. SexAgeSurvival For males the survival chances decreases with an increase in age. Still the chances for survival is low here that is because many passengers from Pclass3 around 81 didn t survive. 2 GoalExplore Learn and apply best practices in various machine learning fields. isna dataset df Cabin i 0 SyntaxError can t assign to function call method 3 https www. describe by Cabin_Prefix and by feature to get the mean and std of the numerical feature for a specific Cabin_Prefix sliced_description_df_by_cabin_prefix df. fit train_df train_df_x_calc train_df Target result best_model_for_submission. Family TitleAll text and code derived from this excellent Resource https www. html Scott s rule https medium. drop columns nominal_categorical_features concatenated_df pd. append encoded_df test_df_copy pd. com omarelgabry a journey through titanic Completing Age Titanic Advanced Feature Engineering Tutorial https www. com orblat titanic ml or blatt Part 4 Acquire training and testing data 5. format feature n for n in range 1 n 1 encoded_df pd. Hence we should use Yeo Johnson Transform Yeo Johnson TransformUnlike Box Cox it supports zero values and negative values. std Calculated_Age_bins _ bins_temp pd. com cross validation 430d9a5fee22 Dabl Quick various model fitting using Dabl 9. index encoded_features. isnull missing_embarked_value update combined_df with missing embarked value checks that changes were saved correctly condition for each independant variable male reasonable range for age 60. 4 What to expect Data Wrangling Feature completing Feature correcting detect outliers handle skewness and kurtosis using power transforms and Shapiro Wilks test Feature Engineering Feature Transformation Feature binning Scott s rule Bayesian Blocks Label encoding OneHotEncoding Feautre Selection Handle multicollinearity using Variance Infalction Factor VIF Modeling Hyperparameters Optimization via Grid Search VotingClassifier Feature Engineering sneak peek Table of Contents1. com startupsci titanic data science solutions some observations were rephrased. pdf 2 https arxiv. To avoid over filtering I will correlate 3 independent variables Sex Pclass Age Fare Cabin_prefix. To achieve Gaussian distribution we ll use and compare two power transforms Yeo Johnson Transform vs. The Dummy Variable Trap leads to the problem known as multicollinearity. toarray n df feature. Cabin https stackoverflow. All other variables are potential predictor or independent variables. Parch SibSp Having 1 2 siblings spouse on board or 1 3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with you. reset_index drop True set train_df_copy. html ExtraTreesClassifier n_estimators 50 70 100 120 criterion gini entropy max_depth 2 4 6 8 10 None min_samples_split 2 5 10. Features Correcting Power Transforms Fare and Age features has outliers are skewed. inf retbins True labels False test_df Family_Size pd. Complete Cabin Cabin Prefix 5. concat PassengerId result axis 1 submit base_results model_selection. extract_surname function is used for extracting surnames of passengers from the Name feature. Shapiro Wilks testThe null hypothesis for Shapiro Wilks test is that the data is a sample from a normal distribution so a p value less than 0. Family_Size https www. Deal with Fare Skewness by logMany machine learning algorithms perform better when the distribution of variables is Gaussian distribution aka normal distribution. This notebook contains a mixture of lessons I ve learned from various notebooks as well as on my own. i for i in dupli. Multicollinearity Variance Infalction Factor VIF https www. Family_Size SibSp Parch 1 2. Feature BinningHandling Continious Features Grouping Binning Banding https miro. It can be used as an alternative to the correlation matrix. com questions 54052471 mapping values in place for example with gender from string to int in pandas d why use global and not mutable default arguments https docs. 1 BackgroundAspired by a brilliant home assignment given by a data science startup and not merely any background I decided to get a hands on understanding of Machine Learning. describe include all for mode Create two dataframes for passengers with cabin each dataframe for numerical categorical set of features https stackoverflow. Multicollinearity Variance Infalction Factor VIF 8. Outliers Fares varied significantly with few passengers 1 paying as high as 512USDs. completing 20 values vs. Complete Cabin by mean estimators grouping binning banding Completing techniques WIP 1. DataFrame data test_survived columns Survived 1st row as the column names submit pd. This feature implies that family survival rate is not applicable to those passengers because there is no way to retrieve their survival rate. Now let us drop one of the dummy variables to solve the multicollinearity issue Summary of Multicollinary Changes based on VIF decrease and feature removalWe can see that Title_4 and Is_Married create multicullinarity. Series of one passenger with no cabin df2 final_df_cabin_prefix_features_description output dicionary key for each cabin prebix with value of list of True False in range for each feature based on SD MODE 5. com orblat titanic ml or blatt Part 3 About This Dataset 3. com max 1007 1 pIptNvUJHFiJ_lizQsxyOw. cut test_df Family_Size bins np. Alternatively passengers shared a group purchase. Big Family 4 7 people5. 1 Automagichttps scikit learn. Ticket string object Meaning an alphanumeric representation of the ticket number of each passenger. Alone SibSp Parch 0Let s check who has multiple independent variables like the above mentioned ones. Ticket_Frequency https www. cross_validate best_model_for_submission train_df train_df_x_calc train_df Target cv cv_split return_train_score True best_model_for_submission. add_subplot 1 2 1 corr_heatmap train_df. Feature Engineering https www. com masumrumi a statistical analysis ml workflow of titanic Cabin Feature one way to complete Cabin feature is by calculating the mean Fare of each Cabin_Prefix. jpeg Discrete Ordinal Definition Ordinal feature means its values may be arranged in some order that makes logical sense. 95 of the Passengers at Q were from Pclass3. Passenger who travel together may have purchased tickets with the same prefix. com questions 17679089 pandas dataframe groupby two columns and get counts 17679517 17679517 https stackoverflow. org pdf physics 0605197. cut Family_SizeAs far as I understand this means that the bins are 1. One of the common ways to check for multicollinearity is the Variance Inflation Factor VIF VIF 1 Very Less Multicollinearity VIF 5 Moderate Multicollinearity VIF 5 Extreme Multicollinearity This is what we have to avoid From the output we see that there s a multicollinearity problem and we can conclude the following 1. head from sklearn. Children age high chance of survival. OneHotEncoding Part 8 Feature Selection 8. 0 correlation to another model run model 10x with 60 30 split intentionally leaving out 10 Ensemble Methods Gaussian Processes Navies Bayes Nearest Neighbor xgboost http xgboost. Meaning Port of Embarkation Options S Q C Southampton Queenstown Cherbourg. Pclass 1 train_df. png Source https towardsdatascience. ", "id": "orblat/titanic-ml-or-blatt", "size": "39184", "language": "python", "html_url": "https://www.kaggle.com/code/orblat/titanic-ml-or-blatt", "git_url": "https://www.kaggle.com/code/orblat/titanic-ml-or-blatt", "script": "pandas.plotting get_estimated_letter_bundle random correct_title_names MinMaxScaler DecisionTreeClassifier naive_bayes collections pyplot bayesian_blocks concat_duplicate_columns astropy.stats correlate_onehotencoded_feature_to_drop get_mean_and_std_of_numerical_variable Perceptron find_onehotencoded_feature_to_drop OneHotEncoder LogisticRegression missingno sklearn.svm ensemble show_values_on_bars sklearn.naive_bayes onehotencode_dataframe get_key_with_max_values create_cabin_prefix_column float_range extract_families_and_titles_from_name get_mean_plus_minus_std KNeighborsClassifier one_hot_encode_dataframe _show_on_single_plot seaborn numpy sklearn.pipeline SGDClassifier scipy.stats drop_features_bulk_dataframes = train_df.Pclass.map({1 shapiro LabelEncoder pandas Pipeline LeaveOneOutEncoder Counter mask_boolean_indexing_of_sex corr_heatmap sklearn.linear_model matplotlib get_array_of_boolean_correlations_for_each_feature_per_cabin_prefix = test_df.Pclass.map({1 get_mode_of_categorical_variable choose_random_key_from_keys_with_max_values LinearSVC XGBClassifier get_boolean_list_for_numerical_correlation_of_passenger_value_mean_and_std linear_model get_columns_containing_null create_is_married apply_to_multiple_dataframes clean_rare_title_names ppscore GaussianNB calculate_vif find_best_features_to_drop_for_best_multicollinarity heatmap statsmodels.stats.outliers_influence apply_to_train_test_combined_dataframes sklearn.tree discriminant_analysis scatter_matrix get_list_of_keys_with_max_values create_family_size_feature variance_inflation_factor TargetEncoder WOEEncoder sklearn.ensemble get_boolean_list_for_categorical_correlation_of_passenger_value_and_mode create_calculated_fare_feature sklearn RandomForestClassifier neighbors matplotlib.pyplot gaussian_process category_encoders StandardScaler tqdm PowerTransformer svm get_estimated_letter_based_on_boolean_array sklearn.neighbors SVC model_selection sklearn.preprocessing tree xgboost statsmodels.api ", "entities": "(('Family Title Title', 'Grouping https www'), 'Correcting') (('fare', 'chances Money Matters'), 'have') (('histogram blocks', 'bayesian_blocks'), 'passengers_without_cabin_column') (('chances', 'age'), 'sexagesurvival') (('describe', 'features https stackoverflow'), 'include') (('we', 'https instead stackoverflow'), 'label') (('com power transforms', 'variable more Gaussian'), 'make') (('that', 'training'), 'create') (('number', 'Pclass'), 'increase') (('True gamma auto decision_function_shape ovo ovr break_ties True l1 l2 loss hinge criterion False False squared_hinge dual True False DecisionTreeClassifier gini', 'None 1 max_features auto log2 ExtraTreeClassifier criterion None 1 max_features auto eigen None best random 2 4 min_samples_leaf 2 gini entropy best random 2 4 min_samples_leaf 2 lsqr auto'), 'range') (('anyone', 'https miro'), 'stop') (('95', 'Pclass3'), 'be') (('Most data', 'Machine data data time series Learning perspective numerical categorical data'), 'type') (('Sex chance', 'men'), 'SexSurvival') (('get_mode_of_categorical_variable cabin_prefix passengers_without_cabin_column', 'pd'), 'pass') (('loc return 1 sliced_description_df_by_cabin_prefix_and_feature top 0 top equalls', 'pd'), 'axis') (('Names', '65 male top male freq'), 'be') (('Numerical data', 'continuous data'), 'have') (('it', 'https t miro'), 's') (('it', 'zero values'), 'use') (('Modelling ChooseModel https Part 9 scikit', 'data science Single Predictive Power Score'), 'parulpandey') (('PassengerId result', 'submit base_results 1 model_selection'), 'concat') (('passengers', 'ticket'), 'find') (('titles', 'age certain group'), 'indicate') (('extract_surname function', 'Name feature'), 'use') (('Question Problem Resources Part 2 who', 'titanic incident'), 'com') (('com power transforms', 'https resource2 www'), 'learn') (('Parch', 'slightly more than 5 VIF'), 'have') (('I', 'dataset'), 'Part') (('inf', 'True labels'), 'retbin') (('CategoricalEmbarked', 'Nominal Embarked character'), 'encode') (('html Passenger', 'https stackoverflow'), 'change') (('Nearly 30', 'siblings'), 'have') (('oob_score GradientBoostingClassifier loss False True False exponential n_estimators', 'True False'), 'bootstrap') (('changes', 'commonalities checks'), 'share') (('we', 'power two transforms'), 'use') (('Continious Features BinningFeature Binning 1 Conversion', 'bin size'), 'bin') (('Few elderly passengers', '65 80'), 'range') (('20 Cabin 687 values', 'Cabin numbers'), 'miss') (('chance', 'Pclass2'), 'be') (('cabin', 'SD MODE'), 'prebix') (('skewness better hence we', 'it'), 'handle') (('One way', 'then transform'), 'be') (('Change Log21 02 21 Kernel', 'suggestions'), 'comment') (('0 negative values', 'power two popular automatic transforms'), 'be') (('categorical nominal that', 'already numerical values'), 'be') (('print get_boolean_list_for_categorical_correlation_of_passenger_value_and_mode cabin_prefix_dictionary n', 'cabin_prefix_dictionary'), 'axis') (('com startupsci titanic data science', 'www'), 'solutionshttps') (('Ticket_Survival_Rate_NA features', 'also same method'), 'create') (('they', 'same ticket'), 'weren') (('too insights', 'Cabin_prefix'), 'decide') (('detect correcting outliers', 'Contents1'), '4') (('It', 'correlation matrix'), 'use') (('he', 'Cabin'), 'have') (('survival EmbarkedPclass C 1 better even majority', 'S.'), 'Embarked') (('class upper passengers', '2 Pclass'), 'be') (('We', 'zero values'), 'use') (('fit_transform X_train y_train', 'import LeaveOneOutEncoder enc LeaveOneOutEncoder Another_name training_set Name_of_col enc'), 'col') (('integer Survived Target', 'train dataset'), 'find') (('I', 'as well own'), 'contain') (('hyperparameters', 'model score'), 'identify') (('Methods Gaussian Processes Navies Bayes Nearest Neighbor intentionally 10 Ensemble xgboost', 'xgboost'), 'correlation') (('Many passengers', 'groups'), 'be') (('list_of_dataframes df', 'df'), 'def') (('rather large family', 'you'), 'show') (('we', 'ticket survival family survival rate rate'), 'survival_rate_na') (('SD SEM', 'Cabin right feature'), 'decide') (('consist', 'maids'), 'nanny') (('things', 'frequencies'), 'be') (('We', 'pipeline'), 'explore') (('histogram blocks', 'histogram https www'), 'bin') (('i', 'else key 1 df'), 'key') (('Sometimes lift', 'Yeo Johnson prior transform'), 'achieve') (('Total samples', 'board'), 'be') (('persons', 'family relations'), 'share') (('better when distribution', 'Gaussian normal distribution'), 'perform') (('DataFrame', 'Age values'), 'randint') (('com jeffd23 scikit', 'www'), 'learn') (('character datatype', 'integer datatype Pclass'), 'have') (('I', '3 independent variables'), 'correlate') (('when somebody', 'ship'), 'prove') (('columns', 'DO https www'), 'use') (('non bias random then 3 standard deviations', 'statistically 99'), 'capture') (('html', '1'), 'attribute') (('column names', 'pd'), 'column') (('who', 'https here www'), 'for') (('so we', 'model'), 'be') (('Sex', '5'), 'have') (('Parch', 'parents'), 'be') (('majority', 'Pclass3 passengers'), 'board') (('dataset', 'correlated independent variables'), 'grow') (('Sex', 'Age'), 'seem') (('which', 'correlated independant variables'), 'represent') (('originally 80 I', 'variance'), 'question') (('chance', 'men'), 'sex') (('Survived', 'training'), 'survive') (('changes', 'age'), 'update') (('t', 'submission import pandas_profiling profile'), 'file') (('Standard Deviation Apparantly SD', 'https various medical journals 1 www'), 'use') (('passengers', 'following independent variables'), 'masumrumi') (('Family_Size_bins', 'True labels'), 'cut') (('variables', 'highly other'), 'learn') (('Huge Family 8 12 people', 'feature selection'), 'clean') (('Age', 'survival 4 high rate'), 'have') (('even it', 'model'), 'be') (('merely I', 'Machine Learning'), 'BackgroundAspired') (('reshape len', 'onehot_encoded 1 onehot_encoder'), 'integer_encoded') (('score', '1 perfect predictive power'), 'range') (('python 45003806 pandas', '20'), 'use') (('Multicollinearity', 'where independent features'), 'occur') (('Methods Gaussian Processes Navies Bayes Nearest Neighbor Discriminant Analysis 56780852 Ensemble xgboost', 'xgboost'), 'com') (('passengers', 'same cabin'), 'find') (('pip install dabl Machine Learning MLA Ensemble Methods Gaussian Navies Bayes Nearest Neighbor Discriminant Analysis Algorithm Selection xgboost', 'xgboost'), 'com') (('Pclass Integer Meaning', 'status social economic SES'), 'represent') (('fit_transform values', 'integer_encoded dummies'), 'print') (('com yassineghouzam titanic', 'ensemble modeling'), 'top') (('that', 'execute function input independant variable pd'), 'correlate') (('3 which', 'other Pclasses'), 'look') (('don t', 'pip install missingno'), 'find') (('heatmap matrix_df cmap BrBG', 'sklearn'), 'annot') (('I', 'skewness reduction'), 'post') (('Complete Age Train Test DataFrames 20 missing values', 'np'), 'use') (('feature selection Sex Age SibSp Parch Ticket_Frequency Survival_Rate Survival_Rate_NA name aka Embarked Cabin_Prefix Title pretty values', 'number generator https random www'), 'selection') (('isna Cabin 0 SyntaxError', 'call method https 3 www'), 'dataset') (('Title_4', 'multicullinarity'), 'let') (('mode operation', 'larger dataset'), 'use') (('us', 'training problem how actual domain'), 'be') (('I', 'rule bin big size'), 'assume') (('discrete data', 'distinct values'), 'assume') (('com', 'chi statistic'), 'parulpandey') (('Fare', 'notebook https amazing www'), 'reduce') (('Hence I', 'option'), 'convert') (('Age outliers', 'Power Transforms Fare'), 'feature') (('who', 'class also 3rd passengers'), 'be') (('Family feature', 'extracted surname'), 'create') (('Alone SibSp s who', 'above mentioned ones'), 'Parch') (('S port', 'samples'), 'have') (('Adding', 'end'), 'be') (('print cabin_prefix_dictionary n', 'cabin_prefix_dictionary'), 'print') (('when we', 'male passengers'), 'conserve') (('I', 'thanks Kaggle'), 'aside') (('he', 'younger'), 'child') (('Function', 'same statsmodels'), 'apply') (('Maximum number', '30 40'), 'be') (('parameters score cross validation http scikit', 'learn'), 'note') (('SibSp Integer', 'passenger'), 'mean') (('Ticket feature', '22 duplicate values'), 'have') (('when it', 'model'), 'get') (('predictor more variables', 'better model'), 's') (('It', 'survived'), 'be') (('0 large families', 'Pclass3'), 'be') (('survival rate', 'Family_Survival_Rate feature'), 'calculate') (('he', 'T cabin'), 'be') (('com questions', 'https global variable stackoverflow'), 'define') (('import re def 1 concat_duplicate_columns', 'df'), 'concat') (('Outliers Fares', '1 as high 512USDs'), 'vary') (('Cabin_Prefix Pclass', 'Sex train_df_copy train_df'), 'preprocesse') (('Define X independent', 'predictor explanatory etc'), 'feature') (('Hyperparameters Optimization Grid We', 'results'), 'Search') (('Older More SibSp Younger More Age', 'Port C'), 'parch') (('com questions', 'https www'), 'label') (('Setup helper FunctionsClassifiersEnsamblingVotingConfusion 2 matrixcross_validation', 'small datasets'), 'avoid') (('Standard Error', 'population estimated mean'), 'be') (('Cabin_Prefix onehotencoder nominal_categorical_features Pclass', 'Sex train_df_copy train_df'), 'preprocesse') (('last column', 'ppscore f plt'), 'question') (('Completing', 'https Titanic www'), 'masumrumi') (('com', 'data science Github https github'), 'parulpandey') (('com blog 2020 03 one hot encoding', 'Feature Selectionhttps www'), 'learn') (('fit_transform reshaped_column label_encoder LabelEncoder', 'label_encoder'), 'integer_encoded') (('who', 'ship'), 'be') (('LabelEncoder', 'n.'), 'label') (('Methods Gaussian Navies Bayes Nearest Neighbor Discriminant Analysis Ensemble xgboost', 'xgboost'), 'store_covariance') (('com questions', '21998354 inplace'), 'fillna') (('FeatureCalculate', 'Mrs.'), 'is_married') (('Hyperparameters Optimization Grid Search Hyperparameters', 'Impact https www'), 'Tuning') (('which', 'infinite VIF'), 'have') (('Insights Most observations', 'https www'), 'take') (('Continious features', 'ML algorithms'), 'dive') (('Split Training Data', 'two datasets'), '25') (('Most Passengers', '15 35'), 'be') (('feature', 'families survival also rate'), 'be') (('process', 'features'), 'be') (('integer datatype', '3'), 'feature') (('DataFrame Survived column data test_survived 1st names', 'pd'), 'column') (('I', 'VIF'), 'col') (('Outlier 2 detection', '1977 IQR'), 'be') (('weighted probabilities', 'html Hard majority Soft Vote'), 'vote') (('Family_Size', 'SibSp Parch'), 'com') (('skewness', 'Multicollinearity Variance Infalction Factor VIF https www'), 'top') (('7 kernel linear poly rbf sigmoid 1 degree', 'True False probability'), 'range') (('cabin_estimator Hence function', 'Cabin feature'), 'mislead') (('html split dataset', 'splitter class http scikit'), 'learn') (('Alternatively passengers', 'group purchase'), 'share') (('This', 'same family'), 'be') (('html', 'pip install dabl data first analysis'), 'remember') (('l1 l2 elasticnet solver newton lbfgs liblinear sag saga multi_class auto ovr PassiveAggressiveClassifier fit_intercept True True SGDClassifier penalty l2 l1 elasticnet True Perceptron penalty l2 l1 elasticnet True BernoulliNB GaussianNB weights uniform distance algorithm auto brute False False leaf_size', 'True False list'), 'None') (('We', 'two rows'), 'be') (('Most passengers', '75 parents'), 'travel') (('Test dataset Parch 0 10 Integer', 'passenger'), 'range') (('therefore calculated', 'much situation'), 'seem') (('histogram blocks', 'bayesian_blocks'), 'cut') (('others', 'skewness'), 'outlier') (('even majority', 'S.'), 'look') (('so I', 'instead skewness'), 'discover') (('multicollinearity we', 'following 1'), 'be') (('value', 'Cabin_Prefix'), 'impute') (('passenger', 'Binary Sex indicator'), 'string') (('Then dataset', 'missing Embarked values'), 'represent') (('Pclass Age Sex Survival Survival chances', 'even Women'), 'be') (('data', 'p value'), 'be') (('This', 'distribution'), 'describe') (('that', 'logical sense'), 'mean') (('Title onehot_encoder', 'reshaped_column df nominal_categorical_columns column'), 'preprocesse') (('2 Pclass', 'Pclass'), 'win') (('Calculated_Fare bins', 'True labels'), 'cut') (('cabin_prefix passengers_without_cabin_column', 'pd'), 'describe') (('rational', 'https www'), 'derive') (('that', 'test set'), 'create') (('Age AgeSurvival Infant Toddlers', 'irrespective class'), 'save') (('We', 'then VIF'), 'find') (('4', 'crash'), 'survive') (('data type agnostic asymmetric that', 'non linear two columns'), 'be') (('people', 'family'), 'masumrumi') (('Family_Survival_Rate', 'Survived test set'), 'calculate') (('concat data_other_cols encoded_data', '1 return'), 'axis') (('deviation r f3ee5e0eca32 normal standard SD', 'distribution normal median'), 'com') (('who', 'NaN Fare'), 'Fare') (('i', 'range'), 'key') (('They', 'LabelEncoder'), 'convert') (('passenger', 'journey'), 'range') (('family survival rate', 'survival rate'), 'imply') (('Tune 12 Model', 'Impact'), 'Tuning') (('PassengerId result', 'column names'), 'concat') (('com analysis ml statistical workflow', 'Cabin_Prefix'), 'masumrumi') (('Around 38 samples', '32'), 'survive') (('Sex Feature', 'I it'), 'remove') (('who', 'same prefix'), 'purchase') (('Alternatively several passengers', 'cabin'), 'share') (('It', 'kaggle python Docker image https github'), 'come') (('columns', 'families'), 'add') (('I', 'Jochen Wilhelm'), 'get') (('inf', 'Age False pd'), 'retbin') (('Standard Deviation SD', 'population'), 'be') (('here that', 'didn t many Pclass3 around 81 survive'), 'be') (('that', 'parts'), 'implement') (('Feature Engineering ML twicking Hyperparamaters', 'etc'), 'include') (('Dummy Variable Trap', 'multicollinearity'), 'lead') (('13623707 13623707Feature', 'features'), 'be') (('we', 'evacuation'), 'be') (('voting classifier http scikit', 'them'), 'learn') (('html 100 criterion 50 70 120 gini', 'max_depth'), 'n_estimators') (('MLA predictions', 'usage 6 print'), 'let') ", "extra": "['gender', 'outcome', 'test', 'bag']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "advanced", "age", "algorithm", "alphanumeric", "analyze", "append", "apply", "argument", "array", "assign", "assignment", "astropy", "attribute", "auto", "average", "background", "barplot", "basic", "become", "best", "bin", "binary", "blog", "board", "boat", "boolean", "bottom", "box", "build", "cabin", "calculate", "calculation", "call", "categorical", "category", "cb", "character", "check", "checking", "child", "children", "choose", "classifier", "code", "col", "color", "column", "comment", "compare", "compute", "condition", "contain", "control", "convert", "copy", "correlation", "could", "count", "create", "criterion", "current", "cut", "cv", "data", "dataframe", "dataset", "def", "default", "define", "degree", "del", "dependent", "describe", "detect", "detection", "df", "dict", "difference", "directory", "discrete", "distance", "distribution", "drop", "dummy", "duplicate", "eda", "en", "encoding", "end", "engineering", "ensemble", "ensembling", "entropy", "environment", "error", "estimation", "evacuation", "evaluate", "even", "execute", "explore", "family", "fare", "feature", "figure", "file", "filter", "find", "fit", "fitting", "fix", "fixed", "float", "following", "form", "format", "found", "framework", "freq", "frequency", "function", "gamma", "gender", "generated", "generation", "generator", "gini", "group", "groupby", "grouped", "handle", "head", "heatmap", "helper", "high", "histogram", "hot", "http", "https machinelearningmastery", "https stackoverflow", "ignore", "image", "implement", "import", "impute", "include", "increase", "index", "indicate", "individual", "input", "int", "integer", "interactive", "issue", "item", "join", "kaggle", "kept", "kernel", "key", "l1", "l2", "label", "lead", "learn", "learning", "learning_rate", "len", "let", "library", "line", "linear", "list", "look", "looking", "majority", "male", "map", "mapping", "match", "max", "max_depth", "max_features", "maximum", "mean", "meaning", "measure", "measurement", "median", "medical", "merge", "method", "might", "min", "missing", "missingno", "mistake", "mixture", "ml", "mode", "model", "most", "movie", "multicollinearity", "multiple", "mutual", "my", "name", "need", "negative", "no", "non", "normal", "not", "notebook", "null", "number", "numeric", "numerical", "object", "open", "operation", "opt", "order", "out", "outcome", "output", "overfit", "overfitting", "passenger", "pdf", "people", "per", "perform", "performance", "performing", "person", "pipeline", "place", "plot", "png", "poly", "population", "positive", "post", "potential", "power", "pre", "precision", "predict", "prediction", "predictor", "prefix", "preprocessing", "print", "probability", "problem", "processing", "profile", "provide", "pyplot", "python", "randint", "random", "range", "ranking", "rare", "ratio", "raw", "re", "record", "reduce", "reindex", "remove", "representation", "reshape", "response", "result", "return", "rich", "right", "row", "run", "running", "sample", "save", "scale", "scenario", "science", "scikit", "score", "second", "section", "selection", "sentence", "session", "set", "several", "shape", "sigmoid", "similar", "size", "skew", "sklearn", "slice", "sort", "sparse", "split", "sqrt", "squared", "stage", "standard", "start", "std", "step", "str", "string", "submission", "subset", "sum", "summary", "survival", "survived", "table", "target", "test", "testing", "text", "theory", "those", "through", "thumb", "ticket", "time", "titanic", "title", "total", "train", "training", "transform", "trend", "tutorial", "type", "under", "understanding", "uniform", "unique", "up", "update", "upper", "usage", "val", "validation", "value", "variability", "variable", "version", "visualization", "vote", "while", "who", "width", "wise", "work", "workflow", "world", "worst", "write", "xgboost"], "potential_description_queries_len": 371, "potential_script_queries": ["api", "heatmap", "matplotlib", "numpy", "random", "seaborn", "shapiro", "svm", "tqdm", "tree"], "potential_script_queries_len": 10, "potential_entities_queries": ["auto", "bin", "categorical", "column", "data", "def", "distance", "entropy", "family", "gamma", "gini", "high", "http", "import", "kernel", "l1", "linear", "male", "medical", "missing", "name", "normal", "poly", "problem", "random", "return", "science", "selection", "sigmoid", "standard", "test", "titanic", "uniform", "validation"], "potential_entities_queries_len": 34, "potential_extra_queries": ["bag"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 378}