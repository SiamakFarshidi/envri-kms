{"name": "train faster rcnn using keras ", "full_name": " h1 config h4 Explore data gen train ", "stargazers_count": 0, "forks_count": 0, "description": "com kishor1210 eda and data processing configParser the data from annotation fileDefine ROI Pooling Convolutional LayerVgg 16 modelRPN layerClassifier layerCalculate IoU Intersection of Union Calculate the rpn for all anchors of all imagesGet new image size and augment the imageGenerate the ground_truth anchorsDefine loss functions for all four outputs Explore data_gen_train data_gen_train is an generator so we get the data by calling next data_gen_train Build the ModelPart 1 https www. arange 0 r_epochs record_df curr_loss r plt. arange 0 r_epochs record_df loss_class_cls r plt. Here I am going to train Faster RCNN with 90 of images datasets. classes_count usask_1 5807 arvalis_1 45716 inrae_1 3701 ethz_1 51489 arvalis_3 16665 rres_1 9635 bg 0 class_mapping usask_1 0 arvalis_1 1 inrae_1 2 ethz_1 3 arvalis_3 4 rres_1 5 bg 6 Save the configuration Shuffle the images with seed Get train data generator which generate X Y image_data cv2. 7 color 1 Add text Draw positive anchors according to the y_rpn_regr cv2. arange 0 r_epochs record_df loss_class_regr c plt. csv file to record losses acc and mAP If this is a continued training load the trained model from before Load the records Training setting Just of sharing the karnel running with 2 epoch you try with min 20 epochs print Average number of overlapping bounding boxes from RPN for previous iterations. title total_loss plt. arange 0 r_epochs record_df elapsed_time r plt. title elapsed_time plt. arange 0 r_epochs record_df loss_rpn_cls b plt. com kishor1210 eda and data processingPart 2 https www. 5 color 1 define the base network VGG here can be Resnet50 Inception etc define the RPN built on the base layers 9 this is a model that holds both the RPN and the classifier used to load save weights for the models Because the google colab can only run the session several hours one time then you need to connect again we need to save the model and load the model to continue training If this is the begin of the training load the pre traind base network such as vgg 16 Create the record. 1 vgg16_weights_tf_dim_ordering_tf_kernels. classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes one hot code for bboxes from above x_roi X corresponding labels and corresponding gt bboxes 3 in here 3 in here A. 5 ya downscale iy 0. width num_anchors Might be 4 18 25 18 if resized image is 400 width and 300 A is the coordinates for 9 anchors for every point in the feature map all 18x25x9 4050 anchors cooridnates anchor_x 128 1 16 8 width of current anchor anchor_y 128 2 16 16 height of current anchor curr_layer 0 8 9 anchors the Kth anchor of all position in the feature map 9th in total shape 18 25 4 shape 4 18 25 Create 18x25 mesh grid For every point in x there are all the y points and vice versa X. Augmentation flag Augment with horizontal flips in training. format mean_overlapping_bboxes epoch_length Generate X x_img and label Y y_rpn_cls y_rpn_regr Train rpn model and get loss value _ loss_rpn_cls loss_rpn_regr Get predicted rpn from rpn model rpn_cls rpn_regr R bboxes shape 300 4 Convert rpn layer to roi bboxes note calc_iou converts from x1 y1 x2 y2 to x y w h format X2 bboxes that iou C. We need to download pretrained weight Pretrained Weight https github. com fchollet deep learning models releases tag v0. Code Credit https github. com kishor1210 train faster rcnn using kerasPart 3 comming soon. Don t need rpn probs in the later process Training data annotation file Number of RoIs to process at once. putText img gt bbox gt_x1 gt_y1 5 cv2. shape 18 25 Y. Where all the required data preprocessing I have done in Part 1 EDA and Data Processing Kernal https www. subplot 1 2 2 plt. title loss plt. com kentaroy47 frcnn from scratch with keras Thank you kentaroy for you nicely documented github repo. putText img pos anchor bbox str i 1 center 0 int anc_w 2 center 1 int anc_h 2 5 cv2. subplot 1 2 1 plt. randint 0 6 0 tall_imgs filename imageset trainval else tall_imgs filename imageset test make sure the bg class is last in the list x 0 is image with shape rows cols channels x 1 is roi with shape num_rois 4 with ordering x y w h Resized roi of the image to pooling size 7x7 Reshape to 1 num_rois pool_size pool_size nb_channels Might be 1 4 7 7 3 permute_dimensions is similar to transpose Block 1 Block 2 Block 3 Block 4 Block 5 x MaxPooling2D 2 2 strides 2 2 name block5_pool x out_roi_pool. Print the process or not Name of base network Setting for data augmentation Anchor box scales Note that if im_size is smaller anchor_box_scales should be scaled Original anchor_box_scales in the paper is 128 256 512 Anchor box ratios Size to resize the smallest side of the image Original setting in paper is 600. classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes Y1 one hot code for bboxes from above x_roi X Y2 corresponding labels and corresponding gt bboxes If X2 is None means there are no matching bboxes Find out the positive anchors and negative anchors If number of positive anchors is larger than 4 2 2 randomly choose 2 pos samples Randomly choose num_rois num_pos neg samples Save all the pos and neg samples in sel_samples in the extreme case where num_rois 1 we pick a random pos or neg sample training_data X X2 sel_samples labels Y1 sel_samples Y2 sel_samples X img_data resized image X2 sel_samples num_rois 4 in here bboxes which contains selected neg and pos Y1 sel_samples one hot encode for num_rois bboxes which contains selected neg and pos Y2 sel_samples labels and gt bboxes for num_rois bboxes which contains selected neg and pos plt. txt which you can find in part 1 kernal. 5 w h are the width and height of ground truth bbox wa ha are the width and height of anchor bboxe tx x xa wa ty y ya ha tw log w wa th log h ha all GT boxes should be mapped to an anchor box so we keep track of which anchor box was best we set the anchor to positive if the IOU is 0. com kishor1210 eda and data processing prerequisite We need to create annotation. FONT_HERSHEY_DUPLEX 0. shape 18 25 Calculate anchor position and size for each feature map point Top left x coordinate Top left y coordinate width of current anchor height of current anchor Apply regression to x y w and h if there is rpn regression layer Avoid width and height exceeding 1 Convert x y w h to x1 y1 x2 y2 x1 y1 is top left coordinate x2 y2 is bottom right coordinate Avoid bboxes drawn outside the feature map shape 4050 4 shape 4050 Find out the bboxes which is illegal and delete them from bboxes list Apply non_max_suppression Only extract the bboxes. 3 and RGB x is the difference between true value and predicted vaue absolute value of x If x_abs C. 7 it does not matter if there was another better box it just indicates overlap we update the regression layer target if this IOU is the best for the current x y and anchor position if the IOU is 0. figure figsize 15 5 plt. arange 0 r_epochs record_df loss_rpn_regr g plt. h5Part 1 https www. Record data used to save the losses classification accuracy and mean average precision Create the config This step will spend some time to load the data e. Augment with 90 degree rotations in training. Set to 300 in here to save training time image channel wise mean to subtract number of ROIs at once stride at the RPN this depends on the network configuration scaling the stdev overlaps for RPN overlaps for classifier ROIs placeholder for the class mapping automatically generated by the parser Print process Make sure the info saved in annotation file matching the format path_filename x1 y1 x2 y2 class_name Note tOne path_filename might has several classes class_name tx1 y1 x2 y2 are the pixel value of the origial image not the ratio value t x1 y1 top left coordinates x2 y2 bottom right coordinates x1 y1 t t t t t t t t t t t t t t t t t t t t t t t t t t t t t x2 y2 if np. shape 1 num_rois channels pool_size pool_size num_rois 4 7x7 roi pooling Flatten the convlutional layer and connected to 2 FC and 2 dropout There are two output layer out_class softmax acivation function for classify the class name of the object out_regr linear activation function for bboxes coordinates regression note no regression target for bg class a and b should be x1 y1 x2 y2 128 256 512 1 1 1 2 sqrt 2 2 sqrt 2 1 3x3 9 calculate the output map size based on the network architecture 3 initialise empty output objectives get the GT box coordinates and resize to account for image resizing get the GT box coordinates and resize to account for image resizing rpn ground truth x coordinates of the current anchor box t ignore boxes that go across image boundaries t t t t t y coordinates of the current anchor box ignore boxes that go across image boundaries bbox_type indicates whether an anchor should be a target Initialize with negative this is the best IOU for the x y coord and the current anchor note that this is different from the best IOU for a GT bbox get IOU of the current GT box and the current anchor box calculate the regression targets if they will be needed x y are the center point of ground truth bbox xa ya are the center point of anchor bbox xa downscale ix 0. Augment with vertical flips in training. shape 4 feature_map. ", "id": "kishor1210/train-faster-rcnn-using-keras", "size": "1312", "language": "python", "html_url": "https://www.kaggle.com/code/kishor1210/train-faster-rcnn-using-keras", "git_url": "https://www.kaggle.com/code/kishor1210/train-faster-rcnn-using-keras", "script": "Flatten sklearn.metrics keras.engine.topology nn_base division Dropout get_file build pyplot non_max_suppression_fast intersection __future__ layer_utils classifier_layer backend as K Config rpn_loss_cls_fixed_num keras.objectives = model_classifier.train_on_batch([X get_new_img_size optparse pyplot as plt union calc_iou get_source_inputs average_precision_score Layer numpy Input get_config iou tensorflow keras.utils pandas RoiPoolingConv(Layer) keras.optimizers categorical_crossentropy * K.sum(y_true[ keras.engine class_loss_regr_fixed_num OptionParser Model get_output_length matplotlib rpn_to_roi keras.models calc_rpn class_loss_regr compute_output_shape SGD InputSpec Adam Conv2D X2[ print_function rpn_loss_regr_fixed_num get_data RMSprop apply_regr GlobalMaxPooling2D generic_utils keras.utils.data_utils rpn_loss_regr augment MaxPooling2D backend apply_regr_np absolute_import __init__ keras.layers * K.mean(categorical_crossentropy(y_true[0 GlobalAveragePooling2D class_loss_cls rpn_layer TimeDistributed initializers get_anchor_gt Dense call get_img_output_length regularizers rpn_loss_cls keras ", "entities": "(('learning com fchollet deep models', 'tag v0'), 'release') (('We', 'Weight https pretrained weight Pretrained github'), 'need') (('com kentaroy47', 'github nicely documented repo'), 'frcnn') (('com kishor1210', 'faster kerasPart'), 'train') (('we', 'https ModelPart 1 www'), 'com') (('3', 'x'), 'be') (('90', 'datasets'), 'go') (('Training data annotation file Number', 'RoIs'), 'need') (('that', 'C.'), 'Generate') (('ratio t x1 y1 top', 'x2 y2 x1 y1 t t t t t t t t t t t t t t t t t t t t t t t t t t t t t x2 left bottom right y2'), 'Set') (('base pre network', '16 record'), 'be') (('which', 'X Y image_data cv2'), 'usask_1') (('IOU', 'anchor'), 'be') (('1 16 8 width', 'x'), 'be') (('which', 'selected neg'), 'y1') (('1 4 3 permute_dimensions', '2 2 1 Block 2 3 Block 4 Block 5 MaxPooling2D 2 2 name'), 'imageset') (('Original setting', 'paper'), 'print') (('which', 'Only bboxes'), 'shape') (('Where required I', 'Processing Kernal https Part 1 EDA www'), 'datum') (('you', 'previous iterations'), 'file') (('We', 'annotation'), 'com') (('IOU', 'current x y position'), '7') (('y', 'anchor bbox center xa'), 'channel') (('step', 'data e.'), 'use') ", "extra": "['annotation', 'test']", "label": "Perfect_files", "potential_description_queries": ["absolute", "acc", "account", "accuracy", "annotation", "architecture", "augment", "augmentation", "average", "bbox", "best", "bg", "bottom", "bounding", "box", "calculate", "case", "center", "channel", "choose", "classification", "classifier", "classify", "code", "colab", "color", "config", "create", "csv", "current", "data", "define", "degree", "difference", "download", "eda", "empty", "encode", "epoch", "every", "extract", "faster", "feature", "figure", "file", "filename", "find", "format", "function", "generate", "generated", "generator", "google", "grid", "ground", "gt", "height", "hot", "https 1 www", "ignore", "image", "img", "info", "int", "iou", "label", "layer", "learning", "left", "linear", "list", "load", "log", "map", "mapping", "matching", "mean", "mesh", "might", "min", "model", "name", "need", "neg", "negative", "network", "new", "next", "no", "not", "number", "object", "out", "output", "overlap", "parser", "part", "pixel", "point", "pooling", "position", "positive", "pre", "precision", "preprocessing", "pretrained", "print", "processing", "randint", "random", "ratio", "rcnn", "record", "regression", "resize", "right", "roi", "rpn", "run", "running", "sample", "save", "scaled", "scaling", "scratch", "selected", "session", "set", "several", "shape", "side", "similar", "size", "softmax", "sqrt", "step", "str", "stride", "subplot", "subtract", "tag", "target", "test", "text", "time", "title", "total", "track", "train", "training", "transpose", "try", "update", "value", "vertical", "weight", "width", "wise"], "potential_description_queries_len": 158, "potential_script_queries": ["backend", "build", "call", "division", "intersection", "matplotlib", "numpy", "plt", "pyplot", "tensorflow"], "potential_script_queries_len": 10, "potential_entities_queries": ["bbox", "center", "file", "pre", "right"], "potential_entities_queries_len": 5, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 166}