{"name": "hubmap read data and build tfrecords ", "full_name": " h1 Disclaimer h1 Objective h1 Reading the Images h1 Reading and Showing a sample image and mask h1 Tiling the Large Images into 512x512 tiles h1 Transforming the Tiles into a TFRecord Dataset h1 Verifying the TFRecord correctness ", "stargazers_count": 0, "forks_count": 0, "description": "IMPORTANT Note that in the case of a channel first TIFF number 3 first we need to swap the axis of the numpy array as noted below. To build the complete dataset I built one at the time. The id corresponds to the images provided. In the same fashion the code below scans the test directory and converts all the images there too. I will be using this dataset on my subsequent notebooks. Here is the function that takes an image slices into tiles calculates tile metadata and commits to storage. We can notice that black and white fall into the higher end of the spectrum. If we select 0 0 we expect to see a black tile. Tiling the Large Images into 512x512 tilesHere are some useful functions that use the numpy slicing capability to select specifc tiles of the image. org tutorials load_data tfrecord but you don t need to read all this in this Notebook I provide an example specific for image data that you can quickly customize. The strategy adopted in this Notebook is to tile the images in 512X512 tiles and then transforming the tiles into TFRecords such that we can later use them as input to train models using GPU or TPU accelerators. I will be using the json files The following function converts the polygons into a numpy boolean mask with the same shape as the image. If we observe the histogram we will notice that it will provide a useful way to filter black and white tiles later. com marcosnovaes hubmap looking at tfrecords https www. create a pandas dataframe to store metadata for each tile create one directory for each row of images create directory populate the metadata for this tile read json mask write the dataframe convert all the test images read json mask write the dataframe read back a record to make sure it the decoding works Create a dictionary describing the features. Libs used in this NotebookLet s look at the input data. But they have very large dimensions we would not be able to train a ML model at these dimensions. a Tile with coordinate 1 0 represents a tile with height offset 1 Tile Size in this case it starts at numpy coordinates 512 0 which means it is the tile below 0 0 a Tile with coordinate 0 1 represents a tile with wodth offset 1 Tile Size in this case it starts at numpy coordinates 0 512 which means it is the tile to the right of 0 0 As can be noticed in the sample image displayed there is a black border and then a lot of white surrounding the tissue. Using the TFRecord format for storing data should be easy but unfortunately it requires data serialization which complicates it a little bit. You CANNOT use reshape instead that will scramble the channels. Objective The objective of this notebook is to provide an example of how to transform the HubMAP Hacking the Kidney competition dataset into a form that can readily used to train models leveraging accelerators. Reading and Showing a sample image and maskThe next cells show the code that can read the first image in the csv file. Let s now derive a metric for masks so that in the future we can easily find tiles with gloms. The test images will be needed for evaluating the loss during training. If we move a little to the right and down we are then in the white zone. WARNING I ran out of memory trying to write both train and tests images in one session. We will also build a pandas dataframe that has the metadata for each tile including the lowpass energy and mask density metrics that we derived above. Voila Let s also verify that the tile metadata file is correct. Now let s move down the image by incrementing the height offset to 9 20 which should be the tile below 8 20 So the glom ends in that tile and there are fewer TRUE pixels. Black tiles have 0 pixels in the lower end while white actually dirty gray has only about 20 pixels in that region. Kaggle has a limit of 50 upper level directories so we will create one dir for each image. If we look back at the polygon dump above it shows that the first glom starts at pixel 10503 4384. So in the next section I takes the approach of tiling up the image and working with tiles. org api_docs python tf train Example format. If we divide both indexes by 512 we expect to find a glom in tile 8 20 Bingo We found our first glom. I was able to process all files and then uploaded the results to a Kaggle daset that I have made public Link to the TFRecord Dataset Produced by this Notebook. I am a coach for this competition the focus of my contributions is on helping users to leverage GCP components GCS TPUs BigQueryetc. I used this option instead in this Notebook. The images in this competition have very high resolution averaging 30 000 x 30 000 pixels and this presents a difficult challenge in memory management. The masks are provided in the csv files in RLE format but we are also provided json files that describe the mask as polygons. This function writes a tile to storage notice the GZIP compression this makes possible for all the tiles to be stored locally without exceeding the HD allowance of the Kaggle machine. Let s call this metric lowpass energy. As we explore the tiles I also calculate the tile histogram. Use my contributions at your own risk I don t garantee that they will help on winning any competition but I am hoping to learn by collaborating with everyone. 8 20 which we previously noticed to contain a glom. But in ML you only need to understand the TFExample https www. I used the tiffile library and it seems to read the images with no problem. It turns out that if we later select lowpass energy 100 we are garanteed to have actual tissue in the slide and we can discard anything with 100. My ideas and contributions represent my own opinion and are not representative of an official recommendation by Google. This is done using protocol buffers https developers. com protocol buffers and that is a bit of a learning curve. com marcosnovaes hubmap tfrecord 512 I have also developed a Notebook that explains how to use the TFRecord Dataset https www. This inline code will each image and mask in the train set swap axes when needed loading the image and mask into numpy arrays and then invoking the above function for each image mask pair. Reading the ImagesSome of the images are in TIFF format some are in BigTIFF. You can also easily costumize this Notebook if you want to produce tiles od different sizes I used 512x512 or if you want to include more metadata for each tile. There may be better ways to solving particular problems I welcome comments and suggestions. For each image you are provided a. It is just not possible to read them all in memory in the Kaggle environment and it is also not possible to build a model using the whole image as input. This metric will be used when we want to filter the training dataset to make sure it includes a certain number of tiles with gloms. For serialization using TFExample we have to make any data fit into either one of 3 types bytes_feature float_feature int_64_featureIn this Notebook and image and mask are passed as bytes_features and the other metadata as int_64. Going further down we find a cortex tile with no gloms. We then see that tiles with some actual tissue have a more even distribution. This will take a long time. So let s try the values 0 0 and 5 5 and we should be a black and a white tile respectively. Parse the input tf. So a Tile with coordinate 0 0 represents the first tile on the top left corner. This loop will take a long time as each image is read just so we can tell its shape. json file with polygon definitions. The next cell reads all tiffs and prints their shapes. When reading them we must check if the 3 is first and swap the axis as needed. It turns out that some TIFF images are channel first number 3 first and others channel last number 3 last. tiff image file and the Run Length Encoding Mask. In this Notebook I provide a little template code for dealing with TFExamples that can be quickly customized for any type of data. This notebooks provides some tips for reading the competitions images and masks and proposes a strategy to deal with the large sizes. Here is the code that takes care of the difference in shapes. org tutorials load_data tfrecord Let s read the tile with col offset 8 and row offset 20 i. Transforming the Tiles into a TFRecord DatasetWe are now ready to read all the images one at a time or we will run out of memory and then writing each tile to a TFRecord file. com marcosnovaes hubmap looking at tfrecords If you want to use the dataset without change you don t need to run the Notebook but do read through it because it provides a lot of insight on how the read the images masks and convert them to TFRecords. Disclaimer Hello Kagglers I am a Solution Architect with the Google Cloud Platform. Verify all train images were written to file. It also builds a pandas dataframe with the metadata for all tiles. We introduce here the read TFRecord functions that follow the model described in this tutorial https www. Simply counting the number of TRUE pixels in the mask is a great metric that indicate the tile contains a glom. Also I try to develop notebooks quickly in order to help users early in competitions. This template is explained in detail in this tutorial https www. This Notebook takes a long time to run because it processes all the competition files and the resulting compressed dataset is 18. verify that we can read all images select an image to investigate read json mask explore a few tiles Utilities serialize data into a TFRecord BytesList won t unpack a string from an EagerTensor. histogram function divides the color spectrum in 10 bins and shows how many pixels call within each bin. So now we know how to read each image and mask and that their types are uint8 and bool respectively. Example proto using the dictionary above. Find and read the competition train. This Notebook will tile the image using offsets for the height index and width index. 1G almost exceeding the Kaggle VM limit. And here is the white one 5 5 Now let s try to find a glomerulus. NOTE The numpy arrays have dimensions height width channels. But notice that the masks are also provided as a. in order to solve large problems. swapaxes function to change the shape using reshape will scramble the channels. IMPORTANT Notice that you need to use the numpy. Verify all images were written Verifying the TFRecord correctnessLet s now read back a sample tile to verify correctness. ", "id": "marcosnovaes/hubmap-read-data-and-build-tfrecords", "size": "10146", "language": "python", "html_url": "https://www.kaggle.com/code/marcosnovaes/hubmap-read-data-and-build-tfrecords", "git_url": "https://www.kaggle.com/code/marcosnovaes/hubmap-read-data-and-build-tfrecords", "script": "read_mask get_tile_mask imread _parse_image_function seaborn numpy show_tile_dist _bytes_feature _int64_feature image_example matplotlib.pyplot create_tfrecord tensorflow pandas imsave write_tfrecord_tiles get_tile _float_feature verify_read read_tf_dataset show_tile_and_mask ", "entities": "(('we', 'GPU accelerators'), 'be') (('we', 'dimensions'), 'have') (('I', 'tile also histogram'), 'calculate') (('then tiles', 'more even distribution'), 'see') (('Utilities', 'EagerTensor'), 'select') (('that', 'TFRecord Dataset https how www'), 'tfrecord') (('how many pixels', 'bin'), 'divide') (('I', 'Notebook'), 'be') (('this', 'memory management'), 'have') (('it', 'black tiles'), 'notice') (('I', 'tiles'), 'take') (('you', 'that'), 'tfrecord') (('Also I', 'early competitions'), 'try') (('how the', 'TFRecords'), 'hubmap') (('numpy arrays', 'dimensions height width channels'), 'NOTE') (('it', 'tissue'), 'represent') (('that', 'image'), 'tile') (('further down we', 'gloms'), 'find') (('we', 'gloms'), 'let') (('we', 'black tile'), 'expect') (('it', 'input'), 'be') (('calculates', 'storage'), 'be') (('test images', 'training'), 'need') (('0 0 we', 'values'), 'let') (('So Tile', 'top left corner'), 'represent') (('next cell', 'shapes'), 'read') (('data', 'bytes_features'), 'have') (('it', 'competition files'), 'take') (('tiles', 'Kaggle machine'), 'write') (('that', 'accelerators'), 'objective') (('that', 'csv file'), 'show') (('row', '20 i.'), 'let') (('I', 'one session'), 'warning') (('first glom', 'pixel'), 'dump') (('8 20 We', 'first glom'), 'expect') (('we', '100'), 'turn') (('here white 5 Now s', 'glomerulus'), 'be') (('function', 'channels'), 'scramble') (('that', 'data'), 'provide') (('just we', 'shape'), 'take') (('you', 'TFExample https only www'), 'need') (('that', 'learning curve'), 'com') (('template', 'https tutorial www'), 'explain') (('masks', 'also a.'), 'notice') (('We', 'spectrum'), 'notice') (('white actually dirty gray', 'region'), 'have') (('we', 'previously glom'), '20') (('org api_docs', 'train Example tf format'), 'python') (('inline code', 'image mask pair'), 'image') (('decoding works', 'features'), 'create') (('I', 'comments'), 'be') (('we', 'TFRecord file'), 'be') (('instead that', 'channels'), 'use') (('Notebook', 'height index index'), 'tile') (('tile', 'glom'), 'be') (('we', 'image'), 'have') (('includes', 'gloms'), 'use') (('following function', 'image'), 'use') (('which', 'it'), 'be') (('I', 'subsequent notebooks'), 'use') (('users', 'GCP components GCS TPUs BigQueryetc'), 'be') (('I', 'time'), 'build') (('It', 'tiles'), 'build') (('types', 'how image'), 'know') (('we', 'that'), 'build') (('that', 'shapes'), 'be') (('This', 'protocol buffers https developers'), 'do') (('that', 'polygons'), 'provide') (('ideas', 'Google'), 'represent') (('i d', 'images'), 'correspond') (('3', 'axis'), 'check') (('3 first we', 'numpy array'), 'note') (('you', 'tile'), 'costumize') (('Verify', 'correctness'), 'write') (('that', 'https tutorial www'), 'introduce') (('So glom', 'tile'), 'let') (('it', 'problem'), 'use') (('I', 'instead Notebook'), 'use') (('IMPORTANT you', 'numpy'), 'notice') (('some', 'BigTIFF'), 'format') (('we', 'white then zone'), 'be') (('I', 'everyone'), 'hope') (('notebooks', 'large sizes'), 'provide') (('Libs', 'input data'), 'look') (('Disclaimer Hello I', 'Solution Google Cloud Platform'), 'Kagglers') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["approach", "array", "bit", "bool", "boolean", "border", "build", "bytes_features", "calculate", "call", "care", "case", "cell", "challenge", "channel", "check", "code", "col", "color", "competition", "contain", "convert", "create", "csv", "data", "dataframe", "dataset", "derive", "describe", "detail", "develop", "dictionary", "difference", "dir", "directory", "end", "energy", "environment", "even", "explained", "explore", "fashion", "file", "filter", "find", "fit", "following", "form", "format", "found", "function", "future", "glom", "gray", "height", "help", "high", "histogram", "id", "image", "include", "including", "index", "indicate", "input", "json", "learn", "learning", "left", "let", "level", "library", "little", "look", "looking", "loop", "lot", "lower", "mask", "memory", "metadata", "metric", "model", "move", "my", "need", "next", "no", "not", "notebook", "number", "numpy", "objective", "official", "offset", "opinion", "option", "order", "out", "pixel", "polygon", "provide", "public", "python", "read", "reading", "recommendation", "record", "reshape", "resolution", "right", "risk", "row", "run", "sample", "section", "select", "serialize", "set", "shape", "slide", "spectrum", "storage", "store", "strategy", "string", "template", "test", "tf", "tfrecord", "through", "tiff", "tile", "time", "tissue", "train", "training", "transform", "try", "tutorial", "type", "unpack", "up", "upper", "verify", "while", "width", "write"], "potential_description_queries_len": 148, "potential_script_queries": ["imread", "imsave", "seaborn", "tensorflow"], "potential_script_queries_len": 4, "potential_entities_queries": ["height", "left", "tf", "width"], "potential_entities_queries_len": 4, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 151}