{"name": "ann with and without keras ", "full_name": " h1 Mushroom Classification with ANN h1 Introduction h4 On this notebook aim is understand how artificial neural network is working which methods are basically using background With this mushroom classification data we will classify mushrooms according to using artificial neural networks without using libraries After that we will make it with keras h4 First of all If you do not informed about logistic regression I highly recomend you to look at logistic regression first There is a kernel about it written by me In this notebook there are information about forward backward propagation creating models etc You can find it here h1 Read and Encode Data h4 Whole data is not null h4 Here i see bunch of unknown words and meaningless letters Lets start with know them and make them meanningfull h2 Column Description h4 For whom wanted to more know h4 class edible e poisonous p h4 cap shape bell b conical c convex x flat f knobbed k sunken s h4 cap surface fibrous f grooves g scaly y smooth s h4 cap color brown n buff b cinnamon c gray g green r pink p purple u red e white w yellow y h4 bruises bruises t no f h4 odor almond a anise l creosote c fishy y foul f musty m none n pungent p spicy s h4 gill attachment attached a descending d free f notched n h4 gill spacing close c crowded w distant d h4 gill size broad b narrow n h4 gill color black k brown n buff b chocolate h gray g green r orange o pink p purple u red e white w yellow y h4 stalk shape enlarging e tapering t h4 stalk root bulbous b club c cup u equal e rhizomorphs z rooted r missing h4 stalk surface above ring fibrous f scaly y silky k smooth s h4 stalk surface below ring fibrous f scaly y silky k smooth s h4 stalk color above ring brown n buff b cinnamon c gray g orange o pink p red e white w yellow y h4 stalk color below ring brown n buff b cinnamon c gray g orange o pink p red e white w yellow y h4 veil type partial p universal u h4 veil color brown n orange o white w yellow y h4 ring number none n one o two t h4 ring type cobwebby c evanescent e flaring f large l none n pendant p sheathing s zone z h4 spore print color black k brown n buff b chocolate h green r orange o purple u white w yellow y h4 population abundant a clustered c numerous n scattered s several v solitary y h4 habitat grasses g leaves l meadows m paths p urban u waste w woods d h4 Cap The cap is the top of the mushroom and often looks sort of like a small umbrella Mushroom caps can come in a variety of colors but most often are brown white or yellow h4 Gills Pores or Teeth These structures appear under the mushroom s cap They look similar to a fish s gills h4 Ring The ring sometimes called the annulus is the remaining structure of the partial veil after the gills have pushed through h4 Stem or Stipe The stem is the tall structure that holds the cap high above the ground h4 Volva The volva is the protective veil that remains after the mushroom sprouted up from the ground As the fungus grows it breaks through the volva h4 Spores Microscopic seeds acting as reproductive agents they are usually released into the air and fall on a substrate to produce a new mushroom h3 Label Encoding h4 Label encoder converts categorical datas to numeric datas which we need to work on h2 Train Test Split h1 ANN with Keras h4 Lets try to classify mushrooms edible or poisonous First we will make it with keras h3 Hyperparameters in ANN h4 There are some hyperparameters that must be chosen intuitively h4 Learning rate can be say like learning speed In other words Incremental step is stepping by the helping of learning rate There are multiple ways to select a good starting point for the learning rate A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training We might start with a large value like 0 1 then try exponentially lower values 0 01 0 001 etc h4 Number iterations is the number of batches of data the algorithm has seen or simply the number of passes the algorithm has done on the dataset Epochs is the number of times a learning algorithm sees the complete dataset Also it can be say like that how many times the model will make forward backward propagation compute cost predict values You will see in without keras part h4 Number of hidden units changeble from kind of problem If data is less complex and is having fewer dimensions or features then neural networks with 1 to 2 hidden layers would work If data is having large dimensions or features then to get an optimum solution 3 to 5 hidden layers can be used But regarding to problem this amount can be very big h4 Number of hidden units are like number of hidden layers If you have too few hidden units you will get high training error and high generalization error due to underfitting and high statistical bias If you have too many hidden units you may get low training error but still have high generalization error due to overfitting and high variance h4 An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network The purpose of the activation function is to introduce non linearity into the output of a neuron We know neural network has neurons that work in correspondence of weight bias and their respective activation function Most common activation functions are sigmoid tanh hiperbolic tangent relu swish These are changeble according to the problem So we call them hyperparameters But for general i can say that sigmoid is not using on hidden layers If it would be used it used in output layer For hidden units most common one is relu h4 Optimizers are like activation functions Some of them are stochastic gradient desctent RMSprop Adagrad momentum adam Each of them have advantages and disadvantages It would be nice if you know what are these advantages and disadvantages It would make sense to use the one that gives the optimum rate or accuracy at the optimum time I can say that most common one is adam Some advantages of Adam is that relatively low memory requirements and usually works well even with little tuning of hyperparameters h4 Some activation functions h4 I leave a gif link like this so that aim of the optimization methods can be visualized in your mind h2 Plot h1 ANN without Keras h4 ANN can be thought as multiple recurring logistic regression h4 In logistic regression there were input and output layers But in ANN there are input hidden at least 1 and output layers h4 Hidden layers nodes do not see inputs For this reason they called as hidden layers h4 Increasing on the number of layers increases nonlinearity This increases exploration However increasing number of layers is not increase accuracy all the time h4 Hidden units are the number of neurons or each node in a layer h4 In Deep Learning there are concept as feature engineering This changed with model architecture engineering Because deep learning models decide which feature important or non important Weights h4 In ANN input layers are not counted as layers If someone says 2 layers network you must understand that there are input layer 1 hidden and output layer h4 First part without keras we will constitute 2 layers network h4 Do not forget the equation z b w1 x1 w2 x2 wn xn h4 Activation z will be our y pred h2 2 Layer Neural Network h4 Path will be like that h2 Size of Layers and Initializing Parameters Weights and Bias h4 Weights and bias will be chosen randomly The simplest way to initialize weights and biases is to set those to small uniform random values which works well for neural networks with a single hidden layer h4 The arranements on the numbers are like that becasue of the matrix multiplication h2 Forward Propagation h4 Forward propagation is almost same with logistic regression The only difference is we use tanh function and we make all process twice For whom asking what is the difference between sigmoid and tanh function Sigmoid returns values 0 to 1 Tanh returns values 1 to 1 h4 For this stage as it was in logistic regression z1 is calculated with fimiliar expression z wx b after the it putted into an activation function tanh For other node it replied The only difference sigmoid is used Lastly probabilistic number from sigmoid is our output h2 Loss and Cost Function h4 Cost function helps us reach the optimal solution The cost function is the technique of evaluating the performance of our algorithm model It takes both predicted outputs by the model and actual outputs and calculates how much wrong the model was in its prediction h4 Cost function is sumation of each inputs loss h4 After forward propagation our model would be contrast the predicted values and true values After each forward propagation we will get new cost values h4 In this model we will use cross entropy loss h4 Cross Entropy Loss h2 Backward Propagation h4 Backward propagation is all about derivative Basically in neural networks you forward propagate to get the output and compare it with the real value to get the error Now to minimize the error you propagate backwards by finding the derivative of error with respect to each weight and then subtracting this value from the weight value The purpose of this calculations is optimize the algorithm h2 Update Parameters h4 This is updating equations By taking derivative of cost function according to weights and biases Then multiply it with \u03b1 learning rate Step by step model will be updated by this method Learning rate can be say like learning speed In other words Incremental step is stepping by the helping of learning rate h2 Prediction with Learnt Parameters h2 Create Model ", "stargazers_count": 0, "forks_count": 0, "description": "com feritebrargrler heart disease classification logistic regression. xn Activation z will be our y_pred 2 Layer Neural Network Path will be like that Size of layers and initializing parameters weights and bias Forward propagation Loss function and Cost function Backward propagation Update Parameters Prediction with learnt parameters weight and bias Create Model Size of Layers and Initializing Parameters Weights and Bias Weights and bias will be chosen randomly. Weights In ANN input layers are not counted as layers. png ANN can be thought as multiple recurring logistic regression. In this notebook there are information about forward backward propagation creating models etc. The only difference is we use tanh function and we make all process twice. For other node it replied. First of all If you do not informed about logistic regression I highly recomend you to look at logistic regression first. If someone says 2 layers network you must understand that there are input layer 1 hidden and output layer. Mushroom caps can come in a variety of colors but most often are brown white or yellow. Hidden units are the number of neurons or each node in a layer. After forward propagation our model would be contrast the predicted values and true values. The purpose of this calculations is optimize the algorithm. They look similar to a fish s gills. 001 etc Number iterations is the number of batches of data the algorithm has seen or simply the number of passes the algorithm has done on the dataset. Optimizers are like activation functions. These are changeble according to the problem. Ring The ring sometimes called the annulus is the remaining structure of the partial veil after the gills have pushed through. First we will make it with keras. This increases exploration. Some of them are stochastic gradient desctent RMSprop Adagrad momentum adam. png attachment 453a194d 89b8 4cef abf8 bac1871789d9. Volva The volva is the protective veil that remains after the mushroom sprouted up from the ground. Increasing on the number of layers increases nonlinearity. png attachment ff6f81d3 6217 40fb a402 516a25648d79. Train Test Split 5 1. If you have too many hidden units you may get low training error but still have high generalization error due to overfitting and high variance. For whom asking what is the difference between sigmoid and tanh function Sigmoid returns values 0 to 1. Then multiply it with \u03b1 learning rate. In Deep Learning there are concept as feature engineering. 1 then try exponentially lower values 0. But regarding to problem this amount can be very big. Cost function is sumation of each inputs loss. Hyperparameters in ANN There are some hyperparameters that must be chosen intuitively. It takes both predicted outputs by the model and actual outputs and calculates how much wrong the model was in its prediction. By taking derivative of cost function according to weights and biases. If it would be used it used in output layer. com search q woptimization deep learning gif tbm isch ved 2ahUKEwi88o2AidzxAhXIwKQKHRnZCbQQ2 cCegQIABAA oq woptimization deep learning gif gs_lcp CgNpbWcQA1D8lAFYxZsBYLmcAWgAcAB4AIABbIgBvgKSAQMwLjOYAQCgAQGqAQtnd3Mtd2l6LWltZ8ABAQ sclient img ei MnHrYLyRKciBkwWZsqegCw bih 610 biw 1366 imgrc nkJHfKXTjfXwGM imgdii ZGctRMduE6Oc7M like this so that aim of the optimization methods can be visualized in your mind. We might start with a large value like 0. But in ANN there are input hidden at least 1 and output layers. The arranements on the numbers are like that becasue of the matrix multiplication. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. Mushroom Classification with ANNContent 1. It would be nice if you know what are these advantages and disadvantages. Label Encoding 4 1. After each forward propagation we will get new cost values. Most common activation functions are sigmoid tanh hiperbolic tangent relu swish. In this model we will use cross entropy loss. png attachment 86c5010f a816 4ec5 92eb a0a6bad30e48. The only difference sigmoid is used. Train Test Split ANN with Keras Lets try to classify mushrooms edible or poisonous. read_csv Input data files are available in the read only. 5 our prediction is sign zero y_head 0 initialize parameters and layer sizes it will make forward backward prop. It would make sense to use the one that gives the optimum rate or accuracy at the optimum time. jfif For this stage as it was in logistic regression z1 is calculated with fimiliar expression z wx b after the it putted into an activation function tanh. Read and Encode Data Whole data is not null. So we call them hyperparameters. Cross Entropy Loss c_e_l. First part without keras we will constitute 2 layers network. However increasing number of layers is not increase accuracy all the time. jpg attachment dfaddaee 56b8 45c9 9fad 91dabe8d0727. Column Description 3 1. png attachment 603bc4b9 6b9b 4f27 bf40 c6021a0d2eff. Here i see bunch of unknown words and meaningless letters Lets start with know them and make them meanningfull. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session Label Encoding empty neural network layer constitution initialize neural network architecture kernel_initializer to initialize weights output layer epoch number of iteration batch size efers to the number of training examples utilized in one iteration. Also it can be say like that how many times the model will make forward backward propagation compute cost predict values You will see in without keras part Number of hidden units changeble from kind of problem. Plot ANN without Keras layers. A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training. An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network. Some advantages of Adam is that relatively low memory requirements and usually works well even with little tuning of hyperparameters Some activation functions act. Hidden layers nodes do not see inputs. ANN with Keras 6 Hyperparameters in ANN 7 Plot 8 1. The simplest way to initialize weights and biases is to set those to small uniform random values which works well for neural networks with a single hidden layer. Forward Propagation Forward propagation is almost same with logistic regression. This changed with model architecture engineering. jfif attachment 00fae41a 881c 420c ad5b a2b82c2e56e3. Step by step model will be updated by this method. If you have too few hidden units you will get high training error and high generalization error due to underfitting and high statistical bias. 5 our prediction is sign one y_head 1 if z is smaller than 0. I can say that most common one is adam. Basically in neural networks you forward propagate to get the output and compare it with the real value to get the error. For hidden units most common one is relu. Do not forget the equation z b w1. Read and Encode Data 2 1. Spores Microscopic seeds acting as reproductive agents they are usually released into the air and fall on a substrate to produce a new mushroom. compute cost update parameters by the amount of number of iterations Forward Propagation Compute Cost Backward Propagation Update Parameters Prediction Print train test Errors. Because deep learning models decide which feature important or non important. The cost function is the technique of evaluating the performance of our algorithm model. png Label encoder converts categorical datas to numeric datas which we need to work on. Tanh returns values 1 to 1 svst. Column Description For whom wanted to more know class edible e poisonous p cap shape bell b conical c convex x flat f knobbed k sunken s cap surface fibrous f grooves g scaly y smooth s cap color brown n buff b cinnamon c gray g green r pink p purple u red e white w yellow y bruises bruises t no f odor almond a anise l creosote c fishy y foul f musty m none n pungent p spicy s gill attachment attached a descending d free f notched n gill spacing close c crowded w distant d gill size broad b narrow n gill color black k brown n buff b chocolate h gray g green r orange o pink p purple u red e white w yellow y stalk shape enlarging e tapering t stalk root bulbous b club c cup u equal e rhizomorphs z rooted r missing stalk surface above ring fibrous f scaly y silky k smooth s stalk surface below ring fibrous f scaly y silky k smooth s stalk color above ring brown n buff b cinnamon c gray g orange o pink p red e white w yellow y stalk color below ring brown n buff b cinnamon c gray g orange o pink p red e white w yellow y veil type partial p universal u veil color brown n orange o white w yellow y ring number none n one o two t ring type cobwebby c evanescent e flaring f large l none n pendant p sheathing s zone z spore print color black k brown n buff b chocolate h green r orange o purple u white w yellow y population abundant a clustered c numerous n scattered s several v solitary y habitat grasses g leaves l meadows m paths p urban u waste w woods d Cap The cap is the top of the mushroom and often looks sort of like a small umbrella. As the fungus grows it breaks through the volva. ANN without Keras 9 2 Layer Neural Network 10 Size of layers and initializing parameters weights and bias 11 Forward propagation 12 Loss function and Cost function 13 Backward propagation 14 Update Parameters 15 Prediction with learnt parameters weight and bias 16 Create Model 17 Introduction On this notebook aim is understand how artificial neural network is working which methods are basically using background With this mushroom classification data we will classify mushrooms according to using artificial neural networks without using libraries. Learning rate can be say like learning speed. Number of hidden units are like number of hidden layers. Learning Rate Number of Iteration Number of Hidden Layers Number of Hidden Units Activation Functions Optimizer Learning rate can be say like learning speed. If data is less complex and is having fewer dimensions or features then neural networks with 1 to 2 hidden layers would work. In other words Incremental step is stepping by the helping of learning rate. png Prediction with Learnt Parameters Create Model This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. There are multiple ways to select a good starting point for the learning rate. There is a kernel about it written by me. png Backward Propagation Backward propagation is all about derivative. If data is having large dimensions or features then to get an optimum solution 3 to 5 hidden layers can be used. Stem or Stipe The stem is the tall structure that holds the cap high above the ground. The purpose of the activation function is to introduce non linearity into the output of a neuron. Loss and Cost Function Cost function helps us reach the optimal solution. In logistic regression there were input and output layers. Each of them have advantages and disadvantages. jpg Label Encoding le. You can find it here https www. Lastly probabilistic number from sigmoid is our output. Gills Pores or Teeth These structures appear under the mushroom s cap. For this reason they called as hidden layers. After that we will make it with keras. Update Parameters This is updating equations. png attachment f594fe50 07d4 4f19 91cf 55d09dd27199. But for general i can say that sigmoid is not using on hidden layers. We know neural network has neurons that work in correspondence of weight bias and their respective activation function. png I leave a gif link https www. Now to minimize the error you propagate backwards by finding the derivative of error with respect to each weight and then subtracting this value from the weight value. Epochs is the number of times a learning algorithm sees the complete dataset. Accurasy Loss 5 represents for 5 nodes z wx b Y real y values for normalization derivative with respect to y_head2 keepdims True writes results in to array x_test is a input for forward propagation if z is bigger than 0. ", "id": "feritebrargrler/ann-with-and-without-keras", "size": "12323", "language": "python", "html_url": "https://www.kaggle.com/code/feritebrargrler/ann-with-and-without-keras", "git_url": "https://www.kaggle.com/code/feritebrargrler/ann-with-and-without-keras", "script": "cross_val_score keras.layers train_test_split predict_ANN initialize_parameters_and_layer_sizes_ANN Sequential # empty neural network seaborn numpy backward_propagation_ANN Dense # layer constitution sklearn.model_selection LabelEncoder matplotlib.pyplot pandas update_parameters_ANN two_layer_neural_network forward_propagation_ANN compute_cost_ANN KerasClassifier sigmoid build_classifier sklearn.preprocessing keras.wrappers.scikit_learn keras.models ", "entities": "(('However increasing number', 'accuracy'), 'increase') (('sigmoid', 'hidden layers'), 'say') (('purpose', 'neuron'), 'be') (('3 to 5 hidden layers', 'then optimum solution'), 'use') (('that', 'optimum time'), 'make') (('they', 'hidden layers'), 'call') (('jpg attachment', '9fad 45c9 91dabe8d0727'), 'dfaddaee') (('one', 'training'), 'be') (('Backward propagation', 'bias Create Model Layers'), 'be') (('aim', 'mind'), 'ved') (('cap', 'high ground'), 'stem') (('arranements', 'matrix multiplication'), 'be') (('we', 'tanh function'), 'be') (('mushroom', 'ground'), 'Volva') (('you', 'weight value'), 'minimize') (('Increasing', 'nonlinearity'), 'increase') (('learning algorithm', 'complete dataset'), 'be') (('Lastly probabilistic number', 'sigmoid'), 'be') (('how weighted sum', 'network'), 'define') (('we', 'cross entropy loss'), 'use') (('Train Test Split ANN', 'mushrooms'), 'try') (('them', 'them'), 'see') (('it', 'volva'), 'break') (('u waste w woods m urban cap', 'often sort of small umbrella'), 'description') (('we', 'layers 2 network'), 'constitute') (('purpose', 'algorithm'), 'optimize') (('Hidden units', 'layer'), 'be') (('it', 'activation function tanh'), 'calculate') (('Learning Rate Number', 'speed'), 'say') (('you', 'error'), 'propagate') (('Optimizers', 'activation functions'), 'be') (('You', 'changeble kind problem'), 'say') (('Propagation Forward Forward propagation', 'almost logistic regression'), 'be') (('most common one', 'hidden units'), 'be') (('we', 'which'), 'convert') (('one y_head 1 z', '0'), '5') (('Some', 'them'), 'be') (('activation functions', 'hyperparameters'), 'be') (('input layers', 'layers'), 'count') (('z', '0'), 'represent') (('you', 'overfitting'), 'have') (('we', 'cost new values'), 'get') (('cost function', 'algorithm model'), 'be') (('that', 'ANN'), 'hyperparameter') (('Label neural network layer empty constitution', 'one iteration'), 'list') (('This', 'model architecture engineering'), 'change') (('you', 'generalization high underfitting high statistical bias'), 'have') (('we', 'libraries'), 'weight') (('it', 'backward prop'), '5') (('I', 'logistic regression'), 'recomend') (('Incremental step', 'rate'), 'step') (('you', 'layers 2 network'), 'understand') (('algorithm', 'dataset'), 'be') (('Cost function', 'inputs loss'), 'be') (('read_csv Input data files', 'read'), 'be') (('Number', 'hidden layers'), 'be') (('Each', 'advantages'), 'have') (('It', 'kaggle python Docker image https github'), 'Prediction') (('us', 'optimal solution'), 'help') (('Update This', 'equations'), 'Parameters') (('which', 'single hidden layer'), 'be') (('Learning rate', 'speed'), 'say') (('actual how much model', 'prediction'), 'take') (('Step', 'method'), 'update') (('png ANN', 'multiple recurring logistic regression'), 'think') (('that', 'weight bias'), 'know') (('they', 'new mushroom'), 'seed') (('These', 'problem'), 'be') (('it', 'output layer'), 'use') (('gills', 'sometimes remaining partial veil'), 'be') (('We', '0'), 'start') (('Gills structures', 'cap'), 'Pores') (('model', 'predicted values'), 'be') (('then neural 1 to 2 hidden layers', 'less fewer dimensions'), 'work') (('Mushroom caps', 'colors'), 'come') ", "extra": "['disease', 'test']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "air", "algorithm", "appear", "approach", "architecture", "array", "background", "backward", "batch", "best", "call", "categorical", "classification", "classify", "close", "color", "compare", "compute", "concept", "contrast", "cost", "create", "current", "data", "derivative", "difference", "directory", "disease", "empty", "encoder", "entropy", "environment", "epoch", "equal", "equation", "error", "even", "expression", "feature", "file", "find", "flat", "forward", "function", "general", "generalization", "gradient", "gray", "green", "heart", "high", "image", "img", "increase", "initialize", "input", "iteration", "kaggle", "kernel", "layer", "learning", "least", "leave", "linear", "link", "list", "little", "load", "look", "lower", "matrix", "memory", "might", "minimize", "missing", "model", "momentum", "most", "multiple", "naive", "need", "network", "neural", "new", "no", "node", "non", "none", "normalization", "not", "notebook", "number", "numeric", "optimization", "optimize", "output", "overfitting", "part", "partial", "performance", "pink", "png", "point", "population", "predict", "prediction", "print", "problem", "processing", "propagation", "purpose", "python", "random", "read", "reason", "regression", "run", "running", "search", "select", "sense", "session", "set", "several", "shape", "sigmoid", "sign", "similar", "single", "size", "smooth", "solution", "sort", "speed", "stage", "start", "stem", "step", "structure", "sum", "surface", "tanh", "technique", "test", "those", "thought", "through", "train", "training", "try", "tuning", "type", "under", "uniform", "up", "update", "value", "version", "weight", "work", "write"], "potential_description_queries_len": 162, "potential_script_queries": ["layer", "neural", "numpy", "seaborn"], "potential_script_queries_len": 4, "potential_entities_queries": ["data", "entropy", "high", "layer", "neural", "partial", "sort"], "potential_entities_queries_len": 7, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 164}