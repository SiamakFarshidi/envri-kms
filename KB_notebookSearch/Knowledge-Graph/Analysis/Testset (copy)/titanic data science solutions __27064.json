{"name": "titanic data science solutions ", "full_name": " h1 Titanic Data Science Solutions h3 This notebook is a companion to the book Data Science Solutions h2 Workflow stages h2 Question and problem definition h2 Workflow goals h2 Refactor Release 2017 Jan 29 h3 User comments h3 Porting issues h3 Best practices h2 Acquire data h2 Analyze by describing data h3 Assumtions based on data analysis h2 Analyze by pivoting features h2 Analyze by visualizing data h3 Correlating numerical features h3 Correlating numerical and ordinal features h3 Correlating categorical features h3 Correlating categorical and numerical features h2 Wrangle data h3 Correcting by dropping features h3 Creating new feature extracting from existing h3 Converting a categorical feature h3 Completing a numerical continuous feature h3 Create new feature combining existing features h3 Completing a categorical feature h3 Converting categorical feature to numeric h3 Quick completing and converting a numeric feature h2 Model predict and solve h3 Model evaluation h2 References ", "stargazers_count": 0, "forks_count": 0, "description": "Confirms our assumption for creating 4 fare ranges. This is required by most model algorithms. Correlating categorical featuresNow we can correlate categorical features with our solution goal. These are candidates for correcting goal. Most passengers 75 did not travel with parents or children. We should consider Age our assumption classifying 2 in our model training. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Wrangle prepare cleanse the data. org wiki Standard_deviation. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correcting by dropping featuresThis is a good starting goal to execute. We may also add to our assumptions based on the problem description noted earlier. org wiki Median values for Age across sets of Pclass and Gender feature combinations. We may analyze data before and after wrangling. Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster can our model determine based on a given test dataset not containing the survival information if these passengers in the test dataset survived or not. Our problem is a classification and regression problem. We can consider correlating Embarked Categorical non numeric Sex Categorical non numeric Fare Numeric continuous with Survived Categorical numeric. com Data Science Solutions Startup Workflow dp 1520545312. Nearly 30 of the passengers had siblings and or spouse aboard. 62 knowing our problem description mentions 38 survival rate. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. Most passengers in Pclass 1 survived. We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. Correlating numerical featuresLet us start by understanding correlations between numerical features and our solution goal Survived. Quick completing and converting a numeric featureWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. Women Sex female were more likely to have survived. This will enable us to drop Parch and SibSp from our datasets. A simple way is to generate random numbers between mean and standard deviation https en. Ticket is a mix of numeric and alphanumeric data types. More accurate way of guessing missing values is to use other correlated features. The completion goal achieves desired requirement for model algorithm to operate on non null values. Within categorical features are the values nominal ordinal ratio or interval based Among other things this helps us select the appropriate plots for visualization. We decide to include this feature in our model. Analyze identify patterns and explore the data. Complete and add Embarked feature to model training. Doing so will also help us in achieving the feature completing goal. So instead of guessing age values based on median use random numbers between mean and standard deviation based on sets of Pclass and Gender combinations. FacetGrid train_df col Pclass hue Gender age_mean guess_df. However there are use cases with exceptions. Certain titles mostly survived Mme Lady Sir or did not Don Rev Jonkheer. Acquire training and testing data. Higher fare paying passengers had better survival. Within numerical features are the values discrete continuous or timeseries based Among other things this helps us select the appropriate plots for visualization. Assumtions based on data analysisWe arrive at following assumptions based on data analysis done so far. In pattern recognition the k Nearest Neighbors algorithm or k NN for short is a non parametric method used for classification and regression. Pclass We observe significant correlation 0. We want to do this early in our project and match these quick correlations with modelled correlations later in the project. The next model Random Forests is one of the most popular. org wiki Random_forest. It may be best to derive a feature or a set of features from these individual features creating 1. Alternatively several passengers shared a cabin. Infants Age 4 had high survival rate. We do this in a single line of code. We may also want round off the fare to two decimals as it represents currency. Seven features are integer or floats. Let us replace Age with ordinals based on these bands. When we plot Title Age and Survived we note the following observations. SibSp and Parch These features have zero correlation for certain values. So is Title as second highest positive correlation. Analyze by pivoting featuresTo confirm some of our observations and assumptions we can quickly analyze our feature correlations by pivoting features against each other. The model generated confidence score is the lowest among the models evaluated so far. org wiki Correlation among a feature and solution goal As the feature values change does the solution state change as well and visa versa This can be tested both for numerical and categorical features in the given dataset. Confirms our classifying assumption 2. Perform a stage earlier than indicated. We may want to create new feature for Age bands. We can only do so at this stage for features which do not have any empty values. We should band age groups creating 3. matches the first word which ends with a dot character within Name feature. Tree models where the target variable can take a finite set of values are called classification trees in these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Create new feature combining existing featuresWe can create a new feature for FamilySize which combines Parch and SibSp. Continous Age Fare. One can approach the problem based on available features within the training dataset. By dropping features we are dealing with fewer data points. We also do not need the PassengerId feature in the training dataset. It also makes sense doing so only for features which are categorical Sex ordinal Pclass or discrete SibSp Parch type. So median Age for Pclass 1 and Gender 0 Pclass 1 and Gender 1 and so on. We want to know how well does each feature correlate with Survival. Confirms correlating 1 and completing 2. We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. Given a set of training samples each marked as belonging to one or the other of two categories an SVM training algorithm builds a model that assigns new test samples to one category or the other making it a non probabilistic binary linear classifier. Ticket feature may be dropped from our analysis as it contains high ratio of duplicates 22 and there may not be a correlation between Ticket and survival. Correlating categorical and numerical featuresWe may also want to correlate categorical features with non numeric values and numeric features. Best practices Performing feature correlation analysis early in the project. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. Drop a stage altogether. thanks Sharan Naribole Correct observation nearly 30 of the passengers had siblings and or spouses aboard. Review Parch distribution using percentiles. This helps us answer questions relating to specific bands Did infants have better survival rate Note that x axis in historgram visualizations represents the count of samples or passengers. Let us create Age bands and determine correlations with Survived. We may validate these assumptions further before taking appropriate actions. Converting a categorical featureNow we can convert features which contain strings to numerical values. We can consider three methods to complete a numerical continuous feature. This can be done by calculating the coefficient of the features in the decision function. Convert the Fare feature to ordinal values based on the FareBand. The model confidence score is the highest among models evaluated so far. Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. A sample is classified by a majority vote of its neighbors with the sample being assigned to the class most common among its k nearest neighbors k is a positive integer typically small. 69 Age and Fare. Consider banding Fare feature. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. Which features are available in the dataset Noting the feature names for directly manipulating or analyzing these. 5 among Pclass 1 and Survived classifying 3. Now we can safely drop the Name feature from training and testing datasets. This simple analysis confirms our assumptions as decisions for subsequent workflow stages. 7 to Kaggle kernel 3. Here are the highlights to note. The upper class passengers Pclass 1 were more likely to have survived. Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations. Most passengers are in 15 35 age range. This turns a continous numerical feature into an ordinal categorical feature. Using multiple plots instead of overlays for readability. PassengerId may be dropped from training dataset as it does not contribute to survival. We can convert the categorical titles to ordinal. There are 60 predictive modelling algorithms to choose from. KNN confidence score is better than Logistics Regression but worse than SVM. The RegEx pattern w. Decision trees where the target variable can take continuous values typically real numbers are called regression trees. We may analyze by visualizing data. Exception in Embarked C where males had higher survival rate. The results from multiple executions might vary. Question and problem definitionCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. Inversely as Pclass increases probability of Survived 1 decreases the most. Note that where applicable we perform operations on both training and testing datasets together to stay consistent. It is a type of linear classifier i. Name feature may contain errors or typos as there are several ways used to describe a name including titles round brackets and quotes used for alternative or short names. This result only accounts for part of the submission dataset. This way Age Class is a good artificial feature to model as it has second highest negative correlation with Survived. The perceptron is an algorithm for supervised learning of binary classifiers functions that can decide whether an input represented by a vector of numbers belongs to some specific class or not. We may also want to understand the implications or correlation of different classes with our solution goal. Cabin Age Embarked features contain a number of null values in that order for the training dataset. With these two criteria Supervised Learning plus Classification and Regression we can narrow down our choice of models to a few. Add Sex feature to model training. While both Decision Tree and Random Forest score the same we choose to use Random Forest as they correct for decision trees habit of overfitting to their training set. Survival among Title Age bands varies slightly. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived not necessarily direct correlation between Embarked and Survived. Pclass 3 had most passengers however most did not survive. Most titles band Age groups accurately. Converting categorical feature to numericWe can now convert the EmbarkedFill feature by creating a new numeric Port feature. We can also create an artificial feature combining Pclass and Age. Infant passengers in Pclass 2 and Pclass 3 mostly survived. FacetGrid train_df col Embarked hue Survived palette 0 k 1 w grid sns. For modeling stage one needs to prepare the data. Correlating certain features may help in creating completing or correcting features. Note that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. In machine learning naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes theorem with strong naive independence assumptions between the features. We may want to classify or categorize our samples. Embarked takes three possible values. Can we create new features based on an existing feature or a set of features such that the new feature follows the correlation conversion completeness goals. Positive coefficients increase the log odds of the response and thus increase the probability and negative coefficients decrease the log odds of the response and thus decrease the probability. We will prefer method 2. We can create another feature called IsAlone. Name feature is relatively non standard may not contribute directly to survival so maybe dropped. Categorical Survived Sex and Embarked. Any suggestions to improve our score are most welcome. Which features within the dataset contribute significantly to our solution goal Statistically speaking is there a correlation https en. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. Sex We confirm the observation during problem definition that Sex female had very high survival rate at 74 classifying 1. Let us start by converting Sex feature to a new feature called Gender where female 1 and male 0. In our case we note correlation among Age Gender and Pclass. Consider Pclass for model training. Which features are numerical Which features are numerical These values change from sample to sample. Six in case of test dataset. Let us now execute our decisions and assumptions for correcting creating and completing goals. The expand False flag returns a DataFrame. Our training dataset has two missing values. We may combine mulitple workflow stages. The question or problem definition for Titanic Survival competition is described here at Kaggle https www. Children Age were more likely to have survived. Now we iterate over Sex 0 or 1 and Pclass 1 2 3 to calculate guessed values of Age for the six combinations. 5 age Logistic Regression Support Vector Machines Gaussian Naive Bayes Perceptron Linear SVC Stochastic Gradient Descent Decision Tree Random Forest submission. Completing a categorical featureEmbarked feature takes S Q C values based on port of embarkation. What is the distribution of numerical feature values across the samples This helps us determine among other early insights how representative is the training dataset of the actual problem domain. Based on our assumptions and decisions we want to drop the Cabin correcting 2 and Ticket correcting 1 features. com c titanic details getting started with random forests Titanic Best Working Classifier https www. We may also want to develop some early understanding about the domain of our problem. Visualize report and present the problem solving steps and final solution. So far we did not have to change a single feature or value to arrive at these. Not bad for our first attempt. ReferencesThis notebook has been created based on great work done solving the Titanic competition and other sources. So for instance converting text categorical values to numeric values. Total samples are 891 or 40 of the actual number of passengers on board the Titanic 2 224. Data preparation may also require us to estimate any missing values within a feature. Which features contain blank null or empty values These will require correcting. How to select the right visualization plots and charts depending on nature of the data and the solution goals. Completing a numerical continuous featureNow we should start estimating and completing features with missing or null values. com sinakhorami titanic titanic best working classifier data analysis and wrangling visualization machine learning preview the data Review survived rate using percentiles. Which features are categorical These values classify the samples into sets of similar samples. Correlating numerical and ordinal featuresWe can combine multiple features for identifying correlations using a single plot. If k 1 then the object is simply assigned to the class of that single nearest neighbor. A journey through Titanic https www. Fares varied significantly with few passengers 1 paying as high as 512. FacetGrid train_df col Embarked grid sns. Which features are mixed data types Numerical alphanumeric data within same feature. The notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle. Survived is a categorical feature with 0 or 1 values. Further qualifies our classifying assumption 2. We can replace many titles with a more common name or classify them as Rare. Note the confidence score generated by the model based on our training dataset. Perform a stage multiple times in our workflow. A histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. Which features may contain errors or typos This is harder to review for a large dataset however reviewing a few samples from a smaller dataset may just tell us outright which features may require correcting. Random forests or random decision forests are an ensemble learning method for classification regression and other tasks that operate by constructing a multitude of decision trees n_estimators 100 at training time and outputting the class that is the mode of the classes classification or mean prediction regression of the individual trees. We decide to use this model s output Y_pred for creating our competition submission of results. Our submission to the competition site Kaggle results in scoring 3 883 of 6 082 competition entries. There are several excellent notebooks to study data science competition entries. Model algorithms may work best when there are no missing values. One way to do this is to detect any outliers among our samples or features. std age_guess rnd. thanks Reinhard Correctly interpreting logistic regresssion coefficients. Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset. These include Logistic Regression KNN or k Nearest Neighbors Support Vector Machines Naive Bayes classifier Decision Tree Random Forrest Perceptron Artificial neural network RVM or Relevance Vector MachineLogistic Regression is a useful model to run early in the workflow. The algorithm allows for online learning in that it processes elements in the training set one at a time. Refactor Release 2017 Jan 29We are significantly refactoring the notebook based on a comments received by readers b issues in porting notebook from Jupyter kernel 2. Acquire dataThe Python Pandas packages helps us work with our datasets. Few elderly passengers 1 within age range 65 80. Guess Age values using median https en. We simply fill these with the most common occurance. Visualize stage may be used multiple times. What are the data types for various features Helping us during converting goal. Translated 32 survival rate. Discrete SibSp Parch. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results. Oldest passengers Age 80 survived. We want to identify relationship between output Survived or not with other variables or features Gender Age Port. Confirms our classifying assumption 3. Reference Wikipedia https en. Analyze by describing dataPandas also helps describe the datasets answering following questions early in our project. Method 1 and 3 will introduce random noise into our models. User comments Combine training and test data for certain operations like converting titles across dataset to numerical values. Confirms classifying 1. We start by acquiring the training and testing datasets into Pandas DataFrames. S port used by most passengers top S Ticket feature has high ratio 22 of duplicate values unique 681. Speeds up our notebook and eases the analysis. 5 and c review of few more best practice kernels. For example Master title has Age mean of 5 years. org wiki Decision_tree_learning. org wiki K nearest_neighbors_algorithm. In the following code we extract Title feature using regular expressions. Workflow stagesThe competition solution workflow goes through seven stages described in the Data Science Solutions book. These feature names are described on the Kaggle data page here https www. Let us drop Parch SibSp and FamilySize features in favor of IsAlone. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. On April 15 1912 during her maiden voyage the Titanic sank after colliding with an iceberg killing 1502 out of 2224 passengers and crew. Note that the model generates a confidence score which is higher than Logistics Regression model. We may want to complete Age feature as it is definitely correlated to survival. We can not create FareBand. FacetGrid train_df col Pclass hue Survived grid sns. Titanic Data Science Solutions This notebook is a companion to the book Data Science Solutions https www. Creating new feature extracting from existingWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival before dropping Name and PassengerId features. uniform age_mean age_std age_mean age_std Convert random age float to nearest. We may not need supply stage to productize or service enable our dataset for a competition. Model predict and solve the problem. Large number of 15 25 year olds did not survive. We also combine these datasets to run certain operations on both datasets together. Question or problem definition. Cabin Age are incomplete in case of test dataset. We may want to complete the Embarked feature as it may also correlate with survival or another important feature. Workflow goalsThe data science solutions workflow solves for seven major goals. Analyze by visualizing dataNow we can continue confirming some of our assumptions using visualizations for analyzing the data. org wiki Support_vector_machine. com omarelgabry titanic a journey through titanic Getting Started with Pandas Kaggle s Titanic Competition https www. Port of embarkation correlates with survival rates. Cabin is alphanumeric. Model evaluationWe can now rank our evaluation of all the models to choose the best one for our problem. org wiki Logistic_regression. The objective of this notebook is to follow a step by step workflow explaining each step and rationale for every decision we take during solution development. Complete the Age feature for null values completing 1. Combine methods 1 and 2. Logistic regression measures the relationship between the categorical dependent variable feature and one or more independent variables features by estimating probabilities using a logistic function which is the cumulative logistic distribution. We decide to retain the new Title feature for model training. Sex is highest positivie coefficient implying as the Sex value increases male 0 to female 1 the probability of Survived 1 increases the most. org wiki Naive_Bayes_classifier. This result is indicative while the competition is running. We may want to engineer the Name feature to extract Title as a new feature. Naive Bayes classifiers are highly scalable requiring a number of parameters linear in the number of variables features in a learning problem. Model predict and solveNow we are ready to train a model and predict the required solution. We may also want to create a Fare range feature if it helps our analysis. Female passengers had much better survival rate than males. Supply or submit the results. Ports of embarkation have varying survival rates for Pclass 3 and among male passengers. We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board. Males had better survival rate in Pclass 3 when compared with Pclass 2 for C and Q ports. Around 38 samples survived representative of the actual survival rate at 32. Wrangle dataWe have collected several assumptions and decisions regarding our datasets and solution requirements. What is the distribution of categorical features Names are unique across the dataset count unique 891 Sex variable as two possible values with 65 male top male freq 577 count 891. The workflow indicates general sequence of how each stage may follow the other. We can not remove the AgeBand feature. This model uses a decision tree as a predictive model which maps features tree branches to conclusions about the target value tree leaves. mean age_std guess_df. This is described on the Kaggle competition description page here https www. 8 SibSp distribution. Cabin values have several dupicates across samples. thanks Reinhard Porting issues Specify plot dimensions bring legend into plot. This can be done with numerical and categorical features which have numeric values. Although there was some element of luck involved in surviving the sinking some groups of people were more likely to survive than others such as women children and the upper class. We will first do this for the Age feature. Pclass varies in terms of Age distribution of passengers. Five features are strings object. ", "id": "startupsci/titanic-data-science-solutions", "size": "27064", "language": "python", "html_url": "https://www.kaggle.com/code/startupsci/titanic-data-science-solutions", "git_url": "https://www.kaggle.com/code/startupsci/titanic-data-science-solutions", "script": "sklearn.naive_bayes sklearn.tree random KNeighborsClassifier DecisionTreeClassifier seaborn numpy LinearSVC SGDClassifier sklearn.ensemble RandomForestClassifier matplotlib.pyplot Perceptron pandas LogisticRegression sklearn.neighbors SVC sklearn.linear_model GaussianNB sklearn.svm ", "entities": "(('we', 'solution goal'), 'correlate') (('One way', 'samples'), 'be') (('we', 'Age Gender'), 'note') (('Cabin Age', 'test dataset'), 'be') (('us', 'IsAlone'), 'let') (('which', 'Parch'), 'create') (('Bayes naive classifiers', 'features'), 'be') (('feature names', 'Kaggle data page'), 'describe') (('features', 'certain values'), 'SibSp') (('ReferencesThis notebook', 'Titanic competition'), 'create') (('it', 'also survival'), 'want') (('supervised we', 'given dataset'), 'perfome') (('Analyze', 'data'), 'identify') (('We', 'numerical continuous feature'), 'consider') (('Port', 'survival rates'), 'correlate') (('Names', '65 male top male freq'), 'be') (('we', 'other'), 'confirm') (('Nearest Neighbors Support Vector Naive Bayes classifier Decision Tree Random Forrest network Relevance Vector MachineLogistic Perceptron neural Regression', 'useful early workflow'), 'include') (('We', 'feature'), 'create') (('We', 'results'), 'decide') (('empty These', 'blank null'), 'feature') (('Nearly 30', 'siblings'), 'have') (('training dataset', 'two missing values'), 'have') (('it', '22 Ticket'), 'drop') (('shipwreck', 'enough passengers'), 'be') (('Cabin values', 'samples'), 'have') (('Few elderly passengers', '65 80'), 'range') (('Workflow stagesThe competition solution workflow', 'Data Science Solutions book'), 'go') (('simple analysis', 'workflow subsequent stages'), 'confirm') (('Master title', '5 years'), 'have') (('where applicable we', 'datasets'), 'note') (('We', 'Age bands'), 'want') (('Survival', 'Title Age bands'), 'vary') (('We', 'Gender Age Port'), 'want') (('how stage', 'other'), 'indicate') (('which', 'numerical values'), 'convert') (('that', 'classification analysis'), 'model') (('histogram', 'automatically defined bins'), 'indicate') (('This', 'ordinal categorical feature'), 'turn') (('we', 'data'), 'continue') (('Female passengers', 'males'), 'have') (('we', 'following observations'), 'note') (('Sex female', '74'), 'confirm') (('We', 'Age feature'), 'do') (('We', 'samples'), 'want') (('Wrangle dataWe', 'datasets'), 'collect') (('This', 'Kaggle competition description page'), 'describe') (('input', 'specific class'), 'be') (('expand False flag', 'DataFrame'), 'return') (('Titanic Data Science notebook', 'book Data Science Solutions https www'), 'Solutions') (('We', 'Pandas DataFrames'), 'start') (('which', 'Name feature'), 'match') (('which', 'logistic function'), 'measure') (('We', 'solution goal'), 'want') (('Pclass', 'passengers'), 'vary') (('Converting', 'numeric Port new feature'), 'convert') (('We', 'training dataset'), 'need') (('Total samples', 'board'), 'be') (('We', 'errors'), 'analyze') (('assumption', 'model training'), 'consider') (('Name feature', 'alternative names'), 'contain') (('More accurate way', 'other correlated features'), 'be') (('one', 'data'), 'need') (('k', 'most common k nearest neighbors'), 'classify') (('it', 'definitely survival'), 'want') (('numerical values', 'sample'), 'be') (('Name feature', 'relatively non directly survival'), 'be') (('visa as well This', 'given dataset'), 'do') (('we', 'required solution'), 'predict') (('we', 'missing values'), 'start') (('Higher fare', 'better survival'), 'have') (('Now we', 'training datasets'), 'drop') (('KNN confidence score', 'SVM'), 'be') (('we', 'data fewer points'), 'deal') (('it', 'one category'), 'mark') (('submission', '6 competition 082 entries'), 'result') (('which', 'Logistics Regression model'), 'note') (('Ticket', 'data numeric types'), 'be') (('Sex', 'new feature'), 'let') (('Infant passengers', 'Pclass'), 'survive') (('it', 'time'), 'allow') (('we', 'which'), 'understand') (('where males', 'survival higher rate'), 'exception') (('features', 'solution significantly goal'), 'be') (('Correlating', 'non numeric values'), 'want') (('We', 'new feature'), 'want') (('passengers', 'test dataset'), 'know') (('predictions', 'feature vector'), 'algorithm') (('This', 'necessarily direct Embarked'), 'be') (('1 probability', '1 most'), 'be') (('We', 'Pclass'), 'create') (('values', 'similar samples'), 'classify') (('Correlating', 'features'), 'help') (('This', 'datasets'), 'enable') (('We', 'subsequent goals'), 'want') (('We', 'categorical titles'), 'convert') (('It', '1'), 'be') (('typically real numbers', 'continuous values'), 'call') (('notebook', 'Kaggle'), 'walk') (('Method', 'models'), 'introduce') (('Name feature', 'Name'), 'want') (('we', 'regular expressions'), 'extract') (('Large number', 'year 15 25 olds'), 'survive') (('where banding', 'useful patterns'), 'be') (('such new feature', 'correlation conversion completeness goals'), 'create') (('us', 'training problem how actual domain'), 'be') (('We', 'most common occurance'), 'fill') (('We', 'model'), 'decide') (('Naive Bayes classifiers', 'learning problem'), 'be') (('outright features', 'just us'), 'contain') (('which', 'so only features'), 'make') (('We', 'goals'), 'use') (('We', 'problem description'), 'add') (('Data preparation', 'feature'), 'require') (('us', 'Survived'), 'let') (('model confidence score', 'models'), 'be') (('us', 'goals'), 'let') (('We', 'model training'), 'decide') (('Pclass We', 'significant correlation'), 'observe') (('it', 'analysis'), 'want') (('Supervised we', 'few'), 'narrow') (('Reinhard Porting plot Specify dimensions', 'plot'), 'thank') (('We', 'Survival'), 'want') (('We', 'Rare'), 'replace') (('Now we', 'six combinations'), 'iterate') (('us', 'visualization'), 'be') (('data types', 'goal'), 'be') (('question', 'Kaggle https here www'), 'describe') (('that', 'most frequently feature'), 'complete') (('simple way', 'deviation https mean en'), 'be') (('us', 'Pclass Gender combinations'), 'let') (('maps', 'target value tree leaves'), 'use') (('S port', '22 duplicate values'), 'have') (('We', 'datasets'), 'combine') (('good artificial it', 'Survived'), 'be') (('us', 'bands'), 'let') (('it', 'significantly results'), 'discard') (('Certain titles', 'Mme Lady mostly Sir'), 'survive') (('it', 'survival'), 'drop') (('Random next model Forests', 'most popular'), 'be') (('Positive coefficients', 'thus probability'), 'increase') (('suggestions', 'score'), 'be') (('features', 'data mixed Numerical alphanumeric same feature'), 'be') (('Question', 'test dataset'), 'define') (('us', 'datasets'), 'help') (('notebooks', 'experts'), 'skip') (('uniform age_mean', 'nearest'), 'age_mean') (('Correlating', 'single plot'), 'combine') (('completion goal', 'non null values'), 'achieve') (('which', 'numeric values'), 'do') (('Correcting', 'featuresThis'), 'be') (('Fares', '1 as high 512'), 'varied') (('Cabin Age Embarked features', 'training dataset'), 'contain') (('it', 'training'), 'drop') (('Most passengers', '75 parents'), 'travel') (('Model', 'problem'), 'rank') (('We', 'later project'), 'want') (('features', 'directly these'), 'be') (('probability', '1 most'), 'increase') (('Males', '2 C ports'), 'have') (('axis', 'samples'), 'help') (('So Title', 'second highest positive correlation'), 'be') (('Doing', 'feature completing goal'), 'help') (('they', 'training set'), 'score') (('nearly 30', 'siblings'), 'thank') (('So far we', 'these'), 'have') (('it', 'currency'), 'want') (('Completing', 'embarkation'), 'take') (('problem description', 'survival 38 rate'), 'mention') (('Survived', 'categorical 0 values'), 'be') (('User comments', 'numerical values'), 'Combine') (('that', 'class labels'), 'call') (('We', 'problem'), 'want') (('we', 'only single value'), 'note') (('Refactor Jan 2017 29We', 'Jupyter kernel'), 'Release') (('This', 'decision function'), 'do') (('We', 'board'), 'want') (('Assumtions', 'data analysis'), 'arrive') (('This', 'submission dataset'), 'result') (('This', 'model most algorithms'), 'require') (('that', 'individual trees'), 'be') (('Analyze', 'early project'), 'help') (('non numeric Fare Sex Numeric', 'Categorical Survived numeric'), 'consider') (('us', 'numerical features'), 'survive') (('Ports', '3 male passengers'), 'have') (('Around 38 samples', '32'), 'survive') (('model algorithm one', 'numerical equivalent values'), 'require') (('service', 'competition'), 'need') (('we', 'solution development'), 'be') (('Alternatively several passengers', 'cabin'), 'share') (('which', 'empty values'), 'do') (('results', 'multiple executions'), 'vary') (('We', 'data'), 'analyze') (('confidence score', 'models'), 'generate') (('we', '1 features'), 'want') (('wrangling visualization machine data Review', 'percentiles'), 'com') (('One', 'training dataset'), 'approach') (('These', 'goal'), 'be') (('1 then object', 'single nearest neighbor'), 'assign') (('however most', '3 most passengers'), 'have') (('We', 'further appropriate actions'), 'validate') ", "extra": "['gender', 'test']", "label": "Perfect_files", "potential_description_queries": ["age", "algorithm", "alphanumeric", "analyze", "answer", "approach", "array", "associated", "best", "binary", "board", "book", "calculate", "case", "categorical", "category", "character", "chart", "children", "choice", "choose", "classification", "classifier", "classify", "code", "coefficient", "col", "combine", "competition", "confidence", "consider", "contain", "conversion", "convert", "correct", "correlation", "correlations", "could", "count", "create", "criteria", "data", "dataset", "decision", "define", "dependent", "derive", "describe", "description", "detect", "develop", "directly", "disaster", "discrete", "distribution", "domain", "dot", "drop", "duplicate", "embarkation", "empty", "enable", "ensemble", "evaluation", "every", "execute", "expand", "explore", "extract", "family", "fare", "feature", "fill", "final", "float", "following", "freq", "function", "general", "generate", "generated", "grid", "help", "high", "histogram", "hue", "improve", "include", "including", "increase", "indicate", "individual", "input", "instance", "integer", "interval", "kernel", "lead", "learning", "life", "line", "linear", "log", "major", "majority", "male", "manipulating", "match", "mean", "median", "method", "might", "missing", "mixed", "mode", "model", "modelling", "most", "multiple", "naive", "name", "nature", "nearest", "need", "negative", "network", "neural", "new", "next", "no", "noise", "non", "not", "notebook", "null", "number", "numeric", "numerical", "object", "objective", "observation", "order", "out", "output", "overfitting", "page", "part", "pattern", "people", "perform", "plot", "positive", "practice", "predict", "prediction", "predictor", "prepare", "present", "probability", "problem", "project", "question", "random", "range", "rank", "ratio", "regression", "relationship", "remove", "replace", "report", "response", "result", "review", "right", "run", "sample", "science", "score", "scoring", "second", "select", "sense", "sequence", "service", "set", "several", "short", "similar", "single", "site", "six", "skew", "solution", "stage", "standard", "start", "state", "std", "step", "submission", "supervised", "survival", "survived", "target", "test", "testing", "text", "those", "through", "time", "titanic", "title", "total", "train", "training", "tree", "try", "turn", "type", "understanding", "uniform", "unique", "up", "upper", "validate", "value", "variable", "vector", "visualization", "vote", "while", "who", "word", "work", "workflow", "year"], "potential_description_queries_len": 237, "potential_script_queries": ["numpy", "seaborn", "sklearn", "svm"], "potential_script_queries_len": 4, "potential_entities_queries": ["alphanumeric", "classifier", "confidence", "data", "directly", "high", "mean", "mixed", "most", "nearest", "neural", "next", "null", "plot", "problem", "single", "tree", "visualization"], "potential_entities_queries_len": 18, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 240}