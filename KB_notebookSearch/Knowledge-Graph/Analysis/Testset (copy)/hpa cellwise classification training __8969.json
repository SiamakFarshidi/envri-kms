{"name": "hpa cellwise classification training ", "full_name": " h1 Human Protein Atlas Single Cell Classification h2 Categorical Classification At a Cellular Level TRAINING h1 TABLE OF CONTENTS h3 0 xa0 xa0 xa0 xa0IMPORTS h3 1 xa0 xa0 xa0 xa0BACKGROUND INFORMATION h3 2 xa0 xa0 xa0 xa0SETUP h3 3 xa0 xa0 xa0 xa0HELPER FUNCTIONS h3 4 xa0 xa0 xa0 xa0PREPARING THE DATASET TF DATA h3 5 xa0 xa0 xa0 xa0MODELLING h1 0 xa0 xa0IMPORTS xa0 xa0 xa0 xa0 h1 1 xa0 xa0BACKGROUND INFORMATION xa0 xa0 xa0 xa0 h3 1 1 APPROACH OVERVIEW h3 1 2 VISUAL HELPER h1 2 xa0 xa0NOTEBOOK SETUP xa0 xa0 xa0 xa0 h1 3 xa0 xa0HELPER FUNCTIONS xa0 xa0 xa0 xa0 h1 4 xa0 xa0PREPARING THE DATASET TF DATA xa0 xa0 xa0 xa0 h3 4 0 INITIALIZE DICTIONARIES MAPPING PATHS TO RELEVANT FILES h3 4 1 TRANSFORM THE PATH MAP INTO LISTS FOR INPUT INTO TF DATA h3 4 2 DETERMINE AN APPROPRIATE CLASS WEIGHTING h3 4 3 Define Model Dataset Parameters h3 4 4 CREATE THE TRAINING AND VALIDATION DATASETS h3 4 5 PREPROCESS THE DATASETS COMBINE RGBY AND ONE HOT ENCODE LABELS h1 5 xa0 xa0MODELLING xa0 xa0 xa0 xa0 h3 5 1 LOAD THE MODEL BACKBONE FOR TRAINING h3 5 2 FIT THE MODEL h3 5 3 VISUALIZE TRAINING ", "stargazers_count": 0, "forks_count": 0, "description": "EfficientNetB1 include_top include_top weights weights pooling pooling input_shape input_shape elif b2 in efficientnet_name eb tf. plot_model eb show_shapes True show_dtype True dpi 55. Segment slide level images get RLEs for all cells in all applicable slide level images 3. We also take this opportunity to identify the labels for each image and create an array as such. 3 VISUALIZE TRAINING WIP Try and get keras plot to work Machine Learning and Data Science Imports Built In Imports Visualization Imports PRESETS Define the path to the root data directory Define the path to the competition data directory Define path to the filtered TP IDs for each class Define the paths to the training tiles for the cell wise classification dataset Define the paths to the training and testing tfrecord and image folders respectively for the competition data Capture all the relevant full image paths for the competition dataset Capture all the relevant full tfrec paths Random Useful Info Define paths to the relevant csv files Create the relevant dataframe objects train_df. png 2 nbsp nbsp NOTEBOOK SETUP nbsp nbsp nbsp nbsp 10514 3 nbsp nbsp HELPER FUNCTIONS nbsp nbsp nbsp nbsp 10514 4 nbsp nbsp PREPARING THE DATASET TF. Identify slide level images containing only one label2. VIEW SCHEDULE AUTO CALCULATED TRAIN DATASET VALIDATION DATASET Adjust class output expectation SEEDING KERNEL INIT Can t figure this out right now img_batch tf. reduce_sum 1 y_hat y axis 0 soft_f1 2 tp 2 tp fn fp 1e 16 cost 1 soft_f1 reduce 1 soft f1 in order to increase soft f1 macro_cost tf. 5 PREPROCESS THE DATASETS COMBINE RGBY AND ONE HOT ENCODE LABELS We also augment visualize shuffle batch and prefetch. BatchNormalization bb. 2 VISUAL HELPER basic_idea_graph https i. DATA nbsp nbsp nbsp nbsp 10514 This section will explore how to use tf. 2 FIT THE MODEL WIP5. Args y int32 Tensor targets array of shape BATCH_SIZE N_LABELS y_hat float32 Tensor probability matrix of shape BATCH_SIZE N_LABELS Returns cost scalar Tensor value of the cost function for the batch y tf. float32 f1 2 tp 2 tp fn fp 1e 16 macro_f1 tf. 203 def macro_double_soft_f1 y y_hat Compute the macro soft F1 score as a cost average 1 soft F1 across all labels. reduce_sum y_hat 1 y axis 0 fn tf. Args y int32 Tensor targets array of shape BATCH_SIZE N_LABELS y_hat float32 Tensor probability matrix from forward propagation of shape BATCH_SIZE N_LABELS Returns cost scalar Tensor value of the cost function for the batch y tf. map_fn fn lambda img tf. 1 LOAD THE MODEL BACKBONE FOR TRAINING WIP5. 1 APPROACH OVERVIEW TRAINING 1. Train a model to classify these tile level images accurately1. 4 CREATE THE TRAINING AND VALIDATION DATASETS WIP4. Dropout dropout x for n_nodes in head_layer_nodes x tf. Augment the dataset rotation flipping horizontal and vertical minor skew 10. compile optimizer OPTIMIZER loss LOSS_FN metrics acc tf. literal_eval x train_df. 0 INITIALIZE DICTIONARIES MAPPING PATHS TO RELEVANT FILES 4. inputs outputs output eb get_backbone b2 eb add_head_to_bb eb n_classes dropout 0. Resize each RGBY tile to be 256px by 256px 6. count_nonzero y_pred y axis 0 tf. Update the dataset greatly increase the number of negative class examples 9. 2 DETERMINE AN APPROPRIATE CLASS WEIGHTING I was using this previously. count_nonzero 1 y_pred y axis 0 tf. reduce_mean cost average on all labels return macro_cost def macro_f1 y y_hat thresh 0. 5 Compute the macro F1 score on a batch of observations average F1 across labels Args y int32 Tensor labels array of shape BATCH_SIZE N_LABELS y_hat float32 Tensor probability matrix from forward propagation of shape BATCH_SIZE N_LABELS thresh probability value above which we predict positive Returns macro_f1 scalar Tensor value of macro F1 for the batch y_pred tf. This version uses the computation of soft F1 for both positive and negative class for each label. float32 192 192 3 224 224 elems img_batch img_batch VISUALIZE EXAMPLES VISUALIZE EXAMPLES def get_backbone efficientnet_name efficientnet_b0 input_shape 224 224 3 include_top False weights imagenet pooling avg if b0 in efficientnet_name eb tf. 1 TRANSFORM THE PATH MAP INTO LISTS FOR INPUT INTO TF. Crop RGBY image around each cell4. Pad each RGBY tile to be square5. Dense n_classes activation sigmoid x return tf. mask_bboxes train_df. reduce_sum 1 y_hat 1 y axis 0 soft_f1_class1 2 tp 2 tp fn fp 1e 16 soft_f1_class0 2 tn 2 tn fn fp 1e 16 cost_class1 1 soft_f1_class1 reduce 1 soft f1_class1 in order to increase soft f1 on class 1 cost_class0 1 soft_f1_class0 reduce 1 soft f1_class0 in order to increase soft f1 on class 0 cost 0. float32 tp tf. float32 fp tf. Filter the images based on certain additional factors to obtain a better training dataset TBD 7. EfficientNetB4 include_top include_top weights weights pooling pooling input_shape input_shape elif b5 in efficientnet_name eb tf. float32 y_hat tf. Starting with a high LR would break the pre trained weights. Dense n_nodes activation relu x x tf. 05 head_layer_nodes 512 x tf. reduce_mean cost average on all labels return macro_cost def macro_soft_f1 y y_hat Compute the macro soft F1 score as a cost. co y6YfBzN basic idea. DATA 5 nbsp nbsp nbsp nbsp MODELLING 0 nbsp nbsp IMPORTS nbsp nbsp nbsp nbsp 10514 1 nbsp nbsp BACKGROUND INFORMATION nbsp nbsp nbsp nbsp 10514 1. literal_eval x convert the compressed string to a 3D uint8 tensor resize the image to the desired size Get class stuff Get the relevant color directories Update map get the relevant full paths Capture all relevant paths REMOVE AFTER UPDATING CLASSBASED NOTEBOOK Initialize Get class counts Exclude mitotic spindle Calculate weights Manually adjust mitotic spindle to a more appropriate value TP_ID_MAP get_tp_id_map PKL_DIR Define the paths to the training files for the tile dataset as a map from class index to list of paths print n. output x tf. DATA Using the command tf. EfficientNetB0 include_top include_top weights weights pooling pooling input_shape input_shape elif b1 in efficientnet_name eb tf. data to setup the input pipeline4. list_files unfortunately either shuffles each list individually or forces the files into a sorted order. EfficientNetB7 include_top include_top weights weights pooling pooling input_shape input_shape else raise ValueError Invalid EfficientNet Name return eb def add_head_to_bb bb n_classes 19 dropout 0. Seperate the channels and store as seperate datasets8. I will still calculate it here but I won t be adding it to the fit function. reduce_mean f1 return macro_f1 Using an LR ramp up because fine tuning a pre trained model. n for i in range 19 print f CLS i 2 n ORIGINAL DISTRIBUTION ORIGINAL_DIST_MAP i n NEW DISTRIBUTION len RED_FILE_MAP i n red_inputs green_inputs blue_inputs yellow_inputs labels class_wts class_cnts get_class_wts RED_FILE_MAP return_counts True multiplier 23. AUC name auc multi_label True tf. count_nonzero y_pred 1 y axis 0 tf. EfficientNetB5 include_top include_top weights weights pooling pooling input_shape input_shape elif b6 in efficientnet_name eb tf. 5 nbsp nbsp MODELLING nbsp nbsp nbsp nbsp 10514 This section will explore how to acquire augment and train the model5. Use probability values instead of binary predictions. Average 1 soft F1 across all labels. BatchNormalization x x tf. EfficientNetB2 include_top include_top weights weights pooling pooling input_shape input_shape elif b3 in efficientnet_name eb tf. We want the files shuffled but we want the shuffling to be identical across the various colour channels. reduce_sum y_hat y axis 0 fp tf. Dropout dropout 2 x output tf. 5 cost_class1 cost_class0 take into account both class 1 and class 0 macro_cost tf. float32 fn tf. EfficientNetB6 include_top include_top weights weights pooling pooling input_shape input_shape elif b7 in efficientnet_name eb tf. 3 Define Model Dataset Parameters 4. reduce_sum 1 y_hat y axis 0 tn tf. greater y_hat thresh tf. Comparison Before and After Manual Heuristic Filtering Function. Human Protein Atlas Single Cell ClassificationCategorical Classification At a Cellular Level TRAINING CREATED BY DARIEN SCHETTLERTABLE OF CONTENTS 0 nbsp nbsp nbsp nbsp IMPORTS 1 nbsp nbsp nbsp nbsp BACKGROUND INFORMATION 2 nbsp nbsp nbsp nbsp SETUP 3 nbsp nbsp nbsp nbsp HELPER FUNCTIONS 4 nbsp nbsp nbsp nbsp PREPARING THE DATASET TF. EfficientNetB3 include_top include_top weights weights pooling pooling input_shape input_shape elif b4 in efficientnet_name eb tf. ", "id": "dschettler8845/hpa-cellwise-classification-training", "size": "8969", "language": "python", "html_url": "https://www.kaggle.com/code/dschettler8845/hpa-cellwise-classification-training", "git_url": "https://www.kaggle.com/code/dschettler8845/hpa-cellwise-classification-training", "script": "glob macro_f1 plot_ex add_head_to_bb get_tp_id_map decode_img lrfn plotly.graph_objects matplotlib.patches macro_double_soft_f1 convert_rgby_to_rgb collections seaborn numpy __load_with_tf PIL plotly.express get_backbone macro_soft_f1 tensorflow_addons __load_with_pil Image matplotlib.pyplot __load_with_cv2 tqdm.notebook tensorflow get_color_path_maps pandas plot_rgb preprocess_path_ds matplotlib.colors Counter tqdm load_image_scaled create_input_list flatten_list_of_lists augment ListedColormap get_class_wts datetime ", "entities": "(('N_LABELS Returns', 'tf'), 'target') (('4', 'TRAINING'), 'CREATE') (('NOTEBOOK Initialize Get class spindle Calculate CLASSBASED counts Exclude mitotic weights', 'print n.'), 'convert') (('TRAIN DATASET', 'SEEDING this'), 'calculated') (('weights input_shape input_shape elif include_top pooling b6', 'efficientnet_name eb'), 'weight') (('input_shape input_shape elif pooling b2', 'efficientnet_name eb'), 'include_top') (('ValueError Invalid EfficientNet else Name', 'eb def'), 'EfficientNetB7') (('version', 'label'), 'use') (('dataset rotation', 'horizontal vertical skew'), 'augment') (('DICTIONARIES INITIALIZE MAPPING', 'RELEVANT FILES'), '0') (('Update', 'class negative examples'), 'increase') (('weights input_shape input_shape elif include_top pooling b3', 'efficientnet_name eb'), 'include_top') (('reduce_mean cost average', 'macro_cost def'), 'return') (('input_shape input_shape elif pooling b4', 'efficientnet_name eb'), 'include_top') (('1 1e 16 cost_class1 soft_f1_class1', 'class 0 cost'), 'reduce_sum') (('n green_inputs blue_inputs', 'class_wts class_cnts'), 'n') (('section', 'how tf'), 'nbsp') (('cost_class1 5 cost_class0', 'account'), 'take') (('reduce_mean cost average', 'cost'), 'return') (('section', 'model5'), 'nbsp') (('fn 1 2 16 soft_f1', 'f1 soft macro_cost'), 'reduce_sum') (('DATASETS ENCODE 5 PREPROCESS ONE HOT We', 'visualize shuffle also batch'), 'combine') (('here I', 'fit function'), 'calculate') (('APPROPRIATE CLASS 2 I', 'this'), 'DETERMINE') (('b0', 'efficientnet_name eb'), 'float32') (('input_shape input_shape elif pooling b5', 'efficientnet_name eb'), 'efficientnetb4') (('We', 'array'), 'take') (('Segment slide level images', 'slide level applicable images'), 'get') (('shuffling', 'colour various channels'), 'want') (('list_files', 'sorted order'), 'shuffle') (('Data Science Imports Visualization PRESETS', 'dataframe relevant objects'), 'try') (('Starting', 'pre trained weights'), 'break') (('include_top weights input_shape input_shape elif b1', 'efficientnet_name eb'), 'weight') (('scalar Tensor macro_f1 value', 'batch'), 'Compute') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["acc", "account", "adjust", "array", "auc", "augment", "average", "avg", "basic", "batch", "bb", "binary", "calculate", "cell", "classification", "classify", "color", "command", "competition", "compile", "computation", "convert", "cost", "create", "csv", "data", "dataframe", "dataset", "def", "directory", "explore", "f1", "figure", "filtered", "fit", "float32", "fn", "forward", "function", "high", "image", "imagenet", "img", "increase", "index", "input", "len", "level", "list", "macro", "map", "matrix", "model", "name", "negative", "number", "optimizer", "order", "out", "output", "path", "plot", "png", "pooling", "positive", "pre", "predict", "print", "probability", "propagation", "range", "reduce", "resize", "return", "right", "score", "section", "setup", "shape", "shuffle", "sigmoid", "size", "skew", "slide", "soft", "store", "string", "stuff", "tensor", "testing", "tfrecord", "tile", "train", "training", "tuning", "up", "value", "version", "vertical", "visualize", "wise", "work"], "potential_description_queries_len": 102, "potential_script_queries": ["datetime", "glob", "lrfn", "numpy", "seaborn", "tensorflow", "tqdm"], "potential_script_queries_len": 7, "potential_entities_queries": ["level", "pooling", "slide", "vertical"], "potential_entities_queries_len": 4, "potential_extra_queries": ["test"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 109}