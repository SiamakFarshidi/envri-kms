{"name": "nlp ml which words predict a recommendation ", "full_name": " h1 Machine Learning with text data h4 Can it be predicted whether or not a customer would recommend a purchased item of clothing based on solely their written review h2 1 Text Preprocessing h3 Noise Removal h3 Lexicon Normalisation h1 2 Getting a text matrix h1 3 Data Exploration h3 Merging datasets h3 Checking out the most salient words h3 Quick peak at the target variable h2 4 Machine Learning h3 Set up h3 Algorithms round 1 h3 Algorithms round 2 h2 5 Best Model Random Forest Classifier h3 Assessing feature importance h3 Precision Recall Curve h3 ROC AUC Curve h3 ROC AUC score h2 6 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "The relation between them can be plotted The red line represents a purely random classifier e. This means that the data is not readily analysable without any pre processing. For example language stopwords commonly used words of a language is am the of in etc URLs or links punctuations and industry specific words. Though they mean different things contextually they all are similar. The specific model in question is known as Term Frequency Inverse Document Frequency TF IDF TF IDF is a weighted model commonly used for information retrieval problems. I ll come back to this again later. Following is a python function to strip out noise throughout the reviews Before moving onto lexicon normalisation I want to gain a sense of the sentiment per review. My sense is this is due to class imbalance. In terms of how this kernel could be extended or improved we could Take a more rigorous approach to mining the text data such as categorising products controlling for spelling errors using more advanced modelling techniques such as topic modelling Make better use of the Polarity metric within Machine Learning Apply a more extensive list of algorithms including Deep Learning to the text data Apply GridSearchCV or RandomisedSearchCV to optimise the final model Create an ensemble of models for better predictionPerhaps I will return to some or all of these points at a later date. Machine Learning with text data Can it be predicted whether or not a customer would recommend a purchased item of clothing based on solely their written review In this my first Machine Learning project working with text data and applying Natural Language Processing NLP techniques I will aim to predict with words alone whether or not a customer would recommend a purchased item of clothing. IDF TF IDF formula gives the relative importance of a term in a corpus list of documents given by the following formula below. Of particular interest to a retailer might be words such as Soft Comfortable Revealing the importance of texture and how the clothes actually feel Fit Size Both similar in nature and high ranking the product needs to fit well. Now let s read in the data all 23 000 clothing reviews. In this Kernel I will proceed with method one below is step one I am happy with that number as a starting point less than 1000 was my initial aim. Getting a text matrixTo analyse a preprocessed data it needs to be converted into features. This bodes well for using our text matrix to predict recommendations. Let s suppose you have a quirky classifier. After that we ll take a look at the sparsity of this representation which lets us know how many nonzero values there are in the dataset. Text PreprocessingText is the most unstructured form of all the available data therefore various types of noise are present in it. 7 for negative examples and 0. Function defined now let s get training I m only going to focus on four algorithms in this kernel starting off with Algorithms round 1Four models down and we re up to 88 Accuracy with the Linear SVC. Accuracy deals with ones and zeros meaning you either got the class label right or you didn t. This information can be displayed visually as a Presicion Recall curve which has usefulness in allowing us to tailor n algorithm to more exact precision and recall requirements. You will recall that I pulled a Polarity statistic during Step 1 let s see how this shapes up per target outcome Comparing the above two graphs it appears as though Polarity can recognise that reviews left by customers who recommended their product gave more positive reviews compared to those who did not recommend. This usually comprises two key steps 1. Quick peak at the target variableA binary variable Recommended will be the focus of upcoming Machine Learning prediction. The more sparse the data is the more challenging it will be to model but that s a discussion for another day Now that we have term counts for each document the TfidfTransformer can be applied to calculate the weights for each term in each document Great we have our weighted words Just a few more steps required below I m going to extract all of the feature names which are the n grams and put these into a DataFrame along with the corresponding weights per review. After initiating a model this function will return an mean accuracy score following 5 folds of cross validation this is to ensure that we are getting a smoothed out representation of both the training and test sets. Taken the text reviews cleaned them and built them into a matrix Briefly explored the data before applying initial Machine Learning Algorithms Balanced the unvenly weighted target variables before re running the same Algorithms with improved findings Identified the best performing model Random Forest Classifier explored it s most important features and computed further Precision Recall metrics including ROC AUC. Various thresholds result in different true positive false positive rates. One is a two part process of using the CountVectorizer class to count how many times each term shows up in each document followed by the TfidfTransformer class generating the weight matrix. Depending upon the usage text features can be constructed using a variety of techniques in this kernel I will be converting the data into statistical features. With the weighted text matrix already created there is no pressing need for any further preprocessing engineering on these features. This step converts all the disparities of a word into their normalized form also known as lemma. This space under the curve is known as the AUC and therefore a larger AUC space is indicative of a better model. However there seems to be an issue with Class 0 would not recommend across all trained models looking at the LinearSVC both Precision Recall are low contributing towards a mediocre 0. Then I am going to add in at the end some summary statistics to understand per review The highest weighted word The weight of this word The total weighting per review. ROC AUC CurveAUC is a metric for binary classification. For this Random Forest Classifier model the graph therefore indicates very strong model performance hoo rah ROC AUC scoreLastly we will compute the size of this AUC space known as the ROC AUC score. To compute accuracy from probabilities you need a threshold to decide when zero turns into one. Data ExplorationNow that we have a td idf weight matrix this can be fed directly into a predictive model. There are two methods of lexicon normalisation Stemming or Lemmatization. Linear SVC Creating a plot for feature importance Set a different title for each axes Make the grid horizontal instead of vertical Compile arrays of columns words and feature importances Bung these into a dataframe rank highest to lowest then slice top 20 Plot the graph Getting prediction probabilities Defining a new function to plot the precision recall curve Compute the true positive and false positive rate Plotting the true positive and false positive rate Computing the ROC AUC score. The most natural threshold is of course 0. Backing up the graph a very strong ROC AUC Score of over 99 has been achieved by this model. With this dataset we re way off that. 7 thereafter Recall plummets eventually down to near zero. Logistic Regression 4. Random Forest Classifier 3. I m only interested in three columns within this project which have been seelcted below. So anything that doesn t feature as part of this will now be removed from the DataFrame. ConclusionJob done In this kernel we have Started with an unstructured table of 23 000 clothing text reviews and corresponding Recommend vs Not Recommend classifications. I ve enjoyed working with this dataset and have learned heaps around using text data to predict an outcome given this has been my first exposure to working with text the buzz in being able to do just this has been great It has also been an enjoyable experience using this data to predict genuinely meaningful and useful insights such as knowing what consumers are looking for in a clothing product and what matters most to them. Algorithms round 2We re in business a drastic improvement on Precision Recall across all algorithms even complemented with improved overall acccuracy on a couple as well. I ll begin with visualising the top predictive features from the Random Forest Classifier Assessing feature importanceTaking the win by a clear margin as the strongest predictor of product recommendation is the word love. Combined these metrics will provide rich insight into individual model performance and will guide better selection towards the best performing model and how best to optimise it. Lexicon Normalisation Noise RemovalAny piece of text which is not relevant to the context of the data and the end output can be specified as the noise. Achieved a final accuracy score above 90 on the model s training dataset and an ROC AUC score above 99. The below code will generate this for us The optimum Precision Recall threshold here looks within the region of 0. Following is the code using python s scikit learn package to convert a text into tf idf vectors Scikit learn provides two methods to get to our end result a TD IDF weight matrix. Machine LearningWhile a lot of the statistics that I created earlier on such as Polarity and Keyword are interesting I want to focus model predictions purely on the weighted text matrix. 8 would be just perfect. The reviews are looking good for this retailer currently Let s find out a little more about this by inspecting the chosen target variable. Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features N different features to the low dimensional space 1 feature which is an ideal ask for any ML model. Before we do this let s explore the current data a little more Merging datasets Checking out the most salient wordsLet s see which words are most frequent throughout the matrix It looks as though there is a definite positive lean throughout these most popular words such as love perfect and flattering. This creates a separate column for each term that contains the count within each document. This step deals with removal of all types of noisy entities present in the text. If I wanted to be more or less restrictive on n gram selection I could adjust the min_df and max_df parameters within my CountVectorizer which controls for the minimum and maximum amount of documents each word should feature in. Cheap This word features in the top list but quite far down the list How important is price when reviewing a product at this retailer Precision Recall CurveFor each person the Random Forest algorithm classifies it computes a probability based on a function and it classifies the review as Recommended when the score is bigger than the threshold or as Not Recommended when the score is smaller than the threshold. It is able to get all the answers right but it outputs 0. But many classifiers are able to quantify their uncertainty about the answer by outputting a probability value. I don t intend to use this for any machine learning purposes more so out of interest to understand whether reviews lean towards positivity or negativity. 9 for positive examples. Now let s see if the 20 highest weighted words throws up a similar list Largely similar albeit a slight difference in ranking. As you decrease the threshold you get more true positives but also more false positives. The entire process of cleaning and standardization of text making it noise free and ready for analysis is known as text preprocessing. Next this function will provide us with the Confusion Matrix how many correct vs incorrect classifications have actually taken place within the given model Last up this function will churn out for us a Classification Report which details other important metrics such as Precision Recall the F1 score which is just the harmonic mean of the former two and support which is the classification count. Upon first inspection it looks like there are some missing reviews fill in. Other interesting questions within this kernel include Which emotive words are most popular Which aspect of the product is most important the fit colour texture price And morever what would such information mean for a retailer looking to best align their strategy to market demand I m not sure about you but that s enough to spike my interest There are 23 000 reviews and a binary target variable for overall product recommendation let s crack on. Therefore I will dive straight into Machine Learning. Set upTo help with Machine Learning I will define a function that will return the most prized statistics in one go. Not just positive words but also negative ones such as disappointed and unfortunately have been useful in the prediction. However now exceeding the 90 bracket is the Random Forest Classifier also boasting great Precision Recall values too. 5 won t get you far but 0. For now however thank you for reading this kernel Please do feel free to share with me your thoughts feedback and any suggestions for improvement I am always willing to learn new or more efficient techniques Cheers. Linear SVC Import the hopeful solution to our problems Setting up new variables for ML Defining a new function with revised inputs for the new SMOTE variables 1. Clearly a threshold of 0. That s the whole point of using AUC it considers all possible thresholds. As depicted earlier there are around five times fewer 0 classifications compared to 1 classifications which can sometimes be problematic within Machine Learning whereby modelling generally tends to work better when there is an almost equal numbers of samples from each class in the target variable. For Example let say there is a dataset of N text documents In any document D TF and IDF will be defined as Term Frequency TF TF for a term t is defined as the count of a term t in a document D Inverse Document Frequency IDF IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T. g a coin flip so the aim is for our classifier represented by the blue ROC curve to be as far away from it as possible. Let s apply this technique and then re run all four models to re assess precision recall performance. In this notebook I will use an Oversampling technique from the handy SMOTE library. This shouldn t come as too much of a surprise given that love is a highly emotive word that conveys a larger feeling of positivity as opposed to like or nice for example. Let s see how the categories stack up There are nearly four times as many recommendations than there are non recommendations. We ll need to keep a note of this as this could throw up a few issues when training algorithms i ll touch on this in a little while. Best Model Random Forest ClassifierGiven that the Random Forest now heads the pack in terms of accuracy let s proceed with some further exploration and perhaps a spot of optimisation too. We can now tackle the next step which is to turn this document into a bag of words representation. I will opt for Lemmatization as this will return the root form of each word rather than just stripping suffixes which is stemming. It s great to see the Naive Bayes Classifier now up at 85 as this algorithm is typically well suited to crunching text data. First up I ll import every library that will be used in this project is imported at the start. Data handling and processing Data visualisation Statistics NLP Machine Learning xa0Reading in data Inspecting the variables Replacing blank variables with unknown ready for processing Importing SKLearn s list of stopwords and then appending with my own words Basic text cleaning function Make lowercase xa0Remove whitespaces Remove special characters Remove punctuation Remove numbers Remove Stopwords Convert to string Applying noise removal function to data xa0Defining a sentiment analyser function Applying function to reviews Instantiate the Word tokenizer Word lemmatizer Define a word lemmatizer function Apply the word lemmatizer function to data Getting a count of words from the documents Ngram_range is set to 1 2 meaning either single or two word combination will be extracted Getting the total n gram count Creating the bag of words representation Instantiating the TfidfTransformer Fitting and transforming n grams Getting a list of all n grams Putting weighted n grams into a DataFrame and computing some summary statistics Merging td idf weight matrix with original DataFrame Printing the first 10 reviews left Getting a view of the top 20 occurring words Getting a view of the top 20 weights Plotting overall recommendations and getting value counts Visualising polarity between recommending and non recommending customers then getting value counts Get a list of columns for deletion Drop all columns not part of the text matrix Create X y variables for Machine Learning Create a train test split of these variables Defining a function to fit and predict ML algorithms 1. Gaussian Naive Bayes 2. The other does both steps in a single TfidfVectorizer class. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. Lexicon NormalisationAnother type of textual noise is about the multiple representations exhibited by single word. For example play player played plays and playing are the different variations of the word play. To overcome this problem we can either DownSample the majority Class or UpSample the minority class. ", "id": "josh24990/nlp-ml-which-words-predict-a-recommendation", "size": "15733", "language": "python", "html_url": "https://www.kaggle.com/code/josh24990/nlp-ml-which-words-predict-a-recommendation", "git_url": "https://www.kaggle.com/code/josh24990/nlp-ml-which-words-predict-a-recommendation", "script": "sklearn.metrics randint as sp_randint randint cross_val_score lemmatize_text sklearn.naive_bayes stopwords sentiment_analyser TfidfTransformer TextBlob model plot_roc_curve TfidfVectorizer plot_precision_and_recall recall_score precision_score SMOTE seaborn numpy LinearSVC sklearn.feature_extraction.text scipy.stats cross_val_predict time textblob sklearn.ensemble model_sm remove_noise sklearn.model_selection CountVectorizer confusion_matrix f1_score RandomForestClassifier imblearn.over_sampling matplotlib.pyplot text stats importance_plotting pandas classification_report sklearn.feature_extraction LogisticRegression scipy precision_recall_curve nltk.corpus roc_auc_score sklearn.linear_model GaussianNB sklearn.svm roc_curve train_test_split statsmodels.api ", "entities": "(('you', 'quirky classifier'), 'let') (('player', 'different word'), 'be') (('space', 'AUC therefore larger better model'), 'know') (('reviews', 'positivity'), 'don') (('missing reviews', 'first inspection'), 'look') (('20 highest weighted words', 'ranking'), 'let') (('I', 'review'), 'be') (('whereby modelling', 'target variable'), 'be') (('therefore various types', 'it'), 'be') (('Various thresholds', 'different true positive false positive rates'), 'result') (('I', 'later date'), 'take') (('one below one I', 'starting point'), 'proceed') (('Both', 'high product'), 'be') (('Now s', '23 clothing 000 reviews'), 'let') (('algorithm', 'text typically well data'), 's') (('when score', 'threshold'), 'feature') (('Precision Recall optimum threshold', '0'), 'generate') (('IDF TF IDF formula', 'following formula'), 'give') (('which', 'just harmonic former two'), 'provide') (('Precision Recall', 'low mediocre 0'), 'seem') (('Random Forest Classifier', 'Precision Recall also great values'), 'exceed') (('s', '23 binary target product 000 overall recommendation'), 'include') (('it', 'text preprocessing'), 'know') (('that', 'now DataFrame'), 'remove') (('coin so aim', 'as far away it'), 'flip') (('it', 'possible thresholds'), 's') (('ROC AUC very strong Score', 'model'), 'achieve') (('I', 'SMOTE handy library'), 'use') (('Create y variables', 'ML algorithms'), 'handling') (('word', 'documents'), 'adjust') (('other', 'TfidfVectorizer single class'), 'step') (('It', 'such love'), 'let') (('us', 'nonzero how many dataset'), 'take') (('the', 'etc URLs punctuations'), 'use') (('highly emotive that', 'example'), 'come') (('D TF', 'term'), 'let') (('this', 'class imbalance'), 'be') (('1 which', 'ML ideal model'), 'be') (('this', 'directly predictive model'), 'ExplorationNow') (('Quick peak', 'binary variable Machine Learning variableA prediction'), 'be') (('it', '0'), 'be') (('I', 'text purely weighted matrix'), 'be') (('alone customer', 'clothing'), 'Learning') (('s', 'precision recall assess performance'), 'let') (('down we', 'Linear SVC'), 'let') (('drastic improvement', 'couple'), 're') (('red line', 'classifier purely random e.'), 'plot') (('ROC AUC', 'binary classification'), 'be') (('s', 'perhaps optimisation'), 'let') (('i', 'little while'), 'need') (('you', 'more true positives'), 'decrease') (('which', 'project'), 'm') (('when zero', 'one'), 'compute') (('that', 'document'), 'create') (('how many times term', 'weight matrix'), 'be') (('Combined', 'how best it'), 'provide') (('I', 'product recommendation'), 'begin') (('that', 'start'), 'import') (('scoreLastly we', 'ROC AUC score'), 'indicate') (('Linear SVC', 'SMOTE new variables'), 'Import') (('who', 'those'), 'recall') (('we', 'training sets'), 'be') (('data', 'pre readily processing'), 'mean') (('class label', 'ones'), 'deal') (('step', 'present text'), 'deal') (('which', 'review'), 'be') (('we', 'classifications'), 'do') (('Term Frequency Inverse Document Frequency TF IDF TF IDF', 'information retrieval weighted commonly problems'), 'know') (('end output', 'noise'), 'specify') (('us', 'tailor more exact precision requirements'), 'display') (('that', 'one'), 'define') (('summary statistics', 'total review'), 'go') (('It', 'exact ordering'), 'aim') (('Scikit', 'TD IDF weight matrix'), 'be') (('most natural threshold', 'course'), 'be') (('Therefore I', 'Machine straight Learning'), 'dive') (('step', 'also lemma'), 'convert') (('what', 'most them'), 'enjoy') (('I', 'always new more techniques'), 'thank') (('many classifiers', 'probability value'), 'be') (('currently s', 'target chosen variable'), 'look') (('it', 'features'), 'get') (('which', 'words representation'), 'tackle') (('text weighted matrix', 'features'), 'create') (('Lexicon NormalisationAnother type', 'single word'), 'be') (('contextually they', 'different things'), 'be') (('I', 'statistical features'), 'construct') (('we', 'DownSample'), 'can') (('Compile instead vertical arrays', 'ROC AUC score'), 'Set') (('which', 'rather just suffixes'), 'opt') (('Random Forest it', 'ROC AUC'), 'clean') ", "extra": "['outcome', 'test', 'bag']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "adjust", "advanced", "algorithm", "answer", "apply", "approach", "bag", "best", "binary", "calculate", "classification", "classifier", "cleaning", "clear", "clothing", "code", "column", "compute", "context", "convert", "correct", "could", "count", "course", "current", "curve", "customer", "data", "dataframe", "dataset", "day", "define", "demand", "difference", "directly", "document", "end", "engineering", "ensemble", "ensure", "equal", "even", "every", "experience", "explore", "exposure", "extract", "feature", "feedback", "fill", "final", "find", "fit", "flip", "following", "form", "formula", "frequent", "function", "generate", "graph", "grid", "help", "high", "import", "importance", "improvement", "include", "including", "individual", "industry", "interest", "issue", "item", "kernel", "key", "label", "language", "learn", "learning", "left", "let", "library", "line", "list", "little", "look", "looking", "lot", "majority", "margin", "market", "matrix", "maximum", "mean", "meaning", "method", "metric", "might", "minimum", "missing", "model", "modelling", "most", "multiple", "my", "nature", "near", "need", "negative", "new", "next", "no", "noise", "non", "normalized", "not", "notebook", "number", "opt", "out", "outcome", "output", "overall", "pack", "package", "part", "peak", "per", "performance", "performing", "person", "place", "player", "plot", "point", "positive", "pre", "precision", "predict", "prediction", "predictor", "preprocessing", "present", "price", "probability", "problem", "processing", "product", "project", "provide", "punctuation", "python", "question", "random", "rank", "ranking", "ratio", "re", "read", "reading", "recall", "recommend", "recommendation", "region", "relation", "relative", "representation", "result", "return", "review", "rich", "right", "run", "running", "scikit", "score", "selection", "sense", "sentiment", "separate", "set", "similar", "single", "size", "slice", "solution", "space", "sparse", "special", "split", "stack", "standardization", "step", "strategy", "string", "summary", "support", "surprise", "table", "target", "technique", "term", "test", "text", "tf", "those", "threshold", "title", "topic", "total", "train", "training", "turn", "type", "under", "up", "usage", "validation", "value", "variable", "vector", "vertical", "view", "weight", "who", "word", "work"], "potential_description_queries_len": 229, "potential_script_queries": ["api", "numpy", "randint", "scipy", "seaborn", "sklearn", "time"], "potential_script_queries_len": 7, "potential_entities_queries": ["binary", "new", "positive", "product", "recall", "target", "vertical"], "potential_entities_queries_len": 7, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 234}