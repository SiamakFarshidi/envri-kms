{"name": "practical model evaluation day 2 ", "full_name": " h2 Load in data h2 Data preparation h1 XGBoost Baseline h1 Cloud AutoML h3 Prepare your account and project h3 Creating your dataset h3 Training our model h1 TPOT h1 H20 ai AutoML h1 Check that we ve saved each of our models h1 Exercise ", "stargazers_count": 0, "forks_count": 0, "description": "com rtatman practical model evaluation day 3 For today s exercise we re going to be working on classifying roles into job titles based on information about the role. Check that we ve saved each of our modelsBefore we wrap up for the day we want to make sure we ve saved all of our models for tomorrow The Cloud AutoML model is saved automatically on GCP but we ve saved each of the other models in our current working directory. This will let you train an AutoML Tables model in your current project. These notebooks are part of Kaggle s Practical Model Evaluation https www. XGBoost BaselineFirst we re going to train a basic XGBoost model using the default arguments. Then you can modify the following code to start your AutoML model training Once our model starts training we don t need to do anything else it s already saved in our GCP account and good to go for tomorrow. Currently most AutoML research is focused on automating model selection and hyperparameter tuning. You should set the region of your project https cloud. Select the bucket where you d like to store your data. Important Make sure in the Choose where to store your data step you pick Region and set the location as us central1 Iowa. com automl tables utm_medium notebook utm_source kaggle utm_campaign automl event page in the Google Cloud Console and click enable API. Actually for Cloud AutoML we don t even need to split our data but we ll look at that in a minute. For TPOT and XGBoost however we need to make sure that all our input data is numeric. com rtatman practical model evaluation day 2 Day 3 Notebook Evaluating our models https www. The libraries we ll be using are XGBoost https xgboost. ai s open source AutoML library http docs. io en latest not automated machine learning we ll be using this as a baseline Cloud AutoML https cloud. ai AutoMLFor our final model we ll be using H20. For now you do need to have a credit card in order to enable billing and you need billing enabled to use Cloud AutoML. This video goes into more detail https youtu. Click on BROWSE under the Select Files button and a side panel will pop up. If you have the code in your notebook to import your dataset right before the code to create your model when you run your notebook top to bottom it ll give you an error because the modelling code was run before the dataset was done importing. We cover XGBoost in more detail in the Intro To Machine Learning course https www. Go to the AutoML Tables https console. com watch v uINleRduCWM utm_medium notebook utm_source kaggle utm_campaign automl event. We re all set ExerciseNow it s your turn Following the steps above use the df_2019 dataframe and Prepare your data split into testing and training encode variables Train your models XGBoost Cloud AutoML TPOT and H20 AutoML Note if you can t or would prefer not to set up billing on order to use Cloud AutoML feel free to skip training that model. Remember to save your models You ll need them tomorrow and since it takes a while to run AutoML code you don t want to have to run it multiple times. ai AutoML http docs. Select Upload files from your computer and select the file with the dataset you want. You can find the livestreams for this event here https youtu. com rebeccaturner data prep for job title classification but if you d like to do your own or do some additional feature engineering feel free Today we ll be building four different models using four different libraries including some automated machine learning libraries. Click on Datasets in the list on the left hand side of your screen and then click on the blue New Dataset text near the top of your screen. If you re not able to enable billing you can still follow along with the rest of the workshop just skip the Cloud AutoML parts. If you haven t created any buckets you ll see the text No buckets found. Follow the prompts to create your bucket. com learn intro to machine learning so I m not going to talk about it here. com rtatman practical model evaluation day 1 Day 2 Notebook Training models with automated machine learning https www. com watch v Rsg_XzgGqZw utm_medium notebook utm_source kaggle utm_campaign automl event goes into more details. com kaggle kaggle survey 2018 and 2019 https www. Data preparationWe do have an additional step of preperation. This is an academic library built on top of scikit learn and my favorite thing about it is that when you export a model you re actually exporting all the Python code you need to train that model. com 2F dsh S 385463669 3A1575309184770524 gmb exp biz false flowName GlifWebSignIn flowEntry SignUp nogm true utm_medium notebook utm_source kaggle utm_campaign automl event if you already have a Google account you can use that one and enable billing https www. com practical model evaluation event which ran from December 3 5 2019. com appengine docs standard nodejs building app creating project utm_medium notebook utm_source kaggle utm_campaign automl event. Automated machine learning or AutoML for short is the task of removing human labor from the process of training machine learning models. com compute docs regions zones utm_medium notebook utm_source kaggle utm_campaign automl event to us central1. Have fun training your models and I ll see you all tomorrow for our final model evaluation Importing libraries set a seed for reproducability read in our data split into predictor target variables Splitting data into training and test set save out the split training data to use with Cloud AutoML encode all features using ordinal encoding you ll need to use a different encoder for each dataframe split encoded dataset train XGBoost model with default parameters and save our model don t change this value don t change this is the only region that works currently these you ll change based on your GCP project data this will come from your specific GCP project name of your uploaded dataset from GCP console column with feature you re trying to predict these can be whatever you like what you want to call your model max time to train model in milli hours from 1000 72000 first you ll need to make sure your model is predicting the right column let our model know that input columns may have missing values and then you ll need to kick off your model training check if it s done yet it won t be create fit TPOT classifier with save our model code print the model code to see what it says initilaize an H20 instance running locally convert our data to h20Frame an alternative to pandas datatables Run AutoML for 20 base models limited to 1 hour max runtime by default View the top five models from the AutoML Leaderboard The leader model can be access with aml. I ve already done some data cleaning https www. com c kaggle survey 2019 Kaggle data science survey. The reason for this is that importing datasets can take a while. org categorical encoding ordinal. First we ll split into training and testing sets For H20 AutoML and Cloud AutoML we don t need to do anything else. be 7RdKnACscjA list PLqFaTIg4myu HA1VGJi_7IGFkKRoZeOFt. Training our modelIn order to train an AutoML model from inside Kaggle Notebooks you ll need to attach a notebook to your Google Cloud Account. ai h2o latest stable h2o docs automl. Day 1 Notebook Figuring out what matters for you https www. leader save the model out we ll need to for tomorrow check to see that we ve saved all of our models. Cloud AutoMLNow let s train our Cloud AutoML model We ll be using both the GCP console and notebook code here so you ll probably want to open those in separate tabs or windows. com automl utm_medium notebook utm_source kaggle utm_campaign automl event an enterprise focused automated machine learning product TPOT https epistasislab. Import your dataset. I ll be using the 2018 data as an example and then have you work through the 2019 data as your exercise. com signup v2 webcreateaccount service cloudconsole continue https 3A 2F 2Fconsole. Let s just double check that that s the case Alright we ve got three models and the code for the notebook. To create a new bucket click on the icon that looks like a shopping basket with a plus sign in it. TPOTAlright now we ll move onto TPOT https epistasislab. html a second open source automated machine learning library developed by researchers at H20. Prepare your account and project First you ll need to create a GCP account https accounts. From there create a new project https cloud. This may take a while. aiLet s get started Load in dataFirst let s load in our pre cleaned data. We ll be using ordinal label encoding https contrib. This video https www. Once your dataset is done importing take a close look at your imported data and make sure it looks the way you d expect. Give your dataset a name and make sure the region is US CENTRAL1. io tpot an open source automated machine learning library developed at the University of Pennsylvania H20. Kaggle competitors are very fond of stacking and H20 is known for hiring a lot of top Kagglers so it s nice to have that automated for us. One thing that I like about this library is that as each model is trained its evaluated both on its own and as part of a stacked ensemble. Creating your datasetFor this workshop we re going to create our AutoML datasets using the GCP console. be xP99eh6nQN0 utm_medium notebook utm_source kaggle utm_campaign automl event. The data will be from the 2018 https www. ", "id": "rtatman/practical-model-evaluation-day-2", "size": "8531", "language": "python", "html_url": "https://www.kaggle.com/code/rtatman/practical-model-evaluation-day-2", "git_url": "https://www.kaggle.com/code/rtatman/practical-model-evaluation-day-2", "script": "sklearn.metrics kaggle.gcp GcpTarget TPOTClassifier h2o.automl H2OAutoML automl_v1beta1 as automl auc XGBClassifier tpot KaggleKernelCredentials sklearn.model_selection confusion_matrix google.cloud pandas category_encoders automl_v1beta1 storage accuracy_score xgboost train_test_split kaggle_secrets ", "entities": "(('model evaluation com rtatman practical day 2 3 Notebook', 'models https www'), 'evaluate') (('that', 'us'), 'be') (('you', 'billing https www'), 'S') (('We', 'label encoding https ordinal contrib'), 'use') (('final we', 'H20'), 'ai') (('you', 'dataset'), 'file') (('XGBoost Cloud AutoML H20 AutoML you', 'model'), 'set') (('model', 'stacked ensemble'), 'be') (('we', 'working current directory'), 'check') (('we', 'Cloud AutoML https baseline cloud'), 'en') (('we', 'minute'), 'don') (('so I', 'it'), 'learn') (('you way d', 'imported data'), 'take') (('You', 'project https cloud'), 'set') (('Click', 'Select Files button'), 'pop') (('s', 'our'), 'start') (('notebooks', 'Practical Model Evaluation https www'), 'be') (('AutoML Currently most research', 'model selection'), 'focus') (('We', 'https www'), 'cover') (('leader model', 'aml'), 'have') (('You', 'event'), 'find') (('what', 'https www'), 'day') (('Data preparationWe', 'preperation'), 'have') (('we', 'default arguments'), 'go') (('importing datasets', 'while'), 'be') (('that', 'it'), 'create') (('you', 'current project'), 'let') (('you', 'don it'), 'remember') (('then you', 'exercise'), 'use') (('dataset', 'error'), 'give') (('data', 'https 2018 www'), 'be') (('where you', 'data'), 'select') (('you', 'Google Cloud Account'), 'need') (('workshop we', 'GCP console'), 'create') (('TPOTAlright now we', 'TPOT https epistasislab'), 'move') (('com watch utm_medium utm_source utm_campaign automl Rsg_XzgGqZw event', 'more details'), 'v') (('input data', 'TPOT'), 'need') (('we', 'role'), 'day') (('else it', 'tomorrow'), 'modify') (('machine Automated learning', 'machine learning models'), 'be') (('we', 'notebook'), 'let') (('region', 'name'), 'give') (('Today we', 'machine learning automated libraries'), 'prep') (('you', 'Cloud AutoML just parts'), 'skip') (('don t', 'anything'), 'split') (('video', 'detail https more youtu'), 'go') (('buckets', 'text'), 'create') (('billing', 'Cloud AutoML'), 'need') (('you', 'model'), 'be') (('you', 'us'), 'make') (('First you', 'GCP account https accounts'), 'prepare') (('enterprise', 'product TPOT https automated machine learning epistasislab'), 'automl') (('which', 'December'), 'com') (('I', 'data cleaning https ve already www'), 'do') (('we', 'models'), 'save') (('here so you', 'separate tabs'), 'let') (('AutoML open library', 'docs'), 's') (('com signup v2 webcreateaccount service cloudconsole', 'https 3A 2F'), 'continue') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["account", "attach", "automl", "baseline", "basic", "bottom", "button", "call", "case", "categorical", "check", "classification", "classifier", "cleaning", "close", "code", "column", "compute", "computer", "convert", "course", "create", "credit", "current", "data", "dataframe", "dataset", "day", "default", "detail", "double", "en", "enable", "encode", "encoder", "encoding", "engineering", "error", "evaluation", "even", "event", "exercise", "exp", "export", "feature", "file", "final", "find", "fit", "following", "fun", "h2o", "hand", "http", "human", "hyperparameter", "import", "including", "input", "instance", "io", "job", "kaggle", "label", "learn", "learning", "left", "let", "library", "list", "load", "look", "lot", "max", "missing", "model", "modelling", "most", "move", "multiple", "my", "name", "near", "need", "new", "not", "notebook", "open", "order", "out", "page", "panel", "part", "pre", "predict", "predictor", "prep", "print", "product", "project", "re", "read", "reason", "region", "research", "rest", "right", "run", "running", "runtime", "save", "science", "scikit", "screen", "second", "select", "selection", "separate", "service", "set", "short", "side", "sign", "source", "split", "standard", "start", "step", "store", "survey", "target", "task", "test", "testing", "text", "those", "through", "time", "title", "train", "training", "turn", "under", "up", "v2", "value", "video", "while", "work"], "potential_description_queries_len": 149, "potential_script_queries": ["auc", "sklearn", "storage", "xgboost"], "potential_script_queries_len": 4, "potential_entities_queries": ["automl", "cleaning", "day", "evaluation", "most", "service"], "potential_entities_queries_len": 6, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 152}