{"name": "practical machine learning with pytorch tbu ", "full_name": " h1 Practical Machine Learning ML with PyTorch TBU h4 Credits Thanks to Practical AI Goku Mohandas and other contributers for such wonderful work h3 Here are some of my kernel notebooks for Machine Learning and Data Science as follows Upvote them if you like them h2 Kernel Notebook Content h2 Basics h2 Deep Learning h2 Advanced h2 Topics h2 1 Basics h2 1 1 Introduction to Python h3 Variables h3 Lists h3 Tuples h3 Dictionaries h3 If statements h3 Loops h3 Functions h3 Classes h3 Additional resources h2 1 2 NumPy h3 NumPy basics h3 Indexing h3 Array math h3 Advanced h3 Additional resources h2 1 3 Pandas h3 Uploading the data h3 Loading the data h3 Exploratory Dats Analysis EDA h3 Data Preprocessing h3 Feature Engineering h3 Saving data h2 1 4 Linear Regression h3 Overview h3 Training h3 Data h3 Scikit learn Implementation h3 Evaluation h3 Inference h3 Interpretability h3 Proof for unstandardizing coefficients h3 Regularization h3 Categorical variables h3 TODO h2 1 5 Logistic Regression h1 Training h1 Data h1 Scikit learn implementation h1 To Be Updated Soon h2 Credits Reference h2 License h3 Please UPVOTE my kernel if you like it or wanna fork it h4 I am open to have your feedback for improving this kernel h3 Thanks for visiting my Kernel and please UPVOTE to stay connected and follow up the further updates ", "stargazers_count": 0, "forks_count": 0, "description": "frac mathbb E y hat y sigma_y W_0 sum_ j 1 k W_jz_j z_j frac x_j bar x _j sigma_j hat y _ scaled frac hat y _ unscaled bar y sigma_y hat W_0 sum_ j 1 k hat W _j frac x_j bar x _j sigma_j hat y _ unscaled hat W _0 sigma_y bar y sum_ j 1 k hat W _j frac sigma_y sigma_j bar x _j sum_ j 1 k frac sigma_y sigma_j x_j RegularizationRegularization helps decrease over fitting. com arunkumarramanan awesome data science for beginners Tensorflow Tutorial and House Price Prediction https www. Having certain weights with high magnitudes will lead to preferential bias with the inputs and we want the model to work with all the inputs and not just a select few. We can interpret our coefficient as follows By increasing X by 1 unit we increase y by W 3. Now you can concat this with your continuous features and train the linear model. Practical Machine Learning ML with PyTorch TBU This kernel is empowering you to use machine learning to get valuable insights from data. W_i W_i alpha frac partial J partial W_i 6. Implement basic ML algorithms and deep neural networks with PyTorch. With L2 regularization we are penalizing the weights with large magnitudes by decaying them. Can account for continuous and categorical features. NumPy basics Indexing Array math Advanced Additional resourcesYou don t to memorize anything here and we will be taking a closer look at NumPy in the later lessons. If you want to learn more right now before diving into machine learning check out this free course Free Python Course https www. The model will be a line of best fit that minimizes the distance between the predicted and target outcomes. Compare the predictions hat y ex. The updates will penalize the probabiltiy for the incorrect classes j and encourage a higher probability for the correct class y. 65 10 Initialize the model with L2 regularization Train Predictions unstandardize them Train and test MSE Unstandardize coefficients 3. Advantages Computationally simple. 65 10 Create data with categorical features Arguments Set seed for reproducability Upload data from GitHub to notebook s local drive Read from CSV to Pandas DataFrame Import packages Preprocessing Drop rows with NaN values Drop text based features we ll learn how to use them in later lessons pclass sex and embarked are categorical features Preprocess the dataset Split the data Separate X and y Standardize the data mean 0 std 1 using training data Apply scaler on training and test data don t standardize outputs for classification Check mean should be 0 std should be 1 Initialize the model Train Probabilities Predictions unstandardize them. The softmax classifier normalizes the linear outputs to determine class probabilities. When we have more than two classes we need to use multinomial logistic regression softmax classifier. com arunkumarramanan awesome deep learning ml tutorials Data Science with Python Awesome Tutorials https www. 4 Linear RegressionIn this lesson we will learn about linear regression. Randomly initialize the model s weights W. com GokuMohandas practicalAI GitHub Awesome Lists Topic https github. A dictionary cannot have two of the same keys. Uploading the dataWe re first going to get some data to play with. is highly recommended by Google as it s developed by googlers along with Notebooks for exercises. Usually not used for classification and only for regression. J theta frac 1 2 sum_ i X_iW y_i 2 frac lambda 2 sum sum W 2 frac partial J partial W X hat y y lambda W W W alpha frac partial J partial W where lambda is the regularzation coefficientRegularization didn t help much with this specific example because our data is generation from a perfect linear equation but for realistic data regularization can help our model generalize well. Pandas is a great python library for data analysis. Training Steps 1. Overview hat y XW where hat y prediction in mathbb R NX1 N is the number of samples X inputs in mathbb R NXD D is the number of features W weights in mathbb R DX1 Objective Use inputs X to predict the output hat y using a linear model. com arunkumarramanan data science with python awesome tutorials Awesome TensorFlow and PyTorch Resources https www. VariablesVariables are objects in python that can hold anything with numbers or text. Compare the predictions hat y with the actual target values y with the objective cost function to determine loss J. The simplified intuition is that the gradient tells you the direction for how to increase something so subtracting it will help you go the other way since we want to decrease loss J theta. This is very easy to do with Pandas and once you create the dummy variables you can use the same steps as above to train your linear model. 5 Logistic RegressionIn the previous lesson we saw how linear regression works really well for predicting continuous outputs that can easily fit to a line plane. io badge license MIT brightgreen. Loading the dataNow that we have some data to play with let s load into a Pandas dataframe. A common objective function for linear regression is mean squarred error MSE. Feed inputs X into the model to receive the logits z XW. However we are going to use Scikit learn s SGDRegressor class which uses stochastic gradient descent. SGD linear regression Go to top top 1. This function calculates the difference between the predicted and target values and squares it. There are many forms of regularization but they all work to reduce overfitting in our models. Support vector machines SVMs https towardsdatascience. When you want to use numerical operations on then they need to be compatible. DictionariesDictionaries are python objects that hold key value pairs. Below is L2 regularization ridge regression. hat y frac e XW_y sum e XW where hat y prediction in mathbb R NX1 N is the number of samples X inputs in mathbb R NXD D is the number of features W weights in mathbb R DXC C is the number of classes Objective Predict the probability of class y given the inputs X. com topics awesome License MIT https img. We re going to load the titanic dataset from the public link below. com arunkumarramanan awesome tensorflow and pytorch resources Awesome Data Science IPython Notebooks https www. Disadvantages Sensitive to outliers since objective is minimize cross entropy loss. Scikit learn Implementation Note The LinearRegression class in Scikit learn uses the normal equation to solve the fit. Each feature has a coefficient which signifies it s importance impact on the output variable y. Apply backpropagation to update the weights W using a learning rate alpha and an optimization technique ie. com arunkumarramanan data scientist s toolkits awesome ds resources Awesome Computer Vision Resources TBU https www. However we are going to use Scikit learn s SGDClassifier class which uses stochastic gradient descent. stochastic gradient descent. com arunkumarramanan data science with r awesome tutorials Data Science and Machine Learning Cheetcheets https www. com support vector machine vs logistic regression 94cc2975433f are a good alternative to counter outliers. 4 This simplifies our cross entropy objective to the following J theta ln hat y_i negative log likelihood. Feed inputs X into the model to receive the predictions hat y. LoopsYou can use for or while loops in python to do something repeatedly until a condition is met. MSE J theta frac 1 2 sum_ i hat y _i y_i 2 4. com arunkumarramanan awesome deep learning resources Data Science with R Awesome Tutorials https www. There are also other types of regularization like L1 lasso regression which is useful for creating sparse models where some feature cofficients are zeroed out or elastic which combines L1 and L2 penalties. For example if there are three classes the predicted class probabilities could look like 0. If you are curious about more checkout the NumPy reference manual https docs. com ArunkumarRamanan practicalAI master LICENSE Please UPVOTE my kernel if you like it or wanna fork it. Miscellaneous You can also use linear regression for binary classification tasks where if the predicted continuous value is above a threshold it belongs to a certain class. You can use it to regualr any model s weights including the ones we will look at in future lessons. com learn python Go to top top 1. But we will cover better techniques for classification in future lessons and will focus on linear regression for continuos regression tasks only. Feedback If you have any ideas or you want any other content to be added to this curated list please feel free to make any comments to make it better. Let s assume that our classes are mutually exclusive a set of inputs could only belong to one class. Training data X y is used to train the model and learn the weights W using stochastic gradient descent SGD. FunctionsFunctions are a way to modularize reusable pieces of code. that are calculated you need to separate the training and test set first before spplying those operations. Go to top top 1. ClassesClasses are a fundamental piece of object oriented Python programming. 4 with the actual target values y ex. com machine learning crash course Machine Learning ML Crash Course with TensorFlow APIs Google s fast paced practical introduction to machine learning A self study guide for aspiring machine learning practitioners Machine Learning Crash Course features a series of lessons with video lectures real world case studies and hands on practice exercises. The main idea is to take the outputs from the linear equation z XW and use the sigmoid logistic function frac 1 1 e z to restrict the value between 0 1. com learn learn python and Kaggle Learn https www. 3 PandasIn this notebook we ll learn the basics of data analysis with the Python Pandas library. com arunkumarramanan awesome data science ipython notebooks Machine Learning Engineer s Toolkit with Roadmap https www. J theta ln hat y_i ln frac e X_iW_y sum_i e X_iW 4. A common objective function for logistics regression is cross entropy loss. If statementsYou can use if statements to conditionally do something. frac partial J partial W_j frac partial J partial y frac partial y partial W_j frac 1 y frac partial y partial W_j frac 1 frac e W_yX sum e XW frac sum e XW e W_yX 0 e W_yX e W_jX X sum e XW 2 frac Xe W_jX sum e XW XP frac partial J partial W_y frac partial J partial y frac partial y partial W_y frac 1 y frac partial y partial W_y frac 1 frac e W_yX sum e XW frac sum e XW e W_yX X e W_yX e W_yX X sum e XW 2 frac 1 P XP XP 2 X P 1 5. com arunkumarramanan machine learning engineer s toolkit with roadmap Hands on ML with scikit learn and TensorFlow https www. com topics awesome GitHub Machine Learning Topic https github. Apply the softmax operation on the logits to get the class probabilies hat y in one hot encoded form. It s good practice to know what types your variables are. J theta frac 1 2 sum_ i hat y _i y_i 2 frac 1 2 sum_ i X_iW y_i 2 frac partial J partial W X hat y y 4. Kernel Notebook Content Basics 1 Python 1 NumPy 1 Pandas 1 Linear Regression 1 Logistic Regression 1 Random Forests 1 KMeans Clustering Deep Learning 2 PyTorch 2 Multilayer Perceptrons 2 Data Models 2 Object Oriented ML 2 Convolutional Neural Networks 2 Embeddings 2 Recurrent Neural Networks 2 Advanced 3 Advanced RNNs 3 Highway and Residual Networks Autoencoders Generative Adversarial Networks Spatial Transformer Networks Topics 4 Computer Vision 4 Time Series Analysis Topic Modeling Recommendation Systems Pretrained Language Modeling Multitask Learning Low Shot Learning Reinforcement Learning 1. 1 Introduction to PythonIn this lesson we will learn the basics of the Python programming language version 3. 4 J theta sum_i y_i ln hat y_i sum_i y_i ln frac e X_iW_y sum e X_iW sum_i 0 ln 0. Data Preprocessing Feature Engineering Saving data Go to top top 1. com arunkumarramanan awesome machine learning ml frameworks Awesome Data Science for Beginners with Titanic Exploration https kaggle. Miscellaneous Softmax classifier is going to used widely in neural network architectures as the last layer since it produces class probabilities. com arunkumarramanan practical machine learning with pytorch Practical Machine Learning ML with TensorFlow The above highlighted work will soon be released and also be based on the below coursework Google Machine Learning Crash Course https developers. Note If you have preprocessing steps like standardization etc. Repeat steps 2 4 until model performs well. DataWe re going to create some simple dummy data to apply linear regression on. Proof for unstandardizing coefficients Note that both X and y were standardized. Advantages Can predict class probabilities given a set on inputs. 2 NumPyIn this lesson we will learn the basics of numerical analysis using the NumPy package. We will first understand the basic math behind it and then implement it in Python. com arunkumarramanan tensorflow tutorial and examples Data Scientist s Toolkits Awesome Data Science Resources https www. Disadvantages The model will perform well only when the data is linearly separable for classification. They each have a value associated with them. com topics machine learning GitHub Deep Learning Topic https github. Highly interpretable. ListsLists are objects in python that can hold a ordered sequence of numbers and text. Note Regularization is not just for linear regression. In the example dictionary below the keys are the name and eye_color variables. class 2 would look like 0 0 1 with the objective cost function to determine loss J. com topics deep learning GitHub Awesome Lists Topic https github. To Be Updated Soon Go to top top Credits Reference practicalAI Goku Mohandas https github. Inference InterpretabilityLinear regression offers the great advantage of being highly interpretable. Apply backpropagation to update the weights W using gradient descent. Learn object oriented ML to code for products not just tutorials. EvaluationThere are several evaluation techniques to see how well our model performed. hat y frac 1 1 e XW where hat y prediction in mathbb R NX1 N is the number of samples X inputs in mathbb R NXD D is the number of features W weights in mathbb R DX1 This is the binomial logistic regression. Categorical variablesIn our example the feature was a continuous variable but what if we also have features that are categorical One option is to treat the categorical variables as one hot encoded variables. Calculate the gradient of loss J theta w. Let s look at how to make some variables. We won t learn everything about Python but enough to do some basic machine learning. com arunkumarramanan hands on ml with scikit learn and tensorflow Practical Machine Learning with PyTorch https www. We need to standardize our data zero mean and unit variance in order to properly use SGD and optimize quickly. DataWe re going to the load the titanic dataset we looked at in lesson 03_Pandas. Scikit learn implementation Note The LogisticRegression class in Scikit learn uses coordinate descent to solve the fit. Besides MSE when we only have one feature we can visually inspect the model. LinearRegression with pros and cons vs. These are the diferent features pclass class of travel name full name of the passenger sex gender age numerical age sibsp of siblings spouse aboard parch number of parents child aboard ticket ticket number fare cost of the ticket cabin location of room emarked port that the passenger embarked at C Cherbourg S Southampton Q Queenstown survived survial metric 0 died 1 survived Exploratory Dats Analysis EDAWe re going to explore the Pandas library and see how we can explore and process our data. TuplesTuples are also objects in python that can hold data but you cannot replace values for this reason tuples are called immutable whereas lists are known as mutable. com arunkumarramanan data science and machine learning cheatsheets Awesome ML Frameworks and MNIST Classification https www. But linear regression doesn t fare well for classification asks where we want to probabilititcally determine the outcome for a given set on inputs. I am open to have your feedback for improving this kernel Hope you enjoyed this kernel Thanks for visiting my Kernel and please UPVOTE to stay connected and follow up the further updates Numerical example Text example int variable float variable text variable boolean variable int variables string variables Making a list Adding to a list Accessing items at specific location in a list the last item the second to last item Slicing Length of a list Replacing items in a list Combining lists Creating a tuple Adding values to a tuple Trying to change a tuples value you can t Creating a dictionary Changing the value for a key Adding new key value pairs Length of a dictionary If statement If statment with a boolean For loop goes from i 0 to i 2 same as x x 1 printing with multiple variables While loop same as x x 1 Create a function Use the function Function with multiple inputs Use the function Create the function Initialize the class For printing Example function Making an instance of a class Using a class s function Set seed for reproducability Scalars scalar 1 D Array notice the float datatype 3 D array matrix Functions Indexing Slicing Integer array indexing Boolean array indexing Basic math or x y or x y or x y Dot product we can specify dtype Sum across a dimension adds all elements add numbers in each column add numbers in each row Transposing Tile Broadcasting Reshaping Removing dimensions squeeze dim 1 Adding dimensions expand dim 1 Read from CSV to Pandas DataFrame First five items Describe features Histograms Unique values Selecting data by feature Filtering only the female data appear Sorting Grouping Selecting row iloc gets rows or columns at particular positions in the index so it only takes integers Selecting specific value Selecting by index loc gets rows or columns with particular labels from the index Rows with at least one NaN value Drop rows with Nan values removes rows with any NaN values reset s row indexes in case any rows were dropped Dropping multiple rows we won t use text features for our initial basic models Map feature values Lambda expressions to create new features Reorganize headers Saving dataframe to CSV See your saved file Arguments Set seed for reproducability Generate synthetic data Generate random linear data Scatter plot Import packages Create data splits Standardize the data mean 0 std 1 using training data Apply scaler on training and test data Check mean should be 0 std should be 1 Initialize the model Train Predictions unstandardize them Train and test MSE Figure size Plot train data Plot test data Show plots Feed in your own inputs Unstandardize coefficients 3. J theta sum_i y_i ln hat y_i sum_i y_i ln frac e X_iW_y sum e X_iW y 0 0 1 hat y 0. We want to use this optimization approach because we will be using this for the models in subsequent lessons. Note Since we standardized our inputs and outputs for gradient descent we need to apply an operation to our coefficients and intercept to interpret them. TODO polynomial regression simple example with normal equation method sklearn. We will also look at ways of interpreting the linear model. Additional resourcesThis was a very quick look at python and we ll be learning more in future lessons. The softmax classifier will use the linear equation z XW and normalize it to product the probabiltiy for class y given the inputs. the frac 1 2 is just for convenicing the derivative operation. com arunkumarramanan awesome computer vision resources to be updated Machine Learning and Deep Learning Awesome Tutorials https www. Credits Thanks to Practical AI Goku Mohandas and other contributers for such wonderful work Here are some of my kernel notebooks for Machine Learning and Data Science as follows Upvote them if you like them Awesome Deep Learning Basics and Resources https www. t to the model weights. This is because we cannot apply any knowledge gained from the test set accidentally during preprocessing training. W W alpha frac partial J partial W 5. ", "id": "arunkumarramanan/practical-machine-learning-with-pytorch-tbu", "size": "20504", "language": "python", "html_url": "https://www.kaggle.com/code/arunkumarramanan/practical-machine-learning-with-pytorch-tbu", "git_url": "https://www.kaggle.com/code/arunkumarramanan/practical-machine-learning-with-pytorch-tbu", "script": "__init__ Namespace SGDRegressor join_name Pets(object) preprocess numpy SGDClassifier add_two sklearn.model_selection matplotlib.pyplot pandas StandardScaler generate_data __str__ sklearn.linear_model argparse sklearn.preprocessing change_name train_test_split sklearn.linear_model.stochastic_gradient get_family_size ", "entities": "(('We', 'Python'), 'understand') (('hat W _ j sigma_y sigma_j bar _ j _ k sigma_j frac 1 sigma_y RegularizationRegularization', 'fitting'), 'mathbb') (('94cc2975433f', 'good outliers'), 'machine') (('softmax classifier', 'class probabilities'), 'normalize') (('Inference InterpretabilityLinear regression', 'great advantage'), 'offer') (('We', 'machine basic learning'), 'win') (('function', 'it'), 'calculate') (('it', 'certain class'), 'use') (('machine practitioners Machine Learning Crash learning Course', 'world case practice real exercises'), 'course') (('lists', 'reason'), 'be') (('model', 'inputs'), 'lead') (('Now you', 'linear model'), 'concat') (('we', 'subsequent lessons'), 'want') (('Disadvantages Sensitive', 'entropy loss'), 'minimize') (('we', 'accidentally training'), 'be') (('you', 'first operations'), 'calculate') (('dictionary', 'keys'), 'be') (('we', 'them'), 'note') (('repeatedly condition', 'something'), 'use') (('we', 'them'), 'penalize') (('data', 'first going'), 'upload') (('statements', 'conditionally something'), 'use') (('lesson we', 'NumPy package'), 'learn') (('s', 'how variables'), 'let') (('above highlighted work', 'coursework Google Machine Learning Crash Course https soon also below developers'), 'com') (('we', 'continuos regression tasks'), 'cover') (('we', 'logistic regression softmax multinomial classifier'), 'have') (('softmax classifier', 'inputs'), 'use') (('com', 'python'), 'learn') (('Scikit SGDClassifier which', 'gradient stochastic descent'), 'go') (('we', 'visually model'), 'besides') (('you', 'standardization'), 'note') (('we', 'more future lessons'), 'be') (('LogisticRegression class', 'fit'), 'learn') (('1 y frac partial y partial W_y', 'frac e sum e XW frac XW 1 e e'), 'w_j') (('you', 'them'), 'be') (('it', 'class probabilities'), 'go') (('here we', 'later lessons'), 'basic') (('ClassesClasses', 'Python programming'), 'be') (('they', 'models'), 'be') (('you', 'reference manual https NumPy docs'), 'be') (('lesson we', 'Python programming language version'), 'introduction') (('FunctionsFunctions', 'code'), 'be') (('65 10', 'MSE Unstandardize coefficients'), 'initialize') (('com data arunkumarramanan learning', 'Awesome ML Frameworks'), 'cheatsheet') (('number', 'inputs'), 'XW') (('we', 'lesson'), 'go') (('you', 'linear model'), 'be') (('X', 'coefficients'), 'note') (('class three predicted probabilities', '0'), 'for') (('Overview hat y hat where prediction', 'linear model'), 'XW') (('s', 'Pandas dataframe'), 'load') (('mutually set', 'only one class'), 'let') (('Feed', 'predictions'), 'input') (('3 we', 'Python Pandas library'), 'PandasIn') (('Randomly', 'weights'), 'initialize') (('Feed', 'logits'), 'input') (('it', 'curated list'), 'feedback') (('Saving data', 'top top 1'), 'go') (('main idea', '0'), 'be') (('how we', 'data'), 'be') (('well only when data', 'linearly classification'), 'perform') (('com arunkumarramanan', 'R Awesome Tutorials https www'), 'awesome') (('i', 'frac'), 'theta') (('common objective function', 'logistics regression'), 'be') (('class', 'loss'), 'look') (('lesson we', 'linear regression'), 'learn') (('dictionary', 'same keys'), 'have') (('which', 'L1 penalties'), 'be') (('we', 'y W'), 'interpret') (('Train 0 1 Predictions', 'Unstandardize coefficients'), 'be') (('it', 'importance output variable'), 'have') (('that', 'numbers'), 'be') (('other way we', 'loss J theta'), 'be') (('They', 'them'), 'have') (('frac', '1 just derivative operation'), 'be') (('Note Regularization', 'just linear regression'), 'be') (('Advantages', 'inputs'), 'predict') (('that', 'line easily plane'), 'RegressionIn') (('Training X y', 'descent stochastic gradient SGD'), 'datum') (('Scikit SGDRegressor which', 'gradient stochastic descent'), 'go') (('Train Probabilities 0 1 Predictions', 'them'), 'Set') (('you', 'wanna it'), 'com') (('that', 'predicted outcomes'), 'be') (('common objective function', 'linear regression'), 'be') (('This', 'mathbb R DX1'), 'frac') (('DataWe', 'linear regression'), 'go') (('We', 'properly SGD'), 'need') (('where we', 'inputs'), 'ask') (('cross', 'J log following theta ln hat negative likelihood'), '4') (('that', 'one hot encoded variables'), 'variablesIn') (('Pandas', 'python data great analysis'), 'be') (('you', 'Free Python Course https www'), 'check') (('We', 'linear model'), 'look') (('then they', 'numerical operations'), 'need') (('updates', 'correct class'), 'penalize') (('model', 'perfect linear equation'), 'frac') (('python that', 'value key pairs'), 'be') (('com arunkumarramanan', 'Python Awesome Tutorials https www'), 'awesome') (('LinearRegression class', 'fit'), 'learn') (('we', 'future lessons'), 'use') (('We', 'public link'), 'go') (('kernel', 'data'), 'ML') (('it', 'exercises'), 'recommend') ", "extra": "['gender', 'outcome', 'test']", "label": "Perfect_files", "potential_description_queries": ["account", "advantage", "age", "appear", "apply", "approach", "array", "associated", "backpropagation", "basic", "best", "binary", "boolean", "cabin", "case", "categorical", "check", "child", "classification", "classifier", "code", "coefficient", "column", "computer", "condition", "content", "correct", "cost", "could", "course", "create", "data", "dataframe", "dataset", "derivative", "dictionary", "difference", "dim", "dimension", "direction", "distance", "drive", "dummy", "entropy", "equation", "error", "evaluation", "everything", "expand", "explore", "fare", "feature", "feedback", "file", "fit", "float", "following", "frac", "function", "future", "gender", "generation", "gradient descent", "gradient stochastic descent", "gradient", "help", "high", "hot", "idea", "implement", "implementation", "importance", "including", "increase", "index", "initialize", "instance", "int", "intuition", "io", "item", "kernel", "key", "knowledge", "language", "layer", "lead", "learn", "learning", "least", "lesson", "let", "library", "line", "linear", "link", "list", "load", "local", "log", "look", "loop", "main", "manual", "math", "matrix", "mean", "method", "metric", "minimize", "ml", "model", "multiple", "my", "name", "need", "negative", "network", "neural", "new", "normal", "normalize", "not", "notebook", "number", "numerical", "object", "objective", "open", "operation", "optimization", "optimize", "option", "order", "ordered", "out", "outcome", "output", "overfitting", "partial", "passenger", "perform", "plot", "practice", "predict", "prediction", "preprocessing", "printing", "probability", "product", "public", "python", "pytorch", "random", "re", "reason", "reduce", "reference", "regression", "regularization", "replace", "reset", "right", "room", "row", "scaled", "scaler", "science", "scikit", "second", "select", "separate", "sequence", "set", "several", "sex", "sigmoid", "size", "softmax", "something", "sparse", "standardization", "std", "string", "sum", "support", "survived", "target", "technique", "tensorflow", "test", "text", "those", "threshold", "ticket", "titanic", "train", "training", "tuple", "tutorial", "unit", "until", "up", "update", "value", "variable", "variance", "vector", "version", "video", "vision", "while", "work", "world"], "potential_description_queries_len": 214, "potential_script_queries": ["argparse", "numpy"], "potential_script_queries_len": 2, "potential_entities_queries": ["case", "derivative", "frac", "gradient", "key", "learning", "log", "negative", "output", "practice", "softmax"], "potential_entities_queries_len": 11, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 216}