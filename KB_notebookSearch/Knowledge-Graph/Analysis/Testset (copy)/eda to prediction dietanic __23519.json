{"name": "eda to prediction dietanic ", "full_name": " h1 EDA To Prediction DieTanic h3 Ashwin h3 Sometimes life has a cruel sense of humor giving you the thing you always wanted at the worst time possible h2 Contents of the Notebook h4 Part1 Exploratory Data Analysis EDA h4 Part2 Feature Engineering and Data Cleaning h4 Part3 Predictive Modeling h2 Part1 Exploratory Data Analysis EDA h3 How many Survived h2 Types Of Features h3 Categorical Features h3 Ordinal Features h3 Continous Feature h2 Analysing The Features h2 Sex Categorical Feature h2 Pclass Ordinal Feature h2 Age Continous Feature h4 Observations h3 Filling NaN Ages h3 Observations h2 Embarked Categorical Value h3 Chances for Survival by Port Of Embarkation h3 Observations h3 Observations h3 Filling Embarked NaN h2 SibSip Discrete Feature h3 Observations h2 Parch h3 Observations h2 Fare Continous Feature h2 Observations in a Nutshell for all features h2 Correlation Between The Features h3 Interpreting The Heatmap h2 Part2 Feature Engineering and Data Cleaning h2 Age band h4 Problem With Age Feature h2 Family Size and Alone h2 Fare Range h2 Converting String Values into Numeric h3 Dropping UnNeeded Features h1 Part3 Predictive Modeling h3 Radial Support Vector Machines rbf SVM h3 Linear Support Vector Machine linear SVM h3 Logistic Regression h3 Decision Tree h3 K Nearest Neighbours KNN h3 Gaussian Naive Bayes h3 Random Forests h1 Cross Validation h2 Confusion Matrix h3 Interpreting Confusion Matrix h3 Hyper Parameters Tuning h4 SVM h4 Random Forests h1 Ensembling h2 Voting Classifier h2 Bagging h4 Bagged KNN h4 Bagged DecisionTree h2 Boosting h4 AdaBoost Adaptive Boosting h4 Stochastic Gradient Boosting h4 XGBoost h4 Hyper Parameter Tuning for AdaBoost h3 Confusion Matrix for the Best Model h2 Feature Importance h4 Observations h3 Thanks a lot for having a look at this notebook If you found this notebook useful Do Upvote ", "stargazers_count": 0, "forks_count": 0, "description": "Okay so the maximum age of a passenger was 80. 5 million to build the Titanic and it sunk under the ocean due to collision. SibSip Discrete FeatureThis feature represents whether a person is alone or with his family members. POSITIVE CORRELATION If an increase in feature A leads to increase in feature B then they are positively correlated. So now we will predict the whether the Passenger will survive or not using some great Classification Algorithms. What s In A Name Feature pOkay so here we are using the Regex A Za z. Filling Embarked NaNAs we saw that maximum passengers boarded from Port S we replace NaN with S. Passengers between age group 15 to 35 died a lot. Passengers with their parents onboard have greater chance of survival. ParchThe crosstab again shows that larger families were in Pclass3. Family_Size 0 means that the passeneger is alone. Observations 1 Some of the common important features are Initial Fare_cat Pclass Family_Size. The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S. They are also known as Nominal Variables. This feature is the summation of Parch and SibSp. First let us understand the different types of features. As the training and testing data changes the accuracy will also change. So lets divide the range from 0 80 into 5 bins. Confusion MatrixIt gives the number of correct and incorrect classifications made by the classifier. It gives an average prediction result based on the prediction of all the submodels. It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family. It will keep me motivated. Eg If I say to group or arrange Sports Person by Sex We can easily segregate them by Male and Female. The crosstab shows that Person with SibSp 3 were all in Pclass3. 4 Port Q had almost 95 of the passengers were from Pclass3. Converting String Values into NumericSince we cannot pass strings to a machine learning model we need to convert features loke Sex Embarked etc into numeric values. But with that we cannot accurately predict or tell whether a passenger will survive or die. The chances of survival is good for somebody who has 1 3 parents on the ship. By looking at all the matrices we can say that rbf SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived. For eg If we have a feature like Height with values Tall Medium Short then Height is a ordinal variable. For RandomForest score is abt 81. We will tune the hyper parameters for the 2 best classifiers i. 4 Important Features Extraction. Lets Dive in little bit more and check for other interesting observations. We can use KNN with small value of n_neighbours as small value of n_neighbours. Still the number of women saved is almost twice the number of males saved. Lets check survival rate with Sex and Pclass Together. To replace these NaN values we can assign them the mean age of the dataset. EnsemblingEnsembling is a good way to increase the accuracy or performance of a model. Money Matters 3 Port Q looks looks to be unlukiest for Men as almost all were from Pclass 3. Looking at the CrossTab and the FactorPlot we can easily infer that survival for Women from Pclass1 is about 95 96 as only 3 out of 94 Women from Pclass1 died. It however reduces as the number goes up. 05 Confusion Matrix for the Best Model Feature ImportanceWe can see the important features for various classifiers like RandomForests AdaBoost etc. Now in the next iteration the learner will focus more on the wrongly predicted instances or give more weight to it. Let s say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90. Alone will denote whether a passenger is alone or not. Looking upon the feature we can see that the names have a salutation like Mr or Mrs. There looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the standards reduces. Observations 1 The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Pclass. The survival rate for Pclass3 is very low. So money and status matters. It is a step by step enhancement of a weak model. 3 For males the survival chances decreases with an increase in age. Fare_Range We have the fare_cat feature. The answer is No because we can t determine which all instances will the classifier will use to train itself. Hyper Parameters TuningThe machine learning models are like a Black Box. Thus it has made more mistakes by predicting dead as survived. Observations in a Nutshell for all features Sex The chance of survival for women is high as compared to men. Observations Here too the results are quite similar. The Objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. 3 Maximum number of deaths were in the age group of 30 40. Ticket It is any random string that cannot be categorised. This means that both the features are containing highly similar information and there is very little or no variance in information. Also we can get or add new features by observing or extracting information from other features. Some of the features being Sex Port Of Embarcation Age etc. The reason may be Pclass The reason is Pclass. A value 1 means perfect negative correlation. Now we cannot sort or give any ordering to such variables. For example gender is a categorical variable having two categories male and female. For this we will use pandas. Cross ValidationMany a times the data is imbalanced i. Interpreting Confusion MatrixThe left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong prredictions made. Embarked Categorical Value Chances for Survival by Port Of EmbarkationThe chances for survival for Port C is highest around 0. com arthurtok introduction to ensembling stacking in python Thanks a lot for having a look at this notebook. 16 with n_estimators 200 and learning_rate 0. Lets examine this further. The survival rates for a women on the ship is around 75 while that for men in around 18 19. This looks to be a very important feature for modeling. 3 The Embark S looks to the port from where majority of the rich people boarded. But we can clearly see that Passenegers Of Pclass 1 were given a very high priority while rescue. The classification accuracy can be sometimes misleading due to imbalance. Now we cannot pass the Fare_Range values as it is. Lets say we want to buy a phone and ask many people about it based on various parameters. It may increase or decrease. Now from the above heatmap we can see that the features are not much correlated. Voting ClassifierIt is the simplest way of combining predictions from many different simple machine learning models. XGBoostWe got the highest accuracy for AdaBoost. We can get a summarized result with the help of confusion matrix which shows where did the model go wrong or which class did the model predict wrong. of correct predictions are 491 for dead 247 for survived with the mean CV accuracy being 491 247 891 82. We will try to increase it with Hyper Parameter Tuning Hyper Parameter Tuning for AdaBoostThe maximum accuracy we can get with AdaBoost is 83. Before understanding the plot let us see what exactly correlation is. NEGATIVE CORRELATION If an increase in feature A leads to decrease in feature B then they are negatively correlated. 2 Survival chances for Passenegers aged 20 50 from Pclass1 is high and is even better for Women. Thus we should train and test our algorithm on each and every instance of the dataset. Contents of the Notebook Part1 Exploratory Data Analysis EDA 1 Analysis of the features. Okay so there are some misspelled Initials like Mlle or Mme that stand for Miss. Clearly if you are alone or family_size 0 then chances for survival is very low. This is called K Fold Cross Validation. Here we can have a relative sort in the variable. 3 We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. It is evident that irrespective of Pclass Women were given first priority while rescue. e children looks to be good irrespective of the Pclass. 2 Port S looks to be very unlucky for Pclass3 Passenegers as the survival rate for both men and women is very low. However we can see the feature Initial which is at the top in many classifiers. Pclass Ordinal FeaturePeople say Money Can t Buy Everything. So we successfully extract the Initials from the Name. Even Men from Pclass1 have a very low survival rate. Is there any way to find out what age band does the passenger lie Bingo we can check the Name feature. 2 The oldest Passenger was saved 80 years. We will try to check the survival rate by using the different features of the dataset. As we had seen earlier the Age feature has 177 null values. This feature may become an important feature during modeling along with the Sex. This is known as MultiColinearity as both of them contains almost the same information. Part3 Predictive ModelingWe have gained some insights from the EDA part. Age_band Problem With Age Feature As I have mentioned earlier that Age is a continous feature there is a problem with Continous Variables in Machine Learning Models. Age Children less than 5 10 years do have a high chance of survival. That s why the name DieTanic. Gaussian Naive Bayes Random ForestsThe accuracy of a model is not the only factor that determines the robustness of the classifier. I will try to fix them. Out of 891 passengers in training set only around 350 survived i. I will replace them with Miss and same thing for other values. Bagged KNNBagging works best with models with high variance. But we can change the dafault base_estimator to any algorithm of our choice. Filling NaN Ages Observations 1 The Toddlers age 5 were saved in large numbers The Women and Child First Policy. AdaBoost Adaptive Boosting The weak learner or estimator in this case is a Decsion Tree. Observations 1 Maximum passenegers boarded from S. This also looks to be an important feature for the model. 2 Removing redundant features. How many Survived It is evident that not many passengers survived the accident. Wow a free luxorious ride. Now the above correlation plot we can see some positively related features. Such a materialistic world. Being alone also proves to be fatal and the chances for survival decreases when somebody has 4 parents on the ship. Part2 Feature Engineering and Data CleaningNow what is Feature Engineering Whenever we are given a dataset with features it is not necessary that all the features will be important. This is known as model variance. The submodels or the basemodels are all of diiferent types. Surprisingly the survival for families with 5 8 members is 0. The accuracies and errors are then averaged to get a average accuracy of the algorithm. PassengerId Cannot be categorised. As this is also continous we can convert into discrete values by using binning. EDA To Prediction DieTanic Ashwin Sometimes life has a cruel sense of humor giving you the thing you always wanted at the worst time possible. But is it the best Lets check other features. To overcome this and get a generalized model we use Cross Validation. Now this is problematic. So we can carry on with all features. Still the chances for survival is low here that is because many passengers from Pclass3 around 81 didn t survive. We had already seen the positive correlation between Sex and Initial so they both refer to the gender. 4 An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. So do you think we should use both of them as one of them is redundant. Now this seems to be very good accuracy for a classifier but can we confirm that it will be 90 for all the new test sets that come over. A value 1 means perfect positive correlation. Ordinal Features An ordinal variable is similar to categorical values but the difference between them is that we can have relative ordering or sorting between the values. Bagged DecisionTree BoostingBoosting is an ensembling technique which uses sequential learning of classifiers. 3 Similarly the Pclass and Fare_cat refer to the status of the passengers and Family_Size with Alone Parch and SibSp. So then we can make a strong judgement about a single product after analysing all different parameters. The number of men on the ship is lot more than the number of women. An example would be getting the Initals feature using the Name Feature. Some of them being SibSp andd Family_Size and Parch and Family_Size and some negative ones like Alone and Family_Size. Thus with cross validation we can achieve a generalised model. Dropping UnNeeded Features Name We don t need name feature as it cannot be converted into any categorical value. SVM Random ForestsThe best score for Rbf Svm is 82. I hope all of you did gain some insights to Machine Learning. com headsortails pytanic 3 For Python Introduction to Ensembling Stacking by Anisotropic https www. e there may be a high number of class1 instances but less number of other class instances. Now lets say that two features are highly or perfectly correlated so the increase in one leads to increase in the other. We should convert it into singleton values same as we did in Age_Band Clearly as the Fare_cat increases the survival chances increases. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it. Lets check the accuracies over various values of n_neighbours. Majority of them being from Pclass3. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. But the problem is there were many people with many different ages. Some other great notebooks for Machine Learning are 1 For R Divide and Conquer by Oscar Takeshita https www. the survival rate decreases as the age increases irrespective of the Pclass. 2 Finding any relations or trends considering multiple features. Parch SibSp Having 1 2 siblings spouse on board or 1 3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with you. Now if I say to group them by their Age then how would you do it If there are 30 Persons there may be 30 age values. We reserve 1 part for testing and train the algorithm over the 4 parts. Fare Continous FeatureThe lowest fare is 0. Ensembling can be done in ways like 1 Voting Classifier2 Bagging3 Boosting. 2 The Passengers from C look to be lucky as a good proportion of them survived. If You Like the notebook and think that it helped you. For family size 4 the chances decrease too. Stochastic Gradient BoostingHere too the weak learner is a Decision Tree. While making or training models we should try to eliminate redundant features as it reduces training time and many such advantages. Lets analyse other features. The graph roughly decreases if the number of siblings increase. Pclass There is a visible trend that being a 1st class passenger gives you better chances of survival. Categorical Features in the dataset Sex Embarked. Passengers at Q were all from Pclass3. So what qcut does is it splits or arranges the values according the number of bins we have passed. The default value is 5. Correlation Between The Features Interpreting The HeatmapThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Family_Size and AloneAt this point we can create a new feature called Family_size and Alone and analyse it. Lets consider the first plot for rbf SVM 1 The no. Following are the algorithms I will use to make the model 1 Logistic Regression2 Support Vector Machines Linear and radial 3 Random Forest4 K Nearest Neighbours5 Naive Bayes6 Decision Tree7 Logistic Regression Radial Support Vector Machines rbf SVM Linear Support Vector Machine linear SVM Logistic Regression Decision Tree K Nearest Neighbours KNN Now the accuracy for the KNN model changes as we change the values for n_neighbours attribute. 2 The Sex feature doesn t seem to give any importance which is shocking as we had seen earlier that Sex combined with Pclass was giving a very good differentiating factor. It is imminent that all the large families in Pclass3 3 died. Part2 Feature Engineering and Data Cleaning 1 Adding any few features. There are some default parameter values for this Black Box which we can tune or change to get a better model. We need to convert these continous values into categorical values by either Binning or Normalisation. Like the C and gamma in the SVM model and similarly different parameters for different classifiers are called the hyper parameters which we can tune to change the learning rate of the algorithm and get a better model. Sex looks to be important only in RandomForests. As discussed above we can clearly see that as the fare_range increases the chances of survival increases. For women the chance of survival from Pclass1 is almost 1 and is high too for those from Pclass2. Looks like Pclass is also an important feature. Types Of Features Categorical Features A categorical variable is one that has two or more categories and each value in that feature can be categorised by them. Lisa Kleypas The sinking of the Titanic is one of the most infamous shipwrecks in history. Continous Features in the dataset Age Analysing The Features Sex Categorical FeatureThis looks interesting. Age Continous Feature Observations 1 The number of children increases with Pclass and the survival rate for passenegers below Age 10 i. I will be using binning i. Fare We have the Fare_cat feature so unneeded Cabin A lot of NaN values and also many passengers have multiple cabins. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle. Now this iterative process continous and new classifers are added to the model until the limit is reached on the accuracy. Fare_RangeSince fare is also a continous feature we need to convert it into ordinal value. On April 15 1912 during her maiden voyage the Titanic sank after colliding with an iceberg killing 1502 out of 2224 passengers and crew. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. 1 The K Fold Cross Validation works by first dividing the dataset into k subsets. We use FactorPlot in this case because they make the seperation of categorical values easy. 3 Converting features into suitable form for modeling. This is Ensembling which improves the stability of the model. The highest correlation is between SibSp and Parch i. Ordinal Features in the dataset PClass Continous Feature A feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column. Then we can take an average of all the noted accuracies over the dataset. Thus we can assign the mean values of Mr and Mrs to the respective groups. 8 with n_estimators 900. In simple words it is the combination of various simple models to create a single powerful model. Due to the averaging there is reduction in variance. There maybe be many redundant features which should be eliminated. Age We have the Age_band feature so no need of this. Embarked This is a very interesting feature. That is if I have a family on board I will try to save them instead of saving myself first. 4 of the total training set survived the crash. So what it does is it looks for strings which lie between A Z or a z and followed by a. Thus it will try to predict the wrong instance correctly. Part3 Predictive Modeling1 Running Basic Algorithms. Now the model will get some instances right while some wrong. com pliptor divide and conquer 0 82297 notebook 2 For Python Pytanic by Heads and Tails https www. So if we pass for 5 bins it will arrange the values equally spaced into 5 seperate bins or value ranges. Part1 Exploratory Data Analysis EDA The Age Cabin and Embarked have null values. Boosting works as follows A model is first trained on the complete dataset. This is known as Hyper Parameter Tuning. Unlike Voting Classifier Bagging makes use of similar classifiers. Sibling brother sister stepbrother stepsisterSpouse husband wife Observations The barplot and factorplot shows that if a passenger is alone onboard with no siblings he have 34. 55 while it is lowest for S. 2 Let s say we divide the dataset into k 5 parts. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn t. How do we check features how do we add new features and some Machine Learning Concepts. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers. e group a range of ages into a single bin or assign them a single value. 8 which we did get earlier. 2 Errors Wrongly Classified 58 dead people as survived and 95 survived as dead. So this is a useless feature. We just cant assign a 4 year kid with the mean age that is 29 years. Even though the the number of Passengers in Pclass 3 were a lot higher still the number of survival from them is very low somewhere around 25. BaggingBagging is a general ensemble method. If you found this notebook useful Do Upvote. Also we will tranform the existing relevant features to suitable form for Predictive Modeling. The Women and Child first policy thus holds true irrespective of the class. Lets see if we can get any new features and eliminate a few. e the SVM and RandomForests. An example for this can be Decision Tree or Random Forests. This is a very unforgetable disaster that no one in the world can forget. For Pclass 1 survived is around 63 while for Pclass2 is around 48. ", "id": "ash316/eda-to-prediction-dietanic", "size": "23519", "language": "python", "html_url": "https://www.kaggle.com/code/ash316/eda-to-prediction-dietanic", "git_url": "https://www.kaggle.com/code/ash316/eda-to-prediction-dietanic", "script": "sklearn.metrics confusion_matrix #for confusion matrix DecisionTreeClassifier #Decision Tree sklearn.naive_bayes sklearn.tree AdaBoostClassifier BaggingClassifier cross_val_score #score evaluation seaborn numpy cross_val_predict #prediction LogisticRegression #logistic regression GradientBoostingClassifier VotingClassifier sklearn.ensemble sklearn sklearn.model_selection train_test_split #training and testing data split matplotlib.pyplot pandas KFold #for K-fold cross validation metrics #accuracy measure GridSearchCV KNeighborsClassifier #KNN sklearn.neighbors RandomForestClassifier #Random Forest sklearn.linear_model GaussianNB #Naive bayes xgboost svm #support vector Machine ", "entities": "(('only that', 'classifier'), 'be') (('who', 'passengers'), 'need') (('Thus we', 'respective groups'), 'assign') (('earlier Age', 'Machine Learning Models'), 'Problem') (('We', 'Binning'), 'need') (('we', 'better model'), 'call') (('So we', 'Name'), 'extract') (('very good begineers', 'Kaggle'), 'be') (('children', 'good Pclass'), 'look') (('Hyper TuningThe machine learning models', 'Black Box'), 'Parameters') (('Observations 1 Maximum passenegers', 'S.'), 'board') (('seperation', 'categorical values'), 'use') (('Sex chance', 'men'), 'observation') (('It', 'submodels'), 'give') (('also continous we', 'ordinal value'), 'be') (('so maximum age', 'passenger'), 'be') (('we', 'various parameters'), 'say') (('submodels', 'diiferent types'), 'be') (('Looks', 'Pclass'), 'be') (('Passengers', 'survival'), 'have') (('1 Toddlers', '5 large numbers'), 'fill') (('classification accuracy', 'sometimes imbalance'), 'be') (('Sex Categorical FeatureThis', 'Features'), 'Features') (('feature', 'Parch'), 'be') (('features', 'features'), 'CleaningNow') (('We', 'classifiers 2 best i.'), 'tune') (('you', 'always worst time'), 'EDA') (('it', 'you'), 'like') (('value', '1 perfect positive correlation'), 'mean') (('Survival 2 chances', 'even Women'), 'be') (('chance', 'Pclass2'), 'be') (('They', 'Nominal also Variables'), 'know') (('We', 'dataset'), 'try') (('it', 'collision'), 'million') (('4 chances', 'family size'), 'decrease') (('both', 'almost same information'), 'know') (('Sex', 'only RandomForests'), 'look') (('ensembling which', 'classifiers'), 'be') (('Age Children', 'survival'), 'have') (('Ensembling', 'ways'), 'do') (('Age Continous Feature 1 number', 'Age'), 'observation') (('Port Money Matters 3 Q', 'almost all Pclass'), 'look') (('one', 'them'), 'think') (('it', 'Fare_Range values'), 'pass') (('person', 'family members'), 'represent') (('Passengers', 'age group'), 'die') (('it', 'categorical value'), 'drop') (('Cross Validation', 'k subsets'), '1') (('Thus it', 'dead'), 'make') (('large families', 'Pclass3'), 'be') (('again larger families', 'Pclass3'), 'show') (('clearly Passenegers', '1 very high priority'), 'see') (('Age We', 'so this'), 'have') (('Even Men', 'survival very low rate'), 'have') (('I', 'other values'), 'replace') (('Lets', 'other features'), 'be') (('rather large family', 'you'), 'show') (('point we', 'it'), 'family_size') (('survival rate', 'passengers'), 'give') (('we', '5 parts'), 'let') (('It', 'weak model'), 'be') (('Some', 'SibSp negative Alone'), 'be') (('we', 'Cross Validation'), 'overcome') (('then chances', 'survival'), 'be') (('exactly correlation', 'plot'), 'let') (('Also we', 'other features'), 'get') (('easily survival', 'Pclass1'), 'infer') (('when somebody', 'ship'), 'prove') (('categorical two categories', 'example'), 'be') (('We', '4 parts'), 'reserve') (('Lisa sinking', 'history'), 'Kleypas') (('we', 'better model'), 'be') (('highest correlation', 'SibSp'), 'be') (('right diagonal', 'wrong prredictions'), 'leave') (('891 passengers', 'training'), 'survive') (('Lets', 'n_neighbours'), 'check') (('value', '1 perfect negative correlation'), 'mean') (('which', 'model'), 'be') (('Boosting', 'first complete dataset'), 'train') (('Pclass visible being', 'survival'), 'be') (('it', 'features column'), 'say') (('Exploratory Data Part1 Analysis', 'features'), 'content') (('all', 'Machine Learning'), 'hope') (('so here we', 'Regex A Za z.'), 's') (('how we', 'new features'), 'check') (('standards', 'Pclass1'), 'look') (('two features', 'other'), 'say') (('chances', 'survival'), 'convert') (('we', 'generalised model'), 'achieve') (('fare_range', 'survival increases'), 'see') (('algorithm', 'training other set'), '4') (('First us', 'features'), 'let') (('Tall Medium then Height', 'values'), 'for') (('Lets', 'Sex'), 'check') (('it', 'single powerful model'), 'be') (('we', 'alphabets'), 'be') (('features', 'very little information'), 'mean') (('how workflow', 'modeling predictive problem'), 'be') (('then how you', 'it'), 'do') (('I', 'instead myself'), 'be') (('It', 'predictions'), 'work') (('feature', 'important Sex'), 'become') (('example', 'this'), 'be') (('So lets', '80 5 bins'), 'divide') (('CV mean accuracy', 'dead 247'), 'be') (('Surprisingly survival', '5 8 members'), 'be') (('Thus we', 'dataset'), 'train') (('Lets', 'rbf SVM'), 'consider') (('who', 'family'), 'be') (('Exploratory Data Analysis Age Cabin', 'null values'), 'Part1') (('Age earlier feature', '177 null values'), 'have') (('Passenger', 'Classification great Algorithms'), 'predict') (('Confusion Matrix', 'RandomForests AdaBoost etc'), '05') (('1 Some', 'common important features'), 'observation') (('names', 'Mr'), 'see') (('classifier', 'itself'), 'be') (('This', 'very important modeling'), 'look') (('t', 'Everything'), 'say') (('we', 'n_neighbours attribute'), 'be') (('values', 'seperate equally 5 bins'), 'arrange') (('he', '34'), 'sible') (('1 survived', '48'), 'be') (('Thus it', 'wrong instance'), 'try') (('two categories', 'them'), 'type') (('that', 'test new sets'), 'seem') (('Bagged KNNBagging', 'high variance'), 'work') (('then they', 'feature B'), 'CORRELATION') (('accuracies', 'algorithm'), 'average') (('model', 'confusion matrix'), 'get') (('learner', 'it'), 'focus') (('irrespective', 'first priority'), 'be') (('notebook', 'Upvote'), 'find') (('survival 1 chances', 'irrespective Pclass'), 'observation') (('Predictive Part3 ModelingWe', 'EDA part'), 'gain') (('it', '90'), 'let') (('who', 'ship'), 'be') (('survival rates', 'around 18 19'), 'be') (('Here we', 'variable'), 'have') (('Sex', 'Categorical dataset'), 'Features') (('where majority', 'rich people'), '3') (('features', 'Now above heatmap'), 'see') (('one', 'world'), 'be') (('3 lot still number', 'very somewhere 25'), 'be') (('number', 'women'), 'be') (('correlation Now above we', 'positively related features'), 'plot') (('it', 'training time'), 'try') (('Voting Classifier Bagging', 'similar classifiers'), 'make') (('also we', 'binning'), 'be') (('Women first policy', 'class'), 'hold') (('survival rate', 'men'), 'look') (('almost 95', 'Pclass3'), 'have') (('Now we', 'such variables'), 'sort') (('Now model', 'instances'), 'get') (('accurately passenger', 'that'), 'predict') (('who', 'correctly passengers'), 'have') (('we', 'Name feature'), 'be') (('we', 'bins'), 'be') (('even majority', 'S.'), 'look') (('that', 'mean age'), 'assign') (('survival rate', 'Pclass3'), 'be') (('AdaBoost Adaptive', 'case'), 'be') (('graph', 'siblings roughly increase'), 'decrease') (('XGBoostWe', 'AdaBoost'), 'get') (('we', 'choice'), 'change') (('3 Maximum number', '30 40'), 'be') (('earlier Sex', 'differentiating very good factor'), '2') (('they', 'gender'), 'see') (('95', 'Wrongly 58 dead people'), 'classify') (('Person', '3 all Pclass3'), 'show') (('which', 'a.'), 'be') (('We', 'Male'), 'segregate') (('Also we', 'Predictive Modeling'), 'tranform') (('we', 'dataset'), 'assign') (('good proportion', 'them'), '2') (('we', 'S.'), 'Embarked') (('Voting ClassifierIt', 'machine learning many different simple models'), 'be') (('limit', 'accuracy'), 'add') (('3 We', 'other parts'), 'continue') (('4', 'crash'), 'survive') (('SVM Random ForestsThe best score', 'Rbf Svm'), 'be') (('EnsemblingEnsembling', 'model'), 'be') (('chances', 'age'), '3') (('problem', 'many many different ages'), 'be') (('we', 'values'), 'feature') (('So then we', 'different parameters'), 'make') (('as possible even newbies', 'it'), 'try') (('Then we', 'dataset'), 'take') (('other great notebooks', 'Oscar Takeshita https www'), 'be') (('that', 'Miss.'), 'be') (('age', 'Pclass'), 'decrease') (('Still number', 'almost twice males'), 'be') (('features', 'numeric values'), 'pass') (('example', 'Name Feature'), 'get') (('we', 'few'), 'see') (('here that', 'didn t many Pclass3 around 81 survive'), 'be') (('also many passengers', 'multiple cabins'), 'fare') (('This', 'also important model'), 'look') (('Confusion MatrixIt', 'classifier'), 'give') (('which', 'many classifiers'), 'see') (('We', 'n_neighbours'), 'use') (('many passengers', 'accident'), 'be') (('we', 'AdaBoost'), 'try') ", "extra": "['gender', 'test', 'bag']", "label": "Perfect_files", "potential_description_queries": ["accuracy", "age", "algorithm", "answer", "assign", "average", "barplot", "basic", "become", "best", "bin", "bit", "board", "build", "case", "categorical", "check", "children", "classification", "classifier", "combined", "confusion", "consider", "convert", "correct", "correlation", "create", "data", "dataset", "default", "diagonal", "difference", "disaster", "discrete", "distribution", "eg", "ensemble", "ensembling", "estimator", "even", "every", "extract", "factor", "family", "fare", "feature", "find", "fix", "form", "found", "gamma", "gender", "general", "graph", "group", "heatmap", "help", "high", "hope", "idea", "importance", "increase", "instance", "iteration", "kid", "learner", "learning", "learning_rate", "left", "let", "life", "linear", "little", "look", "looking", "lot", "majority", "male", "matrix", "maximum", "mean", "men", "minimum", "model", "most", "multiple", "name", "need", "negative", "new", "next", "no", "not", "notebook", "null", "number", "numeric", "out", "overfit", "parameter", "part", "passenger", "people", "performance", "person", "plot", "point", "positive", "predict", "prediction", "problem", "product", "python", "random", "range", "reason", "relative", "replace", "result", "rich", "right", "save", "saving", "science", "score", "sense", "set", "ship", "similar", "single", "size", "sort", "start", "step", "string", "survival", "survived", "technique", "test", "testing", "think", "those", "time", "total", "train", "training", "trend", "try", "tune", "under", "understanding", "until", "up", "validation", "value", "variable", "variance", "weight", "while", "who", "workflow", "world", "worst", "year"], "potential_description_queries_len": 164, "potential_script_queries": ["data", "evaluation", "measure", "numpy", "regression", "seaborn", "sklearn", "split", "svm", "validation", "vector", "xgboost"], "potential_script_queries_len": 12, "potential_entities_queries": ["even", "learning", "lot", "mean", "new", "positive"], "potential_entities_queries_len": 6, "potential_extra_queries": ["bag"], "potential_extra_queries_len": 1, "all_components_potential_queries_len": 174}