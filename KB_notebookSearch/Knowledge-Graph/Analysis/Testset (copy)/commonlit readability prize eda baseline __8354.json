{"name": "commonlit readability prize eda baseline ", "full_name": " h1 Introduction h1 Import libraries h1 Missing values h1 Pre processing excerpt h1 EDA h1 Part of Speech tagging h1 Readability tests h1 Baseline model h1 Submission file ", "stargazers_count": 0, "forks_count": 0, "description": "ai ruchi798 commonlit workspace user ruchi798 To get the API key an account is to be created on the website https wandb. html Logging a custom bar chart for POS distribution Readability tests textstat https pypi. append lins text_standard. We don t have any missing values in the columns of our interest i. Artifact text_props_readability type dataset artifact. coleman_liau_index train_df excerpt i lins textstat. add_file test_excerpt_preprocessed. append flr flesch_kg. finish Function to plot wandb bar chart Function to plot wandb histogram maximum target minimum target maximum standard error minimum standard error color function for the wordcloud flesch_re flesch_kg fog_scale automated_r coleman linsear text_standard for i in range 7 for i in range len text_props flr textstat. org wiki Linsear_Write text_standard Readability Consensus based upon all the above testsLogging the text properties dataset as an artifact This helps me to save on time since I can directly use the saved artifact for my workflow A snapshot of the newly created artifacts Since I have already logged the artifact I can directly use it in this manner There is a strong correlation between our target variable and the Flesch Readability Ease test values. add_file text_props_readability. ai ruchi798 commonlit workspace user ruchi798 Illustrations tools Canva https www. linsear_write_formula train_df excerpt i ts textstat. append flkg fog_scale. org project missingno to visualize missing values in the training set. Logging custom histograms for the distribution of character count word count average word length and sentence count Part of Speech tagging Abbreviation Meaning CC coordinating conjunction CD cardinal digit DT determiner EX existential there FW foreign word IN preposition subordinating conjunction JJ adjective large JJR adjective comparative larger JJS adjective superlative largest LS list item marker MD modal could will NN noun singular NNS noun plural NNP proper noun singular NNPS proper noun plural PDT predeterminer POS possessive ending parent s PRP personal pronoun hers herself him himself PRP dollar sign possessive pronoun her his mine my our RB adverb occasionally swiftly RBR adverb comparative greater RBS adverb superlative biggest RP particle about SYM symbol TO infinite marker to UH interjection goodbye VB verb ask VBG verb gerund judging VBD verb past tense pleaded VBN verb past participle reunified VBP verb present tense not 3rd person singular wrap VBZ verb present tense with 3rd person singular bases WDT wh determiner that what WP wh pronoun who WP dollar sign possessive wh pronoun WRB wh adverb how Higher the grade more the complexity of grammar Penn Part of Speech Tags https cs. to_csv test_excerpt_preprocessed. edu grishman jet guide PennPOS. Data train. flesch_reading_ease train_df excerpt i flkg textstat. org wiki Gunning_fog_index automated_r Automated Readability Index https en. append ts text_props flesch_re flesch_re text_props flesch_kg flesch_kg text_props fog_scale fog_scale text_props automated_r automated_r text_props coleman coleman text_props linsear linsear text_props text_standard text_standard text_props. Artifact test_excerpt_preprocessed type dataset artifact. init project commonlit name excerpt_preprocessed artifact wandb. It came in super handy for calculating scores of various readability tests flesch_re The Flesch Reading Ease formula https en. org wiki Coleman E2 80 93Liau_index linsear Linsear Write Formula https en. Next use secrets to use API Keys more securely Missing values Here I have used a viz module called missingno https pypi. excerpt target and standard_error Pre processing excerpt It s important to preprocess the excerpt before we proceed further Logging the preprocessed dataset as an artifact Using the saved artifact EDA Logging custom histograms for target and standard error distribution Logging a dictionary of custom objects Logging a custom bar chart for license distribution Logging custom bar charts for unigrams bigrams and trigrams sentence_count and target are very highly correlated since extremely long sentences can be complex to read and understand. Score Notes 90 100 very easy to read easily understood by an average 11 year old student 80 90 easy to read 70 80 fairly easy to read 60 70 easily understood by 13 to 15 year old students 50 60 fairly difficult to read 30 50 difficult to read best understood by college graduates 0 30 very difficult to read best understood by university graduates Logging a custom histogram for the distribution of Flesch Reading Ease scores More than 70 of excerpts can be easily understood by 13 15 year olds. add_file train_excerpt_preprocessed. com Preprocessing function find alphabets convert to lower case tokenize words remove stopwords lemmatization train_df excerpt_preprocessed preprocess train_df test_df excerpt_preprocessed preprocess test_df train_df. org project textstat is a library used to calculate statistics from text. append ar coleman. Let s see which excerpts have the highest and lowest Target and Flesch Reading Ease Score Baseline model Submission file Here s a snapshot of my project https wandb. csv the training and testing set id unique ID for excerpt url_legal URL of source license license of source material excerpt text to predict reading ease of target reading ease standard_error measure of spread of scores among multiple raters for each excerpt Note url_legal license and standard error are blank in the test set. org wiki Flesch E2 80 93Kincaid_readability_tests Flesch_reading_ease flesch_kg The Flesch Kincaid Grade Level https en. csv run wandb. text_standard train_df excerpt i flesch_re. Evaluation metric Root Mean Squared Error RMSE RMSE sqrt frac 1 n Sigma_ i 1 n Big frac y_i hat y_i sigma_i Big 2 where y_i original value hat y_i predicted value n number of rows in the test data Import libraries I will be integrating W B for visualizations and logging artifacts CommonLit Project on W B Dashboard https wandb. org wiki Automated_readability_index coleman The Coleman Liau Index https en. automated_readability_index train_df excerpt i cole textstat. flesch_kincaid_grade train_df excerpt i fs textstat. log_artifact artifact run. append cole linsear. Introduction Goal To build algorithms to rate the complexity of reading passages for grade 3 12 classroom use. Artifact train_excerpt_preprocessed type dataset artifact. org wiki Flesch E2 80 93Kincaid_readability_tests Flesch_reading_ease fog_scale The Fog Scale Gunning FOG Formula https en. to_csv text_props_readability. append fs automated_r. Let s explore the distribution of test values for Flesch Readability Ease. init project commonlit name text_props_readability artifact wandb. to_csv train_excerpt_preprocessed. gunning_fog train_df excerpt i ar textstat. ", "id": "ruchi798/commonlit-readability-prize-eda-baseline", "size": "8354", "language": "python", "html_url": "https://www.kaggle.com/code/ruchi798/commonlit-readability-prize-eda-baseline", "git_url": "https://www.kaggle.com/code/ruchi798/commonlit-readability-prize-eda-baseline", "script": "sklearn.metrics DataFrame plot_bt get_top_n_words stopwords Ridge rich.theme UserSecretsClient TfidfVectorizer avg_word_len preprocess training count_tags collections rich.console seaborn numpy sklearn.pipeline sklearn.feature_extraction.text mean_squared_error color_wc make_pipeline mean_squared_error as mse get_top_n_bigram Line2D plot_wb_hist get_top_n_trigram sklearn.model_selection CountVectorizer Console matplotlib.pyplot Theme print WordCloud pandas wordcloud displacy word_tokenize STOPWORDS training_all sent_tokenize Counter pos_tag custom_palette plot_distribution CountVectorizer as CV nltk.tokenize nltk.corpus spacy missingno matplotlib.lines sklearn.linear_model plot_wb_bar rich train_test_split LinearRegression kaggle_secrets nltk ", "entities": "(('fog_scale automated_r coleman', 'range len text_props flr textstat'), 'finish') (('init project', 'name text_props_readability artifact wandb'), 'commonlit') (('Here snapshot', 'project https wandb'), 'let') (('trigrams very highly extremely long sentences', 'unigrams bigrams'), 'processing') (('I', 'target strong variable'), 'help') (('how grade', 'Speech Tags https cs'), 'log') (('linsear', 'text_props'), 'flesch_re') (('It', 'readability various tests'), 'come') (('We', 'interest'), 'don') (('excerpt i', 'textstat'), 'coleman_liau_index') (('Import I', 'logging CommonLit W B Dashboard https wandb'), 'metric') (('ai', 'workspace ruchi798 Illustrations tools Canva https user www'), 'commonlit') (('init project', 'name'), 'commonlit') (('url_legal URL', 'test set'), 'csv') (('Here I', 'viz module'), 'use') (('org project textstat', 'text'), 'be') (('s', 'Flesch Readability Ease'), 'let') (('html Logging', 'textstat https pypi'), 'test') (('More than 70', 'year easily 13 15 olds'), 'note') (('case lower words', 'stopwords lemmatization'), 'com') (('account', 'website https wandb'), 'commonlit') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["account", "append", "artifact", "average", "best", "build", "calculate", "case", "character", "chart", "color", "convert", "correlation", "could", "count", "csv", "custom", "data", "dataset", "dictionary", "digit", "directly", "distribution", "ease", "error", "explore", "file", "find", "formula", "frac", "function", "grade", "him", "histogram", "id", "init", "interest", "item", "key", "largest", "len", "length", "library", "list", "logging", "lower", "manner", "maximum", "measure", "metric", "minimum", "missing", "missingno", "model", "module", "multiple", "my", "name", "not", "number", "parent", "past", "person", "plot", "predict", "present", "processing", "project", "range", "read", "reading", "remove", "run", "save", "sentence", "set", "sign", "source", "spread", "sqrt", "standard", "student", "target", "test", "testing", "text", "time", "tokenize", "training", "type", "unique", "user", "value", "variable", "visualize", "viz", "wandb", "website", "who", "word", "workflow", "year"], "potential_description_queries_len": 102, "potential_script_queries": ["displacy", "nltk", "numpy", "print", "rich", "seaborn", "sklearn", "spacy"], "potential_script_queries_len": 8, "potential_entities_queries": ["len", "project"], "potential_entities_queries_len": 2, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 109}