{"name": "lyft competition understanding the data ", "full_name": " h1 Introduction h1 Acknowledgements h3 A self driving car in action h1 The dataset structure h1 What is LiDAR h1 How does LiDAR work h3 Flash LiDAR Camera h1 Visualizing the data h3 Install lyft dataset sdk and import the necessary libraries h3 Define the path containing the dataset h3 Load the training dataframe h3 Group data by object category h3 Convert numerical features from str to float32 h3 First Exploration h3 center x and center y h3 Distributions of center x and center y h3 Relationship between center x and center y h3 KDE Plot h3 center z h3 Distribution of center z h3 yaw h3 Distribution of yaw h3 width h3 length h3 height h3 Frequency of object classes h3 center x vs class name h3 center y vs class name h3 center z vs class name h3 width vs class name h3 length vs class name h3 height vs class name h1 Digging into the image and LiDAR data h3 Define some functions to help create the LyftDataset class h4 click CODE on the right side h3 Create a class called LyftDataset to package the dataset in a convenient form h4 click CODE on the right side h3 Create another class called LyftDatasetExplorer which will help us to visualize the data h4 click CODE on the right side h3 Create a LyftDataset object from the existing dataset h3 Create a function to render scences in the dataset h3 Render the first scence image and LiDAR h3 Render the second scence image and LiDAR h3 Front Camera h3 Back Camera h3 Front Left Camera h3 Front Right Camera h3 Back Left Camera h3 Back Right Camera h3 Top LiDAR h3 Front Left LiDAR h3 Front Right LiDAR h3 Image and LiDAR animation h3 Animate image data for 3 scences h3 Scence 1 h3 Scence 2 h3 Scence 3 h3 Animate LiDAR data for 3 scences h3 Scence 1 h3 Scence 2 h3 Scence 3 h1 Ending note ", "stargazers_count": 0, "forks_count": 0, "description": "Decode each point. Add reverse indices from log records to map records. class_nameIn the plots below I will explore how the distribution of length changes for different object class_names. This is not surprising because trucks buses and cars almost always have much greater width than pedestrians and bicycles. Basically laser beams are shot in all directions by a laser. Because of this objects that are far ahead and far to the side are not detected at all and only objects which satisfy one or none of those conditions are detected. This comprehensive 3D map provides the car with detailed information so that it can navigate even in complex environments. Get annotations and params from DB. In the box plots above we can notice the same observation as in the violin plot above. instance An enumeration of all object instance we observed. The dataset structure1. The only exception to this trend are the cars. Each image simply consists of three color channels Red R Blue B and Green G that form the RGB color image format. Therefore most pedestrains and bicycles that are detected tend to be far away. center_z center_z corresponds to the xz coordinate of the center of an object s location bounding volume. This depth information combined with the 2D represenation of the image provides an accurate 3D representation of the object. Now let us extract the first sample sample from the first scence. This coordinate represents the height of the object above the x y plane. The image data is in the usual. From the diagram above we can see that the distributions of both center_x and center_y have multiple peaks and are therefore multimodal. I will explain how LiDAR data is collected and stored and then I will talk about the intuition behind this data format. By immediately returning a 3D elevation mesh of target landscapes a flash sensor can be used by an autonomous vehicle to make decisions regarding speed adjustment braking steering etc. length length is simply the length of the bounding volume in which the object lies. com xhlulu s brilliant animation kernel https www. RGB Colour maps Monochrome maps Set the colors for the mask. if ESC is pressed exit. I hope you found this kernel useful or interesting. The center_x distributions for smaller objects like pedestrians and bicycles have very low mean and quartile values as compared to larger objects like cars trucks and buses. The high frame rate of the sensor makes it a useful tool for a variety of applications that benefit from real time visualization such as autonomous vehicle driving. list_scenes Now let us visualize some of the image and LiDAR data. In the violin plots we can clearly see that the length distributions for large vehicles like cars buses and trucks have much larger means as compared to small objects like pedestrians and bicycles. Initialize LyftDatasetExplorer class Store the mapping from token to table index for each table. So most of the times the camera has to look down to see the objects. class_nameIn the plots below I will explore how the distribution of center_z changes for different object class_names. there are two mmajor peaks in the distribution. Filter points with an invalid state. Note You can list all samples in a scence using lyft_dataset. But since the length of the road is much greater than its width and there is a higher chance of the camera s view being blocked from this angle the camera can only find objects narrowly ahead or narrowly behind and not further away. Decorate adds short cut sample_data with sensor information. If space is pressed pause. calibrated sensor Definition of a particular sensor as calibrated on a particular vehicle. This difference in time calculated by the sensors can be used to calculate the depth of the object. In the diagram above we can see that the width is approximately normally distirbuted with a mean of around 2 with some outliers on either side. Therefore the height or z coordinate of the objects relative to the camera are generally negative. This technology is also used in archaeology. The camera can detect objects that are far ahead but not too far to the side. This gives an accurate idea of the 3D shape of the artifact when the artifact cannot be excavated for whatever reason. class_nameIn the plots below I will explore how the distribution of height changes for different object class_names. For each sample in the scene store the ego pose. com gaborfodor eda 3d object detection challenge Lyft Dataset SDK dev kit. com gaborfodor eda 3d object detection challenge Lyft EDA Animations generating CSVs by xhulu https www. The signal that is returned is processed by embedded algorithms to produce a nearly instantaneous 3D rendering of objects and terrain features within the field of view of the sensor. Retrieve sensor pose records Retrieve all sample annotations and map to sensor coordinate system. This is not surprising because trucks buses and cars almost always have much greater length than pedestrians and bicycles. Class level settings for radar pointclouds see from_file. And on the other hand larger objects like cars trucks and buses tend to have a greater height with respect to the camera. Wait a very short time 1 ms. The distributions for the small objects have much greater probability density concentrated at lower values of center_z as compared to large objects. Abort if there are no previous sweeps. Filter by ambig_state. Here we use the lidar sample_data. com xhlulu lyft eda animations generating csvs. 5 and the other is around 2. Acknowledgements NuScences DevKit by Lyft https github. One of the peaks is around 0. if space is pressed pause. Below is a 3D map of an ocean floor generated using LiDAR And of course self driving cars use this technology to identify objects around them in 3D dimensions along with estimating the velocities and orientations of these objects. Initialize map mask for each map record. These images can therefore be stored in a four dimensional tensor with dimensions as batch_size channels width height. Digging into the image and LiDAR data Define some functions to help create the LyftDataset class click CODE on the right side Create a class called LyftDataset to package the dataset in a convenient form click CODE on the right side Create another class called LyftDatasetExplorer which will help us to visualize the data click CODE on the right side Create a LyftDataset object from the existing datasetThe dataset consists of several scences which are 25 45 second clips of image of LiDAR data from a self driving car. This signifies that small objects in general have greater center_y values than large objects. Here the LiDAR file s name is host a101 lidar0 1241893239199111666 1241893264098084346. yaw yaw is the angle of the volume around the z axis making yaw the direction the front of the vehicle bounding box is pointing at while on the ground. Convention x points forward y to the left z up. This is probably because smaller objects like pedestrians and bicycles tend to have a lower height with repsect to the camera. The majority of the objects are cars as we will see later and these constitute a length of around 2 at the peak. Therefore each scence is made up of several samples. Load and render Store here so we don t render the same image twice. There are a few token IDs and a name for each scene. category Taxonomy of object categories e. Rotate Translate Draw the sides Draw front first 4 corners and rear last 4 corners rectangles 3d lines 2d Draw line indicating the front Draw the sides Draw front first 4 corners and rear last 4 corners rectangles 3d lines 2d Draw line indicating the front Lyft Dataset SDK dev kit. The center_y distributions for smaller objects like pedestrians and bicycles have much larger mean and quartile values as compared to larger objects like cars trucks and buses. The height distributions for smaller objects like pedestrians and bicycles have much smaller mean and quartile values as compared to larger objects like cars trucks and buses. Now I will import the other libraries necessary to carry out the exploration. Reset for future plots. They barely have any skew and have greater means than the distributions for pedestrians and bicycles. com xhlulu lyft eda animations generating csvs if you find this interesting. Below is a video featuring a drone equipped with LiDAR. It motivates me to produce more quality content A self driving car in action Before we dive into the technical details of this kernel let us watch an interesting video of a self driving car in action It can be seen in the video that the car is able to effortlessly take turns change lanes stop at red lights etc. The laser beams reflect off the objects in their path and the reflected beams are collected by a sensor. Distribution of center_z In the above diagram we can see that the distribution of center_z has an extremely high positive rightward skew and is clustered around the 20 mark which is approximates its mean value. One can estimate that the mean is between 1 and 2 around 1. In the diagram above we can see that the length has a distribution with a strong positive rightward skew with a mean of around 5 with some outliers on either side. But smaller objects like bicycles and pedestrians cannot remain in the field of view of the camera when they are too close. positive difference Merge with key pc. Maps instance tokens to prev_ann records There are rare situations where the timestamps in the DB are off so ensure that t0 current_time. Homogeneous transformation matrix from sensor coordinate frame to ego car frame. Different sensors collect light from different parts of the object and the times recorded by the sensors would be different. After diving into the theory behind these concepts I will show how this dataset can be packaged into a compact format which makes it easier to query information from the dataset. Visualizing the data Install lyft_dataset_sdk and import the necessary librariesWe will need the lyft_dataset_sdk library because it will help us visualize the image and LiDAR data easily. And finally I will show how to visualize and explore this data using plots and graphs in matplotlib. sensor A specific sensor type. Define the path containing the dataset Load the training dataframe Group data by object category Convert numerical features from str to float32 First ExplorationNow I will explore the data in this particular dataframe and see if I can derive any useful insights from it. Each scence is composed of many samples. Both distributions also have a clear rightward or positive skew. For example it is used in farms to help sow seeds and remove weeds. Each snapshot in the data consists of two forms of information image data and LiDAR data. This technology is used to create 3D representations in many real world scenarios. Just like a camera that takes pictures of distance instead of colors. In the diagram above we can see that the height has a distribution with a strong positive rightward skew with a mean of around 2 with some outliers on either side. The laser pulse repetition frequency is sufficient for generating 3D videos with high resolution and accuracy. txt Modified by Vladimir Iglovikov 2019. Flash LiDAR CameraThe device featured in the image above is called a Flash LiDAR Camera. Code written by Oscar Beijbom 2018. What is LiDAR LiDAR Light Detection and Ranging is a method used to generate accurate 3D representations of the surroundings and it uses laser light to achieve this. Introduction In this kernel I will be explaining the meaning and intuition behind each component in the dataset including the images LiDAR and pointclouds. sample_annotation An annotated instance of an object within our interest. The center_x distribution is more evenly spread out. How does LiDAR work The above GIF roughly demonstrates how LiDAR works. Therefore the mean center_x is clearly greater for larger vehicles like buses and trucks. Licensed under the Creative Commons see licence. This is how humans sense the world around us. The outliers on the right represent larger objecs like trucks and vans and the outliers on the left represent smaller objects like pedestrians and bicycles. The presence of the two peaks at symmetric positions reduces the skew in both directions and they cancel out making the distribution more balanced than the distributions of center_x center_y and center_z. Contrastingly the smaller objects like pedestrians and bicycles have center_x distributions with strong positive rightward skews. Convert to numpy matrix. The pointcloud is basically a set of contours that represent the distance of various objects as measured by the LiDAR. Make list of Box objects including coord system transforms. Calculate the pose on the map and append Compute number of close ego poses. And it can also detect objects that are far to side but not too far ahead. class_nameIn the plots below I will explore how the distribution of center_x changes for different object class_names. In the violin plots above we can see that the distributions of center_x for large vehicles including trucks buses and other vehicles are well spread. Filter by dynProp. The Flash LiDAR uses a single light source that illuminates the field of view in a single pulse. I will also use install the chart_studio library to generate interactive plots. ego_pose Ego vehicle poses at a particular timestamp. Finally LiDAR can also be used to render high quality 3D maps of ocean floors and other inaccesible terrains making it very useful to geologists and oceanographers. But the distribution of center_y purple has a signficantly higher skew that the the distribution of center_x orange. class_nameIn the plots below I will explore how the distribution of width changes for different object class_names. The name matches with the name of the LiDAR data file associated with the given scene. com lyft nuscenes devkit EDA 3D Object Detection Challenge by beluga https www. Move box to ego vehicle coord system parallel to world z plane Move box to ego vehicle coord system Move box to sensor coord system Retrieve sensor pose records If no previous annotations available or if sample_data is keyframe just return the current ones. This signifies that small objects in general have smaller center_y values than large objects. The center_z distributions for smaller objects like pedestrians and bicycles have much smaller mean and quartile values as compared to larger objects like cars trucks and buses. Each sample is annoted with the objects present. Make reverse indexes for common lookups. So I will now look at what these forms of data mean theoretically and then I will visualize this information later in the kernel. Once again the only exception to this trend are the cars. 3D bounding box corners. There is understandably much greater variation in the x and y coordiantes of the object. These distributions also have clearly lower means than the distributions for the larger vehicles. Animate image data for 3 scences Scence 1 Scence 2 Scence 3 Animate LiDAR data for 3 scences Scence 1 Scence 2 Scence 3 Ending noteThis brings me to the end of this kernel. This is probably because the large vehicles tend to be within the field of view of the camera due to their large size. map Map data that is stored as binary semantic masks from a top down view. And the most common vehicle or entity for that matter visible on those roads are cars. This causes the center_y to be greater on average for small objects as compared to large objects. The majority of the objects are cars as we will see later and these constitute a width of around 2 at the peak. A moving robot uses LiDAR to to create a 3D map of its surroundings and using this map it avoids obstacles and completes its tasks. In the violin plots we can clearly see that the length distributions for large vehicles like buses and trucks have much larger means as compared to small objects like pedestrians and bicycles. center_x and center_y center_x and center_y correspond to the x and y coordinates of the center of an object s location bounding volume. This is probably because most objects are very close to the flat plane of the road and therefore there is no great variation in the height of the objects above or below the camera. Now since it is clear what LiDAR is and how it works we can get right to visualizing the dataset. This is probably once again due to the limitations of the camera system. Distribution of yaw In the diagram above we can see that the distribution of yaw is roughly bimodal i. They usually cross the road during a red traffic signal when the traffic halts. The distributions for the small objects have much greater probability density concentrated at higher values of center_y as compared to large objects. The width distributions for smaller objects like pedestrians and bicycles have much smaller mean and quartile values as compared to larger objects like cars trucks and buses. Please upvote xhulu s kernel https www. LiDAR is used to create 3D renderings of 2D scans of artifacts. Only a simple pip install command is required. This results in a negative relationship between center_x and center_y. The colours of these contour lines represent the distance. We can extract and look at the first scence as follows As it can be seen above each scence consists of a dictionary of information. All the other object classes are nowhere near cars in terms of frequency. This is not surprising because trucks and buses almost always have much greater length than pedestrians and bicycles. They tend to have a similar height to that of pedestrians. This indicates that objects are spread out very evenly along the x axis but not likewise along the y axis. We can also print all annotations across all sample data for a given sample as shown below We can also render the image data from particular sensors as follows Front CameraImages from the front camera Back CameraImages from the back camera Front Left CameraImages from the front left camera Front Right CameraImages from the front right camera Back Left CameraImages from the back left camera Back Right CameraImages from the back right cameraWe can pick a given annotation from a sample in the data and render only that annotation as shown below We can also pick a given instance from the dataset and render only that instance as shown below We can also get the LiDAR data collected from various LIDAR sensors on the car as follows Top LiDAR LiDAR data from the top sensor Front Left LiDAR LiDAR data from the front left sensor Front Right LiDAR LiDAR data from the front right sensor Image and LiDAR animationThis section is from xhulu https www. Fuse four transformation matrices into one and perform transform. The focal plane of a Flash LiDAR camera has rows and columns of pixels with ample depth and intensity to create 3D landscape models. These color channels superimpose to form the final colored image. Get records from DB Open CV init Get data from DB Load and render Render Images stored at approx 10 Hz so wait 10 ms. Basically the LiDAR uses light beams to measure the distance of various objects as discussed earlier and this distance information can be visualized as a set of 3D contours. Also most z coordinates are negative because the camera is attached at the top of the car. Reverse index samples with sample_data and annotations. Show updated canvas. This is possible because the car is able to accurately recognize objects in 3D space using information from it s sensors such as image and LiDAR data. This is probably because these large vehicles tend to keep greater distances from the other vehicles and the smaller vehicles do not stay too close to these large vehicles in order to avoid accidents. The distribution does not have any clear skew. width width is simply the width of the bounding volume in which the object lies. if ESC is pressed exit Get logs by location Filter scenes Get records from the database. The majority of the objects are cars as we will see later and these constitute a length of around 5 at the peak. The onboard source of illumination makes Flash lidar an active sensor. This is probably because pedestrians road crossers and bicyclists do not need to maintain large distances with cars and trucks to avoid accidents. Two eyes make observations in 2D and these two pieces of information are combined to form a 3D map depth perception. A NaN in the first point indicates an empty pointcloud. Relationship between center_x and center_y KDE PlotIn the KDE plot above we can see that center_x and center_y seem to have a somewhat negative correlation. Note You can list all the scenes in the dataset using lyft_dataset. sample A snapshot of a scence at a particular instance in time. com xhlulu lyft eda animations generating csvs Lidar Wikipedia https en. This is probably because the car s camera can sense objects on either left or right easily along the x axis due to the width of the road being small. In the violin plots above we can see that the distributions of center_y for small objects including pedestrians and bicycles have a greater mean value than large objects like trucks and buses. It automatically creates a 3D map of the world around it using the process mentioned above. Lookup table for how to decode the binaries. jpeg format which is fairly simple to understand. But the camera cannot detect objects that are both far ahead and far to the side. org wiki Lidar If you find this kernel interesting please drop an upvote. The variation spread of center_z is significantly smaller than that of center_x and center_y. Init Get reference pose and timestamp Homogeneous transform from ego car frame to reference frame Homogeneous transformation matrix from global to _current_ ego car frame Aggregate current and previous sweeps. Poses are associated with the sample_data. Note that a sample is a snapshot of the data at a given point in time during the scene. Basically the 3D target is illuminated with a laser light a focused directed beam of light and the reflected light is collected by sensors. These coordinates represent the location of an object on the x y plane. Distributions of center_x and center_y In the diagram above the purple distribution is that of center_y and the orange distribution is that of center_x. If you did find this kernel interesting please drop an upvote. Create a function to render scences in the dataset Render the first scence image and LiDAR Render the second scence image and LiDAR These images above display the image and LiDAR data collected using the cameras and sensors from various angles on the car. I use functions from that kernel to animate the image and LiDAR data. type List int type List int Use 0 2 6 for moving objects only. scene Consists of 25 45 seconds of a car s journey in a given environment. Frequency of object classesFrom the above diagram it can be seen that the most common object class in the dataset is car. In the violin plots we can clearly see that the width distributions for large vehicles like cars buses and trucks have much larger means as compared to small objects like pedestrians and bicycles. height height is simply the height of the bounding volume in which the object lies. Basically the higher the wavelength of the color of the contour line the greater the distance of the object from the camera. list_sample my_sample token Next let us render a pointcloud for a sample image in the dataset. This type of camera is attached to the top of autonomous cars and these cars use this to navigate while driving. If no parameters are provided use default settings. The yellow boxes around the objects in the images are the bounding boxes or bounding volumes that show the location of the objects in the image. In the violin plots above we can see that the distributions of center_z for small objects including pedestrians and bicycles have a significantly smaller mean value than large objects like trucks and buses. Decorate adds short cut sample_annotation table with for category name. Now a special device called a Flash LiDAR Camera is used to create 3D maps using the information from these sensors. Remove close points and add timevector. visibility currently not used 9. Explicitly assign tables to help the IDE determine valid class members. attribute Property of an instance that can change while the category remains the same. type List int Get the header rows and check if they appear as expected. Now add to canvas Only update canvas if we have not already rendered this one. Now I will move on to the LiDAR data which fewer people might be familiar with. The length distributions for smaller objects like pedestrians and bicycles have much smaller mean and quartile values as compared to larger objects like cars trucks and buses. This is unsurprising because the images are taken from the streets of Palo Alto in Silicon Valley California. The darker purple and blue contour lines represent the closer objects and the lighter green and yellow lines represent the far away objects. log Log information from which the data was extracted. sample_data Contains the data collected from a particular sensor on the car. This process is similar to actual human vision. Each pixel records the time it takes each laser pulse to hit the target and return to the sensor as well as the depth location and reflective intensity of the object being contacted by the laser pulse. class_nameIn the plots below I will explore how the distribution of center_y changes for different object class_names. Load up the pointcloud. The time required for the light to reflect back to the sensor is calculated. It motivates me to produce more quality content Taken from https www. ", "id": "tarunpaparaju/lyft-competition-understanding-the-data", "size": "24612", "language": "python", "html_url": "https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data", "git_url": "https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data", "script": "Box animation get_sample_data map_pointcloud_to_image __eq__ nbr_dims render_sample_data ann_count pyquaternion render list_scenes plotly.graph_objs render_pointcloud_in_image typing BoxVisibility functools Image render_instance init_notebook_mode nbr_points PointCloud(ABC) abc __make_reverse_index__ render_height reduce translate box_velocity lyft_dataset_sdk.utils.map_mask seaborn numpy render_intensity Axes Quaternion getind pandas subsample view_points list_sample matplotlib _render_helper transform plotly.figure_factory rotation_matrix HTML animate_fn lyft_dataset_sdk.lyftdataset rc get_box LyftDataset plotly.offline rotate render_egoposes_on_map ABC bottom_corners __repr__ corners get_sample_data_path PIL copy generate_next_token list_attributes render_sample get_boxes plotly.tools LidarPointCloud(PointCloud) animate_lidar from_file field2token render_cv2 draw_rect lyft_dataset_sdk.utils.geometry_utils get RadarPointCloud(PointCloud) box_in_image IPython.display datetime __init__ get_color List transform_matrix plot Path remove_close matplotlib.axes Dict crop_image animate_images pathlib MapMask Tuple list_categories matplotlib.pyplot LyftDatasetExplorer render_scene tqdm __load_table__ render_ego_centric_map render_annotation abstractmethod from_file_multisweep render_scene_channel ", "entities": "(('Flash LiDAR CameraThe device', 'image'), 'call') (('that', 'single pulse'), 'use') (('camera', 'only objects'), 'be') (('difference', 'object'), 'use') (('we', 'dataset'), 'now') (('theoretically then I', 'later kernel'), 'look') (('reflected beams', 'sensor'), 'reflect') (('height distributions', 'cars trucks'), 'have') (('only exception', 'trend'), 'be') (('Decorate', 'sensor information'), 'add') (('camera', 'car'), 'be') (('Convention x', 'z left up'), 'point') (('It', 'process'), 'create') (('technology', 'world many real scenarios'), 'use') (('Next us', 'dataset'), 'let') (('reflected light', 'sensors'), 'illuminate') (('self driving', 'objects'), 'be') (('I', 'it'), 'define') (('this', 'csvs'), 'com') (('coordinates', 'x y plane'), 'represent') (('center_y', 'large objects'), 'cause') (('You', 'lyft_dataset'), 'note') (('that', 'both far ahead side'), 'detect') (('RGB Colour maps Monochrome maps', 'mask'), 'Set') (('that', 'vehicle such autonomous driving'), 'make') (('that', 'instead colors'), 'like') (('light', 'back sensor'), 'calculate') (('pressed logs', 'database'), 'be') (('it', 'even complex environments'), 'provide') (('Retrieve sensor pose records', 'sensor coordinate system'), 'retrieve') (('most common vehicle', 'visible roads'), 'be') (('They', 'traffic red signal'), 'cross') (('we', 'violin plot'), 'notice') (('distributions', 'multiple peaks'), 'see') (('section', 'xhulu https www'), 'print') (('I', 'interactive plots'), 'use') (('center_x', 'volume'), 'correspond') (('sample_data', 'just current ones'), 'box') (('center_z center_z', 'volume'), 'correspond') (('car', 'such image'), 'be') (('laser Basically beams', 'laser'), 'shoot') (('I', 'image data'), 'use') (('pedestrians road probably crossers', 'accidents'), 'be') (('images', 'Silicon Valley California'), 'be') (('center_x distributions', 'cars trucks'), 'have') (('Class level settings', 'from_file'), 'see') (('category', 'instance'), 'attribute') (('colours', 'distance'), 'represent') (('times', 'sensors'), 'collect') (('that', 'LiDAR'), 'be') (('distributions', 'large objects'), 'have') (('Therefore mean center_x', 'buses'), 'be') (('when they', 'camera'), 'remain') (('center_x', 'somewhat negative correlation'), 'relationship') (('distribution', 'yaw'), 'distribution') (('width clearly distributions', 'pedestrians'), 'see') (('small objects', 'large objects'), 'signify') (('images', 'car'), 'create') (('us', 'image'), 'need') (('center_z distributions', 'cars trucks'), 'have') (('which', 'mean value'), 'distribution') (('camera', 'objects'), 'have') (('probably large vehicles', 'large size'), 'be') (('we', 'already one'), 'add') (('they', 'rows'), 'get') (('length clearly distributions', 'pedestrians'), 'see') (('change lanes', 'red lights'), 'motivate') (('objects', 'y likewise axis'), 'indicate') (('Poses', 'sample_data'), 'associate') (('it', 'weeds'), 'use') (('Therefore height', 'relative camera'), 'be') (('laser pulse repetition frequency', 'high resolution'), 'be') (('it', 'this'), 'be') (('how humans', 'us'), 'be') (('road', 'width'), 'be') (('They', 'pedestrians'), 'have') (('data', 'which'), 'log') (('two pieces', 'map depth 3D perception'), 'make') (('sample', 'objects'), 'annote') (('3 Ending', 'kernel'), 'datum') (('It', 'https www'), 'motivate') (('later these', 'peak'), 'be') (('depth information', 'object'), 'provide') (('finally I', 'matplotlib'), 'show') (('kernel', 'upvote'), 'drop') (('distribution', 'clear skew'), 'have') (('spread', 'center_x'), 'be') (('parameters', 'default settings'), 'use') (('trucks buses', 'pedestrians'), 'be') (('Decorate', 'category name'), 'add') (('object most common class', 'dataset'), 'see') (('They', 'pedestrians'), 'tend') (('batch_size', 'width height'), 'store') (('we', 'object instance'), 'instance') (('name', 'given scene'), 'match') (('LiDAR', 'artifacts'), 'use') (('object', 'which'), 'be') (('Contrastingly smaller objects', 'strong positive rightward skews'), 'have') (('distributions', 'larger vehicles'), 'have') (('Flash LiDAR Camera', 'sensors'), 'call') (('trucks', 'pedestrians'), 'be') (('other inaccesible it', 'very geologists'), 'use') (('sample_data', 'car'), 'contain') (('when artifact', 'reason'), 'give') (('fewer people', 'which'), 'move') (('length', 'side'), 'see') (('distribution', 'center_x orange'), 'have') (('distributions', 'also clear rightward'), 'have') (('Lyft EDA Animations', 'xhulu https www'), 'object') (('orange distribution', 'center_x'), 'distribution') (('coordinate', 'x y plane'), 'represent') (('snapshot', 'information image data'), 'consist') (('cars', 'this'), 'attach') (('width distributions', 'cars trucks'), 'have') (('flash sensor', 'steering etc'), 'use') (('don t', 'same image'), 'load') (('This', 'camera system'), 'be') (('plots', 'object different class_names'), 'class_namein') (('it', 'dataset'), 'show') (('it', 'laser pulse'), 'record') (('com lyft nuscenes devkit', 'beluga https www'), 'EDA') (('Once again only exception', 'trend'), 'be') (('it', 'information'), 'extract') (('com gaborfodor', 'eda object detection Lyft Dataset SDK dev kit'), 'challenge') (('color channels', 'final colored image'), 'superimpose') (('focal plane', 'landscape 3D models'), 'have') (('kernel', 'upvote'), 'Lidar') (('height', 'side'), 'see') (('NaN', 'empty pointcloud'), 'indicate') (('lidar', 'illumination'), 'make') (('center_y distributions', 'cars trucks'), 'have') (('distribution', 'center_x center_y'), 'reduce') (('object other classes', 'frequency'), 'be') (('distributions', 'trucks buses'), 'see') (('technology', 'also archaeology'), 'use') (('that', 'far ahead too side'), 'detect') (('then I', 'data format'), 'explain') (('Now us', 'first scence'), 'let') (('Now I', 'necessary exploration'), 'import') (('I', 'images'), 'introduction') (('that', 'also objects'), 'detect') (('rare where timestamps', 't0 off so current_time'), 'token') (('distributions', 'trucks'), 'see') (('Init', 'ego car _ current _ frame Aggregate current sweeps'), 'get') (('sample', 'scene'), 'note') (('length distributions', 'cars trucks'), 'have') (('outliers', 'pedestrians'), 'represent') (('width', 'side'), 'see') (('assign Explicitly IDE', 'class valid members'), 'table') (('probably most objects', 'camera'), 'be') (('contour darker purple lines', 'lighter green far away objects'), 'represent') (('Red R Blue Green that', 'RGB color image format'), 'consist') (('Render Images', '10 Hz'), 'get') (('that', 'sensor'), 'process') (('probably smaller objects', 'camera'), 'be') (('which', 'conditions'), 'detect') (('Now us', 'image data'), 'let') (('front', 'ground'), 'be') (('it', 'tasks'), 'use') (('that', 'image'), 'be') (('smaller vehicles', 'accidents'), 'be') (('last 4 corners', '2d Draw Lyft Dataset SDK dev front kit'), 'draw') (('map Map that', 'top down view'), 'datum') (('distance earlier information', '3D contours'), 'use') (('process', 'actual human vision'), 'be') (('which', 'self driving car'), 'dig') (('Therefore scence', 'several samples'), 'make') ", "extra": "['annotation', 'biopsy of the greater curvature']", "label": "Perfect_files", "potential_description_queries": ["active", "animate", "animation", "annotation", "appear", "append", "artifact", "assign", "associated", "attribute", "average", "batch_size", "binary", "bounding", "box", "calculate", "camera", "car", "category", "center", "challenge", "check", "clear", "close", "color", "colored", "combined", "command", "content", "contour", "course", "create", "current", "cut", "data", "dataframe", "dataset", "decode", "default", "depth", "derive", "detect", "detected", "detection", "device", "dictionary", "difference", "direction", "display", "distance", "distribution", "drop", "eda", "empty", "end", "ensure", "entity", "even", "exit", "explore", "extract", "field", "file", "final", "find", "flat", "float32", "floor", "focal", "form", "format", "forward", "found", "frame", "frequency", "function", "future", "general", "generate", "generated", "green", "hand", "header", "height", "help", "high", "hope", "human", "idea", "image", "import", "including", "index", "init", "instance", "int", "intensity", "interactive", "intuition", "kernel", "key", "left", "length", "let", "level", "library", "lidar", "light", "line", "list", "log", "look", "lower", "lyft", "majority", "map", "mapping", "mask", "matrix", "mean", "meaning", "measure", "mesh", "method", "might", "most", "move", "multiple", "name", "near", "need", "negative", "no", "none", "not", "number", "numerical", "numpy", "object", "observation", "order", "out", "package", "parallel", "path", "people", "perform", "pixel", "plane", "plot", "point", "pointcloud", "positive", "print", "probability", "query", "radar", "rare", "reference", "relationship", "relative", "remove", "render", "representation", "resolution", "return", "reverse", "right", "sample", "scene", "second", "section", "sense", "sensor", "set", "several", "shape", "short", "side", "signal", "similar", "single", "skew", "source", "space", "special", "speed", "spread", "store", "str", "system", "table", "target", "technology", "tensor", "theory", "those", "time", "timestamp", "token", "tool", "training", "transform", "transformation", "trend", "type", "under", "up", "update", "valid", "value", "variation", "video", "view", "visualization", "visualize", "volume", "while", "width", "work", "world", "xhlulu"], "potential_description_queries_len": 222, "potential_script_queries": ["copy", "datetime", "matplotlib", "pathlib", "rc", "reduce", "rotate", "seaborn", "tqdm", "translate"], "potential_script_queries_len": 10, "potential_entities_queries": ["clear", "colored", "depth", "frame", "human", "left", "lyft", "most", "object", "sensor"], "potential_entities_queries_len": 10, "potential_extra_queries": ["biopsy of the greater curvature", "biopsy", "curvature"], "potential_extra_queries_len": 3, "all_components_potential_queries_len": 230}