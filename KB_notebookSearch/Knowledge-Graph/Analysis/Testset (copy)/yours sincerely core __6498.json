{"name": "yours sincerely core ", "full_name": " h2 Overview h3 Takebacks h2 MyNotes h1 Acknowledgements h1 Levers for different kernels h1 Section Utilities h2 Leverage HTML output for pretty printing logs h2 Profiler utility for performance monitoring h2 MultiProcessing Utility h2 Embeddings Utility h1 DataManager h1 Section Preprocessing Text h2 Text Handling h2 QuoraPreprocessor h1 Section Synthetic Data Generator h2 Overview h2 TopicalWords h2 InsincereVocabGenerator h2 InsincerityWithNER h2 QuestionsGenerator h1 Section Machine Learning h2 Attention Layer h2 Model Factory h2 NLPPatterns h1 The Flow h1 Main Where It Begins h1 Rough Work area ", "stargazers_count": 0, "forks_count": 0, "description": "Assumes that each worker process will write output to a file map out and when they are all done the outputs will be reducedand written out. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. Raw cleaned data Clean Question Text load reduced csv Collect unique words vocab Combined training and test vocab into 1. One model factory per word embedding. This method changes the underlying rawdata and file names for processing training vs test data. Section Synthetic Data Generator OverviewObjective There are only 6 insincere questions. The similar words may fall under 3 kinds 1. Twice for newly discovered topical words Iterate categories PEOPLES Hindu None. Now we need to Build some intuition about the entire dataset eyeballing stats and samples Preprocess the text Generate synthetic data see how below Create word indices Create training sequences each question is an array of indices which refer to words in the embeddings matrix To save developer time save to file embeddings matrix word indices training sequences Generate synthetic data Get external list of terms that are expletives ethnicities and politics using word2vec word similarities. Takebacks Editor issues Coding on Kaggle s Jupyter is painful primarily because lots of code means too much scrolling I am new to Kaggle and AI competitions. read_csv Use join to get full file path. replace punct f punct log_list desc Raw Data Statistics Spelling errors tokenwise Does more than just split the phrase. It means substitutable for the purposes of insincerity. Naming matters to me for clarity of thought and OOP indirections enable better naming. Find the right list of substitutes For each substitutable phrase Output Find multiple word lists with the largest phrase Create generated questions CSV that matches the train. Did it by using filesystem. Can explore merging word embeddings later. Important insight in many published kernels the vocabulary is created from training data only. Each category is mapped to an array of dictionaries of words. QuestionsGeneratorFinally the multi processor generation of questions using phrase substitutions from InsincereBoW above Section Machine Learning Attention LayerCopy pasted from https www. cleanup related words that are not in word_list are newly discovered topics. Mix embeddings GLOVE and PARA seem popular Add features by hand Glove only OOV hand fixes explore words in different embeddings Acknowledgements1. if not found and verbose log_list found_multi_words desc Words of MultiWord not found. These words can be used to expand the seed. It doesn t know anything about natural language. words that relate the most to the list of partially matched 1. Just that bad things said about a group of people and what is said. PARAGRAM has all lowercase. Handles csv files if they exist. Foresight suggested that I might be experimenting with many models and I wasn t clear how the limits on 2h GPU vs 6hr CPU would pan out. horizontally relevant to topical word Textually matching words Words that could be substituted for each other and the source word. Most likely python has these facilities but easier to implement my intentions here than learn new stuff due to time. How can we generate more insincere questions In essence Bad things said about one group of people are likely to be equally bad for another group. DataManagerData loader Section Preprocessing Text Text Handling So far we have loaded up the files provided by the competition and added functionality for their content. Upper case word s index gets assigned to lower case word s vector For paragram let s give the vector for lowercase word to uppercase word Allow any form of a word that is in the word2index and if not present then find the lowercase and and if present but PARAGRAM then find all indices of all forms of the word in word2index. QuoraPreprocessorAssimilation of lots of good and partially good advice on the Kaggle kernels for this competition. Helper function if gWIP log f Not found retval Reduces output files from worker processes to single file. GOOGLENEWS is done differently doesn t use mean std why XX phrase phrase. MultiProcessing UtilityI am new to multiprocessing especially on Python. Since we have created a combined file. com nikhilroxtomar gru with kfold lb 0 689 Model FactoryTo try out different models since inputs and outputs from the model are the same. It receives a model training data as padded sequences. Memory efficient loading except for GOOGLENEWS Word2Vec which is loaded as a whole. So this complies with the rule of no external data sources. Combine all words that vertically belong to the source word which is the topic Topics that are more similar to textually matching words. TopicalWordsUsed GOOGLENEWS Word2Vec to generate words that are similar to each other such that they are substitutable. Returns embeddings matrix for one source. OverviewMain focus Generating synthetic data because only 6 of data is tagged insincereGreat place to start code review would be to find the classic C style main function near the bottom. Section Utilities Leverage HTML output for pretty printing logs Profiler utility for performance monitoringPut profile above any method and performance is automatically tracked. Add newly discovered words to topical words for this category Find multiple word lists with the largest phrase Search for substitutes by matching the longest phrase in the question within each category. can t use member function output_cache_name because GOOGLENEWS has compound words with underscore Allow underscore in single word and check if each component is also in the vocab. words that partially match the seeds and 1. All target methods must have a signature func self cpu_id cpu_count cache_file. The question generation takes a while and would have been much faster on 4 CPU but the training over 2DCNN is much faster on GPU. The Flow Main Where It Begins Rough Work area This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. XXX culling requires more attention. Primary reason for OOP would be readability and avoiding huge function signatures. I followed no external data sources so strictly that I didn t realize I could import intermediate results from other kernelsMyNotes Improve q generation questions not used for generating q. Brevity not a focus. Code quality Lots of OOP not seen commonly with ML projects but no getters setters for dev speed and bloat. So multiprocessing seemed good to invest time and also for learning. Similarity does not mean synonymity. x SpatialDropout1D 0. NLPPatterns The NLPPatterns only operates on numbers. For all insincere questions if there is a term that matches any list above create new questions by interchanging the terms from the appropriate list. And similarly with other taboo topics. For manual review as well as saving dev time save generated questions to file. com theoviel improve your score with text preprocessing v2 Levers for different kernels Many of these have the biggest impact on compute time. For each matching phrase there could be multiple lists of substitutions. Sort list of tuples by the first element size. They are vertically matching. Pickled binary object If there is a CSV file then reduce. Returns an array of words RawData Pre processing Word Count Raw cleaned data Produce vocab dictionary The maxlength of questions at 60 arrived at after counting words in training set. What I learnt is applicable for multi node processing in future. InsincereVocabGeneratorUsing TopicalWords and an initial set of words this class uses similarity function from Word2Vec to discover similar sets of words that could be substituted into a question with each other. InsincerityWithNER This is used to eyeball the highest frequency named entities that are involved in insincerity. Find matching words in related words to source word Add all related words that are not matching words to insincere_vocab category seed topical words by category Insincere_bow is an dictionary of categories. XXX what assumptions can be made about train. This isn t correct because the model for training and predictions has the same embeddings matrix. Another is reuse for future projects. An expletive can probably be replaced with another similar expletive. words that are similar to the seed but not the words in the other 2 kinds. can make a question insincere. 4 x BatchNorm XXX channel 1 channel 2 channel 3 merge interpretation evaluator_cb F1Evaluation validation_data X_val y_val interval 1 verbose 1 Initialize a counter Combined generated cached file with input to create big training data cache. Reduction handles csv. Collect unique words vocab This class was originally written to process some rawdata and write the results to file without focus on training vs test. We don t need to teach model human history. It is used for manually creating the seed for InsincerityVocabGenerator. Lots of focus on i believe what I see and so lots of print statements. For training more data is better. For each insincere question If a word phrase in the question is in the list of taboo words that are similar then Replace that word phrase with other similar words or phrases. Lots of interim results written to filesystem. Additionally words that belong in a category should match the type of Named Entity of the category. Iterate topical words in each category Hindu Sikhs. word2count is a Counter dictionary Starting index at 1 so that index 0 is reserved. Find related words to source word. Explored multiprocessing on python first time. Embeddings Utility Memory and performance efficient way to load embeddings. Taboo topics are Religion Ethnicity group of people Politics Sexuality Words with negative intense emotion expletives Generate taboo words from similarities to some set of seed words in word2vec for each topic above. Get size and add to list of tuples. RawData word substitutes Multi process q generation To help improve generation algorithm. http The final result is cached to file for use by the QuestionGenerator. Get X Get y Get embeddings matrix Select Model Train initialize_cache remove_file sequences remove_file submission. ", "id": "takeseven/yours-sincerely-core", "size": "6498", "language": "python", "html_url": "https://www.kaggle.com/code/takeseven/yours-sincerely-core", "git_url": "https://www.kaggle.com/code/takeseven/yours-sincerely-core", "script": "log_dir sklearn.metrics combine_vocab multiprocessing make_callbacks calculate tensorflow.keras.callbacks find_prediction_threshold EarlyStopping rmtree memclean tensorflow.keras.models map_process collections build save multi_gpu_model main tokenize optimizers ReduceLROnPlateau inspect Sequence pos_phrase apply_patterns Tokenizer sequences_proc sklearn.model_selection f1_score functools show_html move_files InsincereVocabGenerator DetectorFactory __len__ tensorflow.keras filter_substitutes find_substitute_candidates Callback tqdm.autonotebook InsincerityWithNER massage_data clean_cache test_method spacy cpu_cache_name get_topical_words get_prof_data backend as K get_coefs gen_from_one_q train_test_split pp_prof_data make_path reduce get_tmpfile tensorflow.keras.optimizers tensorflow.keras.layers keras.preprocessing.sequence QuoraSequence(Sequence) is_vocab do_substitutes testme on_epoch_end get_tag seaborn numpy gensim.models find_string KeyedVectors shutil output_cache_name create_CPU_LSTM_Model make_word2count remove_file get_NER_label NLPPatterns TestMPCWithFileCache AttentionLayer(Layer) gen_NER ModelCheckpoint constraints IPython.core.display copyfile metrics datapath filter_related_sources keras.utils displacy pandas denumber word2count Counter synthesize_data ModelFactory initialize_cache nltk.corpus F1Evaluation(Callback) find_patterns has_gpu make_tokens compilation pad_sequences validate HTML precompute_mean_std testme2 display create_LSTMGRU_Model stopwords profile tensorflow.python.client keras.callbacks log despace compute_mask wraps class_method train log_prof_data load compute_output_shape defaultdict make_clean_questions DataManager create_2DCNN_ConcatModel glove2word2vec log_current_memory decontract read_embeddings_file with_profiling embeddings detect TopicalWords get_data make_substitutes_bow nlp fix_special_chars respell generate_similar_words depunctuate find_matching_phrases keras.preprocessing.text gensim.test.utils langdetect TextualDataControl mislabeled_sincerity data_generator spawn gensim.scripts.glove2word2vec multi_word_in_vocab create_2DCNN_Model save_binary backend clear_prof_data clean make_sequences QuestionsGenerator __init__ get_model make_vocab create_GPU_GRU3_Model predict display_cache range_partition log_list create_1DCNNNgram_Model __getitem__ generate_bow gen_q_proc create_GPU_LSTM_Model sent_capitalize initializers EmbeddingsControl() load_embedding_index set_mode make_word2index start device_lib sklearn MPHelper matplotlib.pyplot vocab_proc is_substitutable call data_partition save_combined_data tqdm cleaning_proc TestProfiler regularizers contains find_substitute_phrases layers fillna Profiler QuoraPreprocessor(TextualDataControl) keras process find_substitutes_list nogen_q_proc instance ", "entities": "(('final result', 'QuestionGenerator'), 'http') (('that', 'word_list'), 'be') (('class', 'test'), 'write') (('Taboo topics', 'topic'), 'be') (('that', 'category'), 'match') (('how limits', '6hr CPU'), 'suggest') (('Iterate categories', 'Hindu None'), 'twice') (('I', 'Kaggle competitions'), 'issue') (('Words', 'MultiWord'), 'desc') (('Primary reason', 'function huge signatures'), 'be') (('We', 'model human history'), 'don') (('NLPPatterns', 'only numbers'), 'nlppattern') (('that', 'expletives word2vec word similarities'), 'need') (('which', 'whole'), 'loading') (('Raw Data Statistics Spelling errors', 'more just phrase'), 'replace') (('It', 'python docker image https kaggle github'), 'Main') (('that', 'appropriate list'), 'create') (('It', 'insincerity'), 'mean') (('model', 'embeddings same matrix'), 'correct') (('target methods', 'cpu_count func cache_file'), 'have') (('that', 'train'), 'generate') (('that', 'partially seeds'), 'word') (('GOOGLENEWS', 'differently doesn'), 'do') (('Section Machine Learning Attention LayerCopy', 'https www'), 'paste') (('component', 'also vocab'), 'use') (('OOP indirections', 'thought'), 'matter') (('RawData word Multi', 'generation algorithm'), 'substitute') (('It', 'padded sequences'), 'receive') (('that', 'insincerity'), 'insinceritywithner') (('that', 'other similar words'), 'be') (('So multiprocessing', 'time'), 'seem') (('Many', 'compute time'), 'have') (('about one group', 'equally group'), 'generate') (('that', 'other'), 'use') (('getters', 'dev speed'), 'see') (('Add', 'category'), 'find') (('when they', 'outputs'), 'be') (('Most likely python', 'time'), 'have') (('It', 'manually seed'), 'use') (('vocabulary', 'data'), 'create') (('OOV hand only fixes', 'different embeddings'), 'seem') (('that', 'partially matched 1'), 'word') (('expletive', 'probably similar expletive'), 'replace') (('Returns', 'one source'), 'embedding') (('method', 'test data'), 'change') (('So far we', 'added content'), 'DataManagerData') (('training', 'much GPU'), 'take') (('that', 'categories'), 'be') (('words', 'seed'), 'use') (('didn I', 'q.'), 'follow') (('maxlength', 'training set'), 'return') (('csv Collect unique words', 'test 1'), 'clean') (('as well saving', 'generated questions'), 'save') (('retval Reduces', 'single file'), 'function') (('Section Utilities Leverage HTML output', 'method'), 'track') (('inputs', 'model'), 'com') (('similar words', '3 kinds'), 'fall') (('index', 'Starting Counter dictionary 1'), 'be') (('X', 'embeddings matrix Select Model Train initialize_cache remove_file sequences remove_file submission'), 'get') (('assumptions', 'train'), 'xxx') (('such they', 'other'), 'Word2Vec') (('only 6', 'bottom'), 'focus') (('what', 'people'), 'say') (('I', 'print so statements'), 'believe') (('Combined', 'training data big cache'), 'interpretation') (('learnt', 'future'), 'be') (('topic that', 'more textually matching words'), 'combine') (('that', 'other 2 kinds'), 'word') (('PARAGRAM', 'word2index'), 'assign') (('MultiProcessing UtilityI', 'especially Python'), 'be') (('t', 'natural language'), 'doesn') (('category', 'words'), 'map') (('Textually matching that', 'other'), 'relevant') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["area", "array", "avoiding", "binary", "case", "category", "channel", "check", "clarity", "clear", "code", "combined", "competition", "compound", "compute", "correct", "could", "create", "csv", "data", "dataset", "dictionary", "enable", "environment", "essence", "expand", "explore", "external", "factory", "faster", "file", "final", "find", "form", "found", "frequency", "func", "function", "future", "generate", "generated", "generation", "group", "hand", "help", "http", "human", "image", "implement", "import", "improve", "index", "input", "interpretation", "interval", "intuition", "join", "kaggle", "largest", "learn", "let", "linear", "list", "load", "loader", "log", "longest", "lower", "main", "manual", "map", "match", "matching", "matrix", "mean", "merge", "method", "might", "model", "most", "multiple", "multiprocessing", "my", "near", "need", "negative", "new", "no", "node", "not", "object", "out", "output", "people", "per", "performance", "place", "preprocessing", "present", "print", "printing", "processing", "profile", "python", "question", "reason", "replace", "result", "review", "right", "save", "saving", "score", "set", "several", "similar", "similarity", "single", "size", "source", "speed", "split", "start", "std", "stuff", "style", "target", "term", "test", "text", "thought", "time", "topic", "training", "try", "type", "under", "unique", "up", "v2", "vector", "vocab", "while", "word", "worker", "write"], "potential_description_queries_len": 146, "potential_script_queries": ["backend", "build", "calculate", "call", "clean", "copyfile", "core", "defaultdict", "detect", "displacy", "display", "instance", "langdetect", "log", "multiprocessing", "nlp", "numpy", "predict", "profile", "reduce", "seaborn", "shutil", "sklearn", "spacy", "test", "tokenize", "tqdm", "train", "validate"], "potential_script_queries_len": 29, "potential_entities_queries": ["dictionary", "hand", "image", "matching", "similar", "word"], "potential_entities_queries_len": 6, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 169}