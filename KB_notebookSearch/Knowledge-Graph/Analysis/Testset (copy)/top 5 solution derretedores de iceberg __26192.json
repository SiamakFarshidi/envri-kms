{"name": "top 5 solution derretedores de iceberg ", "full_name": " h1 Titanic Competition on Kaggle h1 First Steps h1 Visualizing our data h1 Feature Engineering h3 Surname h3 Title h3 Age Missing h3 Child h3 Family Size h3 Alone h3 Numerical Ticket h3 Group features h4 Group ID h4 Group Type h4 Num Group Survivors and Deceased h3 Split Fare h3 Cabin Letter h3 More than one cabin h1 Exploratory Data Analysis h2 Categorial Values h2 CabinLetter versus Embarked h2 CabinLetter verus Pclass h2 Embarked versus Pclass h2 GroupType and GroupSize h2 MoreCabin verus Pclass h2 New Features h3 Small Group and Family h1 Find inconsistencies h2 Title and Sex h2 Embarked and Group h2 Age and Parch h1 Cleaning data h2 Embarked h2 Fare h2 Age h1 Pre Processing the Data h2 Sex h2 One hot encoding h2 Standartization and Normalization h1 Feature Selection and Basic Machine Learning Base Models h2 Models Performance on Full Feature Set h2 Feature Importances h2 Recursive Feature Elimination RFE h2 Testing Different Feature Sets h3 Settings h3 Testing each subset of similar features h3 Testing my final opinions h2 Getting a final set of features h1 Validations and Hypterparamether Tuning h1 Making the predicting the preparing for submition ", "stargazers_count": 0, "forks_count": 0, "description": "Later we ll try to select only those that seen be lead to the best results. Settings Testing each subset of similar featuresHere we can see that in many cases the base score is actually better than adding more features. Let s define the functions we re going to use to tune the hyperparameters. Them let s visualize them and get some of the main info about them. Margaret is their mother. It was one of the families we put in the same group. Here we see again that some features are very good on tree models while not that good on linear models. Feature Selection and Basic Machine Learning Base ModelsSo finally we can start doing some Machine Learning. Now let s define the models and hyperparameters we re going to search over. So we re going to plot several graphs in order to better understand the trend of the data. So we re ready to proceed. So that means the it might not be necessarly due to the class or city sincethese decks have people from all classes and cities if it was very correlated with the class we would except that the higher survival rates would be in A B and C since these only have first class passengers GroupType and GroupSizeWe see that much the Family and Mixed has more survivors and deceased in sizes of 2 and 3 for family and 3 4 and 5 for mixed. Doolina are siblings and Mrs. 96 Let s try to check what of these sets are the best by summing the columns Fortunately almost all sets are better than the base set. We can try to drop them and see if the scores increase. We can now check how they performed in the test set according to each metric their positions don t have much if we change the sorting metric. I ll show you that this was the only group with different fares Here we can clearly see that all tickets cost about 26. Num Group Survivors and DeceasedNow we re going to calculate the number of survivors of a group. Right now our stragety is creating the maximum amount of features we can. We can try don t RFE again without Mr We also see that Title_Miss doesn t have much impact And that Title_Master and Title_rare seem to be good features. Another good variable to see is the Parch. I think it could be because everyone on that desk is from the first class but I don t know CabinLetter_D and CabinLetter_E are listed as good features. Embarked Let s take care of these few missing values in embarked and fare first. Issac Gerald embarked in C and that he is a sibling of Dr. Cleaning dataLet s now see what we ll do about the null data we have in the data sets. We want to fill the NaN values and keep the shape of the graph as similar as possible. That s a good feature because if we have 5 people in a group and four of them survived and the fifth one we don t know the odds are higher that this person will also survive. B D and E have the higher survival rates. Also we need to split again our data set into the training set and test set. Categorial ValuesSomethings were very excepted while others are very interesting to realize. While there is 2 embarked missing in the training set and one fare missing in the test set. Recursive Feature Elimination RFE Let s now try a different way of feature selection. Let s check the best sets Here we see that indeed using all the classes or not doesn t affect much and it s better not using Age and Embarked. Suppose we have two groups both with four people One of the groups have three people that we know died in the accident and the four we need to predict The other groups have four people that we need to predict. In the feature I ll be look for the best way to find these NaN values but I don t want to lose the NaNs beucase they can be meaningful. Age Missing There are many NaN values in the Age column. Now talking about families and groups. Let s correct the values for this family. Looking at GroupType features we see that none of them were very good nether on linear nor on tree models. If we simply count the number of people we know survived both groups will have a value of zero Therefore it s important to have not just the number of survivors metric but also the number of deceased. Both dataframes have many null values in the Cabin column and some on the Age column. But when the divide the families and groups by size we see that very large families and groups also have higher changes of dying. Feature ImportancesAnd let s examine the feature importances that each model gave to each feature and start checking which features we ll keep or not. Let s start with the easy one CabinLetter_N is one of the best features and we already were expecting that because we saw in the EDA section that there was a huge difference between having a cabin assigned to a row or not CabinLetter_T is quoted as best than letters A and F but that s because some linear classifiers think this is a good but we know that we actually should have already dropped this feature because it s completely useless CabinLetter_A and CabinLetter_F as quoted as the worst features in our dataset. We see Age indeed an horrible feature that could the way the filled the values or something like that I really don t know Embarked is also not good Using or not all the classes doesn t metter much Same thing for the size Using only Mr Master and rare is better. We also see that indeed Mr. The Pclass_1 feature is considered one of the worst features. A similar thing happens with rare and rare doesn t have much examples so it might be a good idea to drop that feature. Let s go column by column Surname TitleThere are many titles that have very few examples so we re going to group them under the same category. We see that the higher the class the higher the median age. There are a lot that we can create and think. Then group 1 will have a deceased number of 3 while group 2 will have a number of 0. To do so we re going to perform what s called Feature Selection selecting only the best features. Hays Thomas Thomas Elias Elias Elias Elias Giles Giles Wiklund Wiklund Andersson Andersson Andersson Andersson Andersson Kink Kink Kink Braund Braund Hansen s bouhgt three different tickets Hansen s bought three different tickets Hansen s bought three different tickets Jensen s bought four different tickets Jensen s bought four different tickets Jensen s bought four different tickets Jensen s bought four different tickets Kiernan Kiernan Jussila Jussila Larsson Larsson Hagland Hagland Francatelli knows Mr and Mrs Morgan Morgan Lamson Lamson Frolicher Frolicher Thomas Thomas Elias Elias Elias Elias Giles Giles Wiklund Wiklund Andersson Andersson Andersson Andersson Andersson Kink Kink Kink Braund Braund Hansen s bouhgt three different tickets Hansen s bought three different tickets Hansen s bought three different tickets Jensen s bought four different tickets Jensen s bought four different tickets Jensen s bought four different tickets Jensen s bought four different tickets Kiernan Kiernan Jussila Jussila Olsen Olsen Larsson Larsson Davies Davies Duran y More Duran y More Ilmakangas Ilmakangas Hagland Hagland Bowerman Francatelli and Morgan Francatelli and Morgan Hays and Potter Lamson s bought two different tickets Lamson s bought two different tickets Mock Andrews and Longley Frolicher Parrish Walton Thomas Elias Elias Elias Elias Giles Giles Wiklund s bought two different tickets Wiklund s bought two different tickets Andersson Andersson Andersson Andersson Kink Kink Braund Braund Hansen s bouhgt three different tickets Hansen s bought three different tickets Jensen Jensen Jensen Jensen Kiernan Kiernan Eustis Jussila Jussila Olsen Olsen Larsson Larsson Davies Duran y More Duran y More Ilmakangas Ilmakangas Hagland Hagland Thomas Elias Elias Elias Elias Andersson Andersson Braund Braund Hansen Hansen Jensen Jensen Jensen Jensen Kiernan Kiernan Jussila Jussila Larsson Larsson Davies Hagland Hagland Hays Hays Hays Everyone in the group was on a B cabin First we set the GroupId to be the index Now we iterate over the dictionaries that have the GroupId as keys Now we reset the index And iterate over the passenger_id Now let s show the modified dataframe entried. This can be useful because for instance if a whole family in the training set survived and there s another family member in the test set odds are this person will also survive. The types of inconsistencies we can find or at least the ones I tought are GroupSize FamilySize where GroupType Family FamilySize 1 but GroupType Family IsAlone False but GroupType Alone IsAlone True but GroupType FamilyLet s begin with the first one By looking at the data we see that a few thing might be the case for these rows We ve got a lower value for the family size because some people might be from the same family but not exactly were counted by the columns SibSp and Parch. Augusta came from S that s a mistake. Based on the surname we can classify each ticket as a person that is traveling alone a family a group of friends or a mixed between family and friends. com company beedata usp mycompany student group of the University of S\u00e3o Paulo Brazil. Cabin LetterFirst I ll fill the NaN Cabins with the letter N. So it could be right but we also need to do some research GroupId is another family with the same ticket but different cities that also is probably a mistake. Here we see that even though Pclass_3 has an overall rank of 4 it not all models agree that this is a good feature. There are still some inconsistencies that I did know how to resolve Let s now look at the third inconsistencies IsAlone False but GroupType AloneWe can see that the third inconsistencies are the same as those I said I don t know what to do. That helps almost every non tree based model. Child Family Size Alone Numerical TicketA thing we can see in the ticket column is the fact that some tickets just have numbers while others have numbers and letters. Group ID Group Type InconsistenciesWe still need to fix some group types and look closer the data in some cases. Visualizing the predicitons versus the trend in the traning set. So this shows that indeed there s a correlation between these group sizes and surviving or not. We can see that it s more probable that they came from city C because the SplitValues are more centred around 40. Thus I think it s better to just have a binary value instead of a NumCabins feature. Looking at the encyclopedia titanica https www. Indeed as we suspected the Family and Mixed types have much more Child and Women rates than the other types. So there are about 20 of missing ages and about 78 of missing cabins. The test set is the one we need to predict the target variable. Exploratory Data AnalysisNow that we ve created lots of features we can plot more graphs to try to visualize correlations between these new features and the target features Survived. Is was produced by a team from the BeeData https www. Let s now look at the second inconsistency FamilySize 1 but GroupType FamilyLooking at the data we see two main cases People that are from the same family but broght two different tickets most of them are consecutive Groups that are Family but were considered Non Family because many people have a different single name. And let s define all the base models we re going to test Now let s define some functions that will be useful to evalute our models. New FeaturesMotiveted by the EAD process we can create new features that will reinforce the trends we saw in the data. Let s try to look at these entried We see that actually we should have just G or E as they cabin letters so we ll fix that. So C is not a mistake. I ll definely keep CabinLetter_N and we can test keeping letters B D E and maybe G. So we re left with just two options S or C. But at the same time we see that they are very good on some models but very bad or others. Better Classes try just class 3 Title try each title Better in some case worst in others Small Sizes FamilyAlmost the same EmbarkedWorst Grouptype Bad Ages try just Age CabinLetter try each letter Testing my final opinionsLet s know try some feature sets that I think are going to be good after doing all this analysis. The cell below has the list of the best estimators before traning. The two graphs must look as similar as possible. In particular we can see that AdaBoost and ExtraTrees have mean accuracy of 99 in the training set but just about 80 in the test set. 5 but larger groups have higher fare because this value is the orginal value multiplied by the number of people that bought the same ticket. The train df has two null values in the Embarked column and the test df has a null value in the fare column it might be worth to take a look later. Let s take a look at our original test set with the new column Prediction to understand the decisions of our model. We see that there s no difference in Sex and Embarked. We can see that many of these features SibSp Parch FamilySize GroupSize and SmallGroup have a better rank on tree models. We see that it s market as the third best feature IsChild is also market as a bad feature probably because the Age is enough MoreCabins is another irrelevant feature AgeMissing and NumericalTicket are also not important we also saw this in the EDA section One very interesting thing is that Title_rare was market as a good feature that seems a bit strange Another very strange thing is the fact that Age is market as a bed feature Let s try to understand better checking only some feature at time. When comparing group features we see that GroupSize was better in the majority of the situation so we might want to keep it instead of SmallGroup. Group features Let s group the tickets and measure some metrics upon these groups. Title and Sex Embarked and GroupHere we see that we have 3 groups GroupId 63 have a couple with same ticket but one came from S and the other from C. Let s try to see the division of the prices by class for the city S. It could also be the case that families will tend to have more child and women than people traveling alone. Therefore it s fair to create a new column SplitFare using the fare divided by the number of buyers. FareNow let s look at the NaN Fare value in the test data. That s because Parch equal to 1 or 2 are mainly kids and greater than that will never be a child therefore it can be also used. encyclopedia titanica. First there are some exceptions we found previously. General Data Science Modules Plotting Modules Basical Modules Machine Learning Classifiers Feature and Model Selection For Kaggle uncomment Here I ll be saving the test set for the submition Here I add SibSp and Parch to the categorical_values because there re only few values Thus I can group them to get some insight Creating a figure Getting the total amount of each category Getting the survivors count for each category Getting the percentages Plottin the total and survived amounts Plotting the percentages as text on top of each bar Adding title and legend Here I just add some extra space for the percentages labels Showing each image at the end of the for loop Creating the figure Plotting the first axis total vs survivors by age Plotting the second axis male vs female by age Adding labels titles and legends removing an auto added ylabel by pandas Showing the figure Carrau Minahan Risien Newell Watt Ware Ware Renouf Jefferys and Denbury Hirvonen Lindqvist Christy Jacobsohn Hocking Hocking Richards Gustafsson Gustafsson Backstrom Vander Vander Klasen Klasen Bourke Bourke Strom Persson Hays Davidson Frauenthal Frauenthal Crosby Crosby Ware Ware Christy Jacobsohn Hocking Hocking Richards Gustafsson Gustafsson Backstrom Vander Vander Klasen Klasen Bourke Bourke Strom Persson Frauenthal Frauenthal Crosby Crosby Ware s bought two different tickets Lindqvist is accompained by the Hirvonen s Hocking and Richards are the same family Hocking and Richards are the same family Hocking and Richards are the same family Backstrom and Gustafsson are the same family Backstrom and Gustafsson are the same family Backstrom and Gustafsson are the same family Klasen s bought two different tickets Klasen s bought two different tickets Bourke s bought two different tickets Strom and Persson are the same family Strom and Persson are the same family Frauenthal s bought two different tickets Frauenthal s bought two different tickets Crosby s bought two different tickets Crosby s bought two different tickets Carrau Hocking Hocking Richard Watt Ware Ware Risien One of the Frauenthal s is in cabin D40 so the others might be at least under the D lobby Denbury is traveling with Jefferys Perreault is traveling with Hays Payne is traveling with Hays First we set the GroupId to be the index Now we iterate over the dictionaries that have the GroupId as keys Now we reset the index And iterate over the passenger_id Now let s show the modified dataframe entried. That probably is a mistake we need to do some research to find where did they come from GroupId is a family with two tickets. It seems like we have a kid with Parch 3. But we can look at the SplitFare value. Visualizing our dataJust printing unique values or try to use pandas commands like value_counts or even pivot_table is not enjoy to really get insightful information about the data. When comparing the family features we see that SibSp and SmallFamily got the better ranks but SmallFamily wasn t good on tree based models while FamilySize was so we might want to stay with FamilySize. The rare Title doesn thave a higher change of surviving which is very interesting I was excepting it to be high. Age and ParchWe except that passengers who are kids shouldn t have a Parch greater than 2. Standartization and NormalizationNow every numberial feature has mean equal 0 and standart deviation equal to 1. Models Performance on Full Feature SetNow let s apply these functions and get a first view of our models in action when applied to the full feature set. Finally CabinLetter_B and CabinLetter_G are listed as median features. The Title have many differences specially in the Master and rare categories. Deck F doesn t have any passengers from class 1. We just need to select the estimator and the features set that we consider the best to submit make the predictions and save them into a. Before doing that though I found that there s one group that have different fares even buying the same ticket. As I tried many different version for the same kind of feature let s try to identify what option is the best for each feature. The same thing happens with the SibSp adults will not have more than one Spouse and kids tend to have a few siblings. A Statistical Analysis ML workflow of Titanic https www. It s strange to see that we have missing values for people that survived. We also see that it s not a normal distribution so let s take the median instead of the mean. Doing some research with the family we see that Mr. So let s group those exceptions together Now let s count the number of survivors and deceased. Finally we need to redo the IsChild calculations. This process is very boring so I ll try to be lean. This process also helps to get more ideias for the creation process of new features so called Feature Engineering. It got almost 85 in many sets XGB also is very good getting the best score so far 84. However this number could be misleading. For the cabin letters we have many cases. Getting a final set of featuresSo after all this jorney to find a very good set of feature we have four sets of features that we can try to use in order to get the best score we can. One interesting thing is that indeed the AgeMissing column seens to have some correlation with the rates of surviving but that sfar from what I personally excepted I was excepting something like we visualized in the CabinLetter. Let s check if that s the case. That s very interesting because we saw there s a high correlaction between the classes and the survival hate The Pclass_2 is also considered a very bed feature. After reading some notebooks of other people we came with these features for each original column Name Surname Title Age Is Child Is Age Missing SibSp and Parch Family Size Is Alone Ticket Group features Group Id Group Size Group Type Group Num Survivors Group Num Deceased Is Numerical Ticket Fare Split Fare Fare divided by Group Size Cabin Cabin Letter Multiple Cabins The columns Pclass and Embarked doesn t seen to offer much more than theirselves in their original format. Talking about the CabinLetter we can see that people with NaN in the cabin entry had a much lower change of surviving than the people that we knew the cabins specially cabins in the lobbies B D and E we ll visualize more about that in the future. So indeed it will be a feature that we want to keep. We can visualy see the correlation between the data with stronger colors representing higher correlations. Here we see that indeed their tickets are sequential. Here we see that the third class tend to pay around 7. Here we got amazing scores for the tree models. Not everything might be useful so we need to be careful. Master got very good ranks on linear models but very bad on tree models. With this analysis we see that the sex and class are very important to decide if a person will survive or not. The same thing happens in the IsChild column. There are many interesting things we visualize here. Using RFE we can visualize the ranks of each feature. We already can see that many models are overfitting the data. This one will be harder since we don t have the Cabin value to help us. The features I ll create are SmallGroup 2 up to 4 members SmallFamily 2 up to 4 members Small Group and Family Find inconsistenciesBefore cleaning the data let s try to find and adjust some inconsistencies in the data. As I was creating a huge section of the notebook I ll just fix them quickly without much explanation. Titanic Competition on KaggleThis is a Python notebook for the Titanic Competition on Kaggle https www. We saw they are indeed the best I ll leave CabinLetter_T out I ll leave Title_rare out Let s also divide our models into Linear and Tree based models because we saw that there are many differences between them Now let s create some functions to help us evaluate each set First let s make feature sets using the fixed set and each other set Let s also add the full set of features Returning the obj we can get all of its attributes Defining some parameters that will be the same for every model Defining the lists we ll use on the tuning function Logistic Regression Classifier Ridge Classifier Stochastic Gradient Descent Decision Tree Classifier ExtraTree Classifier highly randomized version of a Decision Tree Random Forest Classifier SVC Classifier K Nearest Neighbors Classifier Gradient Boosting Classifier Extra Trees Classifier XGBooster AdaBoost on a Decision Tree Classifier Here we choose the traning set we ll tune our models Try using verbose 2 if it s taking too much time. Feature Engineering Now let s create some new features that might be useful to us. Going back to the EAD section we see that these cabins don t have many examples they have about 20 instances so that could be the reason why they are not good CabinLetter_C on the other hand has about 100 entries but it s also listed as a bad feature. We saw that using the ticket we can find many people that were traveling together. Sex We need to transform the Sex column into a numerical column. One more interesting thing is the Pclass. For instance there could be a higher probabilty of not surving if we today don t know the age of that person. Here is a list of them RM W Titanic vF1 https rpmarchildon. So let s treat that case before proceeding Now we can proceed. We created lots of potencial features and saw that many of them have a high correlation with your target variable Survived while some others don t seem to help. Where we see that they are two women that were in the cabin B28 which costed 40 and is the same ticket. Titanic is our first competition and we based our work on some other notebooks available online. She was the personal maid of Ella Holmes. First let s print the age distribution to see how it is before the changes we ll do. Perhaps the Pclass have some dependence with other features or the most important class is the third. In the IsAlone column we see that people traveling alone had a lower survival rate. But they agree that Pclass_1 is a bad feature. That could be because there are many classes on those desks and because they have higher chances of surviving. So even if they are not listed as very good features we might want to keep some of them because the tree based models got the better results. Let s set them as variables Validations and Hypterparamether TuningNow that we ve selected the best feature set we could let s start tuning the hyperparameter for each model and try to extract their best before moving on for the ensambles like stacking and voting ensambles. The steps we need to do are change the Sex column from string to integer do One Hot Encoding in the categorial columns like Pclass and Embarked scale the numerical features to help the models using MinMax or Standartization. We see that the Age is considered a good feature for many of the Tree Based models but a really bad feature for all linear classifiers that makes the overall rank to be very bad but we don t have to really remove this feature. Pre Processing the DataThe last step before finally going into the Machine Learning models is pre processing the data. RFE uses some kind of metric to give each feature a grade and remove the one with the lowest grade at each step. com wp content uploads 2018 06 RM W Titanic vF1. This is probably because Age have all the information we need. And families of size 4 have amost an equal amount of survivors and deceased. Finally we see that the type of group really has some influence. Amelia Bissette embarked in C with the other members of her gropus. If you want to run every cell and don t the searches make sure to comment the cell above and just use the cell below Making the predicting the preparing for submitionIt was being a lot jorney since here but we re finally ready to submit our predictions. I ve already took a closer look at the data and solved all the cases one by one. In the Title we see the same trend for girls and childs. So we can count the number of survivors of the groups. The distribuition of the age after filling the NaN values. That s the case of almost all tree based models. You can follow the progress of each model I m commenting because Kaggle takes A LOT OF TIME to run this so I hardcoded the best estimators Selecting the feature set Selecting the estimator Here I ll show the head of the dataframe we re going to submit Total positive predictions. Sex with the exception of Decision Tree is considered a very good feature for all models. Another thing we see is the fact that AgeMissing and IsChild are not considered good by any model so we probably can remove them. Finally let s look at the last inconsistency IsAlone True but GroupType FamilyWe don t have any cases. First the graphs for GroupNumSurvived and GroupNumDeceased are very obvious The only feature that doesn t seen to have much impact is the NumericalTicket column. This could explain the trends we saw. Ware s Lindqvist and Hirvonen Hocking s and Richards Backstrom s and Gustafsson s Klasen s Bourke s Strom s and Persson s Frauenthal s Crosby s Francatelli s and Morgan s Lamson s Frolicher s Thomas s Elias s Giles Wiklund Andersson s Kink s Braund s Hansen s Jensen s Kiernan s Jussila s Olsen s Larsson s Davies Duran y More s Ilmakangas Hagland s Here we see the changes in the GroupId on the Richards Hocking s Backstrom s and Gustafsson s We group by the id because we saw many people knew each other but didn t buy the same ticket. Then we ll be able to tell based on the features that we already have if a person is going or not to survive. Now let s try to look just at third class. Let s visualize the Age against many categorial variables. We ll use 0 for male 1 for female One hot encodingFinally we need to change the non binary categorical values into binary values using one hot encoding. Also I found that indeed a sister of her called Eliza Johnston was on board and also a friend Alice Harknett. Split FareWhen a ticket was bought by more than one person the original fare is multipled by the number of people that bought the ticket. com masumrumi a statistical analysis ml workflow of titanic by Masum RumiWe hope you like First StepsLet s first of all import our libraries and read the data sets. So S was a mistake as well. Now it s time to filter only the ones that will truly help our models to learn the discard those that won t. They would be cousins uncles or something like that We have a higher value for the family size because some families didn t buy the same ticket We have people that don t have the same surname as the parents or spouse Or it can be some other strange case. More than one cabinThere are just very few examples of people that reserved more than one cabins. org we see that indeed Miss. Here we can have a great view of how each model perform without any parameter tuning. CabinLetter versus Embarked CabinLetter verus Pclass Embarked versus PclassSome conclusions for this part are The only cabins with people from Q are C E and F Decks or lobbies A B and C just have people from the first class Pclass 1 More than 90 of people who embarked from Q are from third class Pclass 3 The other two cities have more passengers and their class distribution is more similar. And they also agree with the exception of the perceptron that Pclass_2 is not a good feature. Linear and other models were also good but tree models are defined getting the best results so we re going to just keep analysing them because we saw that if we find a good feature set for tree models the other models will also benefit from it. Commeting the best results GradientBossting seem to be the best model we have so far. Testing Different Feature SetsSo now let s try many different feature sets using the information we got from the previous tests. Let s look at the whole group to we if we find anything We have a family of five member of which four are kids. Both of them survived. AgeWe also need to decide that to do with the NaN ages. Indeed the higher rates are in families and groups of size 2 to 4. FamilySize and GroupSize don t seem to help a lot so let s use Title Pclass Parch and SibSp and just use Sex to untie. MoreCabin verus PclassHere we see that surprisingly there are about 7 people from class 3 with more than one cabin. We can try to find any pattern that indicate which city these two ladies came from. Let s average the values How many people in more than one cabin are there Creating a figure Getting the total amount of each category Getting the survivors count for each category Getting the percentages Plottin the total and survived amounts Plotting the percentages as text on top of each bar Adding title and legend Here I just add some extra space for the percentages labels Showing each image at the end of the for loop As the groupsize is 1 we can fill with the same value Checking if every age was filled out Setting the random_state and n_jobs parameters of the model Creating a Stratified K Fold object to split the data for the Cross Validation Creting empty arrays for the metrics we ll compute Performing the Cross Validation Fitting the model to the training data Getting the prediction values Computating the scores Computing the feature importance if the model has it Now let s compute the aggregation functions Computing the feature importances mean Returning the aggergations and the feature importances Sorting the importances Some models can t be used with RFE so we need to skip them Getting the ranks for each model Getting the overall ranks Let s fix some features that we saw are very good in the majority of the situations For now let s fix these three features. Here we see that Title_Mr is quoted as one of the best features by many algorithms with some exceptions from both linear and tree based models Ridge Perceptron LinearSVC LDA and ExtraTree but this feature seems to be one of the bests indeed many algorithm cite this as the best feature On the other hand Title_Mrs doesn t seem to be a good feature maybe because Mr already give the information we want. We can use this as a feature as well. Or it could be just that the Mixed group tend to have about 3 members Let s try to compare different features to see have more insights. ", "id": "giatro/top-5-solution-derretedores-de-iceberg", "size": "26192", "language": "python", "html_url": "https://www.kaggle.com/code/giatro/top-5-solution-derretedores-de-iceberg", "git_url": "https://www.kaggle.com/code/giatro/top-5-solution-derretedores-de-iceberg", "script": "RidgeClassifier sklearn.metrics cross_val_score PCA RFECV def_group_type sklearn.naive_bayes sklearn.tree def_group_id reassign_group test_feature_set sklearn.discriminant_analysis overall_performance AdaBoostClassifier tune_models KNeighborsClassifier precision_score MinMaxScaler recall_score def_rare DecisionTreeClassifier get_null_perc more_than_one_cabin seaborn numpy LinearSVC get_features_rank_for_model SGDClassifier train_model_CV XGBClassifier get_null_amount ExtraTreesClassifier show_importances sklearn.decomposition is_small sklearn.ensemble sklearn.model_selection f1_score RandomForestClassifier LinearDiscriminantAnalysis matplotlib.pyplot RFE = t_df.loc[ Perceptron def_group_size pandas ExtraTreeClassifier StandardScaler LogisticRegression accuracy_score RobustScaler is {score_full_class tune_model cabin_letter def_split_fare sklearn.feature_selection get_features_rank GridSearchCV MaxAbsScaler sklearn.neighbors sklearn.linear_model sklearn.preprocessing GaussianNB sklearn.svm SVC StratifiedKFold RandomizedSearchCV xgboost roc_auc_score get_model_name test_all_feature_sets train_test_split \\ ", "entities": "(('I', 'one one'), 'take') (('2 it', 'too much time'), 'see') (('that much Family', 'family'), 'mean') (('very person', 'analysis'), 'see') (('Pre', 'data'), 'process') (('four', 'which'), 'let') (('which', '40'), 'see') (('people', 'exactly columns'), 'be') (('Here we', 'tree models'), 'get') (('that', 'ticket'), 'FareWhen') (('you', 'data sets'), 'masumrumi') (('That', 'almost every non tree based model'), 'help') (('we', 'that'), 'be') (('best we', 'best results'), 'commete') (('so s', 'just Sex'), 'seem') (('we', 'already information'), 'see') (('we', 'us'), 'be') (('s', 'test data'), 'let') (('how model', 'great view'), 'have') (('we', 'previous tests'), 'let') (('Pclass_1 feature', 'worst features'), 'consider') (('third class', 'around 7'), 'see') (('where they', 'two tickets'), 'be') (('So we', 'data'), 'go') (('first we', 'other notebooks'), 'be') (('much Same thing', 'Mr only Master'), 'see') (('don today t', 'person'), 'be') (('they', 'surviving'), 'be') (('process', 'Feature so called Engineering'), 'help') (('others', 'target variable'), 'create') (('She', 'Ella personal Holmes'), 'be') (('who', 'greater 2'), 'have') (('person', 'them'), 's') (('it', 'dataset'), 'let') (('Deck F doesn t', 'class'), 'have') (('scores', 'them'), 'try') (('s', 'family'), 'let') (('So we', 'options just two S'), 'leave') (('we', 'CabinLetter'), 'be') (('already many models', 'data'), 'see') (('it', 'Age'), 'let') (('other models', 'also it'), 'be') (('we', 'feature'), 'visualize') (('so we', 'same category'), 'let') (('cell', 'traning'), 'have') (('we', 'that'), 'suppose') (('We', 'Sex'), 'see') (('Therefore it', 'buyers'), 's') (('different fares', 'even same ticket'), 'before') (('we', 'best score'), 'have') (('IsAlone GroupType FamilyWe don t', 'cases'), 'let') (('dataframes', 'Age column'), 'have') (('that', 'people'), 's') (('people', 'more child'), 'be') (('indeed sister', 'board'), 'find') (('families', 'survivors'), 'amost') (('more than one Spouse', 'few siblings'), 'happen') (('different that', 'same ticket'), 'be') (('we', 'many cases'), 'have') (('normal so s', 'median'), 'see') (('we', 'data'), 'create') (('none', 'tree models'), 'feature') (('Title_Master', 'much impact'), 'try') (('option', 'feature'), 'let') (('modified dataframe', 'passenger_id'), 'Modules') (('So we', 'groups'), 'count') (('AdaBoost', 'test just about set'), 'see') (('that', 'more than one cabins'), 'be') (('Master', 'tree very models'), 'get') (('t know CabinLetter_D', 'good features'), 'think') (('I', 'quickly much explanation'), 'fix') (('we', 'feature'), 'let') (('We', 'higher correlations'), 'visualy') (('it', 'parents'), 'be') (('we', 'letters B D E'), 'keep') (('we', 'that'), 'let') (('same thing', 'IsChild column'), 'happen') (('very I', 'it'), 'thave') (('clearly tickets', 'about 26'), 'show') (('more passengers', 'class third Pclass'), 'CabinLetter') (('many', 'tree models'), 'see') (('Amelia Bissette', 'gropus'), 'embark') (('So this', 'group indeed sizes'), 'show') (('so we', 'probably them'), 'be') (('what', 'Feature only best features'), 'go') (('they', 'very models'), 'see') (('s', 'ensambles'), 'let') (('s', 'more insights'), 'be') (('lot we', 'finally predictions'), 'make') (('hot encodingFinally we', 'one hot encoding'), 'use') (('s', 'feature when full set'), 'let') (('Sex We', 'numerical column'), 'need') (('Embarked s', 'embarked'), 'let') (('Title', 'specially Master categories'), 'have') (('Hot One Encoding', 'MinMax'), 'scale') (('We', 'graph'), 'want') (('Indeed higher rates', 'size'), 'be') (('AgeWe', 'NaN also ages'), 'need') (('we', 'group'), 'go') (('Sex', 'very good models'), 'consider') (('it', 'just binary value'), 'think') (('we', 'same group'), 'be') (('Finally we', 'IsChild calculations'), 'need') (('Here list', 'them'), 'be') (('people', 'survival alone lower rate'), 'see') (('this', '4'), 'see') (('that', 'many people'), 'see') (('that', 'best results'), 'try') (('it', 'look'), 'have') (('it', 'also bad feature'), 'go') (('we', 'features'), 'create') (('two ladies', 'city'), 'try') (('we', 'girls'), 'see') (('he', 'Dr.'), 'embark') (('Now s', 'just third class'), 'let') (('that', 'same ticket'), 'have') (('s', 'them'), 'let') (('that', 'family'), 'classify') (('we', 'hyperparameters'), 'let') (('s', 'time'), 'see') (('MoreCabin we', '3 more than one cabin'), 'verus') (('numberial feature', 'equal 1'), 'mean') (('didn t', 'same ticket'), 's') (('group', '0'), 'have') (('we', 'changes'), 'let') (('XGB', 'also very best score'), 'get') (('Therefore it', 'metric also deceased'), 'survive') (('that', 'those'), 's') (('therefore it', 'mainly that'), 's') (('we', 'sorting metric'), 'check') (('already person', 'that'), 'be') (('we', 'information'), 'be') (('base score', 'actually more features'), 'test') (('type', 'really influence'), 'see') (('that', 'us'), 'let') (('That', 'almost all tree based models'), 's') (('doesn t', 'much impact'), 'be') (('s', 'model'), 'let') (('RFE', 'step'), 'use') (('s', 'S.'), 'let') (('Also we', 'training set'), 'need') (('we', 'Survived'), 'feature') (('Pclass_2', 'perceptron'), 'agree') (('s', 'groups'), 'feature') (('we', 'models'), 'let') (('Non many people', 'different single name'), 'let') (('so we', 'FamilySize'), 'feature') (('they', 'don NaNs beucase'), 'look') (('SplitValues', 'city C'), 'see') (('Family types', 'other types'), 'have') (('I', 'analysis'), 'try') (('Finally CabinLetter_B', 'median features'), 'list') (('Pclass_2', 'high classes'), 's') (('very we', 'don really feature'), 'see') (('Group ID Group Type InconsistenciesWe', 'cases'), 'need') (('Titanic Competition', 'Kaggle https www'), 'be') (('we', 'Parch'), 'seem') (('Now we', 'case'), 'let') (('best', 'a.'), 'need') (('together Now s', 'survivors'), 'let') (('others', 'numbers'), 'be') (('I', 'don what'), 'be') (('s', 'feature selection'), 'elimination') (('very large families', 'also higher changes'), 'have') (('Feature Basic Machine Base finally we', 'Machine Learning'), 'Selection') (('person', 'family test set odds'), 'be') (('Perhaps Pclass', 'other features'), 'have') (('now s', 'three features'), 'let') (('B D', 'survival higher rates'), 'have') (('we', 'instead SmallGroup'), 'see') (('Visualizing', 'data'), 'enjoy') (('tree', 'based better results'), 'list') (('s', 'many categorial variables'), 'let') (('s', 'data'), 'be') (('again features', 'that linear models'), 'see') (('that', 'models'), 'let') (('one', 'C.'), 'see') (('B E', 'future'), 'see') (('we', 'data sets'), 'see') (('columns', 'original format'), 'come') (('it', 'good feature'), 'happen') (('we', 'Total positive predictions'), 'follow') (('Fortunately almost all sets', 'base set'), '96') (('modified dataframe', 'passenger_id'), 'Elias') (('Cabin I', 'letter'), 'LetterFirst') (('we', 'Mr.'), 'see') ", "extra": "['test']", "label": "Perfect_files", "potential_description_queries": ["accident", "accuracy", "adjust", "age", "algorithm", "apply", "auto", "average", "best", "binary", "bit", "board", "cabin", "calculate", "care", "case", "categorical", "category", "cell", "check", "checking", "child", "choose", "city", "classify", "cleaning", "column", "comment", "company", "compare", "competition", "compute", "consider", "content", "correct", "correlation", "correlations", "cost", "could", "count", "create", "creation", "data", "dataframe", "define", "df", "difference", "distribution", "division", "drop", "empty", "end", "equal", "estimator", "evaluate", "even", "every", "everyone", "everything", "extract", "fact", "family", "fare", "feature", "figure", "fill", "filter", "final", "find", "fix", "fixed", "found", "function", "grade", "graph", "group", "hand", "head", "help", "high", "hope", "hot", "hyperparameter", "id", "idea", "image", "import", "importance", "index", "indicate", "info", "instance", "integer", "kid", "lead", "learn", "least", "leave", "left", "let", "letter", "linear", "list", "look", "looking", "loop", "lot", "lower", "main", "majority", "male", "market", "maximum", "mean", "measure", "median", "metric", "might", "missing", "mistake", "mixed", "ml", "model", "most", "my", "need", "new", "no", "non", "none", "normal", "not", "notebook", "null", "number", "numerical", "object", "option", "order", "out", "overall", "overfitting", "parameter", "part", "pattern", "people", "perform", "person", "plot", "positive", "pre", "predict", "prediction", "print", "printing", "processing", "rank", "rare", "re", "read", "reading", "reason", "remove", "research", "reset", "right", "row", "run", "save", "saving", "scale", "score", "search", "second", "section", "select", "selected", "set", "several", "sex", "shape", "sibling", "similar", "single", "situation", "size", "something", "space", "split", "start", "step", "string", "student", "subset", "survival", "survived", "target", "team", "test", "text", "think", "those", "ticket", "time", "titanic", "title", "total", "train", "training", "transform", "tree", "trend", "try", "tune", "tuning", "type", "under", "unique", "up", "value", "variable", "version", "view", "visualize", "while", "who", "work", "workflow", "worst"], "potential_description_queries_len": 229, "potential_script_queries": ["numpy", "seaborn", "xgboost"], "potential_script_queries_len": 3, "potential_entities_queries": ["binary", "hot", "lower", "positive", "single", "tree"], "potential_entities_queries_len": 6, "potential_extra_queries": [], "potential_extra_queries_len": 0, "all_components_potential_queries_len": 232}