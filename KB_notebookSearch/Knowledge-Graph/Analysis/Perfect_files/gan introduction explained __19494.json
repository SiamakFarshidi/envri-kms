{"name": "gan introduction explained ", "full_name": " h1 Introduction to Generative Adversarial Networks GANs h1 Application h3 Image Generation h3 Image to Image Translation h3 Text to Image Translation h3 Image Manipulation h3 Section Summary h1 Concept h3 What is a GAN h3 Why GAN works h3 Objective function h1 Algorithm h1 Architecture h1 Evaluation h1 DCGAN Code Practice ", "stargazers_count": 0, "forks_count": 0, "description": "To note CycleGAN does not require these two datasets to be paired. IS X_g exp left mathbb E _ x_g sim X_g left D_ textrm KL left p left y x_g right parallel p left y right right right right where x_g sim X_g indicates that x_g is an image sampled from X_g D_ textrm KL P parallel Q is the KL divergence between the distributions P and Q p y x_g is the conditional class distribution and p y mathbb E _ x_g sim X_g left p y x_g right is the marginal class distribution. Similarly we can scratch an objective function for G max J G mathbb E _ x_g sim X_g textrm score x_g tag 4 where textrm score still denotes the D function x_g is sampled from the generated distribution X_g. 3 back propagating gradients to update the parameters in Discriminator theta_D. As we have trained D to classify real fake images. when t 1 loss_mae t a e. InceptionV3 network is trained on ImageNet and used to predict the class of given images. These two models compete with each other and eventually reach to a Nash equilibrium where both G and D cannot get better or worse any more. For example in SummerWinter translation we need to collect photos of the same scene in different seasons. Thus D x_g is close to 0 log 1 D x_g will saturate to 0. ii There is no weight and no trainable parameters introduced by this operation. 3 CE loss can mitigate the gradient vanishing problem when we apply the sigmoid to the output layer. Intuitively it would be better to set a large learning rate at the early stage of gradient descent while a small learning rate when closing to the optima. In practice we can only train these two models alternatively. D_ textrm KL calculation will output a small number. Then we feed half batch of generated images x_g and half batch of real images x_r into the D from previous iteration. 9 trainRatio 5 Models Setup Build Generator with convolution layer remove padding same to scale 6x6 up to 7x7 padding same Build Discriminator with convolution layer 28 x 28 x channel 14 x 14 x 32 7 x 7 x 64 3 x 3 x 128 GAN Part Build our GAN Get the batch size training train_ratio times on D while training once on G Get the latent vector Generate fake images from the latent vector Get the logits for the fake images Get the logits for real images Calculate loss of D Get the gradients w. The following equation is a typical binary cross entropy BCE loss. when t 1 loss_ce t log a dL dw t 1 a da dw log a t 1 a a 1 a dn dw t 1 a p e p. Generator Upsampling networks. Actually Discriminator D is like a binary classifier to distinguish real or fake. With the sigmoid function the output denotes the probability of real D x p real. 5a as Saturating loss function. Image ManipulationThere are many other applications in image manipulation. The later one can also be reformulated as 1 D x_f rightarrow 1. 9 are recommended Define the loss functions to be used for discrimiator Define the loss functions to be used for generator Start Training Plot save generated images through training display original display generation. ii It cannot be deep and is not good at extracting features. CycleGAN is one representative unsupervised GAN with such functionality. For the following discussion we use sigmoid output in D by default. Generator Generator G is going to generate real like images to fool D. b The loss function Eq. As discussed in Chapter 20 of Deep Learning there are many other generative models such as Restricted Boltzmann Machine RBM Generative Stochastic Networks GSNs and Variational auto encoder VAE. After that we complete one training iteration for GAN. The cross entropy of the distribution q relative to a distribution p over a given set is defined as follows CE p q mathbb E _p log q Let s apply the CE loss into the loss function for D. A model can generate new data samples when it learns the whole distribution. However the objective J D rightarrow infty is not applicable in optimization. 2a is equivalent to minimizing the negative objective function which is called loss function Eq. First we feed noise vectors z into the G from previous iteration to generate images x_g G z. Next we freeze theta_D and going to update theta_G. In other words we feed a noise vector z sim N 0 1 into the G function and get a generated image x_g G z. ii It cannot be deep and is not good at generating features. If the softmax output is sharp p textrm one class rightarrow 1 p textrm other classes rightarrow 0 the IS will be small and the quality of this image is high. In a typical GAN the Criminal is named as Generator G while the Investigator is Discriminator D. In other words GAN is the most powerful image generative model. when t 1 dL dw 1 da dw 1 a 1 a dn dw 1 a 1 a p 1 e e p Load Packages tf. Architecture Discriminator Downsampling networks Input images mapsto Output probability Dense Fully connected layer i Decreasing number of neurons for layers. Then we can sample any random points from this distribution as the generated data. If we input a high quality image which cannot be classified into ImageNet classes to InceptionV3 network then the softmax output will not be a sharp value. Introduction to Generative Adversarial Networks GANs In this notebook we will be familiar with an advanced deep learning technique named Generative Adversarial Networks GANs. GAN technique is going to let the machine learn to generate new stuff. Investigator Found the new fake dollars and successfully figured all 2nd version fake dollars out. 1 scratched at the beginning. 3 Cross entropy is a divergence to measure the difference between two distributions. Up to now we only update D in current iteration. Objective function Discriminator Discriminator D is going to distinguish all fake images. We hope the output probability of real images close to 1 D x_r rightarrow 1 while the probability of fake images close to 0 D x_f rightarrow 0. 3 together the aggregate objective function for GAN L GAN can be derived as textrm minimax underset theta_G min underset theta_D max L GAN mathbb E _ x_r sim X_r log D x_r mathbb E _ x_g sim X_g log 1 D x_g tag 6. 5a and L D of Eq. com hindupuravinash the gan zoo where includes hundreds variants of GAN. stride 2 Rightarrow moving step frac 1 2. Fr\u00e8chet Inception Distance IS totally depends on the InceptionV3 knowledge on ImageNet. t the generator loss Update the weights of the generator using the generator optimizer Compile Models Optimizer for both the networks learning_rate 0. Application Image GenerationMost of deep learning techniques we learned previously are used for recognition. Inception Score IS measures the KL divergence similar to cross entropy see Def. In the objective function we add a sigmoid transfer function to the output layer. How to move frac 1 2 step Inserting zero columns and rows to the original input. begin align texttt Maxpooling2D 2 2 begin bmatrix 1 2 3 4 5 6 7 8 8 7 4 3 6 5 2 1 end bmatrix mapsto begin bmatrix 6 8 8 4 end bmatrix end align Convolution2D stride 2. a Leftrightarrow min L D mathbb E _ x_r sim X_r D x_r mathbb E _ x_f sim X_f 1 D x_f tag 2. sigmoid x equiv dfrac 1 1 e x Discriminator with softmax sigmoid output. In binary classification these two methods have the same effect. Photo inpainting filling the missing area in a given image. The objective of G is to make D classify the generated images into real type. 2b is still based on mean absolute error. In each training iteration we train D and G sequentially. In 2016 Yann LeCun Facebook AI research director described GAN as the most interesting idea in the last 10 years in Machine Learning. n th version fake dollars. 4 textrm Saturating underset theta_G min L G mathbb E _ x_g sim X_g log 1 D x_g tag 5. DCGAN Code Practice a sigmoid wp b error t a loss_mse e 2 dL dw 2 e de dw 2 e 1 da dw 2 e 1 a a dn dw 2 e 1 a a p 2 e e 1 e p. To note the parameters in Generator theta_G is frozen. For the continuous case P x Q x are probability density functions. Criminal Produced 2nd version fake dollars and the investigator cannot figure them out. How to be a master of producing fake dollars Criminal Produced 1st version fake dollars and the investigator cannot figure them out. Then we feed a batch of generated images x_g into the updated D. max J D mathbb E _ x_r sim X_r D x_r mathbb E _ x_f sim X_f 1 D x_f tag 2. ii Similar to convolution operation these weights become trainable parameters in your model. StackGAN is a conditional GAN which can generate images based on text description. i The output is just selecting the maximum input value within the height width window of input values. 5b back propagating gradients to update the parameters in Generator theta_G. i It is also named as fractional strided convolution. begin align texttt Upsampling2D 2 2 begin bmatrix 1 2 3 4 end bmatrix mapsto begin bmatrix 1 1 2 2 1 1 2 2 3 3 4 4 3 3 4 4 end bmatrix end align ConvTranspose2D Deconvolution2D stride 2. Besides feature level information far surpasses classification level information. However we cannot optimize this combined loss function by changing G and D simultaneously. 4 textrm Non Saturating underset theta_G min L G mathbb E _ x_g sim X_g log D x_g tag 5. Then we modify the objective function Eq. GAN applies a zero sum game to help G to simulate the real distribution. For notation consistency we replace x_f X_f by x_g X_g because generated images x_g are exactly identified as fake images x_f by D. Both of these two measurements are based on the Inception V3 network which is pretrained on ImageNet dataset. mathbb E _ x_r sim X_r log D x_r is minimizing the divergence between real distribution X_r in the training set and the probability distribution defined by D model and mathbb E _ x_f sim X_f log 1 D x_f is maximizing the divergence between generated distribution X_f and the probability distribution defined by D model. begin align D_ textrm KL P parallel Q sum_ x in mathcal X P x log left frac P x Q x right underbrace left sum_ x in mathcal X P x log Q x right _ textrm Cross entropy P Q underbrace left sum_ x in mathcal X P x log P x right _ textrm Cross entropy P P end align This equation is a discrete case of KL divergence where P and Q are two probability distributions mathcal X is the probability space. For example mean squared error is the cross entropy between the empirical distribution and a Gaussian model. Investigator An expert in figuring fake dollars. ii There is no weight and no trainable parameters introduced by this operation. CE loss the norm of gradient nabla is positively related to the error a large error leads to a fast learning speed. However paired datasets are very expensive and often not available. If domain X is a text data distribution GAN is going to generate images based on the input text query. different style images lower resolution images or masked images GAN is going to transform input images X target images Y. Although it is an infant technique there are bunch of models proposed with the suffix GAN such as ACGAN DCGAN WGAN BEGAN CycleGAN StackGAN. The exp in the expression is there to make the values easier to compare so it will be ignored if we use ln IS without loss of generality. D is maximizing mathbb E _ x_g sim X_g log 1 D x_g and G can minimizing this term to compete with D. t the discriminator loss Update the weights of the discriminator using the discriminator optimizer Get the latent vector Generate fake images using the generator Get the discriminator logits for fake images Calculate the generator loss Get the gradients w. AlgorithmIf we combine L G of Eq. Besides the diversity of softmax output indicates the diversity of images. Generally we measure the generated images in two dimensions quality of images and diversity of images. Intuitively we can scratch an objective function of D max J D mathbb E _ x_r sim X_r textrm score x_r mathbb E _ x_f sim X_f textrm score x_f tag 1 where x_r is sampled from the real distribution X_r x_f is sampled from the fake distribution X_f textrm score denotes an evaluation function which gives high scores on real images and low scores on fake images. Generation is always harder than recognition. We only focus on two typical GANs original GANs and Deep Convolutional GAN DCGAN. a Later Ian Goodfellow proposed a more stable and efficient loss function for G. Section SummaryThe underlying functionality of GAN is transforming data distribution from domain X to domain Y. To note GAN does not simply memorize the given dataset. ii These weights become trainable parameters in your model. In mathematical statistics the Kullback Leibler KL divergence also called relative entropy is a measure of how one probability distribution is different from a second reference probability distribution. b In practice Non Saturating L G is better than Saturating L G. FID Vert mu_r mu_g Vert 2 Tr left Sigma_r Sigma_g 2 left Sigma_r Sigma_g right 1 2 right where mu_r is the mean of the real features mu_g is the mean of the generated features Sigma_r is the covariance matrix of the real features Sigma_g is the covariance matrix of the generated features. theta_D denotes the parameters in D model which are the weights in neural networks. Page 132 Deep Learning 2016The nature of Eq. begin align BCE mathbb E _ x_r sim X_r left begin bmatrix 1 0 end bmatrix begin bmatrix log D x_r log 1 D x_r end bmatrix right mathbb E _ x_f sim X_f left begin bmatrix 0 1 end bmatrix begin bmatrix log D x_f log 1 D x_f end bmatrix right end align With algebraic operations the loss function for D can be derived as below. Generative Adversarial Networks GANs is a cutting edge technique of deep neural networks which was first come up by Ian Goodfellow in 2014. This notebook contains background application concept algorithm architecture evaluation and a code demo of typical GANs. For example we first learned how to recognize digits like 0 9 then we tried to mimic the shape of digits and created digits in our styles which is called generation. Investigator Found the new fake dollars and successfully figured all 1st version fake dollars out. Input noise vectors mapsto Output images Dense Fully connected layer i Increasing number of neurons for layers. Our goal is to find the optimal weights to minimize the following BCE loss function. Image to Image TranslationGAN can learn the features of two image collections and translate one images from one domain to another. i The height width window of output values is just repeating the corresponding input value. As for the loss functions in classification we often prefer to use cross entropy CE loss rather than mean absolute error MAE L1 loss and mean squared error MSE L2 loss. For fake images p is 0 1 T q is D x_f 1 D x_f T. This method is exactly a minimax game and is proposed in the original GAN paper. In comparison FID is a more general evaluation and applicable to new datasets. Concept What is a GAN It is like a zero sum game in Game Theory Example 1. IS is derived from the classification output while FID is derived from the feature layer. 3 we also use CE loss on the generated images. Rightarrow underset theta_D min L D mathbb E _ x_r sim X_r log D x_r mathbb E _ x_g sim X_g log 1 D x_g tag 3 Any loss consisting of a negative log likelihood is a cross entropy between the empirical distribution defined by the training set and the probability distribution defined by model. Super resolution recovering the photo realistic texture for the low resolution image. If the output D x is a real number we hope D x_r rightarrow infty and D x_f rightarrow infty to achieve the maximum J D. Besides maximizing the objective function Eq. We can calculate the CE of generated images independently p is 1 0 T q is D x_g 1 D x_g T. Stride here is the reciprocal of the moving step. For example in the early learning step D can easily distinguish generated images and real images. Especially in the recent two years GAN was developed with an exponential increment Fig. GAN is a very new stuff and has a promising future. Without paired datasets CycleGAN can translate landscape photos into a particular painting style such as Monet Van Gogh. For real images p is 1 0 T q is D x_r 1 D x_r T. Corresponding to L D Eq. Apart from comparing the generated images with real images by our eyes there are two common mathematical metrics to evaluate the quality of the generated images Inception Score IS and Fr\u00e8chet Inception Distance FID. Text to Image Translation If we embed text information as the label of corresponding images GAN will learn the mapping between sentiment vectors and image features. However Saturating loss function cannot provide sufficient gradient for G to learn. With this loss function log D x_g is very large and non saturating at the early learning period. Besides it can also translate zebras into horses. MAE MSE loss gradient vanishing when the error is large. If domain X is another image distribution e. There is a trick that L G can simply borrow the second term from L D. After we learn the GAN technique you are highly encouraged to propose some creative ideas on further applications. Similarly we feed noise vectors z into the G from previous iteration to generate images x_g G z. One becomes better means the opponent must be worse. With the loss function Eq. Comparing with these generative models GAN has the state of the art performance in the filed of image generation. 2 between the generated sample distribution and the ImageNet distribution whereas FID calculates the feature level distance between the generated sample distribution and the real sample distribution. Here we use G to map a normal distribution with the generated distribution N 0 1 overset G mapsto X_g. i The output is a linear combination of the input values times a weight for each cell in the height width kernel filter. Criminal An expert in making fake dollars. In other words IS can only evaluate the generated images which should belong to ImageNet dataset. 0 Fix Seeds Data Preparation change as channel last n dim dim channel Set channel It is suggested to use 1 1 input for GAN training Get image size Get number of classes Hyperparameters optimizer Adam lr 0. Algorithm 1 Training Discriminator. There is a website called The GAN Zoo https github. If we define domain X as a normal noise distribution GAN is going to generate images. 5b as Non Saturating loss function. EvaluationGAN is a very new topic it is still an open problem to find a perfect evaluation of GAN or generated images. In this game G is trying to generate real like images to fool D while D is trying to figure all fake images out. a It is the objective function of original GAN with a minimax game. Thus we prefer to map the range to probability space infty infty mapsto 0 1. Let s replace the textrm score function by D. Why GAN works If a model only learns the data points rather than the whole distribution this model can only memorize the dataset. ", "id": "together/gan-introduction-explained", "size": "19494", "language": "python", "html_url": "https://www.kaggle.com/code/together/gan-introduction-explained", "git_url": "https://www.kaggle.com/code/together/gan-introduction-explained", "script": "discriminator_loss __init__ tensorflow.keras.initializers Reshape tensorflow.keras.optimizers tensorflow.keras.layers generator_conv Dropout Sequential change_image_shape Adam tensorflow.keras.datasets.fashion_mnist numpy glorot_normal Input compile matplotlib.pyplot DCGAN(Model) Dense tensorflow generator_loss tensorflow.keras show_img Model plt_img load_data discriminator_conv matplotlib.image \\ train_step ", "entities": "(('quality', 'image'), 'be') (('Image ManipulationThere', 'image many other manipulation'), 'be') (('D', 'real type'), 'be') (('GAN', 'real distribution'), 'apply') (('Then we', 'previous iteration'), 'feed') (('probability relative how one distribution', 'reference probability second distribution'), 'be') (('class p y mathbb E _ p conditional X_g left y right', 'KL distributions'), 'leave') (('Generally we', 'images'), 'measure') (('It', 'minimax game'), 'be') (('First we', 'images'), 'feed') (('together aggregate objective function', 'L GAN E _ mathbb E _ log'), 'derive') (('Stride', 'here moving step'), 'be') (('loss functions', 'display original generation'), 'recommend') (('network', 'given images'), 'InceptionV3') (('We', 'close 0'), 'hope') (('we', 'real fake images'), 'train') (('GAN', 'images'), 'go') (('method', 'minimax GAN exactly original paper'), 'be') (('weights', 'trainable model'), 'become') (('Then we', 'generated data'), 'sample') (('FID', 'feature layer'), 'derive') (('Next we', 'theta_G.'), 'freeze') (('GAN', 'other words'), 'be') (('Then we', 'objective function'), 'modify') (('G', 'sufficient gradient'), 'provide') (('rather whole model', 'only dataset'), 'work') (('WGAN', 'ACGAN such DCGAN'), 'be') (('softmax then output', 'ImageNet classes'), 'input') (('we', 'error MAE L1 error MSE L2 rather absolute loss squared loss'), 'prefer') (('Investigator', 'Generator G'), 'name') (('Then we', 'updated D.'), 'feed') (('conditional which', 'text description'), 'be') (('we', 'different seasons'), 'need') (('height width window', 'input just corresponding value'), 'repeat') (('bmatrix 1 2 3 4 mapsto', 'bmatrix'), 'begin') (('D', 'fake images'), 'try') (('feature level information', 'classification level far information'), 'surpass') (('It', 'Game Theory Example'), 'concept') (('which', 'ImageNet dataset'), 'base') (('GAN', 'sentiment vectors'), 'text') (('T 0 1 q', 'images fake p'), 'be') (('when it', 'whole distribution'), 'generate') (('We', 'only two typical GANs original GANs'), 'focus') (('Cross 3 entropy', 'two distributions'), 'be') (('Calculate loss', 'gradients'), 'Build') (('textrm 4 where score', 'generated distribution'), 'scratch') (('when we', 'output layer'), 'mitigate') (('Investigator', 'successfully 2nd version fake dollars'), 'find') (('stride 2 Rightarrow', 'step'), 'move') (('However we', 'G'), 'optimize') (('FID', 'more general new datasets'), 'be') (('two methods', 'same effect'), 'have') (('Thus we', 'infty mapsto'), 'prefer') (('which', 'styles'), 'learn') (('we', 'generality'), 'be') (('s', 'D.'), 'define') (('we', 'image G generated z.'), 'feed') (('Criminal Produced version 1st fake dollars', 'them'), 'be') (('function Discriminator Discriminator Objective D', 'fake images'), 'go') (('we', 'default'), 'use') (('images', 'layers'), 'network') (('rightarrow', 'J maximum D.'), 'be') (('we', 'GAN'), 'complete') (('CycleGAN', 'such functionality'), 'be') (('goal', 'BCE loss following function'), 'be') (('note', 'two datasets'), 'require') (('where G', 'Nash eventually equilibrium'), 'compete') (('masked GAN', 'X target Y.'), 'low') (('Criminal Produced version 2nd fake dollars', 'them'), 'figure') (('text data GAN', 'input text query'), 'go') (('Discriminator Actually D', 'real'), 'be') (('It', 'extracting features'), 'ii') (('output', 'height width kernel filter'), 'be') (('we', 'D'), 'train') (('_ textrm KL D calculation', 'small number'), 'output') (('T 1 0 q', 'images real p'), 'be') (('Inception Score', 'generated images'), 'be') (('s', 'D.'), 'let') (('generator loss', 'networks'), 'update') (('Ian Later Goodfellow', 'G.'), 'propose') (('we', 'learning advanced deep technique'), 'introduction') (('which', 'neural networks'), 'denote') (('1 Thus close to 0 D', '0'), 'be') (('2b', 'still mean absolute error'), 'base') (('underlying functionality', 'Y.'), 'section') (('which', 'ImageNet dataset'), 'evaluate') (('Besides it', 'horses'), 'translate') (('Similarly we', 'G z.'), 'feed') (('rightarrow infty', 'optimization'), 'be') (('GAN', 'image generation'), 'have') (('EvaluationGAN very new it', 'GAN generated images'), 'be') (('3 loss', 'probability model'), 'underset') (('Image', 'another'), 'learn') (('noise vectors Output Dense Fully connected i', 'layers'), 'mapsto') (('It', 'also fractional strided convolution'), 'name') (('squared error', 'cross empirical distribution'), 'be') (('Load Packages', 'dn e e 1 a 1 1 a a p 1 p'), 'when') (('we', 'current iteration'), 'update') (('1 x_f', 'probability D model'), 'sim') (('which', '2014'), 'be') (('loss function', 'log learning D very large early period'), 'be') (('probability two mathcal X', 'KL discrete divergence'), 'begin') (('KL divergence', 'similar entropy Def'), 'be') (('It', 'classes'), 'change') (('Investigator', 'successfully 1st version fake dollars'), 'find') (('Generator Generator G', 'D.'), 'go') (('It', 'generating features'), 'ii') (('GAN', 'simply given dataset'), 'memorize') (('1 da', 't 1'), 'log') (('which', 'low fake images'), 'scratch') (('which', 'negative objective function'), 'be') (('note', 'Generator theta_G'), 'freeze') (('notebook', 'code typical GANs'), 'contain') (('loss function', 'D'), 'begin') (('we', 'only two models'), 'train') (('Intuitively it', 'when optima'), 'be') (('ImageNet FID', 'sample generated distribution'), '2') (('you', 'further applications'), 'learn') (('output', 'input values'), 'select') (('X_g 1 G', 'D.'), 'maximize') (('Here we', 'generated distribution'), 'use') (('GAN', 'increment exponential Fig'), 'develop') (('Non Saturating L G', 'Saturating L G.'), 'b') (('we', 'previously recognition'), 'use') (('we', 'output layer'), 'add') (('align texttt 2 end bmatrix Maxpooling2D 1 2 4 5 8 8 7 4 6 5 2 1 mapsto', 'bmatrix'), 'begin') (('Yann LeCun Facebook AI research director', 'Machine Learning'), 'describe') (('output', 'D p real real'), 'denote') (('T independently p 1 0 q', 'generated images'), 'calculate') (('machine', 'new stuff'), 'go') (('P x Q x', 'continuous case'), 'be') (('learning step early D', 'easily generated images'), 'distinguish') (('L G', 'L D.'), 'be') (('GAN', 'very new promising future'), 'be') (('3 we', 'generated images'), 'use') (('large error', 'learning fast speed'), 'lead') (('covariance matrix', 'covariance generated features'), 'Vert') (('generated images', 'D.'), 'replace') (('CycleGAN', 'Monet Van such Gogh'), 'translate') (('discriminator logits', 'gradients'), 'update') (('Generation', 'always recognition'), 'be') (('Fr\u00e8chet Inception Distance', 'ImageNet'), 'depend') ", "extra": "[]"}