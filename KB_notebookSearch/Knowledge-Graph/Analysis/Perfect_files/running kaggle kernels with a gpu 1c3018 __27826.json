{"name": "running kaggle kernels with a gpu 1c3018 ", "full_name": " h1 Project understanding\u4e86\u89e3\u9879\u76ee h3 What exactly is the problem \u95ee\u9898\u662f\u4ec0\u4e48 h3 How would a solution look like \u6211\u4eec\u671f\u671b\u7684\u89e3\u51b3\u65b9\u6848 h3 What is known about the domain \u4e13\u4e1a\u9886\u57df\u77e5\u8bc6 h1 Data understanding\u4e86\u89e3\u6570\u636e h3 What data do we have available \u6211\u4eec\u6709\u54ea\u4e9b\u6570\u636e h3 Is the data relevant to the problem \u6570\u636e\u662f\u5426\u4e0e\u95ee\u9898\u76f8\u5173 h3 Is it valid Does it reflect our expectations \u6709\u6548\u5417 \u5b83\u662f\u5426\u53cd\u6620\u4e86\u6211\u4eec\u7684\u671f\u671b h3 Is the data quality quantity recency sufficient \u6570\u636e\u8d28\u91cf \u6570\u91cf \u65b0\u8fd1\u5ea6\u662f\u5426\u8db3\u591f h1 Data visualization h3 Plot some images with and without cancer tissue for comparison \u753b\u51fa\u4e00\u4e9b\u6709\u764c\u7ec4\u7ec7\u548c\u6ca1\u6709\u764c\u7ec4\u7ec7\u7684\u56fe\u50cf\u8fdb\u884c\u6bd4\u8f83 h3 How is the data best transformed for modeling \u5982\u4f55\u6700\u597d\u5730\u4e3a\u5efa\u6a21\u8f6c\u6362\u6570\u636e h3 How may we increase the data quality \u6211\u4eec\u5982\u4f55\u63d0\u9ad8\u6570\u636e\u8d28\u91cf h3 Preprocessing and augmentation \u9884\u5904\u7406\u548c\u6570\u636e\u589e\u5f3a h3 Compute image statistics\u8ba1\u7b97\u56fe\u50cf\u6570\u636e\u7684\u7edf\u8ba1\u4fe1\u606f h3 Plot some of the very bright or very dark images\u753b\u51fa\u4e00\u4e9b\u592a\u4eae\u548c\u592a\u6697\u7684\u56fe\u7247 h1 Baseline model\u57fa\u7ebf\u6a21\u578b Fastai v1 h3 Prepare the data and split train\u51c6\u5907\u6570\u636e\u548c\u62c6\u5206\u8bad\u7ec3\u96c6 h3 What kind of model architecture suits the problem best \u4ec0\u4e48\u6837\u7684\u6a21\u578b\u67b6\u6784\u6700\u9002\u5408\u8fd9\u4e2a\u95ee\u9898 h3 Training \u8bad\u7ec3 h3 1cycle policy 1cycle\u7b56\u7565 h2 Is the model learning \u6211\u4eec\u7684\u6a21\u578b\u5b66\u4e60\u4e86\u5417 h1 Finetuning the baseline model \u5fae\u8c03\u57fa\u7ebf\u6a21\u578b h1 Validation and analysis \u9a8c\u8bc1\u548c\u5206\u6790 h3 How good does the model perform technically \u8be5\u6a21\u578b\u5728\u6280\u672f\u4e0a\u6709\u591a\u597d h3 How good is the model in terms of project requirements \u6a21\u578b\u5728\u9879\u76ee\u8981\u6c42\u65b9\u9762\u6709\u591a\u597d h3 Gradient weighted Class Activation Mapping Grad CAM \u52a0\u6743\u68af\u5ea6\u7c7b\u6fc0\u6d3b\u6620\u5c04 \u4e00\u79cd\u53ef\u89c6\u5316\u89e3\u91ca\u57fa\u4e8e\u68af\u5ea6\u5b9a\u4f4d\u7684\u6df1\u5ea6\u7f51\u7edc h3 ROC curve and AUC ROC\u66f2\u7ebf\u548c\u66f2\u7ebf\u4e0b\u9762\u79efAUC h1 Submit predictions\u63d0\u4ea4\u9884\u6d4b\u6587\u4ef6 h3 TTA\u6d4b\u8bd5\u65f6\u589e\u5f3a test time augmentation h3 Submit the model for evaluation\u63d0\u4ea4 h1 \u90e8\u7f72Deploy \u6216\u6d4b\u8bd5example ", "stargazers_count": 0, "forks_count": 0, "description": "Visualizing the localization map would reveal that and we could focus on getting more diverse data of that breed. com gigascience article 7 6 giy065 5026175 Is the data quality quantity recency sufficient \u6570\u636e\u8d28\u91cf \u6570\u91cf \u65b0\u8fd1\u5ea6\u662f\u5426\u8db3\u591f All glass slides included in the CAMELYON dataset were part of routine clinical care and are thus of diagnostic quality. value_counts \u7edf\u8ba1data label \u8fd9\u4e2aseries\u7684\u6bcf\u79cd\u503c\u7684\u4e2a\u6570 OpenCV reads the image in bgr format by default OpenCV\u9ed8\u8ba4\u4ee5bgr\u683c\u5f0f\u8bfb\u53d6\u56fe\u50cf path\u662f\u56fe\u50cf\u7684\u5730\u5740 We flip it to rgb for visualization purposes \u6211\u4eec\u5c06\u56fe\u7247\u7ffb\u8f6c\u4e3argb\u683c\u5f0f\u7528\u4e8e\u89c2\u770b cv2. ai see commit version 9 of this kernel. \u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5b66\u4e60\u7387\u4ece\u8f83\u4f4e\u7684\u5f00\u59cb\u5230\u8fbe\u4e2d\u95f4\u7684 \u6700\u5927\u5b66\u4e60\u7387 \u7136\u540e\u5b83\u5728\u63a5\u8fd1\u5c3e\u58f0\u65f6\u6162\u6162\u51cf\u5c0f \u6211\u4eec\u7684\u60f3\u6cd5\u662f \u6211\u4eec\u4ece\u4f4e\u7684\u70ed\u8eab\u5b66\u4e60\u7387\u5f00\u59cb \u7136\u540e\u9010\u6e10\u5c06\u5176\u63d0\u9ad8\u5230\u9ad8\u503c \u8f83\u9ad8\u7684\u5b66\u4e60\u7387\u5177\u6709\u6b63\u5219\u5316\u6548\u5e94 \u56e0\u4e3a\u5b83\u4e0d\u5141\u8bb8\u6a21\u578b\u9002\u5e94\u5c16\u9510\u548c\u72ed\u7a84\u7684\u5c40\u90e8\u6700\u5c0f\u503c \u4f46\u8d8b\u5411\u4e8e\u66f4\u5bbd\u548c\u66f4\u7a33\u5b9a\u7684\u5730\u65b9 In the middle of our cycle we start to lower the learning rate as we are hopefully in a good stable area. \u6211\u4eec\u77e5\u9053\u56fe\u50cf\u7684\u6807\u7b7e\u4ec5\u53d7\u4e2d\u5fc3\u533a\u57df 32 x 32px \u7684\u5f71\u54cd \u56e0\u6b64\u5c06\u6570\u636e\u88c1\u526a\u5230\u8be5\u533a\u57df\u662f\u6709\u610f\u4e49\u7684 \u4f46\u662f \u5982\u679c\u6211\u4eec\u6536\u5f97\u592a\u8fd1 \u4e00\u4e9b\u5173\u4e8e\u5468\u56f4\u73af\u5883\u7684\u6709\u7528\u4fe1\u606f\u53ef\u80fd\u4f1a\u4e22\u5931 \u8fd9\u79cd\u5047\u8bbe\u53ef\u4ee5\u901a\u8fc7\u5177\u6709\u4e0d\u540c\u5207\u5272\u5927\u5c0f\u7684\u8bad\u7ec3\u6a21\u578b\u6765\u786e\u8ba4 \u6211\u7684\u521d\u59cb\u7ed3\u679c\u662f32 x 32px\u5c3a\u5bf8\u8868\u73b0\u51fa\u6bd448 x 48px\u66f4\u5dee\u7684\u6027\u80fd \u4f46\u6211\u8fd8\u6ca1\u6709\u5bfb\u627e\u6700\u4f73\u5c3a\u5bf8 How may we increase the data quality \u6211\u4eec\u5982\u4f55\u63d0\u9ad8\u6570\u636e\u8d28\u91cf We could inspect if the data contains bad data too unfocused or corrupted and remove those to increase the overall quality. org Background CAMELYON17 background https camelyon17. As we have test time augmentation our results will probably improve if we do predictions multiple times per image and average out the results. Note that the plot above may differ between runs as we use a random subset of data for computations. html Images \u8c03\u7528\u4e0a\u9762\u5b9a\u4e49\u7684\u51fd\u6570 \u751f\u6210\u4e86\u4e00\u4e2aImage\u5bf9\u8c61 https docs. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u51e0\u79cd\u65b9\u6cd5\u6765\u907f\u514d\u8fc7\u5ea6\u62df\u5408 \u66f4\u591a\u6570\u636e \u6570\u636e\u589e\u5f3a \u6b63\u5219\u5316\u548c\u4e0d\u592a\u590d\u6742\u7684\u6a21\u578b\u67b6\u6784 \u5728\u8fd9\u91cc \u6211\u4eec\u5c06\u5b9a\u4e49\u56fe\u50cf\u589e\u5f3a \u5e76\u5c06\u5b83\u4eec\u76f4\u63a5\u6dfb\u52a0\u5230\u6211\u4eec\u7684\u56fe\u50cf\u52a0\u8f7d\u51fd\u6570\u4e2d \u8bf7\u6ce8\u610f \u5982\u679c\u6211\u4eec\u5728\u6b64\u5904 \u52a0\u8f7d\u56fe\u50cf\u51fd\u6570 \u5e94\u7528\u6570\u636e\u589e\u5f3a \u5219\u5728\u6211\u4eec\u9884\u6d4b \u63a8\u7406 \u65f6\u4e5f\u4f1a\u5e94\u7528\u6570\u636e\u589e\u5f3a \u8fd9\u79f0\u4e3a\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a TTA \u5982\u679c\u6211\u4eec\u4e3a\u6bcf\u4e2a\u56fe\u50cf\u9884\u6d4b\u591a\u6b21\u5e76\u505a\u5e73\u5747 \u53ef\u4ee5\u6539\u5584\u6211\u4eec\u7684\u7ed3\u679c The augmentations we can use for this type of data \u6211\u4eec\u53ef\u4ee5\u7528\u4e8e\u6b64\u7c7b\u6570\u636e\u7684\u589e\u5f3a random rotation\u968f\u673a\u65cb\u8f6c random crop\u968f\u673a\u526a\u88c1 random flip horizontal and vertical both \u968f\u673a\u7ffb\u8f6c \u6a2a\u5411\u548c\u7eb5\u5411 random lighting\u968f\u673a\u8c03\u6574\u50cf\u7d20\u503c random zoom not implemented here \u968f\u673a\u5c3a\u5bf8\u7f29\u653e Gaussian blur not implemented here \u9ad8\u65af\u566a\u97f3We will use OpenCV with image operations because in my experience OpenCV is a lot faster than PIL or scikit image. \u8be5\u6570\u636e\u96c6\u662f\u5728Radboud\u5927\u5b66\u533b\u5b66\u4e2d\u5fc3 \u8377\u5170\u5948\u6885\u4ea8 \u548c\u4e4c\u5f97\u52d2\u652f\u5927\u5b66\u533b\u5b66\u4e2d\u5fc3 \u8377\u5170\u4e4c\u5f97\u52d2\u652f \u6536\u96c6\u7684\u4e24\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u7684\u7ec4\u5408 \u5e7b\u706f\u7247\u901a\u8fc7\u5e38\u89c4\u4e34\u5e8a\u5b9e\u8df5\u4ea7\u751f \u5e76\u4e14\u8bad\u7ec3\u6709\u7d20\u7684\u75c5\u7406\u5b66\u5bb6\u5c06\u68c0\u67e5\u7528\u4e8e\u8bc6\u522b\u8f6c\u79fb\u764c\u7684\u7c7b\u4f3c\u56fe\u50cf \u4f46\u662f \u8fd9\u4e9b\u5c0f\u5c3a\u5bf8\u56fe\u50cf\u6837\u672c\u53ef\u80fd\u4f1a\u9057\u6f0f\u4e00\u4e9b\u6709\u5173\u5468\u56f4\u73af\u5883\u7684\u76f8\u5173\u4fe1\u606f Is it valid Does it reflect our expectations \u6709\u6548\u5417 \u5b83\u662f\u5426\u53cd\u6620\u4e86\u6211\u4eec\u7684\u671f\u671b According to the data description the dataset has been stripped of duplicates. org tutorials beginner former_torchies nn_tutorial. The idea is that we start with a low warm up learning rate and gradually increase it to high. \u6211\u4eec\u53ea\u80fd\u4ece\u9a8c\u8bc1\u96c6\u4e2d\u770b\u5230\u6307\u6807 \u6700\u7ec8\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6307\u6807\u5f88\u53ef\u80fd\u4f1a\u6709\u6240\u4e0d\u540c \u897f\u74dc\u4e66 image. html image2np \u5728\u7b2c\u4e8c\u884c\u7b2ci\u4e2a\u8f74\u4e0a\u5c55\u793a\u8fd9\u5f20\u56fe\u7247 \u7b2c\u4e8c\u884c\u7b2ci\u4e2a\u8f74\u7684x\u8f74\u6ca1\u6709\u523b\u5ea6 \u7b2c\u4e8c\u884c\u7b2ci\u4e2a\u8f74\u7684y\u8f74\u6ca1\u6709\u523b\u5ea6 \u8bbe\u7f6e\u6807\u9898 \u7ed9\u7b2c\u4e8c\u884c\u7b2c\u4e00\u4e2a\u8bbe\u7f6ey\u8f74\u6807\u7b7e Most correct or least losses \u5c55\u793a\u524d\u56db\u4e2a\u6700\u6b63\u786e \u635f\u5931\u503c\u6700\u5c0f \u7684\u56fe\u4f8b \u53ea\u5c55\u793a\u56db\u5f20\u56fe tl_idx\u7684\u540e\u56db\u4e2a\u662f\u635f\u5931\u503c\u6700\u9ad8\u7684 im \u662f\u56fe\u7247 cl \u662f\u7c7b\u522bclass \u503c\u662f0 1 0\u6307\u4ee3\u8d1f\u4f8b 1\u6307\u4ee3\u80bf\u7624 \u8f6c\u6362\u4e3a\u6574\u6570 \u5c06\u56fe\u7247\u4ecepytorch\u683c\u5f0f\u8f6c\u4e3anumpy\u683c\u5f0f https docs. net qq_38906523 article details 80590314 We will use the one cycle policy proposed by Leslie Smith arXiv April 2018 https arxiv. org dev peps pep 0201 id10 Now we can create a new list with the same order as in sample_submission \u73b0\u5728 \u6211\u4eec\u8981\u521b\u5efa\u4e00\u4e2a\u548c\u63d0\u4ea4\u6a21\u677f\u987a\u5e8f\u76f8\u540c\u7684\u5217\u8868 \u5faa\u73af\u5730\u63d0\u53d6sample_list\u4e2d\u7684id \u627e\u5230\u5176\u5728pred_dice\u5b57\u5178\u4e2d\u5bf9\u5e94\u7684\u503c \u751f\u6210\u4e00\u4e2a\u89c4\u5b9a\u987a\u5e8f\u7684\u9884\u6d4b\u6982\u7387\u5217\u8868 Next a Pandas dataframe with id and label columns. Image\u5728torch\u4e2d\u662f\u5bf9\u6bcf\u4e2a\u56fe\u7247\u8fdb\u884c\u5c01\u88c5 https docs. The visual explanation gives transparency to the model making it easier to notice if it has learned the wrong things. \u5728\u673a\u5668\u5b66\u4e60\u751f\u4ea7\u6d41\u7a0b\u4e2d \u4ece\u4e00\u4e2a\u76f8\u5bf9\u7b80\u5355\u7684\u6a21\u578b\u5f00\u59cb \u6bd4\u5982\u6700\u5c0f\u53ef\u884c\u4ea7\u54c1 MVP \u6216\u57fa\u7ebf\u662f\u4e00\u4e2a\u597d\u4e3b\u610f \u901a\u8fc7MVP \u6211\u4eec\u53ef\u4ee5\u975e\u5e38\u5feb\u901f\u5730\u770b\u5230\u662f\u5426\u5b58\u5728\u4e00\u4e9b\u610f\u60f3\u4e0d\u5230\u7684\u95ee\u9898 \u4f8b\u5982\u7cdf\u7cd5\u7684\u6570\u636e\u8d28\u91cf \u8fd9\u5c06\u4f7f\u5f97\u5bf9\u6a21\u578b\u8c03\u4f18\u7684\u4efb\u4f55\u8fdb\u4e00\u6b65\u6295\u8d44\u90fd\u4e0d\u503c\u5f97 Prepare the data and split train\u51c6\u5907\u6570\u636e\u548c\u62c6\u5206\u8bad\u7ec3\u96c6Split train data to 90 training and 10 validation parts. The PCam dataset is derived from the Camelyon16 Challenge dataset https camelyon16. \u5728\u8fd9\u91cc \u6211\u4eec\u5c06\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5e76\u505a\u8fc1\u79fb\u5b66\u4e60\u6765\u8c03\u6574\u6211\u4eec\u7684\u6743\u91cd \u5bfb\u6c42\u66f4\u6df1\u7684\u6a21\u578b\u67b6\u6784\u5c06\u66f4\u5feb\u5730\u53d1\u751f\u8fc7\u62df\u5408 For differenet pretrained model architectures check Fast. pkl file that you ll need to copy with your model file if you want to deploy it on another device. The diagnostic procedure for pathologists is tedious and time consuming as a large area of tissue has to be examined and small metastases can be easily missed. Tumor tissue in the outer region of the patch does not influence the label. 43\u5fae\u7c73\u7684\u6700\u7ec8\u50cf\u7d20\u5206\u8fa8\u7387 According to the data description there is a 50 50 balance between positive and negative examples in the training and test splits. The optimal lr is just before the base of the loss and before the start of divergence. The higher rate is having a regularizing effect as it won t allow the model to settle for sharp and narrow local minima but pushes for wider and more stable one. com gigascience article 7 6 giy065 5026175 CAMELYON16 background https camelyon16. \u6211\u4eec\u5c06\u4f7f\u7528\u57fa\u4e8ePyTorch\u6784\u5efa\u7684Fast. Dark blue hematoxylin binds to negatively charged substances such as nucleic acids and pink eosin to positively charged substances like amino acid side chains most proteins. These kind of images could be caused by bad exposure or cropping to an empty area. html valid_ds \u662f fastai v0. I haven t tried these so I cannot guarantee these will work straight away. html ItemList specific to vision \u6211\u4eec\u5c06\u56fe\u50cf\u52a0\u8f7d\u5230ImageDataBunch\u8fdb\u884c\u8bad\u7ec3 \u4f7f\u7528\u6211\u4eec\u81ea\u5df1\u7684 readCroppedImage \u51fd\u6570\u52a0\u8f7d\u56fe\u50cf \u8fd9\u4e2afastai\u7684\u6570\u636e\u5bf9\u8c61\u5f88\u5bb9\u6613\u81ea\u5b9a\u4e49 \u6211\u4eec\u53ea\u9700\u8981\u7ee7\u627fImageList Training \u8bad\u7ec3We define a convnet learner object where we set the model architecture and our data bunch. We could deploy this for inference to another machine a web server for example. resnet50 \u5206\u5272\u5b57\u7b26\u4e32\u53d6\u7b2c\u4e8c\u4e2a create dataframe for the fastai loader \u521b\u5efa\u4e00\u4e2adataframe \u7528\u4e8e \u5b9a\u4e49fastai\u6570\u636e\u52a0\u8f7d\u5668ImageDataBunch \u521b\u5efa\u4e00\u4e2a\u5b57\u5178 name \u8bad\u7ec3\u96c6\u5730\u5740 \u56fe\u50cf\u540d label \u56fe\u50cf\u540d\u5bf9\u5e94\u7684\u6807\u8bb0 \u521b\u5efa\u4e00\u4e2a\u8bad\u7ec3\u96c6dataframem \u5217\u540d\u662fname label create test dataframe \u521b\u5efa\u6d4b\u8bd5\u96c6dataframe os. pred_class idx \u662f\u7b2cidx\u7684\u9884\u6d4b\u7c7b\u522b classes cl \u662f\u5176\u771f\u5b9e\u6807\u8bb0 interp. com etinuz for sharing his examples of Fastai v1 training and visualization They were helpful when converting this kernel from Fastai v0. The ROC curve is a plot of True positive rate against False positive rate at various thresholds and the area under the curve AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. bool 1 \u662ftrue bool 0 \u662ffalse getrandbits k \u8fd4\u56de\u4e00\u4e2ak\u6bd4\u7279\u4f4d\u968f\u673a\u7684\u6574\u6570. It might also reveal something about the dataset such as bad quality data. \u73b0\u5728\u8bad\u7ec3\u5df2\u7ecf\u5b8c\u6210\u4e86 How good does the model perform technically \u8be5\u6a21\u578b\u5728\u6280\u672f\u4e0a\u6709\u591a\u597d e can only get metrics from our validation set and the final test metrics will be most likely a bit different. \u73b0\u5728\u6211\u4eec\u6709\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6a21\u578b \u6211\u4eec\u9700\u8981\u90e8\u7f72\u8fd9\u4e2a\u6a21\u578b\u5230\u53e6\u4e00\u53f0\u673a\u5668\u7528\u4e8e\u63a8\u7406\u9884\u6d4b \u6bd4\u5982\u4e00\u4e2aweb\u670d\u52a1\u5668 For this we need our saved model and then we need to export the DataBunch. However the training distribution seems to be 60 40 negatives positives. org site CAMELYON17 serve public_html example_high_resolution. 696453 \u56fe\u50cf\u6bcf\u4e2a\u901a\u9053\u7684\u5e73\u5747\u503c and std s of 0. net qq_33453253 article details 79653546 \u7ed9\u7b2c\u4e8c\u884c\u7b2c\u4e00\u4e2a\u8bbe\u7f6ey\u8f74\u6807\u7b7e \u540c\u4e0a Most correct or least losses \u5c55\u793a\u524d\u56db\u4e2a\u6700\u6b63\u786e \u635f\u5931\u503c\u6700\u5c0f \u7684\u56fe\u4f8b \u8c03\u7528\u4e0a\u9762\u7684\u65b9\u6cd5 \u753b\u51fa\u70ed\u56fe probs from log preds \u8ba1\u7b97\u6bcf\u4e2a\u5143\u7d20\u7684\u6307\u6570e x Compute ROC curve \u8ba1\u7b97ROC\u66f2\u7ebf fpr \u5047\u9633\u6027\u7387 tpr \u771f\u9633\u6027\u7387 thresholds \u9608\u503c\u5217\u8868 y \u771f\u5b9e\u6807\u7b7e probs\u9884\u6d4b\u503c pos_label 1\u6807\u7b7e\u4e3a1\u7684\u662f\u6b63\u4f8b \u5176\u90fd\u662f\u53cd\u4f8b https blog. \u63a5\u4e0b\u6765 \u6211\u4eec\u53ea\u8bad\u7ec3\u6a21\u578b\u7684\u5934\u90e8 \u540e\u9762 \u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u5176\u4f59\u90e8\u5206 \u524d\u9762 \u51bb\u7ed3 \u5426\u5219 \u5934\u90e8 \u524d\u9762 \u6743\u91cd\u7684\u968f\u673a\u521d\u59cb\u5316\u53ef\u80fd\u635f\u5bb3\u6a21\u578b\u7684\u76f8\u5bf9\u826f\u597d\u7684\u9884\u8bad\u7ec3\u6743\u91cd \u5728\u5934\u90e8 \u540e\u9762 \u8c03\u6574\u5e76\u4e14\u6a21\u578b\u6709\u4e9b\u5de5\u4f5c\u4e4b\u540e \u6211\u4eec\u53ef\u4ee5\u7ee7\u7eed\u8bad\u7ec3\u6240\u6709\u6743\u91cd \u524d\u9762 We can see that the learning rate starts from lower and reaches the max_lr in the middle. Leslie Smith https arxiv. \u6211\u4eec\u7684\u6a21\u578b\u6700\u6709\u4fe1\u5fc3 \u4e5f\u662f\u6700\u6b63\u786e\u7684\u6570\u636e\u662f\u4ec0\u4e48 This visualization is a good way of understanding what are the images the model struggles with. Note that if we apply augmentation here augmentations will also be applied when we are predicting inference. \u7136\u540e\u5bf9\u9884\u6d4b\u7ed3\u679c\u53d6\u5e73\u5747 \u589e\u5f3a\u6b21\u6570 \u8bbe\u7f6e\u4e00\u4e2a\u9884\u6d4b\u503c\u77e9\u9635 \u521d\u59cb\u503c\u4e3a0 \u5f62\u72b6\u662f len learner. \u6211\u4eec\u5c06\u4f7f\u7528Leslie Smith\u63d0\u51fa\u7684\u4e00\u4e2a\u5468\u671f\u7b56\u7565 \u8be5\u653f\u7b56\u4e3a\u9009\u62e9\u8d85\u53c2\u6570 \u4f8b\u5982\u5b66\u4e60\u7387\u548c\u4f53\u91cd\u8870\u51cf \u5e26\u6765\u4e86\u66f4\u52a0\u4e25\u8c28\u7684\u65b9\u6cd5 \u8fd9\u53ef\u4ee5\u4e3a\u6211\u4eec\u8282\u7701\u5927\u91cf\u65f6\u95f4 \u4f7f\u7528\u6b21\u4f18\u7684\u8d85\u53c2\u6570\u8fdb\u884c\u8bad\u7ec3 \u53e6\u5916 Fastai\u5e93\u4e3a\u4e00\u4e2a\u5468\u671f\u653f\u7b56\u5b9e\u65bd\u4e86\u8bad\u7ec3\u51fd\u6570 \u6211\u4eec\u53ea\u9700\u51e0\u884c\u4ee3\u7801\u5373\u53ef\u4f7f\u7528 Sylvian Gugger wrote a very clear explanation https sgugger. html Hook https pytorch. org site CAMELYON17 serve public_html example_low_resolution. \u5bf9\u6570\u636e\u6807\u51c6\u5316 \u8fd9\u4e9b\u53c2\u6570\u662f\u56fe\u50cf\u4e09\u4e2a\u901a\u9053\u7684\u5747\u503c\u548c\u6807\u51c6 \u6211\u4eec\u5148\u524d\u5728\u7edf\u8ba1\u6b65\u9aa4\u4e2d\u8ba1\u7b97\u4e86\u8fd9\u4e9b check that the imgDataBunch is loading our images ok \u68c0\u67e5imageDataBunch\u662f\u5426\u6b63\u786e\u52a0\u8f7d\u60a8\u7684\u56fe\u50cf Next we create a convnet learner object \u521b\u5efa\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5668\u5bf9\u8c61 ps dropout percentage 0 1 in the final layer ps\u662f\u6700\u540e\u4e00\u5c42\u968f\u673a\u5931\u6d3b\u7684\u6bd4\u4f8b create_cnn becomes cnn_learner \u8fd9\u4e2a\u65b9\u6cd5\u5728\u65b0\u7248\u672c\u4e2d\u6539\u4e3acnn_learner \u83b7\u5f97\u4e00\u4e2a\u7528\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u6a21\u578b imgDataBunch\u662f\u6570\u636e\u52a0\u8f7d\u5668 arch densenet169 dropout pretrained\u5982\u679c\u662ffalse\u5219\u53c2\u6570\u968f\u673a\u521d\u59cb\u5316. get_preds get accuracy \u8ba1\u7b97\u7cbe\u5ea6 https docs. top_losses matpoltlib subplots \u521b\u5efa\u4e00\u7cfb\u5217\u7684\u5b50\u56fe nrows\u5b50\u56fe\u884c\u7684\u4e2a\u6570 ncols\u5b50\u56fe\u5217\u7684\u4e2a\u6570 figsize\u753b\u5e03\u5c3a\u5bf8 ax\u5b50\u56fe\u7684\u8f74 fig\u56fe Random\u5c55\u793a\u968f\u673a\u62bd\u6837\u7684\u56fe\u4f8b \u53ea\u5c55\u793a\u56db\u5f20\u56fe \u5728 0 len tl_idx \u4e4b\u95f4\u968f\u673a\u751f\u6210\u4e00\u4e2a\u6574\u6570 \u968f\u673a\u5f97\u5230\u4e00\u5f20\u56fe\u7247\u7684\u7d22\u5f15 interp. I recommend using an iteration count of at least 300 for more consistent results. valid_ds \u662f validation\u7684\u7f29\u5199 https docs. Anyway removing only a small amount of outliers from this size data set has little or no effect on the prediction performance. com fastai fastai search q lr_find unscoped_q lr_find recorder \u8bb0\u5f55\u4e86epoch\u8f6e\u6b21 loss\u635f\u5931\u503c opt\u4f18\u5316\u5668 metric\u6307\u6807 https docs. \u6211\u4eec\u53ef\u4ee5\u57282e 2\u9644\u8fd1\u9009\u62e9 \u5b66\u4e60\u7387 \u56e0\u4e3a\u5b83\u63a5\u8fd1\u635f\u5931\u51fd\u6570\u5e95\u90e8\u4f46\u4ecd\u7136\u4e0b\u964d Next we train only the heads while keeping the rest of the model frozen. Irregular chromatin pattern esp. \u6839\u636e\u6570\u636e\u63cf\u8ff0 \u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\u4e2d\u7684\u6b63\u4f8b\u548c\u53cd\u4f8b\u4e4b\u95f4\u5b58\u572850 50\u7684\u5e73\u8861 \u4f46\u662f \u8bad\u7ec3\u6570\u636e\u7684\u5206\u5e03\u4f3c\u4e4e\u662f60 40 \u8d1f\u6570 \u6b63\u6570 \u6b63\u4f8b\u6807\u8bb0\u610f\u5473\u7740\u5728\u56fe\u50cf\u7684\u4e2d\u5fc3\u533a\u57df 32 32\u50cf\u7d20 \u4e2d\u5b58\u5728\u81f3\u5c11\u4e00\u4e2a\u80bf\u7624\u7ec4\u7ec7\u50cf\u7d20 \u8d34\u7247\u5916\u90e8\u533a\u57df\u7684\u80bf\u7624\u7ec4\u7ec7\u4e0d\u4f1a\u5f71\u54cd\u6700\u540e\u7684\u6807\u7b7e \u8fd9\u610f\u5473\u7740\u53cd\u4f8b\u6807\u8bb0\u7684\u56fe\u50cf \u4e2d\u5fc3\u6ca1\u6709\u80bf\u7624\u7ec6\u80de \u53ef\u80fd\u5916\u90e8\u533a\u57df\u5305\u542b\u8f6c\u79fb\u764c\u7ec6\u80de \u56e0\u6b64 \u5c06\u56fe\u50cf\u88c1\u526a\u5230\u4e2d\u5fc3\u533a\u57df\u662f\u4e2a\u597d\u4e3b\u610f Image file descriptors\u56fe\u50cf\u6587\u4ef6\u7684\u63cf\u8ff0 Description Format \u683c\u5f0f TIFSize \u5c3a\u5bf8 96 x 96Channels \u901a\u9053\u6570 3Bits per channel \u6bcf\u4e2a\u901a\u9053\u7684\u6bd4\u7279\u6570 8Data type \u6570\u636e\u7c7b\u578b Unsigned charCompression \u538b\u7f29\u65b9\u5f0f Jpeg Is the data relevant to the problem \u6570\u636e\u662f\u5426\u4e0e\u95ee\u9898\u76f8\u5173 This dataset is a combination of two independent datasets collected in Radboud University Medical Center Nijmegen the Netherlands and the University Medical Center Utrecht Utrecht the Netherlands. ai vision models https docs. items \u6d4b\u8bd5\u96c6\u6570\u76ee 2 \u53cd\u4f8b\u548c\u6b63\u4f8b\u6982\u7387 tqdm_notebook\u53ef\u7528\u4e8e\u663e\u793a\u8fdb\u5ea6\u6761 \u5176\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u63d0\u793a\u7528\u7684 preds\u9884\u6d4b\u503c y\u9884\u6d4b\u6807\u8bb0 with_loss False\u4e0d\u8ba1\u635f\u5931\u503c https docs. The slides are produced by routine clinical practices and a trained pathologist would examine similar images for identifying metastases. Returns x y with size of the batch_size if bs 128 then there is a list of 128 elements. However during the acquisition process scanning can fail or result in out of focus images. \u6211\u4eec\u53ef\u4ee5\u4ece\u7ed8\u5236\u7684\u635f\u5931\u66f2\u7ebf\u4e2d\u770b\u51fa \u5728\u521d\u59cb\u4e0b\u964d\u4e4b\u540e\u5b58\u5728\u5c0f\u7684\u4e0a\u5347 \u8fd9\u662f\u7531\u4e8e\u524d\u534a\u5468\u671f\u7684\u5b66\u4e60\u7387\u589e\u52a0\u5f15\u8d77\u7684 \u5f53 max_lr \u5c06\u6a21\u578b\u9003\u79bb\u51fa\u5c40\u90e8\u6700\u5c0f\u503c\u65f6 \u635f\u5931\u6682\u65f6\u4e0a\u5347 \u4f46\u662f\u5f53\u5b66\u4e60\u7387\u964d\u4f4e\u65f6 \u8fd9\u5c06\u6700\u7ec8\u5f97\u5230\u56de\u62a5 Confusion matrix can help us understand the ratio of false negatives and positives and it s a fast way looking at our model s performance. merge\u51fd\u6570\u662f\u5408\u5e76\u5355\u901a\u9053\u6210\u591a\u901a\u9053 \u4e0d\u80fd\u5408\u5e76\u591a\u4e2a\u591a\u901a\u9053\u56fe\u50cf random sampling\u8bad\u7ec3\u6807\u8bb0\u968f\u673a\u62bd\u6837 matpoltlib subplots \u521b\u5efa\u4e00\u7cfb\u5217\u7684\u5b50\u56fe nrows\u5b50\u56fe\u884c\u7684\u4e2a\u65702 ncols\u5b50\u56fe\u5217\u7684\u4e2a\u65705 figsize\u753b\u5e03\u5c3a\u5bf8 axes\u5b50\u56fe\u7684\u8f74 fig\u56fe \u6807\u9898 Negatives\u53cd\u4f8b \u6ca1\u6709\u8f6c\u79fb\u764c idx\u662f\u968f\u673a\u6392\u5217\u7684\u524d\u4e94\u4e2a\u53cd\u4f8b\u7684\u56fe\u7247\u540d i\u662f0 4\u7684\u7d22\u5f15 \u5c06\u8bad\u7ec3\u96c6\u8def\u5f84\u4e0e\u53cd\u4f8b\u56fe\u7247\u540d \u62fc\u63a5\u8d77\u6765 \u5728\u5750\u6807\u4e3a 0 i \u7684\u8f74\u4e0a\u753b\u51fa\u8fd9\u5f20\u53cd\u4f8b\u56fe Create a Rectangle patch \u753b\u51fa\u84dd\u8272\u4e2d\u5fc3\u865a\u7ebf\u77e9\u5f62\u6846 \u5728\u5750\u6807\u4e3a 0 i \u7684\u8f74\u4e0a\u753b\u51fa\u8fd9\u4e2a\u77e9\u5f62\u6846 \u7ed9\u5750\u6807\u4e3a 0 0 \u7684\u8f74\u753b\u51fa\u5176y\u8f74\u6807\u9898 Positives\u6b63\u4f8b \u6709\u8f6c\u79fb\u764c idx\u662f\u968f\u673a\u6392\u5217\u7684\u524d\u4e94\u4e2a\u6b63\u4f8b\u7684\u56fe\u7247\u540d i\u662f0 4\u7684\u7d22\u5f15 \u5c06\u8bad\u7ec3\u96c6\u8def\u5f84\u4e0e\u6b63\u4f8b\u56fe\u7247\u540d \u62fc\u63a5\u8d77\u6765 \u5728\u5750\u6807\u4e3a 1 i \u7684\u8f74\u4e0a\u753b\u51fa\u8fd9\u5f20\u6b63\u4f8b\u56fe Create a Rectangle patch \u753b\u51fa\u7ea2\u8272\u4e2d\u5fc3\u865a\u7ebf\u77e9\u5f62\u6846 \u5728\u5750\u6807\u4e3a 0 i \u7684\u8f74\u4e0a\u753b\u51fa\u8fd9\u4e2a\u77e9\u5f62\u6846 \u7ed9\u5750\u6807\u4e3a 1 0 \u7684\u8f74\u753b\u51fa\u5176y\u8f74\u6807\u9898 \u751f\u6210\u968f\u673a\u53d8\u91cf original size of the images do not change\u539f\u59cb\u56fe\u7247\u5c3a\u5bf8 AUGMENTATION VARIABLES \u6570\u636e\u589e\u5f3a\u7684\u53d8\u91cf final size after crop\u526a\u88c1\u540e\u7684\u56fe\u7247\u5927\u5c0f range 0 180 180 allows all rotation variations 0 no change\u65cb\u8f6c\u89d2\u5ea6 center crop shift in x and y axes 0 no change. html The data block API Create ImageDataBunch using fastai data block API \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668ImageDataBunch \u6839\u636e\u8bad\u7ec3\u96c6dataframe\u7684\u5217\u4e2d\u7684\u8def\u5f84\u521b\u5efatorch\u4e2d\u7684ItemList Where to find the data ItemList. from_learner learner \u8fd9\u662f\u4e4b\u524d\u5b9a\u4e49\u7684 \u8c03\u7528\u4e0a\u9762\u8fd9\u4e2a\u51fd\u6570 hook into forward pass \u524d\u5411\u4f20\u64ad\u65f6\u81ea\u52a8\u8c03\u7528\u8fd9\u4e2a\u94a9\u5b50 \u7528\u4e8e\u68c0\u67e5\u548c\u4fee\u6539\u6bcf\u4e00\u5c42\u7684\u8f93\u51fa https docs. As a quality control measure all slides were inspected manually after scanning. org site CAMELYON17 serve public_html example_mid_resolution. The samples don t have tumor tissue present. This is a simple table that shows the counts in a way of actual label vs. However some useful information about the surroundings could be lost if we crop too close. png attachment image. org Data which contains 400 H E stained whole slide images of sentinel lymph node sections that were acquired and digitized at 2 different centers using a 40x objective. With MVP we can very quickly see if there are some unexpected problems like bad data quality that will make any further investments into the model tuning not worth it. csv and keep their original order \u4ece\u63d0\u4ea4\u6a21\u677f\u4e2d\u53d6\u5f97id\u5e76\u4fdd\u6301\u5176\u987a\u5e8f \u4ecesample_submission. org tutorials beginner saving_loading_models. png How good is the model in terms of project requirements \u6a21\u578b\u5728\u9879\u76ee\u8981\u6c42\u65b9\u9762\u6709\u591a\u597d It is a good idea to look at examples of images from \u67e5\u770b\u56fe\u50cf\u7684\u793a\u4f8b\u662f\u4e2a\u597d\u4e3b\u610f Random samples \u968f\u673a\u6837\u672c Some random predictions \u4e00\u4e9b\u968f\u673a\u6837\u672c\u7684\u9884\u6d4b The most incorrectly labeled \u6700\u4e0d\u6b63\u786e\u7684\u6807\u7b7e What our model predicts wrong with very high probability. You may need to do some debugging. \u8fd9\u663e\u793a\u4e86\u9884\u6d4b\u7c7b\u522b\u7684\u6fc0\u6d3b\u56fe \u70ed\u56fe \u56e0\u6b64\u5982\u679c\u6807\u7b7e\u662f \u80bf\u7624 \u5219\u53ef\u89c6\u5316\u56fe \u4f1a\u663e\u793a \u6a21\u578b\u8ba4\u4e3a\u80bf\u7624\u6240\u5728\u7684\u6240\u6709\u4f4d\u7f6e ROC curve and AUC ROC\u66f2\u7ebf\u548c\u66f2\u7ebf\u4e0b\u9762\u79efAUC Remember AUC is the metric that is used for evaluating submissions. \u6240\u6709\u8fc7\u4e8e\u9ed1\u6697\u548c\u660e\u4eae\u7684\u56fe\u50cf\u90fd\u6807\u8bb0\u4e3a\u53cd\u4f8b\u6837\u672c \u6211\u8ba4\u4e3a\u660e\u4eae\u7684\u90a3\u4e9b\u53ea\u662f\u4ece\u672a\u67d3\u8272\u7684\u90e8\u5206\u88c1\u526a\u6216\u4ed6\u4eec\u6ca1\u6709\u4efb\u4f55\u7ec4\u7ec7 \u666e\u901a\u73bb\u7483 \u6240\u4ee5\u53cd\u4f8b\u6807\u8bb0\u662f\u6b63\u786e\u7684 \u5373\u6837\u672c\u4e0d\u5b58\u5728\u80bf\u7624\u7ec4\u7ec7 \u6211\u4e0d\u592a\u786e\u5b9a\u9ed1\u6697\u7684\u56fe\u50cf \u5b83\u662f\u6765\u81ea\u66b4\u9732\u4e25\u91cd\u533a\u57df\u7684\u5f02\u5e38\u526a\u5207 \u8fd8\u662f\u53ea\u662f\u586b\u5145\u6574\u4e2a\u56fe\u50cf\u7684\u4e00\u4e9b\u975e\u5e38\u5927\u7684\u7ec6\u80de\u90e8\u5206 \u65e0\u8bba\u5982\u4f55 \u4ece\u8be5\u5927\u5c0f\u6570\u636e\u96c6\u4e2d\u4ec5\u53bb\u9664\u5c11\u91cf\u5f02\u5e38\u503c\u5bf9\u9884\u6d4b\u6027\u80fd\u51e0\u4e4e\u6ca1\u6709\u5f71\u54cd\u6216\u6ca1\u6709\u5f71\u54cd Baseline model\u57fa\u7ebf\u6a21\u578b Fastai v1 In ML production pipeline it is a good idea to start with a relatively simple model sort of a minimum viable product MVP or a baseline. We just need to subclass ImageList. However we will continue training further to improve from here. \u817a\u4f53\u5f62\u6210 Single cells. cvtColor image2np \u5c06\u56fe\u7247\u4ecepytorch\u683c\u5f0f\u8f6c\u4e3anumpy\u683c\u5f0f https docs. cvtColor https docs. html ClassificationInterpretation top_losses \u5c06\u8fd4\u56de\u6240\u6709 \u9a8c\u8bc1\u96c6\u7684\u635f\u5931\u503c tl_val \u548c\u5176\u7d22\u5f15 tl_idx \u6309\u964d\u5e8f\u6392\u5e8f top losses will return all validation losses and indexes sorted by the largest first matpoltlib subplots \u521b\u5efa\u4e00\u7cfb\u5217\u7684\u5b50\u56fe nrows\u5b50\u56fe\u884c\u7684\u4e2a\u6570 ncols\u5b50\u56fe\u5217\u7684\u4e2a\u6570 figsize\u753b\u5e03\u5c3a\u5bf8 ax\u5b50\u56fe\u7684\u8f74 fig\u56fe Random \u968f\u673a\u62bd\u6837 \u5728 0 max_offset \u4e4b\u95f4\u968f\u673a\u751f\u6210\u4e00\u4e2a\u6574\u6570 \u8be5\u9a8c\u8bc1\u96c6\u7684\u7d22\u5f15 \u5f97\u5230\u8be5\u56fe\u7247\u7684\u70ed\u56fe\u548c\u80cc\u666f \u56fe\u7247\u957f\u5bbd _ \u662f\u56fe\u7247 cl \u662f\u7c7b\u522bclass \u503c\u662f0 1 0\u6307\u4ee3\u8d1f\u4f8b 1\u6307\u4ee3\u80bf\u7624 \u8f6c\u6362\u4e3a\u6574\u6570\u578b \u5728\u7b2c1\u884c \u7b2ci 1\u5217\u4e0a\u5c55\u793a\u80cc\u666f\u56fe \u989c\u8272\u56fe\u4e3a\u7070\u5ea6\u56fe \u753b\u51fa\u70ed\u56fe interpolation\u63d2\u503c \u4ee3\u8868\u56fe\u7247\u50cf\u7d20\u989c\u8272\u6ca1\u6709\u8fc7\u6e21 https blog. One key challenge is that the metastases can be as small as single cells in a large area of tissue. ai t lesson 6 advanced discussion 31442 6 https forums. html Remember that you must call model. html ClassificationInterpretation. What I like about Fast. org stable modules generated sklearn. Thus it would be a good idea to crop the images to the center region. plot_losses predict the validation set with our model \u9884\u6d4b\u6a21\u578b\u7684\u9a8c\u8bc1\u96c6 \u5206\u7c7b\u6a21\u578b\u7684\u6548\u679c\u89e3\u91ca\u51fd\u6570 https docs. ndarray list python\u5217\u8868\u751f\u6210\u5f0f To know the id s we create a dict of id pred \u628a\u6211\u4eec\u7684id\u548c\u9884\u6d4b\u503c \u7ec4\u6210 \u4e00\u4e2a\u5b57\u5178 python zip \u628a\u591a\u4e2a\u5e8f\u5217\u7684\u5143\u7d20\u5bf9\u5e94\u7f16\u7ec7\u5728\u4e00\u8d77 https www. \u5c0f\u7ec6\u80de\u7fa4 The takeaway from this is probably that irregular nuclear shapes sizes or staining shades can indicate metastases. The losses are temporarily rising when max_lr drives the model out of local minima but this will pay off in the end when the learning rates are decreased. We want to maintain equal ratios of negative positive 60 40 in both training and test splits. html Image y\u662f\u56fe\u7684\u6807\u9898 \u5c55\u793a\u56fe\u50cf https docs. for the other centers to assess the quality of the scan when in doubt a pathologist was consulted on whether scanning issues might affect diagnosis. html See results dl \u8fd4\u56de\u4e00\u4e9b\u6570\u636e\u7528\u4e8e\u9a8c\u8bc1 \u8bad\u7ec3\u6216\u8005\u6d4b\u8bd5 https docs. \u6211\u8981\u611f\u8c22Martijn\u5206\u4eab\u4ed6\u7684Fastai v1\u8bad\u7ec3\u548c\u53ef\u89c6\u5316\u793a\u4f8b \u5c06\u6b64\u5185\u6838\u4eceFastai v0. In addititon Fastai library has implemented a training function for one cycle policy that we can use with only a few lines of code. html image2np \u8f6c\u6362\u4e3a\u6574\u6570 \u5728\u7b2c\u4e00\u884c\u7b2ci\u4e2a\u8f74\u4e0a\u5c55\u793a\u8fd9\u5f20\u56fe\u7247 \u7b2c\u4e00\u884c\u7b2ci\u4e2a\u8f74\u7684x\u8f74\u6ca1\u6709\u523b\u5ea6 \u7b2c\u4e00\u884c\u7b2ci\u4e2a\u8f74\u7684y\u8f74\u6ca1\u6709\u523b\u5ea6 \u8bbe\u7f6e\u6807\u9898 classes interp. \u5728\u6211\u4eec\u8ba1\u7b97\u7edf\u8ba1\u6570\u636e\u7684\u540c\u65f6 \u6211\u4eec\u53ef\u4ee5\u68c0\u67e5\u662f\u5426\u5b58\u5728\u5177\u6709\u6700\u5927\u50cf\u7d20\u503c\u975e\u5e38\u4f4e\u7684 \u51e0\u4e4e\u5168\u9ed1 \u6216\u6700\u5c0f\u50cf\u7d20\u503c\u975e\u5e38\u9ad8\u7684 \u51e0\u4e4e\u5168\u767d \u7684\u56fe\u50cf \u8fd9\u4e9b\u56fe\u50cf\u53ef\u80fd\u662f\u7531\u4e8e\u66dd\u5149\u4e0d\u826f\u6216\u88c1\u526a\u5230\u7a7a\u767d\u533a\u57df\u9020\u6210\u7684 \u5728\u7a7a\u767d\u533a\u57df\u7684\u60c5\u51b5\u4e0b \u56fe\u50cf\u4e0d\u662f\u5f02\u5e38\u503c \u800c\u662f\u540c\u6837\u6709\u6548\u7684\u8d1f\u6837\u672c We find that there is at least one very dark and 6 very bright images. org Background Lymph nodes are small glands that filter the fluid in the lymphatic system and they are the first place a breast cancer is likely to spread. ai \u73b0\u5728\u5df2\u6539\u4e3acnn_learner https docs. Irregular nuclear membrane\u6838\u819c\u4e0d\u89c4\u5219. This is not so crucial here as both labels are almost equally represented but in case we had a rare class random split could cause severe underrepresentation or in worst case leave all rare classes out of one split. \u4e3a\u5176\u4ed6\u4e2d\u5fc3 \u8fdb\u884c \u4ee5\u8bc4\u4f30\u626b\u63cf\u8d28\u91cf \u5982\u6709\u7591\u95ee \u8bf7\u54a8\u8be2\u75c5\u7406\u5b66\u5bb6 \u4e86\u89e3\u626b\u63cf\u95ee\u9898\u662f\u5426\u4f1a\u5f71\u54cd\u8bca\u65ad 1399 H E stained sentinel lymph node sections of breast cancer patients the CAMELYON dataset https academic. org wiki Receiver_operating_characteristic. The best possible solution would yield an AUC of 1 which means we would classify all positive samples correctly without getting any false positives. png ROC curve from a previous run of this kernel\u8fd9\u7bc7kernel\u7684ROC\u66f2\u7ebf What is known about the domain \u4e13\u4e1a\u9886\u57df\u77e5\u8bc6 The histopathological images are glass slide microscope images of lymph nodes that are stained with hematoxylin and eosin H E. html fit_one_cycle \u753b\u51fa\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u635f\u5931 https docs. html accuracy I modified this from the fastai s plot_top_losses https github. \u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u662f\u7528\u82cf\u6728\u7cbe\u548c\u4f0a\u7ea2 H E \u67d3\u8272\u7684\u6dcb\u5df4\u7ed3\u7684\u73bb\u7483\u8f7d\u73bb\u7247\u663e\u5fae\u955c\u56fe\u50cf \u8fd9\u79cd\u67d3\u8272\u65b9\u6cd5\u662f\u533b\u5b66\u8bca\u65ad\u4e2d\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u65b9\u6cd5\u4e4b\u4e00 \u5b83\u4ea7\u751f\u84dd\u8272 \u7d2b\u8272\u548c\u7ea2\u8272 \u6df1\u84dd\u82cf\u6728\u7cbe\u4e0e\u5e26\u8d1f\u7535\u8377\u7684\u7269\u8d28 \u5982\u6838\u9178\u548c\u7c89\u7ea2\u7d20 \u7ed3\u5408\u5230\u5e26\u6b63\u7535\u8377\u7684\u7269\u8d28\u5982\u6c28\u57fa\u9178\u4fa7\u94fe \u5927\u591a\u6570\u86cb\u767d\u8d28 \u901a\u5e38 \u7ec6\u80de\u6838\u67d3\u6210\u84dd\u8272 \u800c\u7ec6\u80de\u8d28\u548c\u7ec6\u80de\u5916\u90e8\u5206\u67d3\u6210\u5404\u79cd\u7c89\u7ea2\u8272 Low resolution\u4f4e\u5206\u8fa8\u7387 Mid resolution\u4e2d\u7b49\u5206\u8fa8\u7387 High resolution\u9ad8\u5206\u8fa8\u7387 https camelyon17. See if how much we improved. predict img 0 \u662ftumor. predict img 0 \u662f1\u65f6 classes int learner. \u6211\u4eec\u7684\u6a21\u578b\u4ee5\u975e\u5e38\u9ad8\u7684\u6982\u7387\u9884\u6d4b\u9519\u8bef\u7684\u6570\u636e\u662f\u4ec0\u4e48 The most correctly labeled \u6700\u6b63\u786e\u7684\u6807\u7b7e What our model is most confident of and gets right. However some relevant information about the surroundings might be left out with these small sized image samples. drop labels too_bright_idx axis 0 \u5220\u9664\u8bad\u7ec3\u96c6\u4e2d\u592a\u4eae\u7684\u56fe\u7247 print After removing outliers we have 0 training samples. \u6211\u4eec\u505a\u4e86\u76f8\u5f53\u6570\u91cf\u7684\u8fed\u4ee3 \u6765\u6267\u884c\u56fe\u50cf\u7ffb\u8f6c\u548c\u65cb\u8f6c\u7684\u4e0d\u540c\u7ec4\u5408\u7684\u64cd\u4f5c The predictions are then averaged. html Recorder \u628a\u8bad\u7ec3\u7684\u5b66\u4e60\u7387\u4fdd\u5b58\u5230lrs\u6570\u7ec4\u4e2d \u628a\u8bad\u7ec3\u7684\u635f\u5931\u503c\u4fdd\u5b58\u5230losses\u6570\u7ec4\u4e2d \u628a\u8bad\u7ec3\u7684\u8870\u51cf\u6743\u91cd\u4fdd\u5b58\u5230wds\u6570\u7ec4\u4e2d reset learner this gets more consistent starting conditions \u91cd\u8bbe\u5b66\u4e60\u5668 WEIGHT DECAY 1e 4 \u8870\u51cf\u6743\u91cd\u662f1e 4 reset learner this gets more consistent starting conditions \u91cd\u8bbe\u5b66\u4e60\u5668 WEIGHT DECAY 1e 2 \u8870\u51cf\u6743\u91cd\u662f1e 2 reset learner Plot weight decays \u753b\u51fa\u4e09\u79cd\u6743\u91cd\u8870\u51cf\u503c\u4e0b\u7684 \u5b66\u4e60\u7387\u4e0e\u635f\u5931\u503c\u7684\u5173\u7cfb\u56fe \u521b\u5efa\u4e00\u4e2a\u5b50\u56fe \u5728\u56fe\u4e2d\u753b\u51fa\u5b66\u4e60\u7387 \u635f\u5931\u503c\u5bf9 \u627e\u5230\u6700\u5c0f\u635f\u5931\u4e0b\u754c y\u8f74\u6807\u7b7e\u662fLoss x\u8f74\u6807\u7b7e\u662fLearning Rate x\u8f74\u5c3a\u5ea6\u662flog\u5bf9\u6570 ax ranges may need some tuning with different model architectures \u5f53\u4f7f\u7528\u4e0d\u540c\u7684\u6a21\u578b\u65f6 ax\u7684\u8303\u56f4\u53ef\u80fd\u9700\u8981\u505a\u8c03\u6574 \u8bbe\u7f6ex\u8f74\u4e0a\u754c\u548c\u4e0b\u754c \u8bbe\u7f6ey\u8f74\u4e0a\u754c\u548c\u4e0b\u754c \u56fe\u4f8b 1e 6 1e 4 1e 2 \u8bbe\u7f6ex\u8f74\u6807\u7b7e\u6587\u672c\u7684\u683c\u5f0f \u6700\u5927\u5b66\u4e60\u7387 \u6700\u5927\u6743\u91cd\u8870\u51cf\u503c 1cycle policy \u6839\u636e1cycle\u7b56\u7565\u62df\u5408\u6a21\u578b https docs. This staining method is one of the most widely used in medical diagnosis and it produces blue violet and red colors. html _cl_int_plot_multi_top_losses before we continue lets save the model at this stage \u5728\u6211\u4eec\u7ee7\u7eed\u4e4b\u524d \u8ba9\u6211\u4eec\u4fdd\u5b58\u6a21\u578b\u8fd9\u4e2a\u9636\u6bb5\u7684\u6a21\u578b\u4e3a1 load the baseline model \u52a0\u8f7d\u4e4b\u524d\u4fdd\u5b58\u7684\u57fa\u7ebf\u6a21\u578b\u6743\u91cd unfreeze and run learning rate finder again \u89e3\u51bb\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u5c42\u5e76\u518d\u6b21\u8fd0\u884c\u5bfb\u627e\u6700\u4f73\u5b66\u4e60\u7387\u65b9\u6cd5 \u4e4b\u524d\u8bbe\u7f6e\u7684\u6700\u5927\u8870\u51cf\u6743\u91cd plot learning rate finder results \u753b\u51fa\u5bfb\u627e\u5b66\u4e60\u7387\u7684\u7ed3\u679c Now smaller learning rates. com fastai fastai blob c15331a7b8b9fa908dbe6c0bcb38ba124e0d2768 fastai data_block. \u9996\u5148 \u6211\u4eec\u8981\u627e\u5230\u6700\u4f73\u7684 \u5b66\u4e60\u7387 \u548c \u6743\u91cd\u8870\u51cf \u503c \u6700\u4f73\u5b66\u4e60\u7387\u6070\u597d\u5728\u635f\u5931\u7684\u57fa\u70b9 \u6700\u4f4e\u70b9 \u4e4b\u524d\u548c\u5f00\u59cb\u53d1\u6563\u4e4b\u524d \u91cd\u8981\u7684\u662f \u5728\u6211\u4eec\u9009\u62e9\u6700\u5927\u5b66\u4e60\u7387\u7684\u5730\u65b9 \u635f\u5931\u503c\u5e94\u4ecd\u5728\u4e0b\u964d As for the weight decay that is the L2 penalty of the optimizer Leslie proposes to select the largest one that will still let us train at a high learning rate so we do a small grid search with 1e 2 1e 4 and 1e 6 weight decays. losses idx \u662f\u635f\u5931\u503c interp. \u5224\u65ad96 x 96\u50cf\u7d20\u7684\u6570\u5b57\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u8f6c\u79fb\u764c\u7ec6\u80de\u7684\u5b58\u5728 \u4e00\u4e2a\u5173\u952e\u7684\u6311\u6218\u662f\u8f6c\u79fb\u764c\u7ec6\u80de\u975e\u5e38\u5c0f How would a solution look like \u6211\u4eec\u671f\u671b\u7684\u89e3\u51b3\u65b9\u6848 Our evaluation metric is area under the ROC curve http en. asymmetry\u4e0d\u89c4\u5219\u7684\u67d3\u8272\u8d28\u6a21\u5f0f \u5c24\u5176\u662f\u4e0d\u5bf9\u79f0. 0\u65f6 \u5b83\u4eec\u975e\u5e38\u6709\u7528 What kind of model architecture suits the problem best \u4ec0\u4e48\u6837\u7684\u6a21\u578b\u67b6\u6784\u6700\u9002\u5408\u8fd9\u4e2a\u95ee\u9898 Here we will be using a pretrained convnet model and transfer learning to adjust the weights to our data. space posts grad cam heatmaps along resnet 34 Then modify our plotting func a bit \u7ec8\u4e8e\u5f00\u59cb\u753b\u56fe\u51fd\u6570\u5566 \u4e0e\u524d\u9762\u7684\u7c7b\u4f3c interp \u5206\u7c7b\u6a21\u578b\u7684\u6548\u679c\u89e3\u91ca\u51fd\u6570 https docs. This is a good place to stop. \u63a5\u4e0b\u6765 \u6211\u4eec\u53ef\u4ee5\u89e3\u51bb\u6a21\u578b\u4e2d\u7684\u6240\u6709\u53ef\u8bad\u7ec3\u53c2\u6570\u5e76\u7ee7\u7eed\u8bad\u7ec3 The model already performs well and now as we unfreeze the bottom layers that have been pre trained with a large number of general images to detect common shapes and patterns all weights are mostly adjusted. listdidr\u8fd4\u56de\u4e00\u4e2a\u5217\u8868\u5305\u542b\u67d0\u8def\u5f84\u4e0b\u6240\u6709\u6587\u4ef6\u540d \u5c06\u6d4b\u8bd5\u96c6\u8def\u5f84\u548c\u6d4b\u8bd5\u56fe\u50cf\u540d\u7ec4\u5408\u6210\u4e00\u4e2a\u5b57\u7b26\u4e32 \u8ffd\u52a0\u5230\u5217\u8868\u4e2d \u6784\u9020\u4e00\u4e2adataframe \u6570\u636e\u662ftest_names \u5217\u540d\u662fname Subclass ImageList to use our own image opening function \u521b\u5efaImageList\u7684\u5b50\u7c7b \u5e76\u81ea\u5b9a\u4e49open\u51fd\u6570\u52a0\u8f7d\u56fe\u7247 \u4e5f\u662f\u7528\u4e8e\u5b9a\u4e49ImageDataBunch \u521b\u5efa\u7c7b \u7ee7\u627f\u81eaImageList PathOrStr Image \u90fd\u662f\u6ce8\u91ca \u8c03\u7528\u4e4b\u524d\u5b9a\u4e49\u7684\u8bfb\u53d6\u56fe\u7247\u7684\u51fd\u6570 str. \u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5668\u5bf9\u8c61 \u6211\u4eec\u5728\u5176\u4e2d\u8bbe\u7f6e\u6a21\u578b\u4f53\u7cfb\u7ed3\u6784\u548c\u6570\u636e\u96c6 create_cnn docs. \u751f\u6210\u4e00\u4e2adataframe \u7b2c\u4e00\u5217\u662f\u63d0\u4ea4\u6a21\u677f\u7684id \u7b2c\u4e8c\u5217label\u662f\u6309\u7167\u89c4\u5b9a\u987a\u5e8f\u6392\u5217\u7684\u9884\u6d4b\u6982\u7387 Export to csv dataframe to_csv \u5c06dataframe\u8f6c\u6362\u4e3acsv\u6587\u4ef6 header True\u4fdd\u7559\u5217\u540d index False\u4e0d\u8981\u4fdd\u5b58\u7d22\u5f15 This is what the first 10 items of submission look like \u67e5\u770b\u63d0\u4ea4\u6570\u636e\u7684\u524d\u5341\u884c pandas dataframe head This will create an export. net eric_pycv article details 72887758 cv2. \u6211\u8fd8\u6ca1\u6709\u5c1d\u8bd5\u8fd9\u4e9b \u4e0d\u4fdd\u8bc1\u80fd\u7acb\u523b\u5de5\u4f5c \u53ef\u80fd\u9700\u8981\u4e00\u4e9b\u8c03\u8bd5 Official documentation\u5b98\u65b9\u6587\u6863 https docs. png Example of a metastatic region in lymph nodes\u6dcb\u5df4\u7ed3\u8f6c\u79fb\u764c\u533a\u57df\u7684\u4f8b\u5b50 CHAMELYON17 https camelyon17. html and torchvision models https pytorch. ai \u8bf7\u53c2\u9605\u6b64\u5185\u6838\u7684\u63d0\u4ea4\u7248\u672c9 I highly recommend the Fastai practical deep learning course for coders v3 https course. However this has not been confirmed by testing. A positive label means that there is at least one pixel of tumor tissue in the center region 32 x 32px of the image. io deepnn gpu servers Sections of this kernel\u8fd9\u7bc7kernel\u7684\u76ee\u5f55 Project understanding\u4e86\u89e3\u9879\u76ee Data understanding\u4e86\u89e3\u6570\u636e Data visualization\u6570\u636e\u53ef\u89c6\u5316 Baseline model Fastai v1 \u57fa\u51c6\u6a21\u578b Validation and analysis\u9a8c\u8bc1\u548c\u5206\u6790 Metrics \u6307\u6807 Prediction and activation visualizations\u9884\u6d4b\u548c\u70ed\u56fe\u53ef\u89c6\u5316 ROC AUC \u6307\u6807 Submit\u63d0\u4ea4 Deploy example \u90e8\u7f72Section Data visualization\u6570\u636e\u53ef\u89c6\u5316 \u7b2c\u4e00\u884c\u662f\u6b63\u5e38\u7684 \u7b2c\u4e8c\u884c\u662f\u6709\u764c\u7684 Section Prediction and activation visualizations\u9884\u6d4b\u548c\u70ed\u56fe\u53ef\u89c6\u5316 \u7b2c\u4e00\u884c\u662f\u968f\u673a\u53d6\u6837 \u7b2c\u4e8c\u884c\u662f\u6700\u9519\u8bef\u7684 \u7b2c\u4e09\u884c\u662f\u6700\u6b63\u786e\u7684 \u9884\u6d4b\u7c7b\u522b \u771f\u5b9e\u7c7b\u522b \u635f\u5931\u503c \u6982\u7387 Project understanding\u4e86\u89e3\u9879\u76ee What exactly is the problem \u95ee\u9898\u662f\u4ec0\u4e48 Binary image classification problem. \u6839\u636e\u6570\u636e\u63cf\u8ff0 \u6570\u636e\u96c6\u5df2\u88ab\u5220\u9664\u91cd\u590d\u6570\u636e \u4f46\u662f \u6211\u4eec\u7684\u6d4b\u8bd5\u4e0d\u80fd\u8bc1\u5b9e\u8fd9\u4e00\u70b9 For the entire dataset when the slide level label was unclear during the inspection of the H E stained slide an additional WSI with a consecutive tissue section immunohistochemically stained for cytokeratin was used to confirm the classification. csv\u8fd9\u4e2a\u6587\u4ef6\u4e2d \u63d0\u53d6\u6d4b\u8bd5\u96c6\u7684id \u5e76\u4fdd\u6301\u539f\u6709\u7684\u987a\u5e8f pandas \u8bfbcsv\u6587\u4ef6 \u63d0\u4ea4\u6587\u4ef6 dataframe. In case of an empty area the image would not be an outlier but equally valid negative sample. On a new machine\u5728\u4e00\u53f0\u65b0\u673a\u5668\u91cc We need to create an empty DataBunch and load it to a learner. com fastai fastai blob master fastai vision learner. export https github. com basveeling pcam and the only difference between these two is that all duplicate images have been removed. This means that our model has started overfitting during the small learning rates. eval to set dropout and batch normalization layers to evaluation mode before running inference \u63a8\u7406\u9884\u6d4b. \u6211\u5f3a\u70c8\u63a8\u8350Fastai \u4e3a\u7f16\u7801\u5458\u63d0\u4f9b\u5b9e\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u8bfe\u7a0b v3 I want to thank Martijn https www. ai V1 https docs. cn content qita 780160 random_state \u53ef\u590d\u73b0\u7684\u968f\u673a\u5207\u5206\u65b9\u5f0f fastai 1. html hook_output \u8f93\u5165oneBatch\u7ed9\u6784\u9020\u7684\u6a21\u578b\u505a\u9884\u6d4b \u53cd\u5411\u4f20\u64ad \u4e24\u4e2a\u89e3\u91ca \u6211\u6682\u65f6\u770b\u4e0d\u61c2 \u901a\u8fc7\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u5220\u9664\u94a9\u5b50 https forums. net qq_33453253 article details 79653546 \u7ed9\u7b2c\u4e00\u884c\u7b2c\u4e00\u4e2a\u8bbe\u7f6ey\u8f74\u6807\u7b7e Most incorrect or top losses \u5c55\u793a\u524d\u56db\u4e2a\u6700\u4e0d\u6b63\u786e \u635f\u5931\u503c\u6700\u5927 \u7684\u56fe\u4f8b \u53ea\u5c55\u793a\u56db\u5f20\u56fe \u5f97\u5230\u8be5\u56fe\u7247\u7684\u70ed\u56fe\u548c\u80cc\u666f \u56fe\u7247\u957f\u5bbd _ \u662f\u56fe\u7247 cl \u662f\u7c7b\u522bclass \u503c\u662f0 1 0\u6307\u4ee3\u8d1f\u4f8b 1\u6307\u4ee3\u80bf\u7624 \u8f6c\u6362\u4e3a\u6574\u6570\u578b \u5728\u7b2c1\u884c \u7b2ci 1\u5217\u4e0a\u5c55\u793a\u80cc\u666f\u56fe \u989c\u8272\u56fe\u4e3a\u7070\u5ea6\u56fe \u540c\u4e0a \u7b2c\u4e8c\u884c\u7b2ci\u4e2a\u8f74\u7684x\u8f74\u6ca1\u6709\u523b\u5ea6 \u7b2c\u4e8c\u884c\u7b2ci\u4e2a\u8f74\u7684y\u8f74\u6ca1\u6709\u523b\u5ea6 \u8bbe\u7f6e\u6807\u9898 f string https blog. py L114 \u968f\u673a\u751f\u6210\u4e00\u4e2a\u6574\u6570 interp \u662f ClassificationInterpretation \u5206\u7c7b\u6a21\u578b\u7684\u6548\u679c\u89e3\u91ca\u51fd\u6570 https docs. plot_losses lets take a second look at the confusion matrix. get_preds 2 57458 2 57458 2 \u5bf9\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u805a\u5408\u76f8\u52a0 \u4e5f\u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\u4e24\u4e2a\u77e9\u9635\u7684\u5143\u7d20\u5bf9\u5e94\u76f8\u52a0 \u5bf9\u8fd9\u51e0\u6b21\u6982\u7387\u4e4b\u548c\u6c42\u5e73\u5747 Next we will transform class probabilities to just tumor class probabilities \u7b2c\u4e00\u884c \u7b2c\u4e00\u4e2a\u6837\u672c\u7684\u53cd\u4f8b\u548c\u6b63\u4f8b\u7684\u6982\u7387 \u6982\u7387\u77e9\u9635\u7b2c\u4e8c\u5217 \u5373\u764c\u75c7\u7684\u6982\u7387 \u7b2c\u4e00\u4e2a\u6837\u672c\u662f\u6b63\u4f8b \u764c\u75c7 \u7684\u6982\u7387 If we wanted to get the predicted class argmax would get the index of the max \u987a\u7740\u7b2c\u4e00\u4e2a\u7ef4\u5ea6 \u884c \u6c42\u6700\u5927\u503c\u7684\u5750\u6807 \u5373\u6c42\u6bcf\u4e2a\u6837\u672c\u7684\u6982\u7387\u6700\u5927\u7684\u5750\u6807 0\u62161 https docs. My initial results with 32 x 32px size showed worse performance than with 48 x 48px but I haven t done a search for optimal size. Sylvian Gugger\u5199\u4e86\u4e00\u7bc7\u975e\u5e38\u660e\u786e\u7684\u6587\u7ae0\u89e3\u91caLeslie\u7684\u5efa\u8bae \u6211\u5efa\u8bae\u4f60\u9605\u8bfb First we find the optimal learning rate and weight decay values. html DatasetType im \u662f\u56fe\u7247 cl \u662f\u7c7b\u522bclass \u503c\u662f0 1 0\u6307\u4ee3\u8d1f\u4f8b 1\u6307\u4ee3\u80bf\u7624 \u5c06\u56fe\u7247\u4ecepytorch\u683c\u5f0f\u8f6c\u4e3anumpy\u683c\u5f0f https docs. Train and Validation augmentations \u5982\u679c\u6211\u4eec\u4e0d\u5728\u8fd9\u91cc\u5e94\u7528\u6570\u636e\u589e\u5f3a \u9700\u8981\u5c06\u4e24\u4e2a\u7a7a\u5217\u8868\u8bbe\u7f6e\u7ed9tfms \u5c06\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6 \u6d4b\u8bd5\u96c6\u6346\u7ed1\u6210\u4e00\u4e2a\u5bf9\u8c61 convert to databunch https docs. 5 times more negative images than positives. Large batch sizes may run out of GPU memory \u6307\u5b9a\u6279\u91cf\u5927\u5c0f \u8fd9\u4e2a\u53c2\u6570\u5bf9\u786c\u4ef6\u6709\u9650\u5236 \u5927\u6279\u91cf\u53ef\u80fd\u4f1a\u8017\u5c3dGPU\u5185\u5b58 input size is the crop size \u8f93\u5165\u7684\u56fe\u50cf\u5c3a\u5bf8 this will extrat the model name as the model file name e. io 2019 03 15 ImageDataBunch. pdf \u8f83\u5c0f\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u7ed3\u6784\u4f3c\u4e4e\u9700\u8981\u8f83\u5927\u7684\u6743\u91cd\u8870\u51cf\u503c \u800c\u8f83\u5927\u7684\u6570\u636e\u96c6\u548c\u8f83\u6df1\u7684\u6a21\u578b\u7ed3\u6784\u4f3c\u4e4e\u9700\u8981\u8f83\u5c0f\u7684\u503c We can select the learning rate around 2e 2 where it is close to the bottom but still descending. \u5b58\u50a8\u6a21\u578b\u63a8\u7406\u9884\u6d4b\u6240\u9700\u8981\u7684\u6700\u5c11\u7684\u4fe1\u606f https docs. Going for a deeper model architecture will start overfitting faster. html forward and backward function hooks fastai \u56de\u8c03 https pouannes. 220k\u8bad\u7ec3\u56fe\u50cf\u548c57k\u8bc4\u4f30\u56fe\u50cf \u6570\u636e\u96c6\u662fPCam\u6570\u636e\u96c6\u7684\u5b50\u96c6 \u8fd9\u4e24\u8005\u4e4b\u95f4\u7684\u552f\u4e00\u533a\u522b\u662f\u6240\u6709\u91cd\u590d\u7684\u56fe\u50cf\u90fd\u5df2\u88ab\u5220\u9664 PCam\u6570\u636e\u96c6\u6765\u81eaCamelyon16 Challenge\u6570\u636e\u96c6 \u5176\u4e2d\u5305\u542b400\u4e2aH E\u67d3\u8272\u6807\u8bb0\u7684\u6dcb\u5df4\u7ed3\u5207\u7247\u7684\u6574\u4e2a\u8f7d\u73bb\u7247\u56fe\u50cf \u4f7f\u752840\u500d\u7269\u955c\u57282\u4e2a\u4e0d\u540c\u7684\u4e2d\u5fc3\u91c7\u96c6\u548c\u6570\u5b57\u5316 PCam\u7684\u6570\u636e\u96c6 \u5305\u62ec\u6b64\u6570\u636e\u96c6 \u4f7f\u752810\u500d\u6b20\u91c7\u6837\u6765\u589e\u52a0\u89c6\u573a \u4ece\u800c\u5f97\u52302. ai\u89c6\u89c9\u6a21\u5757\u548ctorchvision\u6a21\u5757 We load the images to an ImageDataBunch for the training. org docs stable torchvision models. html cnn_learner 1cycle policy 1cycle\u7b56\u7565 Leslie \u5e76\u4e0d\u5efa\u8bae\u76f4\u63a5\u4ece\u5927\u5b66\u4e60\u7387\u5f00\u59cb \u76f8\u53cd \u4ed6\u8ba4\u4e3a\u5e94\u8be5\u4ece\u4f4e\u5230\u9ad8 \u7f13\u6162\u5730\u7ebf\u6027\u63d0\u5347\u5b66\u4e60\u7387 \u7136\u540e\u518d\u7528\u76f8\u540c\u7684\u65f6\u95f4\u7f13\u6162\u5730\u964d\u4f4e\u56de\u6765 https blog. Then it slows back down near the end. models import pytorch\u89c6\u89c9\u6a21\u5757 \u5b9a\u4e49\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc \u7a7a\u6570\u636e densenet169 \u6743\u91cd\u91c7\u7528MODEL_PATH opencv This function will convert image to the prediction format \u8f6c\u6362\u56fe\u50cf\u683c\u5f0f\u5230\u6a21\u578b\u60f3\u8981\u7684\u683c\u5f0f OpenCV reads the image in bgr format by default \u7528opencv\u8bfb\u53d6\u56fe\u7247 \u901a\u9053\u683c\u5f0f\u9ed8\u8ba4\u662fbgr cv2. Large or irregular nucleolus\u5927\u6216\u4e0d\u89c4\u5219\u7684\u6838\u4ec1. io blog callbacks fastai we hook into the convolutional part m 0 of the model \u521b\u5efa\u4e00\u4e2a\u4fdd\u5b58\u4e86\u6a21\u578b\u6fc0\u6d3b\u51fd\u6570\u8f93\u51fa\u7684\u94a9\u5b50 https docs. \u5bf9\u4e8e\u4f5c\u4e3a\u4f18\u5316\u5668\u7684L2\u60e9\u7f5a\u9879\u7684 \u6743\u91cd\u8870\u51cf Leslie\u5efa\u8bae\u9009\u62e9\u6700\u5927\u5e76\u7684\u4ecd\u53ef\u4ee5\u8ba9\u6211\u4eec\u4ee5\u9ad8\u5b66\u4e60\u7387\u8bad\u7ec3\u7684\u8870\u51cf\u6743\u91cd \u56e0\u6b64\u6211\u4eec\u75281e 2 1e 4 1e 6\u8870\u51cf\u6743\u91cd\u8fdb\u884c\u5c0f\u578b\u7f51\u683c\u641c\u7d22 \u9009\u62e9\u6700\u5927\u5b66\u4e60\u7387\u548c\u8870\u51cf\u6743\u91cd\u7684\u601d\u8003 \u5148\u9009\u6700\u5927\u8870\u51cf\u6743\u91cd \u518d\u9009\u6700\u5927\u5b66\u4e60\u7387 We want to select the largest weight decay that gets to a low loss and has the highest learning rate before shooting up. Failing to do this will yield inconsistent inference results. split\u51fd\u6570\u5206\u79bb\u5f97\u5230\u5404\u4e2a\u901a\u9053\u7684\u7070\u5ea6\u503c \u5355\u901a\u9053\u56fe\u50cf https blog. split_by_idx \u6839\u636eval_idx\u62c6\u5206\u8bad\u7ec3\u96c6\u5408\u548c\u9a8c\u8bc1\u96c6 How to split in train valid \u8bbe\u7f6e\u9a8c\u8bc1\u96c6\u7684\u6807\u7b7e Where are the labels \u8bbe\u7f6e\u6d4b\u8bd5\u96c6 dataframe pointing to the test set \u6570\u636e\u589e\u5f3a We have our custom transformations implemented in the image loader but we could apply transformations also here \u6211\u4eec\u5728\u4e4b\u524d\u7684\u56fe\u50cf\u52a0\u8f7d\u5668\u4e2d\u5b9e\u73b0\u4e86\u81ea\u5b9a\u4e49\u7684\u6570\u636e\u589e\u5f3a \u4f46\u6211\u4eec\u4e5f\u53ef\u4ee5\u5728\u8fd9\u91cc\u5e94\u7528\u6570\u636e\u589e\u5f3a Even though we don t apply transformations here we set two empty lists to tfms. \u6dcb\u5df4\u7ed3\u662f\u5c0f\u817a\u4f53 \u53ef\u8fc7\u6ee4\u6dcb\u5df4\u7cfb\u7edf\u4e2d\u7684\u6db2\u4f53 \u5b83\u4eec\u662f\u4e73\u817a\u764c\u53ef\u80fd\u4f20\u64ad\u7684\u7b2c\u4e00\u4e2a\u4f4d\u7f6e \u6dcb\u5df4\u7ed3\u8f6c\u79fb\u7684\u7ec4\u7ec7\u5b66\u8bc4\u4f30\u662fTNM\u5206\u7c7b\u4e2d\u786e\u5b9a\u4e73\u817a\u764c\u5206\u671f\u7684\u4e00\u90e8\u5206 TNM\u5206\u7c7b\u662f\u7528\u4e8e\u5206\u7c7b\u764c\u75c7\u6269\u6563\u7a0b\u5ea6\u7684\u5168\u7403\u516c\u8ba4\u6807\u51c6 \u75c5\u7406\u5b66\u5bb6\u7684\u8bca\u65ad\u7a0b\u5e8f\u7e41\u7410\u4e14\u8017\u65f6 \u56e0\u4e3a\u5fc5\u987b\u68c0\u67e5\u5927\u9762\u79ef\u7684\u7ec4\u7ec7\u5e76\u4e14\u5f88\u5bb9\u6613\u9519\u8fc7\u5c0f\u7684\u8f6c\u79fb\u764c\u7ec6\u80de Useful links for background knowledge\u53c2\u8003\u94fe\u63a5 Patch Camelyon PCam https github. com gigascience article 7 6 giy065 5026175 Data visualizationWe can see that the negative positive ratio is not entirely 50 50 as there are 130k negatives and 90k negatives. png Example of a metastatic region\u8f6c\u79fb\u764c\u4f8b\u5b50 https camelyon17. predict img 0 \u662fnegative \u5f53int learner. The probability ranges from 0 to 1. ai t lesson 6 advanced discussion 31442 2 We can create a utility function for getting a validation image with an activation map \u5de5\u5177\u51fd\u6570 \u753b\u51fa\u4e00\u4e2a\u9a8c\u8bc1\u96c6\u56fe\u7247\u7684\u70ed\u56fe \u8fd4\u56de\u9a8c\u8bc1\u96c6\u56fe\u50cf\u548c\u6fc0\u6d3b\u6620\u5c04 \u70ed\u56fe this gets the model \u8bc4\u4f30\u6a21\u578b https pytorch. 9942634117111718 matplotlib plt. ImageList API docs https docs. While we are calculating statistics we can check if there are images that have a very low maximum pixel intensity almost totally black or very high minimum pixel intensity almost totally white. html \u7b2c\u4e00\u884c\u7b2ci\u4e2a\u8f74\u7684x\u8f74\u6ca1\u6709\u523b\u5ea6 \u7b2c\u4e00\u884c\u7b2ci\u4e2a\u8f74\u7684y\u8f74\u6ca1\u6709\u523b\u5ea6 \u8bbe\u7f6e\u6807\u9898 f string https blog. \u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8d1f \u6b63\u6bd4\u7387\u4e0d\u5b8c\u5168\u662f50 50 \u56e0\u4e3a\u6709130k\u7684\u8d1f\u6570\u548c90k\u7684\u8d1f\u6570 \u8be5\u6bd4\u7387\u63a5\u8fd160 40 \u8fd9\u610f\u5473\u7740\u8d1f\u50cf\u7684\u6570\u91cf\u662f\u6b63\u6570\u76841. \u8be5\u6a21\u578b\u5df2\u7ecf\u8868\u73b0\u826f\u597d \u73b0\u5728 \u5f53\u6211\u4eec\u89e3\u51bb\u5df2\u7ecf\u4f7f\u7528\u5927\u91cf\u4e00\u822c\u56fe\u50cf\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u5e95\u5c42 \u524d\u9762\u7684layer \u4ee5\u68c0\u6d4b\u5e38\u89c1\u7684\u5f62\u72b6\u548c\u56fe\u6848\u65f6 \u6240\u6709\u6743\u91cd\u90fd\u4f1a\u88ab\u8c03\u6574 \u6211\u4eec\u73b0\u5728\u5e94\u8be5\u4ee5\u66f4\u4f4e\u7684\u5b66\u4e60\u7387\u8bad\u7ec3 We can see that the validation performance has separated from the training performance a bit in the end of the cycle. html https matplotlib. bool 1 \u662ftrue bool 0 \u662ffalse 1 \u5c06\u56fe\u50cf\u6bcf\u4e00\u884c\u7684\u50cf\u7d20\u505a\u5012\u5e8f \u76f8\u5f53\u4e8e\u6c34\u5e73\u65b9\u5411\u7ffb\u8f6c 1 \u5c06\u56fe\u50cf\u6bcf\u4e00\u5217\u7684\u50cf\u7d20\u505a\u5012\u5e8f \u76f8\u5f53\u4e8e\u5782\u76f4\u65b9\u5411\u7ffb\u8f6c Random brightness \u968f\u673a\u4eae\u5ea6 \u7ed9\u56fe\u50cf\u6bcf\u4e2a\u50cf\u7d20\u52a0\u4e0a\u4e00\u4e2a\u5728 1 1 \u95f4\u7684\u968f\u673a\u503c Random contrast \u968f\u673a\u5bf9\u6bd4\u5ea6 \u7ed9\u56fe\u50cf\u6bcf\u4e2a\u50cf\u7d20\u4e58\u4e0a\u4e00\u4e2a\u5728 0 2 \u95f4\u7684\u968f\u673a\u503c clip values to 0 1 range \u56fe\u50cf\u4e2d\u50cf\u7d20\u503c\u6bd40\u5c0f\u7684\u53d8\u4e3a0 \u6bd41\u5927\u7684\u53d8\u4e3a1 matpoltlib subplots \u521b\u5efa\u4e00\u7cfb\u5217\u7684\u5b50\u56fe nrows\u5b50\u56fe\u884c\u7684\u4e2a\u6570 ncols\u5b50\u56fe\u5217\u7684\u4e2a\u6570 figsize\u753b\u5e03\u5c3a\u5bf8 axes\u5b50\u56fe\u7684\u8f74 fig\u56fe \u6807\u9898 Negatives \u53cd\u4f8b \u6ca1\u6709\u8f6c\u79fb\u764c idx\u662f\u968f\u673a\u6392\u5217\u7684\u524d\u4e94\u4e2a\u53cd\u4f8b\u7684\u56fe\u7247\u540d i\u662f0 4\u7684\u7d22\u5f15 \u5c06\u8bad\u7ec3\u96c6\u8def\u5f84\u4e0e\u53cd\u4f8b\u56fe\u7247\u540d \u62fc\u63a5\u8d77\u6765 \u5728\u5750\u6807\u4e3a 0 i \u7684\u8f74\u4e0a\u753b\u51fa\u8fd9\u5f20\u53cd\u4f8b\u56fe \u7ed9\u5750\u6807\u4e3a 0 0 \u7684\u8f74\u753b\u51fa\u5176y\u8f74\u6807\u9898 Positives \u6b63\u4f8b \u6709\u8f6c\u79fb\u764c idx\u662f\u968f\u673a\u6392\u5217\u7684\u524d\u4e94\u4e2a\u6b63\u4f8b\u7684\u56fe\u7247\u540d i\u662f0 4\u7684\u7d22\u5f15 \u5c06\u8bad\u7ec3\u96c6\u8def\u5f84\u4e0e\u6b63\u4f8b\u56fe\u7247\u540d \u62fc\u63a5\u8d77\u6765 \u5728\u5750\u6807\u4e3a 1 i \u7684\u8f74\u4e0a\u753b\u51fa\u8fd9\u5f20\u6b63\u4f8b\u56fe \u7ed9\u5750\u6807\u4e3a 1 0 \u7684\u8f74\u753b\u51fa\u5176y\u8f74\u6807\u9898 matpoltlib subplots \u521b\u5efa\u4e00\u7cfb\u5217\u7684\u5b50\u56fe nrows\u5b50\u56fe\u884c\u7684\u4e2a\u6570 ncols\u5b50\u56fe\u5217\u7684\u4e2a\u6570 figsize\u753b\u5e03\u5c3a\u5bf8 axes\u5b50\u56fe\u7684\u8f74 fig\u56fe \u6807\u9898 Negatives \u53cd\u4f8b \u6ca1\u6709\u8f6c\u79fb\u764c idx\u662f\u968f\u673a\u6392\u5217\u7684\u7b2c\u4e00\u4e2a\u53cd\u4f8b\u7684\u56fe\u7247\u540d i\u662f\u7d22\u5f150 \u968f\u673a\u56fe\u50cf\u589e\u5f3a\u4e94\u6b21 \u770b\u6548\u679c \u5c06\u8bad\u7ec3\u96c6\u8def\u5f84\u4e0e\u53cd\u4f8b\u56fe\u7247\u540d \u62fc\u63a5\u8d77\u6765 \u8c03\u7528\u4e4b\u524d\u81ea\u5b9a\u4e49\u7684readCroppedImage\u65b9\u6cd5 As we count the statistics we can check if there are any completely black or white images \u5728\u7edf\u8ba1\u6570\u636e\u65f6 \u6211\u4eec\u53ef\u4ee5\u68c0\u67e5\u662f\u5426\u6709\u5b8c\u5168\u9ed1\u8272\u6216\u767d\u8272\u7684\u56fe\u50cf If no pixel reaches this threshold image is considered too dark \u5982\u679c\u4e00\u5f20\u56fe\u7247\u6ca1\u6709\u50cf\u7d20\u9ad8\u4e8e\u6b64\u9608\u503c \u5219\u56fe\u50cf\u88ab\u8ba4\u4e3a\u592a\u6697 If no pixel is under this threshold image is considerd too bright \u5982\u679c\u4e00\u5f20\u56fe\u7247\u6ca1\u6709\u50cf\u7d20\u4f4e\u4e8e\u6b64\u9608\u503c \u5219\u8ba4\u4e3a\u56fe\u50cf\u592a\u4eae tqdm_notebook\u53ef\u7528\u4e8e\u663e\u793a\u8fdb\u5ea6\u6761 \u5176\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u63d0\u793a\u7528\u7684 \u5c06\u8bad\u7ec3\u96c6\u8def\u5f84\u4e0e\u53cd\u4f8b\u56fe\u7247\u540d \u62fc\u63a5\u8d77\u6765 \u6267\u884c\u81ea\u5b9a\u4e49\u7684\u51fd\u6570 \u4e0d\u505a\u6570\u636e\u589e\u5f3a \u4f46\u505a\u5f52\u4e00\u5316 \u5e76\u5c06\u56fe\u50cf\u7684\u5f62\u72b6\u4ece 96 96 3 \u8f6c\u6210 9216 3 is this too dark \u5982\u679c\u56fe\u50cf\u592a\u6697\u4e86 \u5982\u679c\u56fe\u50cf\u50cf\u7d20\u503c\u6700\u5927\u7684\u4e5f\u6bd4 \u8bbe\u5b9a\u7684\u6700\u5c0f\u9608\u503c\u5c0f \u628a\u8fd9\u5f20\u56fe\u7247\u7684\u540d\u5b57\u8bb0\u5f55\u5728\u5c0f\u672c\u672c\u4e0a do not include in statistics \u8df3\u8fc7\u672c\u6b21\u5faa\u73af \u4e0d\u5c06\u5176\u7eb3\u5165\u6c42\u5e73\u5747 \u6c42\u65b9\u5dee is this too bright \u5982\u679c\u56fe\u50cf\u592a\u4eae\u4e86 \u5982\u679c\u56fe\u50cf\u50cf\u7d20\u503c\u6700\u5c0f\u7684\u4e5f\u6bd4 \u8bbe\u5b9a\u7684\u6700\u5927\u9608\u503c\u5927 \u628a\u8fd9\u5f20\u56fe\u7247\u7684\u540d\u5b57\u8bb0\u5f55\u5728\u5c0f\u672c\u672c\u4e0a do not include in statistics \u8df3\u8fc7\u672c\u6b21\u5faa\u73af \u4e0d\u5c06\u5176\u7eb3\u5165\u6c42\u5e73\u5747 \u6c42\u65b9\u5dee \u6bcf\u4e2a\u901a\u9053\u7684\u50cf\u7d20\u503c\u7684\u5e73\u5747\u4e4b\u548c \u6700\u540e\u7ef4\u5ea6\u662f 3 \u6bcf\u4e2a\u901a\u9053\u7684\u50cf\u7d20\u503c\u7684\u5e73\u65b9\u7684\u5e73\u5747\u4e4b\u548c \u6700\u540e\u7ef4\u5ea6\u662f 3 \u6b63\u5e38\u56fe\u7247\u7684\u4e2a\u6570 \u6bcf\u4e2a\u901a\u9053\u7684\u50cf\u7d20\u503c\u5e73\u5747\u503c \u6700\u540e\u7ef4\u5ea6\u662f 3 \u6bcf\u4e2a\u901a\u9053\u7684\u50cf\u7d20\u503c\u6807\u51c6\u5dee \u6700\u540e\u7ef4\u5ea6\u662f 3 \u592a\u6697\u7684\u56fe\u50cf\u7684\u4e2a\u6570 \u592a\u4eae\u7684\u56fe\u50cf\u7684\u4e2a\u6570 \u6253\u5370\u592a\u6697\u56fe\u7247\u7684\u540d\u5b57 \u6253\u5370\u592a\u4eae\u56fe\u7247\u7684\u540d\u5b57 matpoltlib subplots \u521b\u5efa\u4e00\u7cfb\u5217\u7684\u5b50\u56fe nrows\u5b50\u56fe\u884c\u7684\u4e2a\u6570 ncols\u5b50\u56fe\u5217\u7684\u4e2a\u6570 figsize\u753b\u5e03\u5c3a\u5bf8 axes\u5b50\u56fe\u7684\u8f74 fig\u56fe \u6807\u9898 Too dark \u5c55\u793a\u592a\u6697\u7684\u56fe\u7247 \u904d\u5386\u592a\u6697\u7684\u56fe\u50cf \u6700\u591a\u53ea\u5c55\u793a6\u5f20\u56fe\u7247 \u591a\u4e86\u56fe\u7247\u592a\u5c0f\u4e0d\u597d\u770b\u4e86 \u592a\u6697\u7684\u56fe\u50cf\u7684\u6807\u8bb0 \u5c06\u8bad\u7ec3\u96c6\u8def\u5f84\u4e0e\u56fe\u7247\u540d \u62fc\u63a5\u8d77\u6765 \u5c55\u793a\u8fd9\u5f20\u56fe\u7247 \u8c03\u7528readCroppedImage\u7684\u539f\u56e0\u662f \u9700\u8981rgb\u683c\u5f0f\u7684\u56fe\u50cf\u6570\u636e \u7ed9\u5750\u6807\u4e3a 0 i \u56fe\u50cf\u8bbe\u7f6e\u6807\u9898 \u7ed9\u5750\u6807\u4e3a 0 0 \u56fe\u50cf\u8bbe\u7f6ey\u8f74\u6807\u9898 \u904d\u5386 0 1 0 5 \u7684\u8f74 hide axes if there are less than 6 \u8ba9\u8fd9\u4e9b\u6ca1\u6709\u56fe\u7247\u7684\u8f74\u4e0d\u663e\u793a Too bright \u5c55\u793a\u592a\u4eae\u7684\u56fe\u7247 \u904d\u5386\u592a\u4eae\u7684\u56fe\u50cf \u6700\u591a\u53ea\u5c55\u793a6\u5f20\u56fe\u7247 \u591a\u4e86\u56fe\u7247\u592a\u5c0f\u4e0d\u597d\u770b\u4e86 \u592a\u4eae\u7684\u56fe\u50cf\u7684\u6807\u8bb0 \u5c06\u8bad\u7ec3\u96c6\u8def\u5f84\u4e0e\u56fe\u7247\u540d \u62fc\u63a5\u8d77\u6765 \u5c55\u793a\u8fd9\u5f20\u56fe\u7247 \u8c03\u7528readCroppedImage\u7684\u539f\u56e0\u662f \u9700\u8981rgb\u683c\u5f0f\u7684\u56fe\u50cf\u6570\u636e \u7ed9\u5750\u6807\u4e3a 0 i \u56fe\u50cf\u8bbe\u7f6e\u6807\u9898 \u7ed9\u5750\u6807\u4e3a 1 0 \u56fe\u50cf\u8bbe\u7f6ey\u8f74\u6807\u9898 \u904d\u5386\u6ca1\u6709\u56fe\u50cf\u7684\u8f74 \u5f53\u7136\u8fd9\u91cc\u592a\u4eae\u7684\u56fe\u7247\u6570\u91cf\u521a\u597d\u662f6 range 6 6 \u5c31\u4e0d\u7528\u904d\u5386\u4e86 hide axes if there are less than 6 \u8ba9\u8fd9\u4e9b\u6ca1\u6709\u56fe\u7247\u7684\u8f74\u4e0d\u663e\u793a \u5206\u5272\u8bad\u7ec3\u96c6 \u6d4b\u8bd5\u96c6\u7684\u65b9\u6cd5 we read the csv file earlier to pandas dataframe now we set index to id so we can perform \u6211\u4eec\u4e4b\u524d\u8bfb\u53d6\u4e86pandas dataframe\u7684csv\u6587\u4ef6 \u73b0\u5728\u6211\u4eec\u5c06id\u5217\u4f5c\u4e3a\u7d22\u5f15\u5217 \u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u63a5\u7740\u5f80\u4e0b\u6267\u884c\u4e86 If removing outliers uncomment the four lines below\u5982\u679c\u8981\u5220\u9664\u79bb\u7fa4\u503c \u5f02\u5e38\u503c \u9700\u8981\u5c06\u4e0b\u9762\u56db\u53e5\u8bdd\u53d6\u6d88\u6ce8\u91ca print Before removing outliers we had 0 training samples. \u521b\u5efa\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5668 learner\u662f\u4e00\u4e2aLearner\u5bf9\u8c61 1cycle\u7b56\u7565 \u5f97\u5230\u5728\u4e0d\u540c\u6743\u91cd\u8870\u51cf\u7cfb\u6570\u548c\u5b66\u4e60\u7387\u4e0b \u7684\u635f\u5931\u503c We can use lr_find with different weight decays and record all losses so that we can plot them on the same graph \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5177\u6709\u4e0d\u540c\u8870\u51cf\u6743\u91cd\u7684lr_find\u5e76\u8bb0\u5f55\u6240\u6709\u635f\u5931\u503c \u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u5728\u540c\u4e00\u56fe\u8868\u4e0a\u7ed8\u5236\u5b83\u4eec Number of iterations is by default 100 but at this low number of itrations there might be too much variance \u9ed8\u8ba4\u60c5\u51b5\u4e0b \u8fed\u4ee3\u6b21\u6570\u4e3a100 \u4f46\u5982\u679c\u6b21\u6570\u6bd4\u8f83\u5c11 \u53ef\u80fd\u4f1a\u6709\u6bd4\u8f83\u9ad8\u7684\u65b9\u5dee from random sampling that makes it difficult to compare WD s. \u8ba9\u6211\u4eec\u518d\u770b\u4e00\u4e0b\u6df7\u6dc6\u77e9\u9635 \u770b\u770b\u6211\u4eec\u6539\u8fdb\u4e86\u591a\u5c11 \u5206\u7c7b\u6a21\u578b\u7684\u6548\u679c\u89e3\u91ca\u51fd\u6570 https docs. html image2np attach hooks \u9644\u4e0a\u94a9\u5b50 \u524d\u9762\u81ea\u5b9a\u4e49\u7684\u65b9\u6cd5 get convolutional activations and average from channels \u8fd9\u4e2a\u94a9\u5b50\u628a\u6fc0\u6d3b\u7684\u503c\u5b58\u50a8\u5230\u4e86stored\u91cc\u9762 \u5e76\u653e\u7f6e\u5230cpu\u91cc\u9762 Grad CAM 0 0 \u8fd9\u4e2a\u94a9\u5b50\u628a\u6fc0\u6d3b\u7684\u503c\u5b58\u50a8\u5230\u4e86stored\u91cc\u9762 \u5e76\u653e\u7f6e\u5230cpu\u91cc\u9762 \u6c42\u5747\u503c mult\u662f\u70ed\u70b9 cvIm\u662f\u80cc\u666f \u751f\u6210\u70ed\u56fe\u7684\u5176\u4ed6\u4f8b\u5b50 \u65b9\u6cd5\u633a\u7c7b\u4f3c\u7684 \u6709\u5f88\u591a\u89e3\u91ca \u6709\u65f6\u95f4\u4e86\u7ec6\u770b \u76f8\u4fe1\u80fd\u89e3\u51b3\u5f88\u591a\u7591\u60d1 https dhruvs. com qitvision a complete ml pipeline fast ai \u8fd9\u662f\u8fd9\u7bc7kernel\u7684\u5730\u5740 \u6b63\u5728Commit https www. It is important that the loss is still descending where we select the learning rate. shape 0 \u6253\u5370 \u5728\u5220\u9664\u79bb\u7fa4\u503c\u4e4b\u540e \u8bad\u7ec3\u96c6\u8fd8\u6709\u591a\u5c11\u6761\u6570\u636e \u8fd4\u56de\u8bad\u7ec3\u96c6\u7684\u7d22\u5f15 \u56fe\u50cf\u540d\u5b57 \u6570\u7ec4 \u8fd4\u56de\u8bad\u7ec3\u56fe\u50cf\u5bf9\u5e94\u7684\u6807\u8bb0 split this function returns more than we need as we only need the validation indexes for fastai split \u8fd9\u4e2a\u51fd\u6570\u8fd4\u56de\u7684\u503c \u6bd4\u6211\u4eec\u9700\u8981\u7684\u591a \u56e0\u4e3a\u6211\u4eec\u53ea\u9700\u8981\u9a8c\u8bc1\u96c6\u7d22\u5f15 val_idx \u7ed9fastai\u5e93 Split the train and the validation set for the fitting\u5c06\u539f\u8bad\u7ec3\u96c6\u5206\u5272\u6210\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6 test_size\u662f\u5360\u539f\u6765\u6570\u636e\u96c6\u7684\u6bd4\u4f8b stratify\u662f\u4e3a\u4e86\u4fdd\u6301split\u524d\u7c7b\u7684\u5206\u5e03 https www. This is called test time augmentation TTA and it can improve our results if we run inference multiple times for each image and average out the predictions. 0 \u66f4\u5feb \u66f4\u7b80\u5355 \u66f4\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u5e93 \u5305\u542b\u5b9a\u4e49\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6a21\u578b\u6240\u5fc5\u987b\u7684\u51fd\u6570 import all the models from torchvision specify model architecture densenet169 seems to perform well for this data but you could experiment \u6307\u5b9a\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u7ed3\u6784 densenet169\u4f3c\u4e4e\u5bf9\u8fd9\u4e9b\u6570\u636e\u8868\u73b0\u826f\u597d \u4e0d\u4fe1\u60a8\u53ef\u4ee5\u8bd5\u8bd5 specify batch size hardware restrics this one. \u4f60\u597d \u6211\u662f\u4e03\u6708\u5728\u7ebffan print julyfan \u662f\u4e3a\u4e86\u9632\u4e2d\u65ad \u6587\u6863\u4f1a\u4e0d\u5b9a\u671f\u66f4\u65b0 \u5efa\u8bae\u53bbkaggle fork\u8fd0\u884c\u4ee3\u7801 \u53ef\u4ee5\u4e0d\u7528\u4e0b\u8f7d\u6570\u636e\u96c6\u548c\u914d\u7f6e\u73af\u5883 \u8fd8\u6709\u514d\u8d39\u7684gpu \u6709\u95ee\u9898\u8bf7\u6307\u51fa \u611f\u8c22 \u8f6c\u8f7d\u8bf7\u6ce8\u660e\u51fa\u5904 \u8fd9\u662f\u539f\u4f5c\u8005\u7684kernel https www. predict \u5f53int learner. html sklearn metrics roc curve Compute ROC area \u8ba1\u7b97\u66f2\u9762\u4e0b\u9762\u79ef https scikit learn. Abundant mitotic figures\u4e30\u5bcc\u7684\u6709\u4e1d\u5206\u88c2\u6570\u5b57 Cells in architectural arrangements seen in malignancy highly variable dependent on tumour type and differentiation\u5728\u6076\u6027\u80bf\u7624\u4e2d\u770b\u5230\u7684\u5efa\u7b51\u5b89\u6392\u4e2d\u7684\u7ec6\u80de \u9ad8\u5ea6\u53ef\u53d8 \u4f9d\u8d56\u4e8e\u80bf\u7624\u7c7b\u578b\u548c\u5206\u5316 Gland formation. This means that we start to look for the minima within that area. The ratio is closer to 60 40 meaning that there are 1. 0 reference generated numpy. For example if we would train a dog breed classifier and all of our images of a certain dog breed would have been taken in a dog show competition. \u6211\u4eec\u5c06\u4f7f\u7528OpenCV\u8fdb\u884c\u56fe\u50cf\u9884\u5904\u7406 \u56e0\u4e3a\u6839\u636e\u6211\u7684\u7ecf\u9a8c OpenCV\u6bd4PIL\u6216scikit image\u5feb\u5f97\u591a To see the effects of our augmentation we can plot one image multiple times\u8981\u67e5\u770b\u6211\u4eec\u7684\u589e\u5f3a\u6548\u679c \u6211\u4eec\u53ef\u4ee5\u591a\u6b21\u7ed8\u5236\u4e00\u5e45\u56fe\u50cf. ai software library that is built on PyTorch https pytorch. \u6211\u4eec\u9700\u8981\u63d0\u4ea4\u6bcf\u4e2a\u6d4b\u8bd5\u6837\u672c\u662f\u764c\u75c7\u7684\u6982\u7387 \u8fd9\u4e2a\u6982\u7387\u7684\u53d6\u503c\u8303\u56f4\u662f 0 1 \u90e8\u7f72Deploy \u6216\u6d4b\u8bd5example Now that we have a working model. Compute image statistics\u8ba1\u7b97\u56fe\u50cf\u6570\u636e\u7684\u7edf\u8ba1\u4fe1\u606f Do not use augmentation here \u5728\u8fd9\u91cc\u4e0d\u4f7f\u7528\u6570\u636e\u589e\u5f3a Calculating statistics will give channel averages of 0. Histological assessment of lymph node metastases is part of determining the stage of breast cancer in TNM classification which is a globally recognized standard for classifying the extent of spread of cancer. shape 0 \u6253\u5370 \u5728\u5220\u9664\u79bb\u7fa4\u503c\u4e4b\u524d \u6211\u4eec\u7684\u8bad\u7ec3\u96c6\u6709\u591a\u5c11\u6761\u6570\u636e train_df train_df. com lijiazhang p 10105722. net u014264373 article details 80487766 https scikit learn. \u6838\u591a\u5f62\u6027 \u5927\u5c0f \u5f62\u72b6\u548c\u67d3\u8272\u7684\u53d8\u5316 Nuclear atypia \u6838\u975e\u5178\u578b Nuclear enlargement\u6838\u6269\u5927. com fanhaobei a complete ml pipeline \u8fd9\u662f\u4e03\u6708\u5728\u7ebf\u7684\u7f51\u5740 https www. Otherwise the random initialization of the head weights could harm the relatively well performing pre trained weights of the model. Now if we would train further the model would only memorize features from the training set and the validation set performance would rise. This can potentially save us a lot of time from training with suboptimal hyperparameters. html ClassificationInterpretation \u753b\u51fa\u6df7\u6dc6\u77e9\u9635 https docs. ai is that it includes out of the box support for many recent advancements in deep learning research. one_batch denorm\u662fdenormalize\u7684\u504f\u51fd\u6570 https docs. \u968f\u673a\u62bd\u6837 \u5f88\u96be\u6bd4\u8f83WD \u6743\u91cd\u8870\u51cf \u7684\u6548\u679c \u6211\u5efa\u8bae\u4f7f\u7528\u81f3\u5c11300\u7684\u8fed\u4ee3\u8ba1\u6570\u4ee5\u83b7\u5f97\u66f4\u4e00\u81f4\u7684\u7ed3\u679c \u5b66\u4e60\u7387 \u635f\u5931\u503c \u8870\u51cf\u6743\u91cd \u8fed\u4ee3\u8f6e\u6570 WEIGHT DECAY 1e 6 \u8870\u51cf\u6743\u91cd\u662f1e 6 \u5e2e\u52a9\u4f60\u627e\u5230\u4e00\u4e2a\u6a21\u578b\u7684\u6700\u4f73\u5b66\u4e60\u7387 wd\u6743\u91cd\u8870\u51cf num_it \u8fed\u4ee3\u6b21\u6570 https github. org resources tnm Data understanding\u4e86\u89e3\u6570\u636e What data do we have available \u6211\u4eec\u6709\u54ea\u4e9b\u6570\u636e 220k training images and 57k evaluation images. org wiki Lymph_node_metastasis lymph node metastases can have these features \u5bf9\u4e8e\u8bad\u7ec3\u6709\u7d20\u7684\u75c5\u7406\u5b66\u5bb6\u6765\u8bf4 \u8f6c\u79fb\u764c\u5206\u7c7b\u53ef\u80fd\u4e0d\u662f\u4e00\u4ef6\u5bb9\u6613\u7684\u4e8b \u800c\u5bf9\u4e8e\u672a\u7ecf\u8bad\u7ec3\u7684\u4eba\u6765\u8bf4\u5219\u975e\u5e38\u56f0\u96be \u6839\u636eLibre\u75c5\u7406\u5b66 \u6dcb\u5df4\u7ed3\u8f6c\u79fb\u764c\u7ec6\u80de\u5177\u6709\u4ee5\u4e0b\u7279\u5f81 Foreign cell population key feature Classic location subcapsular sinuses \u5916\u6765\u7ec6\u80de\u7fa4 \u5173\u952e\u7279\u5f81 \u7ecf\u5178\u4f4d\u7f6e \u5305\u819c\u4e0b\u7aa6 Cells with cytologic features of malignancy\u5177\u6709\u6076\u6027\u80bf\u7624\u7ec6\u80de\u5b66\u7279\u5f81\u7684\u7ec6\u80de Nuclear pleomorphism variation in size shape and staining. I am not so sure about the dark image is it an outlier crop from badly exposed area or just some very large cell part filling the whole image. html \u8f93\u51fa\u56fe\u7247\u5217\u8868tensorImg \u7c7b\u522b\u5217\u8868cl https dejanbatanjac. html image2np \u5728\u7b2c\u4e09\u884c\u7b2ci\u4e2a\u8f74\u4e0a\u5c55\u793a\u8fd9\u5f20\u56fe\u7247 \u7b2c\u4e09\u884c\u7b2ci\u4e2a\u8f74\u7684x\u8f74\u6ca1\u6709\u523b\u5ea6 \u7b2c\u4e09\u884c\u7b2ci\u4e2a\u8f74\u7684y\u8f74\u6ca1\u6709\u523b\u5ea6 \u8bbe\u7f6e\u6807\u9898 \u7ed9\u7b2c\u4e09\u884c\u7b2c\u4e00\u4e2a\u8bbe\u7f6ey\u8f74\u6807\u7b7e interp ClassificationInterpretation. ai t how to get data val ds fnames in fastai v1 0 37231 create a batch from the one image one_item Get item into a batch. \u751f\u6210\u764c\u75c7\u9884\u6d4b\u6982\u7387\u5217\u8868 These are in the order of our test dataset and not necessarily in the same order as in sample_submission \u8fd9\u4e9b\u9884\u6d4b\u6982\u7387\u662f\u6309\u7167\u6211\u4eec\u6d4b\u8bd5\u96c6\u4e2d\u7684\u987a\u5e8f\u6392\u5217\u7684 \u7a0d\u540e\u8981\u91cd\u65b0\u6392\u5e8f numpy. There is a good chance that the model would learn to recognize the competition surroundings instead of doggy features with that breed. This saves the internal information classes etc need for inference in a file named export. merge\u51fd\u6570\u662f\u5408\u5e76\u5355\u901a\u9053\u6210\u591a\u901a\u9053 \u4e0d\u80fd\u5408\u5e76\u591a\u4e2a\u591a\u901a\u9053\u56fe\u50cf crop to center to the correct size and convert from 0 255 range to 0 1 range \u526a\u88c1\u56fe\u50cf\u7684\u91cd\u5fc3\u5230\u6b63\u786e\u7684\u5c3a\u5bf8 \u7136\u540e\u5c06\u50cf\u7d20\u503c\u4ece0 255 \u7f29\u653e\u5230 0 1 \u7684\u8303\u56f4 \u56fe\u50cf\u957f \u5bbd \u901a\u9053\u6570 H sz 2\u662f\u4e2d\u5fc3\u5c0f\u56fe\u7684x\u8f74\u7684\u5207\u7247\u7684\u8d77\u70b9 sz H sz 2\u662fx\u8f74\u7684\u5207\u7247\u7684\u7ec8\u70b9 H sz 2\u662f\u4e2d\u5fc3\u5c0f\u56fe\u7684y\u8f74\u7684\u5207\u7247\u7684\u8d77\u70b9 sz H sz 2 \u662fy\u8f74\u7684\u5207\u7247\u7684\u7ec8\u70b9 \u4fdd\u6301\u901a\u9053\u6570\u4e0d\u53d8 This ndarray image has to be converted to tensor before passing on as fastai Image we can use pil2tensor \u8fd9\u4e2andarray\u56fe\u50cf\u5fc5\u987b\u5728\u4f20\u9012\u4e3afastai Image\u4e4b\u524d\u8f6c\u6362\u4e3a\u5f20\u91cftensor\u683c\u5f0f \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528pil2tensor\u65b9\u6cd5\u5b9e\u73b0 pil2tensor\u628apil\u683c\u5f0f\u7684\u56fe\u7247\u6570\u7ec4\u8f6c\u6362\u4e3atorch\u683c\u5f0f\u7684\u56fe\u7247\u5f20\u91cf https docs. predict img 0 \u662f0\u65f6 classes int learner. Here we can see that the model has learned to distinguish tumor and negative sample and it s already performing well. This hypothesis could be confirmed by training models with varying crop sizes. This cannot be more than ORIGINAL_SIZE CROP_SIZE 2 \u968f\u673a\u5e73\u79fb0\u52302\u4e2a\u50cf\u7d20\u503c range 0 100 0 no change \u968f\u673a\u4eae\u5ea6 \u7ed9\u56fe\u50cf\u6bcf\u4e2a\u50cf\u7d20\u52a0\u4e0a\u4e00\u4e2a\u5728 1 1 \u95f4\u7684\u968f\u673a\u503c range 0 100 0 no change \u968f\u673a\u5bf9\u6bd4\u5ea6 \u7ed9\u56fe\u50cf\u6bcf\u4e2a\u50cf\u7d20\u4e58\u4e0a\u4e00\u4e2a\u5728 0 2 \u95f4\u7684\u968f\u673a\u503c 0 or 1 random turn to left or right\u5411\u5de6\u6216\u53f3\u65cb\u8f6c90\u5ea6 augmentations parameter is included for counting statistics from images where we don t want augmentations\u8fd9\u4e2a\u53c2\u6570\u53ef\u4ee5\u7528\u6765\u9009\u62e9\u662f\u5426\u4f7f\u7528\u6570\u636e\u589e\u5f3a OpenCV reads the image in bgr format by default \u7528opencv\u8bfb\u53d6\u56fe\u7247 \u901a\u9053\u683c\u5f0f\u9ed8\u8ba4\u662fbgr We flip it to rgb for visualization purposes \u6211\u4eec\u5c06\u56fe\u7247\u7ffb\u8f6c\u4e3argb\u683c\u5f0f\u7528\u4e8e\u89c2\u770b cv2. org Background TNM classification https www. This time we define the min and max lr of the cycle \u73b0\u5728 \u5b66\u4e60\u7387\u8f83\u4f4e \u8fd9\u6b21\u6211\u4eec\u5b9a\u4e49\u5faa\u73af\u7684\u6700\u5c0f\u548c\u6700\u5927\u5b66\u4e60\u7387 \u6839\u636e1cycle\u7b56\u7565\u62df\u5408\u6a21\u578b https docs. html _cl_int_plot_multi_top_losses Save the finetuned model \u4fdd\u5b58\u5fae\u8c03\u7684\u6a21\u578b if the model was better before finetuning uncomment this to load the previous stage \u5982\u679c\u5728\u5fae\u8c03\u4e4b\u524d\u7684\u6a21\u578b\u66f4\u597d \u5219\u53d6\u6d88\u6ce8\u91ca \u52a0\u8f7d\u524d\u4e00\u9636\u6bb5\u7684\u6a21\u578b \u9884\u6d4b \u9884\u6d4b\u6807\u8bb0 \u635f\u5931\u503c https docs. gov pubmed 21356829 H E stained sentinel lymph node sections of breast cancer patients the CAMELYON dataset https academic. html of Leslie s proposal that I recommend for reading. data https docs. \u4e3a\u4e86\u8bc4\u4f30\u6a21\u578b \u6211\u4eec\u5bf9\u6240\u6709\u6d4b\u8bd5\u56fe\u50cf\u8fdb\u884c\u63a8\u7406\u9884\u6d4b \u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a \u5373\u5bf9\u6d4b\u8bd5\u96c6\u4e2d\u7684\u56fe\u50cf\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u5e76\u591a\u6b21\u9884\u6d4b\u53d6\u5e73\u5747 \u6211\u4eec\u7684\u7ed3\u679c\u53ef\u80fd\u4f1a\u63d0\u9ad8 https www. drop labels too_dark_idx axis 0 \u5220\u9664\u8bad\u7ec3\u96c6\u4e2d\u592a\u9ed1\u7684\u56fe\u7247 train_df train_df. html classification metrics ROC area is 0. 02391 This method produces a coarse localization map highlighting the areas that the model considers important for the classification decision. html Images vision. \u6211\u4eec\u53ef\u4ee5\u770b\u5230\u9a8c\u8bc1\u96c6\u6027\u80fd\u5728\u5468\u671f\u5feb\u7ed3\u675f\u65f6\u4e0e\u8bad\u7ec3\u96c6\u6027\u80fd\u5206\u79bb\u4e86\u4e00\u4e9b \u8fd9\u610f\u5473\u7740\u6211\u4eec\u7684\u6a21\u578b\u5728\u5c0f\u5b66\u4e60\u7387\u671f\u95f4\u5f00\u59cb\u8fc7\u5ea6\u62df\u5408 \u73b0\u5728 \u5982\u679c\u6211\u4eec\u8fdb\u4e00\u6b65\u8bad\u7ec3 \u6a21\u578b\u5c06\u53ea\u8bb0\u4f4f\u8bad\u7ec3\u96c6\u4e2d\u7684\u7279\u5f81 \u5e76\u4e14\u9a8c\u8bc1\u96c6\u635f\u5931\u503c\u4f1a\u63d0\u9ad8 \u8fd9\u662f\u4e00\u4e2a\u505c\u4e0b\u6765\u7684\u597d\u5730\u65b9 Validation and analysis \u9a8c\u8bc1\u548c\u5206\u6790 Now the training is done. org api _as_gen matplotlib. 7 modules imgproc doc miscellaneous_transformations. According to Libre Pathology https librepathology. \u8bf7\u8bb0\u4f4f AUC\u662f\u7528\u4e8e\u672c\u6b21\u6bd4\u8d5b\u7684\u5ea6\u91cf\u6807\u51c6 \u6211\u4eec\u53ef\u4ee5\u7528\u9a8c\u8bc1\u96c6\u8ba1\u7b97\u5b83 \u4f46\u5b83\u5f88\u53ef\u80fd\u4e0e\u6700\u7ec8\u5f97\u5206 \u6d4b\u8bd5\u96c6 \u4e0d\u540c Submit predictions\u63d0\u4ea4\u9884\u6d4b\u6587\u4ef6 TTA\u6d4b\u8bd5\u65f6\u589e\u5f3a test time augmentation To evaluate the model we run inference on all test images. one_item one_batch Get one batch of from the DataBunch. columns \u63d0\u4ea4\u6587\u4ef6\u7684id\u5217 Series\u8f6c\u6362\u4e3alist List of tumor preds. html denormalize \u53cd\u5f52\u4e00\u5316denormalize\u7684\u7406\u89e3 https stackoverflow. \u6211\u4eec\u7684\u6a21\u578b\u5e94\u8be5\u5df2\u7ecf\u5b66\u5230\u4e86\u4e1c\u897f \u5982\u679c\u6ca1\u6709 \u6211\u4eec\u7684\u4ee3\u7801\u6216\u6570\u636e\u53ef\u80fd\u6709\u95ee\u9898 We can see from the plotted losses that there is a small rise after the initial drop which is caused by the increasing learning rate of the first half cycle. \u8fd9\u79cd\u53ef\u89c6\u5316\u662f\u4e86\u89e3\u6a21\u578b\u6240\u9762\u5bf9\u7684\u6570\u636e\u7684\u597d\u65b9\u6cd5 \u5b83\u8fd8\u53ef\u80fd\u63ed\u793a\u6709\u5173\u6570\u636e\u96c6\u7684\u4e00\u4e9b\u4fe1\u606f \u4f8b\u5982\u8d28\u91cf\u4e0d\u4f73\u7684\u6570\u636e Gradient weighted Class Activation Mapping Grad CAM \u52a0\u6743\u68af\u5ea6\u7c7b\u6fc0\u6d3b\u6620\u5c04 \u4e00\u79cd\u53ef\u89c6\u5316\u89e3\u91ca\u57fa\u4e8e\u68af\u5ea6\u5b9a\u4f4d\u7684\u6df1\u5ea6\u7f51\u7edc Grad CAM Visual Explanations from Deep Networks via Gradient based Localization https arxiv. replace \u6362\u5b57\u7b26\u4e32 This ndarray image has to be converted to tensor before passing on as fastai Image we can use pil2tensor \u8fd9\u4e2andarray\u56fe\u50cf\u5fc5\u987b\u5728\u8f6c\u53d8\u6210fastai Image\u4e4b\u524d\u8f6c\u6362\u4e3a\u5f20\u91cftensor\u683c\u5f0f \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528pil2tensor\u65b9\u6cd5\u5b9e\u73b0 pil2tensor\u628apil\u683c\u5f0f\u7684\u56fe\u7247\u6570\u7ec4\u8f6c\u6362\u4e3atorch\u683c\u5f0f\u7684\u56fe\u7247\u5f20\u91cf https docs. predict img 0 \u662f\u9884\u6d4b\u7684\u7c7b\u522b 0\u5bf9\u5e94\u4e86\u53cd\u4f8bnegative\u5373\u6b63\u5e38 1\u5bf9\u5e94\u4e86\u6b63\u4f8b\u5373\u764c\u75c7tumor https docs. figure \u521b\u5efa\u4e00\u4e2a\u65b0\u56fe \u753b\u51fa\u5bf9\u89d2\u7ebf \u8bbe\u7f6ex\u8f74\u4e0a\u754c\u548c\u4e0b\u754c \u8bbe\u7f6ey\u8f74\u4e0a\u754c\u548c\u4e0b\u754c x\u8f74\u6807\u7b7e\u662f y\u8f74\u6807\u7b7e\u662f \u56fe\u7684\u4e0a\u65b9\u6807\u9898 \u8bbe\u7f6e\u56fe\u4f8b make sure we have the best performing model stage loaded \u52a0\u8f7d\u4e4b\u524d\u8bad\u7ec3\u7684\u6548\u679c\u6700\u597d\u7684\u6a21\u578b\u53c2\u6570 Fastai has a function for this but we don t want the additional augmentations it does our image loader has augmentations so we just use the get_preds \u867d\u7136fastai\u6709\u6d4b\u8bd5\u65f6\u589e\u5f3a \u4f46\u662f\u6211\u4eec\u4e4b\u524d\u52a0\u8f7d\u6570\u636e\u7684\u65f6\u5019\u5c31\u505a\u4e86\u6570\u636e\u589e\u5f3a \u6240\u4ee5\u5c31\u4e0d\u7528\u518d\u505a\u4e86 We do a fair number of iterations to cover different combinations of flips and rotations. vision import \u5305\u542b\u5b9a\u4e49\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6a21\u578b\u6240\u5fc5\u987b\u7684\u51fd\u6570 from fastai import \u66f4\u5feb \u66f4\u7b80\u5355 \u66f4\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u5e93 from torchvision. These are means and std s of each three channel and we calculated these previously in the stats step. If you want to use the 0. html fit_one_cycle plot learning rate of the one cycle \u753b\u51fa1cycle\u7b56\u7565\u7684\u5b66\u4e60\u7387\u66f2\u7ebf \u7531\u5c0f\u53d8\u5927 \u518d\u7531\u5927\u53d8\u5c0f https docs. \u5728\u7ecf\u8fc7\u6d4b\u8bd5\u7684\u8870\u51cf\u6743\u91cd\u4e2d 1e 4\u4f3c\u4e4e\u662f\u6ee1\u8db3\u8981\u6c42\u6700\u5927\u7684\u8870\u51cf\u6743\u91cd \u5b83\u5141\u8bb8\u6211\u4eec\u4ee5\u6700\u5927\u7684\u5b66\u4e60\u901f\u5ea6\u8fdb\u884c\u8bad\u7ec3 \u505c\u6b62\u4e0b\u964d\u5f97\u6700\u665a \u8bf7\u6ce8\u610f \u4e0a\u56fe\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u8fd0\u884c\u4e4b\u95f4\u6709\u6240\u4e0d\u540c \u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u968f\u673a\u6570\u636e\u5b50\u96c6\u8fdb\u884c\u8ba1\u7b97 Smaller datasets and architectures seem to require larger values for weight decay while larger datasets and deeper architectures seem to require smaller values. \u6df7\u6dc6\u77e9\u9635\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u7406\u89e3\u5047\u8d1f\u4f8b\u548c\u5047\u6b63\u4f8b\u7684\u6bd4\u4f8b \u8fd9\u662f\u4e00\u79cd\u5feb\u901f\u67e5\u770b\u6a21\u578b\u6027\u80fd\u7684\u65b9\u6cd5 \u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u8868\u683c \u4ee5\u5b9e\u9645\u6807\u7b7e\u4e0e\u9884\u6d4b\u6807\u7b7e\u5bf9\u7684\u65b9\u5f0f\u8ba1\u6570 \u5728\u8fd9\u91cc\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8be5\u6a21\u578b\u5df2\u7ecf\u5b66\u4f1a\u533a\u5206\u80bf\u7624\u548c\u8d1f\u4f8b\u6837\u672c \u6ca1\u6709\u80bf\u7624 \u5e76\u4e14\u5b83\u5df2\u7ecf\u8868\u73b0\u826f\u597d \u4f46\u662f \u6211\u4eec\u5c06\u7ee7\u7eed\u4ece\u8fd9\u91cc\u8fdb\u4e00\u6b65\u8bad\u7ec3 Finetuning the baseline model \u5fae\u8c03\u57fa\u7ebf\u6a21\u578b Next we can unfreeze all the trainable parameters from the model and continue its training. After the heads have adjusted and the model somewhat works we can continue to train all the weights. \u5bf9\u4e8e\u6574\u4e2a\u6570\u636e\u96c6 \u5f53\u5728\u68c0\u67e5H E\u67d3\u8272\u7684\u8f7d\u73bb\u7247\u671f\u95f4\u8f7d\u73bb\u7247\u6c34\u5e73\u6807\u8bb0\u4e0d\u6e05\u695a\u65f6 \u4f7f\u7528\u5177\u6709\u8fde\u7eed\u7ec4\u7ec7\u5207\u7247\u7684\u989d\u5916WSI \u5bf9\u7ec6\u80de\u89d2\u86cb\u767d\u8fdb\u884c\u514d\u75ab\u7ec4\u7ec7\u5316\u5b66\u67d3\u8272 \u4ee5\u786e\u8ba4\u5206\u7c7b 1399 H E stained sentinel lymph node sections of breast cancer patients the CAMELYON dataset 1399 H E\u67d3\u8272\u4e73\u817a\u764c\u60a3\u8005\u7684\u6807\u8bb0\u6dcb\u5df4\u7ed3\u5207\u7247 CAMELYON\u6570\u636e\u96c6 https academic. merge\u51fd\u6570\u662f\u5408\u5e76\u5355\u901a\u9053\u6210\u591a\u901a\u9053 \u4e0d\u80fd\u5408\u5e76\u591a\u4e2a\u591a\u901a\u9053\u56fe\u50cf \u5982\u679caugmentations\u7b49\u4e8efalse \u5373\u4e0d\u505a\u6570\u636e\u589e\u5f3a \u53ea\u5c06\u56fe\u50cf\u50cf\u7d20\u7684\u503c\u5f52\u4e00\u5316 random rotation \u968f\u673a\u65cb\u8f6c \u5728 a b \u4e4b\u95f4\u968f\u673a\u751f\u6210\u4e00\u4e2a\u6574\u6570 \u5982\u679c\u968f\u673a\u65cb\u8f6c90\u5ea6\u5f00\u542f\u4e3a1 rotation\u5c31\u968f\u673a\u52a0\u4e0a90\u5ea6\u62160\u5ea6\u6216\u8d1f90\u5ea6 \u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u65cb\u8f6c\u7684\u4e2d\u5fc3\u70b9 \u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u65cb\u8f6c\u7684\u89d2\u5ea6 \u7b2c\u4e09\u4e2a\u53c2\u6570\u662f\u56fe\u50cf\u7f29\u653e\u56e0\u5b50 1 \u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u8f93\u5165\u56fe\u7247 \u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u4eff\u5c04\u53d8\u6362\u77e9\u9635 \u7b2c\u4e09\u4e2a\u662f\u8f93\u51fa\u56fe\u7247\u7684\u5c3a\u5bf8 random x y shift\u6839\u636ex\u8f74y\u8f74\u968f\u673a\u5e73\u79fb \u5728 a a \u4e4b\u95f4\u751f\u6210\u4e00\u4e2a\u968f\u673a\u6574\u6570 \u7528\u4e8ex\u8f74\u65b9\u5411\u7684\u5e73\u79fb \u5728 a a \u4e4b\u95f4\u751f\u6210\u4e00\u4e2a\u968f\u673a\u6574\u6570 \u7528\u4e8ey\u8f74\u65b9\u5411\u7684\u5e73\u79fb crop to center and normalize to 0 1 range \u4e2d\u5fc3\u526a\u88c1 \u5e76\u5c06\u56fe\u50cf\u50cf\u7d20\u503c\u5f52\u4e00\u5316 \u5f00\u59cb\u526a\u88c1\u7684\u5750\u6807 \u9ed8\u8ba4\u5de6\u4e0a\u89d2\u5750\u6807\u662f 0 0 \u7ec8\u6b62\u526a\u88c1\u7684\u5750\u6807 \u9ed8\u8ba4\u5de6\u4e0a\u89d2\u5750\u6807\u662f 0 0 \u9664\u53f7\u4e4b\u524d\u7684\u662f\u5bf9\u539f\u56fe\u50cf\u505a\u4e2d\u5fc3\u526a\u88c1\u540e\u7684\u56fe\u50cf\u77e9\u9635 \u9664\u662f\u4e3a\u4e86\u5f52\u4e00\u5316 Random flip \u968f\u673a\u7ffb\u8f6c getrandbits k \u8fd4\u56de\u4e00\u4e2ak\u6bd4\u7279\u4f4d\u968f\u673a\u7684\u6574\u6570. \u5355\u7ec6\u80de Small clusters of cells. The dataset is a subset of the PCam dataset https github. This fastai s data object is easily customized for loading images using our own readCroppedImage function. org stable modules classes. \u5728\u6211\u4eec\u8bad\u7ec3\u7684\u4e2d\u671f \u6211\u4eec\u5f00\u59cb\u964d\u4f4e\u5b66\u4e60\u7387 \u56e0\u4e3a\u6211\u4eec\u89c9\u5f97\u5df2\u5904\u4e8e\u4e00\u4e2a\u826f\u597d\u7684\u7a33\u5b9a\u533a\u57df \u8fd9\u610f\u5473\u7740\u6211\u4eec\u5f00\u59cb\u5bfb\u627e\u8be5\u533a\u57df\u5185\u7684\u6700\u5c0f\u503c Is the model learning \u6211\u4eec\u7684\u6a21\u578b\u5b66\u4e60\u4e86\u5417 Our model should have already learned something and if it hasn t there s probably something wrong with our code or with the data. \u6211\u4eec\u53ef\u4ee5\u68c0\u67e5\u6570\u636e\u662f\u5426\u5305\u542b\u9519\u8bef\u6570\u636e \u672a\u805a\u7126\u6216\u635f\u574f\u7684\u56fe\u7247 \u5e76\u5220\u9664\u8fd9\u4e9b\u6570\u636e\u4ee5\u63d0\u9ad8\u6574\u4f53\u8d28\u91cf Preprocessing and augmentation \u9884\u5904\u7406\u548c\u6570\u636e\u589e\u5f3aThere are couple of ways we can use to avoid overfitting more data augmentation regularization and less complex model architectures. com \u8865\u5145\u8d44\u6599 One Page Knowledge https alexiej. 5\u500d Plot some images with and without cancer tissue for comparison \u753b\u51fa\u4e00\u4e9b\u6709\u764c\u7ec4\u7ec7\u548c\u6ca1\u6709\u764c\u7ec4\u7ec7\u7684\u56fe\u50cf\u8fdb\u884c\u6bd4\u8f83Classifying metastases is probably not an easy task for a trained pathologist and extremely difficult for an untrained eye. I think the bright ones are just cropped from a non stained part or they don t have any tissue plain glass so the labels are correct. \u5c06\u8bad\u7ec3\u6570\u636e\u96c6\u5206\u621090 \u7528\u4e8e\u8bad\u7ec3\u548c10 \u7528\u4e8e\u9a8c\u8bc1 \u6211\u4eec\u5e0c\u671b\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\u4e2d\u6570\u636e\u4fdd\u6301\u76f8\u7b49\u7684 \u53cd\u4f8b \u6b63\u4f8b\u4e3a 60 40 \u6bd4\u7387 \u8fd9\u91cc\u5e76\u4e0d\u662f\u90a3\u4e48\u91cd\u8981 \u56e0\u4e3a\u4e24\u4e2a\u6807\u7b7e\u6570\u91cf\u51e0\u4e4e\u90fd\u662f\u76f8\u540c\u7684 \u4f46\u662f\u5982\u679c\u6211\u4eec\u6709\u4e00\u4e2a\u7f55\u89c1\u7684\u7c7b \u968f\u673a\u5206\u5272\u8bad\u7ec3\u96c6\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u4ee3\u8868\u6027\u4e0d\u8db3 \u6216\u8005\u5728\u6700\u574f\u7684\u60c5\u51b5\u4e0b \u5c06\u6240\u6709\u7a00\u6709\u7c7b\u522b\u4ece\u4e00\u4e2a\u5207\u5206\u6570\u636e\u96c6\u4e2d\u6392\u9664 We will use Fast. com andrewkh test time augmentation tta worth it Submit the model for evaluation\u63d0\u4ea4We need to submit the tumor probability for each test sample in this competition. plot_lr and plot the losses of the first cycle \u753b\u51fa\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u7684\u635f\u5931\u66f2\u7ebf https docs. Typically nuclei are stained blue whereas cytoplasm and extracellular parts in various shades of pink. html ClassificationInterpretation top losses will return all validation losses and indexes sorted by the largest first top_losses \u5c06\u8fd4\u56de\u6240\u6709 \u9a8c\u8bc1\u96c6\u7684\u635f\u5931\u503c tl_val \u548c\u5176\u7d22\u5f15 tl_idx \u6309\u964d\u5e8f\u6392\u5e8f https docs. This means that a negatively labeled image could contain metastases in the outer region. py L663 from fastai. ROC\u66f2\u7ebf\u662f\u4e0d\u540c\u9608\u503c\u4e0b\u7684\u771f\u6b63\u4f8b\u7387\u4e0e\u5047\u6b63\u4f8b\u7387\u7684\u66f2\u7ebf\u56fe \u66f2\u7ebf\u4e0b\u9762\u79ef AUC \u7b49\u4e8e\u5206\u7c7b\u5668\u5bf9\u6982\u7387\u6392\u5e8f\u540e \u6b63\u4f8b\u9ad8\u4e8e\u53cd\u4f8b\u7684\u6bd4\u4f8b AUC\u6700\u7406\u60f3\u7684\u503c\u4e3a1 \u8fd9\u610f\u5473\u7740\u6211\u4eec\u5c06\u6b63\u786e\u5730\u5bf9\u6240\u6709\u6b63\u4f8b\u8fdb\u884c\u5206\u7c7b\u800c\u6ca1\u6709\u5c06\u4efb\u4f55\u53cd\u4f8b\u5f53\u505a\u6b63\u4f8b ROC curve \u66f2\u7ebf\u4e0b\u9762\u79efexample https i. \u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u4e0d\u89c4\u5219\u7684\u6838\u5f62\u72b6 \u5927\u5c0f\u6216\u67d3\u8272\u9634\u5f71\u53ef\u80fd\u8868\u660e\u8f6c\u79fb\u764c How is the data best transformed for modeling \u5982\u4f55\u6700\u597d\u5730\u4e3a\u5efa\u6a21\u8f6c\u6362\u6570\u636e We know that the label of the image is influenced only by the center region 32 x 32px so it would make sense to crop our data to that region only. \u56fe\u50cf\u4e8c\u5206\u7c7b\u95ee\u9898 Identify the presence of metastases from 96 x 96px digital histopathology images. com basveeling pcam Hematoxylin and eosin staining of tissue and cell sections https www. \u8be5\u65b9\u6cd5\u4ea7\u751f\u7c97\u7565\u7684\u5b9a\u4f4d\u56fe \u7a81\u51fa\u663e\u793a\u6a21\u578b\u8ba4\u4e3a\u5bf9\u5206\u7c7b\u51b3\u7b56\u91cd\u8981\u7684\u533a\u57df \u89c6\u89c9\u89e3\u91ca\u4e3a\u6a21\u578b\u63d0\u4f9b\u4e86\u900f\u660e\u5ea6 \u53ef\u89e3\u91ca\u6027 \u4f7f\u5f97\u66f4\u5bb9\u6613\u6ce8\u610f\u5230\u5b83\u662f\u5426\u5df2\u7ecf\u5b66\u5230\u4e86\u9519\u8bef\u7684\u4e1c\u897f \u4f8b\u5982 \u5982\u679c\u6211\u4eec\u8981\u8bad\u7ec3\u4e00\u53ea\u72d7\u54c1\u79cd\u5206\u7c7b\u5668 \u6211\u4eec\u6240\u6709\u7684\u67d0\u79cd\u72d7\u54c1\u79cd\u7684\u56fe\u50cf\u90fd\u4f1a\u5728\u72d7\u5c55\u4e2d\u88ab\u62cd\u6444 \u8be5\u6a21\u578b\u5f88\u6709\u53ef\u80fd\u5b66\u4f1a\u8bc6\u522b\u72d7\u5c55\u73af\u5883\u800c\u4e0d\u662f\u72d7\u72d7\u7684\u7279\u5f81 \u53ef\u89c6\u5316\u672c\u5730\u5316\u5730\u56fe\u5c06\u63ed\u793a\u8fd9\u4e00\u70b9 \u6211\u4eec\u53ef\u4ee5\u4e13\u6ce8\u4e8e\u83b7\u5f97\u8be5\u54c1\u79cd\u7684\u72d7\u72d7\u7684\u66f4\u591a\u6837\u5316\u6570\u636e This shows the activation maps of the predicted category so if the label is tumor the visualization shows all the places where the model thinks the tumor patterns are. \u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u7a7a\u7684DataBunch \u7136\u540e\u5c06\u5b83\u52a0\u8f7d\u5230cnn_learner\u4e2d \u56e0\u4e3a\u662f\u63a8\u7406\u9884\u6d4b\u4e0d\u9700\u8981\u6570\u636e os\u6a21\u5757\u8d1f\u8d23\u7a0b\u5e8f\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u7684\u4ea4\u4e92 \u63d0\u4f9b\u4e86\u8bbf\u95ee\u64cd\u4f5c\u7cfb\u7edf\u5e95\u5c42\u7684\u63a5\u53e3 opencv \u56fe\u50cf\u9884\u5904\u7406 \u751f\u6210\u5e38\u89c1\u56fe\u5f62\u5bf9\u8c61\u5982 \u77e9\u5f62 \u692d\u5706 \u5706\u5f62 \u591a\u8fb9\u5f62 \u6d17\u724c \u67d0\u4e2a\u96c6\u5408\u7684\u968f\u673a\u6392\u5217\u7ec4\u5408 \u8bfb\u53d6csv\u6587\u4ef6\u8f6c\u5316\u4e3apandas dataframe quick look at the label stats pd. CAMELYON\u6570\u636e\u96c6\u4e2d\u5305\u542b\u7684\u6240\u6709\u8f7d\u73bb\u7247\u5747\u4e3a\u5e38\u89c4\u4e34\u5e8a\u62a4\u7406\u7684\u4e00\u90e8\u5206 \u56e0\u6b64\u5177\u6709\u8bca\u65ad\u8d28\u91cf \u4f46\u662f \u5728\u91c7\u96c6\u8fc7\u7a0b\u4e2d \u626b\u63cf\u53ef\u80fd\u4f1a\u5931\u8d25\u6216\u56fe\u50cf\u5931\u7126 \u4f5c\u4e3a\u8d28\u91cf\u63a7\u5236\u63aa\u65bd \u626b\u63cf\u540e\u624b\u52a8\u68c0\u67e5\u6240\u6709\u8f7d\u73bb\u7247 \u68c0\u67e5\u7531\u7ecf\u9a8c\u4e30\u5bcc\u7684\u6280\u672f\u4eba\u5458 Q. probs\u662f\u7f6e\u4fe1\u5ea6 \u7ed9\u7b2c\u4e00\u884c\u7b2c\u4e00\u4e2a\u8bbe\u7f6ey\u8f74\u6807\u7b7e Most incorrect or top losses \u5c55\u793a\u524d\u56db\u4e2a\u6700\u4e0d\u6b63\u786e \u635f\u5931\u503c\u6700\u5927 \u7684\u56fe\u4f8b \u53ea\u5c55\u793a\u56db\u5f20\u56fe tl_idx\u7684\u524d\u56db\u4e2a\u662f\u635f\u5931\u503c\u6700\u9ad8\u7684 im \u662f\u56fe\u7247 cl \u662f\u7c7b\u522bclass \u503c\u662f0 1 0\u6307\u4ee3\u8d1f\u4f8b 1\u6307\u4ee3\u80bf\u7624 \u8f6c\u6362\u4e3a\u6574\u6570 \u5c06\u56fe\u7247\u4ecepytorch\u683c\u5f0f\u8f6c\u4e3anumpy\u683c\u5f0f https docs. The policy brings more disciplined approach for selecting hyperparameters such as learning rate and weight decay. The PCam s dataset including this one uses 10x undersampling to increase the field of view which gives the resultant pixel resolution of 2. html DataBunch Normalize with training set stats. Optionally detach and denorm. predict img \u63a8\u7406\u9884\u6d4b \u4e09\u4e2a\u8fd4\u56de\u503c\u662fpredicted class label and probabilities \u7b2c\u4e00\u4e2a\u8fd4\u56de\u503cy learner. The inspection was performed by an experienced technician Q. \u6211\u4eec\u53d1\u73b0\u81f3\u5c11\u6709\u4e00\u4e2a\u975e\u5e38\u6697\u7684\u56fe\u50cf\u548c6\u4e2a\u975e\u5e38\u660e\u4eae\u7684\u56fe\u50cf Plot some of the very bright or very dark images\u753b\u51fa\u4e00\u4e9b\u592a\u4eae\u548c\u592a\u6697\u7684\u56fe\u7247All the dark and bright images are labeled negative. net goldxwang article details 76855200 \u67d0\u4e2a\u70b9\u7684\u503c\u8d8a\u5927 \u989c\u8272\u8d8a\u6df1 alpha\u900f\u660e\u5ea6 cmp color map\u989c\u8272\u56fe extent \u62c9\u4f38\u56fe\u50cf https www. We can calculate it here for ou validation set but it will most likely differ from the final score. ai\u7684\u559c\u6b22\u4e4b\u5904\u5728\u4e8e\u5b83\u5305\u542b\u4e86\u5bf9\u5f00\u5c55\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u7684\u6700\u65b0\u8fdb\u5c55\u7684 \u5f00\u7bb1\u5373\u7528 \u7684\u652f\u6301 \u5982\u679c\u8981\u4f7f\u75280. com questions 4684622 how to normalize denormalize a vector to range 11 convert batch tensor image to grayscale image with opencv\u4ece\u4e00\u4e2a\u989c\u8272\u7a7a\u95f4\u5230\u53e6\u4e00\u4e2a\u989c\u8272\u7a7a\u95f4 cv2. \u5bf9\u4e8e\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u67b6\u6784 \u8bf7\u67e5\u770bFast. html preds_n_avg 0 \u5bf9\u5e94\u4e86Negative 1\u5bf9\u5e94\u4e86 Tumor get test id s from the sample_submission. Here we will define what image augmentations to use and add them directly to our image loader function. We should now train with much lower learning rates. \u6211\u4eec\u5e0c\u671b\u5728\u505c\u6b62\u4e0b\u964d\u524d\u524d\u9009\u62e9\u6700\u5927\u7684\u6743\u91cd\u8870\u51cf\u5f97\u5230\u8f83\u4f4e\u7684\u635f\u5931\u503c \u5e76\u4e14\u5177\u6709\u6700\u9ad8\u7684\u5b66\u4e60\u7387 Out of the tested WD s 1e 4 seems like the largest WD that allow us to train with maximal learning rate. ", "id": "shangxue/running-kaggle-kernels-with-a-gpu-1c3018", "size": "27826", "language": "python", "html_url": "https://www.kaggle.com/code/shangxue/running-kaggle-kernels-with-a-gpu-1c3018", "git_url": "https://www.kaggle.com/code/shangxue/running-kaggle-kernels-with-a-gpu-1c3018", "script": "sklearn.utils tqdm_notebook #\u8fdb\u5ea6\u6761 sklearn.metrics fastai.vision MyImageItemList(ImageList) readImage open train_test_split # \u5206\u5272\u8bad\u7ec3\u96c6\u3001\u6d4b\u8bd5\u96c6\u7684\u65b9\u6cd5 random * # \u66f4\u5feb\u3001\u66f4\u7b80\u5355\u3001\u66f4\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u5e93 *  # pytorch\u89c6\u89c9\u6a21\u5757 *=all the models from torchvision shuffle #\u6d17\u724c\uff1a\u67d0\u4e2a\u96c6\u5408\u7684\u968f\u673a\u6392\u5217\u7ec4\u5408 imageToTensorImage matplotlib.patches numpy getLearner auc hooked_backward sklearn.model_selection plot_heatmap_overview fastai.callbacks.hooks randint # \u968f\u673a\u751f\u6210\u4e00\u4e2a\u6574\u6570 matplotlib.pyplot pandas * # \u5305\u542b\u5b9a\u4e49\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6a21\u578b\u6240\u5fc5\u987b\u7684\u51fd\u6570 getHeatmap tqdm fastai roc_curve readCroppedImage torchvision.models plot_overview ", "entities": "(('further investments', 'model'), 'see') (('m', 'https docs'), '\u521b\u5efa\u4e00\u4e2a\u4fdd\u5b58\u4e86\u6a21\u578b\u6fc0\u6d3b\u51fd\u6570\u8f93\u51fa\u7684\u94a9\u5b50') (('dataset https', 'breast cancer patients'), 'pubmed') (('cell Classic location Foreign population key feature subcapsular', 'size shape'), 'have') (('we', 'breed'), 'reveal') (('slides', 'metastases'), 'produce') (('it', 'fast performance'), 'help') (('we', 'data augmentation more regularization'), '\u5e76\u5220\u9664\u8fd9\u4e9b\u6570\u636e\u4ee5\u63d0\u9ad8\u6574\u4f53\u8d28\u91cf') (('somewhat we', 'weights'), 'adjust') (('ai software that', 'PyTorch https pytorch'), 'library') (('However relevant information', 'image small sized samples'), 'leave') (('We', 'Leslie Smith https arXiv April 2018 arxiv'), 'detail') (('it', 'data'), 'be') (('it', 'minimum viable product'), '\u666e\u901a\u73bb\u7483') (('goldxwang net article', '\u989c\u8272\u8d8a\u6df1 alpha\u900f\u660e\u5ea6 cmp color map\u989c\u8272\u56fe extent https 76855200 \u67d0\u4e2a\u70b9\u7684\u503c\u8d8a\u5927 \u62c9\u4f38\u56fe\u50cf www'), 'details') (('exactly problem', 'image classification Binary problem'), 'understanding\u4e86\u89e3\u9879\u76ee') (('classifier', 'randomly chosen positive instance higher randomly chosen negative one'), 'be') (('html I', 'plot_top_losses https github'), 'accuracy') (('we', 'training 0 samples'), 'bool') (('it', 'cell just very large whole image'), 'be') (('log preds Compute ROC curve', '\u5047\u9633\u6027\u7387 tpr \u771f\u9633\u6027\u7387 \u9608\u503c\u5217\u8868 \u771f\u5b9e\u6807\u7b7e probs\u9884\u6d4b\u503c pos_label 1\u6807\u7b7e\u4e3a1\u7684\u662f\u6b63\u4f8b \u5176\u90fd\u662f\u53cd\u4f8b https fpr y blog'), 'detail') (('test final metrics', 'validation'), 'get') (('we', 'high'), 'be') (('top losses', '\u4e4b\u95f4\u968f\u673a\u751f\u6210\u4e00\u4e2a\u6574\u6570 \u8be5\u9a8c\u8bc1\u96c6\u7684\u7d22\u5f15 \u5f97\u5230\u8be5\u56fe\u7247\u7684\u70ed\u56fe\u548c\u80cc\u666f \u56fe\u7247\u957f\u5bbd _ \u989c\u8272\u56fe\u4e3a\u7070\u5ea6\u56fe \u753b\u51fa\u70ed\u56fe interpolation\u63d2\u503c \u4ee3\u8868\u56fe\u7247\u50cf\u7d20\u989c\u8272\u6ca1\u6709\u8fc7\u6e21 https max_offset \u503c\u662f0 1 \u8f6c\u6362\u4e3a\u6574\u6570\u578b 1\u5217\u4e0a\u5c55\u793a\u80cc\u666f\u56fe blog'), '\u5c06\u8fd4\u56de\u6240\u6709') (('we', 'dog show competition'), 'take') (('com fastai fastai search unscoped_q \u8bb0\u5f55\u4e86epoch\u8f6e\u6b21', 'opt\u4f18\u5316\u5668 metric\u6307\u6807 https docs'), 'q') (('\u5f53\u4f7f\u7528\u4e0d\u540c\u7684\u6a21\u578b\u65f6 ax\u7684\u8303\u56f4\u53ef\u80fd\u9700\u8981\u505a\u8c03\u6574 \u8bbe\u7f6ex\u8f74\u4e0a\u754c\u548c\u4e0b\u754c \u8bbe\u7f6ex\u8f74\u6807\u7b7e\u6587\u672c\u7684\u683c\u5f0f \u8bbe\u7f6ey\u8f74\u4e0a\u754c\u548c\u4e0b\u754c 1e 6 1e 4 1e 2 \u6700\u5927\u5b66\u4e60\u7387', '1cycle policy'), 'learner') (('\u6211\u5f3a\u70c8\u63a8\u8350Fastai \u4e3a\u7f16\u7801\u5458\u63d0\u4f9b\u5b9e\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u8bfe\u7a0b v3 I', 'Martijn https www'), 'want') (('\u6307\u5b9a\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u7ed3\u6784 densenet169\u4f3c\u4e4e\u5bf9\u8fd9\u4e9b\u6570\u636e\u8868\u73b0\u826f\u597d \u4e0d\u4fe1\u60a8\u53ef\u4ee5\u8bd5\u8bd5', 'batch size hardware restrics'), '0') (('we', 'code'), 'implement') (('it', 'WD s.'), '\u7684\u635f\u5931\u503c') (('html fit_one_cycle', 'plot learning one cycle'), 'rate') (('However this', 'testing'), 'confirm') (('\u73b0\u5728', '\u8fd9\u6b21\u6211\u4eec\u5b9a\u4e49\u5faa\u73af\u7684\u6700\u5c0f\u548c\u6700\u5927\u5b66\u4e60\u7387'), '\u5b66\u4e60\u7387\u8f83\u4f4e') (('We', 'learning now much lower rates'), 'train') (('Mid', '\u7ec6\u80de\u6838\u67d3\u6210\u84dd\u8272 Low resolution\u4f4e\u5206\u8fa8\u7387'), '\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u662f\u7528\u82cf\u6728\u7cbe\u548c\u4f0a\u7ea2') (('2 where it', 'bottom'), '\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u7ed3\u6784\u4f3c\u4e4e\u9700\u8981\u8f83\u5927\u7684\u6743\u91cd\u8870\u51cf\u503c') (('we', 'predictions'), 'call') (('ps\u662f\u6700\u540e\u4e00\u5c42\u968f\u673a\u5931\u6d3b\u7684\u6bd4\u4f8b create_cnn', '0 1 final layer'), '\u5bf9\u6570\u636e\u6807\u51c6\u5316') (('i', 'sample_submission'), 'preds_n_avg') (('we', 'test images'), 'augmentation') (('See results', 'https \u8bad\u7ec3\u6216\u8005\u6d4b\u8bd5 docs'), '\u8fd4\u56de\u4e00\u4e9b\u6570\u636e\u7528\u4e8e\u9a8c\u8bc1') (('us', 'learning maximal rate'), 'seem') (('dataset', 'duplicates'), '\u8377\u5170\u5948\u6885\u4ea8') (('We', 'learner'), 'need') (('html \u7b2c\u4e00\u884c\u7b2ci\u4e2a\u8f74\u7684x\u8f74\u6ca1\u6709\u523b\u5ea6', '\u8bbe\u7f6e\u6807\u9898 f string https blog'), '\u7b2c\u4e00\u884c\u7b2ci\u4e2a\u8f74\u7684y\u8f74\u6ca1\u6709\u523b\u5ea6') (('I', 'these'), 'try') (('We', 'visualization purposes'), 'read') (('it', 'most likely final score'), 'calculate') (('which', '2'), 'use') (('we', 'computations'), 'note') (('\u56fe\u50cf\u4e8c\u5206\u7c7b\u95ee\u9898', 'histopathology 96 96px digital images'), 'Identify') (('plot_losses', 'model \u9884\u6d4b\u6a21\u578b\u7684\u9a8c\u8bc1\u96c6 \u5206\u7c7b\u6a21\u578b\u7684\u6548\u679c\u89e3\u91ca\u51fd\u6570 https docs'), 'predict') (('I', 'optimal size'), 'show') (('Thus it', 'center region'), 'be') (('dataset', 'PCam dataset https github'), 'be') (('CAMELYON\u6570\u636e\u96c6\u4e2d\u5305\u542b\u7684\u6240\u6709\u8f7d\u73bb\u7247\u5747\u4e3a\u5e38\u89c4\u4e34\u5e8a\u62a4\u7406\u7684\u4e00\u90e8\u5206 \u4f46\u662f \u5728\u91c7\u96c6\u8fc7\u7a0b\u4e2d \u626b\u63cf\u53ef\u80fd\u4f1a\u5931\u8d25\u6216\u56fe\u50cf\u5931\u7126', '\u4f5c\u4e3a\u8d28\u91cf\u63a7\u5236\u63aa\u65bd \u68c0\u67e5\u7531\u7ecf\u9a8c\u4e30\u5bcc\u7684\u6280\u672f\u4eba\u5458'), '\u56e0\u6b64\u5177\u6709\u8bca\u65ad\u8d28\u91cf') (('data object', 'readCroppedImage own function'), 'customize') (('eric_pycv net article', '72887758 cv2'), 'detail') (('\u8fd9\u79cd\u53ef\u89c6\u5316\u662f\u4e86\u89e3\u6a21\u578b\u6240\u9762\u5bf9\u7684\u6570\u636e\u7684\u597d\u65b9\u6cd5 \u5b83\u8fd8\u53ef\u80fd\u63ed\u793a\u6709\u5173\u6570\u636e\u96c6\u7684\u4e00\u4e9b\u4fe1\u606f \u4f8b\u5982\u8d28\u91cf\u4e0d\u4f73\u7684\u6570\u636e Gradient', 'Localization https Gradient based arxiv'), 'weight') (('We', 'example'), 'deploy') (('we', 'correctly false positives'), 'yield') (('it', 'wrong things'), 'give') (('we', 'hopefully good stable area'), '\u4f46\u8d8b\u5411\u4e8e\u66f4\u5bbd\u548c\u66f4\u7a33\u5b9a\u7684\u5730\u65b9') (('kind', 'empty area'), 'cause') (('one_item one_batch', 'DataBunch'), 'get') (('that', 'submissions'), '\u80bf\u7624') (('which', 'first half cycle'), '\u6211\u4eec\u7684\u4ee3\u7801\u6216\u6570\u636e\u53ef\u80fd\u6709\u95ee\u9898') (('2\u9644\u8fd1\u9009\u62e9 \u5b66\u4e60\u7387 Next we', 'model'), '\u6211\u4eec\u53ef\u4ee5\u57282e') (('still where we', 'learning rate'), 'be') (('We', 'flips'), '\u753b\u51fa\u5bf9\u89d2\u7ebf') (('x 32 so it', 'region'), '\u5927\u5c0f\u6216\u67d3\u8272\u9634\u5f71\u53ef\u80fd\u8868\u660e\u8f6c\u79fb\u764c') (('we', 'pred \u628a\u6211\u4eec\u7684id\u548c\u9884\u6d4b\u503c'), 'python\u5217\u8868\u751f\u6210\u5f0f') (('model', 'classification decision'), '02391') (('ROC\u66f2\u7ebf\u662f\u4e0d\u540c\u9608\u503c\u4e0b\u7684\u771f\u6b63\u4f8b\u7387\u4e0e\u5047\u6b63\u4f8b\u7387\u7684\u66f2\u7ebf\u56fe', 'ROC curve \u66f2\u7ebf\u4e0b\u9762\u79efexample https \u7b49\u4e8e\u5206\u7c7b\u5668\u5bf9\u6982\u7387\u6392\u5e8f\u540e AUC\u6700\u7406\u60f3\u7684\u503c\u4e3a1 \u8fd9\u610f\u5473\u7740\u6211\u4eec\u5c06\u6b63\u786e\u5730\u5bf9\u6240\u6709\u6b63\u4f8b\u8fdb\u884c\u5206\u7c7b\u800c\u6ca1\u6709\u5c06\u4efb\u4f55\u53cd\u4f8b\u5f53\u505a\u6b63\u4f8b i.'), '\u66f2\u7ebf\u4e0b\u9762\u79ef') (('we', 'Image\u4e4b\u524d\u8f6c\u6362\u4e3a\u5f20\u91cftensor\u683c\u5f0f pil2tensor\u628apil\u683c\u5f0f\u7684\u56fe\u7247\u6570\u7ec4\u8f6c\u6362\u4e3atorch\u683c\u5f0f\u7684\u56fe\u7247\u5f20\u91cf https docs'), 'crop') (('slides', 'manually scanning'), 'inspect') (('labels', 'don tissue plain glass'), 'think') (('where we', 'model architecture'), 'specific') (('simple that', 'actual label'), 'be') (('additional WSI', 'classification'), 'use') (('org site', 'public_html example_high_resolution'), 'serve') (('qq_33453253 net article', '79653546 \u7ed9\u7b2c\u4e00\u884c\u7b2c\u4e00\u4e2a\u8bbe\u7f6ey\u8f74\u6807\u7b7e'), 'detail') (('Going', 'model deeper architecture'), 'start') (('evaluation\u63d0\u4ea4We', 'competition'), 'tta') (('it', 'blue violet'), 'be') (('we', 'one split'), 'be') (('that', 'pixel very low maximum intensity'), 'calculate') (('dataset', 'Radboud University Medical Center Nijmegen'), '\u6839\u636e\u6570\u636e\u63cf\u8ff0') (('positive label', '32 image'), 'mean') (('7 modules', 'doc miscellaneous_transformations'), 'imgproc') (('OpenCV', 'lot faster PIL'), '\u6211\u4eec\u5c06\u5b9a\u4e49\u56fe\u50cf\u589e\u5f3a') (('it', 'learning deep research'), 'be') (('Next we', 'training'), '\u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u8868\u683c') (('we', 'training 0 samples'), 'label') (('items', '\u53cd\u4f8b\u548c\u6b63\u4f8b\u6982\u7387 preds\u9884\u6d4b\u503c y\u9884\u6d4b\u6807\u8bb0 with_loss False\u4e0d\u8ba1\u635f\u5931\u503c https 2 tqdm_notebook\u53ef\u7528\u4e8e\u663e\u793a\u8fdb\u5ea6\u6761 \u5176\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u63d0\u793a\u7528\u7684 docs'), '\u6d4b\u8bd5\u96c6\u6570\u76ee') (('\u6211\u8fd8\u6ca1\u6709\u5c1d\u8bd5\u8fd9\u4e9b \u4e0d\u4fdd\u8bc1\u80fd\u7acb\u523b\u5de5\u4f5c Official', 'documentation\u5b98\u65b9\u6587\u6863 https docs'), '\u53ef\u80fd\u9700\u8981\u4e00\u4e9b\u8c03\u8bd5') (('which', 'cancer'), 'be') (('breast first cancer', 'lymphatic system'), 'be') (('data', '\u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668ImageDataBunch Where data'), 'html') (('img \u63a8\u7406\u9884\u6d4b', 'class label'), 'predict') (('optimal lr', 'divergence'), 'be') (('policy', 'such rate'), 'bring') (('evaluation metric', 'ROC curve'), '\u5224\u65ad96') (('This', 'suboptimal hyperparameters'), 'save') (('dataset https', 'breast cancer patients'), '\u4e3a\u5176\u4ed6\u4e2d\u5fc3') (('org site', 'public_html example_mid_resolution'), 'serve') (('small metastases', 'tissue'), 'be') (('figures\u4e30\u5bcc\u7684\u6709\u4e1d\u5206\u88c2\u6570\u5b57 Abundant mitotic Cells', 'tumour highly dependent type'), 'variable') (('Fastai\u5e93\u4e3a\u4e00\u4e2a\u5468\u671f\u653f\u7b56\u5b9e\u65bd\u4e86\u8bad\u7ec3\u51fd\u6570 \u6211\u4eec\u53ea\u9700\u51e0\u884c\u4ee3\u7801\u5373\u53ef\u4f7f\u7528 Sylvian \u8fd9\u53ef\u4ee5\u4e3a\u6211\u4eec\u8282\u7701\u5927\u91cf\u65f6\u95f4 Gugger', 'explanation https very clear sgugger'), '\u6211\u4eec\u5c06\u4f7f\u7528Leslie') (('inspection', 'technician experienced Q.'), 'perform') (('learning rate', 'middle'), '\u540e\u9762') (('this', 'model \u8bc4\u4f30\u6a21\u578b https pytorch'), 'discussion') (('we', 'surroundings'), 'lose') (('data', 'overall quality'), '\u6211\u4eec\u77e5\u9053\u56fe\u50cf\u7684\u6807\u7b7e\u4ec5\u53d7\u4e2d\u5fc3\u533a\u57df') (('we', 'weight 2 1e 4 6 decays'), '\u548c') (('included', 'thus diagnostic quality'), 'be') (('rotation variations', 'change\u65cb\u8f6c\u89d2\u5ea6 center crop 0 x'), '\u521b\u5efa\u4e00\u7cfb\u5217\u7684\u5b50\u56fe') (('\u751f\u6210\u70ed\u56fe\u7684\u5176\u4ed6\u4f8b\u5b50', '\u6709\u5f88\u591a\u89e3\u91ca \u6709\u65f6\u95f4\u4e86\u7ec6\u770b \u76f8\u4fe1\u80fd\u89e3\u51b3\u5f88\u591a\u7591\u60d1 https dhruvs'), 'attach') (('Typically nuclei', 'pink'), 'stain') (('I', 'coders v3 https course'), 'ai') (('function', 'default \u7528opencv\u8bfb\u53d6\u56fe\u7247 \u901a\u9053\u683c\u5f0f\u9ed8\u8ba4\u662fbgr cv2'), 'read') (('We', 'training splits'), 'want') (('etc', 'file'), 'save') (('probs\u662f\u7f6e\u4fe1\u5ea6', 'Most incorrect losses'), '\u7ed9\u7b2c\u4e00\u884c\u7b2c\u4e00\u4e2a\u8bbe\u7f6ey\u8f74\u6807\u7b7e') (('model', 'wider more one'), 'have') (('hypothesis', 'crop varying sizes'), 'confirm') (('\u7ed9fastai\u5e93', 'test_size\u662f\u5360\u539f\u6765\u6570\u636e\u96c6\u7684\u6bd4\u4f8b stratify\u662f\u4e3a\u4e86\u4fdd\u6301split\u524d\u7c7b\u7684\u5206\u5e03 https fitting\u5c06\u539f\u8bad\u7ec3\u96c6\u5206\u5272\u6210\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6 www'), 'split') (('probably we', 'results'), 'improve') (('\u6211\u662f\u4e03\u6708\u5728\u7ebffan print \u662f\u4e3a\u4e86\u9632\u4e2d\u65ad julyfan \u6587\u6863\u4f1a\u4e0d\u5b9a\u671f\u66f4\u65b0', '\u5efa\u8bae\u53bbkaggle fork\u8fd0\u884c\u4ee3\u7801 \u53ef\u4ee5\u4e0d\u7528\u4e0b\u8f7d\u6570\u636e\u96c6\u548c\u914d\u7f6e\u73af\u5883 \u8fd8\u6709\u514d\u8d39\u7684gpu \u8fd9\u662f\u539f\u4f5c\u8005\u7684kernel https \u8f6c\u8f7d\u8bf7\u6ce8\u660e\u51fa\u5904 www'), '\u6709\u95ee\u9898\u8bf7\u6307\u51fa') (('They', 'Fastai v0'), 'etinuz') (('we', '220k training available \u6211\u4eec\u6709\u54ea\u4e9b\u6570\u636e images'), 'understanding\u4e86\u89e3\u6570\u636e') (('we', 'stats previously step'), 'be') (('then we', 'DataBunch'), '\u6bd4\u5982\u4e00\u4e2aweb\u670d\u52a1\u5668') (('largest first top_losses', '\u5c06\u8fd4\u56de\u6240\u6709'), 'return') (('html image2np \u5728\u7b2c\u4e09\u884c\u7b2ci\u4e2a\u8f74\u4e0a\u5c55\u793a\u8fd9\u5f20\u56fe\u7247 \u7b2c\u4e09\u884c\u7b2ci\u4e2a\u8f74\u7684x\u8f74\u6ca1\u6709\u523b\u5ea6 \u7b2c\u4e09\u884c\u7b2ci\u4e2a\u8f74\u7684y\u8f74\u6ca1\u6709\u523b\u5ea6 \u8bbe\u7f6e\u6807\u9898 \u7ed9\u7b2c\u4e09\u884c\u7b2c\u4e00\u4e2a\u8bbe\u7f6ey\u8f74\u6807\u7b7e', 'ClassificationInterpretation'), 'interp') (('\u6211\u4eec\u53ef\u4ee5\u975e\u5e38\u5feb\u901f\u5730\u770b\u5230\u662f\u5426\u5b58\u5728\u4e00\u4e9b\u610f\u60f3\u4e0d\u5230\u7684\u95ee\u9898 \u4f8b\u5982\u7cdf\u7cd5\u7684\u6570\u636e\u8d28\u91cf \u8fd9\u5c06\u4f7f\u5f97\u5bf9\u6a21\u578b\u8c03\u4f18\u7684\u4efb\u4f55\u8fdb\u4e00\u6b65\u6295\u8d44\u90fd\u4e0d\u503c\u5f97', '90 training'), '\u6bd4\u5982\u6700\u5c0f\u53ef\u884c\u4ea7\u54c1') (('crop size this', 'model file name e.'), 'run') (('negative it', 'tumor'), 'see') (('\u5f00\u59cb\u526a\u88c1\u7684\u5750\u6807 \u7ec8\u6b62\u526a\u88c1\u7684\u5750\u6807 \u9664\u53f7\u4e4b\u524d\u7684\u662f\u5bf9\u539f\u56fe\u50cf\u505a\u4e2d\u5fc3\u526a\u88c1\u540e\u7684\u56fe\u50cf\u77e9\u9635 0 0 0 0 Random', 'k \u8fd4\u56de\u4e00\u4e2ak\u6bd4\u7279\u4f4d\u968f\u673a\u7684\u6574\u6570'), '\u5373\u4e0d\u505a\u6570\u636e\u589e\u5f3a') (('removing', 'prediction performance'), 'have') (('These', '\u7a0d\u540e\u8981\u91cd\u65b0\u6392\u5e8f sample_submission \u8fd9\u4e9b\u9884\u6d4b\u6982\u7387\u662f\u6309\u7167\u6211\u4eec\u6d4b\u8bd5\u96c6\u4e2d\u7684\u987a\u5e8f\u6392\u5217\u7684 numpy'), '\u751f\u6210\u764c\u75c7\u9884\u6d4b\u6982\u7387\u5217\u8868') (('sizes', 'staining metastases'), '\u5c0f\u7ec6\u80de\u7fa4') (('It', 'quality such bad data'), 'reveal') (('com', 'ml pipeline \u8fd9\u662f\u4e03\u6708\u5728\u7ebf\u7684\u7f51\u5740 https complete www'), 'fanhaobei') (('that', 'learning highest rate'), '\u6743\u91cd\u8870\u51cf') (('https 0 \u662f\u9884\u6d4b\u7684\u7c7b\u522b 0\u5bf9\u5e94\u4e86\u53cd\u4f8bnegative\u5373\u6b63\u5e38 1\u5bf9\u5e94\u4e86\u6b63\u4f8b\u5373\u764c\u75c7tumor docs', 'img'), 'predict') (('dark images', '\u6211\u4eec\u53d1\u73b0\u81f3\u5c11\u6709\u4e00\u4e2a\u975e\u5e38\u6697\u7684\u56fe\u50cf\u548c6\u4e2a\u975e\u5e38\u660e\u4eae\u7684\u56fe\u50cf very bright very images\u753b\u51fa\u4e00\u4e9b\u592a\u4eae\u548c\u592a\u6697\u7684\u56fe\u7247All'), 'plot') (('\u6216\u8005\u5728\u6700\u574f\u7684\u60c5\u51b5\u4e0b \u5c06\u6240\u6709\u7a00\u6709\u7c7b\u522b\u4ece\u4e00\u4e2a\u5207\u5206\u6570\u636e\u96c6\u4e2d\u6392\u9664 We', 'Fast'), '\u7528\u4e8e\u8bad\u7ec3\u548c10') (('image', 'empty area'), 'be') (('Tumor tissue', 'label'), 'influence') (('I', 'more consistent results'), 'recommend') (('that', '40x objective'), 'Data') (('good model', 'doggy instead breed'), 'be') (('hematoxylin Dark blue binds', 'most proteins'), 'charge') (('Nuclear', 'enlargement\u6838\u6269\u5927'), '\u6838\u975e\u5178\u578b') (('\u67d0\u4e2a\u96c6\u5408\u7684\u968f\u673a\u6392\u5217\u7ec4\u5408 \u8bfb\u53d6csv\u6587\u4ef6\u8f6c\u5316\u4e3apandas', 'label stats'), '\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u7a7a\u7684DataBunch') (('Otherwise random initialization', 'model'), 'harm') (('Sylvian Gugger\u5199\u4e86\u4e00\u7bc7\u975e\u5e38\u660e\u786e\u7684\u6587\u7ae0\u89e3\u91caLeslie\u7684\u5efa\u8bae First we', 'learning weight optimal rate values'), '\u6211\u5efa\u8bae\u4f60\u9605\u8bfb') (('This', 'export'), 'Export') (('org site', 'public_html example_low_resolution'), 'serve') (('scanning issues', 'diagnosis'), 'assess') (('\u753b\u51fa\u4e00\u4e9b\u6709\u764c\u7ec4\u7ec7\u548c\u6ca1\u6709\u764c\u7ec4\u7ec7\u7684\u56fe\u50cf\u8fdb\u884c\u6bd4\u8f83Classifying metastases', 'extremely untrained eye'), 'plot') (('load', 'learning Now smaller rates'), 'save') (('I', 'reading'), 'html') (('\u5728\u8fd9\u91cc \u6211\u4eec\u5c06\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5e76\u505a\u8fc1\u79fb\u5b66\u4e60\u6765\u8c03\u6574\u6211\u4eec\u7684\u6743\u91cd \u5bfb\u6c42\u66f4\u6df1\u7684\u6a21\u578b\u67b6\u6784\u5c06\u66f4\u5feb\u5730\u53d1\u751f\u8fc7\u62df\u5408', 'Fast'), 'check') (('problem best Here we', 'data'), '\u5b83\u4eec\u975e\u5e38\u6709\u7528') (('\u5b9a\u4e49fastai\u6570\u636e\u52a0\u8f7d\u5668ImageDataBunch \u521b\u5efa\u4e00\u4e2a\u5b57\u5178 name \u8bad\u7ec3\u96c6\u5730\u5740 \u56fe\u50cf\u540d label \u56fe\u50cf\u540d\u5bf9\u5e94\u7684\u6807\u8bb0 \u521b\u5efa\u4e00\u4e2a\u8bad\u7ec3\u96c6dataframem', 'fastai loader \u521b\u5efa\u4e00\u4e2adataframe'), 'create') (('model', 'good understanding'), '\u4e5f\u662f\u6700\u6b63\u786e\u7684\u6570\u636e\u662f\u4ec0\u4e48') (('html', '\u53cd\u5f52\u4e00\u5316denormalize\u7684\u7406\u89e3 https stackoverflow'), 'denormalize') (('\u6211\u4eec\u9700\u8981\u63d0\u4ea4\u6bcf\u4e2a\u6d4b\u8bd5\u6837\u672c\u662f\u764c\u75c7\u7684\u6982\u7387 \u90e8\u7f72Deploy 0 1 we', 'working model'), '\u8fd9\u4e2a\u6982\u7387\u7684\u53d6\u503c\u8303\u56f4\u662f') (('model', 'learning small rates'), 'mean') (('plot_losses lets', 'confusion matrix'), 'take') (('validation performance', 'cycle'), '\u6211\u4eec\u73b0\u5728\u5e94\u8be5\u4ee5\u66f4\u4f4e\u7684\u5b66\u4e60\u7387\u8bad\u7ec3') (('model', 'stage \u5982\u679c\u5728\u5fae\u8c03\u4e4b\u524d\u7684\u6a21\u578b\u66f4\u597d \u5219\u53d6\u6d88\u6ce8\u91ca \u52a0\u8f7d\u524d\u4e00\u9636\u6bb5\u7684\u6a21\u578b \u9884\u6d4b \u9884\u6d4b\u6807\u8bb0 \u635f\u5931\u503c https previous docs'), 'Save') (('that', 'H hematoxylin E.'), 'curve') (('set performance', 'training set'), 'memorize') (('org dev peps 0201 Now we', '\u73b0\u5728 \u5faa\u73af\u5730\u63d0\u53d6sample_list\u4e2d\u7684id \u627e\u5230\u5176\u5728pred_dice\u5b57\u5178\u4e2d\u5bf9\u5e94\u7684\u503c Pandas Next i d columns'), 'pep') (('learning when rates', 'end'), 'rise') (('model', 'What'), '\u6211\u4eec\u7684\u6a21\u578b\u4ee5\u975e\u5e38\u9ad8\u7684\u6982\u7387\u9884\u6d4b\u9519\u8bef\u7684\u6570\u636e\u662f\u4ec0\u4e48') (('we', 'Image\u4e4b\u524d\u8f6c\u6362\u4e3a\u5f20\u91cftensor\u683c\u5f0f pil2tensor\u628apil\u683c\u5f0f\u7684\u56fe\u7247\u6570\u7ec4\u8f6c\u6362\u4e3atorch\u683c\u5f0f\u7684\u56fe\u7247\u5f20\u91cf https \u8fd9\u4e2andarray\u56fe\u50cf\u5fc5\u987b\u5728\u8f6c\u53d8\u6210fastai \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528pil2tensor\u65b9\u6cd5\u5b9e\u73b0 docs'), 'replace') (('larger datasets', 'deeper smaller values'), '\u5b83\u5141\u8bb8\u6211\u4eec\u4ee5\u6700\u5927\u7684\u5b66\u4e60\u901f\u5ea6\u8fdb\u884c\u8bad\u7ec3') (('you', 'device'), 'file') (('\u6dcb\u5df4\u7ed3\u662f\u5c0f\u817a\u4f53 \u53ef\u8fc7\u6ee4\u6dcb\u5df4\u7cfb\u7edf\u4e2d\u7684\u6db2\u4f53 \u6dcb\u5df4\u7ed3\u8f6c\u79fb\u7684\u7ec4\u7ec7\u5b66\u8bc4\u4f30\u662fTNM\u5206\u7c7b\u4e2d\u786e\u5b9a\u4e73\u817a\u764c\u5206\u671f\u7684\u4e00\u90e8\u5206 TNM\u5206\u7c7b\u662f\u7528\u4e8e\u5206\u7c7b\u764c\u75c7\u6269\u6563\u7a0b\u5ea6\u7684\u5168\u7403\u516c\u8ba4\u6807\u51c6 \u75c5\u7406\u5b66\u5bb6\u7684\u8bca\u65ad\u7a0b\u5e8f\u7e41\u7410\u4e14\u8017\u65f6', 'background knowledge\u53c2\u8003\u94fe\u63a5 Patch Camelyon PCam https github'), '\u5b83\u4eec\u662f\u4e73\u817a\u764c\u53ef\u80fd\u4f20\u64ad\u7684\u7b2c\u4e00\u4e2a\u4f4d\u7f6e') (('PCam dataset', 'Challenge dataset https camelyon16'), 'derive') (('CAMELYON', '\u5bf9\u4e8e\u6574\u4e2a\u6570\u636e\u96c6 E\u67d3\u8272\u7684\u8f7d\u73bb\u7247\u671f\u95f4\u8f7d\u73bb\u7247\u6c34\u5e73\u6807\u8bb0\u4e0d\u6e05\u695a\u65f6 \u5bf9\u7ec6\u80de\u89d2\u86cb\u767d\u8fdb\u884c\u514d\u75ab\u7ec4\u7ec7\u5316\u5b66\u67d3\u8272 sentinel lymph node breast cancer \u4f7f\u7528\u5177\u6709\u8fde\u7eed\u7ec4\u7ec7\u5207\u7247\u7684\u989d\u5916WSI 1399 H E stained patients'), '\u5f53\u5728\u68c0\u67e5H') (('weights', 'common shapes'), '\u6211\u4eec\u53ef\u4ee5\u89e3\u51bb\u6a21\u578b\u4e2d\u7684\u6240\u6709\u53ef\u8bad\u7ec3\u53c2\u6570\u5e76\u7ee7\u7eed\u8bad\u7ec3') (('augmentations \u5982\u679c\u6211\u4eec\u4e0d\u5728\u8fd9\u91cc\u5e94\u7528\u6570\u636e\u589e\u5f3a \u9700\u8981\u5c06\u4e24\u4e2a\u7a7a\u5217\u8868\u8bbe\u7f6e\u7ed9tfms \u9a8c\u8bc1\u96c6 Train \u5c06\u8bad\u7ec3\u96c6 \u6d4b\u8bd5\u96c6\u6346\u7ed1\u6210\u4e00\u4e2a\u5bf9\u8c61', 'https docs'), 'convert') (('Here we', 'image loader directly function'), 'define') (('metastases', 'tissue'), 'be') (('class predicted argmax', 'max \u987a\u7740\u7b2c\u4e00\u4e2a\u7ef4\u5ea6 \u884c \u6c42\u6700\u5927\u503c\u7684\u5750\u6807 \u5373\u6c42\u6bcf\u4e2a\u6837\u672c\u7684\u6982\u7387\u6700\u5927\u7684\u5750\u6807 0\u62161 https docs'), 'get_preds') (('also when we', 'inference'), 'note') (('model', 'very high probability'), 'be') (('We', 'visualization purposes'), 'be') (('here we', 'two empty lists'), '\u6839\u636eval_idx\u62c6\u5206\u8bad\u7ec3\u96c6\u5408\u548c\u9a8c\u8bc1\u96c6') (('we', 'area'), 'mean') (('Calculating here \u5728\u8fd9\u91cc\u4e0d\u4f7f\u7528\u6570\u636e\u589e\u5f3a statistics', '0'), 'use') (('\u7b2c\u4e00\u884c\u7b2ci\u4e2a\u8f74\u7684x\u8f74\u6ca1\u6709\u523b\u5ea6 \u7b2c\u4e00\u884c\u7b2ci\u4e2a\u8f74\u7684y\u8f74\u6ca1\u6709\u523b\u5ea6 \u8bbe\u7f6e\u6807\u9898 \u5728\u7b2c\u4e00\u884c\u7b2ci\u4e2a\u8f74\u4e0a\u5c55\u793a\u8fd9\u5f20\u56fe\u7247 classes', 'interp'), '\u8f6c\u6362\u4e3a\u6574\u6570') (('from_learner learner \u8fd9\u662f\u4e4b\u524d\u5b9a\u4e49\u7684 \u8c03\u7528\u4e0a\u9762\u8fd9\u4e2a\u51fd\u6570', '\u7528\u4e8e\u68c0\u67e5\u548c\u4fee\u6539\u6bcf\u4e00\u5c42\u7684\u8f93\u51fa https forward pass \u524d\u5411\u4f20\u64ad\u65f6\u81ea\u52a8\u8c03\u7528\u8fd9\u4e2a\u94a9\u5b50 docs'), 'hook') (('one_item', 'batch'), 'create') (('Failing', 'inference inconsistent results'), 'yield') (('duplicate images', 'only two'), 'be') (('we', 'image times\u8981\u67e5\u770b\u6211\u4eec\u7684\u589e\u5f3a\u6548\u679c one multiple \u6211\u4eec\u53ef\u4ee5\u591a\u6b21\u7ed8\u5236\u4e00\u5e45\u56fe\u50cf'), 'image\u5feb\u5f97\u591a') (('negatively labeled image', 'outer region'), 'mean') (('\u521b\u5efaImageList\u7684\u5b50\u7c7b', '\u521b\u5efa\u7c7b PathOrStr Image \u90fd\u662f\u6ce8\u91ca \u8c03\u7528\u4e4b\u524d\u5b9a\u4e49\u7684\u8bfb\u53d6\u56fe\u7247\u7684\u51fd\u6570 \u4e5f\u662f\u7528\u4e8e\u5b9a\u4e49ImageDataBunch str'), 'listdidr\u8fd4\u56de\u4e00\u4e2a\u5217\u8868\u5305\u542b\u67d0\u8def\u5f84\u4e0b\u6240\u6709\u6587\u4ef6\u540d') (('ai\u89c6\u89c9\u6a21\u5757\u548ctorchvision\u6a21\u5757 We', 'training'), 'load') (('tumor patterns', 'places'), '\u6211\u4eec\u6240\u6709\u7684\u67d0\u79cd\u72d7\u54c1\u79cd\u7684\u56fe\u50cf\u90fd\u4f1a\u5728\u72d7\u5c55\u4e2d\u88ab\u62cd\u6444') ", "extra": "['patient', 'test', 'diagnosis', 'metastasis', 'procedure']"}