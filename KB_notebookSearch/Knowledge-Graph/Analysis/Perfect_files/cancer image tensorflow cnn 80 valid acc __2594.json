{"name": "cancer image tensorflow cnn 80 valid acc ", "full_name": " h1 1 Libraries and settings h1 2 Analyze data h1 3 Manipulate data h1 4 Try out sklearn base models h1 5 Build neural network with TensorFlow h1 6 Train and validate the neural network ", "stargazers_count": 0, "forks_count": 0, "description": "We validate and compare each of these base models. The goal is to classify cancerous images IDC invasive ductal carcinoma vs non IDC images. Author Raoul Malm Abstract The dataset consists of 5547 breast histology images each of pixel size 50 x 50 x 3. 50 50 3 layer Conv1 ReLu MaxPool. This takes roughly 45 minutes on the kaggle hardware. Libraries and settings 1 bullet 2. Try out sklearn base models 5. layer convolution max pooling. After that we implement the following neural network architecture input layer. layer convolution max pooling 3 3 3 36 36. The output layer gives for each image a probability for IDC 0 and IDC 1. 1 0 1 0 1 0 function to normalize data scale features using statistics that are robust to outliers convert from 0 255 to 0. com paultimothymooney predicting idc in breast cancer histology images notebook 1. In order to prevent overfitting the training data we generate new images by rotations translations and zoom. Libraries and settings 2. 25 25 36 layer Conv2 ReLu MaxPool. 576 output layer FC ReLu. Results Using 10 fold cross validation the best base models can achieve an accuracy of roughly 76 on the validation sets. Train and validate the neural network 6 bullet Reference Predicting IDC in Breast Cancer Histology Images by paultimothymooney https www. Manipulate data 4. layer fully connected 1024 1 1024. Manipulate data 3 bullet 4. 7 7 36 layer FC ReLu. Build TensorFlow Graph 5 bullet 6. We also implement tensor summaries which can be visualized with TensorBoard. Then the images are normalized and we try out some basic classification algorithms like logistic regregession random forest decision tree and so on. 1 cost function optimisation function predicted probabilities in one hot encoding tensor of correct predictions accuracy tensors to save intermediate accuracies and losses during training number of weights and biases create summary tensors for tensorboard merge all summaries for tensorboard initialize summary writer initialize tensorflow saver function to train the graph train on original or augmented data training and validation data use augmented data parameters start timer looping over mini batches adapt learn_rate get new batch run the graph store losses and accuracies for logging the results summary for tensorboard concatenate losses and accuracies and assign to tensor variables save tensors summaries tf saver tb summary forward prediction of current graph function to load tensors from a saved graph input tensors weights and bias tensors activation tensors training and prediction tensors tensor of stored losses and accuricies during training get losses of training and validation sets get accuracies of training and validation sets get weights get biases load session from file restore graph and load tensors receive activations given the input cross validations cross validations default 20 5 validation set start timer train and validation data of original images create neural network graph instance of nn_class create graph attach saver tensors start tensorflow session attach summaries variable initialization of the default graph training on original data training on augmented data save tensors and summaries of model choose neural network choose neural network confusion matrix normalize plot. Train and validate the neural network load images of shape 5547 50 50 3 load labels of shape 5547 1 0 no cancer 1 cancer shuffle data 0 no cancer 1 cancer convert one hot encodings into labels convert class labels from scalars to one hot vectors e. 1024 add dropout 5. Try out sklearn base models 4 bullet 5. Analyze data 2 bullet 3. Build neural network with TensorFlow 6. layer fully connected 7 7 36 1024 1024. Still I think there is a lot of room for improvement here since I have not spend much time on tuning the model. In a first step we analyze the images and look at the distribution of the pixel intensities. 13 13 36 layer Conv3 ReLu MaxPool. training and validation data use one hot encoding for labels 0 1 dictionaries for saving results generate new images via rotations translations zoom using keras rotations translations zoom get transformed images check image generation computet the accuracy of label predictions store models in dictionary choose models for out of folds predictions start timer cross validations cross validations default 20 5 validation set random_state 123 start timer train and validation data of original images create cloned model from base models predictions accuracies normalized confusion matrix choose model plot boxplot algorithm comparison class that implements the neural network constructor tunable hyperparameters for nn architecture filter size of first convolution layer default 3 number of features of first convolution layer default 36 filter size of second convolution layer default 3 number of features of second convolution layer default 36 filter size of third convolution layer default 3 number of features of third convolution layer default 36 number of neurons of first fully connected layer default 576 tunable hyperparameters for training mini batch size keeping probability with dropout regularization in terms of epochs helper variables current position pointing to current learning rate log results in terms of epochs counting current number of mini batches trained on True use tensorboard visualization True use saver to save the model name of the neural network permutation array function to get the next mini batch adapt length of permutation array shuffle once at the start of epoch at the end of the epoch shuffle data start next epoch set index to mini batch size use augmented data for the next epoch use augmented data use original data generate new images via rotations translations zoom using keras rotations translations zoom get transformed images weight initialization bias initialization positive bias 2D convolution max pooling attach summaries to a tensor for TensorBoard visualization function to create the graph reset default graph variables for input and output 1. Implementing a 90 10 split for the training and validation data and training the neural network for 30 epochs while using data augmentation we can achieve an accuracy of 80 on the validation sets. 2 The neural network is implemented as a python class and the complete TensorFlow session can be saved to or restored from a file. ", "id": "raoulma/cancer-image-tensorflow-cnn-80-valid-acc", "size": "2594", "language": "python", "html_url": "https://www.kaggle.com/code/raoulma/cancer-image-tensorflow-cnn-80-valid-acc", "git_url": "https://www.kaggle.com/code/raoulma/cancer-image-tensorflow-cnn-80-valid-acc", "script": "matplotlib.cm __init__ max_pool_2x2 load_session_from_file attach_saver get_biases forward accuracy_from_dense_labels seaborn numpy next_mini_batch get_weights summary_variable weight_variable get_activations train_graph conv2d get_accuracy load_tensors matplotlib.pyplot create_graph tensorflow pandas attach_summary normalize_data generate_images get_loss nn_class dense_to_one_hot bias_variable one_hot_to_dense save_model ", "entities": "(('Results', 'validation sets'), 'achieve') (('layer convolution max', '3 3 36'), 'pool') (('we', 'rotations translations'), 'in') (('we', 'network architecture input following neural layer'), 'implement') (('that', '255 0'), 'convert') (('We', 'base models'), 'validate') (('Author Raoul Malm dataset', 'pixel each size'), 'Abstract') (('neural network', 'neural network confusion matrix normalize plot'), 'predict') (('neural network', 'TensorFlow complete file'), '2') (('we', 'regregession forest decision logistic random tree'), 'normalized') (('we', 'pixel intensities'), 'analyze') (('images weight transformed initialization', 'input'), 'use') (('we', 'validation sets'), 'achieve') (('This', 'kaggle hardware'), 'take') (('com paultimothymooney', '1'), 'notebook') (('here I', 'model'), 'think') (('0 cancer 1 cancer', 'vectors one hot e.'), 'train') (('output layer', 'IDC'), 'give') (('goal', 'non IDC images'), 'be') (('which', 'TensorBoard'), 'implement') ", "extra": "[]"}