{"name": "amazon deep composer ar cnn music augmentation ", "full_name": " h1 Amazon Deep Composer h2 Training a custom AR CNN model h3 The AWS DeepComposer approach to generating music h1 Start Here h2 Installing dependencies h3 Download and Unzip Training DataSet h3 Optional Download and Unzip pre trained model h1 Preprocess and Parse Training DataSet h2 Preprocessing the data into the piano roll format h3 Reviewing sample piano rolls h3 Why do we use 128 timesteps h3 Creating samples of uniform size shape for model training h4 In the code cells below h3 Augment the data for better results and training h1 Create Setup the Model Setup all Hyper Parameters h1 Generate and Evaluate Plot from the resulting model h2 Performing inference h3 How to change the inference parameters when you perform inference h2 Submitting to the Spin the Model Chartbusters challenge ", "stargazers_count": 0, "forks_count": 0, "description": "That s because each of the 8 bars contains 4 beats. Training Data Generator Validation Data Generator title Creating callbacks for the model and performing initial intitialization Callback For Loss Plots Callback For Saving Model Checkpoints Create A List Of Callbacks Create A Model Instance title Tensorboard Graphs and Stats Load the TensorBoard notebook extension title Main Training Loop. After adding or removing a note from the input the model feeds this new input back into itself. THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. We explain how to acquire the data that you use for this project provide some exploratory data analysis EDA and show how we augment the data during training. json as json_file inference_params json. NOTE Training times can vary greatly based on the parameters that you have chosen and the notebook instance type that you chose when launching this notebook. Setting this value to 0 means no notes will be added to your input melody NOTE If you restrict your model s ability to add and remove notes you risk creating poor compositions. This yields 128 timesteps frac 4 timesteps 1 beat frac 4 beats 1 bar frac 8 bars 1 128 timesteps We found that this level of resolution is sufficient to capture the musical details in our dataset. Setup all Hyper Parameters Generate and Evaluate Plot from the resulting model Performing inference Congratulations You have now trained your very own AR CNN model to generate music. Piano Roll Input Dimensions Number of Filters In The Convolution Growth Rate Of Number Of Filters At Each Convolution Number Of Encoder And Decoder Layers A List Of Dropout Values At Each Encoder Layer A List Of Dropout Values At Each Decoder Layer A List Of Flags To Ensure If batch_normalization Should be performed At Each Encoder A List Of Flags To Ensure If batch_normalization Should be performed At Each Decoder Path to Pretrained Model If You Want To Initialize Weights Of The Network With The Pretrained Model Learning Rate Of The Model Optimizer To Use While Training The Model Batch Size Number Of Epochs title Calculate the number of Batch Iterations Before A Training Epoch Is Considered Finished Total Number Of Time Steps title Creating the data generators that perform data augmentation. Now that you understand the basic theory behind our approach let s dive into the code. We subdivide those 8 bars into 128 timesteps. Then download your notebook checkpoint files and compositions from SageMaker and upload them to your public repository. Start Here Installing dependenciesFirst let s install and import all of the Python packages that we will use in this tutorial. How to change the inference parameters when you perform inference The model performs inference by sampling from its predicted probability distribution across the entire piano roll. Augment the data for better results and trainingWe are going to do the following things in this section 1 Adding or removing notes during training2 Removing random notes from a target piano roll to create input piano rolls3 Adding random notes to the target piano roll to create input piano rolls Create Setup the Model. Creativity temperature To create the output probability distribution the final layer uses a softmax activation. org wiki Bar_ music samples from the dataset. Submitting to the Spin the Model Chartbusters challengeTo submit your composition s and model to the Spin the model chartbusters challenge you will first need to create a public repository on GitHub https github. Create the environment and install required packages title Import all modules and other necessary code Imports Make it a multiple of the batch size for best balanced performance Number of Bars Number of Beats Per Bar number of bars to be shifted Total number of pitches in a Pianoroll Total number of Tracks 100 bpm title The Best Choice Works best stand alone Super Piano Original 2500 MIDIs title Second Best Choice Works best stand alone Alex Piano Only Drafts Original 1500 MIDIs unzip data JSB Chorales. For our purposes we use a custom data generator to perform data augmentation. A higher number of sampling iterations gives the model more time to improve the input melody. Maximum notes to add maxNotesAdded The maximum percentage of notes that can be added during inference. As you saw when we used the play_midi function each sample isn t the same length. You may also need to run this section twice or trice if there are any errors. Use the link from your public repository to make your submission to the Chartbusters challenge title Environment and Dependencies Setup. html library to plot a piano roll track from the dataset. zip d data title Alternative Choice Alex Piano Only Original 450 MIDIs title Super Piano 2 Pre Trained Model floss 1. title Generate MIDI file samples and shuffle the DataSet Parse the MIDI file and get the piano roll Saving the generated samples into a dataset variable Shuffle the dataset title Sampling from a uniform distribution title Calculate Training and Validation sampling lengths title Specifying training hyperparameters. Creating samples of uniform size shape for model training For model training the input piano rolls must be the same size. To use your trained model you will need to update the PATH variable in the cell below. In the next section we show examples of the piano roll format that we use for training the model. Start Training title Optional Save your model manually if necessary title Inference Code Initialization Routine truncate Apply temperature and softmax Mask all pixels that both have a note and were once part of the original input Mask all pixels that both do not have a note and were not once part of the original input Check if the note being removed is from the original input Check if the note being added is not in original input title Loading a saved checkpoint file. To create the input piano rolls during training we need data generators for both the training and validation samples. Setting this value to 0 prevents the model from removing notes from your input melody. load json_file dict to string string to json title Main Generation Loop param type string Generate The Composition set the src and play title Plot and Graph the Output Only first batch MIDI file is plotted and displayed param type slider min 0 max 20 step 1 param type slider min 0 max 20 step 1 param type slider min 1 max 128 step 1 param type slider min 1 max 128 step 1 For plotting Use librosa s specshow function for displaying the piano roll Plot the output title Additional Input Output Comparison Metrics of the specified MIDI file Input Midi Metrics Generated Output Midi Metrics Convert The Input and Generated Midi To Tensors a matrix Plot Input Piano Roll Plot Output Piano Roll. We use two functions to create target piano rolls that are the same size process_midi and process_pianoroll. By training our model to view the problem as edit events rather than as an entire image or just the addition of notes we found that it can offset the accumulation of errors and generate higher quality music. In the code cells below generate_samples is a function used to ingest the midi files and break the files down into a uniform shape plot_pianoroll uses a built in function plot_track from the pypianoroll https salu133445. Now you can see how well your model will perform with an input melody. Inference is an iterative process. The model has been trained to both remove and add notes so it can improve the input melody and correct mistakes that it may have made in earlier iterations. We do this by treating music generation as a series of edit events which can be either the addition or removal of a note. These functions are wrapped in a larger function generate_samples which also takes in constants that are related to subdividing the. And in some cases Runtime also needs to be restarted The MIT Zero License Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files the Software to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and or sell copies of the Software and to permit persons to whom the Software is furnished to do so. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Maximum notes to remove maxPercentageOfInitialNotesRemoved The maximum percentage of notes that can be removed during inference. NOTE If you want to test that your model is training on your custom dataset you can decrease the number of epochs down to 1 in this cell. Download and Unzip Training DataSet Optional Download and Unzip pre trained model Preprocess and Parse Training DataSet Preprocessing the data into the piano roll format Reviewing sample piano rolls Why do we use 128 timesteps In this tutorial we use 8 bar https en. An edit sequence is a series of edit events. io pypianoroll visualization. You also can change the inference parameters to observe differences in the quality of the music generated Sampling iterations samplingIterations The number of iterations performed during inference. You can change the temperature for the softmax to produce different levels of creativity in the outputs generated by the model. In the following cell you start training your model. To help mitigate this problem we train our AR CNN model so that it can detect and then fix mistakes including those made by the model itself. We further divide each beat into 4 timesteps. Please be patient as it may take a while. The AWS DeepComposer approach to generating music Autoregressive based approaches are prone to accumulate errors during training. param type string Create An Inference Object Load The Checkpoint title MODEL INFERENCE GENERATION CONTROLS run auto param type slider min 0. Every edit sequence can directly correspond to a piano roll. 1 param type slider min 0 max 100 step 1 param type slider min 0 max 100 step 1 param type slider min 0 max 100 step 1 with open inference_parameters. 5k training steps title Import the MIDI files from the data_dir and save them with the midi_files variable Finds our random MIDI file from the midi_files variable and then plays it Note To listen to multiple samples from the Bach dataset you can run this cell over and over again. Amazon Deep Composer Training a custom AR CNN model In this Jupyter notebook we guide you through several steps of the data science life cycle. ", "id": "aleksandrsigalov/amazon-deep-composer-ar-cnn-music-augmentation", "size": "5754", "language": "python", "html_url": "https://www.kaggle.com/code/aleksandrsigalov/amazon-deep-composer-ar-cnn-music-augmentation", "git_url": "https://www.kaggle.com/code/aleksandrsigalov/amazon-deep-composer-ar-cnn-music-augmentation", "script": "output play_midi display process_pianoroll augmentation __init__ google.colab keras.layers files plot_piano_roll ArCnnModel random model Dropout pypianoroll Inference Adam PianoRollGenerator convert_midi_to_tensor Conv2D Javascript enum Constants() numpy get_music_metrics utils.midi_utils Input FluidSynth randrange get_sampled_index utils.generate_training_plots get_softmax convert_tensor_to_midi Audio OptimizerType plot_pianoroll matplotlib.pyplot RMSprop mask_not_allowed_notes tensorflow Multitrack generate_samples keras.optimizers inference AddAndRemoveAPercentageOfNotes data_generator midi2audio Loss BatchNormalization sample_notes_from_model load_model Enum UpSampling2D sample_multiple GenerateTrainingPlots Model concatenate MaxPooling2D losses get_indices generate_composition backend Track backend as K keras keras.models process_midi IPython.display HTML ", "entities": "(('each', '4 beats'), 's') (('MIDIs title Second Best Choice best alone Super Piano 2500 Works', 'Alex best alone Piano'), 'create') (('COPYRIGHT HOLDERS', 'OR OTHER SOFTWARE'), 'SHALL') (('we', 'bar 8 https'), 'train') (('edit sequence', 'edit events'), 'be') (('edit sequence', 'piano directly roll'), 'correspond') (('You', 'also section'), 'need') (('you', 'cell'), 'NOTE') (('we', 'tutorial'), 'start') (('plot_pianoroll', 'pypianoroll https salu133445'), 'be') (('you', 'cell'), 'need') (('higher number', 'input melody'), 'give') (('added', 'checkpoint saved file'), 'start') (('Software', 'whom'), 'need') (('which', 'note'), 'do') (('that', 'inference'), 'maxPercentageOfInitialNotesRemoved') (('we', 'data science life cycle'), 'Training') (('further each', '4 timesteps'), 'divide') (('that', 'data augmentation'), 'number') (('Checkpoint MODEL GENERATION CONTROLS', 'auto param type slider'), 'create') (('we', 'data augmentation'), 'use') (('how we', 'training'), 'explain') (('that', 'the'), 'wrap') (('number', 'inference'), 'change') (('you', 'model'), 'start') (('you', 'poor compositions'), 'mean') (('PROVIDED AS', 'PARTICULAR PURPOSE'), 'be') (('model', 'back itself'), 'after') (('piano rolls', 'input'), 'be') (('you', 'cell'), 'title') (('we', 'model'), 'show') (('MIDI piano roll', 'training hyperparameters'), 'sample') (('model', 'piano entire roll'), 'change') (('Performing You', 'music'), 'Generate') (('when we', 'same length'), 'see') (('we', 'training samples'), 'need') (('output probability final layer', 'softmax activation'), 'use') (('We', '128 timesteps'), 'subdivide') (('that', 'inference'), 'note') (('that', 'target piano rolls'), 'use') (('you', 'when notebook'), 'vary') (('how well model', 'input melody'), 'see') (('data', 'input piano rolls Create Setup'), 'augment') (('Model Instance title Tensorboard Graphs', 'TensorBoard notebook extension title'), 'create') (('it', 'earlier iterations'), 'train') (('it', 'model'), 'train') (('Autoregressive based approaches', 'training'), 'approach') (('batch MIDI Only first file', 'Input Midi Output Midi Metrics Generated Tensors'), 'dict') (('it', 'while'), 'be') (('submission', 'title Environment'), 'use') (('you', 'GitHub https github'), 'submit') (('s', 'code'), 'now') (('softmax', 'model'), 'change') (('level', 'dataset'), 'yield') (('it', 'quality higher music'), 'find') ", "extra": "['patient', 'test']"}