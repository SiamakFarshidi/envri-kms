{"name": "effective feature engineering ", "full_name": " h1 Effective feature engineering h2 1 Correlation h2 2 Score gain on a simple model h2 3 Feature importances of Tree models h2 4 Permutation importance h2 5 SHAP values h2 6 Score gain on a complex model ", "stargazers_count": 0, "forks_count": 0, "description": "Score gain on a simple modelI used LinearRegression during feature engineering since it s simple and fast. However sometimes it doesn t represent the actual contribution. pdf 2017 Understanding why a model makes a certain prediction can be as crucial as the prediction s accuracy in many applications. astype category Option 2 pd. Build a simple model3. CorrelationThis is the simplest way to see the relation between features. Let s see player level generated features. I created a Kernel dedicated for feature engineering for this competition. I will show an example in another Kernel. Question What is a fair way for a colition to divide its payoff Depends on the definition of fairness Approach Identify axioms that express properties of a fair payoff division Symmetry Interchangeable agents should receive the same payments Dummy Players Dummy players should receive nothing Additivity v_1 v_2 S v_1 S v_2 S The author of SHAP found that we can apply this concept to machine learning. Intuitively it sounds fair. It s the average of combinations of features. That s why it s represented like 0. After selecting important features I start looking at the actual impact on a complex model. Option 1 Give it as category df matchType df matchType. A Unified Approach to Interpreting ModelPredictions https arxiv. You can see how important rank features are in this competition. So what should we do It depends on the requirement and there is a trade off but I d recommend to build an environment to try new ideas quickly and fail quickly You d earn new ideas through the process. Score gain on a complex modelThe upper things are faster but less accurate and lower things are more accurate but slower. Checking correlation is the fastest way to estimate the impact but it doesn t capture the actual contribution of the score. It d be enough if you just want to see the impact of the new feature you added. Traid off Accurate result The number of trials How they work on a competition is like below. Shapley Value is a solution concept in cooperative game theory proposed in 1953 by Lloyd Shapley https en. Feature importances of Tree modelsTree models can output feature importances. ELI5 shuffles the target feature instead of removing it to make it useless so that we don t need to re train the model again. In machine learning player is feature and contribution is score. For example we have 3 features L M and N. Effective feature engineeringSay you have a model which takes 1 hour to train. Interpretable Machine Learning with XGBoostThis is the background of Interpretable machine learning which is a field receiving a lot of attention recently. Permutation importance5. SHAP valuesSHAP proposed a new fair way to measure contribution which is justified in game theory. They built a simple model which works well only on a local point We don t need to predict on the all points. There are several options for measuring importance like split How many times the feature is used to split gain The average training loss reduction gained when using a feature for splitting. The implementation is available on GitHub slundberg shap A unified approach to explain the output of any machine learning model https github. We can see the score and the execution time. Score gain on a complex modelChecking on the real model. Try various features on a simple model4. this is what I was doing above but more reliable However there is room to discuss how to define measure contribution. Then use the simple model to interpret how it s trained. org wiki Lloyd_Shapley. com lopusz awesome interpretable machine learning 4. The shapley values are calculated like below. If you successfully setup an effective environment for experiments at the beginning of the competition it puts you at an advantage. Permutation importanceThe basic idea is that observing how much the score decreases when a feature is not available the method is known as permutation importance or Mean Decrease Accuracy MDA. They said that you get more accurate result when you check on the actual complex model you carefully built. I d like to share what I ve done so far. In here if the value on the target is close to 0 it means that the feature may be irrelevant to the target. You can find papers and libraries here lopusz awesome interpretable machine learning https github. com slundberg shap SHAP values represent the fair score of features depending on their contribution towards the total score in the set of features. Score gain on a simple model3. Simple models are easy to interpret. This should make us very uncomfortable about relying on these measures for reporting feature importance without knowing which method is best. I m trying from the top of the list when I come up with a new idea. get_dummies df matchType axis 1 Option 3 Drop it for now Not the best solution. Build a complex model5. This method was proposed in this paper Why Should I Trust You Explaining the Predictions of Any Classifier https arxiv. Next let s see aggregated features. I d like to hear how you work on feature engineering. 0033 standard deviation. But it can be another risk if longer training time reduces the number of your trials. To our dismay we see that the feature importance orderings are very different for each of the three options provided by XGBoost For the cover method it seems like the capital gain feature is most predictive of income while for the gain method the relationship status feature dominates all the others. Do you always need to wait for 1 hour to see the impact of a newly added feature I talked with experienced Kaggers about feature engineering. Aside from the main topic of this Kernel it s better to split dataset by match since we predict results by group in match. Removing a feature and see the difference. I m still new to this field. Feature importances of Tree models4. SHAP also can visualize how the score changes when the feature value is low high on each data. Thank you for reading this Kernel. pdf 2016 known as LIME The main contribution of SHAP is that they introduced the concept of Shapley Value to measure the contribution. run_experiments method takes preprocess functions and returns DataFrame. The figure shows the importance of each feature. Train with promising featuresThey go back and forth between steps during a competition. However the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret such as ensemble or deep learning models creating a tension between accuracy and interpretability. ", "id": "rejasupotaro/effective-feature-engineering", "size": "7253", "language": "python", "html_url": "https://www.kaggle.com/code/rejasupotaro/effective-feature-engineering", "git_url": "https://www.kaggle.com/code/rejasupotaro/effective-feature-engineering", "script": "sklearn.metrics lightgbm reload killPlace_over_maxPlace min_by_team headshotKills_over_kills rank_by_team PermutationImportance mean_by_team seaborn numpy original items sum_by_team matplotlib.pyplot run_experiments LGBMRegressor pandas run_experiment eli5.sklearn max_by_team players_in_team walkDistance_over_kills total_distance walkDistance_over_heals teamwork mean_absolute_error median_by_team sklearn.linear_model train_test_split LinearRegression ", "entities": "(('feature when value', 'low data'), 'visualize') (('featuresThey', 'competition'), 'train') (('it', 'feature engineering'), 'use') (('Shapley Value', 'Lloyd Shapley https'), 'be') (('implementation', 'machine learning model https github'), 'be') (('com slundberg shap SHAP values', 'features'), 'represent') (('rank how important features', 'competition'), 'see') (('method', 'permutation importance'), 'be') (('how you', 'feature engineering'), 'd') (('which', 'attention'), 'be') (('they', 'contribution'), 'pdf') (('I', 'Kernel'), 'show') (('we', 'L M'), 'have') (('which', '1 hour'), 'engineeringSay') (('feature', 'target'), 'in') (('It', 'features'), 's') (('t', 'points'), 'build') (('when I', 'new idea'), 'm') (('we', 'match'), 's') (('figure', 'feature'), 'show') (('you', 'new feature'), 'be') (('which', 'game theory'), 'propose') (('it', 'advantage'), 'setup') (('longer training', 'trials'), 'be') (('You quickly quickly d', 'process'), 'depend') (('how it', 'Then simple model'), 'use') (('You', 'papers'), 'find') (('player level', 'features'), 'let') (('training loss average reduction', 'splitting'), 'be') (('I', 'measure more However how contribution'), 'be') (('relationship', 'status feature others'), 'see') (('You', 'Classifier https arxiv'), 'propose') (('Feature importances', 'feature importances'), 'output') (('Option', 'category df matchType df matchType'), 'give') (('you', 'actual complex model'), 'say') (('faster less things', 'complex modelThe upper things'), 'be') (('I', 'competition'), 'create') (('However sometimes it', 'actual contribution'), 'doesn') (('run_experiments method', 'DataFrame'), 'take') (('we', 'model'), 'shuffle') (('even experts', 'accuracy'), 'achieve') (('certain prediction', 'many applications'), 'pdf') (('CorrelationThis', 'features'), 'be') (('I', 'complex model'), 'start') (('I', 'feature engineering'), 'need') (('method', 'feature importance'), 'make') (('we', 'machine learning'), 'question') (('We', 'score'), 'see') (('How they', 'competition'), 'result') (('it', 'score'), 'be') ", "extra": "['test']"}