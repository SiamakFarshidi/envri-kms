{"name": "ii shopee model training with pytorch x rapids ", "full_name": " h1 Model Training Submission h1 1 Introduction h3 Libraries W B h1 2 Load the Data h1 3 Competition Metric h1 4 PyTorch Dataset h3 The Bert Tokenizer data from Abhishek Thakur h1 5 Grouping using Image Embeddings h2 I Retrieving the embeddings h2 II Creating the predictions h3 Bonus 3D Plotting on Image Embeddings Clusters h1 6 Grouping using Text Embeddings h2 I Retrieving the embeddings h2 II Creating the predictions h3 Bonus 3D Plotting on Text Embeddings Clusters h1 7 Final predictions h2 Submission h1 Specs on how I trained h3 on my local machine ", "stargazers_count": 0, "forks_count": 0, "description": "Load the DataLet s read the data by always taking into account the state of the notebook whether is in submission or commiting process. com lukemelas EfficientNet PyTorch. com c shopee product matching discussion 230486 Creating the splits to prevent memory errors Making the prediction Cosine similarity distance Clean environment Add predictions to dataframe text_embeddings_df cudf. input shopee preprocessed data image_embeddings. DataFrame data principalComponents columns pc_1 pc_2 pc_3 finalDf cudf. Again the methodology is highly inspired from PART 2 RAPIDS TfidfVectorizer CV 0. Bonus 3D Plotting on Image Embeddings Clusters We ll use PCA to downsize the data from 1000 features to only 3. To avoid any memory errors you would want to also experiment by pushing 70 000 rows as well to make sure your code isn t crushing somewhere along the way. co transformers preprocessing. This is how the distribution shows in the W B dashboard Submission Note Don t forget to disable the Internet access before submitting. Nikita Kozodoi has kindly already created this for us here https www. com andradaolteanu shopee preprocessed data. To submit it you ll have to set the Internet Off and to comment the lines of code that save information into the W B Project. csv data COMPUTE_CV False dataset_data variable will contain test. csv has more than 3 values CPU data Read in data Set a filepath column Map on for each product all posting_id that are labeled as the same GPU data CPU data No Target Here GPU data data pd. com abhishek bert base uncased code datasetId 431504 sortBy voteCount Pretrained tokenizer that splits sentences into tokens source from transformers library click here for more info https huggingface. Hence I have saved the image_embeddings numpy array here https www. Introduction Goal Building a model that can identify which images contain the same product s. Hence we can t use the label_group as our target y feature. For submission we ll read in test. Competition MetricLet s now understand the competition metric. To consider This competition is a little different as it doesn t use Supervised ML Techniques but Unsupervised ML Techniques. PyTorch DatasetWe ll create a Dataset class called ShopeeDataset that will 1. Model Training Submission 1. Final predictionsNow that we have predictions linked to both image and title embeddings we can combine them and create the final predictions that we ll also submit to the leaderboard. However when we ll commit it the data will access the 70 000 hidden rows in the test. Hence we ll create clusters of a maximum size of 50. I usually like to have this down as it is a very important part of the prediction process. sum Plot Extract the Tf Idf Matrix TODO Extract more features add preprocessing from notebook I Save image_embeddings to W B TODO to be developed https www. ai andrada shopee kaggle workspace user andrada. Return the necessary information to feed into the model afterwards The Bert Tokenizer data from Abhishek Thakur https www. What we are doing is appending to EACH batch of images 16 1000 the ids extracted from BERT 16 16 and the masks 16 16 16 1032 II. The goal is to group similar products together although we have a target variable named label_group in the train dataset there can be multiple other types of groups in the test dataset completely unseen during training. Libraries W B You can find my W B Dashboard on this competition here https wandb. Receive the metadata2. load_state_dict torch. cuda as well Add information from ids and mask as well Concatenate all embeddings Save it to a binary file in NumPy. input pretrained pytorch models resnet50 19c8e357. The Embeddings are actually the abstract representation of the images input an image of 3 256 256 3 channels of size 256x256 output an array of 1000 items which is the abstract representation of the input structure see image below I. fit_transform X principalDf cudf. Remember if COMPUTE_CV True dataset_data variable will contain train. loc 2000 axis 0 data_gpu cudf. Perform image augmentation and tokenization4. Grouping using Text EmbeddingsAs we also have the title of the image available it would be a shame not to use this data for predicting as well. co transformers glossary. Creating the predictions Bonus 3D Plotting on Text Embeddings Clusters We ll use PCA to downsize the data from 1000 features to only 3. values X StandardScaler_gpu. Inspiration HUGE thanks to Chris Deotte for creating a trendsetter notebook with a baseline so we can all get started and to zzy990106 for his PyTorch version on Chris s work. Retrieving the embeddings A TfIdf Process looks like the example below II. In this part we ll create a TfIdf Vectorizer to extract these embeddings. explained_variance_ratio_. Specs on how I trained on my local machine Z8 G4 Workstation 2 CPUs 96GB Memory NVIDIA Quadro RTX 8000 RAPIDS version 0. You can find more on PyToch EfficientNet here https github. Creating the predictionsThe competition says that group sizes are capped at 50 so there is no benefit to predict more than 50 matches. Read in the image and title 3. html attention mask Now we can create the dataset and the dataloader. Note This notebooks uses internet to connect to the W B Dashboard. cuda model_resnet. com cdeotte part 2 rapids tfidfvectorizer cv 0 700 So without doing anything we have a CV score of 0. 17 Libraries CPU Libaries GPU Pytorch Deep Learning Environment check Secrets Color scheme Device Base paths When running on the CuDNN backend two further options must be set Set a fixed value for the hash seed Set COMPUTE_CV value Switch to False if test. concat principalDf y axis 1 All images that have the same phash are identical so we ll add these too Concatenate all predictions Return combined unique preds Plot F1 Score on product Make a custom plot to save into W B Prepare data Create Table. html The output is as follows input_ids indices corresponding to each token in the sentence attention_mask indicates to the model which tokens should be attended to and which should not documentation on attention_mask here https huggingface. com kozodoi efficientnet pytorch. csv data For commiting we ll read in train. Grouping using Image EmbeddingsNow we can safely extract the embeddings from our images using EffNet. concat data_gpu data_gpu data_gpu. save image_embeddings all_image_embeddings Read in image_embeddings all_image_embeddings np. loc 2000 axis 0 Let s look at it Save data to W B Artifacts Find the common values in target and prediction arrays. csv so we can plot a CV score as well When this notebook is commited the data variable will have 34 000 rows. npy Save image_embeddings to W B Clean memory Create the model instance Train the model Creating the splits to prevent memory errors Making the prediction Clean environment Add predictions to dataframe Create dataframe Separating out the features Standardizing the features Separating out the target PCA pca. This means that the amount of observations pushed through the pipeline will double. This notebook has the purpose of going deeper with the explanations regarding the code and process and an attempt of improving the baseline score as we go along. 67 with a submission score in Leaderboard of 0. DataFrame text_embeddings X text_embeddings_df. pth Extract embeddings of the image the EffnetB0 representation We aren t training only extracting the representation Don t forget to append the image to. fit_transform X y data label_group pca PCA_gpu n_components 3 principalComponents pca. Retrieving the embeddings Note Because we do not have Internet access for this notebook we need to import the EffNet model from a dataset. Computes the score by following the formula Create artificial prediction column Get F1 score for each row Instantiate one of the tokenizer classes of the library from BERT Image Augmentation Read in image and text data Transform image transpose channels color height width Tokenize the text using BERT Return dataset info Compute dataloader for test data Extract Efficientnet and put model on GPU model_resnet resnet50 pretrained False. Note The cell below takes 6 mins to run. concat data data data. ", "id": "andradaolteanu/ii-shopee-model-training-with-pytorch-x-rapids", "size": "6723", "language": "python", "html_url": "https://www.kaggle.com/code/andradaolteanu/ii-shopee-model-training-with-pytorch-x-rapids", "git_url": "https://www.kaggle.com/code/andradaolteanu/ii-shopee-model-training-with-pytorch-x-rapids", "script": "torch.utils.data albumentations PCA Compose __init__ cuml.decomposition resnet34 Resize VerticalFlip UserSecretsClient PCA as PCA_gpu TfidfVectorizer DataLoader cuml.neighbors resnet50 get_f1 torch.nn transformers StandardScaler as StandardScaler_gpu seaborn numpy NearestNeighbors HorizontalFlip EfficientNet mpl_toolkits __getitem__ efficientnet_pytorch cuml.experimental.preprocessing sklearn.decomposition ShopeeDataset(Dataset) combine_predictions F1_score Normalize find_matches_cupy matplotlib.pyplot kaggle_secrets pandas set_seed Dataset StandardScaler __len__ torch.nn.functional AutoTokenizer sklearn.preprocessing torchvision.models cuml.feature_extraction.text mplot3d ", "entities": "(('how distribution', 'Internet access'), 'be') (('com andradaolteanu', 'shopee data'), 'preprocesse') (('we', 'baseline score'), 'have') (('together we', 'completely training'), 'be') (('we', 'embeddings'), 'create') (('we', 'EffNet'), 'extract') (('Nikita Kozodoi', 'https us here www'), 'create') (('that', 'GPU data CPU same data'), 'have') (('images', 'same product'), 'build') (('which', 'I.'), 'be') (('Hence we', '50'), 'create') (('DataFrame data principalComponents', 'pc_1'), 'column') (('html attention Now we', 'dataset'), 'mask') (('notebooks', 'W B Dashboard'), 'note') (('we', 'also leaderboard'), 'predictionsNow') (('uncased code datasetId 431504 sortBy voteCount Pretrained that', 'info https here more huggingface'), 'click') (('Don t', 'image'), 'embedding') (('we', 'dataset'), 'retrieve') (('little it', 'ML Supervised Techniques'), 'consider') (('that', 'Dataset class'), 'create') (('data', 'test'), 'access') (('We', 'only 3'), '3d') (('amount', 'pipeline'), 'mean') (('similarity distance Clean environment', 'text_embeddings_df cudf'), 'discussion') (('Tokenize', 'False'), 'compute') (('Hence I', 'numpy https array here www'), 'save') (('we', 'work'), 'HUGE') (('Hence we', 'y feature'), 'use') (('we', '0'), 'com') (('group sizes', 'so more than 50 matches'), 'say') (('isn t', 'somewhere way'), 'avoid') (('Libraries W You', 'https here wandb'), 'b') (('Return', 'W B Prepare data Create Table'), 'concat') (('We', 'only 3'), 'create') (('embeddings', 'NumPy'), 'add') (('Again methodology', 'RAPIDS TfidfVectorizer highly PART 2 CV'), 'inspire') (('Clean environment', 'target'), 'save') (('csv we', 'train'), 'datum') (('0 s', 'target'), 'loc') (('it', 'prediction very important process'), 'like') (('TfIdf Process', 'II'), 'retrieve') (('input', 'data shopee image_embeddings'), 'preprocesse') (('I', 'https developed www'), 'extract') (('available it', 'data'), 'have') (('Set COMPUTE_CV value', 'False'), 'check') (('csv data COMPUTE_CV False dataset_data variable', 'test'), 'contain') (('data variable', '34 000 rows'), 'csv') (('that', 'W B Project'), 'have') (('16 1000 ids', 'BERT'), 'append') (('which', 'attention_mask'), 'be') (('COMPUTE_CV True dataset_data variable', 'train'), 'remember') (('You', 'PyToch https EfficientNet here github'), 'find') ", "extra": "['test']"}