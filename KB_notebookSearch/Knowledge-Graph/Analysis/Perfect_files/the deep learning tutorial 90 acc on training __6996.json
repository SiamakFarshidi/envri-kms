{"name": "the deep learning tutorial 90 acc on training ", "full_name": " h1 Table of Contents h1 Article on medium publication h1 What would be the workflow h1 Problem Identification h1 What data do we have h1 Exploratory data analysis h1 Additional analysis h1 Data preparation including feature engineering h1 Creating a Model h1 Model evaluation h1 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "predict test y_final y_preds 0. Exploratory data analysis One important aspect of machine learning is to ensure that the variables show almost the same trend across train test data. represents the economic status. We can create more features using this Cabin variable. SibSP This variable refers to number of siblings spouse onboard. Approximately 62 of Pclass 1 passenger survived followed by 47 of Pclass2. Developing a model 6. The best way to learn is by practicing so please feel free to tweak the parameters and use the framework to further improve. Variable source is a kind of tag which indicates data source in combined data Let s check the data PassengerID Drop PassengerID Pclass Use as it is Name Extract Salutation from Name variable Convert other salutations to fixed Title Drop Name variable Age Index of NaN age rows Sex Create dummy variables combdata Sex combdata Sex. Step By Step Tutorial For Beginners http www. com content uploads 2016 04 blog twenty one business icebergs sink business 280416. com rp1611 model ensembles for survival prediction a3ecc9f7c2ae and access the blog. Data preparation including feature engineering What we need to do to process following variables PassengerID No action required PClass Have only 3 numerical values. Ticket This variable has alphanumeric value which might not be related to Survival directly but we can use this variable to create some additional features. astype int submission pd. Embarked C Cherbourg Q Queenstown S SouthamptonLet s explore the variable with Survival rate. The kernel is purely for learning purpose so I would keep the kernel simple and leave it as it is post training the model. What data do we have 3. Example Let s start with finding the number of missing values. jpg So it is a classification problem and you are expected to predict Survived as 1 and Died as 0. SibSP 1 and SibSP 2 shows higher chances of survival. Data preparation including feature engineering 5. map male 0 female 1 or Create a variable representing family size from SibSp and Parch Create new feature of family size Analyze the Survival Probability by Fsize Drop FSize variable SibSp Create dummy variables Parch Create dummy variables Ticket Extracting the ticket prefix. com rf image_large Pub p9 AJC 2018 07 12 Images newsEngin. 22048809_071418 titanic_Titanic Image 7 2. input gender_submission. com rp1611 step by step tutorial for beginners All data visualization remains the same however instead of emsembling method I have tried simple neural network in this Kernal. If you find this notebook helpful Please upvote and or leave a comment Import the basic python libraries Read the datasets We have 891 observations 12 columns. Fare Let s check the distribution first. You can click this clink https medium. If you compare the output you will see that missing value percentages do not vary much across train test datasets. Write down things like what are you expected to do what data you might need or let s say what all algorithms you plan to use. Parch Parch indicates number of parents children aboard the Titanic. I will give you one example here. As we do variable analysis try to replicate wherever applicable the code for test data and see if there is any major difference in data distribution. drop labels source Sex_male Fsize LargeF SibSp_8 Parch_9 T_WEP Cabin_T Emb_Q axis 1 inplace True You may want to drop some variables to avoid dummy variable trap X_train train. Frankly speaking no one cares. Note that Parch 3 and Parch 1 shows higher survival probabilities. Exploratory data analysis 4. Name Not relevant from analysis modeling perspective. If you like this notebook or find this notebook helpful Please upvote and or leave a comment Article on medium publication I also wrote an article on medium on the same topic. If there is no prefix replace with U Unknown. If not it would lead to overfitting because model is representing a relationship which is not applicable in the test dataset. Understand the problem first and draft a rough strategy on a piece of paper to start with. Problem Identification 2. What data do we have Let s import necessary libraries bring in the datasets in Python environment first. While Title Age feature represents the Age category of passengers the features like Fare PClass Cabin etc. Based on findings we can conclude that Age Gender features representing social economic status were primary factors affecting the survival of passenger. Table of ContentsThis is my 2nd Kernel for this competition. 687 missing values in train 327 missing values in test data which needs to be treated. Name Can be used to create new variable Title by extracting the salutation from name. Additional analysis Let s create few additional charts to see how different variables are related. The purpose of this kernel is to show how a simple NN model can be constructed. Embarked represents port of embarkation. 99 on training dataset. Link to my previous kernel is pasted below. Fare Check the number of missing value Only 1 value is missing so we will fill the same with median Use the numpy fuction log1p which applies log 1 x to all elements of the column Check the new distribution Cabin Replace the missing Cabin number by the type of cabin unknown U Let s plot the survival probability by Cabin Create dummy variables Embarked Find the number of missing values Fill Embarked missing values of dataset set with mode S Create dummy variables Create dummies for PClass Now You may want to drop some variables to avoid dummy variable trap test. Once we have the datasets in Python environment we can slice dice the data to understand what we have and what is missing. We have 417 observations 11 columns no response Survived column. We will use it as it is. Model evaluation 7. Cabin Alphanumeric variable. This will help you to stay on track. jpg Based on data above female passengers had better chances of survival than male passengers Age The insight below connects back to Ladies and Kids First scene of the movie. Let s look at the distribution. As the analysis output below suggests Emabrked C shows high probabilities of survival. csv index_col PassengerId submission Survived y_final. This might be a representation of class compartment. Before you jump to How to do this part like typical Data Scientists understand What Why part. See the mix of variable types. It s all about experimenting and learning. It shows that a good number of babies young kids survived. Use the groupby univariate bivariate analysis method to compare the distribution across Train Test data PassengerId Not relevant from modeling perspective so we will drop this variable later Pclass Pclass is categorical variable. So here is the workflow. Conclusions That s all you need to solve a data science problem. We will do this during feature engineering process. What would be the workflow I will keep it simple crisp rather than using buzz words useless data science frameworks. to_csv NNPrediction1. The model provides accuracy of 87. Problem Identification Best Practice The most important part of any project is correct problem identification. drop labels Survived Sex_male Fsize LargeF SibSp_8 Parch_9 T_WEP Cabin_T Emb_Q axis 1 Creating the model Inputing the first layer with input dimensions Adding an Dropout layer to prevent from overfitting adding second hidden layer Adding another Dropout layer adding the output layer that is binary 0 1 Visualizing the model Creating an Stochastic Gradient Descent Compiling our model Fitting the ANN to the Training set y_preds model. Sex Create dummy variables Age Missing value treatment followed by creating dummy variables SibSP Create dummy variables Parch Create dummy variables Ticket Create dummy variables post feature engineering Fare Missing value treatment followed by log normalization Cabin Create dummy variables post feature engineering Embarked Create dummy variables PassengerID Pclass Name Title Age Sex Creating Family Size variable using SibSp Parch SibSp Parch Ticket Fare Embarked Creating a Model Model evaluation Conclusion Title Sex_Female Fare PClass seems to be common features preferred for classification. We need to transform this variable using log function and make it more normally distributed. astype int submission. The Fare variable is right skewed. We will drop this feature later after creating a new variable as Title. Sex Based on analysis below female had better chances of survival. Check missing values in train data set Check missing values in train data set Fill empty and NaNs values with NaN Analyze the count of survivors by Pclass Analyze the Survival Probability by Pclass Count the number of passengers by gender Analyze survival count by gender Analyze the Survival Probability by Gender Let s explore the distribution of age by response variable Survived We can also say that the older the passenger the lesser the chance of survival Analyze the count of survivors by SibSP Analyze probability of survival by SibSP Analyze the count of survivors by Parch Analyze the Survival Probability by Parch for some statistics Get the fitted parameters used by the function Now plot the distribution Let s check the unique values Analyze the count of survivors by Embarked variable Analyze the Survival Probability by Embarked Age Pclass Survival Age Embarked Sex Pclass Relation among Pclass Gender Survival Rate Relation among SibSP Gender Survival Rate Relation among Parch Gender Survival Rate Let s combining train test for quick feature engineering. Now the Titanic challenge hosted by Kaggle is a competition in which the goal is to predict the survival or the death of a given passenger based on a set of variables describing age sex or passenger s class on the boat. ", "id": "rp1611/the-deep-learning-tutorial-90-acc-on-training", "size": "6996", "language": "python", "html_url": "https://www.kaggle.com/code/rp1611/the-deep-learning-tutorial-90-acc-on-training", "git_url": "https://www.kaggle.com/code/rp1611/the-deep-learning-tutorial-90-acc-on-training", "script": "keras.layers Dropout Sequential SGD seaborn numpy scipy.stats matplotlib.pyplot skew #for some statistics Activation stats tensorflow Dense pandas norm keras.optimizers StandardScaler scipy sklearn.preprocessing keras.models ", "entities": "(('best way', 'framework'), 'be') (('variables', 'train test data'), 'analysis') (('You', 'X_train train'), 'Fsize') (('I', 'buzz data science simple rather words useless frameworks'), 'be') (('Table', '2nd competition'), 'be') (('which', 'test 327 missing data'), 'value') (('how different variables', 'few additional charts'), 'let') (('Drop Pclass it', 'Sex Create variables combdata Sex dummy combdata'), 'be') (('that', 'Training set'), 'label') (('it', 'model'), 'be') (('value missing percentages', 'train test much datasets'), 'compare') (('Step', 'www'), 'http') (('young kids', 'babies'), 'show') (('Pclass later Pclass', 'variable'), 'use') (('s', 'missing values'), 'let') (('Data typical Scientists', 'What'), 'jump') (('jpg', 'Kids First movie'), 'have') (('This', 'class compartment'), 'be') (('SibSP variable', 'siblings'), 'refer') (('We', 'Title'), 'drop') (('s', 'feature quick engineering'), 'check') (('s', 'Python environment'), 'Let') (('Problem Identification Best most important part', 'project'), 'practice') (('Age Gender', 'passenger'), 'conclude') (('csv index_col PassengerId submission', 'y_final'), 'Survived') (('We', 'feature engineering process'), 'do') (('Embarked C Cherbourg Q Queenstown SouthamptonLet s', 'Survival rate'), 'S') (('Approximately 62', 'Pclass2'), 'survive') (('NN how simple model', 'kernel'), 'be') (('you', 'data science problem'), 'conclusion') (('classification you', '0'), 'jpg') (('what', 'what'), 'slice') (('Parch', 'survival 3 1 higher probabilities'), 'note') (('directly we', 'additional features'), 'have') (('I', 'Kernal'), 'step') (('We', 'Cabin variable'), 'create') (('Parch Parch', 'Titanic'), 'indicate') (('variable analysis', 'data major distribution'), 'try') (('Cabin variables post feature engineering variables Pclass Name Title Age Sex Creating Family Size Create dummy Embarked Create dummy variable', 'common classification'), 'create') (('Sex', 'survival'), 'have') (('We', 'Survived column'), 'have') (('PClass', 'only 3 numerical values'), 'preparation') (('We', '891 observations'), 'find') (('Title Age feature', 'Fare PClass Cabin etc'), 'represent') (('Now You', 'trap dummy variable test'), 'check') (('Link', 'previous kernel'), 'paste') (('it', 'log function'), 'need') (('goal', 'boat'), 'be') (('SibSP', 'survival'), 'show') (('Name', 'name'), 'use') (('which', 'test dataset'), 'lead') (('I', 'same topic'), 'write') (('you', 'algorithms'), 'write') (('Emabrked below C', 'survival'), 'suggest') ", "extra": "['gender', 'test']"}