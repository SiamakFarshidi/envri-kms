{"name": "protein atlas exploration and baseline ", "full_name": " h1 Welcome to the Human Protein Atlas Competition h2 Latest important updates h2 Planned updates h3 Our goal h2 Table of contents h1 Preparation h2 Kernel settings h2 Improved model settings h2 Loading packages and data h2 Extract test names for submission h2 Helper code h2 Extract train targets h1 Exploratory data analysis h2 Which proteins occur most often in train images h3 Take Away h2 How many targets are most common h3 Take away h2 Which targets are correlated h3 Take away h2 How are special and seldom targets grouped h3 Lysosomes and endosomes h3 Rods and rings h3 Peroxisomes h3 Microtubule ends h3 Nuclear speckles h3 Take away h2 How do the images look like h3 Peek into the directory h2 How do images of specific targets look like h3 Take Away h1 Building a baseline model h3 K Fold Cross Validation h3 Collecting ideas h3 Shared Parameter class h3 Image Preprocessor h4 Looking at a preprocessed example image h3 Data Generator h3 CNN Baseline model using keras h3 Training the baseline on the first cv fold h3 What do the results tell us h3 Take Away h3 To which targets do the high and small predicted probabilities belong to h3 Take Away h1 One Step Improvement Model h3 The target wish list h3 Plug and Play h3 Add scoring metrics h3 Track losses and scores h3 More epochs h3 What does the loss tell us h3 Does our model try to classify h3 Make a test submission h3 Choosing the decision thresholds h2 How can we tackle gradient jiggles h3 Dropout Layer h2 Where to go next ", "stargazers_count": 0, "forks_count": 0, "description": "Perhaps this patterns might help to build a more robust model How do the images look like Peek into the directoryBefore we start loading images let s have a look into the train directory to get an impression of what we can find there Ah ok great It seems that for one image id there are different color channels present. This imbalance becomes even more dramatic for seldom targets like rods and rings. Consequently our high accuracy belongs to the high correct prediction of the absence of target proteins. Explore some further concepts like batch normalization4. As our initial weights are not sufficent to solve the classification task we need many such update steps. This is great as this would mean that some weight that could be good for predicting dogs will not change after we process a batch with cats only. read_csv Any results you write to the current directory are saved as output. Visual analysis of target protein distribution in the train set 3. This need not be very complex or very good. who knows Consequently it could be nice to introduce some further statistical quantities like min as well as max and the standard deviation. The competition says that it shows the stained target proteins and consequently it s hopefully the most informative one. It looks like if the cells are of different types. Clicking this code tab again with other targets you can observe that the images can contain different cell densities. Looking into the data description of this competition we can find that Each image is actually splitted into 4 different image files. All probabilities are close to zero and there are only a few with targets where our model predicted a protein structure with higher than 10. Well there is one problem with the competition score that we can use to measure the performance of our model The f1 mean. This means that we already start learning without passing the whole dataset. A simple image generator that yields images of a target protein wishlist. Then the jump was so big that we escaped from nice regions that lead to a local minimum of loss. __init__ modelparameter self. drop Id Target number_of_targets axis 1. One step we try to improve nucleoplasmn and the next perhaps cytosol but with a downgrade of the nucleoplasmn predictions and the next steps it could be the other way round. The images differ in their intensities and the target proteins are not always located the same way. If we take a look at the true target label count we can see that most of our targets are filled with zero. Using tensorflow estimators perhaps this helps with slow prediction process 2. This makes sense For each image we have a high probability to contain either 1 or 2 target protein structures. And in this process microtubules and thier ends are active and participate as well. Now let s consider a dropout layer It randomly selects a given percentage of input neurons and drops them during the current training step. In addition we find that the mitotic spindle often comes together with the cytokinetic bridge. Gradient clippingThese two will be the next improvement steps. We should attach more importance to true positives. Hence our model learns faster. Ok you can see that beside our wishlist targets all others are filled with zero. This tools makes it easy for us to build and train neural networks. Thank you very much We can easily add it to our model. This is of course very bad Let s go one step deeper and take a look at the Cytosol choose another feature if you like. Due to performance reasons I will only use one cv fold to explore results and one repeat Collecting ideasNext we need to setup a simple baseline model. With a batch_size of 128 and number of epochs 10 we obtain these losses Dropout LayerI often read that dropout helps to avoid overfitting but for me it seems that there is one more useful advantage Imagine the cat problem above Given a batch full of cats we only compute gradients for making the predictions of cats better. Ok now let s increase the number of epochs and decrease the batch_size. In this example you can see a maximum amount of 3 images at one iteration. More than 3 targets are very seldom Which targets are correlated Let s see if we find some correlations between our targets. For this purpose let s use the deep learning library keras https keras. Luckily the predict generator runs through the ids given in partition validation one after another and we can easily add them as index. This splitter is now a generator. Using transfer learning3. We don t acutally now how informative they are and in our current state they would blow up our neural network with a huge amount of network weigths that we might not need. And the randomness of frosty the dropout snowman can turn to a problem difficult to graps Which neurons should be dropped are there some for which dropping is good and some for which it is bad After which layer in our network does it make sense to use it Perhaps it would have been better to use some neurons that were dropped a random dropout session during one batch learning step. These 4 files correspond to 4 different filter a green filter for the target protein structure of interest blue landmark filter for the nucleus red landmark filter for microtubules yellow landmark filter for the endoplasmatic reticulum Each image is of size 512 x 512Let s check if the number of files divided by 4 yields the number of target samples How do images of specific targets look like While looking at examples we can build an batch loader Let s try to visualize specific target groups. I have never mind that the results are useful for many kagglers that like to start with the competition but need some starter code or some inspiration. Hence if you call splitters split method it will yield one Fold of the repeated K Folds. For example there could be chunks with very seldom targets that may obtain a bad score and some chunks with very common targets and a very good score. One Step Improvement onestep The target wishlist Add scoring metrics Track losses and scores More epochs Preparation Kernel settingsThis notebook contains model fitting that may take some time. But even though our next goal should be to find out what to tune in such a way that our model really starts learning One Step Improvement Model Ok again let s go one step back and choose the most common target proteins that are present in our data nucleoplasm cytosol and plasma membrane. Let s create an instance of this preprocessor and pass it to the data generator. add Conv2D 32 3 3 activation relu self. Let s take a look at the predicted probabilities Ah I have forgotten to plug in the Ids as index such that we can easily compare with the true validation labels. Some ideas on validation. A baseline model build with keras that is supported by A modelparameter class that holds all parameters that are necessary to build the model to load the data and to preprocess the images. Looking at a preprocessed example imageYou can see that we have lost a lot of information by downscaling the image Data GeneratorI highly build upon the nice data generator presented by Shervine Amidi. To make it easier for you to checkout new content here is an update summary of the latest changes Started to add a table of contents. There is no deviation no difference between them. Consequently if you have a problem with overfitting learning too much out of your training data and loosing generalization performance or you have very small imbalanced batches you should consider dropout as a strategy. Hence let s improve again and turn dropout to a plug and play feature class ImprovedModel BaseLineModel def __init__ self modelparameter my_metrics f1_mean f1_std f1_min f1_max use_dropout True super. Perhaps we can expand its functionality during improvement of the baseline model. use_dropout use_dropout def build_model self self. I m going to use a small class that hold parameters that are used or shared between the data loader the image preprocessor and the baseline model. Its our first attempt to play with and to figure out how to improve. Passing an instance of this class to them reduced the risk of setting different parameters and obtaining mismatch errors for example during build compile of the network layers. But then we will need even more epochs as we make less weight updata aka learning steps But even if this stochastic gradient descent looks wiggly it has an advantage too It can escape from local minima of the loss function. coming soon Latest important updatesBeside building competition code I m still updating this kernel with ideas and code. After that we continue by computing the gradients with the next batch. Take away We can see that many targets only have very slight correlations. This way we use more weight update steps and hopefully makes our model learn more than before What does the loss tell us The loss is very noisy While decreasing the batch size we increased the number of learning steps. As our test data is 38 of size compared to the train set it makes sense to use 3 Fold cross validation where the test set is 33 of size compared to the train set. We gained more learning speed but payed with a reduced gradient quality. Consequently we make large jumps in weight space with each update step batch per batch. Set target values of your choice and the target group iterator will collect all images that are subset of your choice To keep the kernel dense the target group iterator has a batch size which stands for the number of examples you like to look at once. edu shervine blog keras how to generate data on the fly Thank you CNN Baseline model using keras Training the baseline on the first cv foldFirst of all let s choose one partition of train and validation data Next we have to setup our data generators We need two during training one for the train images and one for the validation images. Let s take a closer look to the competition scoring and alternatives F1 macro score Check out this nice implementation of Guglielmo Camporese https www. This way our model sees a different set computes some other gradients and hence the loss minimziation could be more robust. Here we can see a higher standard deviation than for all other samples and perhaps its corresponding distribution starts to diverge trying to get bimodal. Taking a look at this paper http www. The first image you can get by the loader shows endosomes that are spread all over the cells and in the second and third you can find endosomes and lysosomes more concetrated around the nucleus. Perhaps we will make nice predictions for Cytosol but bad ones for Nucleoplasmn. This could be done in sequence of samples or random with replacement. Some ideas on how to proceed. we can see a lot of noise after 100 batches during loss decrease. That s not nice Sometimes these jumps could even lead to exploding losses as well. Before we improve the model using more epochs and smaller batches let s implement a new feature a loss callback. Consequently we find a positive correlation between these targets. with smaller batch size there are fewer samples to learn from to compute gradients from The gradients we obtain may be very specific to the images and class labels that are covered by the batch of the current learning step. Welcome to the Human Protein Atlas Competition I started this kernel notebook to explore the data and build a simple baseline model to play with. With a total number of samples of around 20000 in my training set this means that I have already done 200 weight updates. First of all we should collect some ideas To stay simple let s use only the green channel image of our images per id. In contrast we weren t able to predict the presence of a target protein which is the most relevant part Now a bell should ring Have you ever heard about imbalanced classes and model confusion To which targets do the high and small predicted probabilities belong to Take Away Our baseline model seemed to learn something even if this something does not look very nice. More epochs Improve We can already see that we might need more than 1 epoch to learn the pattern in the data. If you like to dive deeper into target correlations you may like to take a look at my kernel notebook about target clustering. In addition we like to make predictions for the validation data and for the test submission data. With this knowledge we could start setting thresholds manually by comparing the probability distributions for hot and zero representations for each target in the wishlist. How does our model update gradients After each batch After one epoch If we take a look at simple feedforward networks that are close related to CNNs add gradient descent we can see that learning means to compute the derivatives of the loss with respect to the weights over all samples w_ i j new w_ i j old eta cdot partial_ w_ i j E With a set of independent observation samples we can obtain the gradients this way partial_ w_ i j E sum_ n 1 N partial_ w_ i j E_ n We are computing the change of the loss with respect to a change in the weights for each sample one after another. Consequently our model may try to learn from exotics. add MaxPooling2D pool_size 2 2 if self. One question on my mind draws circles What happened to the loss after each batch during one epoch Does it converge Has our model started to learn or does nothing happen Currently we obtain a history after calling fit_generator but this history only contains the loss of train and validation data after one epoch. To make things easier I added a small method to the DataGenerator we already had def get_targets_per_image self identifier return self. 1071 which are 3 28 in our best case. The mean is not robust towards outliers and consequently not very informative to understand the distribution of f1 scores for each target class. Well this really depends on the samples within one batch. it s mainly driven by high contributions. This choice was somehow arbitrarily as I used an example network that can be found to classify mnist digits as a starting point. Each sample that has at least one match with this list is returned. In this example we will see images that contain the protein structures lysosomes or endosomes. Let s use generators to only load data images of our batch and not all in once. Consequently in original gradient descent we need to pass the whole dataset once for just one single update step of gradient descent. In our case this means that each learning step some weights will be untouched and not used to compute gradients. There was a tradeoff we made. If you don t like to wait for compution you can set fit_baseline and or fit_improved_baseline of the KernelSettings class to False This way the prediction probabilities of the corresponding model are loaded as csv from added data source. com allunia in depth protein correlations How are special and seldom targets grouped Lysosomes and endosomesLet s start with these high correlated features Rods and rings Peroxisomes Microtubule ends Nuclear speckles Take away We can see that even with very seldom targets we find some kind of grouping with other targets that reveal where the protein structure seems to be located. But even with that score we should be careful We have 28 different classes that are very different in their frequency of being present. Hence before playing with further strategies we should make a step backwards again. Taking a look at the standard deviation we can see that all samples have nearly the same predicted values. The other images are like references showing microtubules nucleus and endoplasmatic reticulum. Consequently accuracy is not the right score here to measure your performance and validation strategy should be very fine. Helper code Extract train targetsOk great now we can directly work with binary target values. Tune hyperparameters5. In addition we have to be very careful with small batches as its target distribution might not reflect the overall pattern. In contrast small components like the lipid droplets peroxisomes endosomes lysosomes microtubule ends rods and rings are very seldom in our train data. Hence changing the weights with batches of imbalanced classes can lead to jiggles. num_classes activation sigmoid Our last losses were obtained by using dropout with high percentage 25 and 50 of dropped neurons. The train data is splitted into k chunks and each chunk is used once for testing the prediction performance whereas the others are used for training. Ok currently we haven t made any predictions and except from Id all entries are filled with 0. For this purpose we re going to extend the class we have already written. Perhaps we have prevented the success of a learning step given one batch and improved learning given another batch. For example it could be occupied with cats only. As far as I know this means one epoch. org papers volume15 srivastava14a srivastava14a. For this purpose I highly follow a descprition of a post in the www for which you will find the link below. You can see that in my baseline model I m already using dropout. Using keras fit_generator evaluate_generator and predict_generator we can directly connect them to keras without worrying much about how keras does its job. Track losses and scoresOk after adding the metrics we like to observe we should try to obtain more insights into the learning process of our model. and sometimes they could lead to the opposite. values This method just avoids the direct pass to y i the targets per image in a batch. In my case I chose a batch_size of 200. add Flatten self. This way we may already see that some proteins often come together. As our targets show relationships seemed to be grouped somehow the performance per test chunk probably highly depends on the target distribution per test chunk. Their label values are one whereas all others are zero. Let s try to understand why. add Dense 64 activation relu if self. What will happen if we do not use our dropout layers Where to go next optimization objective alias loss choice hyperparameter search linear algebra data processing CSV file I O e. Ideas on how to improve the baseline model by tracking loss with a keras callback. An image preprocessor that rescales reshapes and normalizes the images for feeding into the model. Improved model settingsWithin this kernel you can play with different improvement steps of the baseline model in a plug and play style. Thank you for pushing this kernel that far If you have just found this kernel here is a short summary of what you can find 1. In addition we can now say that we want to shuffle the samples before doing weight updates after each batch again in the next epoch. inside the for loop over temp list ids image identifiers of the batch. Make a test submissionOk now as our model started learning let s make a submission for the leaderboard. How many targets are most common Take away Most train images only have 1 or two target labels. Before we can transform our binary targets back to the submission target column we need to choose threshold for the absence and presence for each of them. We compute the gradients with respect to the weights after processing each batch this way partial_ w_ i j E sum_ m 1 M partial_ w_ i j E_ m Looking at the sum we can see one disadvantage. Consequently if we choose n_repeats 2 we will end up with 6 Folds in total 3 Folds for the first cross validation and again 3 Folds for the repeated cross validation. General better reading flow. This becomes more robust with more samples per batch. This would yield gradients that try to improve the detection of cats thereby changing the weights we might need to identify dogs. But before turning happy and starting to use dropout frequently we should think about its downside Dropping neruons during training and learning with a thinned kind of networks means that we freeze learning each step a bit. use_dropout self. This makes sense as both are participants for cellular division. Before increasing the batch size again and waiting too long for predictions we might improve by choosing another way 1. add Conv2D 16 kernel_size 3 3 activation relu input_shape self. Exploratory data anylsis explore Which proteins occur most often How many targets are most common Which targets are correlated How are special and seldom targets grouped How do the images look like How do images of specific targets look like 3. The information flow through the network is somehow a blackbox for us and this randomness of thinned network learning makes it more difficult to understand what s going on. Now we can overwrite this method in our ImprovedDataGenerator without loosing functionality Plug and Play This part makes fun Actually I don t know if our model learns something meaningful. Now let s transform our prediction back into the target column How can we tackle gradient jiggles Let s try to dive deeper into the problem. The mode is close to the fraction of one hot counts over all samples. Sometimes the whole image is covered with cells and sometimes there are only a few. Weight regularization2. This corresponds to an absence of corresponding target proteins. Encoding of binary target labels out of the given multilabel list per image 2. This way we can see the point of change when train loss is decreasing and validation loss is increasing. During call of predict generator its basepath of the images is changed from train to testpath What do the results tell us Let s have a look at predicted probabilites per target class Even though this accuracy looks nice it s an illusion We are far away from a good model. input_shape self. To reduce this effect we will repeat the K Fold several times and look at scoing distributions in the end. This way we can easily change something of this phase without producing chaos in the model itself or during data loading. As we are working with neural networks that can be demanding in computational resources let s only use 2 repetitions. But what if the dataset it too large to wait a long time just for one of these steps Perhaps it would be sufficient and already good to use only some M N of the N samples to compute the gradients partial_ w_ i j E partial_ w_ i j E sum_ m 1 M partial_ w_ i j E_ m This way we could use only a batch of samples compute the gradients and perform the update of weights. For these classes the prediction will be very difficult as we have only a few examples that may not cover all variabilities and as our model probably will be confused during ins learning process by the major classes. To observe the next 3 examples of your target group just run the cell below again. After doing so I have used each sample once and this means the whole dataset was passed through gradient descent. With a batch size of 10 we are likely to fill up these places with imbalanced targets. Due to this confusion we will make less accurate predictions on the minor classes. Taking a closer look at the base generator you can see that there is just one line code in def data_generation self list_IDs_temp we have to change namely the part with y i. This is just an assumption but perhaps one could use the red channel information to reveal cell types. Hence beside image outliers the target distribution itself influences the learning as well. If we like to detect same targets out of different bright images this will cause problems as we expect them to be in a similar value range. This way we can easily load images and targets given the chunk ids. This can cause jiggles as well. If we are not able to predict them we can go home and stay in bed. Preparation preparation Kernel settings Improved model settings Loading packages and data Extract test names for submission Helper code Extract train targets2. How to solve this jiggle wiggle problem Well first of all we might choose a higher batch_size again Decreasing the batch size we made a tradeoff We increased the learning speed but increased the risk of low quality gradients. This can be bad especially in the case of outlier images that are not representative to explain the pattern in the data. And this can be done to the detriment of the dogs as both classes share weights. At least the flat tail gives hope that learning could be in progress. com guglielmocamporese macro f1 score keras. The target wish listTo introduce a target wishlist that we can change whenever we want we need to improve the data generator. Even for the most common target nucleoplasm there are only 40 of samples that show it and 60 not. As 1 is the best we can only achive 0. my_metrics my_metrics self. model Sequential self. Image PreprocessorLet s write a simple image preprocessor that handles for example the rescaling of the images. We will perform the splitting on the image ids. Imagine we would try to distinguish dogs from cats. This way we can see the worst and gain some insights how the scores are spread over the classes. In addition we can see that some images have higher values whereas others are sallow with overall low values. But we can try to find out by improving our model adding new features and playing with different parameter settings. Computing the gradient with batches is a stochastic process Depending on the batch samples the gradients may sometimes lead to some good update directions in weight space that points to some minimum of loss. The latter I like to do in a plug and play style Setting the flag improve True adds a change to our model whereas improve False uses the old concept we already used in the baseline. It could be advantegous to write a small class that does simple preprocessing per image. X n_samples dim n_channels Initialization Generate data Store sample Store class Generate indexes of the batch Find list of IDs Generate data Datasets Run computation and store results as csv If you already have done a baseline fit once you can load predictions as csv and further fitting is not neccessary in contrast to the base DataGenerator we add a target wishlist to init Run computation and store results as csv If you already have done a baseline fit once you can load predictions as csv and further fitting is not neccessary Run computation and store results as csv If you already have done a baseline fit once you can load predictions as csv and further fitting is not neccessary Run computation and store results as csv If you already have done a baseline fit once you can load predictions as csv and further fitting is not neccessary. Our goal Predict various protein structures in cellular images there are 28 different target proteins multiple proteins can be present in one image multilabel classification 27 different cell types of highly different morphology Table of contents1. Take Away We can see that our model was always very uncertain to predict the presence of a target protein. Add scoring metricsWe have already seen that the accuracy score is an illusion and does not help to figure out how good our predictions are. Choosing the decision thresholdsFor this purpose we need the validation data again. What can we expect Well we are only working with 3 targets so far. For the test submission data we need to create a new instance of an ImagePreprocessor with same model parameters. It suffers under overfitting and looses it s ability to make good predictions on the validation images. pdf linked at keras documentation you can see that the forward and backward propagation is done only with this reduced thinned kind of network. Building a baseline model K Fold Cross ValidationLet s see how many test and train samples we have in this competition To understand the performance of our model we will use k fold cross validation. A data generator that can be used with CPU GPU computing to perform training and validation. Bulding a baseline model baseline Define the model How well does our model perform 4. In addition we have to deal with highly imbalanced classes per single target. Let s turn the Ids given in the validation labels as index as well Ok great. Let s create a dataframe for the test ids as well that we will use later to make our submission. For example we can see that rods and rings have something to do with the nucleus whereas peroxisomes may be located in the nucleus as well as in the cytosol. In this situation the model learns too much details out of the training images. This way you can run the cell until you have seen all images of your group without polluting the kernel Take Away Looking at this few examples we can already obtain some insights The staining of target proteins in the green channel was not equally successful. This would be great at it indicates that the model starts solving the problem of binary classification for this target No it does not seem that our model starts to separate well. Shared Parameter classOk now we will create an instance of this class and pass it to the DataGenerator the BaseLineModel and the ImagePreprocessor. This way we can pass both to the model and observe how the validation performance changes during training on the train images. Nonetheless one question remains Has our model started learning Can we see a separating force that tries to split zero and one predictions Does our model try to classify If this is the case and our model starts learning we should see more bimodal distributions of the predicted probability per target label Jeahy This looks far better than the distributions we obtained with the baseline model Thus increasing epochs while decreasing the batch size helped our model as we made more learning steps. Take a look at the loss values for train and validation data the notebook saved after learning It does not contain losses that are obtained after computing each batch. Here we know for each wishlist prediction the true target value. To make it easier to understand which improvements you currently use let s set them here in front of the analysis Loading packages and dataHow many samples do we have Extract test names for submissionThere are 11702 test images we are asked to make predictions. In contrast endosomes and lysosomes often occur together and sometimes seem to be located at the endoplasmatic reticulum. An image in the batch that causes very high positive or negative gradients for the weight w_ i j have more impact on the overall gradient than images with low absolute values. Exploratory data analysis Which proteins occur most often in train images Take Away We can see that most common protein structures belong to coarse grained cellular components like the plasma membrane the cytosol and the nucleus. Hyperlinks still needed Planned updates 1. This way we can see if the loss decreases during weight update steps And we can see if we need more epochs or if the loss has already converged and settled down. Especially in the red channel we can see morphological differences. ", "id": "allunia/protein-atlas-exploration-and-baseline", "size": "31846", "language": "python", "html_url": "https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline", "git_url": "https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline", "script": "Flatten sklearn.metrics __init__ skimage.transform Adadelta keras.layers learn accuracy_score as accuracy scipy.misc predict find_counts make_title build_model f1_std Dropout Sequential load TargetGroupIterator preprocess DataGenerator(keras.utils.Sequence) keras.losses TrackHistory(keras.callbacks.Callback) imread check_subset listdir on_epoch_end ImagePreprocessor Conv2D __data_generation seaborn numpy get_loader KernelSettings __getitem__ save f1_mean PIL load_image reshape keras.backend VarianceScaling base_f1 sklearn.model_selection compile_model get_targets_per_image PredictGenerator ImprovedDataGenerator(DataGenerator) Image matplotlib.pyplot Dense tensorflow os pandas ImprovedModel(BaseLineModel) ModelParameter keras.optimizers __len__ resize accuracy_score score find_matching_data_entries normalize keras.initializers on_train_begin on_batch_end fill_targets MaxPooling2D BaseLineModel transform_to_target RepeatedKFold make_image_row binary_crossentropy f1_min set_generators keras.models f1_max ", "entities": "(('target proteins', 'intensities'), 'differ') (('now we', 'again next epoch'), 'say') (('you', 'strategy'), 'consequently') (('that', 'image preprocessor'), 'm') (('others', 'overall low values'), 'see') (('we', 'only 3 targets'), 'expect') (('image', 'image actually 4 different files'), 'find') (('Perhaps we', 'baseline model'), 'expand') (('we', 'learning steps'), 'way') (('j E _ way we', 'weights'), 'large') (('it', 'validation images'), 'suffer') (('how predictions', 'metricsWe'), 'see') (('validation how performance', 'train images'), 'pass') (('I', 'already dropout'), 'see') (('2 we', 'cross again 3 repeated validation'), 'end') (('Preparation preparation Kernel Improved model Loading packages', 'data names'), 'setting') (('we', 'already whole dataset'), 'mean') (('class that', 'learning current step'), 'be') (('target distribution', 'overall pattern'), 'have') (('This', 'batch'), 'become') (('you', 'feature'), 'be') (('image that', 'model'), 'preprocessor') (('test 11702 we', 'predictions'), 'make') (('that', 'batch learning one step'), 'turn') (('cells', 'different types'), 'look') (('mitotic spindle', 'often together cytokinetic bridge'), 'find') (('f1', 'model'), 'mean') (('Hence changing', 'jiggles'), 'lead') (('observe', 'just cell'), 'run') (('contrast endosomes', 'often together sometimes endoplasmatic reticulum'), 'occur') (('you', 'one iteration'), 'see') (('we', 'target protein 1 structures'), 'make') (('We', 'quality low gradients'), 'wiggle') (('loss', 'more epochs'), 'see') (('you', 'target clustering'), 'like') (('just perhaps one', 'cell types'), 'be') (('that', 'very common targets'), 'be') (('simple s', 'i d'), 'collect') (('We', 'true positives'), 'attach') (('that', 'starter code'), 'mind') (('F1 macro score', 'Guglielmo Camporese https www'), 'let') (('we', 'targets'), 'be') (('imbalance', 'rods'), 'become') (('validation loss', 'change'), 'see') (('this', 'one batch'), 'depend') (('prediction way probabilities', 'data added source'), 'set') (('we', 'minor classes'), 'make') (('we', 'imbalanced targets'), 'be') (('i', 'low absolute values'), 'image') (('I', 'ideas'), 'come') (('weights', 'gradients'), 'mean') (('last losses', '25 dropped neurons'), 'sigmoid') (('rods', 'train very seldom data'), 'end') (('where model', 'higher 10'), 'be') (('s', 'leaderboard'), 'make') (('we', 'parameter different settings'), 'try') (('already we', 'data'), 'improve') (('we', 'step'), 'think') (('we', 'model same parameters'), 'need') (('60', 'it'), 'be') (('we', 'y i.'), 'see') (('train most common away Most images', 'target only 1 labels'), 'be') (('mean', 'target class'), 'be') (('samples', 'nearly same predicted values'), 'see') (('s', 'target specific groups'), 'correspond') (('we', 'dogs'), 'yield') (('We', 'far away good model'), 'change') (('smaller s', 'loss callback'), 'let') (('Consequently we', 'batch'), 'make') (('model', 'target'), 'be') (('j E _ We', 'one another'), 'gradient') (('Visual analysis', '3'), 'set') (('other images', 'microtubules nucleus'), 'be') (('others', 'training'), 'splitte') (('you', 'output'), 'read_csv') (('that', 'starting point'), 'be') (('classes', 'weights'), 'do') (('way we', 'chunk ids'), 'load') (('s', 'only 2 repetitions'), 'let') (('we', 'wishlist'), 'start') (('consequently it', 'target stained proteins'), 'say') (('s', 'batch'), 'let') (('we', 'step'), 'make') (('we', 'single target'), 'have') (('Consequently high accuracy', 'target proteins'), 'belong') (('other gradients', 'set different computes'), 'see') (('K', 'end'), 'repeat') (('that', 'time'), 'onestep') (('model', 'target protein'), 'take') (('Sometimes jumps', 'even losses'), 's') (('We', 'validation images'), 'kera') (('purpose we', 'validation data'), 'choose') (('s', 'data generator'), 'let') (('next it', 'nucleoplasmn predictions'), 'try') (('whole dataset', 'gradient descent'), 'use') (('how scores', 'classes'), 'see') (('that', 'data nucleoplasm cytosol'), 'but') (('Consequently it', 'min'), 'know') (('How images', '3'), 'explore') (('Hyperlinks', 'Planned still updates'), 'need') (('history', 'one epoch'), 'draw') (('peroxisomes', 'as well cytosol'), 'see') (('others', 'zero'), 'see') (('Sometimes whole image', 'cells'), 'cover') (('test where set', 'train set'), 'make') (('we', 'loss decrease'), 'see') (('i', 'one image'), 'help') (('protein most common structures', 'plasma membrane'), 'analysis') (('you', 'link'), 'follow') (('Perhaps we', 'improved batch'), 'prevent') (('staining', 'green channel'), 'run') (('sometimes they', 'opposite'), 'lead') (('you', 'here short what'), 'thank') (('that', 'list'), 'return') (('method', 'batch'), 'value') (('protein where structure', 'other targets'), 'allunia') (('model', 'training images'), 'learn') (('I', 'baseline simple model'), 'welcome') (('Perhaps we', 'bad Nucleoplasmn'), 'make') (('we', 'that'), 'don') (('we', 'gradient descent'), 'need') (('we', 'one disadvantage'), 'compute') (('many targets', 'only very slight correlations'), 'take') (('we', 'cats'), 'be') (('it', 'mainly high contributions'), 'drive') (('Hence s', 'feature class ImprovedModel BaseLineModel'), 'let') (('we', 'cross k fold validation'), 'build') (('you', 'contents'), 'be') (('we', 'cats'), 'imagine') (('we', 'return already def get_targets_per_image self identifier self'), 'add') (('that', 'data'), 'be') (('way we', 'data loading'), 'change') (('we', 'class'), 'go') (('them', 'value similar range'), 'cause') (('that', 'training'), 'compute') (('now s', 'batch_size'), 'let') (('such we', 'validation easily true labels'), 'let') (('perhaps corresponding distribution', 'bimodal'), 'see') (('endosomes', 'more nucleus'), 'show') (('image simple that', 'target protein wishlist'), 'generator') (('I', 'weight already 200 updates'), 'mean') (('that', 'protein structures lysosomes'), 'see') (('more what', 'network thinned learning'), 'be') (('we', 'cats'), 'obtain') (('This', 'replacement'), 'do') (('us', 'neural networks'), 'make') (('model', 'major classes'), 'be') (('we', 'optimization alias loss choice hyperparameter search linear algebra data processing CSV Where next objective file'), 'happen') (('it', 'cats'), 'occupy') (('we', 'index'), 'run') (('we', 'already baseline'), 'add') (('s', 'learning library keras https deep keras'), 'let') (('Consequently model', 'exotics'), 'try') (('entries', '0'), 'make') (('we', 'them'), 'transform') (('very much We', 'model'), 'thank') (('target 28 different multiple proteins', 'contents1'), 'predict') (('that', 'very frequency'), 'be') (('we', 'next batch'), 'continue') (('model Improved you', 'style'), 'settingswithin') (('mode', 'samples'), 'be') (('s', 'deeper problem'), 'let') (('how keras', 'job'), 'connect') (('forward propagation', 'network'), 'see') (('This', 'target corresponding proteins'), 'correspond') (('we', 'test submission data'), 'like') (('learning', 'progress'), 'give') (('that', 'batch'), 'take') (('Consequently we', 'targets'), 'find') (('now we', 'target directly binary values'), 'train') (('How model', '4'), 'bulde') (('Data GeneratorI', 'Shervine Amidi'), 'see') (('you', 'examples'), 'collect') (('perhaps this', 'prediction slow process'), 'help') (('It', 'loss function'), 'need') (('We', 'image ids'), 'perform') (('we', 'home bed'), 'be') (('you', 'csv'), 'dim') (('we', 'more learning steps'), 'remain') (('even something', 'something'), 'weren') (('as well we', 'later submission'), 'let') (('we', 'morphological differences'), 'see') (('Shared classOk now we', 'DataGenerator'), 'Parameter') (('that', 'images'), 'build') (('both', 'cellular division'), 'make') (('It', 'training current step'), 'let') (('Passing', 'network layers'), 'reduce') (('target distribution', 'learning'), 'influence') (('we', 'update many such steps'), 'be') (('We', 'reduced gradient quality'), 'gain') (('images', 'cell different densities'), 'click') (('we', 'way'), 'before') (('Here we', 'target true value'), 'know') (('that', 'loss'), 'be') (('we', 'baseline simple model'), 'use') (('that', 'image'), 'be') (('s', 'index'), 'let') (('we', 'model'), 'try') (('we', 'data generator'), 'wish') (('model', 'something'), 'overwrite') (('it', 'K repeated Folds'), 'yield') (('that', 'images'), 'write') (('most', 'zero'), 'see') (('somehow performance', 'test chunk'), 'depend') (('Consequently accuracy', 'right here performance'), 'be') ", "extra": "['test']"}