{"name": "how to become a property tycoon in new york ", "full_name": " h1 How to predict House Prices and hopefully become a Property Tycoon in New York h1 1 Load Data and Clean it h1 2 Data Inspection h2 2 1 Dependent Variable Inspection h2 2 2 Independent Variables Inspection h1 3 Modelling h2 3 1 Data Preparation h2 3 2 Split into Training Testing Data h2 3 3 Running the Different Models h1 4 Conclusion and Next Steps ", "stargazers_count": 0, "forks_count": 0, "description": "Anyways let s check out the same graphs again. 2 Split into Training Testing DataThe step is necessary to ensure that the model is flexible and general enough so that can predict accurately unseen or new data. There are also around a third of all observations with missing Square Feet data. 1 Load Data and Clean itLet s update the BOROUGH names first according to the instructions found on Kaggle. So the original 85k don t count. Recap and Revisit of Entire Data Set After I ve removed some observations due to their prices which I have treated as outliers how many NULL SQUARE FEET observations remain Unfortunately there are still a third of observations remaining that contain no SQAURE FEET data. There are 765 duplicates. I have done a few iterations of what a good cap on sales prices is and settled for 1 property needs to be more expensive than 100 000 USD2 property needs to be cheaper than 5 000 000 USDEverything is else is a different animal and mixing all together in one model will decrease accuracy. Those are only a handful of observations though. 2 We only used 30k observations out of a potential of 70k. That s a fair chunk of data. What is more interesting is the BUILDING AGE. I will work on this in a future iteration. Building Class Category Some of the categories could potentially be merged in a future iteration. I ve been toying with the idea of clustering properties on their SALE PRICE classifiying them as something like cheap normal expensive and luxury in order to avoid this issue. Update Data I have already done some prior inspection of the data and am now updating the dataset by deleting columns and changing the data type of some of the variables. BOROUGH This is all in good shape and no surpises here. 40 which is certainly not great but maybe not bad given the limited amount of data available. I ll show the importance of SQAURE FEET in the result section at the end of the workbook. 5 Some buildings were built in the YEAR 0 which again is wrong. From the descriptive statistics we could also tell that 75 of the properties in this dataset are cheaper than 950 000 USD. Convert categorical variables into dummy indicator variables i. Everything is done obviously with the aim of becoming a property tycoon one of the most important things to know in order to achieve that is the value of a property. More Data Visualisations 3 Modelling 3. In a next iteration I will therefore explore the possibilities to impute the SQUARE FEET data because I deleted a lot here. 1 Data Preparationscikit works best with normalized data i. Those observations will have to be deleted. Let s remove them High Level Data Inspection and Validation Let s show this visually. It s easier that way to see where the NULL values are and how many there are. And we re down from 70k observations to 55k which is 79 remaining. 4 There are properties have 0 SQUARE FEET which shouldn t be possible unless they don t exist yet or the data is wrong. It turns out SPOILER ALERT that this is the best predictor of SALE PRICE in this dataset which means I want to keep as much data as possible instead of just throwing them away. Maybe this isn t all that surprising given that Manhattan is home to a lot of very expensive property. Remember that there were a fair amount of NULL SALE PRICE observations and duplicates etc. The log of 1 however is. In addition I get rid of the observations the sum of COMMERCIAL UNITS and RESIDENTIAL UNITS doesn t match TOTAL UNITS. 1 Dependent Variable Inspection SALE PRICE Let s start with the dependent variable as this is the one I want to predict. After updating the data let s check if there are any duplicate values in here. and this what it looks after Transforming the independent variables Some of the variables contain zeroes which is why I need to add 1 so that I can take the log before normalising it you can see that in the table below. The same is true for UNITS data. But first I get all the relevant variables and one hot encode the categrical variables which is necessary for scikit to work. There are also a fair number of properties cheaper than 100 000 USD which seems too cheap in my opinion. I will now test a few different models to see which one performs best. Capture the necessary data Plot number of available data per variable Remove observations with missing SALE PRICE Set the size of the plot Plot the data and configure the settings Set the size of the plot Plot the data and configure the settings Set the size of the plot Get the data and format it Plot the data and configure the settings Remove observations that fall outside those caps Set the size of the plot Plot the data and configure the settings Set the size of the plot Plot the data and configure the settings Set the size of the plot Get the data and format it Plot the data and configure the settings Capture the necessary data Plot number of available data per variable Removes all NULL values Keeps properties with fewer than 20 000 Square Feet which is about 2 000 Square Metres Only a handful of properties with 0 total units are remaining and they will now be deleted Remove data where commercial residential doesn t equal total units Correlation Matrix Compute the correlation matrix Generate a mask for the upper triangle Set up the matplotlib figure Generate a custom diverging colormap Draw the heatmap with the mask and correct aspect ratio Choose only the variables I want to use in the model Select the variables to be one hot encoded For each categorical column find the unique number of categories. 6 Some properties have a SALE PRICE of 0 which is also wrong or a transfer but not actually a sale. I will first look at the dependent variable SALE PRICE which is the one I want to predict. Total Units I am deleting the outliers with very large numbers of TOTAL UNITS and those with 0 units. Now let s get an overview of some descriptive stats of the numerical variables in the data set. I train the model with the training data and then check how good it performs on the unseen testing data. SQUARE FEET I need to get rid of the NULL values and a few outliers. The reason why I need to add 1 is because I can t take the log of 0 it is not defined. What we can see by looking at the first few rows is that the column Unnamed 0 is an artifact from the data load and is not needed. UNITS SQUARE FEET BUILDING AGE 3. 3 Running the Different ModelsFinally the moment we have all been waiting for. Both contain similar information the second is however a bit more practical. alpha Perform 10 fold CV ridge_cv_scores Append the mean of ridge_cv_scores to ridge_scores Append the std of ridge_cv_scores to ridge_scores_std Display the plot Instantiate a ridge regressor ridge Fit the model Perform 5 fold cross validation ridge_cv Print the 5 fold cross validation scores Create the hyperparameter grid Instantiate the ElasticNet regressor elastic_net Setup the GridSearchCV object gm_cv Fit it to the training data Predict on the test set and compute metrics. What we see from the two graphs above is that there are a lot of outliers. head EASE_MEANT is empty and can be dropped Unnamed 0 is an artifact from the data load and can be deleted SALE PRICE is object but should be numeric LAND and GROSS SQUARE FEET is object but should be numeric SALE DATE is object but should be datetime Both TAX CLASS attributes should be categorical Delete the duplicates and check that it worked Check the number of rows and columns Get a high level overview of the data types the amount of NULL values etc. I am not quite sure what to make of that yet. And there some missing Sale Prices. In a next iteration I will also fine tune these to get an even better result. There is potential to impute those values but we will have to see how well that will work. YEAR BUILT Next one up is YEAR BUILT. After removing the missing SALE PRICES we are left with 70k observations down from 85k at the very start. So without further ado let s go for it. Some interesting observations to note 1 There are ZIP CODES with a value of 0 which is probably wrong. Transforming the dependent variable SALE PRICE This is what SALE PRICE looks before the transformation. 3 Instead of throwing outlier SALE PRICE data I will try to use clustering to classify properties w hich would allow me to keep the outlier data maybe. data that has a mean around 0 and a distribution around that. After that I will look at the independent variables which are the ones I use to predict the price. 2 Independent Variables Inspection I have already looked at each variable in more detail and only show the ones I am going to keep for the model. I will use1 Linear Regression 2 Random Forest Regression 3 Ridge Regression 4 and ElasticNet 1 Linear Regression 2 Random Forest Feature Importance of Random Forest 3 Ridge Regression 4 ElasticNet and GridSearch 4 Conclusion and Next Steps1 An untuned Random Forest Regression managed to get a R2 of 0. What does the plot look like now OK that s a lot more realistic. Currently I have removed all the NULL and outlier data of SQUARE FEET and TOTAL UNITS but in a future iteration I will try to impute some data points to keep as much as data as possible. 2 75 of properties have no COMMERCIAL UNITS 3 At least 50 of all properties have only 1 TOTAL UNIT. There seem to be some buildings that were built in the year 0 which can t be correct. This tells us how many columns we are adding to the dataset. I could still predict a price for those cases but there is no way of verifying the accuracy of the predictions. The column EASEMENT is completely empty and will be deleted. Using the log allows me to get rid of the skew in the data and have a more normal distribution. The numerical variables don t need one hot encoding but will have to be normalised. Delete the old columns. This is done for both Create the regressor linreg Fit the regressor to the training data Predict the labels of the test set y_pred Compute 5 fold cross validation scores cv_scores Print the 5 fold cross validation scores Compute 5 fold cross validation scores cv_scores Print the 5 fold cross validation scores Plot the feature importances of the forest Import necessary modules Setup the array of alphas and lists to store scores Create a ridge regressor ridge Compute scores over range of alphas Specify the alpha value to use ridge. Import the modules Data Scaler Regression Metrics Read the data Renaming BOROUGHS Change the settings so that you can see all columns of the dataframe when calling df. However YEAR BUILT isn t quite the variable we are looking for. and add the new one hot encoded variables Take the log and normalise Add 1 to Units Take the log and standardise Add 1 to Units Take the log and standardise Add 1 to BUILDING AGE Take the log and standardise Split data into training and testing set with 80 of the data going into training X are the variables features that help predict y which tells us whether an employee left or stayed. In the next section I will normalise standardise the data and also take the log in order to get rid of the skewness and to allow for a more normal distribution. SQUARE FEET BUILDING AGE and BOROUGH were the most important features determining the SALE PRICE. 20 of all Sale Prices are NULL which is what I wanted to predict. How to predict House Prices and hopefully become a Property Tycoon in New YorkI am going to clean and visualise data and build a model to predict housing prices in New York. 2 Data InspectionAgain I have already done some data inspection and I will not show those variables that aren t useful for the model. ", "id": "akosciansky/how-to-become-a-property-tycoon-in-new-york", "size": "8495", "language": "python", "html_url": "https://www.kaggle.com/code/akosciansky/how-to-become-a-property-tycoon-in-new-york", "git_url": "https://www.kaggle.com/code/akosciansky/how-to-become-a-property-tycoon-in-new-york", "script": "sklearn.metrics cross_val_score matplotlib.style Ridge Lasso mean_squared_error seaborn numpy RandomForestRegressor sklearn.ensemble sklearn sklearn.model_selection metrics KFold display_plot matplotlib.pyplot stats pandas StandardScaler ElasticNet scipy GridSearchCV sklearn.linear_model sklearn.preprocessing train_test_split LinearRegression ", "entities": "(('then how it', 'testing unseen data'), 'train') (('yet data', 't'), '4') (('column', 'data 0 load'), 'be') (('I', 'data'), 'remove') (('I', 'possible instead just them'), 'turn') (('which', '0'), '6') (('we', 'dataset'), 'tell') (('s', 'this'), 'let') (('which', '55k'), 're') (('we', 'very start'), 'leave') (('how that', 'values'), 'be') (('100 cheaper than 000 which', 'too opinion'), 'be') (('that', 'that'), 'datum') (('Next Random Forest GridSearch 4 untuned Regression', '0'), 'will') (('that', 'accurately unseen data'), 'split') (('3 At least 50', 'TOTAL only 1 UNIT'), 'have') (('else different mixing', 'accuracy'), 'do') (('I', 'that'), 'be') (('Everything', 'property'), 'do') (('numerical variables', 'don one hot encoding'), 'need') (('employee', 'us'), 'add') (('How predict', 'New York'), 'go') (('one', 'now a few different models'), 'test') (('Those', 'only observations'), 'be') (('I', 'issue'), 'toy') (('isn Maybe that surprising Manhattan', 'very expensive property'), 't') (('I', 'SALE first dependent variable PRICE'), 'look') (('Total I', '0 units'), 'unit') (('moment we', 'Different ModelsFinally'), '3') (('me', 'outlier data'), 'try') (('you', 'table'), 'and') (('I', 'even better result'), 'tune') (('I', 'price'), 'look') (('SALE PRICE', 'transformation'), 'transform') (('me', 'more normal distribution'), 'allow') (('Update Data I', 'variables'), 'do') (('that', 'SQAURE FEET data'), 'Recap') (('I', 'lot'), 'explore') (('second', 'similar information'), 'be') (('which', 'data'), '40') (('RESIDENTIAL doesn', 't TOTAL UNITS'), 'rid') (('I', 'future iteration'), 'work') (('This', 'good shape'), 'BOROUGH') (('see', 'above outliers'), 'be') (('also 75', '950 000 USD'), 'tell') (('I', 'more normal distribution'), 'normalise') (('I', 'workbook'), 'show') (('scikit', 'relevant variables'), 'get') (('SQUARE I', 'NULL values'), 'foot') (('we', 'variable'), 'BUILT') (('that', 'What'), 'look') (('SQUARE FEET BUILDING AGE', 'SALE most important PRICE'), 'be') (('s', 'same graphs'), 'let') (('number', 'NULL values'), 'be') (('elastic_net GridSearchCV object', 'test set'), 'Perform') (('I', 'what'), 'be') (('0 which', 'YEAR'), '5') (('I', 'predictions'), 'predict') (('Data 1 Preparationscikit', 'data best normalized i.'), 'work') (('array', 'ridge'), 'do') (('t', 'year'), 'seem') (('I', 'dependent variable'), 'let') (('We', '70k'), '2') (('s', 'data'), 'let') (('Select', 'categories'), 'capture') (('which', '0'), 'be') (('I', 'model'), 'Inspection') (('Building Class Category Some', 'potentially future iteration'), 'merge') (('you', 'when df'), 'import') (('it', '0'), 'be') (('Now s', 'data set'), 'let') (('that', 'model'), '2') ", "extra": "['test']"}