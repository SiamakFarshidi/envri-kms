{"name": "pytanic ", "full_name": " h1 Python walk through for Titanic data analysis h2 This is a work in progress Comments and critical feedback are always welcome h2 Outline h1 1 Load Data and Modules h1 2 Initial Exploration h3 Missing values h3 Cabin numbers h2 Ticket numbers h1 3 Relations between features h1 4 Filling in missing values h1 5 Derived engineered features h3 Child h3 Cabin known h3 Deck h3 Ttype and Bad ticket h3 Age known h3 Family h3 Alone h3 Large Family h3 Shared ticket h3 Title h3 Fare cat h3 Fare eff cat h1 6 Preparing for modelling h1 7 Modelling h2 Splitting the train sample into two sub samples training and testing h2 Test and select the model features h2 Run and describe several different classifiers h2 Examining Optimising one classifier in more detail h2 Model validation h2 Ranking of models and features h2 Stacking Ensemble methods h1 8 Preparing our prediction for submission ", "stargazers_count": 0, "forks_count": 0, "description": "C and F are around 60. Does the age dependent survival change with sex How are pclass and fare related Are they strongly enough connected so that one of them is superfluous Let s find out. The parameter cv here defines the number of folds or alternatively something more complex as described in the docs http scikit learn. It seems that embarking at Q improved your chances for survival if you were a 3rd class passenger. Mrs does not contain many teenagers but has a sizeable overlap with Miss especially in the range of 20 30 years old. Another convention is to initialise this sequence of models with a single prediction value like the mean of the training survival values. These estimates are smoothed and therefore extend beyond the actual values look closely at the dotted zero level. For this we create a Fare_eff feature above which we derive by dividing Fare by the number of people sharing a ticket Ticket_group which we also newly created. We re looking for something a bit more subtle here. There are NaN in this column. We also create our training and testing feature sets. The step sizes can vary from iteration to iteration. Relations between features relations complete 1. This tree can now consider the full number of training samples for splitting a node at another feature instead of having to deal with the decreased sample after the first original node and the resulting impact of random fluctuations. Normally I would recommend not to ignore warnings but to fix what s causing them. 05 is usually not significant and therefore solely based on these numbers we cannot say whether the SibSp 5 sample behaves different than the rest. html Support Vector Machine Support Vector Machine This classifier fits a set of hyper plane s in the high dimensional space of the training features so that this plane has the largest distance to any training data points. Nonetheless this seems useful. almost everyone who embarked at Q went to 3rd class this means that the clear separation in the factorplot for Q isn t very meaningful unfortunately. We learn Male children appear to have a survival advantage in 2nd and 3rd class. An additional randomness is introduced by selecting random thresholds for each feature and using the best performing threshold. For instance if the off diagonal elements were 0 and 30. Does it matter much Probably not. Name is the name of the passenger. This plot is inspired by and realised much more aesthetically in the comprehensive Ensemble Stacking Kernel by Anisotropic https www. What about the other strong feature Sex Now this is somewhat expected since it explains the difference between S and the other ports. This gives us a better impression how robust our results are towards generalisation i. We will come back to this in the modelling stage when we will study feature importances and significances soon. The confusion matrix plot would allow us to identify significant imbalances in our prediction between the false positives and the false negatives. Feel free to try them out and let me know whether you find them useful. org stable auto_examples model_selection plot_confusion_matrix. With these optimised parameters let s have a look at the feature importance that this classifier gives us As expected Pclass and Sex have the most impact but our engineered features are doing not bad either. Let s investigate that a bit more Ok now from here it looks more like S is the interesting port since survival is less probably for that one if you are a 3rd class passenger. TODO check cumulative fare question For each class there is strong evidence that the cheaper cabins were worse for survival. Preparing our prediction for submission Finally we pick our favourite classifier and predict the expected survival for the passengers in the test data set. This can be done again and again for n_estimator number of times. Stacking of classifiers that have less correlation gives better results. A similar effect can be seen in a boxplot Go to the top of the page top 4. And combine the available features of train and test data sets. Derived engineered featuresThe next idea is to define new features based on the existing ones that allow for a split into survived not survived with higher confidence than the existing features. We fill in the 1 missing Fare value in the test data frame accordingly. A 60 yr old 3rd class passenger without family on board. In the model parameters this factor is called the learning_rate. further away from the life boats. Additional notes Parameters min_samples_split and min_samples_leaf control the number of samples at a leaf note. By default the data is divided up into k equally sized sub samples or folds and the classifier is trained on k 1 of them and evaluated on the remaining one e. Most children in 2nd class survived and the majority in 3rd class did too. There is little use in having a classifier that replicates perfectly the training data by following every random noise feature in that data called overfitting but doesn t perform well with new data. This is an iterative process in which you improve your model step by step until you have found the largest feature combination which still has significant impact. Following that parameter min_weight_fraction_leaf is less biased towards dominating classes. It s an approach similar to bootstrapping where we use smaller samples from our data set to check whether the classifier gives similar results for each of them. It might be worth our while to include this feature in at least the initial stages of modelling to see how it performs. All the other titles we group into Not Young. Together these sets of values define a grid which is quite easy to visualise in two dimensions. One visualisation of this process is a tree trunk branching off into successively smaller structures. But I think that it doesn t cover all the signal in the Parch feature. One of the most popular ways to check a model for robustness is called cross validation. For our case there doesn t seem to be an imbalance. We learn Age decreases progressively as Pclass decreases from 1st to 3rd Most older passengers are 1st class but very few children are. Again the upper right and lower left triangle contain the same information. The only passenger on deck T died but that s hardly robust statistics. SibSp Parch Having 1 3 siblings spouses parents children on board SibSp 1 2 Parch 1 3 suggests proportionally better survival numbers than being alone SibSp Parch 0 or having a large family travelling with you. This adds more depth to the overall content but it also makes the whole notebook rather extensive. The barplot shows mean survival fractions and the associated 95 confidence limits which are large for the sparse samples. which classifier which meta parameters. It s hard to say at this stage whether there is any real impact left for the Embarked feature once we correct for these connections. But for less than 25 of cabins known this might not be very useful. We could go through the trouble here to identify families by last name. Perfect correlation would have a correlation index of 1 perfect anti correlation negative correlation would have 1 obviously each feature is perfectly correlated with itself leading to the deep red diagonal. We can try out I suppose one could take the starting letters which might indicate specific decks on the ship and use them as an additional classifier. Important parameters n_neighbors choosing the right k depends heavily on the data. Earlier we had a look at the Survived statistics of the individual features in the overview figure. This scheme will soon be used throughout this kernel. This part of the analysis is called Feature Engineering. Now that is interesting. The result is written to a submission file according to the competition rules 418 rows only include the columns PassengerId and Survived. Beyond that the best decks for survival were B D and E with about 66 chance. We should include the Child feature in our model testing. We learn Several of these derived parameters are strongly correlated with Sex and Pclass. A small number will lead to overfitting a large number prevents learning. Parch is another ordinal integer features that gives the number of parents or children travelling with each passenger. As an example we ll be using the Extremely Randomized Trees but any other classifier can be substituted instead. html for plotting confusion matrices. Making inferences based on just a few cases is often unwise. It shows scatter plots for the different feature combinations plus a distribution of each feature on the diagonal. An individual estimator may have a poor accuracy but if you combine several of them the resulting mean or median average will have a reduced uncertainty. org stable modules cross_validation. More background info here http scikit learn. where was contributed by GeorgeChou https www. This is called Stochastic Gradient Decent Source 1 http blog. The plot tells us that the survival chances were much lower for the cheaper cabins. This means that we grow our trees from a sub sample of the training sample using bootstrapping boostrap True and estimate the accuracy based on those entries that were not picked i. For additional insight we compare the feature _importance output of all the classifiers for which it exists The feature importance tells us how much impact an individual feature has on the decisions within the classifier. The largest number of cases we have is for B vs C. Both factors mean that the impact of Young has to be studied carefully. As we see above a ticket is not always shared by people with a common name. The overall result is not very surprising Sex and Pclass are the dominant features while everything else is of similar significantly lower importance. decision stumps whereas for Bagging Averaging to be successful we want to overfit a littleThe random in random forest comes from the method of training each tree using a random bootstrap sample i. This is called tuning of the hyper parameters http scikit learn. Common last names might not be unique in the passenger list 2. I ve borrowed that one straight from this very nice kernel because it s a useful summary display of how our models perform At face value some classifiers perform better than others. Others will require you to examine the data or parts of it in more specific ways. Alongside the individual features we also compute a median importance. But let s investigate this for a moment and check how it would transform the Fare distribution. I recommend that you weigh the arguments and make your own decision. Now we want to combine the results of different kinds of classifiers to improve our prediction. In addition they are fast to compute and only require relatively little data to perform well. This conflates the impact of Age and Pclass on the survival chances. Solution Use a different classifier. org stable modules grid_search. A module could be defined further down once it is needed but I prefer to have them all in one place to keep an overview. It does exactly what the name suggests each individual classifier makes a certain prediction and then the majority vote is used for each row. Set ylabels Fill in initial metrics tp tn etc. Boosting works best for weak learners e. Rule of thumb for classification max_features sqrt all_features. com kaggle ensembling guide. For those of you who prefer R over Python or want to try out both I m currently building a new R kernel for Titanic https www. Similar to the standard error of the mean for sampling normal distributions. But more men were travelling alone than women did. We learn There remains a potential trend for males and for 3rd class passengers but the uncertainties are large. This feature is a mix of SibSp and Parch which increases the overall numbers we can work with but might smooth out some more subtle effects. This kind of plot is vastly more useful for a set of continuous variables instead of the categorical or integer values we have here. I think that without external information which we are avoiding in this notebook we can t do much better in trying to tie the ticket number to the survival statistics. Random outcomes with 2 possibilities like heads or tails when flipping a coin follow the binomial distribution https en. Solution pruning setting maximum depth or PCA beforehand to find the right number of features. Preparing for modellingBefore we start exploring the different models we are modifying the categorical string column types to integer. In addition many modelling procedures break down when missing values are involved and the corresponding rows will either have to be removed completely or the values need to be estimated somehow. Especially among the 3rd class passengers. That is roughly 20 of the cases that were classified correctly. Below we decide to assign different somewhat arbitrary weight according to how we think each classifier performs. However the significant difference between e. One more step is to provide a sampling of rows and features like in the random forest discussed above to increase the diversity in tree splits and thereby a larger amount of information for the method to work with. Note This is not a streamlined analysis but it contains certain redundancies with the purpose of featuring and exploring different visualisation and modelling tools that can be useful in approaching a binary classification problem. Derived engineered features derived largely complete 1. Then we evaluate the score of the classifier at each grid point and pick the one parameter combination that gives us the best score. org stable modules svm. The closer to white a colour is the weaker the correlation. Rather small numbers though. Some properties and connections will be immediately obvious. We compute the new features in the combined data set to make sure that all feature realisations are complete and then split the combine data again into train and test. An additional concept is Shrinkage. The individual significances are not overwhelming but the trend itself might be useful. We will base our Fare prediction on the median of the 3rd class fares Go to the top of the page top 5. The other two passenger classes are more interesting especially for the male children. This is easy to visualise in 2 dimensions as e. In the plot stronger correlations have brighter colours in either red positive correlation or blue negative correlation. Now we are connecting individual clues to get a glimpse of the bigger picture. Seriously you should check it out. This is an ordinal integer feature. The Loss Function describes how much the prediction is improved when shifting the predicted values by a certain amount. Of course for newbies I also recommend the excellent kernels featured in the Titanic Tutorials https www. Except possibly for the Perceptron. The barplots show the fraction of people per group who survived. However this shift avoids computing issues for the zero fare passengers and it makes little difference for our understanding of the fare groups. Check out the other visualisations in your forked copy. TODO Why do some people have multiple cabin numbers What does that mean Ticket numbers That seems to be a hopeless variable at first because it just looks like random strings. Of course the causality might as well go the other way but that s not really the question here. Cabin gives the cabin number of each passenger. An example would be rich woman vs poor man but this particular distinction should be handled well by most classifiers. In addition to the tree parameters the most important settings are n_estimators number of trees. We know that by definition some of our engineered features will have a high collinearity i. TODO This part is still quite rudimentary and will be expanded in future versions. org stable modules generated sklearn. Gradient Boosting Gradient boosting This is what we call the step by step improvement of a weak classifier like a tree with only 1 node by successively applying this classifier to the residuals of the previous classifier s results. The important parameters are n_estimators number of boosting stages more is better learning_rate smaller steps need more stages max_depth tune for best performance depends on interaction of features subsample only train on a sub sample of the data set drawn without replacement. The larger the better although improvements become marginal eventually max_features number of random features per subset. org stable modules neighbors. The last plot doesn t inspire much confidence in a strong correlation between Deck and Bad _ticket but maybe it will be useful otherwise. Also given that so few cabin numbers are know it doesn t seem that there are good records of them. Will we ever know Maybe not. The easiest method to combine different classifiers is through a Voting Classifier. Maybe these two actually shared a ticket cabin and we have another transcription data entry error The ticket numbers are very similar and someone could easily write 303 instead of 304. Except for 3rd class the survival for Embarked Q is close to 100 split between male and female. Therefore it seems that between more 1st class passengers embarking at C and more men at S there doesn t seem to be much actual influence in the port of embarkation. We use the matplotlib subplot tool to line up the individual plots in a grid. Go to the top of the page top 3. Let s see whether that s significant Just about formally significant i. This feature is a character string of variable length but similar format. All the other rare titles like Don or Lady have average ages that are high enough to count as Not Young. Fare _eff _cat Let s investigate the Fare affair in more detail. A standard deviation of zero means that there s no difference. com georgechou in the comments. Test and select the model features Now we are ready to model. html gradient tree boosting In addition This is the only instance where we import a module right when it s needed instead of up top. Num positive examples Num negative examples Draw the grid boxes Set xlabels These coordinate might require some tinkering. The tentative imbalance between male and female 3rd class probably reflect the observation we made earlier that men were more likely to travel alone. Many thanks Now let s study the new features and see how they relate to the survival chances Child The Pclass 1 plot looks interesting at first but there are only 3 children in this group which makes the apparent pattern just random noise. We learn There don t seem to be strong differences in Age among the Embarked categories that would point at an imbalance that goes beyond the influence of Pclass and Sex. Let s try to do better than that. Here we see that in the testing data set based on our train test split 12 people who survived were misclassified as dead whereas 21 who died were misclassified as having survived. It is well possible that some of our bad tickets are merely statistical fluctuations from the base survival rate of 38. Of course it s not the tickets themselves that are bad for survival but the possibility that the ticket numbers might encode certain areas of the ship that would have led to higher or lower survival chances. First a simple cross validation using the helper function cross _val _score. I prefer the approach to list all the new features that we define together in one place to keep an overview. We excluded PassengerID which is merely a row index. Initial Exploration Look at your data in as many different ways as possible. std label adjust these methods to my notation training and train test split parameters for reproducibility set folds for out of fold prediction Class to extend the Sklearn classifier this basically unifies the way we call each classifier function for out of fold prediction split data in NFOLDS training vs testing samples select train and test sample train classifier on training sample predict classifier for testing sample predict classifier for original test sample take the median of all NFOLD test sample predictions changed from mean to preserve binary classification Put in our parameters for selected classifiers Random Forest parameters max_features 0. This majority process can either give all individual votes the same importance or assign different weights to make some classifiers have more impact than others. However the differences between the methods are relatively small and more likely due to more or less over fitting than anything else. I recommend to briefly describe the details of your submission e. But most large families were travelling in 3rd class. The method of Gradient Decent uses this Loss Function to iteratively move into the direction of its greatest decent i. Removing less important features will help you to reduce the noise in your prediction and allow your model to generalise to new data which is our priority goal in machine learning. org stable modules ensemble. Let s follow up the numbers for Pclass vs Embarked with a pandas crosstab plot We learn a high percentage of those embarked at C were 1st class passengers. Here are the age distributions for those We see that Master is capturing the male children teenagers very well whereas Miss applies to girls as well as younger women up to about 40. The clever way of computing the Shared _ticket values using group _by and np. The method used for computing the scores is by default the native scoring method of the classifier but can be changed. Advantages Effectiv in high dimensions and versatile with different kernel options. Go to the top of the page top 8. A plot will explain it better than 1000 words. Bagging Bagging is a general ensemble method. Problem Being just not easy to fit to certain concepts that don t lend themselves to clear yes or no decisions. Read more in the extensive Kaggle Ensemble Guide https mlwave. From there you can download it and submit it by going to Leaderboard Submit Predictions in the tab list below the competition header. For instance Fare _cat and Fare. We study the correlation of Age with Pclass using a violin plot which is also split between survived right half and not survived left half. In higher dimensions only mathematics can save you. Of course this assumption should be tested by doing the last name thing too. For example we fit a tree determine its results prediction survived vs not survived compute the residuals of this prediction vs the real survival numbers all in the training data of course and then fit another tree to these residuals. For a final overview before the modelling stage we have another look at the correlation matrix between all old and new features We designed a number of new features and unsurprisingly several of those are correlated with the original features we used to create them. We learn Pclass is somewhat correlated with Fare 1st class tickets would be more expensive than 3rd class ones SibSp and Parch are weakly correlated large families would have high values for both solo travellers would have zero for both Pclass already correlates with Survived in a noticeable wayIn addition we plot a Pairplot of the numerical features. Here we just focus a bit on the underlying idea. Large _Family In the same way having a large family appears to be not good for survival. com varimp a mostly tidyverse tour of the titanic makes a convincing case for predicting Embarked S for these two passengers see also the comments. for k 4 we use 4 samples leave each of them out once and train on the other 3 then evaluate on the one we ve left out. The upper right vs lower left triangle that make up this plot contain the same information since the corresponding cells show the correlation coefficients of the same features. Fare This is case where a linear scaling isn t of much help because there is a smaller number of more extreme numbers. Combining them into parch 4 gives us 9 vs 1 which is much better. Otherwise there is no significant difference within each class. Admittedly 4 different ones are a bit of an overkill but why not document what we found. But wouldn t it be nice to combine all these different classifiers to get a more accurate overall prediction This is possible through an approach called Ensemble methods. A little follow up For SibSp we see in the plot that most of the differences are not very significant overlapping error bars. There seems to be some impact here that isn t captured by the passenger class. I m happy to see that my notes are useful for others who are starting out in data analysis and machine learning and I hope that you will be able to get some inspiration from this kernel. These base results will be used as new features Extra Trees Random Forest AdaBoost Gradient Boost Support Vector Classifier Survived surv_pred. Therefore a shared ticket might actually be a stronger predictor. 5 AdaBoost parameters Gradient Boosting parameters max_features 0. We also recover the age difference between the ticket classes that was already obvious in earlier plots. Naively one would assume that those cheap cabins were mostly located deeper inside the ship i. This is something we will explore in more detail below. Afterwards we will try to gradually adapt and simplify the approach to make use of the work we have already done above for all the individual classifiers. Therefore one should assume that it s more likely to know someone s cabin number if they survived. For this we define everyone under 30 or with a title of Master Miss or Mlle Mademoiselle as Young. The initial modelling will allow us to decide which features are worth to take to the next step. Larger values suppress noise but smooth out decision boundaries. We can try out Working hypothesis if your group mostly family survived then you survived as well unless you were a man and presumably helped your wife daughter lady friend. The latter one which you can comment also includes the possibility to plot a normalised confusion matrix. hist by combine Pclass bins np. Personally I like histograms for a first look at comparing two or more populations in case of scaled features. Solution balance the data set by either sampling the same number of samples from each class or by adjusting the sample_weight parameter to normalise the sum of the class weights to the same value. Load Data and Modules Load Python modules The list of modules grows step by step by adding new functionality that is useful for this project. The 2nd point is somewhat curious since we recall from above that the survival chances for Q were actually slightly better than for S. nbsp Best of success and enjoy learning matplotlib inline for seaborn issue dummy tab. Here we use Taner s function and also include the official sklearn example http scikit learn. Usually it s most interesting to start with the strong signals in the correlation plot and to examine them more in detail. However the last plot should also indicate that. This avoids overfitting. We learn Different percentages of passenger classes and sexes have embarked from different ports which is reflected in the lower survival rates for S more men fewer 1st class compared to C more women and 1st class. Cabin _known As suspected it is more likely to know the cabin of a passenger who survived. org stable modules tree. Missing values Knowing about missing values is important because they indicate how much we don t know about our data. We know that some titles can indicate a certain age group. In addition there is some variation between the 1st class male passengers but it doesn t look overly significant. Once more this info was digested from the sklearn documentation http scikit learn. Lower learning rates make for a slower decent which seems to be empirically more effective. This is a categorical text string feature. Final validation with the testing data set TODO Expand this section Ranking of models and features Ranking of models. The devil here is in the details Why is Sex so much weaker for the boosting algorithms And why have features like Alone more impact when boosted Is it because of the lower tree depth What can we learn from these discrepancies with respect to parameter optimisation for the individual classifiers Stacking Ensemble methods Each of the individual classifiers we have used above has its strengths and weaknesses and we should always choose the classifier that s best equipped to handle a certain problem and or has been found to perform with the highest accuracy. Bagging for a decision tree classifier should be the same as using a Random Forest see below. Not significantly so of course but certainly not worse even though S had a higher percentage of 1st and 2nd class passengers. html Naive Bayes Naive Bayes http scikit learn. This is another string feature. Intuitively classifiers that are highly correlated like ExtraTrees and GradientBoost above are already so similar that stacking doesn t change the result in a significant way. html which shows more information if we need it. Further randomness is introduced by making the node split dependent on a random subset of features instead of all of them. Shared _ticket Sharing a ticket appears to be good for survival. If sub samples are used then the remaining samples the ones not in the bag we re drawing the data from can be used in out of bag oob estimates oob _score True. As far as I can see there s still quite a bit of variation here. 1 and 3 based on large enough numbers suggests that this new feature could still contain some useful information. com c titanic tutorials. Positive vs negative correlation needs to be understood in terms of whether an increase in one feature leads to an increase positive or decrease negative in the correlated feature. This is consistent with the other statistics that show that women were more likely to travel together with larger families. The highest ages are well consistent with the overall distribution. SibSp is another ordinal integer feature describing the number of siblings or spouses travelling with each passenger. there were more males among the 3rd class passengers. Alternatively you can use a seaborn heatmap for a quick and easy but less pretty plot. This difference between 18 40 yr olds might be a better feature than Sex and Age by themselves. Examining Optimising one classifier in more detail For each of these various classifiers we can have a closer look to improve their performance and understand their output. This transformation can be easily achieved using the base 10 logarithm The 1 means that our boundaries are slightly shifted in terms of the real Fare. TODO Say something about the contributions and follow up with some ANOVA like analysis Run and describe several different classifiers Based on the first look we define the input columns we ll be working with. There s a difference of about 30 vs 40 and it should be significant Very much so. org stable modules naive_bayes. In summary we have 1 floating point feature Fare 1 integer variable Age 3 ordinal integer features Plcass SibSp Parch 2 categorical text features Sex Embarked and 3 text string features Ticket Cabin Name. Visualising the tree helps to understand how well it is fitting the data. For SibSp we have 15 vs 3 5 vs 0 and 7 vs 0. Here we want to look at correlations between the predictor features and how they could affect the target Survived behaviour. The new feature is called Fare_eff_cat and behaves as follows Go to the top of the page top 6. A confusion matrix contains more information than a simple score because it shows how many data points of each class were correctly incorrectly classified. by playing with a forked copy rather than reading the whole thing in one go. Ticket is a character string of variable length that gives the ticket number. Lower numbers decrease variance and increase bias. OK let s go through the features one by one to see what we find. html bagging meta estimator Decision Tree Decision Tree One of the classifiers that s easiest to visualise. Filling in missing valuesAfter studying the relations between the different features let s fill in a few missing values based on what we learned. There s a lot going on in this figure so take your time to look at all the details. Is this another of these cases It actually is. This is necessary since not all classifiers can handle string input. com arthurtok introduction to ensembling stacking in python Now we continue to examine these initial indications in more detail. Hence decision tree. Based on this plot we define a new feature called Bad _ticket under which we collect all the ticket numbers that start with digits which suggest less than 25 survival e. If you re very new to this subject then I recommend to go through each section on it s own e. For this to work we need to adjust for the zero fare entries. Comments and critical feedback are always welcome. First a broad overview. I have also indicated Age 10 which we will use to define children vs teenagers in the engineering part below. left out of the bag. Here single trees are combined through the average of the prediction probabilities. io 2015 10 28 confusion matrix. Problem Creating biased trees if some classes dominate. org wiki Binomial_distribution. However the corresponding number are not very large. org wiki Bayes 27_theorem under the naive assumption that all predictor features are independent from each other and only related to the target variable. The really expensive Fares in Pclass 1 are pretty much all gone. Family We learn Again we find that having 1 3 family members works best for survival. Whether there is actual signal in them that a model can use to improve the learning accuracy needs to be investigated. Admittedly these are quite a few grouping levels but 30 C vs 20 S are numbers that are still large enough to be useful in this context. Let s study the relation between Fare and Pclass in more detail We learn There is a broad distribution between the 1st class passenger fares rich super rich There s an interesting bimodality in the 2nd class cabins and a long tail in the 3rd class ones. This might be enough to explain all the variance in the Age _known variable. Also this feature should be evaluated in our modelling step to see if it s still significant in the presence of the Sex feature. Consequently the result of the final splits are called leaf notes on a tree it doesn t get smaller than leafs. This process is called K fold cross validation. This kind of plot is a more detailed visualisation of relationships between variables. com arthurtok introduction to ensembling stacking in python into our script. html forest Extremely Randomised Trees Extremely Randomised Trees is an ensemble classifier similar to random forests. For instance Young was designed to replace Age and Title as a combination of the two. Voting can be more powerful when used with weights so that several weaker classifiers can only successfully vote against one two stronger ones if they consistently agree on a specific prediction. csv will now appear in the Output tab of this kernel. However we have seen before that there might be imbalances in the dominating features Sex and Plcass that create an apparent signal. com 2017 01 23 a kaggle master explains gradient boosting Source 2 http scikit learn. Just by themselves the last two are definitely not impressive. What we want to find are the best predictors for survival. Not many of the children there survived but then there were not many children to begin with. A natural choice in this case is to transform the values logarithmically. Therefore we will use 2 Age Groups updating to the Young variable we defined above. Here we also define a consistent colour scheme for the distinguishing between survived not survived. Thereby this is an ensemble method which combines the results of individual classifiers to improve the accuracy. Here the size of each step multiplied by a factor 0 1. There a bit more tuning might be appropriate. Advantages of decision trees are that they can deal with both numerical and categorical data are able to handle multi output problems and are easy to follow and interpret. Embarking at C resulted in a higher survival rate than embarking at S. org wiki Perceptron K Nearest Neighbours Nearest Neighbours a non parametric classifier that uses the training data closest to each test data point to classify it. The weak classifier itself does not necessarily have to be a tree but a tree seems to be the favourite approach to use here. In addition we include the option to use a confusion matrix from this website http notmatthancock. More importantly there is a reasonable argument to be made for this new Fare_eff feature to represent the actual fare better than the original feature. There are two types of ensemble methods boosting used below and averaging or bagging see above. Instead of reducing the residuals and the corresponding squared errors Gradient Boosting focusses on minimising the Loss Function by training the classifier on the gradient of this function. Let s come back to this point in discussing the derived features. This might be related to the fact that women were more likely to share a cabin and it would therefore indicate that the Fare might be a fare per cabin and not per passenger. Turns out that we are more likely to know the age of higher class passengers or women which are the strongest survival predictors we have found so far. 2 Extra Trees Parameters max_features 0. Alone Travelling alone appears bad enough to be significant. Let s see about that in the derived features. For males being in 1st class gives a survival boost otherwise the proportions look roughly similar. Cabin numbers This is a tricky feature because there are so many missing values and the strings don t all have the same number or formatting. Let s follow that up a bit. At the start of this section we define a new feature Fare _cat as fare categories in the same way. The support vectors are a subset of training data points used in the decision function. In the modelling step we will first determine which of the features carry the most signal to be done and then use them to train a number of different classifiers. Plus scrolling only works with arrow keys at least for me on Firefox. Fill in secondary metrics accuracy true pos rate etc. So is parch sibsp 0 i. com headsortails tidy titarnic along the same philosophy as this one here. There is a notable shortage of teenagers compared to the crowd of younger kids. But this could have natural reasons. LightGBM The LightGBM https github. This is a bit of a generalisation in terms of how Miss and Mrs overlap but it might be a useful starting point. It uses the famous Bayes Theorem https en. The idea is to address the issue of missing Age values by combining the Age and Title features into a single feature that should still contain some of the signal regarding survival. Embarked shows the port of embarkation as a categorical character value. most negative first derivative. Metaphorically speaking this is the part where the detective finds the clues. Example if sex male then go left else go right. We start out by copying the relevant parts of the script verbatim standing on the shoulders of giants and so on. We learn the following things from studying the individual features Age The medians are identical. We start with an overview plot of the feature relations Here we show a correlation matrix for each numerical variable with all the other numerical variables. Here s how the standard deviations compare We might even be at a stage now where we can investigate the few outliers more in detail That s really cheap for a 1st class cabin. Source http scikit learn. Let s find the two passengers and assign the most likely port based on what we found so far These are two women that travelled together in 1st class were 38 and 62 years old and had no family on board. Strong correlations between two other features would suggest that only one of them is necessary for our model and including the other would in fact induce noise and potentially lead to over fitting. This feature should be tested in the modelling stage. We are aware that some of the survival fractions we see above are based on small number statistics e. Feel free to check it out and let me know your feedback Outline Note the hyper links kind of work in that they take you to the corresponding section but create a separate HTML page every time you click one of them. Here we also use an Out of bag score oob _score True. It s like a correlation matrix in a sense. And for Age we will choose a different approach below. Violin plots are a modified version of boxplots where the shape is a kernel density estimate of the underlying distribution. Finally let s check what s going on between Age and Embarked The curious distribution for the Q survivors somewhat follows the overall trend for 3rd class passengers which make up the vast majority of Q but is notably narrower. Too many Cabin numbers are missing. This could be useful. test of course doesn t have the column that indicates survival. We should test the predictive power in our modelling. The principles of bootstrapping and the out of bag score can be applied to most classifiers and we already used them in the bagging classifier above. For categorical features we will use barplots plus standard deviation bars to better judge the significance. We learn There is a strong impact of Sex and Pclass on this new feature. Note that since we are selecting by Age which has many missing values a number of children will be in the Child False group. plot kind bar stacked True color nosurv_col surv_col foo combine Age. html Random Forest Random Forest As the name suggests this classifier is using a number of decision trees instead of just a single one. This is expected to increase the accuracy of the final prediction. Disadvantages include Problem A tendency to overfitting. Missing values missing complete 1. Nevertheless Miss is more likely to indicate a younger woman. 02 Taner s code sklearn example code from http notmatthancock. For unbalanced problems setting class _weight balanced might be helpful compare decision tree notes. However in my opinion we have better reasons to impute C instead. For a view into Pclass vs Sex let s use a mosaic plot for a 2 dimensional overview. com kiralt in the comments we also use a Confusion Matrix to evaluate the performance of our classifier. However we see again that a large part of this effect disappears once we control for Sex and Pclass. Sharing a ticket number is not uncommon. This is the default setting. and making it run in our environment. Preparing our prediction for submission submit complete 1. Logistic Regression again this time with only the selected columns Perceptron Perceptron This is a binary classifier that creates a linear decision boundary based on a hyper plane in the parameter space. arange 0 81 1 layout 3 1 sharex True figsize 8 12 for some reason in this plot the colours for m f are flipped max_depth 3 class_weight balanced min_weight_fraction_leaf 0. Age is the integer age of the passenger. groups of pointplots from the seaborn plotting package to visualise the categorical relations We learn Both the factorplot and the mosaicplot indicate that almost all females that died were 3rd class passengers. Despite this oversimplification Naive Bayes classifiers are performing well in many cases. 2 Support Vector Classifier parameters Create objects for each classifier Create Numpy arrays of train test and target dataframes to feed into our models Create our OOF train and test predictions. However here we get 1 warning per n_estimators from a depreciation warning in the inner workings of the classifier over which we have no control. Above we are creating a kind of summary dashboard where we collect relevant visualisations to study the distributions of the individual features. Load Data and Modules load complete 1. Sex is an indicator whether the passenger was female or male. There might be a correlation with other variables here though. Fare _cat Let s remind ourselves of the distribution of Fare with respect to Pclass To simplify this broad distribution we decide to classify the fares into 3 fare categories 0 10 10 100 and above 100. K is simply the number of neighbours that are making the decision by majority vote. html true negative false positive etc. Another recent kernel definitely worth checking out https www. The factorplot suggests that bad tickets are worse for male passengers and 3rd class passengers. Here we will see how the distributions of survivors and non survivors compare. Parch in 1 3 and Sibsp in 1 2 is good. Python walk through for Titanic data analysis This is a work in progress. How much does it actually matter Well in the big picture these are only 2 passengers and their impact on our model accuracy won t be large. LightGBM is often signficantly faster than XGBoost and achieves at least a similar accuracy. First we make sure that the passengers in each group really had the same Fare values Almost 100 yes. There are NaN values in this column. We use overlapping histograms for ordinal features and barplots for categorical features. By flat out predicting that everyone in the testing data set died we would get a 60 accuracy. Here left and right defines a split at a so called node the decision itself. The minimum maxim values for pclass age sibsp parch show us the range of these features. We learn parch 4 and sibsp 3 is bad. For sparse X convert to sparse csc_matrix to speed up the learningAll of the information above is digested from the sklearn documentation http scikit learn. Also we see that there s quite a range in fares. We see that the distributions become significantly narrower and that the tails and bimodality become much weaker after getting rid of the zero fare values for both groups. This is a small number that we could ignore but we are curious aren t we It s Mr Osen and Mr Gustafsson on Ticket 7534. Above we created a new feature called Ttype which defines the type of a ticket through the first digit of the ticket number. put back into the bag I suppose. Just change the comment tags to switch between the options. 1 line that separates 2 classes see the link below. As the kernel continues to grow it is branching out in more detail into the different data analysis steps. Overall there is a certain amount of variance and we re not going to be able to pinpoint a certain age based on the title. For larger numbers of Parch we have 4 vs 0 4 vs 1 and 1 vs 0. Embarked Well that does look more interesting than expected. A random forest is an averaging classifier for which we train several estimators independently and then average over their individual predictions. For this we look at the combined data set to make sure that we don t miss any titles that might be in train or test only Ok so we have 18 different titles but many of them only apply to a handful of people. Relations between featuresAfter inspecting the available features individually you might have realised that some of them are likely to be connected. This is a kind of inbuilt cross validation step since the accuracy score of the classifier is estimated on data it wasn t trained on. behave similarly with other new or existing features. The first split can be followed up by additional ones to narrow down the decision criteria based on the subset defined by each previous split. We can use a binomial test to estimate the probability that 5 non survivors out of a total 5 passengers with SibSp 5 happened due to chance assuming the overall 38 survival chance for the entire sample. However it s noticeable that fewer young adults have survived ages 18 30 ish whereas children younger than 10 ish had a better survival rate. Nonetheless we ll try because we are optimistic people at heart. Hopefully this will result in a better understanding of stacking. Problem Unstable to small variations in the data. Deck Ok so what can we tell from the Deck derived from the Cabin number First of all the overall survival statistics is much better than for the full sample which is what we found above. Ada Boost AdaBoost A boosting classifier that fits sequences of weak learners that are progressively weighted toward those features that the previous weak learners misclassified. Also in the test data there is one Fare missing cheeky selection almost 100 Age values are unknown and only 91 Cabin numbers were preserved. Every time we can think of a new feature we come back here to define it and then study it further down. However it could also mean that the listed value is the cumulative fare per cabin and it was simply recorded as the same value for each passenger. Maybe a transcription error in the data itself And that s quite expensive for a 3rd class ticket. html is a rapid classification method. For now we just continue with a rather intuitive set of important features. weights uniform assigns equal weight to each neighbour whereas distance gives more weight to neighbours that are closer. Strictly speaking bagging is only the correct term if the sub samples are drawn with replacement i. how well the classifier that was trained on a particular sample can be applied to new data. Otherwise it s called Pasting. The names also contain titles and some persons might share the same surname indicating family relations. We should test it out in the modelling stage. First we define some plotting function then we plot. But in these days when you were travelling as a group family did everyone really get their own ticket Let s find out how many unique ticket numbers there are Interesting. Another way of checking the actual numbers are through cross tables Passengers with more than 3 children parents on board had low survival chances. ModellingLet s summarise briefly what we found in our data exploration sex and ticket class are the main factors there seem to be additional impacts from age young men vs young women male children relatives parch 1 3 sibsp 1 2 somewhat explained by sex but not completely maybe the cabin deck but not many are known other apparent effects appear to be strongly connected to the sex class features port of embarkation fare sharing a ticket large family travelling alone known cabin number known age Splitting the train sample into two sub samples training and testing This is best practice for evaluating the performance of our models which should not be tested on the same data they are trained on. One suggestion is to use a large number of highly overfitted trees with small split limits and no depth limit. Let s try it out Our usual factorplot examination highlights the differences between Pclass as expected but also shows some interesting variations within the Sex feature. Nonetheless it is a valuable exploratory tool that has a place in everyone s toolbox. com arthurtok introduction to ensembling stacking in python and the references therein. The dominating ones are Mr 581 Miss 210 Mrs 170 and Master 53 with the number referring to the combined data. Coming soon The next step will use the pre packaged stacking classifier of the mlxtend package. Preparing for modelling encode complete 1. What are the types of data and their typical shape and content Together with the PassengerId which is just a running index and the indication whether this passenger survived 1 or not 0 we have the following information for each person Pclass is the Ticket class first 1 second 2 and third 3 class tickets were used. Also there are no obvious outliers that would indicate problematic input data. In addition we see that the Fare was identical for all the passengers in each ticket group. The estimator above it s a KNN is used multiple times on subsets of the training sample and then it uses the average vote. Therefore it would be more useful to replace the predominantly tree based sample of classifiers with a more diverse set. If you want a step by step overview then have a good look at Anisotropic s Kernel https www. Possibly travelling alone Sort of yes. Only 2 values stand out. Initial Exploration explore complete 1. Finally we model a fare category Fare_cat as an ordinal integer variable based on the logarithmic fare values Because of the larger number of Miss vs Master mostly women are classified as Young. However since the main point of this challenge is to practice data analysis it is certainly worth to take your time to examine the question in a bit more detail. Fare is a float feature showing how much each passenger paid for their rather memorable journey. In addition already a grouping without the Parch and SibSp features suggests similar numbers for women in 1st class embarking from C 71 vs S 69 in contrast to the larger overall number of all 1st class passengers leaving from S. The idea is to define a number of possible values for each hyper parameter. Also we will start to use factorplots i. We use the dashed lines in the plot above for an empirical division into 3 classes which separate the cheaper Fare_eff of a Pclass group from the more expensive ones of the next one. in the corresponding text field so that you remember the model for this score and don t have to re submit something that you had done already. min_samples_leaf 5 is a useful initial value. We learn In the training data a large majority of Cabin numbers are missing together with 177 Age values and 2 Embarked values. We learn Bad _ticket might be a lower order effect that could give us some additional accuracy. Modelling model medium completeness to be extended 1. eXtreme Gradient Boosting XGBoost eXtreme Gradient Boosting It s not just a good name for a band but XGBoost was also the flavour of the month tool for kaggle competitions in 2016. Their Fares are close enough though to include them in the general treatment. Boys have proportional better survival chances than men whereas girls have similar chances as women have. Best to keep that in mind. Model validation We want to make sure that our classifiers are not overfitting random data features. Pclass There s a clear trend that being a 1st class passenger gives you better chances of survival. This means it s a way to average over a large number of individual classifiers to improve their accuracy by reducing the variance noise. Following a suggestion by Taner https www. In our training data set about 60 of the passengers didn t survive. The matrix gives us an overview as to which features are particularly interesting for our analysis. Each tree is a series of if then else decisions. Because this is ultimately our goal to apply the classification method we learn from the training data to any data in particular the one that is used to judge this competition. We learn For females the survival chances appear to be higher between 18 and 40 whereas for men in that age range the odds are flipped. Think of it as an average of estimators. In the next step we will try to incorporate the information from the great Introduction to Ensembling Stacking in Python by Anisotropic https www. We have already encountered this strategy in our Random Forests or Bagging estimators above where the aim was to get a more accurate estimate from combining multiple runs of a single classifier like a Decision Tree for instance. Now let s think for a moment Identical fares could mean that the fare for a cabin was shared equally among the passengers in which case our previous treatment would have been justified. We start with a Logistic Regression to assess the importance of the individual model features. This goes some way to explain features like better survival for SibSp 1 3. In fact in the plot above the offset had already been applied as well. Or Family and SibSp Parch. For once it splits much cleaner between the Pclasses So well in fact that defining new fare categories seems almost redundant because Pclass already captures most of this signal. Go to the top of the page top 7. Age _known Similar to the known Cabin numbers what about the passengers for which we know the age As we would expect intuitively it appears that we are more likely to know someones age if the survived the disaster. Ttype and Bad _ticket Let s have a look at the ticket numbers and see whether we can extract some additional deck information from them. This is a simple yet powerful method that works well for irregular decision boundaries. But again the sharing of tickets is more frequent with females and 1st class passengers. We ve already used this cross validation above to compute the scores for the individual classifiers. one with replacement of the original training set. In my opinion the only training feature for which it makes sense to fill in the NAs is Embarked. Title What can we learn from the titles in the passenger names These could give us a direct independent way to estimate the missing age values so let s look at all the available titles their frequency and mean age. plot_confusion_matrix cnf_matrix classes class_names normalize True title Normalized confusion matrix for clf label in zip clf_tree clf_knn clf_svm clf_ext clf_gb clf_xgb clf_pctr clf_log clf_rf clf_bag clf_vote tree knn svm extra gb xgb percep logistic RF Bag Ensemble scores cross_val_score clf X y cv 5 scoring accuracy print Accuracy 0. This is reflected in the relatively low correlation index of the SVM with everything else. Life just isn t fair. Above we extract the standard deviation of the Fares among the ticket groups. com Microsoft LightGBM is another gradient boosting tool which in 2017 was beginning to eclipse the XGBoost as Kaggle s go to method for efficient boosting. For instance Master is a boy while Mr is a man. Both strongly positive or negative correlations with the Survived feature are valuable. In 1st class younger adults had better survival chances than older ones. This is the part where the detective puts individual clues together to see whether their sum is more than its parts. We will start with a grid search algorithm to find the best parameters to run our classifier. astype float axis 0. Intuitively this doesn t seem so plausible since you typically record what is paid for a ticket and not for a cabin. Other correlations are visible in the heatmap above. ", "id": "headsortails/pytanic", "size": "59288", "language": "python", "html_url": "https://www.kaggle.com/code/headsortails/pytanic", "git_url": "https://www.kaggle.com/code/headsortails/pytanic", "script": "sklearn.metrics SklearnHelper(object) cross_val_score __init__ matplotlib.gridspec sklearn.naive_bayes lightgbm statsmodels.graphics.mosaicplot predict AdaBoostClassifier BaggingClassifier StackingClassifier train KNeighborsClassifier show_confusion_matrix seaborn numpy plot_confusion_matrix ExtraTreesClassifier GradientBoostingClassifier get_oof VotingClassifier sklearn.ensemble sklearn sklearn.model_selection confusion_matrix KFold RandomForestClassifier matplotlib.pyplot stats Perceptron pandas LogisticRegression fit mlxtend.classifier svm mosaic scipy GridSearchCV sklearn.neighbors show_confusion_matrix2 sklearn.linear_model GaussianNB tree xgboost train_test_split ", "entities": "(('Rule', 'all_features'), 'sqrt') (('we', 'engineering part'), 'indicate') (('it', 'data analysis different steps'), 'continue') (('family 1 3 members', 'best survival'), 'learn') (('many', 'people'), 'look') (('gb xgb percep logistic RF cross_val_score y cv scoring extra Bag Ensemble 5 accuracy', 'Accuracy'), 'class') (('which', 'still significant impact'), 'be') (('we', 'already above individual classifiers'), 'try') (('that', 'highest accuracy'), 'be') (('we', 'same way'), 'define') (('which', 'priority machine learning'), 'help') (('results prediction', 'residuals'), 'fit') (('only mathematics', 'you'), 'save') (('passengers', 'Fare really same values'), 'make') (('we', 'then one'), 'use') (('Disadvantages', 'overfitting'), 'include') (('we', 'sample SibSp 5 different rest'), 'say') (('how they', 'target'), 'want') (('time you', 'them'), 'feel') (('which', 'additional classifier'), 'try') (('color nosurv_col surv_col True foo', 'Age'), 'stack') (('Important choosing', 'heavily data'), 'parameter') (('you', 'competition header'), 'download') (('This', 'smaller more extreme numbers'), 'Fare') (('how distributions', 'survivors'), 'see') (('somewhat it', 'S'), 'about') (('I', 'https Titanic www'), 'for') (('strong cheaper cabins', 'survival'), 'check') (('Also we', 'fares'), 'see') (('we', 'more subtle effects'), 'be') (('Here single trees', 'prediction probabilities'), 'combine') (('simple yet powerful that', 'decision well irregular boundaries'), 'be') (('We', 'grid'), 'use') (('you', 'that'), 'in') (('when we', 'feature importances'), 'come') (('We', 'test data frame'), 'fill') (('classifiers', 'data random features'), 'validation') (('order lower that', 'additional accuracy'), 'learn') (('Of course assumption', 'name last thing'), 'test') (('we', 'them'), 'let') (('Now we', 'more detail'), 'com') (('Set ylabels', 'tn'), 'fill') (('that', 'ticket number'), 'be') (('we', 'which'), 'create') (('This', 'times'), 'do') (('We', 'C'), 'let') (('As I', 'still variation'), 'far') (('that', 'new data'), 'apply') (('Ticket class first 1 second 2 third 3 tickets', 'person'), 'be') (('we', 'notmatthancock'), 'include') (('kind', 'variables'), 'be') (('we', 'Randomized Extremely Trees'), 'substitute') (('completely values', 'addition'), 'break') (('Embarking', 'S.'), 'result') (('features', 'particularly analysis'), 'give') (('survival chances', 'much cheaper cabins'), 'tell') (('that', 'factorplot'), 'indicate') (('we', '0'), 'have') (('median average', 'reduced uncertainty'), 'have') (('natural choice', 'values'), 'be') (('some', 'them'), 'realise') (('Common last names', 'passenger list'), 'be') (('This', 'e.'), 'be') (('very someone', 'instead 304'), 'share') (('One suggestion', 'small split limits'), 'be') (('We', 'new feature'), 'learn') (('that', 'project'), 'module') (('we', 'integer'), 'start') (('really expensive Fares', 'Pclass'), 'be') (('significantly tails', 'groups'), 'see') (('so s', 'available titles'), 'title') (('we', 'common name'), 'share') (('classifiers', 'string input'), 'be') (('Here we', 'score oob _ score True'), 'use') (('csv', 'kernel'), 'appear') (('apparent pattern', 'just random noise'), 'let') (('we', 'Anisotropic https www'), 'try') (('recent kernel', 'https definitely www'), 'worth') (('SibSp', 'passenger'), 'be') (('how results', 'i.'), 'give') (('cheap cabins', 'mostly deeper ship'), 'assume') (('Extra Trees Random Forest AdaBoost Gradient Boost Support Vector Classifier Survived', 'new features'), 'use') (('We', 'categorical features'), 'use') (('we', 'what'), 'let') (('_ maybe it', 'Deck'), 'inspire') (('majority then vote', 'row'), 'do') (('we', 'more detail'), 'be') (('that', 'deck T'), 'die') (('support vectors', 'decision function'), 'be') (('where detective', 'clues'), 'be') (('org Perceptron Nearest Neighbours Nearest non parametric that', 'it'), 'wiki') (('dominating ones', 'combined data'), 'be') (('Relations', '1'), 'complete') (('less correlation', 'better results'), 'stack') (('that', 'survival'), 'have') (('Kaggle', 'efficient boosting'), 'be') (('that', 'enough Young'), 'have') (('part', 'analysis'), 'call') (('well some', '38'), 'be') (('previous weak learners', 'progressively features'), 'classifier') (('TODO', 'models'), 'set') (('convention', 'training survival values'), 'be') (('valuable exploratory that', 'toolbox'), 'be') (('soon next step', 'mlxtend package'), 'use') (('medians', 'Age'), 'learn') (('One visualisation', 'tree trunk successively smaller structures'), 'be') (('off diagonal elements', 'instance'), 'be') (('Several', 'strongly Sex'), 'learn') (('s', 'derived features'), 'let') (('method', 'thereby larger information'), 'be') (('learning accuracy', 'actual them'), 'be') (('that', 'existing features'), 'be') (('typically what', 'cabin'), 'seem') (('it', 'about 30 vs 40'), 's') (('features', 'next step'), 'allow') (('Alternatively you', 'quick pretty plot'), 'use') (('we', 'Young variable'), 'use') (('large majority', 'Age together 177 values'), 'learn') (('it', 'bit more detail'), 'be') (('that', 'classification binary problem'), 'note') (('Pclass clear being', 'survival'), 's') (('boundaries', 'real Fare'), 'achieve') (('then it', 'average vote'), 's') (('it', 'Sex feature'), 'evaluate') (('it', 'NAs'), 'embark') (('passenger other two classes', 'more especially male children'), 'be') (('that', 'entries'), 'mean') (('individual feature', 'classifier'), 'compare') (('you', 'Kernel https www'), 'have') (('doesn there t', 'embarkation'), 'seem') (('Here we', 'other numerical variables'), 'start') (('this', 'cabins'), 'be') (('only one', 'potentially fitting'), 'suggest') (('we', 'C'), 'have') (('stronger correlations', 'red positive correlation'), 'have') (('We', 'something'), 'look') (('t', 'data'), 'be') (('which', '1'), 'give') (('we', 'title'), 'be') (('last two', 'Just themselves'), 'be') (('Embarked', 'character categorical value'), 'show') (('feature realisations', 'again train'), 'compute') (('strings don tricky so many missing all', 'same number'), 'number') (('which', 'slower decent'), 'make') (('Mrs', '20 30 years old'), 'contain') (('We', 'modelling stage'), 'test') (('Naive Bayes classifiers', 'well many cases'), 'perform') (('that', 'link'), 'see') (('therefore Fare', 'passenger'), 'relate') (('which', 'ticket number'), 'create') (('that', 'survival higher chances'), 's') (('plot', 'better 1000 words'), 'explain') (('s', 'group family'), 'get') (('already grouping', 'S.'), 'suggest') (('the', 'disaster'), 'know') (('we', 'classifier'), 'kiralt') (('This', 'survival chances'), 'conflate') (('binary that', 'parameter space'), 'Regression') (('1st very few children', '3rd Most older passengers'), 'learn') (('We', 'model individual features'), 'start') (('classifier', 'different somewhat arbitrary weight'), 'decide') (('new feature', 'page'), 'call') (('1 obviously feature', 'deep red diagonal'), 'have') (('most', 'differences'), 'follow') (('it', 's'), 're') (('you', 'survival'), 'seem') (('don t', 'themselves'), 'be') (('closer', 'colour'), 'be') (('titles', 'age certain group'), 'know') (('Mr', 'instance'), 'be') (('we', 'control'), 'get') (('they', 'consistently specific prediction'), 'be') (('Earlier we', 'overview figure'), 'have') (('mostly women', 'Young'), 'model') (('scikit', 'docs alternatively more complex http'), 'define') (('classes', 'biased trees'), 'problem') (('engineered features', 'most impact'), 'let') (('2017 01 23 kaggle master', 'http Source 2 scikit'), 'com') (('Fare', 'ticket group'), 'see') (('s', 'them'), 'change') (('increase', 'positive correlated feature'), 'need') (('where shape', 'kernel density underlying distribution'), 'be') (('That', 'class really 1st cabin'), 's') (('that', 'roughly cases'), 'be') (('as well you', 'presumably wife'), 'try') (('method', 'classifier'), 'be') (('we', 'fare zero entries'), 'need') (('predictor features', 'target only variable'), 'Bayes') (('you', 'own decision'), 'recommend') (('Now we', 'bigger picture'), 'connect') (('two that', 'board'), 'let') (('non 5 survivors', 'entire sample'), 'use') (('step sizes', 'iteration'), 'vary') (('Other correlations', 'heatmap'), 'be') (('odds', 'age'), 'learn') (('t', 'case'), 'seem') (('sub only correct samples', 'replacement i.'), 'be') (('new feature', 'still useful information'), 'suggest') (('how it', 'at least initial stages'), 'be') (('Therefore it', 'more diverse set'), 'be') (('classifiers', 'others'), 'give') (('This', 'SibSp'), 'go') (('Lower numbers', 'bias'), 'decrease') (('which', 'survival less than 25 e.'), 'define') (('they', 'only relatively little data'), 'be') (('Here size', 'factor'), 'multiply') (('they', 'output multi problems'), 'be') (('Solution', 'features'), 'prune') (('I', 'overview'), 'define') (('it', 'Parch feature'), 'think') (('earlier men', 'probably observation'), 'reflect') (('I', 'submission e.'), 'recommend') (('which', 'quite two dimensions'), 'define') (('factorplot usual examination', 'Sex feature'), 'let') (('we', 'why what'), 'be') (('scheme', 'soon kernel'), 'use') (('who', 'group'), 'show') (('we', 'don data'), 'be') (('This', 'progress'), 'walk') (('We', 'modelling'), 'test') (('We', 'model testing'), 'include') (('strongly positive correlations', 'Survived feature'), 'be') (('which', 'C more women'), 'learn') (('tree', 'resulting random fluctuations'), 'consider') (('particular distinction', 'well most classifiers'), 'be') (('that', 'class quite 3rd ticket'), 'error') (('very well Miss', 'as well younger about 40'), 'be') (('Bagging', 'Random Forest'), 'be') (('part', 'still quite future versions'), 'TODO') (('we', 'what'), 'ok') (('it', 'fare groups'), 'avoid') (('how it', 'Fare distribution'), 'let') (('predict classifier', 'Random Forest max_features'), 'adjust') (('also whole notebook', 'overall content'), 'add') (('competition 418 rows', 'only columns'), 'write') (('that', 'html bagging estimator Decision Tree Decision meta One classifiers'), 'Tree') (('maxim minimum values', 'features'), 'show') (('code', 'http notmatthancock'), 'sklearn') (('most large families', '3rd class'), 'travel') (('we', 'connections'), 's') (('classifier', 'decision trees'), 'Forest') (('uncertainties', 'class 3rd passengers'), 'learn') (('who', '12 people'), 'see') (('we', 'output'), 'have') (('again sharing', 'more females'), 'be') (('it', 'leafs'), 'call') (('s', 'more detail'), 'let') (('which', 'next one'), 'use') (('dominant everything', 'else similar significantly lower importance'), 'be') (('survival', 'close to 100 male'), 'be') (('node', 'instead them'), 'introduce') (('we', '60 accuracy'), 'predict') (('small number', 'number large prevents'), 'lead') (('classifier', 'remaining one e.'), 'divide') (('we', 'bagging classifier'), 'apply') (('survival strongest we', 'class higher passengers'), 'turn') (('Male children', '2nd class'), 'learn') (('some', 'collinearity high i.'), 'know') (('we', 'survival statistics'), 'think') (('persons', 'family relations'), 'contain') (('women', 'similar chances'), 'have') (('who', 'passenger'), 'suspect') (('Young', 'two'), 'design') (('similar effect', 'page'), 'see') (('It', 'diagonal'), 'show') (('Now we', 'model features'), 'test') (('feature', 'modelling stage'), 'test') (('1st class younger adults', 'older ones'), 'have') (('html Extremely Randomised Extremely Randomised Trees', 'ensemble similar random forests'), 'forest') (('majority', '3rd class'), 'survive') (('other we', 'Young'), 'title') (('Also obvious that', 'input problematic data'), 'be') (('we', 'it'), 'html') (('t', 'well new data'), 'be') (('highest ages', 'well overall distribution'), 'be') (('we', '4 vs 0'), 'have') (('plane', 'training data points'), 'Machine') (('number', 'Child False group'), 'note') (('corresponding cells', 'same features'), 'contain') (('which', 'PassengerID'), 'exclude') (('want', 'best survival'), 'be') (('right when it', 'instead up top'), 'tree') (('what', 'them'), 'recommend') (('they', 'cabin more number'), 'assume') (('we', 'deviation standard better significance'), 'use') (('convincing case', 'also comments'), 'make') (('where we', 'individual features'), 'create') (('almost Pclass', 'signal'), 'for') (('Cabin', 'passenger'), 'give') (('training data', 'passengers didn t survive'), 'set') (('However last plot', 'also that'), 'indicate') (('cross First simple validation', '_ val _ score'), 'cross') (('which', 'Q'), 'let') (('Fares', 'general treatment'), 'be') (('we', 'C.'), 'be') (('Now we', 'prediction'), 'want') (('factor', 'parameters'), 'call') (('text 3 string', 'Ticket Cabin Name'), 'have') (('Here we', 'just bit underlying idea'), 'focus') (('together sum', 'parts'), 'be') (('you', 'confusion normalised matrix'), 'include') (('where aim', 'instance'), 'encounter') (('that', 'apparent signal'), 'see') (('how well it', 'data'), 'help') (('we', 'numerical features'), 'learn') (('I', 'Titanic Tutorials https www'), 'recommend') (('We', 'classifier'), 'start') (('us', 'false positives'), 'allow') (('Usually it', 'more detail'), 's') (('you', 'kernel'), 'm') (('instead categorical we', 'continuous variables'), 'be') (('we', '0 10 10 100 100'), 'let') (('feature', 'character variable length'), 'be') (('that', 'best score'), 'evaluate') (('we', 'overview'), 'prefer') (('t', '1st class male passengers'), 'be') (('easiest method', 'Voting Classifier'), 'be') (('It', 'sense'), 's') (('variance', 'variable'), 'be') (('It', 'Bayes Theorem famous https'), 'use') (('We', 'last name'), 'go') (('This', 'approach'), 'be') (('sklearn documentation http above scikit', 'information'), 'learn') (('Above we', 'ticket groups'), 'extract') (('we', 'number statistics above small e.'), 'be') (('which', 'half'), 'study') (('large family', 'survival'), 'Family') (('Again upper right left triangle', 'same information'), 'contain') (('classifiers', 'better others'), 'borrow') (('Nevertheless Miss', 'more younger woman'), 'be') (('Here we', 'survived'), 'define') (('However differences', 'anything'), 'be') (('confidence associated 95 which', 'sparse samples'), 'show') (('Derived engineered features', 'largely complete 1'), 'derive') (('ensemble which', 'accuracy'), 'be') (('most important settings', 'n_estimators trees'), 'parameter') (('it', 'terms'), 'be') (('best decks', 'B about 66 chance'), 'beyond') (('XGBoost', '2016'), 'Gradient') (('max_depth tune', 'replacement'), 'be') (('we', 'bootstrap sample random i.'), 'stump') (('tree', 'then else decisions'), 'be') (('you', 'less probably one'), 'let') (('we', 'then it'), 'time') (('survival chances', 'actually slightly S.'), 'be') (('many', 'children'), 'survive') (('One', 'robustness'), 'call') (('them', 'them'), 'feel') (('we', 'previous results'), 'be') (('we', 'optimistic heart'), 'try') (('It', 'cases'), 'be') (('we', 'also median importance'), 'compute') (('difference', 'themselves'), 'be') (('that', 'competition'), 'be') (('that', 'Pclass'), 'learn') (('We', 'feature also training sets'), 'create') (('classifier', 'them'), 's') (('how much prediction', 'certain amount'), 'describe') (('these', 't'), 'matter') (('Making', 'just a few cases'), 'be') (('Personally I', 'scaled features'), 'like') (('that', 'neighbours'), 'equal') (('We', 'class long 3rd ones'), 'let') (('This', 'final prediction'), 'expect') (('we', 'input columns'), 'say') (('We', 'page'), 'base') (('we', 'them'), 'have') (('max_depth 3 class_weight', 'm f'), 'arange') (('We', 'giants'), 'start') (('classifier Create Numpy arrays', 'OOF train predictions'), 'create') (('that', 'majority vote'), 'be') (('ticket', 'survival'), 'appear') (('impact', 'Young'), 'mean') (('that', 'already earlier plots'), 'recover') (('Cabin only 91 numbers', 'one cheeky selection'), 'be') (('then we', 'plotting function'), 'plot') (('that', 'survival'), 'be') (('idea', 'hyper parameter'), 'be') (('http scikit', 'sklearn also official example'), 'use') (('We', 'individual classifiers'), 'use') (('method', 'greatest decent i.'), 'use') (('K', 'cross validation'), 'call') (('Hopefully this', 'stacking'), 'result') (('additional randomness', 'best performing threshold'), 'introduce') (('It', 'Mr Mr Ticket'), 'be') (('Set coordinate', 'tinkering'), 'example') (('we', 'independently then individual predictions'), 'be') (('otherwise proportions', 'survival boost'), 'give') (('previous treatment', 'case'), 'let') (('This', 'everything'), 'reflect') (('above already so stacking doesn', 'significant way'), 'be') (('they', 'same data'), 'summarise') (('data how many points', 'class'), 'contain') (('that', 'still enough context'), 'be') (('we', 'bag oob'), 'use') (('clear separation', 'Q isn t'), 'mean') (('we', 'important features'), 'continue') (('estimates', 'closely dotted zero level'), 'smooth') (('women', 'more together larger families'), 'be') (('we', 'different approach'), 'choose') (('it', 'passenger'), 'mean') (('s', '2 dimensional overview'), 'let') (('plot', 'Anisotropic https www'), 'inspire') (('it', 'variance noise'), 'mean') (('Larger values', 'decision boundaries'), 'suppress') (('Others', 'more specific ways'), 'require') (('we', 'Mlle Young'), 'define') (('Solution', 'same value'), 'balance') (('hopeless it', 'just random strings'), 'have') (('bad tickets', 'male passengers'), 'suggest') (('t', 'good them'), 'seem') (('Finally we', 'test data set'), 'pick') (('way', 'survival low chances'), 'be') (('certainly even S', 'class 1st passengers'), 'bad') (('first split', 'previous split'), 'follow') (('first which', 'different classifiers'), 'determine') (('Initial Exploration', 'as many different ways'), 'look') (('18 30 ish children', 'survival younger than 10 better rate'), 's') (('SibSp alone 0 large family', 'you'), 'Parch') (('larger improvements', 'subset'), 'well') (('passenger', 'rather memorable journey'), 'be') (('we', 'Sex'), 'see') (('scikit', 'hyper parameters'), 'call') (('integer ordinal that', 'passenger'), 'be') ", "extra": "['outcome', 'test', 'bag', 'procedure']"}