{"name": "fine grained sentiment analysis with transformer ", "full_name": " h1 Fastai with HuggingFace Transformers BERT RoBERTa XLNet XLM DistilBERT h1 Introduction Story of transfer learning in NLP h1 Integrating transformers with fastai for multiclass classification h2 Libraries Installation h2 The example task h2 Main transformers classes h2 Util function h2 Data pre processing h3 Custom Tokenizer h3 Custom Numericalizer h3 Custom processor h2 Setting up the Databunch h3 Custom model h2 Learner Custom Optimizer Custom Metric h2 Discriminative Fine tuning and Gradual unfreezing Optional h2 Train h2 Export Learner h2 Creating prediction h1 Conclusion h1 References ", "stargazers_count": 0, "forks_count": 0, "description": "TrainNow we can finally use all the fastai built in features to train our model. html bertconfig for the configuration class. embeddings learner. To sum it up if we look carefully at the fastai implementation we notice that 1. References Hugging Face Transformers GitHub Nov 2019 https github. Creating predictionNow that the model is trained we want to generate predictions from the test dataset. com p fastai with transformers bert roberta xlnet xlm distilbert 4f41ee18ecb2 source email 29c8f5cf1dc4 writer. The BaseTokenizer object https docs. From this analyse we suggest two ways to adapt the fastai numericalizer 1. co created the well known transformers library https github. As mentioned in the HuggingFace documentation BERT RoBERTa XLM and DistilBERT are models with absolute position embeddings so it s usually advised to pad the inputs on the right rather than the left. Therefore we can simply create a new class TransformersBaseTokenizer that inherits from BaseTokenizer and overwrite a new tokenizer function. com huggingface transformers models always output tuples every model s forward method always outputs a tuple with various elements depending on the model and the configuration parameters. Medium article Fastai with Transformers BERT RoBERTa XLNet XLM DistilBERT https medium. Here s a quick run down One sentence from the Movie Reviews dataset 156061 An intermittently pleasing but mostly routine effort. To adapt our transformers to multiclass classification before loading the pre trained model we need to precise the number of labels. For those models the encoding methods should be called with add_prefix_space set to True. We can find this learning rate by using a learning rate finder which can be called by using lr_find. Even if the first solution seems to be simpler Transformers does not provide for all models a straightforward way to retreive his list of tokens. Print the available values for pretrained_model_name shortcut names corresponding to the model_type used. edu if you have more questions This makes our dataset significantly harder to analyze since each phrase isn t broken up individually. In our case the parameter pretrained_model_name is a string with the shortcut name of a pre trained model tokenizer configuration to load e. STUDENT Yes you re on the right track The subject guide page that you navigated to with the picture of of Greg Nelson is a list of good resources. html load_learner you have to be careful that each custom classes like TransformersVocab are first defined before executing load_learner. You can also find this information on the HuggingFace documentation https huggingface. The transformers library is standalone but incorporating it with the fastai library provides a simpler implementation compatible with powerful fastai tools like Discriminate Learning Rate Gradual Unfreezing or Slanted Triangular Learning Rates. Dev Sharma s article Using RoBERTa with Fastai for NLP https medium. Here we finally unfreeze all the layers. He also explained key techniques to fine tune the models like Discriminate Learning Rate Gradual Unfreezing and Slanted Triangular Learning Rates. The Tokenizer object https docs. Each link will take you to a database with thousands of articles. Custom processorNow that we have our custom tokenizer and numericalizer we can create the custom processor. html Discriminative layer training. Therefore I implemented the second solution which runs for each model type. In Kaggle the fastai and torch libraries are already installed. ai videos lesson 4 given by Jeremy Howard in a class at BYU. layer 2 learner. STUDENT Hi Be with you in just a second PATRON I am under subject guides nutrition dietetics and food science and find articles. This allows the classifier to train more than the rest of the layers while still allowing us to take advantage of the pretrained model. ai Fastai documentation Nov 2019 https docs. I can t seem to find articles on anything I type in. co transformers pretrained_models. Create a new class TransformersVocab that inherits from Vocab and overwrite numericalize and textify functions. Next we will use fit_one_cycle with the chosen learning rate as the maximum learning rate. com analytics vidhya using roberta with fastai for nlp 7ed3fed21f6c which makes pytorch_transformers library compatible with fastai. Let s first analyse how we can integrate the transformers tokenizer within the TokenizeProcessor function. The TokenizeProcessor object https docs. There are multible ways to create a DataBunch in our implementation we use the data block API https docs. Here 2e 3 seems to be a good value. Seems to not working For DistilBERT For roberta base list_layers learner. ai course https course. Custom NumericalizerIn fastai NumericalizeProcessor object https docs. Although these models are powerful fastai does not include them. Libraries InstallationBefore starting the implementation you will need to install the fastai and transformers libraries. layer 8 learner. Fastai with HuggingFace Transformers BERT RoBERTa XLNet XLM DistilBERT Introduction Story of transfer learning in NLPIn early 2018 Jeremy Howard co founder of fast. html Jeremy Howard Sebastian Ruder Universal Language Model Fine tuning for Text Classification May 2018 https arxiv. We can find all the shortcut names in the transformers documentation here https huggingface. He demonstrated how the fastai library makes the ULMFit method significantly easier to implement for someone without much code experience. html berttokenizer for the tokenizer class and BertConfig https huggingface. g bert base uncased. For more information please check the fastai documentation here https docs. Put learn in FP16 precision mode. layer 11 learner. We can now submit our predictions to Kaggle In our example without playing too much with the parameters we get a score of 0. seed_all seed Data pre processingTo match pre training we have to format the model input sequence in a specific format. layer 1 learner. A tokenizer class to pre process the data and make it compatible with the given model. Unfortunately the model architectures are too different to create a unique generic function that can split all the model types in a convenient way. com huggingface transformers Fast. In this implementation be careful about 3 things 1. To do so you can modify the config instance or either modify like in Keita Kurita s article https mlexplained. layer 0 learner. com 2019 05 13 a tutorial to fine tuning bert with fast ai May 2019 Dev Sharma s article Using RoBERTa with Fastai for NLP https medium. This differs from our library dataset where the dataset consists of entire conversations loaded at once. layer 9 learner. An instruction to perform that split is described in the fastai documentation here https docs. co transformers in each model section. com analytics vidhya using roberta with fastai for nlp 7ed3fed21f6c Section 1. Regarding XLNET it is a model with relative position embeddings therefore you can either pad the inputs on the right or on the left. Some models like RoBERTa require a space to start the input string. com huggingface transformers installation. edu ndfs PATRON gotcha PATRON just one click away PATRON thankyou STUDENT You re welcome STUDENT Anything else I can help with PATRON I m good thanks STUDENT Good luck Just shoot us another chat or an email at science_reference byu. com 2019 05 13 a tutorial to fine tuning bert with fast ai as the function get_preds does not return elements in order by default you will have to resort the elements into their correct order. You will see later that those classes share a common class method from_pretrained pretrained_model_name. For Slanted Triangular Learning Rates we have to use the function one_cycle. The sentiment labels are 0 Negative 1 Somewhat negative 2 Neutral 3 Somewhat positive 4 PositiveThis will provide a perfect context for our Library Chat analysis where the labels are 0 Dissatisfied Frustrated 1 Dissatisfied 2 Neither 3 Satisfied 4 Above and BeyondThe data is loaded into a DataFrame using pandas. The implementation gives interesting additional utilities like pre trained tokenizers optimizers and configs. In his demo he used an AWD LSTM neural network pre trained on Wikitext 103 and got state of the art results. We can decide to divide the model in 14 blocks 1 Embedding 6 transformer 1 classifierIn this case we can split our model in this way Check groups Note that I haven t found any documentation about studying the influence of Discriminative Fine tuning and Gradual unfreezing or even Slanted Triangular Learning Rates with transformers. Meanwhile this tutorial is a good starter. If you find any interesting documentation please let me know. 60 does not produce any errors 2. read_csv This is where the models are built and what we will use to read them. fastai pretrained fastai tokenizer will cooperate with our transformer tokenizer. 06146 Keita Kurita s article A Tutorial to Fine Tuning BERT with Fast AI https mlexplained. Where and what were you trying to search PATRON I m trying to find articles for my nutrition research paper PATRON under the guide page i clicked on the the nutrition tab PATRON and then i m on find articles PATRON i tried searching in the search bar my topic but I know its not right because only one or two articles came up PATRON So i think i have to click somewhere else to search more articles but i m not sure where. html BaseTokenizer implements the function tokenizer t str List str that takes a text t and returns a list of its tokens. The difficulty here is that each pre trained model requires exactly the same specific pre process tokenization numericalization that the pre process used during the pre train part. NumericalizeProcessor vocab vocab. com 2019 05 13 a tutorial to fine tuning bert with fast ai Section Initializing the Learner the num_labels argument. It is worth noting that in the dataset there are no individual movie reviews but rather phrases taken out of context and split into smaller parts each with an assigned sentiment label. Therefore we first freeze all the groups but the classifier with We check which layer are trainable. Formerly knew as pytorch transformers or pytorch pretrained bert this library brings together over 40 state of the art pre trained NLP models BERT GPT 2 RoBERTa CTRL implemented in PyTorch as opposed to Tensorflow Julia etc. This year the transformers became an essential tool to NLP. com huggingface transformers. It consists of using the functions convert_tokens_to_ids and convert_ids_to_tokens in respectively numericalize and textify. Parameters model_type roberta pretrained_model_name roberta base model_type bert pretrained_model_name bert base uncased Python cpu vars cpu vars gpu vars defining our model architecture transformer_model model_class. com huggingface transformers https github. Thereby you will have to implement a custom split for each different model architecture. This time instead of using LSTMs they all use a more powerful architecture based on the Transformer cf. com r url https 3A 2F 2Farxiv. Most of the models require special tokens placed at the beginning and end of the sequences. As we are not using an RNN we have to limit the sequence length to the model input size. Notice we are passing the include_bos False and include_eos False options. One way to access them is to create a custom model. Fortunately the tokenizer class from transformers provides the correct pre process tools that correspond to each pre trained model. postDistributed sk 119c3e5d748b2827af3ea863faae6376 I made another version available on my GitHub https github. As a result besides significantly outperforming many state of the art tasks it allowed with only 100 labeled examples to match performances equivalent to models trained on 100 more data. As a result without even tunning the parameters you can obtain rapidly state of the art results. Like the ULMFiT method we will use Slanted Triangular Learning Rates Discriminate Learning Rate and gradually unfreeze the model. In order to switch easily between classes each related to a specific model type HuggingFace provides a dictionary that allows loading the correct classes by just specifying the correct model type name. I hope you enjoyed this first article and found it useful. In our case we are only interested in accessing the logits. 156062 An intermittently pleasing but mostly routine effort 156063 An 156064 intermittently pleasing but mostly routine effort 156065 intermittently pleasing but mostly routine 156066 intermittently pleasing but 156067 intermittently pleasing 156068 intermittently 156069 pleasing 156070 but 156071 mostly routine 156072 mostly 156073 routine 156074 effort 156075. html TokenizeProcessor takes the tokenizer argument as a Tokenizer object. As we will see later fastai manages it automatically during the creation of the DataBunch. It worth noting that the integration of HuggingFace transformers with fastai has already been demonstrated in Keita Kurita s article A Tutorial to Fine Tuning BERT with Fast AI https mlexplained. Attention is all you need https arxiv. html bertforsequenceclassification for the model class BertTokenizer https huggingface. It aims to make you understand where to look and modify both libraries to make them work together. html The data block API for flexibility. For example if you want to use the Bert architecture for text classification you would use BertForSequenceClassification https huggingface. Now you can predict examples with Export LearnerIn order to export and load the learner you can do these operations As mentioned here https docs. So you just have to install transformers with The current versions of the fastai torch and transformers libraries are respectively 1. More precisely I try to make the minimum amount of modifications in both libraries while making them compatible with the maximum amount of transformer architectures. Note that in addition to this NoteBook and the Medium article https medium. Integrating transformers with fastai for multiclass classificationBefore beginning the implementation note that integrating transformers and fastai can be done in multiple different ways. And one from our dataset 3133 PATRON I think I am searching wrong. md installation and here https github. com c sentiment analysis on movie reviews overview. PATRON thank you STUDENT Hi Sorry for the wait. There was someone at the desk I had to help quickly. layer 6 learner. It is worth noting that in this case we use the transformers library only for a multi class text classification task. Check batch and tokenizer Check batch and numericalizer Custom modelAs mentioned here https github. You can like decribed in the Dev Sharma s article https medium. Because of that I think that pre trained transformers architectures will be integrated soon to future versions of fastai. com analytics vidhya using roberta with fastai for nlp 7ed3fed21f6c Sep 2019 linear algebra data processing CSV file I O e. This optimizer matches Pytorch Adam optimizer Api therefore it becomes straightforward to integrate it within fastai. 0 does not produce any errors The example taskThe given task is a multi class text classification on Movie Reviews https www. com 2019 05 13 a tutorial to fine tuning bert with fast ai which makes pytorch_pretrained_bert library compatible with fastai. Learner Custom Optimizer Custom MetricIn pytorch transformers HuggingFace implemented two specific optimizers BertAdam and OpenAIAdam that have been replaced by a single AdamW optimizer. This is because fastai adds its own special tokens by default which interferes with the CLS and SEP tokens added by our custom tokenizer. Although these articles are of high quality not all of them are currently compatible with the last version of transformers and even the last one which this implementation is based off of in a large part is deprecated. Thanks for reading and don t hesitate in leaving questions or suggestions. For example if we use the DilBERT model we can observe the architecture by calling print learner. ai and Sebastian Ruder introduced the Universal Language Model Fine tuning for Text Classification https medium. The point here is to allow anyone expert or non expert to easily achieve state of the art results and to make NLP cool again. A configuration class to load store the configuration of the given model. Therefore using these tools does not guarantee better results. To do so just follow the instructions here https github. Custom TokenizerThis part can be a little bit confusing because a lot of classes are wrapped around each other with similar names. We then unfreeze the second group of layers and repeat the operation. Likely it allows you to use Slanted Triangular Learning Rates Discriminate Learning Rate and even Gradual Unfreezing. In the fastai library data pre processing is done automatically during the creation of the DataBunch. As you will see in the DataBunch API the tokenizer and numericalizer are passed in the processor argument under the following format processor TokenizeProcessor tokenizer tokenizer. To use our one_cycle we will need an optimum learning rate. 70059 which leads us to the 5th position on the leaderboard ConclusionIn this NoteBook I explain how to combine the transformers library with the beloved fastai library. Main transformers classesIn transformers each model architecture is associated with 3 main classes A model class to load store a particular pre trained model. from_pretrained pretrained_model_name num_labels 5 Show graph of learner stats and metrics after each epoch. For that reason I decided to create simple solutions that are generic and flexible. It is worth noting that for reproducing BertAdam specific behavior you have to set correct_bias False. As specified in Keita Kurita s article https mlexplained. transformers This is the one we ll be using although it s configured to take any of them. postDistributed sk 119c3e5d748b2827af3ea863faae6376. We will pick a value a bit before the minimum where the loss still improves. Below you can find the dossier of each pre process requirement for the 5 model types used in this tutorial. Setting Up the Tokenizer retreive the list of tokens and create a Vocab object. NB The functions __getstate__ and __setstate__ allow us to correctly use export and load_learner functions. We evaluate the outputs of the model on classification accuracy. Note here that we use slice to create separate learning rate for each group. To do so we have to first tokenize and then numericalize the texts correctly. Since the introduction of ULMFiT Transfer Learning has become very popular in NLP and even Google BERT Transformer XL XLNet Facebook RoBERTa XLM and OpenAI GPT GPT 2 have begun to pre train their own models on very large corpora. layer 7 learner. The first time I heard about ULMFiT was listening and following along to a fast. layer 4 learner. Fortunately HuggingFace https huggingface. html NumericalizeProcessor takes as vocab argument a Vocab object https docs. Let me take a look STUDENT Ok I have the Guide page pulled up. com fastai fastai blob master README. pooler create a link to download the dataframe which was saved with. This allows for rapid generation of visualizations. html Tokenizer takes the tok_func argument as a BaseTokenizer object. co transformers model_doc bert. layer 5 learner. For that reason this tutorial integrates only the transformer architectures that have a model for sequence classification implemented. Discriminative Fine tuning and Gradual unfreezing Optional To use discriminative layer training and gradual unfreezing fastai provides one tool that allows to split the structure model into groups. html pretrained models. layer 3 learner. layer 10 learner. These model types are BERT from Google XLNet from Google CMU XLM from Facebook RoBERTa from Facebook DistilBERT from HuggingFace Util functionFunction to set the seed for generating random numbers. ULMFiT was the first Transfer Learning method applied to NLP. For each text movie review the model has to predict a label for the sentiment. bert CLS tokens SEP padding roberta CLS prefix_space tokens SEP padding distilbert CLS tokens SEP padding xlm CLS tokens SEP padding xlnet padding CLS tokens SEP It is worth noting that we don t add padding in this part of the implementation. Setting up the DatabunchFor DataBunch creation you need to set the processor argument as our new custom processor transformer_processor and manage the padding correctly. ", "id": "chrisb304/fine-grained-sentiment-analysis-with-transformer", "size": "21558", "language": "python", "html_url": "https://www.kaggle.com/code/chrisb304/fine-grained-sentiment-analysis-with-transformer", "git_url": "https://www.kaggle.com/code/chrisb304/fine-grained-sentiment-analysis-with-transformer", "script": "torch.optim fastai.text __init__ fastai.tabular TransformersBaseTokenizer(BaseTokenizer) TransformersVocab(Vocab) * #pretrained fastai tokenizer will cooperate with our transformer tokenizer. seed_all BertForSequenceClassification DistilBertConfig #This is the one we'll be using XLMForSequenceClassification * #This allows for rapid generation of visualizations. BertConfig XLNetForSequenceClassification PretrainedConfig RobertaConfig tokenizer XLNetConfig AdamW Path although it's configured to take any of them. forward transformers __getstate__ XLMConfig numpy pathlib PreTrainedTokenizer CustomTransformerModel(nn.Module) DistilBertTokenizer DistilBertForSequenceClassification XLMTokenizer XLNetTokenizer textify __setstate__ functools IPython.display PreTrainedModel RobertaTokenizer pandas fastai.callbacks partial get_preds_as_nparray numericalize fastai create_download_link __call__ BertTokenizer RobertaForSequenceClassification HTML ", "entities": "(('One way', 'custom model'), 'be') (('encoding methods', 'True'), 'call') (('Most', 'sequences'), 'require') (('TrainNow we', 'model'), 'use') (('implementation', 'tokenizers trained optimizers'), 'give') (('them', 'transformer architectures'), 'try') (('pre processing', 'DataBunch'), 'do') (('We', 'classification accuracy'), 'evaluate') (('I', 'fast'), 'listen') (('we', '0'), 'submit') (('PATRON', 'Hi STUDENT wait'), 'thank') (('that', 'model type just correct name'), 'in') (('isn t', 'significantly phrase'), 'edu') (('BERT RoBERTa 2 CTRL', 'etc'), 'know') (('configuration class', 'given model'), 'store') (('they', 'Transformer cf'), 'use') (('layer', 'first groups'), 'freeze') (('we', 'print learner'), 'observe') (('html TokenizeProcessor', 'Tokenizer object'), 'take') (('Next we', 'learning maximum rate'), 'use') (('that', 'numericalize functions'), 'create') (('integrating', 'multiple different ways'), 'integrate') (('it', 'right'), 'mention') (('we', 'fastai numericalizer'), 'suggest') (('later classes', 'from_pretrained pretrained_model_name'), 'see') (('I', 'GitHub https available github'), 'sk') (('I', 'Slanted Triangular Learning even transformers'), 'decide') (('model types', 'random numbers'), 'be') (('therefore you', 'left'), 'pad') (('fastai fastai pretrained tokenizer', 'transformer tokenizer'), 'cooperate') (('it', '100 more data'), 'as') (('we', 'test dataset'), 'want') (('ULMFit method', 'code much experience'), 'demonstrate') (('it', 'first article'), 'hope') (('link', 'articles'), 'take') (('models', 'powerful them'), 'include') (('taskThe', 'class text Movie Reviews https multi www'), 'produce') (('You', 'HuggingFace documentation https huggingface'), 'find') (('still us', 'pretrained model'), 'allow') (('parameter pretrained_model_name', 'e.'), 'be') (('seed_all seed Data pre processingTo we', 'specific format'), 'pre') (('we', 'them'), 'read_csv') (('you', 'Slanted Triangular Learning Rates Discriminate Learning Rate'), 'allow') (('transformers library', 'Discriminate Learning Rate Gradual Unfreezing'), 'be') (('we', 'labels'), 'adapt') (('we', 'implementation'), 'roberta') (('I', 'anything'), 'seem') (('Keita Kurita article', 'Fast AI https'), 'mlexplaine') (('model classesIn architecture', 'particular pre trained model'), 'associate') (('you', 'correct_bias False'), 'be') (('little bit lot', 'similar names'), 'be') (('This', 'visualizations'), 'allow') (('year transformers', 'essential NLP'), 'become') (('therefore it', 'fastai'), 'match') (('where loss', 'bit minimum'), 'pick') (('library', 'fastai'), 'vidhya') (('article Medium Fastai', 'Transformers'), 'BERT') (('I', 'dataset'), 'one') (('sequence classification', 'model'), 'integrate') (('Jeremy early 2018 Howard', 'fast'), 'BERT') (('Setting', 'Vocab object'), 'retreive') (('I', 'food articles'), 'be') (('where dataset', 'entire conversations'), 'dataset') (('Check Check batch batch', 'numericalizer https modelAs here github'), 'mention') (('pre trained model', 'train pre pre part'), 'be') (('that', 'AdamW single optimizer'), 'implement') (('you', 'good resources'), 'student') (('NoteBook I', 'fastai beloved library'), '70059') (('we', 'custom processor'), 'create') (('Guide page', 'look'), 'let') (('He', 'Discriminate Learning Rate Gradual Unfreezing'), 'explain') (('intermittently pleasing 156067', 'intermittently 156068'), 'effort') (('you', 'fastai libraries'), 'library') (('so we', 'first then texts'), 'have') (('html Tokenizer', 'BaseTokenizer object'), 'take') (('_ _ getstate _ _ _ setstate _ us', 'correctly export functions'), 'allow') (('we', 'data block API https docs'), 'be') (('that', 'groups'), 'tuning') (('STUDENT good Good luck', 'science_reference byu'), 'gotcha') (('you', 'Keita article https mlexplained'), 'modify') (('you', 'https here docs'), 'predict') (('models', 'input string'), 'require') (('it', 'them'), 'transformer') (('transformers trained architectures', 'fastai'), 'think') (('shortcut names', 'transformers documentation'), 'find') (('first how we', 'TokenizeProcessor function'), 'let') (('that', 'simple solutions'), 'for') (('Thereby you', 'model different architecture'), 'have') (('forward method', 'model'), 'com') (('which', 'model type'), 'implement') (('I', 'desk'), 'be') (('we', 'include_bos False False options'), 'notice') (('i', 'somewhere else more articles'), 'try') (('ai', 'Text Classification https medium'), 'introduce') (('you', 'art results'), 'as') (('which', 'SEP custom tokenizer'), 'be') (('implementation', 'large part'), 'be') (('here we', 'group'), 'note') (('It', 'respectively numericalize'), 'consist') (('NLP', 'art results'), 'be') (('ULMFiT', 'Transfer Learning first NLP'), 'be') (('we', 'class text classification only multi task'), 'be') (('that', 'tokenizer new function'), 'create') (('we', '1'), 'notice') (('Therefore using', 'better results'), 'guarantee') (('that', 'pre trained model'), 'provide') (('that', 'tokens'), 'implement') (('model', 'sentiment'), 'have') (('we', 'function'), 'have') (('integration', 'Fast AI https'), 'worth') (('You', 'article https medium'), 'decribe') (('transformers libraries', 'fastai torch'), 'have') (('dossier', 'tutorial'), 'find') (('pytorch_pretrained_bert library', 'fastai'), 'com') (('which', 'dataframe'), 'create') (('we', 'sequence model input size'), 'use') (('it', 'given model'), 'class') (('Dissatisfied Frustrated 1 2 3 4 BeyondThe data', 'pandas'), 'be') (('you', 'BertForSequenceClassification https huggingface'), 'use') (('them', 'where libraries'), 'aim') (('that', 'convenient way'), 'be') (('Transformers', 'tokens'), 'seem') (('tokenizer', 'format processor TokenizeProcessor tokenizer following tokenizer'), 'pass') (('Thanks', 'don questions'), 'hesitate') (('html NumericalizeProcessor', 'vocab Vocab object https docs'), 'take') (('movie individual rather phrases', 'sentiment each assigned label'), 'be') (('we', 'only logits'), 'be') (('you', 'padding'), 'set') (('which', 'lr_find'), 'find') (('We', 'operation'), 'unfreeze') (('we', 'learning optimum rate'), 'need') (('me', 'interesting documentation'), 'let') (('instruction', 'fastai documentation'), 'describe') (('fastai libraries', 'Kaggle'), 'instal') (('we', 'gradually model'), 'use') (('Google BERT Transformer XL XLNet RoBERTa even XLM', 'very large corpora'), 'become') (('he', 'art results'), 'use') (('custom classes', 'first load_learner'), 'have') (('you', 'correct order'), 'com') (('Parameters roberta base model_type bert model_type roberta bert pretrained_model_name base', 'model architecture'), 'uncase') (('later fastai', 'DataBunch'), 'manage') ", "extra": "['test']"}