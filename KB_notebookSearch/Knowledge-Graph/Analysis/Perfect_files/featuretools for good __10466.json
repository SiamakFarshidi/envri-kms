{"name": "featuretools for good ", "full_name": " h1 Featuretools for Good h2 Automated Feature Engineering h3 Data Preprocessing h2 Missing Values h2 Domain Knowledge Feature Construction h3 Remove Squared Variables h2 Remove Highly Correlated Columns h1 Establish Correct Variable Types h1 EntitySet and Entities h1 Normalize Household Table h3 Table Relationships h1 Deep Feature Synthesis h1 Feature Selection h3 Training and Testing Data h1 Correlations with the target h2 Subset to Relevant Data h3 Labels for Training h2 Custom Evaluation Metric for LightGBM h1 Modeling with Gradient Boosting Machine h2 Feature Importances h1 Custom Primitive h3 Range Primitive h3 Correlation Primitive h1 More Featuretools h1 Post Processing Function h2 Results After Post Processing h1 Increase number of features h2 Remove Zero Importance Features h1 Add in Divide Primitive h2 Increase to 1500 features h2 Go to 2000 h1 Try Modeling with more folds h1 5000 Features h3 5000 features with 10 fold modeling h1 Comparison of Models h1 Save Data h1 Conclusions ", "stargazers_count": 0, "forks_count": 0, "description": "The cells below remove any columns that aren t in the data these may have been removed due to correlation. I m not running the GBM with a random seed so the same set of features can produce different cross validation results. Data Preprocessing These steps are laid out in the kernel A Complete Introduction and Walkthrough https www. I think we should be able to add more features as long as we continue to impose feature selection. However why go to the trouble if Featuretools can do that for us That one call alone gave us 147 features to train a model This was only using the default primitives as well. The objective of this data science for good problem is to predict the poverty of households in Costa Rica. Below we convert the Boolean variables to the correct type. Post Processing FunctionThere are a number of steps after generating the feature matrix so let s put all of these in a function. Remove any duplicated columns. Increase to 1500 features1000 is clearly not enough Most of these features are highly correlated but we can still find useful features as evidenced by the feature importances. The gradient boosting machine seems very good at cutting through the swath of features. com getting_started install. Featuretools for GoodIn this notebook we will implement automated feature engineering with Featuretools https docs. Training and Testing Data Correlations with the targetFeaturetools has built features with moderate correlations with the Target. We need to make sure the length of the labels matches the length of the training dataset. __ The tool is incredibly simple to use and delivers considerable value creating features that we never would have imagined. This library is extremely easy to get started with and very powerful as the score from this kernel illustrates. Then we convert the float variables. Featuretools uses the table relationships to aggregate features as required. For anyone new to featuretools check out the documentation https docs. These are created because some of transform primitives might have affected the Target. Subset to Relevant Data Labels for TrainingWe ll now get into modeling. This is a little tedious but also necessary. Save DataWe can save the final selected featuretools feature matrix created with a maximum of 2000 features. Eventually we re probably going to be overfitting to the training data but the we can address that through regularization and feature selection. Individual Variables these are characteristics of each individual rather than the household Boolean Yes or No 0 or 1 Ordered Discrete Integers with an ordering2. The cross validation accuracy continues to increase as we add features. Feature SelectionWe can do some rudimentary feature selection removing one of any pair of columns with a correlation greater than 0. com minute quick start for the Costa Rican Household Poverty Challenge. The new table is derived from the data table and we need to bring along any of the household level variables. The three columns not in the above lists are Id Idhogar and Target. A random seed would ensure consistent results but may have a singificant effect on the predictions. html or an introductory blog post here. Finally the same with the ordinal variables. Using feature primitives Deep Feature Synthesis can build hundreds or 1000s as we will later see of features from the relationships between tables and the columns in tables themselves. To prevent featuretools from building the exact same features we already have we can add drop_exact and pass in the feature names as strings using the get_name functionality. Try Modeling with more foldsAs a final model we ll increase the number of folds to 10 and see if this results in more stable predictions across folds. These shows the predictions on an individual not household level we set all individuals to 4 if they did not have a head of household. This relationship links the two tables and allows us to create deep features by aggregating individuals in each household. Remove Highly Correlated Columns Establish Correct Variable TypesWe need to specify the correct variables types 1. At first we ll limit the features to 1000. EntitySet and EntitiesAn EntitySet in Featuretools holds all of the tables and the relationships between them. Table RelationshipsNormalizing the entity automatically adds in the relationship between the parent household and the child ind. I look forward to seeing what the community can come up with for this problem. We can use more primitives or write our own to build more features. Feature ImportancesThe utility function below plots feature importances and can show us how many features are needed for a certain cumulative level of importance. Go to 2000This is getting ridiculous. Household variables Boolean Yes or No Ordered Discrete Integers with an ordering Continuous numericBelow we manually define the variables in each category. Although these correlations only show linear relationships they can still provide an approximation of what features will be useful to a machine learning model. Remove columns with a missing percentage above the missing_threshold 4. Since these are the same for all members of a household we can directly add these as columns in the household table using additional_variables. My focus is now going to shift to modeling but I encourage anyone to keep adjusting the featuretools implementation. Modeling with Gradient Boosting MachineThe hyperparameters used here _have not been optimized_. ConclusionsFeaturetools certainly can make our job easier for this problem Adding features continues to improve the validation score with mixed effects on the public leaderboard. Manual feature engineering is limited both by human creativity and time constraints but automated methods have no such constraints. All that s left is to model The cell below runs the gradient boosting machine model and saves the results. Defining a custom evaluation metric for Light GBM is not exactly straightforward but we can manage. Deep Feature SynthesisHere is where Featuretools gets to work. The next step is to optimize the model for these features. We ll call the first table data since it contains all the information both at the individual level and at the household level. At the moment Featuretools is the only open source Python library available for automated feature engineering. In this case the instances are households. This will be used for Bayesian optimization of model hyperparameters. com automated feature engineering in python 99baf11cc219 We ll read in the data and join the training and testing set together. Missing Values Domain Knowledge Feature Construction Remove Squared VariablesThe gradient boosting machine does not need the squared version of variables it if already has the original variables. This is meant only as a first pass at modeling with these features. The distribution is close to what we observe in the training labels which are provided on the household level. We ll write a simple function that finds the range of a numeric column. Only one way to find out through data Let s look at the performance of models so far. There still might be additional gains to increasing the number of features and or using different custom primitives. There are two types of primitives which are operations applied to data Transforms applied to one or more columns in a _single table_ of data Aggregations applied across _multiple tables_ using the relationships between tablesWe generate the features by calling ft. At the moment we only have a single table but we can create multiple tables through normalization. The gradient boosting machine implemented in LightGBM usually does well Custom Evaluation Metric for LightGBMThis is the F1 Macro score used by the competition. All of the variable types have already been confirmed. Correlation Primitive More FeaturetoolsWhy stop with 150 features Let s add in a few more primitives and start creating more. Custom PrimitiveTo expand the capabilities of featuretools we can write our own primitives to be applied to the data. For example it will automatically aggregate the individual level data at the household level. They involve correcting missing values creating a few features that Featuretools can build on top of. com willkoehrsen a complete introduction and walkthrough. Most of these features are aggregations we could have made ourselves. It s concerning that there is so much variation between folds but that is going to happen with a small imbalanced testing set. We should also make sure the len of test_ids the idhogar of the testing households is the same as the length of the testing dataset. This build features using any of the applicable primitives for each column in the data. __Featuretools should be a default part of your data science workflow. We need to remove any columns containing derivations of the Target. Extract the training and testing data along with labels and ids needed for making submissions Results After Post Processing Increase number of features Remove Zero Importance Features Add in Divide PrimitiveNext we ll add a divide transform primitive into the deep feature synthesis call. Normalize Household TableNormalization allows us to create another table with one unique row per instance. https towardsdatascience. Remove one out of every pair of columns with a correlation threshold above the correlation_threshold 6. 5000 Features 5000 features with 10 fold modeling Comparison of ModelsAt this point we might honestly ask if there is any benefit to increasing the number of features. Replace infinite values with np. Range PrimitiveWe can also make a custom primitive that calculates the correlation coefficient between two columns. Automated Feature EngineeringAutomated feature engineering should be a _default_ part of your data science workflow. The index of the household table is idhogar which uniquely identifies each household. Remove columns with only a single unique value. To start with we use the default agg and trans primitives in a call to ft. ", "id": "willkoehrsen/featuretools-for-good", "size": "10466", "language": "python", "html_url": "https://www.kaggle.com/code/willkoehrsen/featuretools-for-good", "git_url": "https://www.kaggle.com/code/willkoehrsen/featuretools-for-good", "script": "sklearn.metrics display make_scorer lightgbm s_corr_calc post_process macro_f1_score range_calc collections seaborn numpy most to least p_corr_calc plot_feature_importances sklearn.model_selection f1_score model_gbm make_agg_primitive matplotlib.pyplot featuretools.variable_types pandas Counter featuretools StratifiedKFold featuretools.primitives IPython.display ", "entities": "(('This', 'model hyperparameters'), 'use') (('it', 'household level'), 'call') (('feature Manual engineering', 'time automated such constraints'), 'limit') (('Adding features', 'public leaderboard'), 'make') (('Featuretools', 'top'), 'involve') (('s', 'more'), 'stop') (('these', 'Ordered Discrete ordering2'), 'be') (('Data', 'Complete Introduction'), 'lay') (('s', 'results'), 'be') (('that', 'two columns'), 'make') (('already we', 'get_name functionality'), 'prevent') (('it', 'household level'), 'aggregate') (('boosting gradient machine', 'already original variables'), 'miss') (('Data Training Correlations', 'Target'), 'build') (('that', 'testing small imbalanced set'), 's') (('extremely very score', 'kernel'), 'be') (('com', 'complete introduction'), 'willkoehrsen') (('s', 'models'), 'way') (('length', 'training dataset'), 'need') (('Subset', 'now modeling'), 'get') (('we', 'data'), 'expand') (('which', 'uniquely household'), 'be') (('this', 'folds'), 'try') (('us', 'instance'), 'allow') (('objective', 'Costa Rica'), 'be') (('we', 'category'), 'variable') (('we', 'additional_variables'), 'add') (('we', 'household level variables'), 'derive') (('Save DataWe', 'features'), 'save') (('which', 'household level'), 'be') (('we', 'themselves'), 'build') (('us', 'household'), 'link') (('_', 'ft'), 'be') (('_ _ Featuretools', 'data science default workflow'), 'be') (('how many features', 'importance'), 'feature') (('we', 'that'), '_') (('three columns', 'above lists'), 'be') (('some', 'Target'), 'create') (('We', 'Target'), 'need') (('EntitySet EntitySet', 'them'), 'hold') (('point we', 'features'), 'feature') (('We', 'more features'), 'use') (('same set', 'cross validation different results'), 'run') (('we', 'ft'), 'use') (('community', 'problem'), 'look') (('Remove', 'variables correct types'), 'need') (('these', 'correlation'), 'remove') (('as long we', 'feature selection'), 'think') (('we', 'features'), 'continue') (('idhogar', 'testing dataset'), 'make') (('that', 'numeric column'), 'write') (('99baf11cc219 We', 'training'), 'com') (('we', 'ourselves'), 'be') (('Automated Feature EngineeringAutomated feature engineering', '_ _ data science workflow'), 'be') (('notebook we', 'Featuretools https docs'), 'featuretool') (('instances', 'case'), 'be') (('This', 'features'), 'mean') (('Custom Evaluation usually well Metric', 'F1 Macro competition'), 'do') (('This', 'data'), 'build') (('highly we', 'feature importances'), 'be') (('Table', 'parent household'), 'add') (('This', 'default only primitives'), 'go') (('next step', 'features'), 'be') (('so s', 'function'), 'be') (('Below we', 'correct type'), 'convert') (('Featuretools', 'aggregate features'), 'use') (('boosting gradient machine', 'features'), 'seem') (('exactly we', 'Light GBM'), 'be') (('Feature SelectionWe', 'greater 0'), 'do') (('features', 'machine learning model'), 'provide') (('All', 'variable types'), 'confirm') (('now I', 'featuretools implementation'), 'go') (('they', 'household'), 'show') (('we', 'feature synthesis primitive deep call'), 'extract') (('we', 'regularization selection'), 'go') (('random seed', 'predictions'), 'ensure') (('we', 'normalization'), 'have') (('Featuretools', 'Python feature only open available automated engineering'), 'be') (('we', '1000'), 'limit') ", "extra": "['test']"}