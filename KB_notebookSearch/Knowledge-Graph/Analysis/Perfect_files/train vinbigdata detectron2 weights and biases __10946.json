{"name": "train vinbigdata detectron2 weights and biases ", "full_name": " h1 Detectron 2 Training and weights and biases integration h1 Author Sourabh Chauhan h2 Detectron 2 Training with weights and biases monitoring h1 Table of Contents h1 Dataset preparation h1 Installation h1 Training method implementations h2 Data preparation h1 Customizing detectron2 trainer h2 Mapper for augmentation h2 Evaluator h2 Loss evaluation hook h2 LR scheduling h1 Loading Data h1 Data Visualization h1 Training h1 Visualize loss curve competition metric AP40 ", "stargazers_count": 0, "forks_count": 0, "description": "Ref detectron2 docs Dataloader https detectron2. pop annotations None dataset_dict. 3 RandomContrast p 0. This is the inverse of data loading logic in datasets coco. This can be done by adding Hook which calculates the loss to the trainer. com c vinbigdata chest xray abnormalities detection discussion 207955. read_csv imgdir test_meta. dimension of precision TxRxKxAxM IoU dimension of recall TxKxAxM stats 2 _summarize 1 iouThr. roi_batch_size_per_image 128 faster and good enough for this toy dataset default 512 eval_period 20 aug_kwargs HorizontalFlip p 0. inspired from Detectron https github. In practice loading all the taining image arrays are too heavy to be loaded on memory so these are loaded inside DataLoader on demand This is done by mapper class in detectron2 as I will expain later. pop sem_seg_file_name None return dataset_dict h w it will be modified by code below aug_input T. best most iou find the proposal box that covers the best covered gt box record the iou coverage of this gt box mark the proposal box and the gt box as used append recorded iou coverage level thresholds torch. Jump directly to weights and biases integration cell training Make sure to add wandb_api key to your notebook using Add ons. These models are summarized in MODEL_ZOO. You just need to wrap above training scripts by main method and use launch method provided by detectron2. You can change which one to use by setting cfg. overflew 9 hour limit of GPU time on kaggle sessions UPDATE 2021 2 16 Training 1024 1024 on R101 backbone UPDATE 2021 2 12 Training on the full size image instead of 256 256 Table of Contents Dataset preparation dataset Installation installation Training method implementations train_method Customizing detectron2 trainer custom_trainer Advanced topic skip it first time Mapper for augmentation mapper Evaluator evaluator Loss evaluation hook loss_hook Loading Data load_data Data Visualization data_vis Training training Visualize loss curve competition metric AP40 vis_loss Visualization of augmentation by Mapper vis_aug Next step next_step Dataset preparationPreprocessing x ray image format dicom into normal png image format is already done by xhlulu in the below discussion Multiple preprocessed datasets 256 512 1024px PNG and JPG modified and original ratio https www. put_scalars validation_loss mean_loss is called to put this validation loss to the storage which will be saved to metrics. 15 rotate_limit 10 p 0. This leads to a different definition of small medium large. Here I modified COCOEvaluator implementation to calculate AP with IoU 0. This is because model s evaluation is done in model. html I implemented MyMapper which uses augmentations implemented in detectron2 and AlbumentationsMapper which uses albumentations library augmentations. This dataset_dicts contains the metadata for actual data fed into the neural network. It is nice to start with however I want to customize the training behavior more to improve the model s performance. NOTE this config means the number of classes but a few popular unofficial tutorials incorrect uses num_classes 1 here. Resize 800 800 it will be modified by code below if not self. png Detectron 2 Training with weights and biases monitoring UPDATE 2021 2 21 Training 512 512 on R101 backbone 3 000 iterations comparison with the laptop output v16 UPDATE 2021 2 20 Training 1024 1024 on R101 backbone 10 000 iterations training anchor box change things UPDATE 2021 2 20 Training 1024 1024 on R101 backbone 18 000 iterations UPDATE 2021 2 16 Training 1024 1024 on R101 backbone 30 000 iterations did not run. 95 but we need AP with IoU 0. We can make own Trainer class MyTrainer here for this purpose and override methods to provide customized behavior. com facebookresearch detectron2 blob master configs COCO Detection faster_rcnn_R_50_FPN_3x. Famouns dataset s evaluator is already implemented in detectron2. Please refer official example train_net. 5 to be consistent with the annotation format. It is loaded beforehand of the training on memory so it should contain all the metadata image filepath etc to construct training dataset but should not contain heavy data. width width of the image. height height of the image. LR_SCHEDULER_NAME as you can see from the docs. Data VisualizationIt s also very easy to visualize prepared training dataset with detectron2. put_scalars timetest 11 from detectron2. read_csv datadir sample_submission. plotly models setup GPU check Installing Detectron2 For debug. augmentations aug_input image aug_input. Therefore we subtract 0. I insert LossEvalHook before evalutor to work well. is_train dataset_dict. com apofeniaco training on detectron2 with a validation set and plot loss on it to avoid overfitting 6449418fbf4e Now all the preparation has done MyTrainer overwraps build_evaluator method of DefaultTrainer provided by detectron2 to support validation dataset evaluation. We can analyze plot them to check how the training proceeded. json jsonl format during training. image_id id of the image index is used here. Loss evaluation hookWe implemented Evaluator and now we can calculate competition metric however validation loss is not calculated inside Evaluator. However our predictions are floating point coordinates. image category_id np. html build_lr_scheduler supports only 2 types of LR scheduling WarmupMultiStepLR default WarmupCosineLR. 4 Use the COCO default keypoint OKS sigmas unless overrides are specified COCOAPI requires every detection and every gt to have keypoints so we just take the first entry from both Copying inference_on_dataset from evaluator. csv To get number of data. We can follow installation instruction https github. com drive 16jcaJoc6bCFAQ96jDe2HwtXj7BMD_ m5 scrollTo QHnVupBBn9eR Colab Notebook or version 7 of this kernel https www. Here I will just use the dataset VinBigData Chest X ray Resized PNG 256x256 https www. 40 and replaced to show this value instead of AP with IoU 0. Training method implementationsBasically we don t need to implement neural network part detectron2 already implements famous architectures and provides its pre trained weights. array obj category_id for obj in dataset_dict annotations dtype np. We remove the bbox field to let mask AP use mask area. Copy so the caller can do whatever with results unmap the category ids for COCO cocoapi does not handle empty results very well Saving generated box proposals to file. com facebookresearch detectron2 blob master tools train_net. 3 optical distrotion not applied here ElasticTransform alpha 120 sigma 120 0. Load 1 image to get image size. I will demonstrate these augmentations later so you can skip reading the code and please just jump to next. float32 compute recall for each iou threshold ar 2 np. 5 ShiftScaleRotate scale_limit 0. Inference from the notebook should be combined with 2 Class filter to obtain a incerement of public leaderboard metric of about 0. This annotator does not find anything skip. com corochann vinbigdata detectron2 train This notebook includes a Weights and biases integration of the Detectron2 models to follow hyperparameters of the datasets. py L222 L252 noqa precision has dims iou recall cls area range max dets area range index 0 all area ranges max dets index 1 typically 100 per image tabulate it use RLE to encode the masks because they are too large and takes memory since this evaluator stores outputs of the entire dataset counts is an array encoded by mask_util as a byte stream. HACKING overwrite iouThrs to calc ious 0. io en latest _modules detectron2 solver build. Note that current implementation is not efficient in the sense that Evaluator s evaluation and LossEvalHook s loss calculation run separately even if both need a model forward calculation for same validation data. evaluation import COCOEvaluator PascalVOCDetectionEvaluator return PascalVOCDetectionEvaluator dataset_name not working return COCOEvaluator dataset_name bbox False output_dir output_folder flags General Data config all_train or valid20 original or wbf Training config images per batch this corresponds to total batch size WarmupMultiStepLR default or WarmupCosineLR Overwrite by param_dict flags_dict debug True outdir results debug imgdir_name vinbigdata chest xray resized png 256x256 split_mode valid20 iter 100 debug small value should be set. md we need to know CUDA and pytorch version to install correct detectron2. com facebookresearch Detectron blob a6a835f5b8208c45d0dce217ce9bbda915f44df7 detectron datasets json_dataset_evaluator. Now the methods are ready. maxDets 2 Infering it from predictions should be better Test set json files do not contain annotations evaluation must be performed using the COCO evaluation server. AugInput image transforms self. com facebookresearch detectron2 blob master INSTALL. To calculate validation loss we need to call model with the training mode. com facebookresearch detectron2 blob master MODEL_ZOO. 5 RandomBrightnessContrast p 0. pop sem_seg_file_name None return dataset_dict h w Copyright c Facebook Inc. Below LossEvalHook calculates validation loss in _do_loss_eval method and self. Predicted box_proposals are in XYXY_ABS mode. the standard metrics Compute per category AP from https github. To define custom dataset we need to create list of dict dataset_dicts where each dict contains following file_name file name of the image. Here implemented VinbigdataEvaluator is constructed we can also use COCOEvaluator here. py L255 noqa Record max overlap value for each gt box Return vector of overlap values all small medium large 96 128 128 256 256 512 512 inf sort predictions in descending order TODO maybe remove this and make it explicit in the documentation guard against no boxes find which proposal box maximally covers each gt box and get the iou amount of coverage for each gt box find which gt box is best covered i. trapz recalls thresholds When evaluating mask AP if the results contain bbox cocoapi will use the box area as the area of the instance instead of the mask area. com xhlulu vinbigdata chest xray resized png 256x256 to skip the preprocessing and focus on modeling part. 95 1e 5 step dtype torch. eval mode and it outputs bounding box prediction but does not output loss. We can finetune these pre trained architectures. Thankfully utf 8 works out which is also what the pycocotools _mask. Visualize loss curve competition metric AP40As I explained the calculated metrics are saved in metrics. Mapper for augmentation Mapper class is used inside pytorch DataLoader. Python3 s json writer which always produces strings cannot serialize a bytestream unless you decode it. I will tune these parameters through the competition to improve model s performance. Detectron 2 Training and weights and biases integration Author Sourabh ChauhanGoals Integrate the Detectron2 models for computer vision with wandb api for hyperparameter optimization in VBD project. TrainingIt s actually very easy to use multiple gpus for training. Loading DataThis Flags class is to manage experiments. http file home sourabh kaggle xrays VBD chestxray hyper_param_06 03. com corochann vinbigdata detectron2 train scriptVersionId 51628272 for the simple usage of detectron2 how to train custom dataset. XYXY_ABS is used here meaning that absolute value of xmin ymin xmax ymax annotation is used in the bbox. Its content is saved to metric. yaml for this kernel. Please upvote the dataset as well Installationdetectron2 is not pre installed in this kaggle docker so let s install it. put_scalar validation_loss mean_loss return losses How loss is calculated on train_loop self. Notes This notebook uses corochan s Detectron2 implementation for VBD dataset http https www. 05 alpha_affine 120 0. It provides Visualizer class we can use it to draw an image with bounding box as following. EvaluatorTo evaluate validation dataset to calculate competition metric we need Evaluator. log validation loss local dict_mylaptop_val_loss x step_local dict_mylaptop_val_loss y 1. Trainer has attribute storage and calculated metrics are summarized. In this competition we need object detection model I will choose R50 FPN https github. 03 same problem with elastic transform RandomResizedCrop height 100 width 100 p 0. You can refer the Detectron2 Beginner s Tutorial https colab. Default build_lr_schduler method docs https detectron2. 1 args parse Read data kaggle input vinbigdata 1024 image dataset vinbigdata Read in the data CSV files alias sample_submission pd. category_id class label id for each bounding box with shape n_boxes get_vinbigdata_dicts is for train dataset preparation and get_vinbigdata_dicts_test is for test dataset preparation. Data preparation detectron2 provides high level API for training custom dataset. COCOEvaluator only calculates AP with IoU from 0. Ref Training on Detectron2 with a Validation set and plot loss on it to avoid overfitting https medium. 5 RandomGamma p 0. record image_id index objs record annotations objs utils configs T. print row print row class_name class_name row class_name It is No finding Use this No finding class with the bbox covering all image area. main scripts starts from here. io en latest tutorials data_loading. int64 Remove unnecessary field. It is responsible for converting dataset_dicts into actual data fed into the neural network and we can insert augmentation process in this Mapper class. build_hooks This method defines how to construct hooks. In COCO annotations keypoints coordinates are pixel indices. Customizing detectron2 trainer This section is advanced I recommend to jump to Training scripts section for the first time of reading. pass aug_kwargs to cfg Let training initialize from model zoo pick a good LR Small value Frequent save need a lot of storage. build_train_loader build_test_loader These class methods deine how to construct DataLoader for training data validation data respectively. The output will be in the following directory kaggle working results v23 model_final. build_evaluator This class method defines how to construct Evaluator. json files can be directly used for inference. 3 OpticalDistortion distort_limit 2 shift_limit 0. For example many kinds of AP Average Precision is calculted in COCOEvaluator. LR schedulingTo further customize learning rate scheduling you may override build_lr_scheduler class method to construct any pytorch LRScheduler. DefaultTrainer is used in the example which provides the starting point to train your model with custom dataset. Here AlbumentationMapper is passed to construct DataLoader to insert customized augmentation process. annotation This is the ground truth annotation data for object detection which contains following bbox bounding box pixel location with shape n_boxes 4 bbox_mode BoxMode. ", "id": "sourabhchauhan/train-vinbigdata-detectron2-weights-and-biases", "size": "10946", "language": "python", "html_url": "https://www.kaggle.com/code/sourabhchauhan/train-vinbigdata-detectron2-weights-and-biases", "git_url": "https://www.kaggle.com/code/sourabhchauhan/train-vinbigdata-detectron2-weights-and-biases", "script": "albumentations LossEvalHook(HookBase) build_detection_test_loader COCOEvaluator DatasetCatalog UserSecretsClient BoxMode detectron2.utils.visualizer detection_utils detectron2.evaluation.evaluator _derive_coco_results collections pycocotools.cocoeval plotly.graph_objs field get_vinbigdata_dicts plotly.express build_hooks _summarizeDets typing sklearn.model_selection log_every_n_seconds _tasks_from_predictions KFold _do_loss_eval detectron2.evaluation launch save_yaml Union distutils.util create_small_table detectron2.data __call__ MetadataCatalog convert_to_coco_json get_vinbigdata_dicts_test detectron2.utils.logger COCOeval detectron2.data.datasets.coco reset detectron2.structures dataclasses load_yaml seaborn numpy detectron2.config tabulate detectron2.utils.comm detectron2 _evaluate_box_proposals CfgNode as CN tools IPython.core.display model_zoo tqdm.notebook _get_loss pandas build_test_loader HookBase MyMapper get_cfg VinbigdataEvaluator(DatasetEvaluator) detectron2.utils.file_io build_evaluator update detectron2.evaluation.fast_eval_api AlbumentationsMapper plotly.figure_factory HTML preprocessing pycocotools.mask display pairwise_iou plotly.offline Visualizer detectron2.data.transforms OrderedDict _summarize Flags build_detection_train_loader _evaluate_predictions_on_coco MyTrainer(DefaultTrainer) PascalVOCDetectionEvaluator plotly.io detection_utils as utils build_train_loader setup_logger scipy detectron2.engine.hooks after_step __init__ dataclass COCOeval_opt PathManager lightgbm _eval_box_proposals Optional DefaultTrainer Any Path Dict Boxes pathlib evaluate detectron2.engine detectron2.config.config _summarizeKps plotly COCO sklearn instances_to_coco_json matplotlib.pyplot vin_summarize DatasetEvaluator DefaultPredictor _eval_predictions strtobool CfgNode tqdm subplots catboost xgboost pycocotools.coco process kaggle_secrets ", "entities": "(('we', 'also COCOEvaluator'), 'implement') (('many kinds', 'COCOEvaluator'), 'calculte') (('later you', 'just next'), 'demonstrate') (('3 OpticalDistortion', '2 shift_limit'), 'distort_limit') (('TrainingIt', 'training'), 's') (('I', 'performance'), 'tune') (('args parse data kaggle input image dataset 1 1024 Read', 'sample_submission pd'), 'Read') (('dataset_dicts', 'neural network'), 'contain') (('mask AP', 'mask area'), 'remove') (('which', 'bbox_mode n_boxes 4 BoxMode'), 'annotation') (('models', 'MODEL_ZOO'), 'summarize') (('category ids', 'box very well generated proposals'), 'copy') (('you', 'it'), 'serialize') (('we', 'Mapper class'), 'be') (('t', 'pre trained weights'), 'method') (('We', 'trained architectures'), 'finetune') (('we', 'box'), 'provide') (('which', 'albumentations library augmentations'), 'implement') (('This', 'datasets coco'), 'be') (('Frequent save', 'storage'), 'pass') (('that', 'gt iou coverage level thresholds used append recorded torch'), 'find') (('competition we', 'Evaluator'), 'need') (('detectron2 solver', '_ io latest modules'), 'build') (('We', 'override customized behavior'), 'make') (('Data preparation detectron2', 'custom dataset'), 'provide') (('print row print row class_name class_name row It', 'image area'), 'class_name') (('json files', 'directly inference'), 'use') (('which', 'custom dataset'), 'use') (('TxKxAxM 2 _', '1 iouThr'), 'stat') (('30 000 iterations', 'R101 2021 2 16 Training 1024 1024 backbone'), 'train') (('get_vinbigdata_dicts_test', 'dataset test preparation'), 'd') (('it', 'loss'), 'output') (('it', 'heavy data'), 'load') (('I', 'evalutor'), 'insert') (('calculated metrics', 'metrics'), 'metric') (('COCOEvaluator', '0'), 'calculate') (('I', 'R50 FPN https github'), 'need') (('This', 'small medium large'), 'lead') (('Mapper class', 'pytorch DataLoader'), 'use') (('I', 'reading'), 'customize') (('Inference', 'about 0'), 'combine') (('Famouns evaluator', 'already detectron2'), 'implement') (('record index objs record annotations', 'utils configs'), 'image_id') (('ymin xmax ymax annotation', 'bbox'), 'use') (('com xhlulu vinbigdata chest xray', 'part'), 'resize') (('evaluator stores outputs', 'byte stream'), 'dim') (('LossEvalHook', '_ do_loss_eval method'), 'calculate') (('dataset_dict it', 'aug_input T.'), 'sem_seg_file_name') (('I', 'detectron2'), 'be') (('You', 'detectron2'), 'need') (('output', 'directory kaggle working following results'), 'be') (('You', 'Tutorial https Detectron2 colab'), 'refer') (('method', 'how hooks'), 'define') (('Now preparation', 'validation dataset evaluation'), 'train') (('800 800 it', 'code'), 'Resize') (('However predictions', 'point coordinates'), 'float') (('annotations evaluation', 'COCO evaluation server'), 'maxdet') (('tutorials a few popular unofficial incorrect', 'num_classes'), 'mean') (('1024px 256 512 PNG', 'below discussion'), 'UPDATE') (('models plotly GPU', 'Installing debug'), 'check') (('Here I', 'IoU'), 'modify') (('LR learning schedulingTo further customize you', 'pytorch'), 'rate') (('you', 'docs'), 'LR_SCHEDULER_NAME') (('We', 'installation instruction https github'), 'follow') (('bbox cocoapi', 'instance'), 'recalls') (('gt box', 'gt box find'), 'overlap') (('how training', 'them'), 'analyze') (('split_mode valid20 iter 100 debug small value', 'png'), 'return') (('however I', 'more performance'), 'want') (('md we', 'pytorch correct detectron2'), 'need') (('we', 'training mode'), 'need') (('loss separately even both', 'validation same data'), 'note') (('notebook', 'VBD dataset http https www'), 'Notes') (('put_scalar validation_loss mean_loss return How loss', 'train_loop self'), 'loss') (('class build_train_loader build_test_loader methods', 'data validation data'), 'deine') (('we', 'evaluator'), 'Use') (('where dict', 'image'), 'define') (('calculated metrics', 'attribute storage'), 'have') (('com corochann vinbigdata detectron2 notebook', 'datasets'), 'train') (('html build_lr_scheduler', 'scheduling WarmupMultiStepLR default LR WarmupCosineLR'), 'support') (('Here I', 'just dataset'), 'use') (('so s', 'it'), 'upvote') (('Predicted box_proposals', 'XYXY_ABS mode'), 'be') (('Here AlbumentationMapper', 'augmentation customized process'), 'pass') (('d', 'image index'), 'image_id') (('validation however loss', 'Evaluator'), 'evaluation') (('which', 'metrics'), 'call') (('Data VisualizationIt', 'detectron2'), 's') (('evaluation', 'model'), 'be') (('You', 'cfg'), 'change') (('com Detectron blob detectron', 'json_dataset_evaluator'), 'facebookresearch') (('which', 'trainer'), 'do') (('Loading', 'experiments'), 'be') (('class method', 'how Evaluator'), 'define') (('Detectron 2 Training', 'VBD project'), 'Integrate') ", "extra": "['annotation', 'test']"}