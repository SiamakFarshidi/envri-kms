{"name": "introduction to ensembling stacking in python ", "full_name": " h1 Introduction h1 Feature Exploration Engineering and Cleaning h2 Visualisations h1 Ensembling Stacking models h3 Helpers via Python Classes h3 Out of Fold Predictions h1 Generating our Base First Level Models h1 Second Level Predictions from the First level Output h3 Second level learning model via XGBoost h3 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "Create a new feature Title containing the titles of passenger names Group all non common titles into one single grouping Rare Mapping Sex Mapping titles Mapping Embarked Mapping Fare Mapping Age Feature selection Some useful parameters which will come in handy later on for reproducibility set folds for out of fold prediction Class to extend the Sklearn classifier Class to extend XGboost classifer Put in our parameters for said classifiers Random Forest parameters max_features 0. These models can all be conveniently invoked via the Sklearn library and are listed as follows 1. In the section of code below we essentially write a class SklearnHelper that allows one to extend the inbuilt methods such as train predict and fit common to all the Sklearn classifiers. feature_importances_. Having now concatenated and joined both the first level train and test predictions as x_train and x_test we can now fit a second level learning model. Some additional steps that may be taken to improve one s score could be 1. Beware if set to too high a number might run the risk of overfitting. com kaggle ensembling guide Load in our libraries Going to use these 5 base models for the stacking Load in the train and test datasets Store our passenger ID for easy access Some features of my own that I have added in Gives the length of the name Feature that tells whether a passenger had a cabin on the Titanic Feature engineering steps taken from Sina Create new feature FamilySize as a combination of SibSp and Parch Create new feature IsAlone from FamilySize Remove all NULLS in the Embarked column Remove all NULLS in the Fare column and create a new feature CategoricalFare Create a New feature CategoricalAge Define function to extract titles from passenger names If the title exists extract and return it. Introduce a greater variety of base models for learning. Here are two most correlated features are that of Family size and Parch Parents and Children. values xaxis dict title Pop ticklen 5 zeroline False gridwidth 2 Create the new column containing the average of values axis 1 computes the mean row wise xaxis dict title Pop ticklen 5 zeroline False gridwidth 2 Generate Submission File. Creating a Stacking ensemble Helpers via Python ClassesHere we invoke the use of Python s classes to help make it more convenient for us. Pairplots Finally let us generate some pairplots to observe the distribution of data from one feature to the other. Extra Trees classifier 3. This is good from a point of view of feeding these features into your learning model because this means that there isn t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Once again we use Seaborn to help us. As per the Sklearn documentation most of the classifiers are built in with an attribute which returns feature importances by simply typing in. Furthermore since having mentioned about Objects and classes within the OOP framework let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier. The Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. 2 Support Vector Classifier parameters Create 5 objects that represent our 4 models Create Numpy arrays of train test and target Survived dataframes to feed into our models Creates an array of the train data Creats an array of the test data Create our OOF train and test predictions. Beware if set to too high a number would run the risk of overfitting as one would be growing the tree too deep verbose Controls whether you want to output any text during the learning process. Second level learning model via XGBoostHere we choose the eXtremely famous library for boosted tree learning model XGBoost. Support Vector Machine Parameters Just a quick summary of the parameters that we will be listing here for completeness n_jobs Number of cores used for the training process. My other standalone Kaggle script 2 which implements exactly the same ensembling steps albeit with different parameters discussed below gives a Public LB score of 0. For further information about the algorithm check out the official documentation 1. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as suchSo I have not yet figured out how to assign and store the feature importances outright. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho Visualisations Pearson Correlation Heatmap let us generate some correlation plots of the features to see how related one feature is to the next. I ll still leave both features in for the purposes of this exercise. As per the code below we are therefore having as our new columns the first level predictions from our earlier classifiers and we train the next classifier on this. com mmueller allstate claims severity stacking starter run 390867 2 https www. Having prepared our first layer base models as such we can now ready the training and test test data for input into our classifiers by generating NumPy arrays out of their original dataframes as follows Output of the First level Predictions We now feed the training and test data into our 5 base classifiers and use the Out of Fold prediction function we defined earlier to generate our first level predictions. gamma minimum loss reduction required to make a further partition on a leaf node of the tree. However one cannot simply train the base models on the full training data generate predictions on the full test set and then output these for the second level training. io en latest Anyways we call an XGBClassifier and fit it to the first level train and target data and use the learned model to predict the test data as follows Just a quick run down of the XGBoost parameters used in the model max_depth How deep you want to grow your tree. It was built to optimize large scale boosted tree algorithms. Allow a handful of minutes for the chunk of code below to run. Bear with me for those who already know this but for people who have not created classes or objects in Python before let me explain what the code given above does. Anyways please feel free to leave me any comments with regards to how I can improve 1 https www. AdaBoost classifer 4. Well it is no surprise that our task is to somehow extract the information out of the categorical variables Feature Engineering Here credit must be extended to Sina s very comprehensive and well thought out notebook for the feature engineering ideas so please check out his work Titanic Best Working Classfier 1 by Sina 1 https www. Implementing a good cross validation strategy in training the models to find optimal parameter values 2. There you will find that there are a whole host of other useful parameters that you can play around with. values xaxis dict title Pop ticklen 5 zeroline False gridwidth 2 Scatter plot size feature_dataframe AdaBoost feature importances. 808 which is good enough to get to the top 9 and runs just under 4 minutes. In creating my base classifiers I will only use the models already present in the Sklearn library and therefore only extend the class for that. com sinakhorami titanic titanic best working classifierAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric a format suitable to feed into our Machine Learning models. If set to 1 all cores are used. I myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling stacking script that I managed to chance upon and study was one written in the AllState Severity Claims competition by the great Faron. Plotly Barplot of Average Feature Importances Having obtained the mean feature importance across all our classifiers we can plot them into a Plotly bar plot as follows Second Level Predictions from the First level Output First level output as new features Having now obtained our first level predictions one can think of it as essentially building a new set of features to be used as training data for the next classifier. Anyway please check out his script here Stacking Starter 1 by Faron Now onto the notebook at hand and I hope that it manages to do justice and convey the concept of ensembling in an intuitive and concise manner. Essentially we have created a wrapper class to extend the various Sklearn classifiers so that this should help us reduce having to write the same code over and over when we implement multiple learners to our stacker. Please check out the full description via the official Sklearn website. In a nutshell stacking uses as a first level base the predictions of a few basic classifiers and then uses another model at the second level to predict the output from the earlier first level predictions. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration. Out of Fold PredictionsNow as alluded to above in the introductory section stacking uses predictions of base classifiers as input for training to a second level model. Therefore I ll print out the values from the code above and then simply copy and paste into Python lists as below sorry for the lousy hack Create a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package. The more uncorrelated the results the better the final score. Creating NumPy arrays out of our train and test sets Great. Interactive feature importances via Plotly scatterplots I ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers via a plotly scatter plot by calling Scatter as follows Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe. Till next time Peace Out 1 http mlwave. ConclusionI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. eta step size shrinkage used in each boosting step to prevent overfitting Producing the Submission file Finally having trained and fit all our first level and second level models we can now output the predictions into the proper format for submission to the Titanic competition as follows Steps for Further Improvement As a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. Correlation Heatmap of the Second Level Training set There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores. 2 Extra Trees Parameters max_features 0. com arthurtok titanic simple stacking with xgboost 0 808 Feature Exploration Engineering and Cleaning Now we will proceed much like how most kernels in general are structured and that is to first explore the data on hand identify possible feature engineering opportunities as well as numerically encode any categorical features. Again credit must be extended to Faron and Sina. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. The rest of the code are simply methods of the class which simply call the corresponding methods already existing within the sklearn classifiers. Therefore I am pretty sure there is a lot of room to improve and add on to that script. IntroductionThis notebook is a very basic and simple introductory primer to the method of ensembling combining base learning models in particular the variant of ensembling known as Stacking. Gradient Boosting classifer 5. 1 https xgboost. 5 AdaBoost parameters Gradient Boosting parameters max_features 0. Ensembling Stacking modelsFinally after that brief whirlwind detour with regards to feature engineering and formatting we finally arrive at the meat and gist of the this notebook. To do so we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows Takeaway from the Plots One thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. The material in this notebook borrows heavily from Faron s script although ported to factor in ensembles of classifiers whilst his was ensembles of regressors. n_estimators Number of classification trees in your learning model set to 10 per default max_depth Maximum depth of tree or how much a node should be expanded. Therefore this cuts out redundancy as won t need to write the same methods five times if we wanted to invoke five different classifiers. Generating our Base First Level Models So now let us prepare five learning models as our first level classification. This means that when you want to create an object classifier you have to give it the parameters of clf what sklearn classifier you want seed random seed and params parameters for the classifiers. In short a class helps to extend some code program for creating objects variables for old school peeps as well as to implement functions and methods specific to that class. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void. For other excellent material on stacking or ensembling in general refer to the de facto Must read article on the website MLWave Kaggle Ensembling Guide 1. For any newcomers to programming one normally hears Classes being used in conjunction with Object Oriented Programming OOP. The larger the more conservative the algorithm will be. Feature importances generated from the different classifiers Now having learned our the first level classifiers we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code. These base results will be used as new features Extra Trees Random Forest AdaBoost Gradient Boost Support Vector Classifier Create a dataframe with features Scatter plot size feature_dataframe AdaBoost feature importances. This runs the risk of your base model predictions already having seen the test set and therefore overfitting when feeding these predictions. Random Forest classifier 2. def init Python standard for invoking the default constructor for the class. ", "id": "arthurtok/introduction-to-ensembling-stacking-in-python", "size": "12873", "language": "python", "html_url": "https://www.kaggle.com/code/arthurtok/introduction-to-ensembling-stacking-in-python", "git_url": "https://www.kaggle.com/code/arthurtok/introduction-to-ensembling-stacking-in-python", "script": "SklearnHelper(object) __init__ plotly.offline predict AdaBoostClassifier train seaborn numpy plotly.graph_objs get_oof sklearn.ensemble KFold plotly.tools matplotlib.pyplot pandas (RandomForestClassifier fit SVC sklearn.cross_validation get_title sklearn.svm feature_importances xgboost ", "entities": "(('title', 'it'), 'go') (('com mmueller allstate claims severity', 'https starter 390867 2 www'), 'run') (('dict title mean row wise xaxis Pop', 'Generate Submission 5 zeroline False gridwidth 2 File'), 'ticklen') (('study', 'great Faron'), 'be') (('we', 'training process'), 'Parameters') (('Again credit', 'Faron'), 'extend') (('notebook', 'Stacking'), 'be') (('five times we', 'five different classifiers'), 'cut') (('more one', 'better scores'), 'set') (('feature', 'unique information'), 'be') (('you', 'learning process'), 'run') (('values', 'xaxis dict title Scatter plot 5 zeroline False gridwidth 2 size'), 'ticklen') (('material', 'regressors'), 'borrow') (('This', 'therefore when predictions'), 'run') (('I', 'exercise'), 'leave') (('we', 'level earlier first predictions'), 'feed') (('It', 'tree large scale boosted algorithms'), 'build') (('which', 'feature importances'), 'build') (('you', 'that'), 'find') (('Therefore I', 'Plotly package'), 'print') (('notebook', 'learning models'), 'be') (('models', '1'), 'invoke') (('Now us', 'feature importance dataframe'), 'importance') (('we', 'tree learning boosted model'), 'model') (('I', 'feature yet how importances'), 'invoke') (('code', 'what'), 'bear') (('how related one feature', 'next'), 'let') (('that', 'one s score'), 'be') (('that', 'OOF train predictions'), 'create') (('which', 'enough top 9'), '808') (('features', 'Machine Learning now suitable models'), 'com') (('which', 'sklearn already classifiers'), 'be') (('we', 'Helper Sklearn Class'), 'let') (('So now us', 'level first classification'), 'generate') (('one', 'Sklearn common classifiers'), 'write') (('how much node', 'max_depth Maximum tree'), 'set') (('over when we', 'stacker'), 'create') (('we', 'notebook'), 'arrive') (('notebook', 'void'), 'seem') (('notebook', 'Titanic Best Working https 1 Sina 1 www'), 'be') (('The', 'more results'), 'uncorrelate') (('Therefore I', 'script'), 'be') (('value', 'iteration'), 'suppresse') (('it', 'intuitive manner'), 'check') (('Extra Random Forest AdaBoost Gradient Boost Support Vector Classifier', 'feature_dataframe AdaBoost feature importances'), 'use') (('2 which', '0'), 'give') (('how I', 'https 1 www'), 'feel') (('However one', 'level second training'), 'train') (('steps', 'ensemble stacker'), 'output') (('general that', 'as well numerically categorical features'), 'simple') (('Finally us', 'other'), 'let') (('Here two most correlated features', 'Family size'), 'be') (('How deep you', 'tree'), 'late') (('that', 'code'), 'generate') (('many newcomers', 'Kaggle'), 'be') (('one', 'next classifier'), 'obtain') (('we', 'learning now second level model'), 'fit') (('classifiers Random said Forest', 'parameters max_features'), 'create') (('it', 'more us'), 'invoke') (('too high number', 'overfitting'), 'run') (('Pearson Correlation plot', 'too many strongly one'), 'utilise') (('as well which', 'more than 2 levels'), 'hear') (('seed random seed', 'classifiers'), 'mean') (('we', 'this'), 'have') (('normally Classes', 'Object Oriented Programming OOP'), 'hear') (('I', 'that'), 'use') ", "extra": "['test']"}