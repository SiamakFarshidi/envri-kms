{"name": "complete beginner your first titanic submission ", "full_name": " h3 Scoring and challenges h3 Steps to complete this tutorial on your own computer h2 1 Process the data h3 Load data h3 Prepare the data to be read by our algorithm h1 2 Create and fit the decision tree h3 Visualize default tree optional h1 3 Make Predictions h1 4 Create csv to upload to Kaggle h1 5 Submit file to Kaggle h1 Challenges h3 Level 1 First time on Kaggle h3 Level 2 Submitted to Kaggle before h3 Level 3 Some familiarity with scikit learn ", "stargazers_count": 0, "forks_count": 0, "description": "Process the data Load dataOur data has the following columns PassengerId Each passenger s id Survived Whether the passenger survived or not 1 yes 0 no Pclass The passenger class 1st class 1 2nd class 2 third class 3 Sex Each passenger s sex Age Each passenger s age Prepare the data to be read by our algorithmLet s look at the first 3 corresponding target variables. I have intentionally left lots of room for improvement regarding the model used currently a simple decision tree classifier. Another way to access the parameters is to place your cursor in between the parentheses and then press shift tab. Submit file to KaggleGo to the submission section https www. Congratulations you re on the leaderboard ChallengesThe default decision tree gives a score of. Navigate to the directory where you have this notebook and the type the following command. Scoring and challenges If you simply run the code below your score will be fairly poor. Compare this model to a decision tree with depth 3. Display first 3 target variables Create classifier object with default hyperparameters Fit our classifier using the training features and the training target values Create decision tree. If you are on a Mac use Command. At the bottom of the tutorial are challenges which if you follow them will significantly improve your score. com getting set up in jupyter notebooks using anaconda to install the jupyter pandas sklearn etc if you are confused 4. To complete this visualization section you must be going through the code on your computer. Remove each below to run the two lines below. But if you would like to run the code locally on your own computer you can follow the steps below. com jlawman Meetup blob master 11. Our classifier will use this to know what the output should be for each of the training instances. For example you could reduce the maximum depth of the tree to 3 by setting max_depth 3 with the following command clf DecisionTreeClassifier max_depth 3 To change multiple hyperparameters seperate out the parameters with a comma. Download Titanic dataset https www. DecisionTreeClassifier for decision tree classifiers. This tutorial walks you through submitting a. the first passenger 22 year old male did not survive but the second passenger 38 year old female did survive. DecisionTreeClassifier. When you get to the challenge stage you can return here and tune hyperparameters in this cell. org follow my installation tutorial http joshlawman. Create a DataFrame with the passengers ids and our prediction regarding whether they survived or not Visualize the first 5 rows Convert DataFrame to a csv file that can be uploaded This is saved in the same directory as your notebook. Download this kernel as a notebook https github. metrics train_test_split function to see which give you the best score Level 3c Can you take the list from challenge 3b and then have the best models in the list vote on how each prediction should be made Load data Drop features we are not going to use Look at the first 3 rows of our training data Convert male female to 1 0 so that our decision tree can be built Fill in missing age values with 0 presuming they are a baby if they do not have a listed age Select feature column names and target variable we are going to use for training Look at the first 3 rows we have over 800 total rows of our training data. Try to improve the prediction by using the challenge prompts which are suitable to your level. org stable modules generated sklearn. Run every cell in the notebook except the optional visualization cells. Place that folder in the same directory as your notebook. com jlawman Meetup click Clone or Download then unzip the file and pull out the notebook you want. Level 2b Can you visualize your data using matplotlib or seaborn to glean other insights of how to improve your predictions Level 3 Some familiarity with scikit learnLevel 3a Can you use GridSearchCV from sklearn. Drag your file from the directory which contains your code and make your submission. Visualize default tree optional This is not a necessary step but it shows you how complex the tree is when you don t restrict it. dot file Remove each below to uncomment the two lines and export the file. ensemble import RandomForestClassifier clf RandomForestClassifier Level 2 Submitted to Kaggle beforeLevel 2a Can you include other features that were dropped to improve your score Don t forget to deal with any missing data. To zoom out press control. Install Jupyter Notebooks https jupyter. Create a Kaggle account https www. Place both files in a folder named input. For example to change the learning rate and minimum samples per leaf and the maximum depth fill in the parentheses with the following clf DecisionTreeClassifier max_depth 3 min_samples_leaf 2 The other parameters are listed below. com c titanic submit of the Titanic competition. model_selection on the Random Forest Classifier to tune the hyperparameters and improve your score Level 3b Can you train a list of models and then evaluate each one using sklearn. This is input which our classifier will use as an input. You can also access the list of parameters by reading the documentation http scikit learn. csv file of predictions to Kaggle for the first time. dot Tpng titanic_tree. If you are new to GitHub go the repository folder https github. ipynb with empty cells from my GitHub. Submit CSV containing the predictions. This is the measure of whether the passenger survived or not i. 70813 placing you at rank 8 070 out of 8 767. Note if you want to generate a new tree png you need to open terminal or command prompt after running the cell above. Display decision tree Blue on a node or leaf means the tree thinks the person did not survive Orange on a node or leaf means that tree thinks that the person did survive In Chrome to zoom in press control. 7 20Meetup 20 20Decision 20Trees Submit 20your 20first 20Kaggle 20prediction 20 20Titanic 20Dataset. Steps to complete this tutorial on your own computer The kernel below can be run in the browser. Can you improve it Level 1 First time on KaggleLevel 1a Can you try to give the tree a max depth to improve your score Level 1b Can you import a different tree models such as the Random Forest Classifier to see how it affects your score Use the following code line to create it. The idea of this tutorial is to get you started and have you make the decisions of how to improve your score. Create and fit the decision treeThis tree is definitely going to overfit our data. Create csv to upload to Kaggle 5. Make predictions using the features from the test data set Display our predictions they are either 0 or 1 for each training instance depending on whether our algorithm believes the person survived or not. ", "id": "jlawman/complete-beginner-your-first-titanic-submission", "size": "5803", "language": "python", "html_url": "https://www.kaggle.com/code/jlawman/complete-beginner-your-first-titanic-submission", "git_url": "https://www.kaggle.com/code/jlawman/complete-beginner-your-first-titanic-submission", "script": "DecisionTreeClassifier display IPython.core.display export_graphviz sklearn.tree Image pandas ", "entities": "(('data', 'target first 3 corresponding variables'), 'process') (('tutorial', 'submitting a.'), 'walk') (('you', 'cell'), 'note') (('t', 'it'), 'optional') (('70813', '8 070 8 767'), 'place') (('you', 'repository folder https github'), 'go') (('I', 'decision tree currently simple classifier'), 'leave') (('where you', 'following command'), 'navigate') (('dot file', 'file'), 'remove') (('you', 'cell'), 'return') (('you', 'computer'), 'complete') (('you', 'sklearn'), '2b') (('how it', 'it'), 'improve') (('kernel', 'below browser'), 'step') (('we', 'training data'), 'train_test_split') (('way', 'parentheses'), 'be') (('you', 'how score'), 'be') (('person', 'training instance'), 'make') (('tree', 'definitely data'), 'create') (('This', 'notebook'), 'save') (('output', 'training instances'), 'use') (('which', 'submission'), 'drag') (('you', 'score'), 'be') (('3 2 other parameters', 'clf following DecisionTreeClassifier'), 'change') (('you', 'significantly score'), 'be') (('then one', 'sklearn'), 'model_selection') (('which', 'level'), 'try') (('you', 'notebook'), 'click') (('documentation http scikit', 'parameters'), 'access') (('classifier', 'input'), 'be') (('person', 'press control'), 'mean') (('you', 'etc'), 'sklearn') (('you', 'score'), 'give') (('you', 'comma'), 'reduce') (('org', 'joshlawman'), 'follow') (('you', 'steps'), 'follow') (('score Don t', 'missing data'), 'submit') ", "extra": "['test']"}