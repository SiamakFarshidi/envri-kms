{"name": "ner using crf ", "full_name": " h1 NER USING CRF h2 Exploring Visualizing our data h2 Modeling the Data h3 Random Forest Classifier h3 Conditional Random Fields classifier ", "stargazers_count": 0, "forks_count": 0, "description": "However the precision and recall metrics of the classes individually have improved but not much. metrics import flat_classification_report The dataset does not have any header currently. The IOB Tagging system contains tags of the form B CHUNK_TYPE for the word in the Beginning chunk I CHUNK_TYPE for words Inside the chunk O Outside any chunkThe IOB tags are further classified into the following classes geo Geographical Entity org Organization per Person gpe Geopolitical Entity tim Time indicator art Artifact eve Event nat Natural Phenomenon Penn Treebank tagset https www. eu tagsets penn treebank tagset text The 20English 20Penn 20Treebank 20tagset Sketch 20Engine 20 earlier 20version. Classification reports are used to obtain the values of these metrics in a text format per class. Since the problem statement is a simple classification problem we will start with a simple tree based model Random Forest using a simple feature map. Conditional Random Fields classifierA Conditional Random Field CRF is a standard model for predicting the most likely sequence of labels that correspond to a sequence of inputs. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session Data analysis Data visualisation from sklearn_crfsuite import CRF scorers metrics import sklearn_crfsuite from sklearn_crfsuite import scorers from sklearn_crfsuite import metrics from sklearn_crfsuite. It seems the features which require the model to take proper decisions are missing. Some models like decision trees and neural networks are often be able to get 100 accuracy on the training data but perform much worse on new data. In short we try to provide a sequence of features to the model for each word the sequence containing POS tags capitalisations type of word title etc. This can be broken down into two sub tasks identifying the boundaries of the NE and identifying its type. It is a supervised learning method which has been proven to be better than the tree based models when it comes to NER. Simple tree based models have been proven to provide decent performance in building NERC systems. We can either work on this model alone by improving the features or ensembling it with a more contextual model or use a different model altogether. Random Forest ClassifierWe will use 5 fold cross validation as an input parameter to the classifier i. The context information behind each word needs to be fed to the model as well so that the predictions are more accurate. Modeling the Data With the basic EDA done and understanding the dataset we can move to the modeling stage. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. Recall TP TP FN F1 score F1 Score is the weighted average of Precision and Recall. Compared to the Random Forest classifier the CRF classifier did better as the scores have improved. Quite surprising most of the words are tagged as outside of any chunk. we will divide the dataset into 5 subsets and train test on them. Lets find the number of words in the dataset Lets visualize how the sentences are distributed by their length Lets find out the longest sentence length in the dataset Words tagged as B org Words tagged as I org Words tagged as B per Words tagged as I per Words tagged as B geo Words tagged as I geo Words tagged as I geo Words distribution across Tags Words distribution across Tags without O tag Words distribution across POS Simple feature map to feed arrays into the classifier. In order to use CRF we will enhance the feature set and create more features which can be used by the model to predict the tags correctly. Lets check the dataset again without the O tags. Also we add new features such as upper lower digit title etc. True Positives TP These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. These words can be considered as fillers and their presence might impact the classifier performance as well. True Negatives TN These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. False Positives FP When actual class is no and predicted class is yes. Precision TP TP FP Recall Sensitivity Recall is the ratio of correctly predicted positive observations to the all observations in actual class yes. The model is basically memorizing words and tags which will not suffice. uk csutton publications crftut fnt. Maybe the model is again remembering words and not taking into the context information completely. pdf Future Scope we can do the hyperparameter tuning This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. com shoumikgoswami ner using random forest and crf Libraries Exploring Visualizing our dataBefore going further we will try to understand what the dataset is all about and what all the features mean. This is important in order to understand how the classifiers will perform and help us interpret the results. Therefore this score takes both false positives and false negatives into account. We can use the first row as a header as it has the relevant headings. The dataset has the following columns or features Index Index numbers for each word Numeric type Sentence The number of sentences in the dataset We will find the number of sentences below Numeric type Word The words in the sentence Character type POS Parts Of Speech tags these are tags given to the type of words as per the Penn TreeBank Tagset Categorical type Tag The tags given to each word based on the IOB tagging system described above Target variable Categorical type so this is dataset is for learning purpose there we are having a same length of sentenceNow that we know the words and sentences lets try to understand what sort of words each tag contains. We will try tuning the model manually to see if we can improve it. Performance metricsBefore we move to the modeling part it is important to understand the performance metrics on the basis of which the models will be evaluated. read_csv Input data files are available in the read only. We divide the dataset into train and test sets Lets see how the input array looks like Random Forest classifier Lets check the performance Feature set Creating the train and test set Creating the CRF model We predcit using the same 5 fold cross validation Lets evaluate the mode Tuning the parameters manually setting c1 10. It is important that the classifier has proper features fed in to improve the performance. reference link https www. So our dataset mostly contains words related to geopolitical entities geographical locations and person names. Whereas a discrete classifier predicts a label for a single sample without considering neighboring samples a CRF can take context into account e. Since we need to take into account the context as well we create features which will provide consecutive POS tags for each word. NER USING CRFThe goal of a named entity recognition NER system is to identify all textual mentions of the named entities. the linear chain CRF which is popular in natural language processing predicts sequences of labels for sequences of input samples. Study material CRF http homepages. Since we are dealing with Information Extraction we will use the following metrics to evaluate the models Precision Recall F1 scoreThe metrics mentioned above are calculated using True False positives and True False negatives respectively. We will make the first row as the heading remove the first row and re index the dataset so we are basically having only those rows where sentence column is not null A class to retrieve the sentences from the dataset ths is how a sentence will look like. Random Forest being one of the most popular tree based models can learn the underlying rules according to which terms are tagged. Therefore we will train on one subset and test on the other and repeat for every subset so that the classifier classifies correctly on average and the performance estimate is not overly optimisticAlthough we have a good average score the model performed quite badly. The precision and recall values of most of the classes were 0. Precision Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. Named entity recognition is a task that is well suited to the type of classifier based approach. for each word and also consider the consecutive words in the list. False Negatives FN When actual class is yes but predicted class in no. This will help us in understanding what each tag type and sub type represents. It is essential that the model is evaluated by these metrics per class to make sure we have a good model. In particular a tagger can be built that labels each word in a sentence using the IOB format where chunks are labelled by their appropriate type. It is the harmonic mean of the both Precision and RecallF1 Score 2 Recall Precision Recall Precision For a decent classifier we would prefer high precision and recall values. ", "id": "bavalpreet26/ner-using-crf", "size": "7688", "language": "python", "html_url": "https://www.kaggle.com/code/bavalpreet26/ner-using-crf", "git_url": "https://www.kaggle.com/code/bavalpreet26/ner-using-crf", "script": "sklearn.metrics __init__ cross_val_score scorers chain itertools seaborn numpy sent2labels flat_classification_report cross_val_predict sklearn.ensemble sklearn.model_selection metrics RandomForestClassifier feature_map matplotlib.pyplot pandas classification_report getsentence(object) word2features sklearn_crfsuite.metrics GridSearchCV CRF RandomizedSearchCV sent2features sklearn_crfsuite make_scorer ", "entities": "(('which', 'basically words'), 'memorize') (('which', 'proper decisions'), 'seem') (('Random Forest ClassifierWe', 'classifier i.'), 'use') (('models', 'which'), 'move') (('better scores', 'Random Forest classifier'), 'do') (('features', 'all what'), 'com') (('We', 'different model'), 'work') (('model', 'good average score'), 'train') (('that', 'inputs'), 'be') (('Lets', 'O again tags'), 'check') (('CRF', 'account e.'), 'predict') (('when it', 'NER'), 'be') (('which', 'word'), 'create') (('NER goal', 'named entities'), 'USING') (('terms', 'which'), 'learn') (('us', 'results'), 'be') (('Precision TP TP FP Recall Sensitivity Recall', 'actual class'), 'be') (('which', 'tags'), 'enhance') (('we', 'modeling stage'), 'model') (('we', 'high precision values'), 'be') (('as well predictions', 'model'), 'need') (('we', 'train them'), 'divide') (('value', 'predicted class'), 'TN') (('we', 'good model'), 'be') (('Precision Precision', 'positive observations'), 'be') (('that', 'classifier based approach'), 'be') (('where chunks', 'appropriate type'), 'build') (('tag', 'words'), 'have') (('Negatives False When actual class', 'no'), 'fn') (('Simple tree based models', 'NERC systems'), 'prove') (('However precision metrics', 'classes'), 'improve') (('t', 'sklearn_crfsuite'), 'list') (('So dataset', 'geopolitical entities geographical locations'), 'contain') (('models', 'new data'), 'be') (('precision values', 'classes'), 'be') (('tag type', 'what'), 'help') (('This', 'type'), 'break') (('I', 'classifier'), 'find') (('manually we', 'it'), 'try') (('Maybe model', 'context information'), 'remember') (('O', 'Geopolitical tim Time indicator Artifact eve Natural Phenomenon Penn Treebank tagset https Entity art www'), 'contain') (('Quite surprising most', 'as outside chunk'), 'tag') (('classifier', 'performance'), 'be') (('Classification reports', 'class'), 'use') (('read_csv Input data files', 'read'), 'be') (('we', 'word title etc'), 'try') (('metrics import dataset', 'header'), 'flat_classification_report') (('We', 'manually c1'), 'divide') (('sentence', 'how'), 'make') (('it', 'relevant headings'), 'use') (('Precision Recall F1 scoreThe metrics', 'True False negatives'), 'use') (('classification simple we', 'Random feature simple map'), 'be') (('Therefore score', 'false account'), 'take') (('fillers', 'classifier performance'), 'consider') (('value', 'predicted class'), 'tp') (('Also we', 'digit title such upper lower etc'), 'add') (('which', 'input samples'), 'predict') (('Recall TP TP FN F1 score F1 Score', 'weighted Precision'), 'be') (('It', 'kaggle python Docker image https github'), 'Scope') ", "extra": "['organization', 'test']"}