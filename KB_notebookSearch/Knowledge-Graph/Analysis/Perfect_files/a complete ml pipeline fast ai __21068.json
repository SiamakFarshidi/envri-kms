{"name": "a complete ml pipeline fast ai ", "full_name": " h1 Project understanding h3 What exactly is the problem h3 How would a solution look like h3 What is known about the domain h1 Data understanding h3 What data do we have available h3 Is the data relevant to the problem h3 Is it valid Does it reflect our expectations h3 Is the data quality quantity recency sufficient h1 Data visualization h3 Plot some images with and without cancer tissue for comparison h3 How is the data best transformed for modeling h3 How may we increase the data quality h3 Preprocessing and augmentation h3 Compute image statistics h3 Plot some of the very bright or very dark images h1 Baseline model Fastai v1 h3 Prepare the data and split train h3 What kind of model architecture suits the problem best h3 Training h3 1cycle policy h2 Is the model learning h1 Finetuning the baseline model h1 Validation and analysis h3 How good does the model perform technically h3 How good is the model in terms of project requirements h3 Gradient weighted Class Activation Mapping Grad CAM h3 ROC curve and AUC h2 Remember AUC is the metric that is used for evaluating submissions We can calculate it here for ou validation set but it will most likely differ from the final score h1 Submit predictions h3 TTA h3 Submit the model for evaluation h2 We need to submit the tumor probability for each test sample in this competition The probability ranges from 0 to 1 h1 Deploy example ", "stargazers_count": 0, "forks_count": 0, "description": "Useful links for background knowledge Patch Camelyon PCam https github. 02391 This method produces a coarse localization map highlighting the areas that the model considers important for the classification decision. You may need to do some debugging. My initial results with 32 x 32px size showed worse performance than with 48 x 48px but I haven t done a search for optimal size. Visualizing the localization map would reveal that and we could focus on getting more diverse data of that breed. On a new machine We need to create an empty DataBunch and load it to a learner. Compute image statistics Do not use augmentation here Calculating statistics will give channel averages of 0. Next we will transform class probabilities to just tumor class probabilities If we wanted to get the predicted class argmax would get the index of the max get test id s from the sample_submission. We just need to subclass ImageList. However we will continue training further to improve from here. We find that there is at least one very dark and 6 very bright images. 5 times more negative images than positives. ai see commit version 9 of this kernel. Plot some images with and without cancer tissue for comparisonClassifying metastases is probably not an easy task for a trained pathologist and extremely difficult for an untrained eye. html TrainingWe define a convnet learner object where we set the model architecture and our data bunch. Going for a deeper model architecture will start overfitting faster. One key challenge is that the metastases can be as small as single cells in a large area of tissue. What I like about Fast. Large batch sizes may run out of GPU memory input size is the crop size this will extrat the model name as the model file name e. org Background CAMELYON17 background https camelyon17. As we have test time augmentation our results will probably improve if we do predictions multiple times per image and average out the results. I highly recommend the Fastai practical deep learning course for coders v3 https course. Note that the plot above may differ between runs as we use a random subset of data for computations. This function will convert image to the prediction format crop to center to the correct size and convert from 0 255 range to 0 1 range. According to Libre Pathology https librepathology. Prepare the data and split trainSplit train data to 90 training and 10 validation parts. As for the weight decay that is the L2 penalty of the optimizer Leslie proposes to select the largest one that will still let us train at a high learning rate so we do a small grid search with 1e 2 1e 4 and 1e 6 weight decays. py L114 top losses will return all validation losses and indexes sorted by the largest first Random Most incorrect or top losses Most correct or least losses hook into forward pass we hook into the convolutional part m 0 of the model We can create a utility function for getting a validation image with an activation map this gets the model create a batch from the one image convert batch tensor image to grayscale image with opencv attach hooks get convolutional activations and average from channels Grad CAM Then modify our plotting func a bit top losses will return all validation losses and indexes sorted by the largest first Random Most incorrect or top losses Most correct or least losses probs from log preds Compute ROC curve Compute ROC area make sure we have the best performing model stage loaded Fastai has a function for this but we don t want the additional augmentations it does our image loader has augmentations so we just use the get_preds We do a fair number of iterations to cover different combinations of flips and rotations. These are in the order of our test dataset and not necessarily in the same order as in sample_submission To know the id s we create a dict of id pred Now we can create a new list with the same order as in sample_submission Next a Pandas dataframe with id and label columns. Thus it would be a good idea to crop the images to the center region. How may we increase the data quality We could inspect if the data contains bad data too unfocused or corrupted and remove those to increase the overall quality. The idea is that we start with a low warm up learning rate and gradually increase it to high. org docs stable torchvision models. Then it slows back down near the end. html create_cnn 1cycle policyWe will use the one cycle policy proposed by Leslie Smith arXiv April 2018 https arxiv. The augmentations we can use for this type of data random rotation random crop random flip horizontal and vertical both random lighting random zoom not implemented here Gaussian blur not implemented here We will use OpenCV with image operations because in my experience OpenCV is a lot faster than PIL or scikit image. The losses are temporarily rising when max_lr drives the model out of local minima but this will pay off in the end when the learning rates are decreased. Plot some of the very bright or very dark imagesAll the dark and bright images are labeled negative. We want to maintain equal ratios of negative positive 60 40 in both training and test splits. The visual explanation gives transparency to the model making it easier to notice if it has learned the wrong things. for the other centers to assess the quality of the scan when in doubt a pathologist was consulted on whether scanning issues might affect diagnosis. resnet50 create dataframe for the fastai loader create test dataframe Subclass ImageList to use our own image opening function This ndarray image has to be converted to tensor before passing on as fastai Image we can use pil2tensor Create ImageDataBunch using fastai data block API Where to find the data How to split in train valid Where are the labels dataframe pointing to the test set We have our custom transformations implemented in the image loader but we could apply transformations also here Even though we don t apply transformations here we set two empty lists to tfms. The PCam dataset is derived from the Camelyon16 Challenge dataset https camelyon16. The most correctly labeled What our model is most confident of and gets right. The takeaway from this is probably that irregular nuclear shapes sizes or staining shades can indicate metastases. org resources tnm Data understanding What data do we have available 220k training images and 57k evaluation images. Smaller datasets and architectures seem to require larger values for weight decay while larger datasets and deeper architectures seem to require smaller values. pkl file that you ll need to copy with your model file if you want to deploy it on another device. In addititon Fastai library has implemented a training function for one cycle policy that we can use with only a few lines of code. The diagnostic procedure for pathologists is tedious and time consuming as a large area of tissue has to be examined and small metastases can be easily missed. com gigascience article 7 6 giy065 5026175 Data visualizationWe can see that the negative positive ratio is not entirely 50 50 as there are 130k negatives and 90k negatives. These are means and std s of each three channel and we calculated these previously in the stats step. Tumor tissue in the outer region of the patch does not influence the label. 1399 H E stained sentinel lymph node sections of breast cancer patients the CAMELYON dataset https academic. If you want to use the 0. The probability ranges from 0 to 1. We can see that the learning rate starts from lower and reaches the max_lr in the middle. org Background Lymph nodes are small glands that filter the fluid in the lymphatic system and they are the first place a breast cancer is likely to spread. The optimal lr is just before the base of the loss and before the start of divergence. check that the imgDataBunch is loading our images ok Next we create a convnet learner object ps dropout percentage 0 1 in the final layer We can use lr_find with different weight decays and record all losses so that we can plot them on the same graph Number of iterations is by default 100 but at this low number of itrations there might be too much variance from random sampling that makes it difficult to compare WD s. Save the finetuned model if the model was better before finetuning uncomment this to load the previous stage get accuracy I modified this from the fastai s plot_top_losses https github. Low resolution Mid resolution High resolution https camelyon17. Cells in architectural arrangements seen in malignancy highly variable dependent on tumour type and differentiation. While we are calculating statistics we can check if there are images that have a very low maximum pixel intensity almost totally black or very high minimum pixel intensity almost totally white. This is not so crucial here as both labels are almost equally represented but in case we had a rare class random split could cause severe underrepresentation or in worst case leave all rare classes out of one split. The higher rate is having a regularizing effect as it won t allow the model to settle for sharp and narrow local minima but pushes for wider and more stable one. org wiki Receiver_operating_characteristic. The best possible solution would yield an AUC of 1 which means we would classify all positive samples correctly without getting any false positives. After the heads have adjusted and the model somewhat works we can continue to train all the weights. ROC curve and AUCRemember AUC is the metric that is used for evaluating submissions. com gigascience article 7 6 giy065 5026175 CAMELYON16 background https camelyon16. pdf We can select the learning rate around 2e 2 where it is close to the bottom but still descending. Dark blue hematoxylin binds to negatively charged substances such as nucleic acids and pink eosin to positively charged substances like amino acid side chains most proteins. These kind of images could be caused by bad exposure or cropping to an empty area. The dataset is a subset of the PCam dataset https github. See if how much we improved. I haven t tried these so I cannot guarantee these will work straight away. However some relevant information about the surroundings might be left out with these small sized image samples. Is it valid Does it reflect our expectations According to the data description the dataset has been stripped of duplicates. This fastai s data object is easily customized for loading images using our own readCroppedImage function. We could deploy this for inference to another machine a web server for example. Identify the presence of metastases from 96 x 96px digital histopathology images. It is important that the loss is still descending where we select the learning rate. How would a solution look like Our evaluation metric is area under the ROC curve http en. I want to thank Martijn https www. This is called test time augmentation TTA and it can improve our results if we run inference multiple times for each image and average out the predictions. To see the effects of our augmentation we can plot one image multiple times. png ROC curve from a previous run of this kernel What is known about the domain The histopathological images are glass slide microscope images of lymph nodes that are stained with hematoxylin and eosin H E. For the entire dataset when the slide level label was unclear during the inspection of the H E stained slide an additional WSI with a consecutive tissue section immunohistochemically stained for cytokeratin was used to confirm the classification. Gland formation. org wiki Lymph_node_metastasis lymph node metastases can have these features Foreign cell population key feature Classic location subcapsular sinuses Cells with cytologic features of malignancy Nuclear pleomorphism variation in size shape and staining. com etinuz for sharing his examples of Fastai v1 training and visualization They were helpful when converting this kernel from Fastai v0. For this we need our saved model and then we need to export the DataBunch. The ROC curve is a plot of True positive rate against False positive rate at various thresholds and the area under the curve AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. Baseline model Fastai v1 In ML production pipeline it is a good idea to start with a relatively simple model sort of a minimum viable product MVP or a baseline. Finetuning the baseline modelNext we can unfreeze all the trainable parameters from the model and continue its training. Train and Validation augmentations convert to databunch Normalize with training set stats. 0 import all the models from torchvision specify model architecture densenet169 seems to perform well for this data but you could experiment specify batch size hardware restrics this one. Small clusters of cells. png Example of a metastatic region https camelyon17. This staining method is one of the most widely used in medical diagnosis and it produces blue violet and red colors. It might also reveal something about the dataset such as bad quality data. How good is the model in terms of project requirements It is a good idea to look at examples of images from Random samples Some random predictions The most incorrectly labeled What our model predicts wrong with very high probability. However the training distribution seems to be 60 40 negatives positives. org site CAMELYON17 serve public_html example_high_resolution. This means that we start to look for the minima within that area. The ratio is closer to 60 40 meaning that there are 1. Sylvian Gugger wrote a very clear explanation https sgugger. I think the bright ones are just cropped from a non stained part or they don t have any tissue plain glass so the labels are correct. For example if we would train a dog breed classifier and all of our images of a certain dog breed would have been taken in a dog show competition. Leslie Smith https arxiv. ROC curve example https i. ai software library that is built on PyTorch https pytorch. Gradient weighted Class Activation Mapping Grad CAM Grad CAM Visual Explanations from Deep Networks via Gradient based Localization https arxiv. Note that if we apply augmentation here augmentations will also be applied when we are predicting inference. Abundant mitotic figures. This is a good place to stop. Typically nuclei are stained blue whereas cytoplasm and extracellular parts in various shades of pink. Deploy example Now that we have a working model. Histological assessment of lymph node metastases is part of determining the stage of breast cancer in TNM classification which is a globally recognized standard for classifying the extent of spread of cancer. 696453 and std s of 0. com gigascience article 7 6 giy065 5026175 Is the data quality quantity recency sufficient All glass slides included in the CAMELYON dataset were part of routine clinical care and are thus of diagnostic quality. Sections of this kernel Project understanding Data understanding Data visualization Baseline model Fastai v1 Validation and analysis Metrics Prediction and activation visualizations ROC AUC Submit Deploy example Section Data visualization Section Prediction and activation visualizations Project understanding What exactly is the problem Binary image classification problem. How is the data best transformed for modeling We know that the label of the image is influenced only by the center region 32 x 32px so it would make sense to crop our data to that region only. html and torchvision models https pytorch. org site CAMELYON17 serve public_html example_low_resolution. Otherwise the random initialization of the head weights could harm the relatively well performing pre trained weights of the model. The model already performs well and now as we unfreeze the bottom layers that have been pre trained with a large number of general images to detect common shapes and patterns all weights are mostly adjusted. This means that a negatively labeled image could contain metastases in the outer region. Now if we would train further the model would only memorize features from the training set and the validation set performance would rise. Official documentation https docs. However this has not been confirmed by testing. A positive label means that there is at least one pixel of tumor tissue in the center region 32 x 32px of the image. This shows the activation maps of the predicted category so if the label is tumor the visualization shows all the places where the model thinks the tumor patterns are. csv and keep their original order List of tumor preds. Data API docs https docs. This can potentially save us a lot of time from training with suboptimal hyperparameters. I recommend using an iteration count of at least 300 for more consistent results. In case of an empty area the image would not be an outlier but equally valid negative sample. ai is that it includes out of the box support for many recent advancements in deep learning research. Submit the model for evaluationWe need to submit the tumor probability for each test sample in this competition. com basveeling pcam Hematoxylin and eosin staining of tissue and cell sections https www. com fastai fastai blob master fastai vision learner. Anyway removing only a small amount of outliers from this size data set has little or no effect on the prediction performance. Irregular nuclear membrane. Confusion matrix can help us understand the ratio of false negatives and positives and it s a fast way looking at our model s performance. Irregular chromatin pattern esp. Image file descriptors Description Format TIFSize 96 x 96Channels 3Bits per channel 8Data type Unsigned charCompression Jpeg Is the data relevant to the problem This dataset is a combination of two independent datasets collected in Radboud University Medical Center Nijmegen the Netherlands and the University Medical Center Utrecht Utrecht the Netherlands. ai vision models https docs. The policy brings more disciplined approach for selecting hyperparameters such as learning rate and weight decay. This visualization is a good way of understanding what are the images the model struggles with. The predictions are then averaged. The slides are produced by routine clinical practices and a trained pathologist would examine similar images for identifying metastases. The PCam s dataset including this one uses 10x undersampling to increase the field of view which gives the resultant pixel resolution of 2. com basveeling pcam and the only difference between these two is that all duplicate images have been removed. We can see that the validation performance has separated from the training performance a bit in the end of the cycle. In the middle of our cycle we start to lower the learning rate as we are hopefully in a good stable area. For differenet pretrained model architectures check Fast. However during the acquisition process scanning can fail or result in out of focus images. I am not so sure about the dark image is it an outlier crop from badly exposed area or just some very large cell part filling the whole image. png Example of a metastatic region in lymph nodes CHAMELYON17 https camelyon17. This means that our model has started overfitting during the small learning rates. This time we define the min and max lr of the cycle lets take a second look at the confusion matrix. The inspection was performed by an experienced technician Q. As a quality control measure all slides were inspected manually after scanning. WEIGHT DECAY 1e 6 reset learner this gets more consistent starting conditions WEIGHT DECAY 1e 4 reset learner this gets more consistent starting conditions WEIGHT DECAY 1e 2 reset learner Plot weight decays ax ranges may need some tuning with different model architectures 1cycle policy plot learning rate of the one cycle and plot the losses of the first cycle predict the validation set with our model before we continue lets save the model at this stage load the baseline model unfreeze and run learning rate finder again plot learning rate finder results Now smaller learning rates. According to the data description there is a 50 50 balance between positive and negative examples in the training and test splits. org site CAMELYON17 serve public_html example_mid_resolution. The samples don t have tumor tissue present. ai V1 https docs. First we find the optimal learning rate and weight decay values. There is a good chance that the model would learn to recognize the competition surroundings instead of doggy features with that breed. This saves the internal information classes etc need for inference in a file named export. We load the images to an ImageDataBunch for the training. We can calculate it here for ou validation set but it will most likely differ from the final score. Nuclear atypia Nuclear enlargement. Out of the tested WD s 1e 4 seems like the largest WD that allow us to train with maximal learning rate. Preprocessing and augmentationThere are couple of ways we can use to avoid overfitting more data augmentation regularization and less complex model architectures. Next we train only the heads while keeping the rest of the model frozen. Here we can see that the model has learned to distinguish tumor and negative sample and it s already performing well. This hypothesis could be confirmed by training models with varying crop sizes. This is a simple table that shows the counts in a way of actual label vs. However some useful information about the surroundings could be lost if we crop too close. Large or irregular nucleolus. org Background TNM classification https www. Validation and analysisNow the training is done. We can see from the plotted losses that there is a small rise after the initial drop which is caused by the increasing learning rate of the first half cycle. gov pubmed 21356829 H E stained sentinel lymph node sections of breast cancer patients the CAMELYON dataset https academic. What kind of model architecture suits the problem best Here we will be using a pretrained convnet model and transfer learning to adjust the weights to our data. org Data which contains 400 H E stained whole slide images of sentinel lymph node sections that were acquired and digitized at 2 different centers using a 40x objective. html of Leslie s proposal that I recommend for reading. We want to select the largest weight decay that gets to a low loss and has the highest learning rate before shooting up. quick look at the label stats OpenCV reads the image in bgr format by default We flip it to rgb for visualization purposes random sampling Negatives Create a Rectangle patch Positives Create a Rectangle patch original size of the images do not change AUGMENTATION VARIABLES final size after crop range 0 180 180 allows all rotation variations 0 no change center crop shift in x and y axes 0 no change. With MVP we can very quickly see if there are some unexpected problems like bad data quality that will make any further investments into the model tuning not worth it. Here we will define what image augmentations to use and add them directly to our image loader function. Submit predictions TTATo evaluate the model we run inference on all test images. We should now train with much lower learning rates. How good does the model perform technically We can only get metrics from our validation set and the final test metrics will be most likely a bit different. Export to csv This is what the first 10 items of submission look like This will create an export. This cannot be more than ORIGINAL_SIZE CROP_SIZE 2 range 0 100 0 no change range 0 100 0 no change 0 or 1 random turn to left or right augmentations parameter is included for counting statistics from images where we don t want augmentations OpenCV reads the image in bgr format by default We flip it to rgb for visualization purposes random rotation the center point is the rotation anchor random x y shift crop to center and normalize to 0 1 range Random flip Random brightness Random contrast clip values to 0 1 range Negatives Positives Negatives As we count the statistics we can check if there are any completely black or white images If no pixel reaches this threshold image is considered too dark If no pixel is under this threshold image is considerd too bright is this too dark do not include in statistics is this too bright do not include in statistics Too dark hide axes if there are less than 6 Too bright hide axes if there are less than 6 we read the csv file earlier to pandas dataframe now we set index to id so we can perform If removing outliers uncomment the four lines below split this function returns more than we need as we only need the validation indexes for fastai fastai 1. Is the model learning Our model should have already learned something and if it hasn t there s probably something wrong with our code or with the data. ", "id": "qitvision/a-complete-ml-pipeline-fast-ai", "size": "21068", "language": "python", "html_url": "https://www.kaggle.com/code/qitvision/a-complete-ml-pipeline-fast-ai", "git_url": "https://www.kaggle.com/code/qitvision/a-complete-ml-pipeline-fast-ai", "script": "sklearn.utils sklearn.metrics randint fastai.vision MyImageItemList(ImageList) readImage open tqdm_notebook random *=all the models from torchvision imageToTensorImage matplotlib.patches shuffle numpy getLearner auc hooked_backward sklearn.model_selection plot_heatmap_overview * # fastai 1.0 fastai.callbacks.hooks matplotlib.pyplot pandas getHeatmap tqdm fastai roc_curve readCroppedImage torchvision.models train_test_split plot_overview ", "entities": "(('further investments', 'model'), 'see') (('dataset https', 'breast cancer patients'), 'pubmed') (('we', 'breed'), 'reveal') (('tumor patterns', 'places'), 'show') (('it', 'fast performance'), 'help') (('slides', 'metastases'), 'produce') (('somewhat we', 'weights'), 'adjust') (('ai software that', 'PyTorch https pytorch'), 'library') (('I', 'coders v3 https course'), 'recommend') (('However relevant information', 'image small sized samples'), 'leave') (('it', 'data'), 'be') (('where we', 'model architecture'), 'define') (('classifier', 'randomly chosen positive instance higher randomly chosen negative one'), 'be') (('it', 'cell just very large whole image'), 'be') (('which', 'first half cycle'), 'see') (('then we', 'DataBunch'), 'need') (('we', 'high'), 'be') (('2 where it', 'bottom'), 'select') (('we', 'dog show competition'), 'take') (('we', 'code'), 'implement') (('weights', 'common shapes'), 'perform') (('However this', 'testing'), 'confirm') (('We', 'learning now much lower rates'), 'train') (('Plot', 'extremely untrained eye'), 'be') (('we', 'predictions'), 'call') (('here Calculating', '0'), 'give') (('us', 'learning maximal rate'), 'seem') (('We', 'learner'), 'need') (('Gradient', 'Localization https Gradient based arxiv'), 'weight') (('I', 'these'), 'try') (('best Here we', 'data'), 'suit') (('it', 'most likely final score'), 'calculate') (('here we', 'two empty lists'), 'create') (('What', 'Deploy example'), 'section') (('which', '2'), 'use') (('we', 'computations'), 'note') (('validation performance', 'cycle'), 'see') (('I', 'optimal size'), 'show') (('evaluation metric', 'ROC curve'), 'look') (('Thus it', 'center region'), 'be') (('dataset', 'PCam dataset https github'), 'be') (('Cells', 'tumour highly dependent type'), 'variable') (('data object', 'readCroppedImage own function'), 'customize') (('We', 'example'), 'deploy') (('we', 'correctly false positives'), 'yield') (('html create_cnn 1cycle policyWe', 'Leslie Smith https arXiv April 2018 arxiv'), 'use') (('it', 'wrong things'), 'give') (('kind', 'empty area'), 'cause') (('we', 'hopefully good stable area'), 'start') (('x 32 so it', 'region'), 'transform') (('we', 'training'), 'finetune') (('Sylvian Gugger', 'explanation https very clear sgugger'), 'write') (('we', 'weight 2 1e 4 6 decays'), 'as') (('We', 'flips'), 'return') (('still where we', 'learning rate'), 'be') (('model', 'classification decision'), '02391') (('slides', 'manually scanning'), 'inspect') (('labels', 'don tissue plain glass'), 'think') (('simple that', 'actual label'), 'be') (('additional WSI', 'classification'), 'use') (('that', 'learning highest rate'), 'want') (('AUCRemember that', 'submissions'), 'be') (('org site', 'public_html example_high_resolution'), 'serve') (('I', 'plot_top_losses https github'), 'save') (('Going', 'model deeper architecture'), 'start') (('larger datasets', 'deeper smaller values'), 'seem') (('test final metrics', 'validation'), 'perform') (('function', '0 1 range'), 'convert') (('that', 'pixel very low maximum intensity'), 'calculate') (('we', 'one split'), 'be') (('it', 'blue violet'), 'be') (('First we', 'learning weight optimal rate values'), 'find') (('positive label', '32 image'), 'mean') (('it', 'learning deep research'), 'be') (('which', 'cancer'), 'be') (('breast first cancer', 'lymphatic system'), 'be') (('optimal lr', 'divergence'), 'be') (('it', 'minimum viable product'), 'v1') (('policy', 'such rate'), 'bring') (('This', 'suboptimal hyperparameters'), 'save') (('org site', 'public_html example_mid_resolution'), 'serve') (('small metastases', 'tissue'), 'be') (('inspection', 'technician experienced Q.'), 'perform') (('we', 'test images'), 'evaluate') (('we', 'surroundings'), 'lose') (('Next we', 'model'), 'train') (('Typically nuclei', 'pink'), 'stain') (('We', 'training splits'), 'want') (('etc', 'file'), 'save') (('it', 'WD s.'), 'check') (('rotation variations', 'change center crop 0 x'), 'look') (('model', 'wider more one'), 'have') (('we', 'one image'), 'plot') (('hypothesis', 'crop varying sizes'), 'confirm') (('probably we', 'results'), 'improve') (('dataset https', 'breast cancer patients'), 'academic') (('we', 'stats previously step'), 'be') (('They', 'Fastai v0'), 'etinuz') (('We', 'training'), 'load') (('negative it', 'tumor'), 'see') (('model', 'good understanding'), 'be') (('Now we', 'working model'), 'deploy') (('we', 'training 220k images'), 'understanding') (('Train augmentations', 'training stats'), 'convert') (('removing', 'prediction performance'), 'have') (('It', 'quality such bad data'), 'reveal') (('dataset', 'Radboud University Medical Center Nijmegen'), 'tifsize') (('dark images', 'very bright very imagesAll'), 'plot') (('image', 'empty area'), 'be') (('Now we', 'i d columns'), 'be') (('Tumor tissue', 'label'), 'influence') (('I', 'more consistent results'), 'recommend') (('that', '40x objective'), 'Data') (('good model', 'doggy instead breed'), 'be') (('hematoxylin Dark blue binds', 'most proteins'), 'charge') (('This', 'export'), 'be') (('you', 'batch size hardware specify restrics'), 'specify') (('glass slides', 'thus diagnostic quality'), 'be') (('data', 'overall quality'), 'increase') (('we', 'fastai fastai'), 'be') (('Otherwise random initialization', 'model'), 'harm') (('cell feature Classic location Foreign key subcapsular', 'size shape'), 'have') (('org site', 'public_html example_low_resolution'), 'serve') (('scanning issues', 'diagnosis'), 'assess') (('I', 'reading'), 'html') (('dataset', 'duplicates'), 'be') (('model', 'learning small rates'), 'mean') (('that', 'H hematoxylin E.'), 'curve') (('set performance', 'training set'), 'memorize') (('crop this', 'model file name e.'), 'run') (('learning when rates', 'end'), 'rise') (('you', 'device'), 'file') (('sizes', 'staining metastases'), 'be') (('learning rate', 'middle'), 'see') (('OpenCV', 'lot faster PIL'), 'implement') (('time we', 'confusion matrix'), 'define') (('PCam dataset', 'Challenge dataset https camelyon16'), 'derive') (('lets', 'learning Now smaller rates'), 'DECAY') (('i', 'sample_submission'), 'transform') (('metastases', 'tissue'), 'be') (('Here we', 'image loader directly function'), 'define') (('also when we', 'inference'), 'note') (('model', 'very high probability'), 'be') (('we', 'data augmentation more regularization'), 'be') (('we', 'area'), 'mean') (('duplicate images', 'only two'), 'be') (('negatively labeled image', 'outer region'), 'mean') ", "extra": "['biopsy of the greater curvature', 'patient', 'test', 'diagnosis', 'metastasis', 'procedure']"}