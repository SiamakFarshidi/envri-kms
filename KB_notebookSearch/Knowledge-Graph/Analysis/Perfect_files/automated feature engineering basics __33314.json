{"name": "automated feature engineering basics ", "full_name": " h1 Introduction Automated Feature Engineering Basics h2 Feature Engineering h1 Problem h2 Dataset h3 Read in Data and Create Small Datasets h1 Featuretools Basics h1 Entities and Entitysets h1 Relationships h1 Feature Primitives h1 Deep Feature Synthesis h3 DFS with Default Primitives h3 DFS with Selected Aggregation Primitives h2 Notes on Basic Implementation h1 Results h2 Feature Performance Experiments h2 Correlations h3 Correlations with the Target h3 Visualize Distribution of Correlated Variables h4 Collinear Features h2 Feature Importances h2 Remove Low Importance Features h2 Align Train and Test Sets h2 Appendix GBM Model Used Across Feature Sets ", "stargazers_count": 0, "forks_count": 0, "description": "In an ideal scenario we would have a set of independent features but that rarely occurs in practice. I have made the entire dataset available here https www. Visualize Distribution of Correlated VariablesOne way we can look at the resulting features and their relation to the target is with a kernel density estimate plot. aspx a service dedicated to provided lines of credit loans to the unbanked population. predict_proba valid_features num_iteration best_iteration 1 Record the best score valid_score model. 777 1 hour Four 1156 0. There is one row for every made payment and one row for every missed payment. Featuretools has a default method for doing this available in the selection module. It s safest to just join them together and treat them as a single dataframe. com blog secret to data science success by one of the developers of Featuretools. AMT_PAYMENT which is the average over a client s loans of the minimum value of previous credit application installment payments. In the plot below we show the distribution of two of the newly created features colored by the value of the target. In a _parent table_ each individual has a single row. Moreover manual feature engineering is limited both by human time constraints and imagination we simply cannot conceive of every possible feature that will be useful. This can be useful to look at the resulting features before starting an extended computation. For the correlations we will focus on the feature_matrix_spec the features we made by specifying the primitives. These represent simple calculations many of which we already use in manual feature engineering that can be stacked on top of each other to create complex features. Featuretools the only library for automated feature engineering at the moment will not replace the data scientist but it will allow her to focus on more valuable parts of the machine learning pipeline such as delivering robust models into production. append overall Dataframe of validation scores metrics pd. I m not sure if this is allowing data leakage into the train set and if these feature creation operations should be applied separately. ____Featuretools demonstrably adds value when included in a data scientist s toolbox. png Read in Data and Create Small DatasetsWe will read in the full dataset sort by the SK_ID_CURR and keep only the first 1000 rows to make the calculations feasible. image https storage. To generate a subset of the features run the code cell below. We still are not using much domain knowledge but this feature set will be more manageable. array test_features col. align test_features join inner axis 1 No categorical indices to record cat_indices auto Integer label encoding elif encoding le Create a label encoder label_encoder LabelEncoder List for storing categorical indices cat_indices Iterate through each column for i col in enumerate features if features col. __bureau__ data concerning client s previous credits from other financial institutions. ConclusionsIn this notebook we went through a basic implementation of using automated feature engineering with featuretools for the Home Credit Default Risk dataset. The next step from here is improving the features we actually build and performing feature selection. The vital role of the data scientist now comes down to choosing the correct set of primitives and selecting the best features from among all the candidates. loan_amount that is a deep feature with a depth of 1. An entity in featuretools must have a unique index where none of the elements are duplicated. Therefore the bureau dataframe is the child of the app dataframe. mean train_scores Needed for creating dataframe of validation scores fold_names list range n_folds fold_names. However while we get a lot of features in featuretools this function call is not very well informed. It is ideal tool for problems such as the Home Credit Default Risk competition where there are several related tables that need to be combined into a single dataframe for training and one for testing. To show the effect of a categorical variable on the distribution of a numeric variable we can color the plot by th value of the categorical variable. __POS_CASH_BALANCE__ monthly data about previous point of sale or cash loans clients have had with Home Credit. First we ll make an empty entityset named clients to keep track of all the data. For an example of using manual feature engineering check out part one https www. edu wp content uploads 2017 10 DSAA_DSM_2015. This is a standard supervised classification task __Supervised__ The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features __Classification__ The label is a binary variable 0 will repay loan on time 1 will have difficulty repaying loan DatasetThe data is provided by Home Credit http www. DataFrame feature feature_names importance feature_importance_values Overall validation score valid_auc roc_auc_score labels out_of_fold Add the overall scores to the metrics valid_scores. DataFrame dataframe with SK_ID_CURR and TARGET probabilities predicted by the model. com willkoehrsen feature engineering using feature tools. com willkoehrsen home credit default risk feature tools. To create a feature with a depth of two we could stack primitives by taking the maximum value of a client s average montly payments per previous loan such as MAX previous MEAN installments. When we look at correlations with the target we need to be careful about the multiple comparisons problem https towardsdatascience. The most important feature created by featuretools was MAX bureau. Each row is one month of a previous credit and a single previous credit can have multiple rows one for each month of the credit length. best_iteration_ Record the feature importances feature_importance_values model. The specified featuretools dataset was able to achieve nearly the same performance as the hand engineered features on the test set with 8 of the time invested. There are 7 different data files __application_train application_test__ the main training and testing data with information about each loan application at Home Credit. com is an open source Python package for automatically creating new features from multiple tables of structured related data. The children can then have multiple children of their own. append valid_score train_scores. Each current loan in the application data can have multiple previous loans. Slightly advanced note we need to be careful to not create a diamond graph https en. While the absolute value of the importances can be difficult to interpret looking at the relative value of the importances allows us to compare the relevance of features. The maximum of this value over the previous loans is therefore represented by this feature. __bureau_balance__ monthly data about the previous credits in bureau. Extracting as much information as possible from the available datasets is crucial to creating an effective solution. An example would be taking the absolute value of a column or finding the difference between two columns in one table. A parent is a single individual but can have mutliple children. Entities can also have time indices where each entry is identified by a unique time. best_score_ train auc valid_scores. encoding str default ohe method for encoding categorical variables. There are a few concepts that we will cover along the way Entities and EntitySets https docs. The SK_ID_CURR 100002 has one row in the parent table and multiple rows in the child. By going through these ideas one at a time we can build up our understanding of how featuretools which will later allow for us to get the most out of it. DFS stacks feature primitives to form features with a depth equal to the number of primitives. In order to isolate the effect of the features the same model was used to test a number of different feature sets. reshape 1 Record the categorical indices cat_indices. zeros len feature_names Empty array for test predictions test_predictions np. Let s look at the number of features with 0 importance which almost certainly can be removed from the featureset. 1 n_jobs 1 random_state 50 Train the model model. Each row is one month of a credit card balance and a single credit card can have many rows. 99 correlation with each other which is nearly perfectly positively linear. Unfortunately this will not run in a Kaggle kernel due to the computational expense of the operation. Using correlations is fine as a first approximation for identifying good features but it is not a rigorous feature selection method. We can calculate the number of top 100 features that were made by featuretools. If there are very highly correlated varibables we might want to think about removing some of them. html is a collection of tables and the relationships between them. For the other dataframes we must pass in make_index True and then specify the name of the index. com the multiple comparisons problem e5573e8b9578 if we make a ton of features some are likely to be correlated with the target simply because of random noise. reshape 1 test_features col label_encoder. Feature EngineeringThe objective of feature engineering https en. org wiki Curse_of_dimensionality The next call we make will specify a smaller set of features. get_dummies test_features Align the dataframes by the columns features test_features features. For intstance if we have a categorical variable that is represented as an integer we might want to let featuretools know the right type. 783 12 hours Three 1803 0. Although we did not use the advanced functionality of featuretools we still were able to create useful features that improved the model s performance in cross validation and on the test set. com willkoehrsen introduction to manual feature engineering and part two https www. We can first one hot encode the data we ll have to do this anyway for our model and then align the dataframes on the columns. This results in ambiguity so the approach we have to take instead is to link app to cash through previous. Each previous credit has its own row in bureau and is identified by the SK_ID_BUREAU Each loan in the application data can have multiple previous credits. Automated feature engineering https towardsdatascience. The app and bureau dataframe are linked by the SK_ID_CURR variable while the bureau and bureau_balance dataframes are linked with the SK_ID_BUREAU. We also see several important features with a depth of two such as MEAN previous_app. This is important because we are going to want to apply the same exact procedures to each dataset. First we read in some of the feature matrix using the nrows argument of pandas read_csv function. predict_proba test_features num_iteration best_iteration 1 k_fold. We establish a relationship between previous the parent and cash the child using SK_ID_PREV. Remove Low Importance FeaturesFeature selection is an entire topic by itself but one thing we can do is remove any features that have only a single unique value or are all null. com willkoehrsen start here a gentle introduction. Correlations both between the features and the TARGET and between features themselves Feature importances determined by a gradient boosting machine model Feature Performance ExperimentsTo compare a number of different feature sets for the machine learning task I set up several experiments. html adding a relationship Feature primitives https docs. Here we will touch on the concepts of automated feature engineering with featuretools and show how to implement it for the Home Credit Default Risk competition. __Transformation__ an operation applied to one or more columns in a single table. DFS with Default PrimitivesIf you are interested in running this call on the entire dataset and making the features I wrote a script for that here https www. If you are new to this competition I suggest checking out this post to get started https www. Notes on Basic ImplementationThese calls represent only a small fraction of the ability of featuretools https docs. DAYS_CREDIT represents the number of days before the current application at Home Credit that the applicant applied for a loan at another credit institution. CorrelationsNext we can look at correlations within the data. com automated feature engineering in python 99baf11cc219 aims to help the data scientist with the problem of feature creation by automatically building hundreds or thousands of new features from a dataset. If we directly link app and cash via SK_ID_CURR previous and cash via SK_ID_PREV and app and previous via SK_ID_CURR then we have created two paths from app to cash. We would probably want to remove some of these highly correlated variables in order to help the model learn and generalize better. Feature importances can be used for dimensionality reduction. com guides tuning_dfs. feature_importances pd. append train_score Clean up memory gc. __Even the default set of features in featuretools was able to achieve similar performance to hand engineered features in less than 10 of the time. We will stick to the basics so we can get the ideas down and then build upon this foundation in later work when we customize featuretools. We did not specify the variable types when creating entities did not use the relative time variables and didn t touch on custom primitives https docs. The training application data comes with the TARGET with indicating 0 the loan was repaid and 1 the loan was not repaid. append valid_auc train_scores. com willkoehrsen home credit default risk feature tools data in the file called feature_matrix. The objective is to use historical financial and socioeconomic data to predict whether or not an applicant will be able to repay a loan. The same analysis could be applied to the default feature set. fit train_features train_labels eval_metric auc eval_set valid_features valid_labels train_features train_labels eval_names valid train categorical_feature cat_indices early_stopping_rounds 100 verbose 200 Record the best iteration best_iteration model. Each row is one month of a previous point of sale or cash loan and a single previous loan can have many rows. Below we specify all six relationships and then add them to the EntitySet. 786 0. A list of the available features primitives in featuretools can be viewed below. com automated_feature_engineering primitives. drop columns SK_ID_CURR TARGET test_features test_features. drop columns SK_ID_CURR One Hot Encoding if encoding ohe features pd. n_splits Record the out of fold predictions out_of_fold valid_indices model. html Entities and EntitysetsAn entity is simply a table or in Pandas a dataframe. Control using only data from the application dataset Test One manual feature engineering using only the application bureau and bureau_balance data Test Two manual feature engineering using all datasets Test Three featuretools default features in the feature_matrix Test Four featuretools specified features in the feature_matrix_spec Test Five featuretools specified features combined with manual feature engineering The number of features is after one hot encoding the validation receiver operating characteristic area under the curve ROC AUC is calculated using 5 fold cross validation the test ROC AUC is from the public leaderboard and the time spent designing is my best estimate of how long it took to make the dataset Test Number of Features Validation ROC AUC Test ROC AUC Time Spent Control 241 0. The most important feature created by featuretools was the maximum number of days before current application that the client applied for a loan at another institution. 785 0. As an example the app dataframe has one row for each client SK_ID_CURR while the bureau dataframe has multiple previous loans SK_ID_PREV for each parent SK_ID_CURR. The correlation between this feature and the target is extremely weak and could be only noise. Altogether there are a total of 6 relationships between the tables. com minute quick start is an open source Python library for automatically creating features out of a set of related tables using a technique called deep feature synthesis http www. 25 hours One 421 0. array test_features Create the kfold object k_fold KFold n_splits n_folds shuffle False random_state 50 Empty array for feature importances feature_importance_values np. We will work with a subset of the data because this is a computationally intensive job that is outside the capabilities of the Kaggle kernels. Each individual in the parent table can have multiple rows in the _child table_. __credit_card_balance__ monthly data about previous credit cards clients have had with Home Credit. Trying to interpret this feature is difficult but my best guess is a client s maximum value of average number of atm drawings per month on previous credit card loans. get_dummies features test_features pd. It s clear that featuretools delivered value on this problem but it still did not leave us without a job. 05 reg_alpha 0. collect Make the submission dataframe submission pd. append i Catch error if label encoding scheme is not valid else raise ValueError Encoding must be either ohe or le print Training Data Shape features. DataFrame dataframe of training features to use for training a model. Feature primitives fall into two categories __Aggregation__ function that groups together child datapoints for each parent and then calculates a statistic such as mean min max or standard deviation. com c home credit default risk using the featuretools library. com loading_data using_entitysets. At the end of this notebook we ll look at the features themselves as well as the results of modeling with different combinations of hand designed and automatically built features. Although we want to be careful about placing too much value on the feature importances they can be a useful method for dimensionality reduction and understanding the model. The model which can be viewed in the appendix is a basic LightGBM algorithm using 5 fold cross validation for training and evaluation. 25 hours It s hard to say which set is exactly the best although I trust the cross validation scores more than the public leaderboard but there are huge discrepancies is the time for development. Correlations with the TargetSeveral of the features created by featuretools are among the most correlated with the TARGET in terms of absolute magnitude. However that does not mean they are necessarily important. Moreover automated feature engineering took a fraction of the time spent manual feature engineering while delivering comparable results. Typically this process is done by hand using pandas operations such as groupby agg or merge and can be very tedious. This will be very useful when we need to define relationships in featuretools. com automated_feature_engineering afe. __The next steps are to take advantage of the advanced functionality in featuretools combined with domain knowledge to create a more useful set of features. __previous_application__ previous applications for loans at Home Credit of clients who have loans in the application data. Each previous application has one row and is identified by the feature SK_ID_PREV. All entities in the entity can be related to each other. Must include the TARGET column. DataFrame SK_ID_CURR test_ids TARGET test_predictions Make the feature importance dataframe feature_importances pd. html or seed features or interesting values Nonetheless in this notebook we were able to learn the basic foundations which will allow us to more effective use the tool as we learn how it works. 784 0. When I did feature engineering by hand it took about 12 hours to create a comparable size dataset. com static papers DSAA_DSM_2015. To perform DFS in featuretools we use the dfs function passing it an entityset the target_entity where we want to make the features the agg_primitives to use the trans_primitives to use and the max_depth of the features. DFS with Selected Aggregation PrimitivesWith featuretools we were able to go from 121 original features to almost 1700 in a few lines of code. 25 hours Five 1624 0. This shows the distribution of a single variable and can be thought of as a smoothed histogram. shape 0 Lists for recording validation and training scores valid_scores train_scores Iterate through each fold for train_indices valid_indices in k_fold. They can also be used to help us better understand a problem. html is an operation applied to a table or a set of tables to create a feature. First we establish a control dataset and then we carry out a series of experiments and present the results. Features that are highly correlated with one another can diminish interpretability and generalization performance on the test set. enable del model train_features valid_features gc. html aggregations and transformations Deep feature synthesis https docs. Then we establish a relationship between app the parent and previous now the child using SK_ID_CURR. We end up with a lot of features but they are probably not all relevant to the problem. 745 0. com willkoehrsen introduction to manual feature engineering p2 applied to this competition. Extract the ids train_ids features SK_ID_CURR test_ids test_features SK_ID_CURR Extract the labels for training labels features TARGET Remove the ids and target features features. The importance of creating the proper features cannot be overstated because a machine learning model can only learn from the data we give to it. __installments_payment__ payment history for previous loans at Home Credit. Any thoughts would be much appreciated Featuretools Basics Featuretools https docs. The observations are in the rows and the features in the columns. array features test_features np. We can look for pairs of correlated features and potentially remove any above a threshold. An example is calculating the maximum previous loan amount for each client. DataFrame dataframe of testing features to use for making predictions with the model. n_splits Make predictions test_predictions model. For a good take on why features are so important here s a blog post https www. Using a computer with 64GB of ram this function call took around 24 hours I don t think I m technically breaking the rules of my university s high powered computing center. Every loan has its own row and is identified by the SK_ID_CURR. The best way to think of a one to many relationship is with the analogy of parent to child. The original paper on automated feature engineering using deep feature synthesis https dai. RelationshipsRelationships are a fundamental concept not only in featuretools but in any relational database. These correlations were calculated using the entire training section of the feature matrix. Collinear FeaturesThese variables all have a 0. Using an EntitySet allows us to group together multiple tables and manipulate them much quicker than individual tables. We simply used the default aggregations without thinking about which ones are important for the problem. dtype object Map the categorical features to integers features col label_encoder. ProblemThe Home Credit Default Risk competition is a supervised classification machine learning task. Then featuretools will be able to create features on app derived from both previous and cash by stacking multiple primitives. org wiki Multicollinearity made by featuretools. Removing the low information features and aligning the dataframes has left us with 1689 features Feature selection will certainly play an important role when using featuretools. We need to pass in an index if the data has one or make_index True if not. shape 0 Empty array for out of fold validation predictions out_of_fold np. For example if we take the maximum value of a client s previous loans say MAX previous. Here we will use the default aggregation and transformation primitives a max depth of 2 and calculate primitives for the app entity. That only gives us 884 features and takes about 12 hours to run on the complete dataset. 779 1. Too many irrelevant features can decrease performance by drowning out the important features related to the curse of dimensionality https en. Introduction Automated Feature Engineering BasicsIn this notebook we will walk through applying automated feature engineering to the Home Credit Default Risk dataset https www. Feature PrimitivesA feature primitive https docs. Also based on examining some of the features it seems there might be issues with collinearity between features https en. Parameters features pd. DataFrame dataframe with training and validation metrics ROC AUC for each fold and overall. shape Extract feature names feature_names list features. 766 0. org wiki Feature_engineering is to create new features alos called explantory variables or predictors to represent as much information from an entire dataset in one table. This ensures we will not read in the entire 2 GB file. In theory this allows us to calculate features for any of the entities but in practice we will only calculate features for the app dataframe since that will be used for training testing. There are not datetimes in any of the data but there are relative times given in months or days that we could consider treating as time variables. com kaggle media competitions home credit home_credit. Including them all in the model is unnecessary because it would be encoding redundant information. com questions 15810339 how are feature importances in randomforestclassifier determined from including the feature in the model. Currently only app bureau and previous have unique indices SK_ID_CURR SK_ID_BUREAU and SK_ID_PREV respectively. We ll join the train and test set together but add a separate column identifying the set. columns Convert to np arrays features np. For each relationship we need to specify the parent variable and the child variable. feature_importances_ k_fold. DataFrame fold fold_names train train_scores valid valid_scores return submission feature_importances metrics Uncomment and run if kernel does not already have featuretools pip install featuretools pandas and numpy for data manipulation featuretools for automated feature engineering matplotlit and seaborn for visualizations Suppress warnings from pandas Read in the datasets and limit to the first 1000 rows sorted by SK_ID_CURR This allows us to actually see the results in a reasonable amount of time Add identifying column Append the dataframes Entity set with id applications Entities with a unique index Entities that do not have a unique index Relationship between app and bureau Relationship between bureau and bureau balance Relationship between current app and previous apps Relationships between previous apps and cash installments and credit Add in the defined relationships Print out the EntitySet List the primitives in a dataframe Default primitives from featuretools DFS with specified primitives DFS with default primitives Specify the aggregation primitives Most negative correlations Most positive correlations Need to reset index for loc to workBU plot repaid loans plot loans that were not repaid Label the plots Iterate through the columns Find correlations above the threshold Read in the feature importances and sort with the most important at the top List of the original features after one hot Iterate through the top 100 features Sort features according to importance Normalize the feature importances to add up to one Make a horizontal bar chart of feature importances Need to reverse the index to plot most important on top Set the yticks and labels Plot labeling Remove features with only one unique value Separate out the train and test sets One hot encoding Align dataframes on the columns. I took the work done in this notebook and ran the methods on the entire dataset with the results available here https www. valid_metrics pd. shape print Testing Data Shape test_features. Deep Feature SynthesisDeep Feature Synthesis DFS is the process featuretools uses to make new features. LGBMClassifier n_estimators 10000 boosting_type goss objective binary class_weight balanced learning_rate 0. html Relationships between tables https docs. com willkoehrsen intro to tuning automated feature engineering Appendix GBM Model Used Across Feature Sets pythondef model features test_features encoding ohe n_folds 5 Train and test a light gradient boosting model using cross validation. Now we define each entity or table of data. Defining the relationships is relatively straightforward and the diagram provided by the competition is helpful for seeing the relationships. 757 8 hours Two 1465 0. An aggregation works across multiple tables using relationships between tables. Because this process is computationally expensive we can run the function using features_only True to return only a list of the features and not calculate the features themselves. test_features pd. Now let s take a look at some of the features we have built and modeling results. com guides advanced_custom_primitives. This feature is originally recorded as negative so the maximum value would be closest to zero. 782 13. The diagram below provided by Home Credit shows how the tables are related. The bureau dataframe in turn is the parent of bureau_balance because each loan has one row in bureau but multiple monthly records in bureau_balance. This can be thought of a data structute with its own methods and attributes. DataFrame dataframe with the feature importances from the model. split features Training data for the fold train_features train_labels features train_indices labels train_indices Validation data for the fold valid_features valid_labels features valid_indices labels valid_indices Create the model model lgb. We are only using a sample of the features so this might not be representative of the entire dataset. For example we could use the most important features in order to concentrate on these aspects of a client when evaluating a potential loan. Later we can convert to a script and run with the entire datasets. Two tables are linked via a shared variable. Let s write a short function to visualize the 15 most important features. best_score_ valid auc train_score model. Align Train and Test SetsWe also want to make sure the train and test sets have the same exact features. An EntitySet https docs. Another area to investigate is highly correlated features known as collinear features. We will look explore tuning featuretools in an upcoming notebook https www. ResultsTo determine whether our basic implementation of featuretools was useful we can look at several results Cross validation scores and public leaderboard scores using several different sets of features. Featuretools https docs. org wiki Diamond_graph where there are multiple paths from a parent to a child. Feature ImportancesThe feature importances returned by a tree based model represent the reduction in impurity https stackoverflow. Automated feature engineering like many topics in machine learning is a complex subject built upon a foundation of simpler ideas. Either ohe for one hot encoding or le for integer label encoding n_folds int default 5 number of folds to use for cross validation Return submission pd. Featuretools will automatically infer the types of variables but we can also change them if needed. ", "id": "willkoehrsen/automated-feature-engineering-basics", "size": "33314", "language": "python", "html_url": "https://www.kaggle.com/code/willkoehrsen/automated-feature-engineering-basics", "git_url": "https://www.kaggle.com/code/willkoehrsen/automated-feature-engineering-basics", "script": "featuretools kde_target_plot seaborn numpy matplotlib.pyplot selection pandas plot_feature_importances ", "entities": "(('we', 'code'), 'featuretool') (('same model', 'feature different sets'), 'use') (('they', 'probably all problem'), 'end') (('we', 'also them'), 'infer') (('some', 'simply random noise'), 'com') (('com feature how importances', 'model'), 'question') (('we', 'parent variable'), 'need') (('loan', 'SK_ID_CURR'), 'have') (('current loan', 'multiple previous loans'), 'have') (('when creating', 'didn t custom primitives https docs'), 'specify') (('Then we', 'previous SK_ID_CURR'), 'establish') (('_ POS_CASH_BALANCE _ _ monthly data', 'cash loans Home Credit'), '_') (('previous application', 'feature'), 'have') (('RelationshipsRelationships', 'relational database'), 'be') (('html Entities entity', 'simply Pandas'), 'be') (('submission', 'dataframe submission pd'), 'make') (('we', 'primitives'), 'focus') (('it', 'good features'), 'be') (('we', 'feature actually selection'), 'improve') (('look', 'kernel density estimate plot'), 'distribution') (('relatively diagram', 'relationships'), 'be') (('aggregation', 'tables'), 'work') (('we', 'when potential loan'), 'use') (('that', 'test set'), 'diminish') (('where several related that', 'testing'), 'be') (('that', 'complex features'), 'represent') (('applicant', 'loan'), 'be') (('entities', 'other'), 'relate') (('We', 'set'), 'join') (('com', 'structured related data'), 'be') (('we', 'columns'), 'encode') (('Feature SynthesisDeep Feature Synthesis Deep DFS', 'process new features'), 'be') (('that', 'test set'), 'be') (('credit single card', 'many rows'), 'be') (('us', 'much quicker individual tables'), 'allow') (('we', 'results'), 'let') (('loan', 'multiple previous credits'), 'have') (('com', 'cross validation'), 'feature') (('that', 'only single unique value'), 'be') (('DFS stacks', 'primitives'), 'feature') (('which', 'training'), 'be') (('bureau SK_ID_CURR dataframe', 'parent'), 'have') (('_ _ credit_card_balance _ _ monthly data', 'Home Credit'), 'have') (('children', 'own'), 'have') (('we', 'time variables'), 'be') (('we', 'features'), 'curse_of_dimensionality') (('arrays', 'np'), 'feature') (('Typically process', 'groupby such agg'), 'do') (('app Currently only bureau', 'unique indices'), 'have') (('SK_ID_CURR 100002', 'multiple child'), 'have') (('that', 'simply possible feature'), 'limit') (('com minute quick start', 'feature deep www'), 'http') (('html', 'feature'), 'be') (('we', 'features'), 'determine') (('Below we', 'EntitySet'), 'specify') (('feature Automated engineering', 'simpler ideas'), 'be') (('cash single previous loan', 'many rows'), 'be') (('list', 'featuretools'), 'view') (('it', 'redundant information'), 'be') (('array test_features', 'n_folds False feature n_splits 50 Empty importances'), 'create') (('correlation', 'feature'), 'be') (('This', 'extended computation'), 'be') (('that', 'featuretools'), 'calculate') (('we', 'target'), 'show') (('_ _ Transformation _ _ operation', 'single table'), 'apply') (('this', 'entire dataset'), 'use') (('we', 'comparisons problem https multiple towardsdatascience'), 'look') (('individual', 'single row'), 'have') (('feature set', 'domain still much knowledge'), 'use') (('We', 'potentially any threshold'), 'look') (('parent', 'single mutliple children'), 'be') (('s', '15 most important features'), 'let') (('where entry', 'unique time'), 'have') (('when we', 'featuretools'), 'stick') (('calculations', 'only first 1000 rows'), 'read') (('We', 'SK_ID_PREV'), 'establish') (('why features', 'good take'), 'be') (('n_splits', 'predictions test_predictions model'), 'make') (('Later we', 'entire datasets'), 'convert') (('It', 'single dataframe'), 's') (('we', 'dataset'), 'be') (('data', 'make_index one True'), 'need') (('which', 'credit application installment previous payments'), 'be') (('that', 'training testing'), 'allow') (('html', 'them'), 'be') (('we', 'previous'), 'be') (('example', 'client'), 'calculate') (('Unfortunately this', 'operation'), 'run') (('I', 'https started www'), 'suggest') (('I', 'https here www'), 'DFS') (('That', 'complete dataset'), 'give') (('Test dataset Number', 'Features Validation ROC AUC Test ROC'), 'control') (('1 loan', 'TARGET'), 'come') (('features', 'features'), 'perform') (('ones', 'problem'), 'use') (('Two tables', 'shared variable'), 'link') (('ConclusionsIn notebook we', 'Home Credit Default Risk dataset'), 'go') (('_ next steps', 'features'), '_') (('Then featuretools', 'multiple primitives'), 'be') (('loan', 'multiple monthly bureau_balance'), 'be') (('feature creation operations', 'train set'), 'm') (('categorical features', 'col label_encoder'), 'feature') (('Collinear FeaturesThese variables', '0'), 'have') (('correlations', 'feature matrix'), 'calculate') (('they', 'model'), 'be') (('drop columns', 'Hot SK_ID_CURR One ohe pd'), 'feature') (('her', 'production'), 'replace') (('example', 'one table'), 'take') (('we', 'it'), 'overstate') (('I', 'results'), 'take') (('we', 'way'), 'be') (('Here we', 'app entity'), 'use') (('computationally we', 'features'), 'be') (('then we', 'cash'), 'create') (('I', 'computing high powered center'), 'take') (('This', 'own methods'), 'think') (('have', 'same exact features'), 'want') (('We', 'notebook https upcoming www'), 'look') (('who', 'application data'), 'application') (('label encoding scheme', 'ValueError else Encoding'), 'raise') (('then we', 'results'), 'establish') (('validation score valid_auc roc_auc_score Overall labels', 'metrics valid_scores'), 'feature') (('we', 'GB entire 2 file'), 'ensure') (('target features', 'ids'), 'extract') (('min max', 'such mean'), 'fall') (('I', 'several experiments'), 'correlation') (('MAX', 'previous loans'), 'say') (('we', 'MAX MEAN such previous installments'), 'stack') (('feature', 'feature_importance_values model'), 'importance') (('_ default Even set', 'time'), '_') (('how it', 'tool'), 'feature') (('i', 'col'), 'join') (('we', 'automatically features'), 'look') (('categorical', 'cat_indices'), 'index') (('very highly correlated we', 'them'), 'be') (('bureau', 'bureau_balance SK_ID_BUREAU'), 'link') (('Now we', 'data'), 'define') (('Correlations', 'absolute magnitude'), 'be') (('area', 'collinear highly correlated features'), 'be') (('best guess', 'credit card previous loans'), 'be') (('DatasetThe data', 'Home Credit http www'), 'be') (('Slightly advanced we', 'diamond graph https'), 'note') (('Align', 'test_features features'), 'feature') (('categorical_feature cat_indices', 'valid train'), 'train_features') (('Featuretools', 'selection module'), 'have') (('very when we', 'featuretools'), 'be') (('model', 'order'), 'want') (('most important feature', 'featuretools'), 'be') (('we', 'categorical variable'), 'color') (('single previous credit', 'credit length'), 'be') (('Extracting', 'effective solution'), 'be') (('applicant', 'credit institution'), 'represent') (('Feature importances', 'dimensionality reduction'), 'use') (('exactly I', 'huge development'), 's') (('Too many irrelevant features', 'dimensionality https'), 'decrease') (('CorrelationsNext we', 'data'), 'look') (('This', 'smoothed histogram'), 'show') (('vital role', 'candidates'), 'come') (('where none', 'elements'), 'have') (('later us', 'it'), 'by') (('it', 'job'), 's') (('it', 'features https'), 'seem') (('observations', 'columns'), 'be') (('us', 'features'), 'allow') (('First we', 'data'), 'make') (('same analysis', 'default feature set'), 'apply') (('_ _ _ _ Featuretools', 'data when toolbox'), 'add') (('Introduction Automated Feature Engineering notebook we', 'Home Credit Default Risk dataset https www'), 'BasicsIn') (('valid_indices', 'model model lgb'), 'feature') (('individual', '_ child table _'), 'have') (('First we', 'pandas read_csv function'), 'read') (('Notes', 'featuretools https docs'), 'represent') (('only one unique value', 'columns'), 'fold') (('that', '1'), 'loan_amount') (('_ _ bureau _ _ data', 'previous other financial institutions'), 's') (('client', 'institution'), 'be') (('function call', 'featuretools'), 'be') (('we', 'index'), 'for') (('best way', 'child'), 'be') (('maximum', 'therefore feature'), 'represent') (('bureau Therefore dataframe', 'app dataframe'), 'be') (('Here we', 'Home Credit Default Risk competition'), 'touch') (('org wiki Feature_engineering', 'one table'), 'be') (('We', 'MEAN such previous_app'), 'see') (('which', 'almost certainly featureset'), 'let') (('train_scores', 'validation scores fold_names list range n_folds fold_names'), 'mean') (('that', 'rarely practice'), 'have') (('feature Moreover automated engineering', 'comparable results'), 'take') (('also us', 'better problem'), 'use') (('how tables', 'Home below Credit'), 'show') (('originally negative so maximum value', 'zero'), 'record') (('featuretools specified dataset', 'time'), 'be') (('which', '99 other'), 'correlation') (('it', 'size comparable dataset'), 'take') (('Feature ImportancesThe feature importances', 'impurity https stackoverflow'), 'represent') (('computationally intensive that', 'Kaggle kernels'), 'work') (('class_weight', 'learning_rate'), 'balance') (('Feature selection', 'when featuretools'), 'leave') (('com', 'dataset'), 'aim') (('featuretools', 'right type'), 'for') ", "extra": "['test', 'procedure']"}