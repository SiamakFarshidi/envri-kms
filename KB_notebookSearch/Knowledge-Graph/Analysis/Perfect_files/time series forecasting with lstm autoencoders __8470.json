{"name": "time series forecasting with lstm autoencoders ", "full_name": " h1 Time series forecasting with deep learning LSTM autoencoders h2 Predict future sales h4 Data fields description h3 Dependencies h3 Loading data h3 Join data sets h3 Let s take a look at the raw data h3 Time period of the dataset h1 Data preprocessing h1 Time series processing h4 Currently I have one series 33 months for each unique pair of shop id and item id but probably would be better to have multiple smaller series for each unique pair so I m generating multiple series of size 12 one year for each unique pair h4 Dropping identifier columns as we don t need them anymore h3 Train and validation sets h3 Reshape data h4 First let s begin with how a regular RNN time series approach could be h1 Regular LSTM model h1 Autoencoder h3 LSTM Autoencoder h4 You should be aware that the better the autoencoder is able to reconstruct the input the better it internally encodes the input in other words if we have a good autoencoder we probably will have an equally good encoder h4 Let s take a look at the layers of the encoder decoder model h4 About the autoencoder layers h4 LSTM h4 RepeatVector layer h4 TimeDistributed layer h4 Defining the encoding model h4 Now let s encode the train and validation time series h4 Add new encoded features to the train and validation sets h4 Now we can use the new encoded feature that is a representation of the whole time series and train a less complex model that does not receives sequenced data as input h1 MLP with LSTM encoded feature h1 Comparing models h4 Model training h4 Regular LSTM on train and validation h4 MLP with LSTM encoder on train and validation h3 Build test set h4 Since we have two models I ll build test sets to apply on both of them h3 Regular LSTM model test predictions h3 Reshape data h4 Making predictions h4 Encoding the time series h4 Add encoded features to the test set h4 Making predictions h4 Predictions from the regular LSTM model h4 Predictions from the MLP model with LSTM encodded feature ", "stargazers_count": 0, "forks_count": 0, "description": "Time series forecasting with deep learning LSTM autoencoders The purpose of this work is to show one way time series data can be effiently encoded to lower dimensions to be used into non time series models. Here I ll encode a time series of size 12 12 months to a single value and use it on a MLP deep learning model instead of using the time series on a LSTM model that could be the regular approach. First let s begin with how a regular RNN time series approach could be. RepeatVector layer Here is something we don t usually see this layers basically repeats it s input n times the reason to use it is because the last layers from the encoder part the layer with one neuron don t return sequences so it does not outputs a sequenced data this way we can t just add another LSTM layer after it we need a way to turn this output into a sequence of the same time steps of the model input this is where RepeatVector layers comes in. Set seeds to make the experiment more reproducible. Data fields description ID an Id that represents a Shop Item tuple within the test set shop_id unique identifier of a shop item_id unique identifier of a product item_category_id unique identifier of item category date_block_num a consecutive month number used for convenience. Autoencoder Now we will build an autoencoder to learn how to reconstruct the input this way it internally learns the best way to represent the input in lower dimensions. The first part of the data preparation is from my other kernel Model stacking feature engineering and EDA https www. MLP with LSTM encoded feature test predictions For the MLP model with the encoded features I m only using the current month item_count and the encoded time series feature from our LSTM encoder model. Also I m leaving only monthly item_cnt 0 and 20 as this seems to be the distributions of the test set. The reconstruct model is composed of an encoder and a decoder the encoder is responsible for learning how to represent the input into lower dimensions and the decoder learns how to rebuild the smaller representations into the input again. Time series shape data points time steps features. Encoding the time series Add encoded features to the test set. You are predicting a monthly amount of this measure item_price current price of an item item_name name of item shop_name name of shop item_category_name name of item category Dependencies Loading data Join data sets Let s take a look at the raw data Time period of the datasetI m leaving only the shop_id and item_id that exist in the test set to have more accurate results. We could output the model with another LSTM layer with one neuron and return_sequences True parameter but using a TimeDistributed layer wrapping a Dense layer we will have the same weights for each outputted time step. The task is to forecast the total amount of products sold in every shop for the test set. Add new encoded features to the train and validation sets. If you want to check out some interesting different approaches on time series problems take a look at this kernel Deep Learning for Time Series Forecasting https www. Only shops that exist in test set. MLP with LSTM encoder on train and validation. com lstm autoencoders make sure to check out. Currently I have one series 33 months for each unique pair of shop_id and item_id but probably would be better to have multiple smaller series for each unique pair so I m generating multiple series of size 12 one year for each unique pair. You are provided with daily historical sales data. com dimitreoliveira model stacking feature engineering and eda. October 2015 is 33 date date in format dd mm yyyy item_cnt_day number of products sold. What I want is to encode the whole series into a single value so I need the output from the layer with a single neuron in this case it s the third LSTM layer. The results are pretty close also they may change a bit depending on the random initialization of the networks weights so I would say they are very similar in terms of performance. Note that the list of shops and products slightly changes every month. Making predictions. Now let s encode the train and validation time series. com dimitreoliveira deep learning for time series forecasting. We are asked to predict total sales for every product and store in the next month and our data is given by day so let s aggregate the data by month. Model training Regular LSTM on train and validation. Build test set Since we have two models I ll build test sets to apply on both of them. Regular LSTM model test predictions For the regular LSTM model we just need the last 12 months because that s our series input size. Here is a structural representations of an autoencoder After the models is trained we can keep only the encoder part and we ll have a model that is able to do what we want. TimeDistributed layer This layer is more common sometimes is used when you want to mix RNN layers with other kind of layers. Predictions from the regular LSTM model. By solving this competition you will be able to apply and enhance your data science skills. Train and validation sets. As you can see this is just the same value repeated some times to match the same shape of the model input. Predictions from the MLP model with LSTM encodded feature. Only items that exist in test set. January 2013 is 0 February 2013 is 1. Label 12 1. This way I ll also get the missing months from each shop_id and item_id and then replace them with 0 otherwise would be nan. Just a disclaimer you absolutely can get better results on any of the used models I did not spent too much time tuning the models hyper parameters as this is just for demonstration purpose so if you want to give the code a try you should surely tune a little more if you get better results or any good insight about the models or architecture please let me know. com encoder decoder long short term memory networks Defining the encoding model. Creating a robust model that can handle such situations is part of the challenge. You should be aware that the better the autoencoder is able to reconstruct the input the better it internally encodes the input in other words if we have a good autoencoder we probably will have an equally good encoder. Comparing models As you can see I tried to build both models with a similar topology type number of layers and neurons so it could make more sense to compare them. Now we can use the new encoded feature that is a representation of the whole time series and train a less complex model that does not receives sequenced data as input. Predict future salesWe are asking you to predict total sales for every product and store in the next month. Another good explanation about the used layers https machinelearningmastery. Dropping identifier columns as we don t need them anymore. This work was inspired by this Machinelearningmastery post A Gentle Introduction to LSTM Autoencoders https machinelearningmastery. Let s see what it outputs. Let s take a look at the layers of the encoder_decoder model About the autoencoder layers LSTM This is just a regular LSTM layer a layer that is able to receive sequence data and learn based on it nothing much to talk about. MLP with LSTM encoded feature For the MLP model I m only using the current month item_count and the encoded time series feature from our LSTM encoder model the idea is that we won t need the whole series because we already have a column that represents the whole series into a single value it s like a dimensionality reduction. Data preprocessing I m dropping all features but item_cnt_day because I ll be using only it as a univariate time series. I ll take only the encoding part of the model and define it as a new one. Time series processing As I only need the item_cnt feature as a series I can get that easily by just using a pivot operation. ", "id": "dimitreoliveira/time-series-forecasting-with-lstm-autoencoders", "size": "8470", "language": "python", "html_url": "https://www.kaggle.com/code/dimitreoliveira/time-series-forecasting-with-lstm-autoencoders", "git_url": "https://www.kaggle.com/code/dimitreoliveira/time-series-forecasting-with-lstm-autoencoders", "script": "sklearn.metrics sklearn.model_selection seed_everything mean_squared_error Model numpy matplotlib.pyplot tensorflow.keras.layers tensorflow pandas optimizers train_test_split tensorflow.keras Sequential ", "entities": "(('we', 'time outputted step'), 'output') (('first part', 'feature engineering'), 'be') (('we', 'don t them'), 'need') (('Add', 'test set'), 'encode') (('so I', '12 one year unique pair'), 'have') (('so s', 'month'), 'ask') (('Time series', 'data points time steps features'), 'shape') (('I', 'pivot easily just operation'), 'processing') (('we', 'probably equally good encoder'), 'be') (('task', 'test set'), 'be') (('it', 'them'), 'compare') (('0 this', 'test set'), 'm') (('it', 'case'), 'be') (('that', 'input'), 'use') (('You', 'sales daily historical data'), 'provide') (('Now s', 'validation train series'), 'let') (('way I', '0'), 'get') (('time series problems', 'Deep Time Series Forecasting https www'), 'take') (('MLP', 'time LSTM encoder encoded model'), 'encode') (('that', 'convenience'), 'field') (('they', 'performance'), 'change') (('decoder', 'input'), 'compose') (('just same value', 'model input'), 'be') (('future salesWe', 'next month'), 'ask') (('it', 'dimensionality reduction'), 'encode') (('I', 'time univariate series'), 'datum') (('experiment', 'seeds'), 'Set') (('time series data', 'non time series models'), 'series') (('that', 'more accurate results'), 'predict') (('more sometimes when you', 'layers'), 'layer') (('com encoder', 'encoding model'), 'decoder') (('RepeatVector where layers', 'model input'), 'be') (('Predictions', 'feature'), 'encodde') (('that', 'just last 12 months'), 'prediction') (('October', 'item_cnt_day products'), 'be') (('you', 'data science skills'), 'be') (('that', 'challenge'), 'be') (('list', 'shops'), 'note') (('way it', 'lower dimensions'), 'build') (('work', 'Autoencoders https machinelearningmastery'), 'inspire') (('LSTM just regular that', 'nothing'), 'let') (('I', 'new one'), 'take') (('we', 'what'), 'be') (('that', 'LSTM model'), 'encode') (('me', 'good models'), 'hyper') (('I', 'them'), 'test') ", "extra": "['test']"}