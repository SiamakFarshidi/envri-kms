{"name": "beginner s guide to audio data ", "full_name": " h1 Freesound General Purpose Audio Tagging Challenge h3 Contents h2 1 Exploratory Data Analysis h3 Loading data h3 Distribution of Categories h3 Reading Audio Files h3 Audio Length h2 2 Building a Model using Raw Wave h3 Keras Model using raw wave h4 Some sssential imports h4 Configuration h4 DataGenerator Class h4 Normalization h4 Training 1D Conv h4 Ensembling 1D Conv Predictions h2 3 Introuction to MFCC h4 Generating MFCC using Librosa h2 4 Building a Model using MFCC h3 Preparing data h4 Normalization h4 Training 2D Conv on MFCC h4 Ensembling 2D Conv Predictions h2 5 Ensembling 1D Conv and 2D Conv Predictions h2 Results and Conclusion h2 Coming Soon ", "stargazers_count": 0, "forks_count": 0, "description": "predict_generator train_generator use_multiprocessing True workers 6 max_queue_size 20 verbose 1 np. 1 kHz 16 bit PCM https upload. 1 kHz Each second in the audio consists of 44100 samples. Building a Model using MFCCWe will build now build a 2D Convolutional model using MFCC. 2 seconds the audio will consist of 44100 3. It is useful for preprocessing and feeding the data to a Keras model. If you wish to train the bigger model change COMPLETE_RUN True at the beginning of the kernel. png Freesound is a collaborative database of Creative Commons Licensed sounds. I have trained the model locally and uploaded its output files as a dataset. We use some Keras callbacks to monitor the training. input freesound audio tagging audio_train train_set. fit_generator train_generator callbacks callbacks_list validation_data val_generator epochs config. Ensembling 1D Conv and 2D Conv Predictions 1d_2d_ensembling 6. Reading Audio FilesThe audios are Pulse code modulated https en. The minimum number of audio samples in a category is 94 while the maximum is 300 2. n_folds for i train_split val_split in enumerate skf train_set train. input freesound audio tagging audio_train val_set. index batch_size 128 preprocessing_fn audio_norm predictions model. The __len__ method tells Keras how many batches to draw in each epoch. input freesound audio tagging audio_train train. max_epochs use_multiprocessing True workers 6 max_queue_size 20 model. label_idx batch_size 64 preprocessing_fn audio_norm val_generator DataGenerator config. NormalizationNormalization is a crucial preprocessing step. Building a Model using Raw Wave 1d_model_building Model Discription 1d_discription Configuration configuration DataGenerator class data_generator Normalization 1d_normalization Training 1D Conv 1d_training Ensembling 1D Conv Predictions 1d_ensembling 3. exists logs PREDICTION_FOLDER shutil. org wiki Sampling_ 28signal_processing 29 of 44. EarlyStopping stops the training once validation loss ceases to decrease TensorBoard helps us visualize training and validation loss and accuracy. h5 i Save train predictions predictions model. ModelCheckpoint saves the best weight of our model using validation data. Let s take a tour of the data visualization and model building through this kernel. clear_session X y X_val y_val X_train train_split y_train train_split X_train val_split y_train val_split checkpoint ModelCheckpoint best_ d. MFCC mimics the logarithmic perception of loudness and pitch of human auditory system and tries to eliminate speaker dependent characteristics by excluding the fundamental frequency and their harmonics. 809 2D Conv on MFCC verified labels only 168 361 0. Let s analyze their relative complexity and strength. org wikipedia commons 3 3c Freesound_project_website_logo. Few of the labels are Trumpet Squeak Meow Applause and Finger_sapping. We will ensemble these predictions later. If we want to perform some action after each epoch like shuffle the data or increase the proportion of augmented data we can use the on_epoch_end method. If we just want to classify some sound we should build features that are speaker independent. Training on Manually Verified Labels Change this to True to replicate the result To play sound in the notebook Hi hat Using wave library Using scipy Read and Resample the audio Random offset Padding Normalization Other Preprocessing Make a submission file Hi hat Random offset Padding Make a submission file Make a submission file. Before we jump to MFCC let s talk about extracting features from the sound. logs PREDICTION_FOLDER fold_ i i write_graph True callbacks_list checkpoint early tb print 50 print Fold i model get_2d_conv_model config history model. We use this weight to make test predictions. to_csv PREDICTION_FOLDER predictions_ d. png Generating MFCC using LibrosaThe library librosa has a function to calculate MFCC. fit X y validation_data X_val y_val callbacks callbacks_list batch_size 64 epochs config. pythonPREDICTION_FOLDER predictions_1d_conv if not os. org wiki Audio_bit_depth with a bit depth https en. Ensembling 1D Conv and 2D Conv Predictions Results and ConclusionSo far we have trained two models. png http recognize speech. Instead of a linear scale our perception system uses a log scale. h5 i Save train predictions train_generator DataGenerator config. n_classes Normalization pythonmean np. mkdir PREDICTION_FOLDER if os. Freesound General Purpose Audio Tagging Challenge Logo https upload. csv i Ensembling 2D Conv Predictions 5. iloc val_split checkpoint ModelCheckpoint best_ d. The second model will take the MFCCs as input. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators. input freesound audio tagging audio_test y_train to_categorical train. npy i predictions Make a submission file top_3 np. Majority of the audio files are short. Anything that is global as far as the training is concerned can become the part of Configuration object. Results and Conclusion conclusion 1. exists PREDICTION_FOLDER os. rmtree logs PREDICTION_FOLDER skf StratifiedKFold train. Let s now analyze the frame length distribution in Train and Test. Some sssential imports ConfigurationThe Configuration object stores those learning parameters that are shared between data generators models and training functions. iloc train_split val_set train. Exploratory Data Analysis eda Loading data loading_data Distribution of Categories distribution Reading Audio Files audio_files Audio Length audio_length 2. join list x for x in top_3 test label predicted_labels test label. csv i Ensembling 1D Conv PredictionsNow that we have trained our model it is time average the predictions of 10 folds. com zaffnet images master images raw_model. In other words we should extract features that depend on the content of the audio rather than the nature of the speaker. com images FeatureExtraction MFCC MFCC_Flowchart. It turns out that these techniques are still useful. mean X_train axis 0 std np. npy i predictions Save test predictions predictions model. jpg Important Due to the time limit on Kaggle Kernels it is not possible to perform 10 fold training of a large model. n_folds for i train_split val_split in enumerate skf K. input freesound audio tagging audio_train X_test prepare_data test config. We also generate a submission file. Exploratory Data Analysis Loading data Distribution of CategoriesWe observe that 1. png Bit depth 16 The amplitude of each sample in the audio is one of 2 16 65536 possible values. Note Sequence are a safer way to do multiprocessing. The distribution of audio length across labels is non uniform and has high variance. We will try Geometric Mean averaging and see what will be our Public LB score. 0001 Training 1D ConvIt is important to convert raw labels to integer indicesHere is the code for 10 fold training We use from sklearn. There are four abnormal length in the test histogram. save PREDICTION_FOLDER train_predictions_ d. DataGenerator ClassThe DataGenerator class inherits from keras. label_idx n_folds config. One of the challenges is that not all labels are manually verified. Preparing data pythonX_train prepare_data train config. Building a Model using MFCC 2d_model_building Preparing Data 2d_data Normalization 2d_normalization Training 2D Conv on MFCC 2d_training Ensembling 2D Conv Predictions 2d_ensembling 5. Introduction to MFCC intro_mfcc Generating MFCC using Librosa librosa_mfcc 4. Any feature that only gives information about the speaker like the pitch of their voice will not be helpful for classification. h5 i monitor val_loss verbose 1 save_best_only True early EarlyStopping monitor val_loss mode min patience 5 tb TensorBoard log_dir. The aim of this competition is to classify audio files that cover real world sounds from musical instruments humans animals machines etc. index train_set. Our 1D Conv model is fairly deep and is trained using Adam Optimizer with a learning rate of 0. For 10 fold CV the number of prediction files should be 10. npy i predictions Save test predictions test_generator DataGenerator config. 785 2D Conv on MFCC 168 361 0. A creative solution should be able to partially rely on these weak annotations. The number of audio samples per category is non nform. label_idx batch_size 64 preprocessing_fn audio_norm history model. index val_set. We will explain MFCC later Keras Model using raw waveOur model has the architecture as follows raw https raw. save PREDICTION_FOLDER test_predictions_ d. Samplig rate 44. We don t hear loudness on a linear scale. For those interested here is the detailed explanation http practicalcryptography. Happy Kaggling Contents1. Model Number of Trainable parameters Public LB score 1D Conv on Raw wave 360 513 0. predict X_test batch_size 64 verbose 1 np. std X_train axis 0 X_train X_train mean stdX_test X_test mean std Training 2D Conv on MFCC pythonPREDICTION_FOLDER predictions_2d_conv if not os. The dummy model is just for debugging purpose. Let s listen to an audio file in our dataset and load it to a numpy arrayLet s plot the audio framesLet s zoom in on first 1000 frames Audio LengthWe shall now analyze the lengths of the audio files in our datasetWe observe 1. StratifiedKFold for splitting the trainig data into 10 folds. We get both training and test predictions and save them as. If you like this work please show your support by upvotes. label_idx num_classes config. Taking these things into account Davis and Mermelstein came up with MFCC in the 1980 s. Also a good feature extraction technique should mimic the human speech perception. Introuction to MFCCAs we have seen in the previous section our Deep Learning models are powerful enough to classify sounds from the raw audio. The simplest method is rescaling the range of features to scale the range in 0 1. org wiki Audio_bit_depth of 16 and a sampling rate https en. argsort predictions axis 1 3 predicted_labels. Building a Model using Raw WaveWe will build two models 1. org wikipedia commons thumb b bf Pcm. We do not require any complex feature engineering. One such technique is computing the MFCC Mel Frquency Cepstral Coefficients from the raw audio. Also the proportion of maually_verified labels per category is non uniform. input freesound audio tagging audio_test test. com miscellaneous machine learning guide mel frequency cepstral coefficients mfccs. load_weights best_ d. 844 1D Conv 2D Conv Ensemble N A 0. predict_generator test_generator use_multiprocessing True workers 6 max_queue_size 20 verbose 1 np. The underlying mathematics is quite complicated and we will skip that. http recognize speech. predict X_train batch_size 64 verbose 1 np. We fit the model using DataGenerator for training and validation splits. 895 As we can see 2D Convolution on MFCC performs better than 1D Convolution on Raw waves. During test time only X is returned. If we want to double the perceived loudness of a sound we have to put 8 times as much energy into it. The first model will take the raw audio 1D array as input and the primary operation will be Conv1D2. Once initialized with a batch_size it computes the number of batches in an epoch. So if the duration of the audio file is 3. But before the Deep Learning era people developed techniques to extract features from audio signals. The __getitem__ method takes an index which is the batch number and returns a batch of the data both X and y after calculating the offset. Let s compute the MFCC of an audio file and visualize it. logs PREDICTION_FOLDER fold_ d i write_graph True callbacks_list checkpoint early tb print Fold i print 50 if COMPLETE_RUN model get_1d_conv_model config else model get_1d_dummy_model config train_generator DataGenerator config. ", "id": "fizzbuzz/beginner-s-guide-to-audio-data", "size": "14922", "language": "python", "html_url": "https://www.kaggle.com/code/fizzbuzz/beginner-s-guide-to-audio-data", "git_url": "https://www.kaggle.com/code/fizzbuzz/beginner-s-guide-to-audio-data", "script": "GlobalAveragePooling1D Flatten __init__ DataGenerator(Sequence) keras.layers keras.callbacks tqdm_notebook GlobalAveragePooling2D relu Dropout prepare_data on_epoch_end get_1d_dummy_model (Convolution1D __data_generation seaborn numpy models __getitem__ (EarlyStopping optimizers softmax Sequence get_2d_conv_model (Convolution2D matplotlib.pyplot Dense keras.utils pandas scipy.io __len__ tqdm to_categorical audio_norm BatchNormalization get_2d_dummy_model losses sklearn.cross_validation StratifiedKFold wavfile LearningRateScheduler backend keras get_1d_conv_model backend as K Config(object) keras.activations IPython.display ", "entities": "(('1D Conv model', '0'), 'be') (('interested here detailed explanation', 'practicalcryptography'), 'be') (('that', 'speaker'), 'extract') (('i', 'submission file'), 'make') (('that', 'features'), 'want') (('input freesound audio', 'audio_test y_train train'), 'tag') (('Also proportion', 'category'), 'be') (('We', 'training'), 'use') (('com images FeatureExtraction', 'MFCC_Flowchart'), 'MFCC') (('COMPLETE_RUN get_1d_conv_model else model config model', '50'), 'fold') (('So duration', 'audio file'), 'be') (('perception system', 'log scale'), 'use') (('MFCC', 'fundamental frequency'), 'mimic') (('submission file', 'submission file'), 'change') (('pythonX_train prepare_data', 'data'), 'config') (('feature extraction Also good technique', 'speech human perception'), 'mimic') (('We', 'feature complex engineering'), 'require') (('we', 'it'), 'want') (('_ _ len _ _ method', 'epoch'), 'tell') (('it', 'time 10 folds'), 'Ensembling') (('you', 'upvotes'), 'show') (('input freesound audio', 'audio_train'), 'tag') (('i', 'test predictions predictions model'), 'save') (('png Freesound', 'Creative Commons collaborative Licensed'), 'sound') (('Model Number', 'Public LB 1D Conv'), 'score') (('it', 'epoch'), 'compute') (('Training 2D Conv', 'MFCC pythonPREDICTION_FOLDER predictions_2d_conv'), 'std') (('We', 'predictions'), 'ensemble') (('It', 'Keras model'), 'be') (('png Generating MFCC', 'MFCC'), 'have') (('Exploratory Data Analysis Loading data Distribution', '1'), 'observe') (('We', 'sklearn'), '0001') (('creative solution', 'partially weak annotations'), 'be') (('We', 'training splits'), 'fit') (('ModelCheckpoint', 'validation data'), 'save') (('Bit 16 amplitude', '2 16 65536 possible values'), 'depth') (('One such technique', 'raw audio'), 'compute') (('it', 'large model'), 'Important') (('which', 'offset'), 'take') (('X_test prepare_data', 'input freesound audio_train'), 'audio') (('I', 'dataset'), 'train') (('s', 'model kernel'), 'let') (('h5 i', 'train predictions predictions model'), 'save') (('test label predicted_labels', 'test label'), 'list') (('that', 'data generators models'), 'object') (('quite we', 'that'), 'be') (('as far training', 'Configuration object'), 'become') (('Building', 'MFCC'), 'build') (('simplest method', '0'), 'rescale') (('s', 'it'), 'let') (('s', 'Train'), 'let') (('dummy model', 'just purpose'), 'be') (('2D 809 Conv', 'labels'), 'verify') (('2 seconds audio', '44100'), 'consist') (('maximum', 'category'), 'be') (('that', 'classification'), 'be') (('humans', 'machines'), 'be') (('h5 i', 'train predictions train_generator DataGenerator config'), 'save') (('labels', 'challenges'), 'be') (('distribution', 'non high variance'), 'be') (('argsort predictions', '1 3 predicted_labels'), 'axis') (('com zaffnet images master', 'raw_model'), 'image') (('Building', 'two models'), 'build') (('predict_generator use_multiprocessing True workers', '6 20 1 np'), 'test_generator') (('early tb', 'get_2d_conv_model config history model'), 'fold') (('i', 'test predictions test_generator DataGenerator config'), 'save') (('We', 'test predictions'), 'use') (('We', 'linear scale'), 'don') (('number', 'prediction files'), 'be') (('fit_generator train_generator', 'callbacks_list validation_data val_generator epochs config'), 'callback') (('Exploratory Data Analysis', 'Files Audio Length Audio audio_length'), 'read') (('Taking', '1980 s.'), 'come') (('Deep Learning models', 'raw audio'), 'be') (('which', 'generators'), 'guarantee') (('second model', 'input'), 'take') (('EarlyStopping val_loss min 1 save_best_only early patience', 'val_loss'), 'verbose') (('index', 'preprocessing_fn audio_norm predictions 128 model'), 'batch_size') (('Few', 'labels'), 'be') (('We', 'them'), 'get') (('what', 'Geometric Mean averaging'), 'try') (('we', 'Raw waves'), '895') (('s', 'relative complexity'), 'let') (('people', 'audio signals'), 'develop') (('Keras later Model', 'https raw raw'), 'explain') (('s', 'sound'), 'let') (('label_idx', '64 preprocessing_fn audio_norm history model'), 'batch_size') (('X_train batch_size', '64 1 np'), 'predict') (('we', 'on_epoch_end method'), 'want') (('input', 'audio 1D raw array'), 'take') (('frames Audio first 1000 LengthWe', 'datasetWe'), 'analyze') (('2D Conv Predictions far we', 'two models'), 'train') (('Note Sequence', 'safer multiprocessing'), 'be') (('you', 'kernel'), 'wish') (('us', 'training'), 'stop') (('number', 'category'), 'be') ", "extra": "['annotation', 'test']"}