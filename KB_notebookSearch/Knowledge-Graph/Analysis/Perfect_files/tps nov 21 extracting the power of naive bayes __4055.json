{"name": "tps nov 21 extracting the power of naive bayes ", "full_name": " h1 Understand the Models You Love h1 About Naive Bayes h1 Importing Packages and Sample Data h1 Splitting Data h1 Making Pipelines with Transformers h1 Gaussian Naive Bayes h1 Analysing the X train data Experimenting h1 Best Variable Transformation h1 Var Smoothing h1 Permutation Importance h1 Converting to Discrete h1 References ", "stargazers_count": 0, "forks_count": 0, "description": "Multinomial Used for discrete counts3. com 8ebe947b0538431322197ecd5324bade_l3. I would make changes to the important parameters and mention their impact. Please note that these parameter observations are made independent of each other and only for the current data we have. This procedure breaks the relationship between the feature and the target thus the drop in the model score is indicative of how much the model depends on the feature. In simple terms a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. com watch v H3EjCKtlVog on Gaussian Naive Bayes2. I did a similar experiment last month with Random Forest https www. com prashant111 naive bayes classifier in python by Prashant BanerjeeVideos 1. For numerical variable normal distribution is assumed bell curve which is a strong assumption hence we will do variable transformation first. s f f 1 features indices f importances indices f fitting our pipeline with a subset of the data with these variables first 39 variables have positive importance hence ignoring them. com watch v O2L2Uv9pdDA on understanding NB importing evaluation and data split packages importing modelling packages taking only 10000 rows as sample fit a probability distribution to a univariate data sample estimate parameters fit distribution sort data into classes calculate priors create PDFs Print the feature ranking for f in range X_test. The purpose of this exercise is to become better at extracting maximum power from it and see if non NN models can be used too. com better naive bayes 6. For speed I am choosing a simple test split of 30 size on 10000 samples. https scikit learn. shape 1 print d. Following are the types of Naive Bayes algorithms 1. ReferencesLinks 1. com raahulsaxena tps oct 21 understand random forest parameters Feel free to run your own experiments and upvote if you find this code useful About Naive BayesIt is a classification technique based on Bayes Theorem with an assumption of independence among predictors. Understand the Models You LoveIn this month s TPS I am checking out some new algorithms I came across. com markosthabit tbs november naive bayes by Markos Thabit3. com wp content uploads 2015 09 Bayes_rule 300x172 300x172. In statistics Laplace Smoothing is a technique to smooth categorical data. org stable modules generated sklearn. Bernoulli Used when features are binary in nature 0s and 1s We will use Gaussian NB after transforming our features. svg Analysing the X_train data Experimenting Best Variable TransformationQuantile Transformer works best because it helps variables assume normal distribution useful for NB Var SmoothingPortion of the largest variance of all features that is added to variances for calculation stability. https towardsdatascience. com rayhanlahdji tps 1121 naive bayes for naive souls by Rayhan Lahdji2. org naive bayes classifiers 2. Since our dataset has weakly correlated variables NB might work well here. Gaussian Assumes that features follow a normal distribution. Permutation ImportanceThe permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. com introduction to na C3 AFve bayes classifier fa59e3e24aaf3. I have shared references towards the end. png It perform wells in case of categorical input variables compared to numerical variable s. I am choosing Naive Bayes right now. Converting to Discrete6 bins without any scaling give good results even better than simple quantile transformer scaling on continuous variables. com blog 2017 09 naive bayes explained 4. https machinelearningmastery. com wp content ql cache quicklatex. Importing Packages and Sample Data Splitting Data Making Pipelines with Transformers Gaussian Naive BayesMethods of Improvement http www. Laplace Smoothing is introduced to solve the problem of zero probability. in Feature Importance in Naive Bayes Classifiers 5qob5d5sFWNotebooks 1. ", "id": "raahulsaxena/tps-nov-21-extracting-the-power-of-naive-bayes", "size": "4055", "language": "python", "html_url": "https://www.kaggle.com/code/raahulsaxena/tps-nov-21-extracting-the-power-of-naive-bayes", "git_url": "https://www.kaggle.com/code/raahulsaxena/tps-nov-21-extracting-the-power-of-naive-bayes", "script": "sklearn.metrics sklearn.naive_bayes permutation_importance QuantileTransformer MinMaxScaler percentiler datatable numpy seaborn sklearn.pipeline MultinomialNB scipy.stats fit_distribution sklearn.model_selection matplotlib.pyplot stats pandas Pipeline StandardScaler RobustScaler sklearn.inspection scipy roc_auc_score StratifiedKFold GaussianNB sklearn.preprocessing train_test_split ", "entities": "(('Laplace Smoothing', 'categorical data'), 'be') (('I', 'impact'), 'make') (('com blog 2017 09 naive bayes', '4'), 'explain') (('I', 'new algorithms'), 'understand') (('strong hence we', 'variable transformation'), 'assume') (('Sample Data Splitting Making', 'www'), 'import') (('Laplace Smoothing', 'zero probability'), 'introduce') (('presence', 'other feature'), 'assume') (('NN non models', 'it'), 'be') (('we', 'only current data'), 'note') (('code', 'predictors'), 'tps') (('priors', 'range'), 'calculate') (('We', 'features'), 'use') (('feature when single value', 'model score'), 'define') (('I', '10000 samples'), 'choose') (('types', 'Naive Bayes algorithms'), 'be') (('that', 'normal distribution'), 'follow') (('I', 'end'), 'share') (('Converting', 'continuous variables'), 'give') (('png It', 'variable numerical s.'), 'perform') (('I', 'Random Forest https last month www'), 'do') (('first 39 variables', 'hence them'), 'index') (('that', 'calculation stability'), 'work') (('model', 'feature'), 'break') ", "extra": "['test', 'procedure']"}