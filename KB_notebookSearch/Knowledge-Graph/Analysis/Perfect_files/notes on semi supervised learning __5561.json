{"name": "notes on semi supervised learning ", "full_name": " h1 Notes on semi supervised learning h2 Introduction h2 Demonstration h3 Synthetic benchmark h3 Application benchmark h1 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "circles http scikit learn. However this turned out to be foolish as the class clusters were not sufficiently well distributed. The requisite section of the sklearn documentation http scikit learn. As you can see the performance of this algorithm on this test dataset is almost synonymous with that of LabelPropagation and again note that the misclassified points in 3 and 4 are mostly make_classification artifacts. semi_supervised import LabelPropagation trans LabelPropagation trans. seed 42 y_unlabeled_pred trans. predict X_unlabeled import missingno as msno msno. I would argue that the two problems are just rephrasings of one another semi supervised learning is classifying missing dependent variables while missing data imputation is classifying missing predictor variables. Here I demonstrate the LabelSpreading algorithm. If there are n points in the dataset building a self similarity matrix requires O n 2 comparison operations. sklearn implements a pair of classifiers for this task. org wiki Self similarity_matrix on the dataset then classifying each unlabeled point by finding the existing labeled point it is most similar to. org stable _images sphx_glr_plot_label_propagation_structure_001. It s a useful tool to know about more generally for missing data imputation from a limited sample size but the algorithms have poor performance characteristics for larger samples. Oh well Here s the code for this ConclusionSemi supervised learning is a restatement of the missing data imputation problem which is specific to the small sample missing label case. png On the other hand they are very computationally expensive. If you are familiar with the sklearn style this should seem intimately familiar by now Synthetic benchmarkNow some code In the plots above I show class clusters with different levels of class separation from almost none to almost complete. Note that the presence of a handful of wayward points in the dominant clusters blue and white is actually a suprising artifact of make_classification. Thus these algorithms generalize to a wide variety of spaces like e. Thus these algorithms do not scale well All that being said semi supervised learning is not that different from missing data imputation. The clusters will be tuned to have different degrees of class separation but will basically all be distributed according to the following template Here is the recipe for applying LabelPropagation to the data. This is known as the semi supervised learning problem. These are the semi supervised learning techniques. In this plot I show for each of these levels the performance of the LabelPropagation labeling algorithm how well it performs on each class and also which points exactly it gets wrong. Since imputing missing data is likely to be a vastly more common problem than generalizing labels it is worth keeping these techniques in mind especially for missing data imputation from a small number of samples. As you can see class differenciability is extremely important for successful labeling. This problem gets its own name likely because it is so commonly encountered in research and dataset generation contexts. If the set of labels is sufficiently large standard machine learning algorithms like kNN and SVM classification may be used to assign classes to unlabeled data. So these algorithms can be used to do that as well. If the number of labels available is very small however the performance of these algorithms can be suboptimal. DemonstrationI will demonstrate these algorithms in action. fit X_labeled y_labeled np. Notes on semi supervised learning IntroductionFor some kinds of data you will run into the problem of having many samples but not having labels for all of those samples only for a subset of them. For this reason I include them in my notes on Simple techniques for missing data imputation https www. org stable modules label_propagation. Note that the algorithms provide regularization and kernel hyperparameters that I will not tune or explore here. Application benchmarkI tried applying this algorithm to a real dataset the Open Powerlifting Database to assign Division information to fields lacking it. These are LabelPropagation and LabelSpreading. Respective to using standard machine learning algorithms for this task the one adaptation that these semi supervised algorithms make is that they generalize better when there are very few labels. This hopefully isn t surprising. html semi supervised is a wee bit light on the details but basically 1 LabelSpreading is a regularized version of LabelPropagation and 2 they work by building a self similarity matrix https en. com residentmario simple techniques for missing data imputation. First we ll use the following synthetic dataset. For this synthetic dataset it will not make much of a difference but the higher the level of noise in the dataset the more important parameter tuning becomes. In those cases perhaps try applying machine learning to the problem directly. The accuracy was nearly 0. A set of techniques and algorithms specific to this problem exists. It is semi supervised because it lies in between unsupervised learning which does not use labels and supervised learning which requires them. In a semi supervised learning problem you don t have all the labels or none of them only some of them. This situation occurs particularly often in research contexts where it s often easy to get a small number of labelled data points via hand labelling but significantly harder to gather the full dataset if the full dataset is sufficiently large. ", "id": "residentmario/notes-on-semi-supervised-learning", "size": "5561", "language": "python", "html_url": "https://www.kaggle.com/code/residentmario/notes-on-semi-supervised-learning", "git_url": "https://www.kaggle.com/code/residentmario/notes-on-semi-supervised-learning", "script": "seaborn numpy matplotlib.pyplot missingno label LabelPropagation sklearn.semi_supervised pandas LabelSpreading make_classification sklearn.datasets ", "entities": "(('semi', 'learning problem'), 'know') (('Thus algorithms', 'e.'), 'generalize') (('algorithms', 'larger samples'), 's') (('exactly it', 'class'), 'show') (('it', 'generation contexts'), 'get') (('you', 'them'), 'have') (('So algorithms', 'that'), 'use') (('supervised learning', 'data that imputation'), 'be') (('set', 'specific problem'), 'exist') (('I', 'almost none'), 'seem') (('it', 'existing labeled point'), 'similarity_matrix') (('full dataset', 'significantly full dataset'), 'occur') (('class differenciability', 'extremely successful labeling'), 'be') (('I', 'data imputation https missing www'), 'include') (('sklearn', 'task'), 'implement') (('First we', 'following synthetic dataset'), 'use') (('I', 'hyperparameters'), 'note') (('they', 'other hand'), 'png') (('SVM classification', 'unlabeled data'), 'be') (('Here I', 'LabelSpreading algorithm'), 'demonstrate') (('again misclassified points', '3'), 'be') (('self similarity matrix', 'comparison n 2 operations'), 'require') (('These', 'learning techniques'), 'be') (('requisite section', 'sklearn documentation http scikit'), 'learn') (('DemonstrationI', 'action'), 'demonstrate') (('which', 'label small sample missing case'), 's') (('missing', 'predictor missing variables'), 'argue') (('2 they', 'self similarity matrix https'), 'be') (('it', 'samples'), 'be') (('Application benchmarkI', 'it'), 'try') (('available very however performance', 'algorithms'), 'suboptimal') (('supervised which', 'them'), 'supervised') (('presence', 'blue actually suprising make_classification'), 'note') (('parameter more important tuning', 'dataset'), 'make') (('they', 'task'), 'be') (('you', 'them'), 'supervised') (('Here recipe', 'data'), 'tune') ", "extra": "['test']"}