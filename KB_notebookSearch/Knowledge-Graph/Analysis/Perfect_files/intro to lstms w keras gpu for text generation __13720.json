{"name": "intro to lstms w keras gpu for text generation ", "full_name": " h1 Applied Introduction to LSTMs with GPU for text generation h2 Part one Data Preparation h3 Read in the data h3 Explore the data h3 Prepare sequence data for input to LSTM h4 Subset the data to form a corpus h4 Format the corpus into arrays of semi overlapping sequences of uniform length and next characters h4 Represent the sequence data as sparse boolean tensors h2 Part two Modeling h3 Defining an LSTM network model h3 Training the model and generating predictions h2 Conclusion h3 Inspiration for next steps ", "stargazers_count": 0, "forks_count": 0, "description": "Subset the data to form a corpus2. Represent the sequence data from 2 as sparse boolean tensors Subset the data to form a corpusIn the next two cells we ll grab only messages from 55a7c9e08a7b72f55c3f991e fromUser. to numeric indices in the cell below. Since we don t care if our model generates text with correct capitalization we use tolower. And the next character following the sequence folks. just doing the new signee stuf is the character f to finish the word stuff. Defining an LSTM network model2. And I can only assume the sentence starting With all of the various frameworks. One of my frustrations with following non interactive tutorials e. Lower values of temperature e. Read more about why to use Keras as a deep learning framework here https keras. I m also just going to use the first 20 of the data as a sample since we don t need more than that to generate halfway decent text. 2 will generate safe guesses whereas values of temperature above 1. There are two sections to this part 1. The shape we define for our input is identical to our data by this point which is exactly what we need. In any case you will still want to grab some lunch or go for a walk while you wait for the model to train and generate predictions if you re running this code interactively. com learn deep learning a series of videos and hands on notebook tutorials in Kernels. Try out more complicated network architectures like adding dropout layers. 2 for the values of temperature feel free to tweak these too. I m using Keras which is a popular and easy to use interface to a TensorFlow backend. Read in the dataLooks good Explore the dataIn my plot below you can see the number of posts from the top ten most active chat participants by their user id in freeCodeCamp s Gitter So userid 55a7c9e08a7b72f55c3f991e is the most active user in the channel with over 140 000 messages. There are two parts to this notebook 1. 1 to optimize the weights in our model you can experiment with different learning rates here and categorical_crossentropy as our loss function. This is pretty far from the 3D shape the input layer of our LSTM network requires model. Represent the sequence data as sparse boolean tensorsThe next cell will take a few seconds to run. If you want to review more of the theoretical underpinnings I recommend that you check out this excellent blog post Understanding LSTM Networks http colah. just doing the new signee stuff. We ll use RMSprop with a learning rate of 0. Part two ModelingIn part two we do the actual model training and text generation. is referring to JavaScript. I ve printed out the first 10 strings in the array so you can see we re chunking the corpus into partially overlapping equal length sentences. Note that we re using our model to predict based on a random sequence or seed from our original subsetted data user start_index random. Using epoch 1 to be consistent with the training epochs printed by Keras define the checkpoint fit model using our gpu. You can see how the next character following the first sequence hi folks. Finally we name our callback function generate_text which we ll add to the list of callbacks when we fit our model in the cell after this one. If you re running this interactively in your own notebook you can click the blue square Stop button next to the console at the bottom of your screen to interrupt the model training. Before we feed it any data the cell below defines a couple of helper functions with code modified from this script https github. Now we can compile our model. Now that you can use GPUs in Kernels with 6 hours of run time you can train much more computationally intensive models than ever on Kaggle. We start with a sequential model and add an LSTM as an input layer. In this way it should be clear now how next_chars is the data labels or ground truth for our sequences in sentences and our model trained on this labeled data will be able to generate new next characters as predictions given sequence input. I ll use a GPU to train the model in this notebook you can request a GPU for your session by clicking on the Settings tab from a kernel editor. Training the model and generating predictions Defining an LSTM network modelLet s start by reading in our libraries. id 55a7c9e08a7b72f55c3f991e to subset the data and collapse the vector of strings into a single string. Below you can see the models layers optimizers and callbacks we ll be using. randint 0 len user maxlen 1. Format the corpus into arrays of semi overlapping sequences of uniform length and next charactersThe rest of the code used here is adapated from this example script https github. Prints generated text. The two nice things about this tutorial using Kernels is that a I ll try to give you glimpses into the data at every significant step and 2 you can always fork this notebook and boom you ve got a copy of my environment data Docker image and all with no downloads or installs necessary whatsoever. Finally we ll use add an activation layer with softmax as our activation function as we re in essence doing multiclass classification to predict the next character in a sequence. Anyway so the second is defining a callback function to print out predicted text generated by our trained LSTM at the first and then every subsequent fifth epoch with five different settings of temperature each time see the line for diversity in 0. ConclusionAnd there you have it If you ran this notebook interactively you hopefully caught the model printing out generated text character by character to dramatic effect. Applied Introduction to LSTMs with GPU for text generationIn this Python notebook kernel I will use the text from freeCodeCamp s Gitter chat logs https www. This way we can fiddle with the temperature knob to see what gets us the best generated text ranging from conservative to creative. We ll use their messages to train the LSTM to generate novel 55a7c9e08a7b72f55c3f991e like sentences. Prepare sequence data for input to LSTMRight now we have a dataframe with columns corresponding to user ids and message text where each row corresponds to a single message sent. So how do we get from a dataframe to sequence data in the correct shape I ll break it into three steps 1. py originally written by Fran\u00e7ois Chollet author of Keras and Kaggler to prepare the data in the correct format for training an LSTM. Reading in exploring and preparing the data2. You can try forking this kernel and experimenting with more or less data if you want. Cross entropy is the same as log loss commonly used as the evaluation metric in binary classification competitions on Kaggle except in our case there are more than two possible outcomes. So we ll expect our novel sentences to look roughly like this if we re successful. The first one sample samples an index from an array of probabilities with some temperature. When temperature is low we may get lots of the s and and s when temperature is high things get more unpredictable. Since we re training a character level model we relate unique characters e. We have another callback ModelCheckpoint which will save the best model at each epoch if it s improved based on our loss value find the saved weights file weights. The shape we end up with will be input_shape maxlen len chars where maxlen is 40 and len chars is the number of features i. You can see how our model improved from the first epoch to the last. com keras team keras blob master examples lstm_text_generation. com datasets sortBy hottest group public page 1 pageSize 20 size all filetype all license all tagids 11208. characters in our case. I ve selected a batch_size of 128 which is the number of samples or sequences our model looks at during training before updating. You have to download it and compare it locally which is a pain. Yep sounds like they re on topic as far as freeCodeCamp goes. We re creating a sparse boolean tensors x and y encoding character level features from sentences and next_chars to use as inputs to the model we train. Compare the speed up effect of using a CPU versus a GPU on a minimal example. But first let s take a look at the first few messages from 55a7c9e08a7b72f55c3f991e to get a sense for what they re chatting about I see words and phrases like documentation pair coding BASH Bootstrap CSS etc. If you rerun this code yourself by clicking Fork Notebook you can print out all of the characters used. Now our model is ready. io posts 2015 08 Understanding LSTMs. Especially if you have experience installing CUDA to use GPUs for deep learning you ll appreciate how wonderful it is to have an environment already setup for you. And overall lower levels of diversity generate text with a lot of repetitions whereas higher levels of diversity correspond to more gobbledegook. By the end you ll learn how to format text data as input to a character level LSTM model implemented in Keras and in turn use the model s character level predictions to generate novel sequences of text. This next cell step gives us an array sentences made up of maxlen 40 character sequences chunked in steps of 3 characters from our corpus user and next_chars an array of single characters from user at i maxlen for each i. I m also adding a dense output layer. But first what is an LSTM Long Short Term Memory network anyway In this notebook we ll take a hands on approach to implementing this flavor of recurrent neural network especially equipped to handle longer distance dependencies including ones you get with language in Keras a deep learning framework. add LSTM batch_size input_shape time_steps features where batch_size is the number of sequences in each sample can be one or more time_steps is the size of observations in each sample and features is the number of possible observable features i. Format the corpus from 1 into arrays of semi overlapping sequences of uniform length and next characters3. Training the LSTM on a single user id s chat logs and generating novel text as outputYou can follow along by simply reading the notebook or you can fork it click Fork notebook and run the cells yourself to learn what each part is doing interactively. This give the model one less dimension to learn. You can experiment with different numbers here if you want. Quick pause to ask what is temperature exactly Temperature is a scaling factor applied to the outputs of our dense layer before applying the softmax activation function. We ve explored the data and reshaped it correctly so we that we can use it as an input to our LSTM model. The text generated by the model s predictions in the first epoch didn t really resemble English at all. static code shared on GitHub is that it s often hard to know how the data you want to work with differs from the code sample. Learn more about deep learning on Kaggle Learn https www. Experiment with different hyper parameters like the amount of training data number of epochs or batch sizes temperature etc. Training the model and generating predictionsFinally we ve made it Our data is ready x for sequences y for next characters we ve chosen a batch_size of 128 and we ve defined a callback function which will print generated text using model. 0 will start to generate riskier guesses. com free code camp all posts public main chatroom to train an LSTM network to generate novel messages click on the Data tab of this kernel to view the data preview. hdf5 in the Output to predict based on different data in a new kernel what it would be like if the user in this tutorial completed someone else s sentences. And of course let s not forget to put our GPU to use This will make training prediction much faster than if we used a CPU. Part one Data PreparationIn part one I ll first read in the data and try to explore it enough to give you a sense of what we re working with. Try out the same code with different data fork this notebook go to the Data tab and remove the freeCodeCamp data source then add a different dataset good examples here https www. org tutorials using_gpu allowing_gpu_memory_growth Read in only the two columns we need We don t want bots user id for CamperBot Show first 100 characters helper function to sample an index from a probability array Function invoked for specified epochs. is the character h to start the word hello. Think of it as the amount of surpise you d have at seeing an English word start with st versus sg. I ll use text from one of the channel s most prolific user ids as the training data. predict at the end of the first epoch followed by every fifth epoch with five different temperature setting each time. I hope you ve enjoyed learning how to start from a dataframe containing rows of text to using an LSTM model implemented using Keras in Kernels to generate novel sentences thanks to the power of GPUs. unique count of characters from our corpus. Can you tweak the model or its hyperparameters to generate even better text Try it out for yourself by forking this notebook kernel click Fork Notebook at the top. Inspiration for next stepsHere are just a few ideas for how to take what you learned here and expand it 1. In the cell below we define the model. hdf5 in the Output tab of the kernel. Let s fit our model with these specifications and epochs 15 for the number of epochs to train. In a nutshell it defines how conservative or creative the model s guesses are for the next character in a sequence. ", "id": "mrisdal/intro-to-lstms-w-keras-gpu-for-text-generation", "size": "13720", "language": "python", "html_url": "https://www.kaggle.com/code/mrisdal/intro-to-lstms-w-keras-gpu-for-text-generation", "git_url": "https://www.kaggle.com/code/mrisdal/intro-to-lstms-w-keras-gpu-for-text-generation", "script": "ModelCheckpoint on_epoch_end keras.layers keras.callbacks numpy matplotlib.pyplot RMSprop Activation Dense tensorflow LambdaCallback pandas sample keras.optimizers keras.models Sequential LSTM ", "entities": "(('len chars', 'features i.'), 'be') (('we', 'model'), 'create') (('cell next step', 'i.'), 'give') (('next cell', 'a few seconds'), 'take') (('you', 'GPUs'), 'hope') (('as far freeCodeCamp', 'topic'), 'sound') (('we', 'length partially overlapping equal sentences'), 'print') (('com keras team keras', 'blob master examples'), 'lstm_text_generation') (('2', 'these'), 'feel') (('exactly Temperature', 'softmax activation function'), 'pause') (('you', 'model training'), 'click') (('you', 'loss here function'), 'experiment') (('you', 'more data'), 'try') (('text', 'really English'), 'resemble') (('input layer', 'model'), 'be') (('ModelingIn Part two two we', 'model actual training'), 'part') (('so second', '0'), 'define') (('We', '55a7c9e08a7b72f55c3f991e sentences'), 'use') (('how it', 'already you'), 'appreciate') (('locally which', 'it'), 'have') (('is', 'sequence input'), 'be') (('character h', 'word'), 'be') (('com', 'Kernels'), 'learn') (('you', 'code'), 'want') (('model', 'training'), 'select') (('much faster we', 'CPU'), 'let') (('So 55a7c9e08a7b72f55c3f991e', '140 over 000 messages'), 'read') (('we', 'tolower'), 'use') (('part', 'yourself what'), 'train') (('I', 'three steps'), 'get') (('how model', 'last'), 'see') (('which', 'TensorFlow backend'), 'use') (('don t', 'halfway decent text'), 'go') (('when we', 'one'), 'name') (('We', '0'), 'use') (('you', 'top'), 'tweak') (('0', 'riskier guesses'), 'start') (('we', 'script https github'), 'feed') (('often how you', 'code sample'), 'be') (('we', 'unique characters'), 'train') (('t', 'specified epochs'), 'using_gpu') (('I', 'documentation pair coding BASH Bootstrap CSS etc'), 'let') (('you', 'Kaggle'), 'now') (('you', 'characters'), 'print') (('high things', 's'), 'get') (('you', 'Docker all downloads'), 'be') (('we', 'what'), 'part') (('I', 'training data'), 'use') (('you', '1'), 'be') (('we', 'roughly this'), 'expect') (('commonly evaluation', 'case'), 'be') (('first one sample', 'temperature'), 'sample') (('message where row', 'single message'), 'have') (('This', 'one less dimension'), 'give') (('one time_steps', 'i.'), 'add') (('which', 'model'), 'make') (('we', '55a7c9e08a7b72f55c3f991e fromUser'), 'represent') (('what', 'best generated text'), 'way') (('We', 'input layer'), 'start') (('notebook', 'then different dataset good examples'), 'try') (('higher levels', 'more gobbledegook'), 'generate') (('Python notebook generationIn I', 'Gitter chat logs https www'), 'Applied') (('we', 'exactly what'), 'be') (('here you', 'different numbers'), 'experiment') (('i', 'single string'), 'd') (('Using', 'gpu'), 'define') (('user', 'else sentences'), 'hdf5') (('filetype', 'tagids'), 'dataset') (('we', 'models layers optimizers'), 'see') (('15 number', 'epochs'), 'let') (('we', 'sequence'), 'use') (('Understanding LSTM Networks', 'colah'), 'want') (('how guesses', 'sequence'), 'define') (('You', 'how next first sequence'), 'see') (('you', 'text'), 'learn') (('2', '1'), 'generate') (('I', 'various frameworks'), 'assume') (('it', 'weights file saved weights'), 'have') (('all', 'data preview'), 'com') (('you', 'learning deep framework'), 'be') (('interactively you', 'dramatic effect'), 'have') (('you', 'kernel editor'), 'use') (('you English word', 'sg'), 'think') (('Format', 'example script https here github'), 'adapate') (('we', 'data original subsetted user'), 'note') (('correctly so we', 'LSTM model'), 'explore') ", "extra": "['outcome', 'test']"}