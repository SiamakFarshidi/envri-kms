{"name": "wheat detection eda augmentation fasterrcnn ", "full_name": " h1 Wheat Spike Detection h1 Wheat Spike Detection h1 h1 Basic Data Exploration h1 Basic Data Exploration h1 h2 How are the images distributed by source h2 How about em wheat heads h1 Looking into the box h1 Image Augmentation h1 Image Augmentation h1 h1 Train Model h1 Train Model h1 ", "stargazers_count": 0, "forks_count": 0, "description": "We will look at images in these 3 ranges. The first is when we want to start from a pre trained model and just finetune the last layer. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. Unfortunately R CNN becomes rather slow due to these multiple steps involved in the process. The first step is to write a function to prepare the dataset. Next we define the dataloaders. Thus being able to detect and characterize spikes from images of wheat fields is an essential component in a wheat breeding pipeline for the selection of high yielding varieties. We will know better once we look at the images. com 2015 01 wheat starch image 825x338. Finally these maps are classified and the bounding boxes are predicted. I am going to try my best to explain the proces in the most layman terms. Since this is my first Kaggle competition and I have not really done object detection analysis I am going to use the Albumentation library to play around a little bit to understand the different features available. I am very new to DL and object detection. labels for each bounding box background is always 0. R CNN extracts a bunch of regions from the given image using selective search and then checks if any of these boxes contains an object. Below steps sumarize the steps Take an input image and pass it to the ConvNet which returns feature maps for the image Apply Region Proposal Network RPN on these feature maps and get object proposals Apply ROI pooling layer to bring down all the proposals to the same size Finally pass these proposals to a fully connected layer in order to classify any predict the bounding boxes for the imageThe Faster R CNN model will be implemented with PyTorch. These include but not limited to flips adding blur increase sharpness and more. We first extract feature maps from the input image using ConvNet and then pass those maps through a RPN which returns object proposals. We also have heads with the shoe background. Also an other thing to note is there is a clear demarcation in the brightness of the pictures. I can keep playing with this all day But it is time to move on Train Model We will train a FasterRCNN model for this analysis. It will be helpful if we take care of it now since we have to analyze this later. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session import required libraries plotly libraries initialize source paths read data table expand the bbox column into seprate columns add xmax ymax and area columns for bounding box count distinct images by source Change the bar mode create list of images per the regions identified define a function to display the images define functions for augmentation and display i already have the area in my dataframe there is only one class so all will be 1 suppose all instances are not crowd define transformation functions load an instance segmentation model pre trained pre trained on COCO get number of input features for the classifier replace the pre trained head with a new one split the dataset in train and test set train on the GPU or on the CPU if a GPU is not available define the number of classes one for wheat and one for background get the model using our helper function move model to the right device construct an optimizer for epoch in range num_epochs train for one epoch printing every 10 iterations update the learning rate evaluate on the test dataset evaluate model valid_data_loader device device. adding weather aspects to the data. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. It will help us to know how many images are there what are the sources for the data image sizes bounding box information etc. How are the images distributed by source How about em wheat heads The ethz_1 source has a significant number of wheat heads per image approximately 69 per image. In this section we will look at the bounding boxes. Deep neural networks need a lot of data to be effective. com albumentations team albumentations and is a really good resource for beginners. The maximum area is approximately 500 000. read_csv Input data files are available in the read only. There is a fantastic library called Albumentations that helps in creating augmenations quickly and effectively within a few lines of code. area area of the bounding box 5. That is where image augmentation comes into play. Finally these features are then used to detect objects. As stated earlier these will have an impact on the IOU and may have to be dealt with. The recent development in technology has also enabled ML to step into the realm of agriculture. The other is when we want to replace the backbone of the model with a different one for faster predictions for example. a target dictionary that needs the following mandatory fields 1. Fast R CNN on the other hand passes the entire image to ConvNet which generates regions of interest instead of passing the extracted regions from the image. Also instead of using three different models as we saw in R CNN it uses a single model which extracts features from the regions classifies them into different classes and returns the bounding boxes. The function below will do all of the above. In this case we will use the pre trained model and finetune the last layer since our dataset is not that large. Image analysis has significantly enhanced the potential for achieving high throughput analysis of crop fields. As a first step we will create the individual bbox columns and also create the bounding box area column. Now let s look at the overall distribution for the wheat heads by image. Now we setup the model. It is the process of creating more images from the existing training data by applying transformations. These steps are pretty self explanatory. Basic Data Exploration First we will look at the data source to get an intial understanding of what is there. coordinates of the bounding boxes 2. Maybe these are very small and will be highly concentrated. The area becomes important for IOU detection. And has also enabled in detecting diseases in the crops at a very initial stage. The github link is here https github. There are some images with a counts greater than 100 and as less as 1. All these steps are done simultaneously thus making it execute faster as compared to R CNN. For the images with less than 5 wheat heads we see these are very small heads. Specifically the area distribution and how many of these narrow boxes we have in the dataset. For wheat breeding purposes assessing the production of wheat spikes as the grain bearing organ is a useful proxy measure of grain production. Looking into the box. jpg Machine learning has a lot of applications in various industries. We have some exteremly large bounding boxes with area greater than 100 000. So we will start only with a flip for the train. Fast R CNN is however not fast enough when applied on a large dataset as it also uses selective search for extracting the regions. This is could be one of those wheat heads that is still forming. Next we split the data into a train_set 2 708 images and a validation set 665 images. The library was created by Kaggle Grandmasters and has helped win Kaggle competitions. Some of these are very helpful to increase the accuracy of the models. Note that the validation set should not be augmented There are two common situations where one might want to modify one of the available models in torchvision modelzoo. image id unique identifier 4. We first extract these regions and for each region CNN is used to extract specific features. The IQR is between 28 and 59 wheat heads per image with a median value of 43. Nevertheless let s a take a look at some of the images with both types of bounding boxes. The minimum size required is a 800 x 800 image. The data prep step in PyTorch requires three main parameters an image with size h and w. The smallest are of the bounding box is 2. Wheat Spike Detection https cdn. There are some really large boxes that don t make a lot sense Image Augmentation We have only about 3 300 images in the training data. And Some of them are on the ground and a few are not clearly visbible. The images seem sharper but we need to look at more of these and also analyze those parameters. We will have to use some augmentation techniques for these. We have already seen that the albumentations library is a good resource. The images with count between 30 and 60 look good. Faster R CNN fixes the problem of selective search by replacing it with Region Proposal Network RPN. Some of them are dark and some of them are light. Wow those look cluttered but they are a good source of information. The bounding box area distribution is extremely skewed. The spikes stand out This is really cool. iscrowd instacnces with iscrowd True will be ignored i don t know what this means and have set it to False In addition we can add masks if available and also specify transformations. ", "id": "ajaysamp/wheat-detection-eda-augmentation-fasterrcnn", "size": "7875", "language": "python", "html_url": "https://www.kaggle.com/code/ajaysamp/wheat-detection-eda-augmentation-fasterrcnn", "git_url": "https://www.kaggle.com/code/ajaysamp/wheat-detection-eda-augmentation-fasterrcnn", "script": "albumentations torch.utils.data __init__ torchvision.models.detection.faster_rcnn plotly.offline torchvision.models.detection.rpn get_train_transform get_aug SequentialSampler DataLoader matplotlib.patches ToTensorV2 FasterRCNN iplot collate_fn numpy plotly.graph_objs __getitem__ torchvision.models.detection get_valid_transform visualize_bbox PIL plotly.express WheatDataset(Dataset) AnchorGenerator get_bbox Image matplotlib.pyplot train_one_epoch pandas albumentations.pytorch.transforms FastRCNNPredictor Dataset plotly.io __len__ get_model_instance_segmentation plot_image visualize init_notebook_mode aug_plots torch.utils.data.sampler engine ", "entities": "(('what', 'intial understanding'), 'look') (('jpg Machine learning', 'various industries'), 'have') (('when we', 'example'), 'be') (('ethz_1 source', 'approximately image'), 'be') (('earlier these', 'IOU'), 'have') (('Now s', 'image'), 'let') (('that', 'wheat heads'), 'be') (('Thus being', 'high yielding varieties'), 'be') (('We', 'analysis'), 'keep') (('it', 'regions'), 'be') (('We', '3 ranges'), 'look') (('R Faster CNN', 'Region Proposal Network RPN'), 'fix') (('Also other thing', 'pictures'), 'be') (('I', 'layman most terms'), 'go') (('simultaneously thus it', 'R faster CNN'), 'do') (('We', 'area'), 'have') (('Next we', '665 images'), 'split') (('which', 'image'), 'CNN') (('there sources', 'data image box information'), 'help') (('library', 'Kaggle competitions'), 'create') (('few', 'ground'), 'be') (('So we', 'train'), 'start') (('R Unfortunately CNN', 'process'), 'become') (('We', 'these'), 'have') (('when we', 'just last layer'), 'be') (('These', 'blur increase sharpness'), 'include') (('fantastic that', 'code'), 'be') (('lot', 'data'), 'need') (('target that', 'following mandatory fields'), 'dictionary') (('data prep step', 'size h'), 'require') (('10 iterations', 'test dataset evaluate device model device'), 'list') (('Some', 'models'), 'be') (('are', 'bounding box'), 'be') (('images', 'between 30'), 'look') (('they', 'good information'), 'look') (('function', 'above'), 'do') (('these', 'wheat less than 5 heads'), 'see') (('we', 'if also transformations'), 'ignore') (('now we', 'this'), 'be') (('we', 'also parameters'), 'seem') (('better we', 'images'), 'know') (('labels', 'bounding box background'), 'be') (('area', 'IOU detection'), 'become') (('which', 'PyTorch'), 'take') (('recent development', 'agriculture'), 'enable') (('We', 'training data'), 'be') (('read_csv Input data files', 'read'), 'be') (('IQR', '43'), 'be') (('grain', 'grain useful proxy production'), 'be') (('Nevertheless s', 'boxes'), 'let') (('which', 'object proposals'), 'extract') (('I', 'detection'), 'be') (('It', 'kaggle python Docker image https github'), 'come') (('It', 'transformations'), 'be') (('Image analysis', 'crop fields'), 'enhance') (('We', 'shoe background'), 'have') (('CNN', 'specific features'), 'extract') (('then any', 'object'), 'extract') (('first step', 'dataset'), 'be') (('we', 'bounding box area also column'), 'create') (('image where augmentation', 'play'), 'be') (('Finally features', 'then objects'), 'use') (('we', 'dataset'), 'distribution') (('two common where one', 'torchvision modelzoo'), 'note') (('different features', 'little bit'), 'be') (('some', 'them'), 'be') (('we', 'bounding boxes'), 'look') (('dataset', 'last layer'), 'use') (('which', 'bounding boxes'), 'CNN') ", "extra": "['disease', 'test']"}