{"name": "who needs a gpu shallow learning in julia ", "full_name": " h1 COLLABORATION WELCOME h2 CHANGELOG h3 Update 2021 03 27 h2 Introduction h3 Goals h3 Backstory h2 Installing Julia h2 Installing home baked dependencies h1 TODO Walk through individual components h1 Putting it all together at least the Julia part h2 Modeling in Python h2 Conclusion h3 Opportunities for future work ", "stargazers_count": 0, "forks_count": 0, "description": "First it s written purely in Julia not Python. If this sounds interesting to you please continue reading GoalsThe primary goal of this notebook is to take a crack at the competition from scratch segmentation and prediction with a an atypical set of methods and a different programming language and package ecosystem. Thus the goals above came about naturally. Instead it s much easier to export a more standard RLE encoding of the masks into JSON from Julia and then to use Python to re encode via the function provided by the organizers and the pycocotools library. The Python code below basically goes through the typical GBM baseline approach to non computer vision Kaggle competitions. One simple approach would be to get a set of cross validated predictions using the full data and then drop the labels from cells whcih are predicted with significantly lower confidence than the other cells in the same image. csv and parse multihot encode labels resulting in columns id 0 1. TODO Walk through individual componentsIn this latest rounds of updates I m pushing to getting my work running end to end including submission of predictions. However I have started losing steam on this project and doing the remainder in Julia would take much more work than just using Python. It can run offline thanks to the downloads being moved to another notebook. pairs of classes that show up together and training a model that predicts the presence absence of the interaction. cannot use precompiled packages with pyjulia on linux but we can use a system image with PyCall pre compiled to speed that up confirm we re using more than one thread restore channel axes if missing drop er channel if we re skipping it optionally rescale each channel so its color histogram matches that of the average across all channels red green and blue are channels directly we treat yellow is as an even mix of red and green by adding to both channels downscale image segment nuclei using marker based watershep segment full cells upscale segmentation maps create evenly spaced colors palette using HSV across various brightness levels define function to max out saturation of nuclei pixels local modules angle slices squeeze univariate summaries of pixel information quantiles bi channel correlations protein speckle information drop background which is value 0 drop empty fraction of total speckles in this region copy the cell nucleus summary if no nonnucleus region exists happens when cell on edge of image only push the RLE mask if featurization succeeds load train. I ve moved the modeling over to LightGBM in Python. I ve implemented some hefty feature engineering including spatially aware features in Julia. In order to make this notebook work offline I separated most of the installation steps into another notebook which you can take a look at if you re curious https www. Since it s clunky actually developing Julia in Kaggle notebooks no autocomplete can t interrup long running computation etc. To any more veteran members of the Julia community who are reading through I would deeply appreciate any feedback and constructive criticism on my code. When it comes to improving my submission I think there are some fairly simple things to try adding to the GBM modeling. As output we get feature vectors for all the cells identified by segmenting all the train and test images as well as JSON files which contain RLE encoded segmentation masks for all the test cells. Unfortunately this buries the logic for segmentation a bit. Unfortunately with my more serious feature enginering code I now have over 500 features per image about 30x as many features as before. The Hessian inversion in the full Newton Raphson solver could cost 30 3 27 000 times the compute and so it doesn t seem feasible to continue using that solver. I hope this work is inspiring to the Julia community. It seems possible https github. Modeling in PythonI ve decided to switch over to Python for the modeling section here at the end. The secondary goal is to try and run everything fast on a CPU. Why I have now completed Julia code to segment the images and compute spatially aware statistics that encode each cell as a fixed length numeric feature vector and the last step of fitting a model is relatively small after all. It s harder but possible to write Julia code for GPU than CPU anyhow and as I m just starting to figure things out I figured I d try to make my approach efficient enough to run on a laptop CPU. I m still a beginner trying to learn and every tip helps I also hope this work is inspiring to folks who aren t afraid to think creatively in the face of cookie cutter deep learning model frameworks. There s also the interesting technical challenge of seeing how much of my Julia code I can port over to run on GPU. precompile trigger parallel processing of load segment extract featurize remove empty results create dataframe of features giant list of masks parallel processing of load segment extract featurize remove empty results create dataframe of features giant list of masks JSON requires the entire file to be read at once so we need to chunk it if we want the reader to be able to read some at a time load and align the training data load test data load and re encode masks note it s faster to convert the lists to numpy arrays rather than having the numba jitted code take in Python lists convert input mask to expected COCO API input RLE encode mask compress and base64 encoding Train one model per class model_params dict n_estimators 1_000 learning_rate 0. I m releasing my code and ideas publicly here but if you use significant parts of this to make your solution better please consider inviting me to your team _ CHANGELOG Update 2021. It would be good to take into account the correlation between classes and the fact taht when multiple classes are present together the features look different than when the classes are present individually. For one I m familiar with several popular ML libraries in Python but I m not yet familiar with any of the Julia ML ecosystem. I ve been a career long Python programmer and this is only my second project with the language my first being a library to download and parse stock data from IEX https github. Sadly even though Julia is the Ju in Ju pyter Kaggle no longer supports Julia Kernels directly so we have to use Julia through a Python kernel which is a bit painful. 9 subsample_freq 1 small model for testing train models on each class run predictions create submission let s choose thresholds so that the rate we guess a particular label is a fixed multiple of the class frequency. We load the features into DataFrames feed that into LightGBM and boom we get predictions. I started working locally on my laptop which means no GPU. com CellProfiling HPA Cell Segmentation project but it gives solid results and runs quite efficiently 1sec to segment a 400x400 resized image on a single core of a laptop CPU most of the runtime is actually from doing higher resolution feature computation. com lukemerrick julia download. Not to devalue all the creativity and endless tweaking that goes into making a competitive deep learning submission but I hope more folks out there will try more way out there approaches like this. 27I ve overhauled the notebook to be aimed more at an actual submission. 02 num_leaves 127 colsample_bytree 0. Second it uses no deep learning. Putting it all together at least the Julia part Below is a script that handles the loading segmentation and feature encoding of all the cell images. Additionally it seems pretty nonsensical to try and re implement in Julia the complicated segmentation mask encoding function needed for submission. Filtering the training data would be good too since a lot of cells in images may not actually match the label for the whole image. A less unconventional next step would be to get some autograd and convolutions going but in Julia rather than Python. There is a little nuance in that we train one model per class independently a naive approach ignoring class interaction ConclusionSo here we have it no pre training no GPU all thanks to Julia. I show off my RLE encoding decoding skills to move segmentation masks from Julia to Python I use numba to JIT compile Python code to get a 100x speedup for reading RLE masks IntroductionThis notebook is a bit atypical for a Kaggle image competition. 1 subsample 0. Already I feel that I can try out so many things that just aren t feasible in Python numpy like non ML segmentation algorithms hand implemented logistic regression code and building custom visualizations of images. It would probably require re implementing the underlying Watershed algorithm from ImageSegmentation. com lukemerrick InvestorsExchange. jl seems like a fun time but also like another big chunk of work to dive into so I ll have to see. COLLABORATION WELCOME If the ideas code features or predictions below are interesting or helpful to you please reach out Given how different this appraoch is to the common Python Deep Learning approach there might be opportunity for us to team up and ensemble our models. In a previous version of this notebook I just used a Newton Raphson Logistic Regression solver I wrote from scratch in Julia. In future updates I plan to come back and show step by step how the segmentation and feature engineering work. Certainly I doubt my segmentation code gives results as clean as those from the offical HPA Cell Segmentation https github. Opportunities for future workCollaboration welcome The first thing I want to do is go back and explain what all the Julia code does with lots of images showing the intermediate steps of segmentation spatial feature engineering etc. BackstoryI m new to the Julia programming language but I m very excited about it. com louismullie watershed cuda. jl https juliaimages. I got interested in this competition from a friend just after wrapping up my first project in Julia and I thought to myself hey let s take a crack at doing this in Julia just as a learning exercise. One thing to try would be finding the top target interactions i. Installing JuliaBefore we can run any code we have to install the Julia language. 20 imagesegmentation which seems both daunting and interesting. I m planning to come back and run the lower level functions and better explain the segmentation details but for now I m working to getting things going end to end. Installing home baked dependenciesIn addition to making a semi polished public package I also created modules to organize utility functions. ", "id": "lukemerrick/who-needs-a-gpu-shallow-learning-in-julia", "size": "8720", "language": "python", "html_url": "https://www.kaggle.com/code/lukemerrick/who-needs-a-gpu-shallow-learning-in-julia", "git_url": "https://www.kaggle.com/code/lukemerrick/who-needs-a-gpu-shallow-learning-in-julia", "script": "reduce sklearn.metrics multiprocessing lightgbm Path average_precision_score numba numpy binary_mask_to_ascii pathlib njit fast_rle_to_mask _rle_to_mask_1d sklearn.model_selection cpu_count rle_to_mask KFold in enumerate(tqdm(target_df.columns)) functools pycocotools julia.api pandas tqdm _mask reencode_mask Julia _mask as coco_mask ", "entities": "(('which', 'test cells'), 'encode') (('Julia code', 'feature engineering segmentation spatial etc'), 'welcome') (('approach', 'laptop enough CPU'), 's') (('I', 'utility functions'), 'bake') (('that', 'interaction'), 'pair') (('who', 'code'), 'appreciate') (('secondary goal', 'fast CPU'), 'be') (('GoalsThe primary goal', 'methods'), 'continue') (('t', 'long computation'), 'develop') (('I', 'GPU'), 's') (('I', 'Julia ML ecosystem'), 'for') (('Modeling', 'here end'), 'decide') (('I', 'step'), 'plan') (('t', 'solver'), 'cost') (('Below that', 'feature cell images'), 'be') (('One simple approach', 'same image'), 'be') (('autograd', 'rather Python'), 'be') (('featurization', 'load train'), 'use') (('solution', 'team'), 'm') (('when classes', 'classes'), 'be') (('ConclusionSo here we', 'thanks Julia'), 'be') (('It', 'ImageSegmentation'), 'require') (('It', 'notebook'), 'run') (('I', 'Julia'), 'use') (('segmentation code', 'HPA Cell Segmentation https offical github'), 'doubt') (('work', 'predictions'), 'walk') (('we', 'Julia language'), 'run') (('Unfortunately this', 'segmentation'), 'bury') (('First it', 'purely Julia'), 'write') (('27I ve', 'more actual submission'), 'overhaul') (('I', 'GBM fairly simple modeling'), 'think') (('too lot', 'whole image'), 'be') (('first', 'stock IEX https parse github'), 'be') (('One thing', 'target top interactions'), 'find') (('which', 'Python kernel'), 'support') (('I', 'as many features'), 'have') (('I', 'Julia'), 'implement') (('work', 'Julia community'), 'hope') (('particular label', 'class fixed frequency'), 'create') (('IntroductionThis notebook', 'Kaggle image bit competition'), 'show') (('losing', 'just Python'), 'start') (('I', 'over Python'), 'move') (('just aren', 'images'), 'feel') (('us', 'models'), 'welcome') (('I', 'work'), 'seem') (('we', 'predictions'), 'load') (('you', 'https curious www'), 'separate') (('s', 'learning just exercise'), 'get') (('I', 'this'), 'devalue') (('i', 'multihot encode parse columns'), 'csv') (('Python code', 'computer vision Kaggle competitions'), 'go') (('I', 'very it'), 'm') (('Train one model', '1_000 learning_rate'), 'featurize') (('who', 'learning model cookie cutter deep frameworks'), 'm') (('Additionally it', 'submission'), 'seem') (('which', 'GPU'), 'start') (('things', 'end'), 'm') (('most', 'resolution feature actually higher computation'), 'com') (('that', 'model'), 'be') (('Instead it', 'organizers'), 's') ", "extra": "['test']"}