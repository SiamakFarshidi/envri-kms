{"name": "deeplearninginbme assignment2 ", "full_name": " h1 Deep Learning in Biomedical Engineering h2 Assignment 2 h3 Due date 11 59 pm Febraury 25 2021 h2 Please enter your full name and UNI here h2 Introduction h2 Instructions h2 How to submit h3 1 5 pts According to the CIFAR10 dataset descriptions and other online resources please identify the quantities below h3 2 0 pts Import the required packages in the following code block h3 3 5 pts Load train and test sets using Pytorch datasets functions h3 4 10 pts Using the matplotlib library make a figure with N times4 grid cells where N is the number of classes Display one random sample of each class pulled from the train set in its corresponding row that depends on its class index and the first column and its histogram in the second column Repeat this for the third and fourth columns but pull images from the test set h3 5 5 pts Split up the train set into new train and validation sets so that the number of samples in the validation set is equal to the number of samples in the test set Then create a DataLoader for each set including the test set with a batch size of 32 h3 6 5 pts Display the number of samples for each class in the train validation and test sets as a stacked bar plot similar to the FirstTutorial h3 7 10 pts According to the LeNet architecture below create a fully connected model Also identify the architeture s hyper parameters activation functions and tensor shapes h3 8 5 pts Create an instance of ADAM optimizer with an initial learning rate of 0 0001 and an instance of Mean Squared Error MSE loss function Briefly explain the ADAM optimizer algorithm and the MSE loss function h3 9 15 pts Train the model for 200 epochs using the train set and validate your model s performance at the end of each epoch using the validation sets h3 10 5 pts Display the learning curve and illustrate the best epoch Explain your criteria for the best epoch h3 11 10 pts Load the model s weights at the best epoch and test your model performance on the test set Display the confusion matrix and classification report h3 12 5 pts Display five random samples of each class titled with the true label and the predicted label Comment on your model s performance h3 13 20 pts Repeat the training validation and testing with the Cross Entropy loss function and initial learning rate of 0 005 Explain how the model s performance changed h1 Reference ", "stargazers_count": 0, "forks_count": 0, "description": "arange 10 y_onehot None. Please remove the comments before filling the blocks. Fully connected layer with an input of 120 neurons and an output of 84 neurons. Load the model s weights at the best epoch and test your model performance on the test set. Repeat the training validation and testing with the Cross Entropy https pytorch. Copy the Public URL 4. This is normal but it is understandable since lenet5 is very simple network. Some regularization methods should be applied to the architecture to solve the issue. Download your completed notebook as a. How to submit After you have completed the assignment 1. Make sure the intensity range is between 0 1 and images are stored as tensor type. Display the learning curve and illustrate the best epoch. According to the LeNet architecture below create a fully connected model. Don t comment on every detail. The Adam optimizer calculates an exponential moving average of the gradient and the squared gradient. It is used if the dataset is normally distributed and you want to penalize outliers a lot. They are initialized as zero vectors and they are biased towards zero. To increase the training speed use the GPU accelerator. You may use transformers while downloading the dataset to do the job. ipynb files on the CourseWorks https courseworks2. Just remember they are using a different architecture and they are using TensorFlow for implementations. Make the saved version public Share Public 3. html loss functions Explaine ADAM and MSE here 1. com soroush361 deeplearninginbme firsttutorial. With the same loss function we can clearly see the more complicated Alexnet offers better performance than Lenet with a test accuracy of 65. To avoid overwriting your previously trained model change the save directories in the training loop. Split up the train set into new train and validation sets Get the testset OPtimizer and loss function Train our CNN with Alexnet architecture for classification. Deep Learning in Biomedical Engineering Assignment 2 Due date 11 59 pm Febraury 25 2021 Please enter your full name and UNI here Mingyang Zang mz2846 IntroductionIn this assignment you will implement train and test LeNet https en. numpy y_onehot np. Display one random sample of each class pulled from the train set in its corresponding row that depends on its class index and the first column and its histogram in the second column. Currently the markdown blocks have a comment like your answer here. You can always add more blocks if you need to or it just makes your answers well organized. 0001 and an instance of Mean Squared Error MSE https pytorch. size 0 1 If GPU available move the model to GPU. from_numpy y_onehot y_onehot torch. The MSE loss function is L y hat y frac 1 N sum_ i 1 N y_i hat y _i 2 Where y is the true value hat y is the predicted value N is the number of classes. If you are using an online code or paper make sure you cite their work properly to avoid plagiarism. DataLoader for each set including the test set with a batch size of 32. xlabel Intensity plt. The output should be 6x28x28. Import the required packages in the following code block. The code blocks are only for Python codes and comments and currently have a comment like Your code here. dataset backpropagate the loss update the parameters print statistics Get the accuracy num_correct1 torch. Write comments for your codes in the big picture mode. Using the matplotlib library make a figure with N times4 grid cells where N is the number of classes. item accuracy num_correct len trainloader. plot histograms i 0 histograms i 1 c colors i label Channel 0. plot img PLot the test image and its histgram. The result is epoch 63 11. LeNet Architecture https raw. Implementing and reporting results using other architectures than LeNet will grant you an extra 20 on grade. Both architectures experience serious overfitting issue and generalization gap enlarges as the training proceed. ylabel Abundance Architecture Define the model x x. format epoch train_epoch_loss 1 Choose the best epoch Get the test image Covert list to tensor define the prediction function. The title is in the format of True_label Predicted_label Get the predicted label img np. Essentially you need to copy all the codes above and just change the loss function and edit the learning rate. CrossEntropyLoss loss function and initial learning rate of 0. You can always come back here and import another package please keep them all in the following block to make your code well organized. float32 y_onehot torch. Display the number of samples for each class in the train validation and test sets as a stacked bar plot similar to the FirstTutorial https www. The output is 6x14x14. \u5982\u679c\u6d4b\u8bd5\u7684\u8bdd\u7528MNIST\u5219\u53ef\u4ee5\u4f7f\u752810 \u5411\u524d\u4f20\u64ad in_channel class_num If GPU available move the model to GPU. The learning curve shows the model s loss and accuracy at the end of each epoch for all epochs 200 epochs. Adam with an initial learning rate of 0. e Intensity range your answers here a 60000 samples 50000 for training and 10000 for testing b 10 classses c 32x32 color images d airplane 0 automobile 1 bird 2 cat 3 deer 4 dog 5 frog 6 horse 7 ship 8 truck 9 e 0 255 2. Look at this tutorial Pytorch CIFAR10 https pytorch. The Pytorch CIFAR10 https pytorch. The input has dimension 3x32x32. html algorithms Loss function list https pytorch. One comment line at the top of each code block is necessary to explain what you did in that block. In this Kaggle Notebook https www. Define the new optimizer and CroessEntropy loss function Train our CNN for classification. Explain how the model s performance changed. subplot 5 4 4 i 4 plt. Then create a DataLoader https pytorch. Keep in mind that y is a one hot vector like y begin bmatrix 0 0 1 vdots end bmatrix This example of y indicates that the sample belongs to class ID 2 remember it is zero indexed and hat y begin bmatrix 0. org docs stable generated torch. format epoch train_epoch_loss 1 Loss curve Accuracy curve Choose the best epoch Get the test image Covert list to tensor define the prediction function Extra credit Define the model First phase Second phase Third phase fully connected layer AlexNet\u4e0a\u9762\u662f1000. Keep in mind that you identified the W H and N Which refers to the number of classes in the first question. From the graph we can see the generalization gap becomes larger and larger. Architecture hyper parameter includes the number of layers number of kernels in each layer size of the kernels stride zero padding size. Altogether it is a 7 layers network. The first convolution layer has 16 channels with a kernel size of 5 zero padding and a stride of 1. org tutorials beginner blitz cifar10_tutorial. Preprocess the image and enlarge the images to 96 96 since this size better fits the Alexnet. vt and st are estimates of the first moment the mean and the second moment of the gradients respectively. plot img plt. item accuracy num_correct1 len validationloader. Display five random samples of each class titled with the true label and the predicted label. Define the optimizer and MSE loss function Create the directory Train our CNN for classification. com soroush361 deeplearninginbme firsttutorial for more help. transpose img 0 1 2 0 axs i 0. For your information here is the mathematics behind the ADAM optimizer For each parameter w j v_t beta_1v_ t 1 1 beta_1 g_t s_t beta_2s_ t 1 1 beta_2 g_t 2 Delta w j eta frac v_t sqrt s_t epsilon g_t w j_ t 1 w j_t Delta w j Where eta is the initial learning rate g_t is the gradient at time t along w j v_t is the exponential average of gradients along w j s_t is the exponential average of squares of gradients along w j beta_1 beta_2 are the hyper parameters and epsilon is a small number to avoid dividing by zero. The output has dimension 16x5x5. Name your variables properly that represent the data they are holding such as test_set. plot img Display the images and histograms Split up the train set into new train and validation sets Get the testset Get the label from trainset Get the label from validationset Get the label from testset Count the number Plot the figure Check GPU availablity Define the model x x. The output should be 16x10x10. Reload the dataset. The criteria for the best epoch can be the minimum loss or maximum accuracy or other criteria. The maxpooling layer has 2x2 filter. Look at the FirstTutorial https www. Samples must be pulled from the test set. org wiki LeNet model to classify CIFAR10 http www. For ADAM optimizer keep other arguments as default. reshape images 3 32 32 img plt. dataset backpropagate the loss update the parameters print statistics y_onehot labels. Comment about the model s performance The model has an overall test accuracy of 62. The input has dimension 16x10x10. InstructionsDepending on each question there are empty blocks you need to fill. Comment on your model s performance. Save a version You can use Quick Save to avoid re running the whole notebook 2. com roblexnana cifar10 with cnn for beginer you may find useful information about how your outputs must look. According to the CIFAR10 http www. dataset reset the gradient backpropagate the loss update the parameters print statistics y_onehot labels. The input has dimension 6x28x28. We also observe that the loss and accuracy fluctuate which may the result of higher learning rate. Explain your criteria for the best epoch. Define the display function fig axs plt. Repeat this for the third and fourth columns but pull images from the test set. zero_grad forward backward optimize loss loss_CrossEntropyLoss outputs y_onehot. cuda Get the accuracy num_correct1 torch. Load train and test sets using Pytorch datasets functions. For more help look at this implementation https github. Note I trained 100 epoch to save GPU Quota and time. long zero the parameter gradients forward backward optimize loss loss_CrossEntropyLoss outputs y_onehot. com icpm pytorch cifar10 blob master models LeNet. Display the confusion matrix and classification report. Make sure none of the samples in the validation set exists in the new train set. subplots 10 2 PLot the train image and its histgram. html tutorial will also help you here. Also identify the architeture s hyper parameters activation functions and tensor shapes. transpose img 1 2 0 img torch. title Histogram colors Red Green Blue histograms GetHistograms img display_range for i in range 3 plt. 98 which is not bad. The test accuracy is lower than the model with MSE loss function. MSE is the average squared difference between the estimated values and the true values. dataset save models Loss curve Accuracy curve Choose the best epoch call our best model and use that for inference on the test set Get the test image Covert list to tensor define the prediction function Display the confusion matrix Classification Report Reload the testset 5 test images of 10 labels for each category. org docs stable optim. from_numpy y_onehot loss loss_CrossEntropyLoss validation_label_predicted y_onehot. cuda Get the accuracy num_correct torch. format colors i plt. html dataset descriptions and other online resources please identify the quantities below a Total Number of samples b Number of classes c Image size d Write class names and their corresponding label index e. tensor y_onehot dtype torch. Upload the Public URL and the. Although you may use other available online resources such as GitHub Kaggle Notebooks it is highly recommended to try your best to do it yourself. loop over the dataset multiple times running_loss 0. 0 get the inputs data is a list of inputs labels Convert labels to one hot vector y_onehot labels. png Describe the model s architecture hyper parameters 1. dataset print Epoch train_loss. Fully connected layer with an input of 84 neurons and an output of 10 neurons. However you need to create a new instance of the architecture. I choose the epoch which has the lowest validation loss as the best epoch. Truck may be classified as car or ship which are transportations. Create an instance of ADAM optimizer https pytorch. Using other students works is absolutely forbidden and considered cheating. The model has severe overfitting issue. com soroush361 DeepLearningInBME main Ass1_Arch1. The input has dimension 6x14x14. Train the model for 200 epochs using the train set and validate your model s performance at the end of each epoch using the validation sets. The markdown blocks are only for plain or laTex text that you may use for answering descriptive questions. Split up the train set into new train and validation sets so that the number of samples in the validation set is equal to the number of samples in the test set. For those misclassification labels it seems like the model misclassify labels under the same super category For example dogs may be classified as horse or cat which are animals in general. Do not forget to save the model at the end of each epoch. For other optimization algorithms and loss functions check the links below Optimizers list https pytorch. Otherwise the weights would be the same as the last epoch or the best epoch in the last part. The output is 16x5x5. com drvaibhavkumar alexnet in pytorch cifar10 clas 83 test accuracy Preprocess the images 10 train images of 10 labels 10 test images of 10 labels Get the train image list full of images Get the test image list full of images Takes one color image and intensity range then returns the histogram and edges values as a list that each row will have two elements the edges and histogram values. Obviously you don t need to re import the dataset and the libraries. ipynb file File Download 5. Briefly explain the ADAM optimizer algorithm and the MSE loss function. long zero the parameter gradients optimizer. Just by looking at the architecture itself you should be able to identify the hyper parameters. com soroush361 deeplearninginbme firsttutorial 2. loop over the dataset multiple times get the inputs data is a list of inputs labels Convert labels to one hot vector zero the parameter gradients forward backward optimize Get the accuracy num_correct torch. The first convolution layer has 6 channels with a kernel size of 5 zero padding and a stride of 1. MSELoss loss function. dataset save models print Epoch train_loss. 8 vdots end bmatrix shows the probability of belonging to each class for the same sample and predicted by the model. ", "id": "mz2846/deeplearninginbme-assignment2", "size": "11862", "language": "python", "html_url": "https://www.kaggle.com/code/mz2846/deeplearninginbme-assignment2", "git_url": "https://www.kaggle.com/code/mz2846/deeplearninginbme-assignment2", "script": "torch.optim torch.utils.data sklearn.metrics __init__ test10 DataLoader DisplayFunction SGD Adam tensorflow.keras.datasets.mnist forward torch.nn seaborn numpy predict_with_pytorch sklearn.model_selection confusion_matrix Model(nn.Module) ImageGrid matplotlib.pyplot keras.utils pandas classification_report AlextNet(nn.Module) accuracy_score to_categorical torch.nn.functional MyDisplayFunction GetHistograms TensorDataset train_test_split mpl_toolkits.axes_grid1 torchvision.transforms ", "entities": "(('you', 'hyper parameters'), 'be') (('You', 'job'), 'use') (('loss functions', 'https pytorch'), 'check') (('where N', 'classes'), 'make') (('learning curve', 'epochs 200 epochs'), 'show') (('model', '62'), 'comment') (('1 images', 'tensor type'), 'make') (('none', 'train new set'), 'make') (('title Histogram Red Green Blue histograms', 'range'), 'color') (('code', 'following block'), 'come') (('they', 'such test_set'), 'name') (('Mingyang Zang mz2846 UNI here assignment you', 'LeNet https'), 'Learning') (('it', 'lenet5'), 'be') (('inputs data', 'one hot vector y_onehot labels'), 'get') (('they', 'implementations'), 'remember') (('just answers', 'always more blocks'), 'add') (('GPU', 'GPU'), '\u5411\u524d\u4f20\u64ad') (('1 i c i', 'Channel'), 'histogram') (('number', 'Check availablity model'), 'Display') (('update', 'print statistics'), 'backpropagate') (('Essentially you', 'learning rate'), 'need') (('generalization gap', 'graph'), 'see') (('which', 'horse'), 'seem') (('you', 'block'), 'be') (('dataset save models', 'Epoch train_loss'), 'print') (('LeNet', 'grade'), 'grant') (('You', 'whole notebook'), 'save') (('that', 'first second column'), 'display') (('hat zero y', 'bmatrix'), 'keep') (('html dataset', 'classes c Image size d Write class names'), 'identify') (('vdots end 8 bmatrix', 'model'), 'show') (('criteria', 'best epoch'), 'be') (('testset function', 'classification'), 'split') (('title', 'label img predicted np'), 'get') (('row', 'two elements'), 'Preprocess') (('it', 'it'), 'use') (('Covert list', 'prediction function'), 'epoch') (('test accuracy', 'MSE loss function'), 'be') (('Obviously you', 'don dataset'), 'need') (('Samples', 'test set'), 'pull') (('Architecture hyper parameter', 'kernels stride zero padding size'), 'include') (('Otherwise weights', 'best last part'), 'be') (('predicted N', 'classes'), 'be') (('number', 'test set'), 'split') (('0 1 GPU', 'GPU'), 'size') (('update', 'statistics y_onehot labels'), 'backpropagate') (('you', 'descriptive questions'), 'be') (('long parameter', 'optimize loss loss_CrossEntropyLoss forward backward outputs'), 'zero') (('However you', 'architecture'), 'need') (('hyper parameters', 'small zero'), 'be') (('MSE', 'average squared estimated values'), 'be') (('size', 'better Alexnet'), 'preprocess') (('Third phase', 'layer fully AlexNet\u4e0a\u9762\u662f1000'), 'epoch') (('empty you', 'question'), 'be') (('inputs multiple times data', 'parameter zero forward backward accuracy'), 'get') (('which', 'best epoch'), 'choose') (('Covert list', 'category'), 'choose') (('clearly more complicated Alexnet', '65'), 'see') (('e Intensity', 'c color images 10 32x32 airplane 0 automobile 1 bird 2 3 deer 4 5 6 horse'), 'range') (('parameter', 'optimizer'), 'gradient') (('code blocks', 'code'), 'be') (('you', 'properly plagiarism'), 'make') (('normally you', 'outliers'), 'use') (('convolution first layer', '1'), 'have') (('they', 'zero'), 'initialize') (('generalization training', 'serious overfitting issue'), 'experience') (('Which', 'first question'), 'keep') (('regularization methods', 'issue'), 'apply') (('how outputs', 'useful information'), 'roblexnana') (('also loss which', 'learning higher rate'), 'observe') (('works', 'other students'), 'forbid') (('parameters', 'statistics y_onehot labels'), 'print') (('vt', 'mean second gradients'), 'be') (('I', 'GPU Quota'), 'note') (('Adam optimizer', 'gradient'), 'calculate') (('which', 'car'), 'classify') (('How you', 'assignment'), 'submit') (('markdown Currently blocks', 'answer'), 'have') (('Load train', 'functions'), 'dataset') ", "extra": "['onset', 'test']"}