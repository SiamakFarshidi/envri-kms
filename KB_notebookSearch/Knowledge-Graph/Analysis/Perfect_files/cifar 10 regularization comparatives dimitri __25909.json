{"name": "cifar 10 regularization comparatives dimitri ", "full_name": " h1 5th Project Image Classification CIFAR 10 h3 Author Arthur Dimitri Brito Oliveira h1 Introduction h1 Dataset Description h1 Objectives h2 Dependencies h1 Loading Data h1 Preprocessing h3 Feature Scaling h1 Neural Network Architecture h3 Theoretical Fundaments h3 Activation Functions h3 z w 1x 1 w 2x 2 w nx n h3 y g z g w 1x 1 w 2x 2 w nx n h4 Relu h4 Softmax h3 Z L w L a L 1 b L h2 a L frac e z L sum j 1 n t i h3 Convolution Layers h2 y n sum n infty infty x k h n k h3 Pooling Layers h2 Flatten Layer h2 Dense Layer h2 Regularization Layers h3 Dropout Layers h1 Training the Neural Network h2 Regularization of Parameters h3 Weight Decay h3 min w b J w b w in R N x b in R h2 J w b sum i 1 N x L hat y i y i frac lambda 2m w 2 h2 dw L from back prop frac lambda m w L h2 w L w L alpha dw L h3 Batch Normalization h2 Z XW h2 Z Z sum i 1 m Z i h2 hat Z frac Z sqrt epsilon frac 1 m sum i 1 m Z 2 h2 a hat Z beta h1 Metrics For Evaluating the Classifier h2 Confusion Matrix h2 Metrics Precision Recall and F1 score h3 Precision h2 Precision frac TP TP FP h3 Recall h2 Recall frac TP TP FN h3 F1 score h2 F1 2 frac Precision Recall Precision Recall h2 Baseline Model h2 Baseline Model Regularization Techniques h1 More Complex Model Regularization Techniques h3 ImageDataGenerator width shift range 0 1 height shift range 0 1 horizontal flip True zoom range 0 2 rotation range 10 h1 Deeper Network h1 Best Model Loading h1 Confusion Matrix h1 Classification Metrics h1 Save Best Model h1 Conclusions h4 1 Overfitting was adressed but underfitting came out Our model seems to be stuck on 90 on both training and validation sets Analyzing the model we suppose that it is sufficiently complex to solve the problem Nevertheless we applied agressive dropout rates at the main convolution layers this is the layers with more neurons Reading some approaches in the original paper that introduced the dropout layers by Hinton 2012 a dropout layer with k prop 0 5 is used at the end of the Dense layer It was not applied on convolutional layers A more recent research by Park 2017 points that it is useful for the network combining other regularization techniques with dropouts after convolutional layers obtaining aproximately 8 of error on the validation set For further improvements we plan to test more variations of the network with lower dropout rates at hidden units and other tests following Hinton 2012 approach to compare them h4 2 In most of the cases it seems that the model shows a trending of improvement If they could be tested for more epochs until convergence using early stoppings as the main regularizer there might be advantages for the model accuracy on unseen data h4 3 The long time taken to converge might be related to few hyperparameter tunning Since we found Nadam as the best choice to solve the problem we could experimentally find an optimal learning rate by doing more experiments We also could fine tune the weight decay regularization We just used one type of the possible kernel regularizers and we set it with the default value so it might be non optimal for our problem h4 4 By analyzing our confusion matrix we see that we need to improve classification of the 3rd and 5th classes We plan to analyze better these classes on the dataset and maybe engineer some features to help the classifier identify more patterns on these classes Apllying more advanced preprocessing techniques could also help the training such as making some patterns stand out more h4 5 The main objective here was developing a neural network from scratch It means that it would probably be not really optimized For a better model transfer learning could also be applied There are many architectures consolidated for solving computer vision problem such as VGG16 Resnet etc with way more complex chained layers Since the first layers capture generic patterns from images we could train just the final layers and get a more accurate network h4 6 For aditional implementation we also recommend other techniques for scaling the image such as pixel scaling For hyperparameter tunning we plan to use GridSearchCV from sklearn to automate some parameter comparison and find the most promising model to begin the experiments h1 References h3 1 Aur\u00e9lien G\u00e9ron Hands on Machine Learning with Scikit Learn and TensorFlow 2017 O Reilly h3 2 http deeplearningbook com br h3 3 https blog exsilio com all accuracy precision recall f1 score interpretation of performance h3 4 https www deeplearning ai tensorflow in practice h3 5 https machinelearningmastery com dropout for regularizing deep neural networks h3 6 https machinelearningmastery com how to reduce overfitting in deep learning with weight regularization h3 7 https medium com thevatsalsaglani multi class image classification using cnn over pytorch and the basics of cnn fdf425a11dc0 h3 8 https stats stackexchange com questions 173663 increasing number of neurons in convolutional net h3 9 https www kaggle com roblexnana cifar10 with cnn for beginer h3 10 https www quora com Why would one use larger strides in convolutional NNs as opposed to smaller strides ", "stargazers_count": 0, "forks_count": 0, "description": "If we zoom the Nadam optimizer we can see that it makes the model converge a little faster than the others reaching 90 of accuracy on the training set close to the 10th epoch. Seeking to get an even more complex feature detection we added a layer with 128 neurons. com questions 173663 increasing number of neurons in convolutional net 9 https www. Example A not predidcted as B3. The way the penalties are applied make these two methods distinct in practice. We hope that it reduces overfitting. F1 2 frac Precision. Most of the efforts should be spent on improving classification for animal labels specially Cats Dogs and Deers. w_nx_n So the summing junction a linear combination of the inputs is transformed in a non linear output. If they could be tested for more epochs until convergence using early stoppings as the main regularizer there might be advantages for the model accuracy on unseen data. We have a model 90 of accuracy on the validation set that doesn t overfit much. ai tensorflow in practice 5 https machinelearningmastery. The final model is loaded from disk. Reading some approaches in the original paper that introduced the dropout layers by Hinton 2012 a dropout layer with k prop 0. A more recent research by Park 2017 points that it is useful for the network combining other regularization techniques with dropouts after convolutional layers obtaining aproximately 8 of error on the validation set. Although applying the model makes the model slower to train using a higher and controlled dropout gives the model an opportunity to acquire more independent representations of the data making the ability of predicting unseen data more accurate. This is very useful since it adds non linear properties to the inputs. Multiply the normalized output by some arbitrary parameter and add some random parameter too a hat Z beta In practice what it does is make the cost function contour plot more simetric making the path for the gradient descend based learning more uniform. When it comes to the confusion matrix some classes seem to be very misclassified. False Negatives FN when an element is not classified as its belonging class. For aditional implementation we also recommend other techniques for scaling the image such as pixel scaling. This indicates that the model is learning a more generalized representation of the problem but it could have its performance increased by applying more severe penalties or adding some dropout layers or having a more complex model. L1 is robust dealing with outliers while L2 have trouble with that. Since we found Nadam as the best choice to solve the problem we could experimentally find an optimal learning rate by doing more experiments. Compute the previous layer inputs operating with biases Z XW 2. Dependencies Loading Data Preprocessing Feature ScalingThe main objective with this preprocessing technique is to keep the pixels in a controlled range 0 to 1 making the distribution look like a gaussian centered at zero. The augmentation procedures were made the same way as before. The last dense layer gets these operations and by applying an activation function gives the probabilities for each class. For a layer L the summing junction would be a product of the previous activation with the weights of the layer plus the biases associated with the layer L Z L w L a L 1 b L The activation on the softmax layer is a L frac e z L sum_ j 1 n t_i So it gives a probability for each class taking into consideration all the class elements. Consulting some discussions on Stack Exchange we find that adding dropout layers after each convolutional layer isn t a recommended strategy. Here we are using a previous explained Batch Norm technique after the activation of each convolutional layer and another penalization l1_l2 which is a combination of the two classical weight decay techniques at each layer. This shows that although a high accuracy on test set our model have a good amount of true positives diagonals but it lacks accuracy on some classes. 5th Project Image Classification CIFAR 10 Author Arthur Dimitri Brito Oliveira arthur. Each filter applied to the input will generate what is called as feature map that gives a representation of the input in respect to one aspect. Convolution Layers A convolution is a mathematical operation that slides one function over another and measures the integral or the area behind the curve oftheir pointwise multiplication. In most of the cases it seems that the model shows a trending of improvement. It was not applied on convolutional layers. The intuition behind it is that it can t rely on any feature so it has to spread out the weights. We apply an activation function to these feature maps and keep convolving them with more and more filters applying pooling techniques to get more sparse representations and catch even more details from the image. From here we see that there is some pattern on the diagonal. Some dark gray squares outside the main diagonal show the classes that were misclassified. It gives even more sparse representation of the features in the image. There are many architectures consolidated for solving computer vision problem such as VGG16 Resnet etc with way more complex chained layers. The Deer class is mostly misclassified as 6. Here we see some of the worst mistakes made by our model. Here we apply categorical transformation to the class labels Neural Network Architecture Theoretical Fundaments Activation FunctionsThere is a representation of a neuron with its inputs The summing junction is given by the product of the input by a wheight which indicates the influence of the given input to the output. As can be seen the performance on the test set was dramatically improved. Neurons in the first convolutional layer are not connected to every single pixel in the input image but only to pixels in their receptive fields. The difference between L1 and L2 regularizations is only how we sum the penalties. This is useful when we are reaching the end of the network before the fully connected layers. Probably our model is good at catching basic visual patterns for theses species but fails on detecting more specific ones. com all accuracy precision recall f1 score interpretation of performance 4 https www. com dropout for regularizing deep neural networks 6 https machinelearningmastery. Besides it every layer has a weight decay regularization. 1 2 The CIFAR 10 dataset contains 60 000 32x32 color images in 10 different classes. Example A predicted as A2. 2 rotation_range 10 It is a common pattern on deep learning CNNs to increase the dropout rate for large layers and reduce it for small ones Hands on Machine Learning with Scikit Learn and TensorFlow 2017 O Reilly. Our model has a better f1 score for all of the bad performance classes in the previous report. read_csv Input data files are available in the read only. The aim is to take the inputs from the feature analysis and apply weights to classify the initial input. Best Model Loading Confusion MatrixAs can be seen specially for the classes 3 cat and 5 dog there are a lot of false positives. There are 6 000 images of each class. It forces the network to minimize the cost and make the weights have a lower influence on the final function and thus making the decision boundaries more generic and thus prevents overfitting. The figure shows a plateau on the validation set for all of the optimizers and an increasing loss value. In the cell below there is the model that we used. The model seems to have controlled the overfitting behavior and we see a trend of improvement on the accuracy value on the test set. This motivated an addition of a new convolutional layer with the double number of neurons aiming to catch more patterns and train for more epochs applying the same data augmentation procedures and a higher dropout rate. com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e. ReluFor the negative input values the result is zero that means the neuron does not get activated. Recall frac TP TP FN F1 scoreF1 Score is the weighted average of Precision and Recall. From these plots we have some insights to improve our classifier. A not predicted as A Precision Precision measures the amount of correctly predicted labels in respect to the total positive observations. Precision frac TP TP FP Recall Recall measures the percentage of elements positively labeled in respect to all the observations in the actual class. The class of Cats is very misclassified as Dog or Frog. Apllying more advanced preprocessing techniques could also help the training such as making some patterns stand out more. It is one of the most widely used datasets for machine learning research. With that the behavior of the network is expected to be more generalistic by extracting features that are more useful in general cases and not in a combination isolated example. Normalize the output of the layer hat Z frac Z sqrt epsilon frac 1 m sum_ i 1 m Z 2 4. This can guide us to apply softer regularization techniques or have a more complex model in the next steps. br 3 https blog. A very low probability doesn t have a impactful effect on the model results. At the end we add two dense layers to reshape the tensors and apply some activation functions to predict the labels. True Positives TP correctly predicted classes. Wikipedia Objectives We aim to load this dataset apply one hot encoding to the labels and normalize the image pixels before we dive into the machine learning pipeline. Batch NormalizationTo increase the stability of a neural network batch normalization normalizes the output of a previous activation layer. It can be compared to the sigmoid function but in this case it gives probabilities for multiple classes. 1 horizontal_flip True zoom_range 0. It seems that a higher value makes the algorithm get lost finding the mininum value on the cost function contour plot. It is useful to compare two models and a more powerful metric than simply accuracy. 3 The 10 different classes represent airplanes cars birds cats deer dogs frogs horses ships and trucks. The results are down sampled or pooled feature maps that highlight the most present feature in the patch. A similar analysis is made in A Comparative Analysis of Gradient Descent Based Optimization Algorithms on Convolutional Neural Networks Dogo et. The pictures below show the results of previous tests. The darkest ones on the diagonal reveal the bad behavior on test set fot classes 3 and 5 mostly as mentioned before. Subtract it from its mean Z Z sum_ i 1 m Z_i 3. The layers that have most of the parameters should be the ones that we are concerned about overfitting the model. Regularization Layers Dropout LayersHow could we deal with overfitting with a simple approach Dropout is the answer. Baseline Model Regularization TechniquesConsidering the overfitting seen in the previous model we could apply some regularization techniques to keep the model learning and generalizing after some epochs of training. However there might be some underfitting on the data since there isn t much of improvement from the 100th epoch. 5 is used at the end of the Dense layer. z w_1x_1 w_2x_2. We plan to analyze better these classes on the dataset and maybe engineer some features to help the classifier identify more patterns on these classes. The loss curve also behaves better getting lower at each subsequent epoch. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session load weights into new model Errors are difference between predicted labels and true labels lists all downloadable files on server. From it one can see how many classes have been misclassified and how good the model behaved for each one Metrics Precision Recall and F1 scoreFirst we need to define some terms 1. br IntroductionThis work describes the process of designing a Convolutional Neural Network for multiclass classification. Normalizing causes a faster convergence of the model. It means that some neurons are stochastically dropped out of the network with a certain rate. The recall metric reveals that the model isn t having a good performance on classifying the actual elements from its belonging classes. In order to prevent that without taking out features we can apply what is called weight decay. References 1 Aur\u00e9lien G\u00e9ron Hands on Machine Learning with Scikit Learn and TensorFlow 2017 O Reilly 2 http deeplearningbook. Normally using a small dropout probability from 20 to 50 is recommended for the hyperparameter testing. com Why would one use larger strides in convolutional NNs as opposed to smaller strides This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. So we tried a pattern of increasing from 0. It means that it would probably be not really optimized. It is very popular in convolutional networks because it is proved to cause a faster convergence of the model and it prevents the vanishing gradients problem in the context of backpropagation. The basic steps of the method are 1. We also could fine tune the weight decay regularization. It is about knowing how many elements predicted as a label really belong to it. It also reaches a lower val_loss. This combination of techniques might cause a shortage of information to keep the model learning and it might slowe its convergence. At the end of this project we want a model that can predict satisfactorily the inputs with a relatively low error. com how to reduce overfitting in deep learning with weight regularization 7 https medium. 2 in the first layer to 0. Looking just at the macro average can disguise the poor performance on these classes. These dropout rates are common in these kind of CNN problems and we are being more agressive adressing overfitting aiming to have enough hypothesis to trade off between overfitting and underfitting. However for further enhancements we list 1. The worst results can be observed in the classes 3 and 5. As may be seen in the figures above the behavior on the test set changed substatially. Nevertheless we applied agressive dropout rates at the main convolution layers this is the layers with more neurons. The main objective here was developing a neural network from scratch. Pooling LayersMaximum pooling or max pooling calculates the maximum value in each window sliding over a feature map. Since the first layers capture generic patterns from images we could train just the final layers and get a more accurate network. For data augmentation we used a keras preprocessing function applying shift ranges in both dimensions and image mirroring ImageDataGenerator width_shift_range 0. Softmax When we want to describe a probability distribution over a discrete variable consisting of n classes the softmax function should be applied. Flatten LayerFlattening means to reshape the previous matrix into one dimension. This principle is show above after the first convolutional layer What controls the filter sliding and hence the output representation of the image is a parameter called stride. In L1 whe just sum de absolute values of the weights whilst in L2 whe sum the square value of those weights. For hyperparameter tunning we plan to use GridSearchCV from sklearn to automate some parameter comparison and find the most promising model to begin the experiments. The 5th class has a slightly better performance on recall but also lacks in correctly predicting true positives correctly. Classification Metrics Comparing this with the previous model the bigger one we can infer some informations. For further improvements we plan to test more variations of the network with lower dropout rates at hidden units and other tests following Hinton 2012 approach to compare them. com thevatsalsaglani multi class image classification using cnn over pytorch and the basics of cnn fdf425a11dc0 8 https stats. Moreover the model seems to solve the problem pretty well for a build from scratch approach. If we want to keep the same size after the convolution layer we should apply a zero padding at the edges of the image. The decay values were set as the default ones. L1 gives a simples solution and more interpretable and can t deal with complex patterns whilst L2 does the opposite. Most squares have cleares white color and it means the classifier predicts correctly most of the labels. In turn each neuron in the second convolutional layer is connectedonly to neurons located within a small rectangle in the first layer. We just used one type of the possible kernel regularizers and we set it with the default value so it might be non optimal for our problem. It means that our network fails to generalize satisfactorily. Metrics For Evaluating the Classifier Confusion MatrixIt shows visually how many labels were predicted correctly. In the case of images the function being slided is called a filter or kernel. Besides all these evidences of not enhancement our strategies of regularization seem to be sufficient to avoid the overfitting but maybe it was too much affecting the model s learning even in the training set. In general it reaches a higher recall and precision for the problematic classes previously mentioned. Since the network is getting deeper we should take care with more agressive regularization techniques batch norm weight decay data augmentation and dropout to prevent the model from overfitting. True Negatives TN correctly predicted element from its not original class. al where they find Nadam the best option among nine optimizers in terms of accuracy and loss values. 1 height_shift_range 0. Some changes on the hyperparameters were made such as increasing learning rate but a high overshoot was noticed on the loss value. It reduces overfitting because it has a slight regularization effects. This makes us discard the hypothesis of having a model that was too simple for the problen. Our model seems to be stuck on 90 on both training and validation sets. There s a plateau around 90 of accuracy on the test set. As one can see the optimizers had a similar behavior searching for the global minimum of the cost function on the training set. From now on we are going to use Nadam as our optimizer. For a better model transfer learning could also be applied. This indicates that we should apply some regularization techniques and improve the model. However after 15 epochs the loss starts not to decrease and the validation accuracy starts not to evolve and this could be seen as an overfitting behavior. Dense LayerConsists in a linear operation between the weights in which all the inputs are connected to the ouput. At them we could adress a more agressive dropout rate. Dataset Description The CIFAR 10 dataset Canadian Institute For Advanced Research is a collection of images that are commonly used to train machine learning and computer vision algorithms. The long time taken to converge might be related to few hyperparameter tunning. w_nx_n Then the output of the neuron taking into consideration a certain level of activity treshold is y g z g w_1x_1 w_2x_2. If it is 1 for a 2x2 kernel there will be always an overlap. y n sum_ n infty infty x k h n k As a simple explanation what a convolution layer does is extract features depending on the filter applied and convert it into a lower dimension mantaining the main features of the original input. False Positives FP wrongly predicted element. We ommited here the calling of the function not to have aditional GPU time. Analyzing the model we suppose that it is sufficiently complex to solve the problem. So a flatten layer in keras reshapes the tensor into a 1xN matrix with N being the number of elements in that tensor. Combining these two techniques may give the model a good capacity of generalization. By analyzing our confusion matrix we see that we need to improve classification of the 3rd and 5th classes. Our recall for the 3rd class is the worst of all meaning that our model is dealing with Cats too much as false negatives in comparison to the true positives. As one can see an aditional convolutional layer didn t make much of improvement. Notice that due to time limitation in respect to the Kaggle GPU we are just showing pictures of the loss and accuracy curves. Some theoretical fundaments are presented and the steps that led to the final model are detailed. For the sake of GPU time remaining we are just showing the results obtained with this approach. For most part of the training the validation has higher accuracy rates even applying shuffling techniques on both datasets. The efficiency of theses methods was detailed in the previous theoretical sections. Training the Neural Network Regularization of Parameters Weight DecayIf our network is too complex deeper there is an overfitting trend. Deeper Network Since it took many hours to run the training we are just showing the architecture and the results obtained. Save Best Model ConclusionsWe can conclude that the main objective previously outlied at the begining was reached. Let s that we have a cost function that we want to minimize min_ w b J w b w in R N_x b in R J w b sum_ i 1 N_x L hat y i y i frac lambda 2m w 2 To update the weights from backpropagation in the layer L we also add a penalty dw L from back prop frac lambda m w L w L w L alpha dw L So that s why it is called weight decay. Besides it we want a model that manages addressing the overfitting underfitting trade off. Overfitting was adressed but underfitting came out. Testing the model with three optimizers RMSprop Adam and Nadam with the same learning rate gives some initial results. Recall Precision Recall Baseline ModelSince convolutional neural networks are a kind of feedfoward networks specialized in images let s take some generic CNN design pattern as a guideline Input Convolution Pooling Convolution Pooling Fully Conected Layer OutputStructuring this with Keras gives For instance we have 32x32x3 input images and a first convolutional layer with 32 3x3 filters and stride 1 and 32 32x32 feature maps at the output of this layer since we applied a zero padding to keep the original input dimensions. Besides it the model overfits fast. More Complex Model Regularization TechniquesGiven that we have achieved a relatively good result with the baseline model around 80 on test set we could test adding one more convolutional layer. All the techniques applied seemed to be useful to solve the initial overfitting problem. The main objective is to know how much of the actual label we ve correctly predicted. At some epochs even before the 300th epoch the model reaches 90 but at the end it reaches only 87. The training and validation accuracy curves sticked to each other demonstrating that the model is making good generalizations. com roblexnana cifar10 with cnn for beginer 10 https www. ", "id": "arthurdimitri/cifar-10-regularization-comparatives-dimitri", "size": "25909", "language": "python", "html_url": "https://www.kaggle.com/code/arthurdimitri/cifar-10-regularization-comparatives-dimitri", "git_url": "https://www.kaggle.com/code/arthurdimitri/cifar-10-regularization-comparatives-dimitri", "script": "Flatten CnnModel keras.preprocessing.image Nadam __init__ sklearn.metrics Adadelta keras.layers keras.callbacks model_from_json np_utils Dropout Sequential keras.datasets Adamax Adam on_epoch_end Conv2D collections seaborn numpy FileLinks cifar10 confusion_matrix ImageDataGenerator Image matplotlib.pyplot RMSprop Activation Dense keras.utils tensorflow pandas classification_report keras.optimizers Counter BatchNormalization Callback regularizers MaxPooling2D backend backend as K keras FileLink keras.models MyThresholdCallback(tf.keras.callbacks.Callback) IPython.display ", "entities": "(('worst results', 'classes'), 'observe') (('layer', 'weight decay regularization'), 'have') (('Nadam', 'accuracy'), 'al') (('only how we', 'penalties'), 'be') (('performance', 'test set'), 'improve') (('hat Z', 'epsilon'), 'normalize') (('Here we', 'model'), 'see') (('L1', 'opposite'), 'give') (('figure', 'optimizers'), 'show') (('we', 'that'), 'be') (('it', 'classes'), 'show') (('Normalizing', 'model'), 'cause') (('1 10 dataset', '10 different classes'), '2') (('that', 'overfitting underfitting trade'), 'want') (('why it', 'L'), 'let') (('more path', 'more uniform'), 'multiply') (('Combining', 'generalization'), 'give') (('Probably model', 'more specific ones'), 'be') (('we', 'test set'), 'seem') (('more filters', 'image'), 'apply') (('we', 'ImageDataGenerator width_shift_range'), 'use') (('it', 'weights'), 'be') (('that', 'one aspect'), 'generate') (('algorithm', 'cost function contour plot'), 'seem') (('decay values', 'default ones'), 'set') (('it', 'only 87'), 'reach') (('doesn', 'validation'), 'have') (('network', 'Parameters Weight DecayIf'), 'train') (('we', 'overfitting'), 'take') (('techniques', 'overfitting initial problem'), 'seem') (('model', 'close 10th epoch'), 'see') (('two methods', 'practice'), 'make') (('Normally using', 'hyperparameter testing'), 'recommend') (('2 10 It', 'Scikit Learn'), 'rotation_range') (('Moreover model', 'scratch approach'), 'seem') (('recall metric', 'belonging classes'), 'reveal') (('linear combination', 'non linear output'), 'w_nx_n') (('pictures', 'previous tests'), 'show') (('br IntroductionThis work', 'multiclass classification'), 'describe') (('this', 'more neurons'), 'be') (('aim', 'initial input'), 'be') (('similar analysis', 'et'), 'make') (('we', '3rd classes'), 'see') (('we', 'model'), 'indicate') (('last dense layer', 'class'), 'get') (('we', 'experiments'), 'plan') (('we', '128 neurons'), 'add') (('we', 'more accurate network'), 'capture') (('convolution layer', 'original input'), 'infty') (('it', 'backpropagation'), 'be') (('model', 'unseen data'), 'give') (('model', 'training sets'), 'seem') (('we', 'classifier'), 'have') (('very it', 'inputs'), 'be') (('that', 'dark gray main diagonal show'), 'square') (('We', 'GPU aditional time'), 'ommite') (('This', 'next steps'), 'guide') (('we', 'training'), 'Regularization') (('it', 'convergence'), 'cause') (('we', 'optimizer'), 'go') (('just sum de absolute values', 'weights'), 'sum') (('Negatives True TN', 'original class'), 'predict') (('L2', 'that'), 'deal') (('we', 'dropout more agressive rate'), 'adress') (('Dropout', 'simple approach'), 'LayersHow') (('we', 'input original dimensions'), 'be') (('function', 'images'), 'call') (('Looking', 'classes'), 'disguise') (('we', '1'), 'list') (('we', 'one more convolutional layer'), 'TechniquesGiven') (('it', 'non problem'), 'use') (('deer dogs frogs', 'ships'), 'represent') (('visually how many labels', 'Classifier Confusion MatrixIt'), 'show') (('bad behavior', '3'), 'one') (('It', 'machine learning research'), 'be') (('efficiency', 'previous theoretical sections'), 'detail') (('we', 'them'), 'plan') (('we', 'labels'), 'add') (('that', 'machine commonly learning'), 'description') (('classifier', 'labels'), 'cleare') (('It', 'kaggle python Docker image https github'), 'com') (('maybe it', 'training even set'), 'seem') (('such patterns', 'also training'), 'help') (('that', 'too problen'), 'make') (('we', 'machine learning pipeline'), 'aim') (('previous layer', 'biases'), 'compute') (('It', 'convolutional layers'), 'apply') (('Most', 'Cats specially Dogs'), 'spend') (('Deer class', 'mostly 6'), 'misclassifie') (('we', 'image'), 'apply') (('validation', 'datasets'), 'have') (('specially classes', '5 false positives'), 'see') (('classifier', 'classes'), 'plan') (('general it', 'problematic classes'), 'reach') (('they', 'unseen data'), 'be') (('we', 'model'), 'be') (('model', 'previous report'), 'have') (('class', 'very Dog'), 'be') (('distribution', 'zero'), 'Data') (('decision thus boundaries', 'more thus overfitting'), 'force') (('classes', 'confusion matrix'), 'seem') (('it', 'multiple classes'), 'compare') (('Project Image 5th Classification', 'Author Arthur Dimitri Brito Oliveira 10 arthur'), 'CIFAR') (('probability very low doesn', 'model results'), 'have') (('5th class', 'also correctly true positives'), 'have') (('Batch NormalizationTo', 'activation previous layer'), 'increase') (('convolutional layers', 'validation set'), 'research') (('It', 'image'), 'give') (('mathematical that', 'curve pointwise oftheir multiplication'), 'Layers') (('it', 'sufficiently problem'), 'analyze') (('we', 'pixel such scaling'), 'recommend') (('What', 'output hence image'), 'show') (('we', 'more experiments'), 'find') (('neuron', 'result'), 'be') (('loss curve', 'also better subsequent epoch'), 'behave') (('t', 'server'), 'list') (('main objective', 'scratch'), 'develop') (('main objective', 'previously begining'), 'conclude') (('It', 'more powerful simply accuracy'), 'be') (('Precision frac TP TP FP Recall Recall', 'actual class'), 'measure') (('Positives True TP', 'correctly classes'), 'predict') (('softmax function', 'n classes'), 'apply') (('Testing', 'initial results'), 'give') (('we', 'actual label'), 'be') (('we', 'diagonal'), 'see') (('when we', 'fully connected layers'), 'be') (('scoreFirst we', 'terms'), 'see') (('class', 'class elements'), 'be') (('neuron', 'first layer'), 'be') (('how many elements', 'really it'), 'be') (('taking', 'activity'), 'w_nx_n') (('that', 'patch'), 'be') (('which', 'layer'), 'use') (('model', 'improvement'), 'seem') (('that', 'k prop'), 'read') (('Negatives False when element', 'belonging class'), 'fn') (('optimizers', 'training set'), 'see') (('this', 'overfitting behavior'), 'start') (('performance', 'more complex model'), 'indicate') (('Recall frac TP TP FN scoreF1 Score', 'weighted Precision'), 'f1') (('that', 'relatively low error'), 'want') (('that', 'final model'), 'present') (('Neurons', 'only receptive fields'), 'connect') (('what', 'features'), 'in') (('we', 'isn recommended strategy'), 'find') (('augmentation procedures', 'same way'), 'make') (('which', 'output'), 'apply') (('it', 'regularization slight effects'), 'reduce') (('So we', '0'), 'try') (('we', 'informations'), 'compare') (('Positives False FP', 'wrongly element'), 'predict') (('results', 'just architecture'), 'Network') (('read_csv Input data files', 'read'), 'be') (('long time', 'hyperparameter few tunning'), 'relate') (('it', '2x2 kernel'), 'be') (('layer didn aditional convolutional t', 'improvement'), 'make') (('model', 'true positives'), 'be') (('This', 'data augmentation same procedures'), 'motivate') (('model', 'good generalizations'), 'stick') (('N', 'tensor'), 'reshape') (('high overshoot', 'loss value'), 'make') (('we', 'loss curves'), 'notice') (('neurons', 'certain rate'), 'mean') (('that', 'example'), 'expect') (('we', 'overfitting'), 'be') (('We', 'weight decay also fine regularization'), 'tune') (('inputs', 'ouput'), 'LayerConsists') (('Flatten LayerFlattening', 'one dimension'), 'mean') (('we', 'approach'), 'show') (('Pooling LayersMaximum pooling', 'feature map'), 'calculate') ", "extra": "['biopsy of the greater curvature', 'test', 'procedure']"}