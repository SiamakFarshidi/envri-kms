{"name": "welcome to deep learning cnn 99 ", "full_name": " h2 Convolutional Neural Networks h2 Load the data h2 Train the model h2 Evaluate h2 Submit ", "stargazers_count": 0, "forks_count": 0, "description": "Here they have 16 32 filters that use nine weights each to transform a pixel to a weighted average of itself and its eight neighbors. This forces the net to learn features in a distributed way not relying to much on a particular weight and therefore improves generalization. In contrast what I will show you here is nearly state of the art. As you can see there are quite a few parameters that could be tweaked number of layers number of filters Dropout parameters learning rate augmentation settings. Not too bad considering the minimal amount of training so far. Good luck linear algebra convert to one hot encoding example Increase this when not on Kaggle kernel 1 for ETA 0 for silent For speed. a 10x1 array with one 1 and nine 0 s with the position of the 1 showing us the value. It s easy however to create a net that overfits with perfect results on the training set and very poor results on the validation data. Each data point consists of 784 values. Here in the Kernel we will only look at each image 4 5 times so the difference is smaller. If done in the right way it can force the net to only learn translation invariant features. If this happens you could try increasing the Dropout parameters increase augmentation or perhaps stop training earlier. The MaxPooling layers just look at four neighboring pixels and picks the maximal value. EvaluateWe only used a subset of the validation set during training to save time. We use a Keras function for augmentation. I will show you how it is done in Keras which is a user friendly neural network library for python. If we use the standard initialization methods for weights however data between 0 and 1 should make the net converge faster. Now let s check performance on the whole validation set. If you instead wants to increase accuracy try adding on two more layers or increase the number of filters. Ignore the 100 results on the leaderboard they were created by learning the test set through repeat submissions Here goes 1 http yann. A fully connected net just treats all these values the same but a CNN treats it as a 28x28 square. We then speed things up only to reduce the learning rate by 10 every epoch. csv y_hat consists of class probabilities corresponding to the one hot encoding of the training labels. Convolutional Neural NetworksIf you want to apply machine learning to image recognition convolutional neural networks CNN is the way to go. io backend set_image_dim_orderingIt would be possible to train the net on the original data with pixel values 0 to 255. SubmitTo easily get to the top half of the leaderboard just follow these steps go to the Kernel s output and submit submission. Batch Normalization is a technical trick to make training faster. If you train this model over hundreds of epochs augmentation will definitely improve your performance. If you have Keras installed for Theano backend you might start seeing some error message soon related to channel ordering. com exdb lenet If you don t already have Keras 1 you can easily install it through conda or pip. In the Kernel 20 minutes training we will achieve 99 but if you train it overnight or with a GPU you should reach 99. As it is only nine weights we can stack many convolutional layers on top of each other without running out of memory time. Keras has a function for this We will use a very small validation set during training to save time in the kernel. With proper training we should get really good results. I now select the class with highest probabilitySubmitting from this notebook usually gives you a result around 99 with some randomness depending on weight initialization and test train data split. We train once with a smaller learning rate to ensure convergence. Dropout is a regularization method where the layer randomly replaces a proportion of its weights to zero for each training sample. This is often done with trial and error and there is no easy shortcut. This notebook is written for the tensorflow channel ordering. This can easily be solved 1. As the same nine weights are used over the whole image the net will pick up features that are useful everywhere. This means generating more training data by randomly perturbing the images. Many other notebooks here use a simple fully connected network no convolution to achieve 96 97 which is a poor result on this dataset. It has been sweeping the board in competitions for the last several years but perhaps its first big success came in the late 90 s when Yann LeCun 1 used it to solve MNIST with 99. The most important part are the convolutional layers Conv2D. Another important method to improve generalization is augmentation. If you then ensemble over several runs you should get close to the best published accuracy of 99. The model needs to be compiled before training can start. This reduces the size of the image by half and by combining convolutional and pooling layers the net be able to combine its features to learn more global features of the image. It relies on either tensorflow or theano so you should have these installed first. Metrics is only used for evaluation. If you ve successfully come this far you can now create similar CNN for all kinds of image recognition problems. Train the modelKeras offers two different ways of defining a network. Keras is already available here in the kernel and on Amazon deep learning AMI. Thes two graphs explain the difference It s easy to understand why a CNN can get better results. See the example with the position of the 1 showing the correct value for the digit in the graph above. As optimizer we could have used ordinary stochastic gradient descent SGD but Adam is faster. We now reshape all data this way. If this had been RGB images there would have been 3 channels but as MNIST is gray scale it only uses one. The labels were given as integers between 0 and 9. relu is the activation function x max x 0. Getting convergence should not be a problem unless you use an extremely large learning rate. 3 by averaging over 5 good runs and you can get higher than that if you train overnight. We need to convert these to one hot encoding i. In fact we have only gone through the training data approximately five times. In the end we use the features in two fully connected Dense layers. io Load the dataAs always we split the data into a training set and a validation set so that we can evaluate the performance of our model. We will the Sequential API where you just add on one layer at a time starting from the input. Keras wants an extra dimension in the end for channels. As our loss function we use logloss which is called categorical_crossentropy in Keras. ", "id": "toregil/welcome-to-deep-learning-cnn-99", "size": "6749", "language": "python", "html_url": "https://www.kaggle.com/code/toregil/welcome-to-deep-learning-cnn-99", "git_url": "https://www.kaggle.com/code/toregil/welcome-to-deep-learning-cnn-99", "script": "Flatten sklearn.metrics keras.preprocessing.image keras.layers keras.callbacks train_test_split Dropout Sequential MaxPool2D to_categorical # convert to one-hot-encoding Adam keras.utils.np_utils Conv2D numpy sklearn.model_selection confusion_matrix ImageDataGenerator matplotlib.pyplot Dense keras.optimizers BatchNormalization LearningRateScheduler keras.models ", "entities": "(('Dropout', 'rate augmentation settings'), 'be') (('far you', 'image recognition problems'), 'create') (('This', 'randomly images'), 'mean') (('we', 'training only data'), 'go') (('we', 'really good results'), 'get') (('notebook', 'channel tensorflow ordering'), 'write') (('gray it', 'only one'), 'be') (('that', 'features'), 'use') (('We', 'augmentation'), 'use') (('it', 'only translation invariant features'), 'force') (('we', 'Dense two fully connected layers'), 'use') (('which', 'neural network user friendly python'), 'show') (('io backend', '255'), 'be') (('This', 'therefore generalization'), 'force') (('where you', 'input'), 'will') (('CNN', 'recognition convolutional neural networks'), 'want') (('you', 'learning extremely large rate'), 'be') (('you', '99'), 'ensemble') (('1 you', 'conda'), 'lenet') (('you', 'filters'), 'try') (('which', 'poor dataset'), 'use') (('net', 'image'), 'reduce') (('which', 'Keras'), 'use') (('This', 'often trial'), 'do') (('Now s', 'validation whole set'), 'let') (('that', 'validation very poor data'), 's') (('important method', 'generalization'), 'be') (('why CNN', 'better results'), 'Thes') (('MaxPooling layers', 'maximal value'), 'look') (('Keras', 'deep learning AMI'), 'be') (('you', 'perhaps training'), 'try') (('labels', '0'), 'give') (('csv', 'training labels'), 'consist') (('Train', 'network'), 'offer') (('Adam', 'SGD'), 'use') (('you', 'definitely performance'), 'improve') (('show', 'here nearly art'), 'be') (('Keras', 'channels'), 'want') (('We', 'convergence'), 'train') (('just steps', 'submission'), 'get') (('you', 'channel soon ordering'), 'start') (('you', 'that'), '3') (('4 5 times difference', 'only image'), 'look') (('only nine we', 'memory time'), 'stack') (('We', 'encoding one hot i.'), 'need') (('luck linear Good algebra', '0 silent speed'), 'convert') (('Yann when LeCun', '99'), 'sweep') (('that', 'itself'), 'have') (('We', 'kernel'), 'have') (('regularization where layer', 'training sample'), 'be') (('same CNN', '28x28 square'), 'treat') (('Batch Normalization', 'technical training'), 'be') (('we', 'model'), 'split') (('they', 'http Here 1 yann'), 'ignore') (('these', 'tensorflow'), 'rely') (('you', '99'), 'achieve') (('net converge', 'however 0'), 'datum') (('We', '10'), 'speed') (('I', 'test train data split'), 'select') ", "extra": "['test']"}