{"name": "elo use of featuretools permutation importance ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "Then I decided to try featuretools for automated feature engineering and use Permutation Importance to select the relevant features. Most of the features are categorical in datasets but marked as numerical so manually I had to change the variable type of many features to Id and Categorical to avoid getting rubbish features. I started by developing ther baseline model using the features available in the training dataset achieved score of around 3. A better approach could be to label encode all the features before performing will make that optimization going forward. While using featuretools for automated feature engineering below are some of the challenges I encountered 1. Adding relationships to the entity set is so natural in featuretools just like anybody would be dealing with databases. This is simply selecting the features based on a threshold from Permutation Importance. 001 I remove almost 70 of the features generated from featuretools. Possible that Feature importances and SHAP values can give better results but I do not understand their maths very well especially the SHAP values. Now comes the interesting part of running featuretools to get automated feature engineering. To increase accuracy further I see two options please let me know if you have other ideas as well 1. Comes the final part of feature engineering DFS or Deep Feature Synthesis where different features get stacked up that s mean the term deep comes from. With more depth you get transformation of transformations that can be useful at times but are very hard to interpret so avoiding that. To select features I am using the 3 methods 1. linear algebra data processing CSV file I O e. Using Light GBM generally I prefer XGBoost but they provide similar accuracy. Turned off GPU to increase RAM from 14GB to 17GB. If someone had to do all this manually this is the way to go about it. Hyper tune the paramters in Light GBM XGBoost model. One I need to thoroughly understand how cards business work but then lot of features are anonymized and it is difficult to comprehend them. SHAP valuesI prefer to perform feature selection based on Permutation Importance simply because that s the best I understand. I will try learning them better. This library might take a little while to understand completely but an excellent and elegant solution. The biggest problem by far at least in this competition was the large size of historical transactions file. As I mentioned above due to memory constraints I had to do this twice by deleting the intermediary files in between. In built feature importance of Light GBM2. Permutation Importance3. Everytime I was running out of RAM and the kernel died. Same featuretools engineering for the test set. Next I did some manual feature engineering by aggregating purchase amount feature from historical and new merchant transactions getting an accuracy of roughly 3. Performed operations on each dataset and then immediately deleted them to retain precious memory. Perform manual feature engineering based on domain knowledge. I have kept the maximum depth as 1. Permutation Importance is easy to comprehend and a natural way to remove useless features if you randomly shuffle a feature and it doesn t reduce your accuracy then that feature is not a good indicator. So even with a low threshold of 0. Performed featuretools feature engineering twice on train and test dataset that I could have done in 1 go by combining the two sets. Tackling this I had to take some redundant steps but yes they worked. These are some features that contained string data so label encoding them below. One problem that I am facing not able to resolve completely due to the fact that automtaed feature engineering has generated so many junk features that any single feature is having very less impact on overall accuracy. 98 implying that the features in training dataset were not very useful. ", "id": "tandonarpit6/elo-use-of-featuretools-permutation-importance", "size": "3930", "language": "python", "html_url": "https://www.kaggle.com/code/tandonarpit6/elo-use-of-featuretools-permutation-importance", "git_url": "https://www.kaggle.com/code/tandonarpit6/elo-use-of-featuretools-permutation-importance", "script": "featuretools sklearn.metrics sklearn.feature_selection sklearn.model_selection LabelEncoder mean_squared_error pyplot numpy lightgbm matplotlib sklearn.preprocessing pandas pyplot as plt eli5.sklearn SelectFromModel train_test_split PermutationImportance ", "entities": "(('Then I', 'relevant features'), 'decide') (('manually this', 'it'), 'be') (('I', 'featuretools'), '001') (('they', 'redundant steps'), 'tackle') (('I', '1'), 'be') (('they', 'similar accuracy'), 'prefer') (('features', 'training dataset'), 'imply') (('Now interesting part', 'feature automated engineering'), 'come') (('I', 'Permutation Importance'), 'prefer') (('This', 'Permutation Importance'), 'select') (('I', 'two sets'), 'feature') (('I', 'around 3'), 'start') (('kernel', 'RAM'), 'run') (('numerical so manually I', 'rubbish features'), 'be') (('that', 'very so that'), 'get') (('I', 'twice intermediary files'), 'have') (('I', 'SHAP very well especially values'), 'possible') (('Hyper', 'GBM XGBoost Light model'), 'tune') (('it', 'them'), 'one') (('optimization', 'features'), 'be') (('I', '3 methods'), 'select') (('library', 'little while'), 'take') (('you', 'as well 1'), 'see') (('term', 'that'), 'come') (('Performed operations', 'precious memory'), 'delete') (('single feature', 'overall accuracy'), 'problem') (('then feature', 't accuracy'), 'be') (('biggest problem', 'transactions large historical file'), 'be') (('Next I', 'roughly 3'), 'do') (('that', 'so them'), 'be') (('just anybody', 'databases'), 'be') ", "extra": "['test']"}