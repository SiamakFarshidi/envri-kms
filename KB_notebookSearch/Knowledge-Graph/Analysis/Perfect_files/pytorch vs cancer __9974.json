{"name": "pytorch vs cancer ", "full_name": " h2 Introduction h2 Load required libraries h2 Understand the Dataset h3 Annotations h3 Candidates h2 Missing data h2 Load the data h2 Train the Model h2 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "More information https luna16. There are two CSV files that we ll be working with annotations. com gzuidhof full preprocessing tutorial has a great walkthrough and visualization of this data format. png We will create a namedtuple to store the information that we combine from the candidates and annotations. py The imports have slightly been modified to make the code work pylint disable unidiomatic typecheck Make a copy of all the candidates. Next we create DataLoader s out of our datasets. The code used to set up caching is from the Deep Learning with PyTorch book and can be found in the book s GitHub repository https github. Another thing to note is that there can be multiple annotations and candidates in a single CT scan. The SimpleITK package has a very simple API that we ll use to get details about the CT scan. com sentdex first pass through data w 3d convnet. Candidates that have a value of False for is_nodule will come last in the list with the same relative order of diameters as above. IntroductionThis notebook was created as a part of the Weights Biases PyTorch Book Reading Group https community. org articles hounsfield unit which goes from 1000 to 3000. We need to also convert the output we want from the model is_nodule into a PyTorch tensor. Now let s put it all together in a few utility functions and a Dataset class. com sentdex s TensorFlow model in this notebook https www. We read the CT scan and store it as a NumPy array. png attachment b9b5d21a 10c2 437e bc94 383e0182f2c5. Therefore when looking for a candidate s diameter from the diameters dict we ll assume that if the center coordinates of the candidate are less than half the radius of the annotated nodule away from the center coordinates of the annotated nodule then they are the same nodule. have the same seriesuid. We will walk through and understand all the steps using a single candidate before putting the code together into utility functions and a PyTorch Dataset. That path will be available at index 0 of the result. The value will be a list of all center coorinates in the CT scan with that seriesuid Loop through every annotation Create a tuple to represent the center Append the center to the corresponding seriesuid Using a namedtuple makes it easy to access values in a tuple using indexes or field names A list to store all candidates in the dataset Create a tuple to represent the candidate center We suffix the name with _xyz to make it clear that we re using the patient coordinate system http dicomiseasy. The next step would be to convert this chunk of CT scan to a PyTorch tensor. The numbers below the accuracy give us a clearer picture of what is going on. The huge difference between candidates and annotations tells us that we will have many candidates for which we won t have a diameter in the annotations file. The cell below finds all such seriesuid s. Pick every 350th candidate so that we have about 1k candidates in the dataset It takes agonizingly long to load more data If this is the validation set keep every validation_stride item If this is the training set delete every validation_stride item Get the i th candidate We want to resize each CT scan to the following dimensions Use the utility function to fetch the CT scan Convert the CT scan to a tensor Convert the target to a tensor Put the model in training mode to activate dropout Keep a track of the loss and correct predictions for the epoch Track the total number of positives and true positives Track the total number of negatives and true negatives Standard PyTorch training process Calculate loss and correct predictions in batch Calculate positives and true positives in batch Calculate negatives and true negatives in batch Put the model in evaluation mode to deactivate dropout Keep track of loss predictions and other numbers we are interested in just like in the training loop Don t calculate gradients Dimensions of X BS 1 10 18 18 Create an instance of the model Use the GPU if it is available Use the cross entropy loss function Use the AdamW optimizer. The diameters dict will have each seriesuid as a key. class is 0 if the mass isn t a nodule and 1 if it is a nodule both malignant and benign. That sentence was a mouthful Maybe this diagram will be easier to understand image. series_uid elif len filepaths 1 multiple_cts. Most of the code used to read the data comes from the amazing book we re reading in the group Deep Learning with PyTorch https www. The idea was to find a notebook on Kaggle with a TensorFlow model trained on the Luna16 dataset and try to convert it to PyTorch. It would be unrealistic to expect the model to learn how to predict positive samples with just a couple of data points. For now we ll try not to worry about this and move on with the data that we have left. org SimpleITK Notebooks 01_Image_Basics. We can now train the model. We will be using the cross entropy loss https ravimashru. Similarly we group the candidates that are part of the same CT scan and then use the diameters dictionary we created above to fetch each candidate s diameter. com sentdex first pass through data w 3d convnet in PyTorch. There are over 750k candidates. Load required libraries Understand the DatasetThe Luna16 Lung Nodule Analysis 2016 dataset contains chest CT scans and annotations indicating where there are nodules in each CT scan and their diameters. glob f kaggle input luna16 subset c. ai c community events pytorch book 32 hosted by Sanyam Butani https www. Since the list contains tuples the order of fields will determine the way the list is sorted. The files could be in any one of the subset folders in the dataset. We lost almost 50 of the data PANIC Well we d panic if this wasn t something we re just playing with. raw files associated with the candidate. The glob package can return multiple files that match the pattern specified. com books deep learning with pytorch. There are 1 186 total annotations available. However if fine for now as we won t be using the diameter information when building this simple model. This will allow us to easily access the centers and respective diameters of all nodules in a particular CT scan. csv and candidates_V2. dev blog 2021 07 18 understanding cross entropy loss function to train our model so we need two columns for the output one hot encoded values of the boolean is_nodule value we re interested in. That s not the entire story. We also get the following information Center point of reference of the CT scan also known as the origin Size of each voxel short for volume pixel since each CT scan can have a different size of voxels Direction matrix that has a direction vector for of each axis in the CT scanWe convert the origin and voxel size to NumPy arrays so that they are easier to use in calculations. So this model is not very useful right now. AnnotationsThe annotations file contains the center and diameter of each mass in CT scans. All candidates with a diameter of zero recall this happens when the candidate coordinates are not close enough to any annotation coordinates or there is no corresponding annotation for the particular candidate will come after these. We can now load this file using the SimpleITK package. Load the dataWe ll now walk through how we want to convert the data we have into a format that we can consume with PyTorch. org The model we will build will try to predict whether a particular region of a CT scan has a nodule or not. com deep learning with pytorch dlwpt code issues 27 The code in this cell is from the Deep Learning with PyTorch book s GitHub repository https github. html available if you want to read more. png Missing dataThere are some seriesuid s in the dataset that don t have corresponding CT scans in the dataset. We will extract a chunk by getting a list of three slices one for each direction and then using that to extract the actual values from the CT scan. This notebook is my attempt to train the model that Sentdex https www. However for simplicity we ll assume that the dataset contains only one. The actual values in the CT scan are in Hounsfield units HU https radiopaedia. That s equivalent to 41 RGB color images of resolution 512x512 for a single CT scan Since most of the CT scan doesn t contain any interesting to us we will extract 3 dimensional chunks of the CT scan that contain nodules as input for our model. There are 443 seriesuid s in the annotations and candidates CSV files that don t have corresponding. Let s say we want to extract a chunk of size 10 along the index column and 18 rows and columns. seriesuid is the unique identifer of the CT scan. Among these candidates those with the largest diameter will come before those with smaller diameters. mhd if len filepaths 0 missing_cts. Also because our dataset is highly imbalanced we need a better strategy to train our model and also a better indication of model performance instead of accuracy. coordX coordY and coordZ are coorindates of the center of the mass. com deep learning with pytorch dlwpt code blob master util disk. png attachment 93cf076d 063b 4b3f 90d4 1a1ab3f6fe04. Since it takes about 10 minutes to run I have made those seriesuid s available as a separate dataset https www. We then sort the list of candidates in reverse order. The first epoch takes a long time but once the data is cached the other epochs are super fast ConclusionWe ve got an accuracy of over 99 on the validation set Time to pop the champagne But hang on. First let us set up some utility functions for caching the dataset. We now remove from our list of candidates those that don t have a CT scan. We use a range of 1000 or 1000 to remove extremely dense materials from the CT scan. Train the ModelThis model is a PyTorch version of Sentdex https www. CandidatesThe candidates file contains a class flag for each mass in the CT scans. The glob module allows us to find the file by using patterns instead of manually looking inside each of the folders. We will first group all annotations that are part of the same CT scan i. I guess we ll have to keep that champagne on the ice for a little longer. We now convert the origin and voxel size from the patient coorindate system previously mentioned to coordinates that we can use to index into the NumPy array representing the CT scan. We ll load data from this dataset to save precious GPU time on Kaggle. If this is the case then the candidate will have a diameter of 0. We now create an instance of the model the loss function and an optimizer to train the model. We use a VALIDATION_STRIDE of 10 which means every 10th CT scan will be in the validation set. if it terminates because all iterations are complete and not by a break statement So if we go into this else block then all 3 coordinates are within half the radius and we can consider the candidate and the annotation as the same nodule We don t need to look at any other remaining annotations because we ve already found a match time from concurrent. This notebook https www. In particular after sorting we ll have all candidates with the value of is_nodule as True at the beginning of the list. They have great documentation https simpleitk. Going forward we ll need the scan to be in the order index row column We will create three slices one for each direction to use to extract a region of interest from the CT scan Get start and end index for the dimension so that the nodule center is at the center of the 3d array we extract Adjust the indexes if the start_index is out of the CT scan array Do the same check for the end_index Create a tensor from the NumPy array of the CT scan chunk convert it to a tensor of float32 Add an extra dimension to represent a single channel in the 3d image We need to change the imports and use some other libraries for caching to work See https github. The center of the nodule is at index irc in the complete scan ct_scan. html We begin by assuming the candidate doesn t have a corresponding annotation. The X Y and Z coordinates of the center can be slightly different in the annotations and candidate files. The entire CT scan is currently very large for us to work with. mhd inside the subset folders We removed all candidates that don t have corresponding CT scan files This line is another fail safe to know when a CT scan doesn t exist Convert the coordinates of the center of the candidate from the patient coordinate system to column row index Since we ll be using column row and index values to index into arrays we round them to the nearest integer. series_uid return missing_cts multiple_cts all_missing all_multiple with ThreadPoolExecutor as executor total len candidates middle total 2 quarter middle 2 startidx 0 quarter middle middle quarter endidx quarter middle middle quarter total for res in executor. We now have a CT scan chunk ct_scan_chunk. futures import ThreadPoolExecutor def find_missing_and_multiple startidx endidx missing_cts multiple_cts for c in tqdm candidates startidx endidx filepaths glob. mhd file for a given seriesuid. As we ll see later this will significantly speed up training after the first epoch. com sentdex built in his notebook First pass through Data w 3D ConvNet https www. com 2013 06 getting oriented using image plane. We then fetch the diameters of the CT scan we re looking at currently and loop over them to find a match for the candidate Extract the center and diameter of the annotation from the tuple For each of the coordinates X Y and Z we check if the candidate and the annotation are close by remember the really long and complicated sentence above Since we ve stored coordinates as tuples we can index into them Find the absolute difference between the two coordinates If the coorindate of the candidate is more than half the radius away we don t consider it the same nodule as the annotation we re currently looking at The else block of a for loop in Python executes if the loop ends naturally i. map find_missing_and_multiple startidx endidx all_missing res 0 all_multiple res 1 missing_uids uid for uid in all_missing Look for the file. With the utility function and caching set up we can now create a PyTorch datset. com mashruravi luna16missingcandidates. The model predicted all negatives correctly but didn t make any correct predictions for the positive data points. We use the glob module to find the. This probably happened because we don t have enough positive samples in the training and validation set. We now have three slices we can use in each direction to extract the chunk we need. ", "id": "mashruravi/pytorch-vs-cancer", "size": "9974", "language": "python", "html_url": "https://www.kaggle.com/code/mashruravi/pytorch-vs-cancer", "git_url": "https://www.kaggle.com/code/mashruravi/pytorch-vs-cancer", "script": "torch.optim torch.utils.data __init__ BytesIO fetch GzipDisk(Disk) eval_loop MODE_BINARY diskcache.core DataLoader getCache ThreadPoolExecutor find_missing_and_multiple forward torch.nn collections core numpy store __getitem__ concurrent.futures io LunaModel(nn.Module) LunaDataset(Dataset) cassandra.cqltypes tqdm.notebook pandas Dataset __len__ BytesType tqdm Disk diskcache getCtScanChunk namedtuple SimpleITK FanoutCache train_loop ", "entities": "(('We', 'NumPy array'), 'read') (('we', 'coordinate patient system'), 'be') (('CT 10th scan', 'validation set'), 'use') (('2016 dataset', 'CT where scan'), 'require') (('what', 'clearer picture'), 'give') (('center', 'complete scan'), 'be') (('notebook', 'Weights Biases'), 'create') (('code', 'GitHub repository https github'), 'use') (('idea', 'PyTorch'), 'be') (('us', 'folders'), 'allow') (('we', 'above diameter'), 'group') (('annotations file', 'CT scans'), 'contain') (('actual values', 'Hounsfield units HU https radiopaedia'), 'be') (('next step', 'PyTorch tensor'), 'be') (('didn correctly t', 'data positive points'), 'predict') (('highly we', 'instead accuracy'), 'also') (('we', 'candidates'), 'create') (('path', '0 result'), 'be') (('we', 'boolean value'), 'blog') (('we', 'PyTorch'), 'load') (('org articles hounsfield which', '3000'), 'unit') (('s', 'dataset https separate www'), 'make') (('dataset', 'only one'), 'assume') (('thing', 'CT multiple single scan'), 'be') (('We', 'model'), 'create') (('We', 'CT scan'), 'use') (('full preprocessing', 'data format'), 'have') (('we', 'CT scan'), 'convert') (('Now s', 'utility together a few functions'), 'let') (('We', 'https See github'), 'create') (('X Y coordinates', 'slightly annotations'), 'be') (('futures import ThreadPoolExecutor find_missing_and_multiple startidx endidx missing_cts def multiple_cts', 'tqdm candidates'), 'startidx') (('model', 'data points'), 'be') (('They', 'documentation https great simpleitk'), 'have') (('they', 'calculations'), 'get') (('Maybe diagram', 'image'), 'be') (('then candidate', '0'), 'have') (('particular we', 'list'), 'have') (('all_multiple', 'file'), 'map') (('We', 'Kaggle'), 'load') (('particular region', 'nodule'), 'org') (('seriesuid', 'CT unique scan'), 'be') (('unidiomatic typecheck', 'candidates'), 'modify') (('We', 'CT scan chunk now ct_scan_chunk'), 'have') (('We', 'CT scan'), 'extract') (('we', 'that'), 'try') (('we', 'PyTorch now datset'), 'create') (('we', 'PyTorch tensor'), 'need') (('loop', 'Python'), 'fetch') (('However we', 'when simple model'), 'use') (('we', 'ice'), 'guess') (('27 code', 'PyTorch GitHub repository https github'), 'com') (('we', 'Deep PyTorch https www'), 'use') (('DataLoader', 'datasets'), 'create') (('probably we', 'training set'), 'happen') (('First us', 'dataset'), 'let') (('com sentdex', 'Data w ConvNet https First 3D www'), 'pass') (('we', 'nearest integer'), 'remove') (('diameters dict', 'key'), 'have') (('we', 'index 10 column'), 'let') (('don t', 'corresponding'), 'be') (('we', 'concurrent'), 'terminate') (('we', 'annotations file'), 'tell') (('corresponding annotation', 'these'), 'happen') (('those', 'smaller diameters'), 'come') (('Train', 'PyTorch Sentdex https www'), 'be') (('com sentdex', 'PyTorch'), 'pass') (('we', 'chunk'), 'have') (('we', 'CT scan'), 'have') (('don t', 'CT scan'), 'remove') (('later this', 'first epoch'), 'speed') (('then they', 'annotated nodule'), 'dict') (('it', 'AdamW optimizer'), 'pick') (('files', 'dataset'), 'be') (('list', 'way'), 'tuple') (('candidate doesn t', 'corresponding annotation'), 'begin') (('We', 'utility together functions'), 'walk') (('us', 'CT particular scan'), 'allow') (('We', 'reverse order'), 'sort') (('We', 'SimpleITK package'), 'load') (('CandidatesThe candidates', 'CT scans'), 'file') (('that', 'diameters'), 'come') (('ConclusionWe super fast ve', 'champagne'), 'take') (('that', 'model'), 's') (('coordX coordY', 'mass'), 'be') (('We', 'loss https cross entropy ravimashru'), 'use') (('notebook', 'model'), 'be') (('executor len total candidates', 'executor'), 'multiple_ct') (('don t', 'CT corresponding dataset'), 'be') (('com sentdex', 'data w 3d first convnet'), 'pass') (('that', 'same CT'), 'group') (('CSV two we', 'annotations'), 'be') (('pattern', 'multiple files'), 'return') (('we', 'something'), 'lose') ", "extra": "['annotation', 'patient', 'test', 'lung']"}