{"name": "heart disease prediction w knn naivebayes trees ", "full_name": " h2 Problem Statement and Objective h2 Dataset h2 Data Preprocessing h2 Handling Duplicates and Outliers Data Cleaning h2 Feature Selection Data Reduction h2 Dummy Variables Data Transformation h2 Exploratory Data Analysis EDA h2 Dimension and Variables or Attributes of Dataset h2 Frequency Distribution Table h2 Measure of Central Tendency and Dispersion h2 Skewness of Dataset h2 Correlation Matrix h2 Visualization of Attributes h2 Realibility Fitting Probability Distribution h1 Model Development h2 K Nearest Neighbours KNN Modelling h2 Decision Tree Classification h2 Naive Bayes Classifier h2 Performance Evaluation of K Nearest Neighbour Decion Tree and Naive Bayes Classification Model h2 Interpretation of Model h2 References ", "stargazers_count": 0, "forks_count": 0, "description": "thalach maximum heart rate achieved 9. The Weibull distribution is widely used in reliability and life data analysis due to its versatility. oldpeak ST depression induced by exercise relative to rest 11. However all the scatter plot are dispersed with correlation of absolute value less than 0. However the above conclusions are induced from the basic visualization of the attributes. Hence to determine the optimal K number of neighbours for this case study the model performance scorse is calculated and assessed for K of 1 to 20 using looping. 5 with asymptomatic chest pain with fasting blood sugar of greater than 120mg dl resting electrocardiographic showing proabable or definite left ventricular hypertropy by Estes criteria with exercise induced angina with upsloping slope of the peak exercise ST segment 0 number of major vessels colored by flourosopy and normal thal is predicted to suffer from heart disease. Evaluating machine learning models with a confusion matrix. Handling Duplicates and Outliers Data Cleaning Missing values are detected using the isnull syntax. online Available at Accessed 4th December 2020 Brownlee J. From the scatter plot person with high heart rate and low blood pressure will have higher chance to suffer from heart disease. This high accuracy model can be used as reference for healthcare instituitions for prediction of presence of heart disease in United States citizens. exang exercise induced angina 1 yes 0 no 10. The higher density of KDE plots implies the high probability of occurence. 5 with other predictors is chosen. Probabilistic Model Selection with AIC BIC and MDL. 9 Measures of Central Tendency and Dispersion. This would help to reduce the mortality rate caused by heart disease. online Available at Accessed 4th December 2020 CDC. skew fbs and ca are highly skewed. What When and How of Scatterplot Matrix in Python Data Analytics. Filter method is chosen as wrapper method such as Reduce Feature Elimination RFE function in scikit learn is limited to certain machine learning algorithms such as linear regression and decision tree. From the bar chart of Age versus frequency age of 41 to 45 and 51 to 54 are having a high amount of heart disease impling that they are highly risked people to have heart disease. online Available at Accessed 4th December 2020 Singh D. 5 IQR where Q1 and Q3 represents the first and third quartile respectively and IQR represents the interquartile range. Dummy Variables Data Transformation Data transformation is done by converting categorical attributes to dummy variables for the ease of model construction using the. As all attributes has a absolute correlation coefficient of less than 0. The lower score of AIC and BIC indicates a better probability distribution model. 5 to 1 is moderately skewed and highly skewed if more than 1. Characteristics of the Weibull Distribution. Weibull Distribution and Weibull Analysis. The model developed will reduce the burden for healthcare instituition as it is able to identify and predict the presence of heart disease efficiently. Thus in this assessment AIC and BIC are both taken into consideration in choosing the best probability distribution model. Since dummy varialbes is created hence there is no need to perform concept hierarchy or relevelling to reduce the factor levels of the attributes. In filter method only relevant features are taken. References Algorithmia. online Available at Accessed 4th December 2020 loading packages and libraries directory of dataset detect missing values print min and max of acceptable range since thalach is integer attribute rounded to integer rounded to integer cap outliers with min and max of acceptable range correlation matrix categorical variables to convert to dummy variables convert to dummy variables data frame with dummy variables combine dummy variables with dataset drop categorical variabes as they are converted to dummy variables scatter plot of radius and compactness scatter plot of radius and texture scatter plot of radius and smoothness scatter plot of radius and concavity scatter plot of radius and compactness scatter plot of radius and texture scatter plot of radius and smoothness scatter plot of radius and concavity correlation matrix split dataset to train and test split train and test for decision tree model 80 20 split suggested number of neighbours to determine optimum number of neighbours n_neighbors means k 1 as index starts from 0 in array modeling KNN to determine optimum number of maximum leaf nodes n_neighbors means k 1 as index starts from 0 in array since max leaf node of 6 to 9 has the same scores thus 9 is used for better accuracy. Is there any reason to prefer the AIC or BIC over the other. The normal distribution has a skewness of 0 McNeese 2016. Dimension and Variables or Attributes of Dataset The dataset is a dataframe with dimension consists of 31 features including the dummy variables and 301 instances as shown using the. online Available at Accessed 4th December 2020 JornalDev. Skewness of Dataset Skewness measures a dataset s symmetry or lack of symmetry. As shown in the correlation matrix all attributes has absolute correlation of less than 0. In this case study the worst case scenario is the False Negatives FN which is located in the top right of the confusion matrix. Thus KNN is chosen to be the model to predict presence of heart disease in this case study. Weibull distribution is commonly used assess product reliability analyze life data and model failure times from biology industry to economic industry Stephanie 2017. However the KNN model will perform better if it was trained with a larger dataset with more instances it will help improve the KNN model s capability to predict unknown data. Accoding to rule of thumb skewness of 0. 5 with each other hence there will be no multicollinearity. Like Decision Tree Classifier Naive Bayes Classifier is not affected by monotonic transformation hence normalization is not required Dernoncourt 2016. online Available at Accessed 4th December 2020 Kumar A. K Nearest Neighbours KNN Modelling K Nearest Neighbours KNN is a supervised learning algorithm which is also a classification algorithm. Features with absolute correlation coefficient of greater than 0. Normalization is done after Train and Test dataset split using the MinMaxScaler Kedarps 2017. Realibility Fitting Probability Distribution Probability distribution is fitted to the quantitative variables to select the best probability model. Heart disease is the leading cause of death in the United States with about 655 000 Americans die from heart disease each year and that is about 1 in every 4 deaths CDC n. Decision Tree Classification in Python. 5 with each other and also with target thus no attributes are removed from the dataset. Exploratory Data Analysis EDA Exploratory Data Analysis EDA analyses the dataset gives insight about dataset with visuals to discover patterns and abnormalies. The Weibull 2p or 2 parameters distribution consists parameters of shape and scale where Weibull 3p or 3 parameters distribution consists paramters of shape scale and location Weibull n. online Available at Accessed 4th December 2020 CK 12. Decision Tree Classification Decision Tree Classification is a supervised machine learning algorithm which has a tree like structure consisting of nodes leaves and branches. This is to determine the best model that best fit this case study of prediction of presence of heart disease. From the bar form correlation matrix chol and fbs has the least correlation with the target. Available at Accessed 4th December 2020 Navlani A. Decision trees has the capability of handling high dimensionality data with good accuracy. Model Development dataset2 is used for model development which consists of dummy variables created from categorical variables as shown in Data Preprocessing while doing Data Transformation. Diagonally from top left to right the plots represent univariate distribution of data for the variable in that column for instance distribution of age distribution of sex and others. online Available at Accessed 4th December 2020 Kaufmann J. A precise conclusion or rules should be extracted from the classification model. online Available at Accessed 4th December 2020 Khurana S. Data cleaning includes handling missing values and noises data transformation includes data discretization concept hierarchy normalization and standardization. There are many ways to evalute the model performance depending on the model. In this assessment various classification models will be developed and evaluated to determine the best model for prediction of heart disease in United States citizens. 077 thus it is considered as high variance thus oldpeak is spread out Kaufmann 2014. In layman terms AIC indicates the overfitting issues and BIC indicates the underfitting issues as they both penalize free parameters Ash3323 and Taylor 2018. The KNN model devolped for this case study is considered performing well with accuracy of 88. The mathematical symbol in skewness value indicates whether the variables is postiviely or negatively skewed. 5 is fairly symmetrical. Frequency Distribution Table Frequency Distribution Table for Categorical Variables are shown below which measures if the dataset is biased or having imbalance class or not. As Cleveland is one of the major city in United States the dataset used is relevant for use in prediction of heart disease in United States citizen. In other words scatter plot matrix represents bi variate or pairwise relationship between different combinations of variables in grid form Kumar 2020. Visualization of Attributes Attributes are visualized to discover general patterns in the attributes and have a rough conclusion about the attributes. This can be seen from the boxplot plotted in data cleaning too as the boxes are approximately evenly separated. Hence prediction of heart disease is one of the most important focus in the section of clinical data analysis Rawat 2019. The original dataset consists of 76 attributes but published experiments refer the dataset using a subset of 14 attributes with 303 instances. Since the model built in this case study is classification model thus confusion matrix is used. Naive Bayes Classifiers. The Coefficient of Variation CV is calculated by dividing standard deviation by mean. Dummy variables uses 0 or 1 to indicate absence or presence of the categorical value sorting them to mutually exclusive categories. The dataset originally consists of 14 attributes where 5 of them are quantitative attributes including integers and float and 9 of them are qualitative categorical attributes which were encoded to integers in the raw dataset. From the bar chart female tends to have higher risk to suffer from heart disease. Scatter plot matrix is a matrix of scatter plots where each scatter plot in the grid is created between different combinations of variables. From the plot shown above the optimal K value is 10 with a model performance score of 88. sex 1 male 0 female 3. Data reduction involves numerosity reduction and dimensionality reduction GeeksforGeeks 2019. The above scatter density plot matrix is associated with Kernel Density Estimation KDE plots which estimates the probability density of a variable and also the probability distribution JournalDev n. However Decision Tree is easily affected by imbalanced or biased dataset. Do you have to normalize data when building decision trees using R. In FN patients with heart disease is classified as no heart disease this might cause delay in treatment for the patient due to misdiagnosis and wrong prediction by the model causing death in serious cases. The dataset description are as below UCI Machine Learning Repository n. Part of the EDA has already performed in data preprocessing for outlier detection using Quartile method with boxplot and for data reduction using correlation matrix plot. KNN Classification using Scikit learn. Decision Tree Classifier Naive Bayes Classifier and K Nearest Neighbours will be modelled and evaluated to choose the best model with high performance metrics. 5 and F1 score of 87. Machine learning algorithms such as Naive Bayes Classification K Nearest Neighbour Logistic Regression Decision Tree Classification and others which helps to classifies the presence of heart disease with the given symptoms. The distance of the newly added point with the neighbours can be calculated using Euclidean distance formula and the distance is then sorted to find the nearest neighbours. Decision trees are not affected by monotonic transformation such as normalization hence normalization is not needed Rapaio 2015. A CV value of lesser than 1 indicates a low variance. AIC and BIC are both calculated using the maximum likelihood which maximizes the conditional probability of observing the data X given a specific probability distribution and its parameters theta Brownlee 2019. AIC and BIC works in a similar way but they focus on different situation. As accuracy sometimes may be misleading thus F1 score is also used to evaluate model performance. validate model by using test set to predict decision tree. online Available at Accessed 4th December 2020 Weibull. As shown in the histograms and scatter plots all the attributes are considered less disperse. Hence normalization is done to have a good KNN model performance. 05 mV Value 2 showing probable or definite left ventricular hypertrophy by Estes criteria 8. Dataset The dataset used is the Heart Disease Data Set from the Cleveland database created in 1988 by V. However oldpeak has CV value of 1. The dataset is loaded from the downloaded directory and the first five rows of dataset is shown above. Skewness with absolute value of 0. Available at Accessed 4th December 2020 Rapaio. Why do you need to scale data in KNN. Medical Center Long Beach and Cleveland Clinic Foundation Robert Detrano M. This is to reduce overfitting and multicollinearity which will cause misleading prediction. Since K Nearest is dependent on the major votes of the neighbours it is easily affected by attributes with large range as these attributes tends to dominate the other attributes which in turn affects the model performance. From the KNN model prediction a male which is 55 years old with trestbps of 140 chol of 250 thalach of 160 oldpeak of 2. online Available at Accessed 4th December 2020 Dernoncourt F. A perfectly symmetrical data set will have a skewness of 0. Performance Evaluation of K Nearest Neighbour Decion Tree and Naive Bayes Classification Model Model performance of KNN Decition Tree and Naive Bayes Classifier is calculated and evaluated using accuracy F1 score and confusion matrix. From the scatter plot a higher heart rate implies a higher chance of having heart disease. The optimal number of maximum leaf nodes is calculated using the loop to determine the best model score with maximum leaf node ranged from 2 to 20. However all predictors or independent variable has a absolute correlation coefficient of less than 0. The dataset does not consist any missing values as shown in the code segment. Weibull 2P best fits for the all quantitative attributes as Weibull 2P has low AIC and BIC compared to other distribution fitted for the quantitative attributes. Problem Statement and Objective Heart disease describes a range of conditions that affects a person s heart functioning performance. Statistical approaches to estimating how well a given model fits a dataset and how complex the model is calculating using the Akaike Information Criterion AIC which is derived from frequentist probability and Bayesian Information Criterion BIC which is derived from Bayesian probability. Naive Bayes Classifier also assumes that each predictors contributes equally to the outcome which implies that they all have the same probability to contribute to the outcome Khurana 2020. get_dummies syntax. ca number of major vessels 0 3 colored by flourosopy 13. Are the Skewness and Kurtosis Useful Statistics. Since the dataset used in this case study is not biased hence it is suitable to use Decision Tree for modeling Navlani 2018. For classification model confusion matrix is used for regression model methods such as root mean square error RMSE mean absolute error MAE relative absolute error RAE and others are used Singh 2019. It can be measured using range standard deviation and variance and also coefficient of variation Kaufmann 2014. Data preprocessing involves data cleaning data transformation and data reduction. Heart Disease Prediction. The outliers are handled by capping them with the acceptable range of minimum and maximum value with the Winsorization method. It may not be true since it is comparing in a univariate or bivariate way. It is also suggested to train the model with dataset from different locations and countries such as dataset from Asia. Data preprocessing transforms raw data to useful and efficient format for human and machine learning process. cp 4 values chest pain type Value 1 typical angina Value 2 atypical angina Value 3 non anginal pain Value 4 asymptomatic 4. online Available at Accessed 4th December 2020 Ash3323 Taylor J. Correlation Matrix Correlation Matrix heatmap is shown in Feature Selection or Data Reduction in Data Preprocessing. As decision tree mimic the way human makes decision thus it is easy to understand and interpret by humans. mode the mean median 2nd Quartile or 50 and mode for the quantitative attributes are approximately the same thus they are approximately normal. It considers K number of neighbours nearest the unknown and newly added point which classifies the unknown point based on the neighbours votes for instance if 5 out of 6 neighbours has a target of presence of heart disease then the newly added unknown point will be classified as presence of heart disease. From the scatter density plot matrix all variables are moderately related to target. It is suggested to have sqrt number of instances of neighbours however this is just as a guideline to decided the number of neighbours Navlani 2018. Data are partitioned recursively from the root node to leaf node. online Available at Accessed 4th December 2020 Whitworth G. 5 with the target and absolute correlation coefficient of less than 0. online Available at Accessed 4th December 2020 UCI Machine Learning Repository. ca is considered as qualitative categorical attributes as it only consists of 4 type of unique values. Everything You Need To Know About Heart Disease. Whenver a new unknown record is added it will be classified based on its 10 nearest neighbours votes. Available at Accessed 4th December 2020 Rawat S. Naive Bayes Classifier assumes that each predictors are independent from each other which implies that it requires minimal multicollinearity. Heart disease includes Arrythymia Atherosclerosis Cardioyopathy Congenital heart defects Coronary artery disease CAD and heart infections which is caused by various factors including heart defects anxiety diabetes excessive use of alcohol and caffeine and others Whitworth 2020. Thus K value for this case study is set to 10. online Available at Accesse 4th December 2020 GeeksforGeeks. online Available at Accessed 4th December 2020 Kedarps. Measure of Central Tendency and Dispersion Measures of central tendency of the dataset measures the center or middle of the dataset which can be measured using the mean median and mode CK 12 2020. Measures of dispersion of the dataset measures how disprese or spread out the data in the dataset are. As the dataset used in this case study has little to no multicollinearity hence Naive Bayes Classifier is suitable for this case study. online Available at Accessed 4th December 2020 McNeese B. The histogram and scatter plot for quantitative variales is also able to show the dispersion and central tendancy of the data points. Naive Bayes Classifier Naive Bayes Classifier the most straightforward and fast classification algorithm suitable for use in high dimensionality dataset. Naive Bayes Classifier is a collection of classification algorithm based on Bayes Theorem which take likelihood prior probability and posterior probability of an event into account. EDA process involves exploring the dimension of dataset central tendency and dispersion of dataset summary statistics and others which will be demonstrated below. The IQR outlier detection method accepts range of values from Q1 1. Scatter plot matrix is useful in investigating the feature correlation and multicollinearity. There are various algorithms to build decision tree such as ID3 CART C4. thal 3 normal 6 fixed defect 7 reversable defect14. F1 score is a harmonic balance between precision and recall which is commonly used for biased dataset. target 1 presence 0 absence The dataset consisting 5 quantitative attributes which are age trestbps chol thalach and oldpeak 9 qualitative categorical attributes which are sex cp fbs restecg exang slope ca thal and target. The dataset is now free from duplicates and outliers. As the KNN model in this case study is built using a United States based dataset the model tends to overfit which implies that the model is only capable to predict heart disease of United States citizen but not citizens from other country. It will not only reduce the cost of diagnosis but also improves the efficiency in diagnosis and treatment as patients are diagnosed efficiently and early treatment can be given for better recovery. Suggestions mentioned above can be implemented for a better prediction model that will be used not only in United States but also all over the world with high accuracy and precise prediction. online Available at Accessed 5th November 2020 Stephanie. Prediction of heart disease can be achieved by implementing machine learning algorithms on data provided by healthcare institutions. As shown in the plot the distribution of heart disease and no heart disease is considered evenly distribute as there are no big difference in count. As each type of heart diseases has different kind of symptoms and all symptoms are complicated thus it is difficult to identify the presence of heart disease. The accuracy and F1 score of KNN is also higher compared with Decision Tree and Naive Bayes Classifier with accuracy of 88. 5 hence no multicollinearity. Data Preprocessing in Data Mining. The model with the best performance will then be used identify if a person with the given conditions is suffering from heart disease or not. What is Predictive Model Performance Evaluation. UCI Machine Learning Repository n. Interpretation of Model The KNN model is interpreted by giving an unknown new data for prediction as shown below. In conclusion the KNN model best fits the case study to predict if a United States citizens suffers from heart disease or not. With the Naive Bayes classifier why do we have to normalize the probabilities after calculating the probabilities of each hypothesis. trestbps resting blood pressure in mm Hg on admission to the hospital 5. Hence the maximum leaf node is set to 9 to provide a better accuracy results. Heart Disease Data Set. These categorical attributes are then transformed to dummy variables in data transformation resulting in 31 columns in total. sex cp exang oldpeak are moderately skewed. online Available at Accessed 4th December 2020 Navlani A. The boxplot for quantitative attributes which are age trestbps chol thalach and oldpeak is plotted and outliers is detected for the mentioned attributes except age using IQR outlier detection method. Data Preprocessing Data preprocessing is the one of the major step in building a machine learning algorithms as only quality and clean dataset produces quality model that gives quality results. From the plot above the optimal maximum number of leaf nodes is 6 to 9 with model score of 77. What do you consider a good standard deviation. chol serum cholestoral in mg dl6. Accuracy measures how correct the predictions are precision measures how often the model is correct when predicting the positive class and recall measures how many actual positives are predicted correctly Algorithmia 2020. From the confusion matrix KNN model has the lowest FN among the other models hence KNN is more preferable. According to research there are no optimal K number of neighbours which suits all kind of dataset as each dataset has different requirements. slope the slope of the peak exercise ST segment Value 1 upsloping Value 2 flat Value 3 downsloping 12. The DecisionTreeClassifier from Sckit learn is using CART algorithm to build decision tree. Life data analysis refers to data analysis involving prediction of lifetime. Feature Selection Data Reduction Feature selection is done by using the filter method as various machine learning algorithms will be applied to build various models. This is because different countries has different culture and lifestyle and different ethnics has different genetic which also contributes to the causes of heart disease thus the KNN model developed in this case study is more biased for United States citizens. restecg resting electrocardiographic results values 0 1 2 Value 0 normal Value 1 having ST T wave abnormality T wave inversions and or ST elevation or depression of 0. Heart disease describes a range of conditions that affects a person s heart. Seaborn Kdeplot A Comprehensive Guide. AIC attempt to select an unknown model which has high dimensional reality whereas BIC finds only True models. age sex thalach slope thal and target is negatively skewed and other attributes are positively skewed. fbs fasting blood sugar 120 mg dl 1 true 0 false 7. Hence this dataset will produce a good model as it is not biased. 5 with target thus a bar form correlation of features with target is plotted to have better insights of the correlation. ", "id": "janrauhl/heart-disease-prediction-w-knn-naivebayes-trees", "size": "27902", "language": "python", "html_url": "https://www.kaggle.com/code/janrauhl/heart-disease-prediction-w-knn-naivebayes-trees", "git_url": "https://www.kaggle.com/code/janrauhl/heart-disease-prediction-w-knn-naivebayes-trees", "script": "sklearn.metrics display sklearn.naive_bayes sklearn.tree KNeighborsClassifier MinMaxScaler DecisionTreeClassifier seaborn numpy Fit_Everything plot_tree get_dummies sklearn.model_selection f1_score confusion_matrix matplotlib.pyplot pandas accuracy_score reliability.Fitters sklearn.neighbors sklearn.preprocessing GaussianNB train_test_split IPython.display ", "entities": "(('model performance scorse', 'looping'), 'study') (('Data', 'node'), 'partition') (('accuracy', '88'), 'be') (('precise conclusion', 'classification model'), 'extract') (('why we', 'hypothesis'), 'have') (('accuracy high model', 'United States citizens'), 'use') (('which', 'account'), 'be') (('Life data analysis', 'lifetime'), 'refer') (('Coronary artery disease heart which', 'alcohol'), 'include') (('age trestbps chol outliers', 'detection IQR outlier method'), 'plot') (('dataset', 'imbalance class'), 'show') (('Data preprocessing', 'data transformation reduction'), 'involve') (('Naive Bayes hence Classifier', 'case study'), 'be') (('you', 'R.'), 'have') (('However above conclusions', 'attributes'), 'induce') (('Data reduction', 'numerosity reduction'), 'involve') (('scatter plot matrix', 'grid form Kumar'), 'represent') (('Dummy Variables Data Transformation Data transformation', 'the'), 'do') (('dataset', '301 the'), 'dimension') (('CV value', 'low variance'), 'indicate') (('heart higher rate', 'heart disease'), 'imply') (('classification various models', 'United States citizens'), 'develop') (('data perfectly symmetrical set', '0'), 'have') (('however this', 'neighbours'), 'suggest') (('this', 'serious cases'), 'classify') (('dataset', 'V.'), 'Dataset') (('that', 'heart'), 'describe') (('scatter attributes', 'histograms'), 'consider') (('Data', 'learning human process'), 'transform') (('categorical attributes', 'total'), 'transform') (('they', 'heart highly risked disease'), 'have') (('Negatives False which', 'confusion matrix'), 'study') (('it', 'minimal multicollinearity'), 'assume') (('Data cleaning', 'data discretization concept hierarchy normalization'), 'include') (('thus it', 'heart disease'), 'have') (('United States citizens', 'heart disease'), 'fit') (('variables', 'skewness value'), 'indicate') (('F1 sometimes thus score', 'model also performance'), 'use') (('it', 'unknown data'), 'help') (('you', 'good standard deviation'), 'consider') (('it', 'unique values'), 'consider') (('outliers', 'Winsorization method'), 'handle') (('then person', 'heart disease'), 'use') (('Scatter plot matrix', 'feature correlation'), 'be') (('Weibull where 3p', 'shape scale'), 'consist') (('K Thus value', '10'), 'set') (('original dataset', '303 instances'), 'consist') (('which', 'sex cp fbs restecg exang thal'), 'target') (('X', 'probability specific distribution'), 'calculate') (('dataset', 'first five dataset'), 'load') (('Weibull distribution', 'life data versatility'), 'use') (('variables', 'moderately target'), 'be') (('T', 'ST 0'), 'value') (('DecisionTreeClassifier', 'decision tree'), 'use') (('scatter However plot', 'absolute value'), 'disperse') (('hence normalization', 'such normalization'), 'affect') (('Part', 'correlation matrix plot'), 'perform') (('which', 'model performance'), 'be') (('Realibility Fitting Probability Distribution Probability distribution', 'probability best model'), 'fit') (('Decision trees', 'good accuracy'), 'have') (('which', 'summary dataset statistics'), 'involve') (('Prediction', 'healthcare institutions'), 'achieve') (('mV 2 showing', 'Estes'), '05') (('model', 'other country'), 'dataset') (('Dummy variables', 'mutually exclusive categories'), 'use') (('KNN model', 'prediction'), 'interpretation') (('lower score', 'probability distribution better model'), 'indicate') (('5 to 1', 'moderately highly more than 1'), 'be') (('it', 'heart disease'), 'reduce') (('they', 'different situation'), 'work') (('devolped', '88'), 'consider') (('then newly added unknown point', 'heart disease'), 'consider') (('It', 'Asia'), 'suggest') (('it', 'univariate way'), 'be') (('IQR', 'Q1'), 'accept') (('Skewness', 'symmetry'), 'measure') (('attributes', 'less than 0'), 'have') (('classification confusion thus matrix', 'case study'), 'be') (('which', 'misleading prediction'), 'be') (('that', 'heart disease'), 'be') (('exercise ST segment 0 number', 'heart disease'), 'rest') (('heart disease', 'evenly big count'), 'consider') (('it', 'neighbours 10 nearest votes'), 'add') (('dataset description', 'UCI Machine Learning Repository as n.'), 'be') (('KNN thus model', 'United States more citizens'), 'be') (('they', 'free parameters'), 'indicate') (('too boxes', 'data cleaning'), 'see') (('Hence prediction', 'data analysis clinical Rawat'), 'be') (('optimal number', '20'), 'calculate') (('Bayesian Information Criterion which', 'Bayesian probability'), 'approach') (('they', 'outcome'), 'assume') (('AIC', 'probability distribution best model'), 'take') (('This', 'heart disease'), 'help') (('distance', 'then nearest neighbours'), 'calculate') (('dataset', 'now duplicates'), 'be') (('efficiently early treatment', 'better recovery'), 'reduce') (('dataset', 'patterns'), 'Analysis') (('it', 'good model'), 'produce') (('Thus KNN', 'case study'), 'choose') (('histogram plot', 'data central points'), 'be') (('which', 'commonly biased dataset'), 'be') (('respectively IQR', 'interquartile range'), 'IQR') (('thus attributes', 'dataset'), 'remove') (('Hence normalization', 'KNN model good performance'), 'do') (('Normalization', 'MinMaxScaler Kedarps'), 'do') (('learning supervised machine which', 'nodes'), 'be') (('that', 'quality results'), 'be') (('that', 'functioning performance'), 'describe') (('which', 'mean median'), 'measure') (('higher density', 'occurence'), 'imply') (('Coefficient', 'mean'), 'calculate') (('dataset', 'code segment'), 'consist') (('thus it', 'humans'), 'be') (('Data Missing values', 'isnull syntax'), 'handle') (('which', 'given symptoms'), 'learning') (('Decision Tree Classifier Naive Bayes Classifier', 'performance high metrics'), 'model') (('normal distribution', '0 McNeese 2016'), 'have') (('dummy varialbes', 'attributes'), 'be') (('qualitative categorical which', 'raw dataset'), 'consist') (('leaf Hence maximum node', 'accuracy better results'), 'set') (('how many actual positives', 'when positive class measures'), 'measure') (('Visualization', 'attributes'), 'visualize') (('product commonly assess reliability', 'industry economic Stephanie'), 'use') (('year that', 'CDC n.'), 'be') (('Weibull 2P', 'quantitative attributes'), 'fit') (('Decision However Tree', 'easily imbalanced biased dataset'), 'affect') (('It', 'variation also Kaufmann'), 'measure') (('However predictors', 'less than 0'), 'have') (('bar form thus correlation', 'correlation'), 'plot') (('dataset', 'United States citizen'), 'be') (('male which', '2'), 'from') (('that', 'high accuracy'), 'implement') (('hence KNN', 'other models'), 'be') (('Performance Evaluation', 'Naive Bayes accuracy F1 score'), 'calculate') (('chart female', 'heart disease'), 'tend') (('Filter method', 'such linear regression'), 'choose') (('hence normalization', 'Dernoncourt'), 'affect') (('BIC', 'only True models'), 'attempt') (('various machine learning algorithms', 'various models'), 'do') (('only relevant features', 'filter method'), 'take') (('dataset', 'different requirements'), 'be') (('You', 'Heart Disease'), 'everything') (('approximately thus they', 'quantitative attributes'), 'be') (('which', 'Data Transformation'), 'use') (('scatter where plot', 'variables'), 'be') (('error MAE error absolute relative absolute RAE', 'regression model methods'), 'use') (('probability also JournalDev', 'variable'), 'associate') (('hence it', 'Navlani'), 'be') (('exang exercise', 'angina'), 'induce') (('thus 9', 'better accuracy'), 'available') (('plots', 'sex'), 'left') (('Correlation Matrix Correlation Matrix heatmap', 'Data Data Preprocessing'), 'show') ", "extra": "['biopsy of the greater curvature', 'disease', 'outcome', 'patient', 'test', 'diagnosis']"}