{"name": "w1d1t1 basicpytorch ", "full_name": " h1 Tutorial 1 PyTorch h1 Tutorial Objectives h2 Tutorial slides h1 Setup h2 Install dependencies h2 Figure Settings h2 Helper Functions h1 Section 1 Welcome to Neuromatch Deep learning course h2 Video 1 Welcome and History h2 Video 2 Why DL is cool h2 Describe what you hope to get out of this course in about 100 words h1 Section 2 The Basics of PyTorch h2 Section 2 1 Creating Tensors h3 Video 3 Making Tensors h3 Coding Exercise 2 1 Creating Tensors h2 Section 2 2 Operations in PyTorch h3 Video 4 Tensor Operators h3 Coding Exercise 2 2 Simple tensor operations h2 Section 2 3 Manipulating Tensors in Pytorch h3 Video 5 Tensor Indexing h3 Coding Exercise 2 3 Manipulating Tensors h2 Section 2 4 GPUs h3 Video 6 GPU vs CPU h3 Coding Exercise 2 4 Just how much faster are GPUs h2 Section 2 5 Datasets and Dataloaders h3 Video 7 Getting Data h3 Coding Exercise 2 5 Display an image from the dataset h1 create a tensor of size 2 x 4 h1 print its size and the tensor h1 dimensions permuted h1 print its size and the permuted tensor h4 Video 8 Train and Test h4 Video 9 Data Augmentation Transformations h3 Coding Exercise 2 6 Load the CIFAR10 dataset as grayscale images h1 Section 3 Neural Networks h2 Video 10 CSV Files h2 Section 3 1 Data Loading h3 Generate sample data h2 Section 3 2 Create a Simple Neural Network h3 Video 11 Generating the Neural Network h3 Coding Exercise 3 2 Classify some samples h2 Section 3 3 Train Your Neural Network h3 Video 12 Train the Network h3 Helper function to plot the decision boundary h3 Visualize the training process h3 Video 13 Play with it h3 Exercise 3 3 Tweak your Network h4 Video 14 XOR Widget h3 Interactive Demo 3 3 Solving XOR h1 Section 4 Ethics h2 Video 15 Ethics h1 Bonus h2 Video 16 Be a group h2 Video 17 It s a wrap h2 Video 18 Syllabus h1 Appendix h2 Official PyTorch resources h3 Tutorials h3 Documentation ", "stargazers_count": 0, "forks_count": 0, "description": "Note that we rerun the set_seed function to ensure reproducibility. Mathematically speaking XOR represents the inequality function i. add another hiddern layer Transformation from the hidden to the output layer Specify the computations performed on the data Pass the data through the layers Choose the most likely label predicted by the network Pass the data through the networks Choose the label with the highest score Train the neural network will be implemented later Create new NaiveNet and transfer it to the device Print the structure of the network title Video 12 Train the Network title Helper function to plot the decision boundary Code adapted from this notebook https jonchar. the computational graph. py tensor 24 24 tensor 0 2 1 3 2 1 3 10 tensor 3 2 1 5 tensor 1 1 1 3 2 3 0 Section 2. matmul differs from dot in two important ways. com Andrew Saxe https www. py Sample input tensor 0. io Lyle Ungar https www. Multiplication by scalars is not allowed. 3 Manipulating TensorsUsing a combination of the methods discussed above complete the functions below. Be sure to run all of the cells in the setup section. seed worker_seed g_seed torch. Install dependencies Figure Settings Helper Functions Important note Scratch Code Cells If you want to quickly try out something or take a look at the data you can use scratch code cells. The size of the returned tensor remains the same as that of the original. Now add one hidden layer with three units play with the widget and set weights by hand to solve this dataset perfectly. For this we need to convert the dataloader object to a Python iterator using the function iter and then we can query the next batch using the function next. 3 Solving XORHere we use an open source and famous visualization widget developed by Tensorflow team available here https github. empty just allocates the memory. Creating random tensors and tensors like other tensors Reproducibility PyTorch random number generator You can use torch. Click for solution https github. Use worker_init_fn and a generator to preserve reproducibility pythondef seed_worker worker_id worker_seed torch. Construct tensors directly Some common tensor constructors Notice that. Checkout the pytorch documentation https pytorch. For now the goal is just to see your network in action You will usually implement the train method directly when implementing your class NaiveNet. Feel free to expand them and have a look at what you are loading in but you should be able to fulfill the learning objectives of every tutorial without having to look at these cells. We can do this by going to the runtime tab at the top of the page. com NeuromatchAcademy course content dl tree main tutorials W1D1_BasicsAndPytorch solutions W1D1_Tutorial1_Solution_49a61fb7. If you start building your own projects built on this code base we highly recommend looking at them in more detail. com NeuromatchAcademy course content dl tree main tutorials W1D1_BasicsAndPytorch solutions W1D1_Tutorial1_Solution_51c270eb. manual_seed to seed the RNG for all devices both CPU and CUDA pythonimport torchtorch. py tensor 20 24 31 27 Click for solution https github. 6 Load the CIFAR10 dataset as grayscale imagesThe goal of this excercise is to load the images from the CIFAR10 dataset as grayscale images. seed 0 Random number generators in other libraries pythonimport numpy as npnp. We can access elements according to their relative position to the end of the list by using negative indices. Why might this be Section 2. 1 Data LoadingFirst we need some sample data to train our network on. html FAQ including guidance on GPU usage Books for reference https www. com NeuromatchAcademy course content dl tree main tutorials W1D1_BasicsAndPytorch solutions W1D1_Tutorial1_Solution_d99622ef. Now let s take a look at one of them in detail. forward All neural network modules need to implement the forward method. Because of that pesky singleton dimension x 0 gave us the first row instead Permutation Sometimes our dimensions will be in the wrong order For example we may be dealing with RGB images with dim 3x48x64 but our pipeline expects the colour dimension to be the last dimension i. Let s load the CIFAR10 https www. Execute the celll multiple times to verify that the numbers printed are always the same. These cells will import the required Python packages e. 4 GPUs Video 6 GPU vs CPUBy default when we create a tensor it will not live on the GPU When using Colab notebooks by default will not have access to a GPU. For example 1 selects the last element 1 3 selects the second and the third elements and 2 will select all elements excluding the last and second to last elements. org Deep Learning by Ian Goodfellow Yoshua Bengio and Aaron Courville title Tutorial slides markdown These are the slides for the videos in this tutorial today title Install dependencies Imports title Figure Settings title Helper Functions TODO better errors and error handling title Video 1 Welcome and History title Video 2 Why DL is cool title Video 3 Making Tensors we can construct a tensor directly from some common python iterables such as list and tuple nested iterables can also be handled as long as the dimensions make sense tensor from a list tensor from a tuple of tuples tensor from a numpy array the numerical arguments we pass to these constructors determine the shape of the output tensor there are also constructors for random numbers uniform distribution normal distribution there are also constructors that allow us to construct a tensor according to the above constructors but with dimensions equal to another tensor uniform distribution normal distribution Turn seed to False or change my_seed numpy array to copy later Uncomment below to check your function title Video 4 Tensor Operators this only works if c and d already exist Pointwise Multiplication of a and b The operator is exponentiation sum note the axis is the axis you move across when summing matrix multiplication multiplication of tensor a1 with tensor a2 and then add it with tensor a3 Computing expression 1 init our tensors Use torch. Visualize the training process Execute this cell Video 13 Play with it Exercise 3. NOTE I am assuming that GPU stuff might be covered in more detail on another day but there could be a bit more detail here. The example here is meant to demonstrate the process of creating and training a neural network end to end. That is a true output result if one and only one of the inputs to the gate is true. me and Vincenzo Lamonaco https www. You can also use torch. On Day 2 we will focus on linear networks but you will work with some more complicated architectures in the next days. py Example output Section 3 Neural NetworksNow it s time for you to create your first neural network using PyTorch. They allow you to run Python code but will not mess up the structure of your notebook. The method will be used to train the network parameters and will be implemented later in the notebook. io Vikash Gilja https tnel. empty does not return zeros but seemingly random small numbers. A full list of all methods can be found in the appendix there are a lot All of these operations should have similar syntax to their numpy equivalents. com and Feryal Behbahani https feryal. Programing the Network PyTorch provides a base class for all neural network modules called nn. Discuss Try and reduce the dimensions of the tensors and increase the iterations. io Tim Lillicrap https contrastiveconvergence. ca neuro blake richards phd Josh Vogelstein https jovo. The color of the points is determined by the labels y_orig. 1 Creating TensorsBelow you will find some incomplete code. reshape though for now we will just use. edu ungar Week 2 making things work Alona Fyshe https webdocs. 4 Just how much faster are GPUs Below is a simple function. What happens if we try and perform operations on tensors on devices We cannot combine cuda tensors and cpu tensors in this fashion. Here you will specify what layers will the network consist of what activation functions will be used etc. ca alona Alexander Ecker https eckerlab. permute You may also see. In order to start using GPUs we need to request one. initial_seed 2 32 numpy. seed worker_seed random. 2 Operations in PyTorch Tensor Tensor operations We can perform operations on tensors using methods under torch. Module and implement some important methods __init__ In the __init__ method you need to define the structure of your network. Helper function to plot the decision boundary Plot the loss during training Plot the loss during the training to see how it reduces and converges. html tensor methods https pytorch. It is hence a bit faster if you are looking to just create a tensor. The data will be stored in a file called sample_data. Initialize the device variable set to cuda Convert the 2D points to a float32 tensor Upload the tensor to the device Convert the labels to a long interger tensor Upload the tensor to the device title Video 11 Generating the Neural Network Inherit from nn. net notebooks Artificial Neural Network with Keras Transfer the data to the CPU Check if the frames folder exists and create it if needed Set min and max values and give it some padding Generate a grid of points with distance h between them Predict the function value for the whole gid Plot the contour and training examples Implement the train function given a training dataset X and correcsponding labels y The Cross Entropy Loss is suitable for classification problems Create an optimizer Stochastic Gradient Descent that will be used to train the network Number of epochs List of losses for visualization Pass the data through the network and compute the loss We ll use the whole dataset during the training instead of using batches in to order to keep the code simple for now. com NeuromatchAcademy course content dl tree main tutorials W1D1_BasicsAndPytorch solutions W1D1_Tutorial1_Solution_69b74721. columns axis 1 the second element of the shape. We will all learn Deep Learning. t it is an attribute not a method. For dot multiplication you can use torch. 2 Simple tensor operationsBelow are two expressions involving operations on matrices. edu bio and Akash Srivastava https akashgit. net timothylillicrap index. html pre loaded image datasets Google Colab Resources https research. Generally in this course all Deep learning is done on the GPU and any computation is done on the CPU so sometimes we have to pass things back and forth so you ll see us call Coding Exercise 2. numel is an easy way of finding the number of elements in a tensor Click for solution https github. textbf A begin bmatrix 2 4 5 7 end bmatrix begin bmatrix 1 1 2 3 end bmatrix begin bmatrix 10 10 12 1 end bmatrix and b begin bmatrix 3 5 7 end bmatrix cdot begin bmatrix 2 4 8 end bmatrix The code block below that computes these expressions using PyTorch is incomplete fill in the missing lines. Note that you can use the __call__ method of a module directly and it will invoke the forward method net does the same as net. 5122 device cuda 0 Network output tensor 0. cpu moving to gpu alternatively you can use y y. Video 9 Data Augmentation Transformations Dataloader Another important concept is the Dataloader. The shuffle argument is used to shuffle the order of the samples across the minibatches. We can denote this image format as C H W. This section will walk you through the process of Creating a simple neural network model Training the network Visualizing the results of the network Tweeking the network Video 10 CSV Files Section 3. linspace behave how you would expect them to if you are familar with numpy. org James Evans https sociology. edu directory james evans He He https hhexiy. You should not expect the network to actually classify the points correctly because it has not been trained yet. predict This is not an obligatory method of a neural network module but it is a good practice if you want to quickly get the most likely label from the network. We have 50000 samples loaded. To get around this we can use. 5 Display an image from the datasetLet s try to display the image using matplotlib. 3 Manipulating Tensors in Pytorch Video 5 Tensor Indexing Indexing Just as in numpy elements in a tensor can be accessed by index. We can now see that we have a 4D tensor. org vision stable transforms. matmul to multiply tensors. You need to reorder the dimensions of the tensor using the permute method of the tensor. 1 Creating Tensors Video 3 Making TensorsThere are various ways of creating tensors and when doing any real deep learning project we will usually have to do so. CUDA is an API developed by Nvidia for interfacing with GPUs. Numpy like number ranges The. The common standard arithmetic operators and have all been lifted to elementwise operations Tensor Methods Tensors also have a number of common arithmetic operations built in. For simplicity today we will not use both datasets separately but this topic will be adressed in the next days. Fill in the missing code to construct the specified tensors. org doc stable reference generated numpy. view The view method in particular https pytorch. Function A This function takes in two 2D tensors A and B and returns the column sum of A multiplied by the sum of all the elmements of B i. Note the lack of brackets for Tensor. org docs stable tensors. edu Ioannis Mitliagkas http mitliagkas. Then proceed to the next cell. You will also see the. Feel free to skip if you already know this Matrix Operations The symbol is overridden to represent matrix multiplication. com NeuromatchAcademy course content dl tree main tutorials W1D1_BasicsAndPytorch solutions W1D1_Tutorial1_Solution_8a7b1b66. Read a few abstracts. org docs stable generated torch. manual_seed my_seed DataLoader train_dataset batch_size batch_size num_workers num_workers worker_init_fn seed_worker generator g_seed We can now query the next batch from the data loader and inspect it. transforms package and you can also combine them using the Compose transform. When loading data from a CSV file we can reference the columns directly by their names. Code hint python create a tensor of size 2 x 4input_var torch. Interactive Demo 3. Here we will implement it as a function outside of the class in order to have it in a ceparate cell. Read our Code of Conduct https docs. You could also do the same for the biases by clicking on the tiny square to each neuron s bottom left. io Week 3 more magic Tim Lillicrap https contrastiveconvergence. 5 Datasets and Dataloaders Video 7 Getting DataWhen training neural network models you will be working with large amounts of data. Coding Exercise 2. This dimension can quite easilly mess up your matrix operations if you don t plan on it being there. PyTorch provides us with a layer of abstraction and allows us to launch CUDA kernels using pure Python. For the second part you should set the weights by clicking on the connections and either type the value or use the up and down keys to change it by one increment. manual_seed 0 For custom operators you might need to set python seed as well pythonimport randomrandom. PyTorch NumPy set global or environment variables and load in helper functions for things like plotting. org Surya Ganguli https ganguli gang. Clear the previous gradients and compute the new ones Adapt the weights of the network Store the loss Print the results at every 1000th epoch Create a new network instance a train it title Visualize the training process markdown Execute this cell Make a list with all images Save the gif title Video 13 Play with it title Video 14 XOR Widget markdown Play with the parameters to solve XOR markdown Do you think we can solve the discrete XOR only 4 possibilities with only 2 hidden units param Select Yes No title Video 15 Ethics title Video 16 Be a group title Video 17 It s a wrap title Video 18 Syllabus. Module https pytorch. You need to inherit from nn. C begin bmatrix 2 3 1 10 end bmatrix Out begin bmatrix 0 2 1 3 2 1 3 10 end bmatrix Hint pay close attention to singleton dimensions Function C This function takes in two 2D tensors D and E. cc virtual 2021 paper_vis. Video 2 Why DL is cool Describe what you hope to get out of this course in about 100 words. ca neuro blake richards phd Jane Wang http www. org vision stable datasets. Each sample consists of an image and its corresponding label. size print input_var dimensions permutedinput_var input_var. Once you have done this your runtime will restart and you will need to rerun the first setup cell to reimport PyTorch. The second dimensions is the height H of the image and the third is the width W. An optimized autograd engine for automatically computing derivatives. If the dimensions allow it this function returns the elementwise sum of D shaped E and D else this function returns a 1D tensor that is the concatenation of the two tensors e. html dataset which contains color images of 10 different classes like vehicles and animals. It specifies the computations the network needs to do when data is passed through it. randn 2 4 print its size and the tensorprint input_var. Transformations Another useful feature when loading a dataset is applying transformations on the data color conversions normalization cropping rotation etc. We can see that the first output tensor s axis 0 length 6 is the sum of the two input tensors axis 0 lengths 3 3 while the second output tensor s axis 1 length 8 is the sum of the two input tensors axis 1 lengths 4 4. The resulting data structure can be treated as a list containing data samples and their corresponding labels. com Now go to the visualization of ICLR papers https iclr. If we want to compute an operation that combines tensors on different devices we need to move them first We can use the. php and Blake Richards https www. dot to compute the dot product of two tensors Computing expression 2 title Video 5 Tensor Indexing make a 5D tensor 2D 1D and back to 2D printing the zeroth element of the tensor will not give us the first number lets get rid of that singleton dimension and see what happens now adding singleton dimensions works a similar way and is often used when tensors being added need same number of dimensions lets insert a singleton dimension x has dimensions color image_height image_width we want to permute our tensor to be image_height image_width color permute 1 2 0 means the 0th dim of my new tensor the 1st dim of my old tensor the 1st dim of my new tensor the 2nd the 2nd dim of my new tensor the 0th Create two tensors of the same shape concatenate them along rows concatenate along columns printing outputs TODO multiplication the sum of the tensors TODO flatten the tensor C TODO create the idx tensor to be concatenated to C TODO concatenate the two tensors TODO check we can reshape E into the shape of D TODO reshape E into the shape of D TODO sum the two tensors TODO flatten both tensors TODO concatenate the two tensors in the correct dimension title Video 6 GPU vs CPU common device agnostic way of writing code that can run on cpu OR gpu that we provide for you in each of the tutorials we can specify a device when we first create our tensor we can also use the. When we have multidimensional tensors indexing rules work the same way as numpy. html but with GPU acceleration. The documentation can be found in the appendix. Transposes of 2D tensors are obtained using torch. D begin bmatrix 1 1 1 3 end bmatrix E begin bmatrix 2 3 0 2 end bmatrix Out begin bmatrix 3 2 1 5 end bmatrix D begin bmatrix 1 1 1 3 end bmatrix E begin bmatrix 2 3 0 end bmatrix Out begin bmatrix 1 1 1 3 2 3 0 end bmatrix Hint torch. By following Runtime Change runtime type and selecting GPU from the Hardware Accelerator dropdown list we can start playing with sending tensors to GPUs. Creating an object of type datasets. 3 Tweak your NetworkYou can now play around with the network a little bit to get a feeling of what different parameters are doing. 1 10 or 256 1 3. py Example output Video 8 Train and Test Training and Test Datasets When loading a dataset you can specify if you want to load the training or the test samples using the train argument. For more information on the GPU usage policy you can view in the appendix Now we have a GPU The cell below should return True. It is a wrapper around the Dataset that splits it into minibatches important for training the neural network and makes the data iterable. com tensorflow playground. Flatten and reshape There are various methods for reshaping tensors. You can get to a point where the cpu only function is faster than the GPU function. Similarly it is also common to have to reshape a 1D tensor into a 2D tensor. Section 2 The Basics of PyTorchPyTorch is a Python based scientific computing package targeted at two sets ofaudiences A replacement for NumPy to use the power of GPUs A deep learning platform that provides significant flexibility and speedAt its core PyTorch provides a few key features A multidimensional Tensor https pytorch. This will be covered in detail when you are introduced to autograd tomorrow To convert a size 1 tensor to a Python scalar we can invoke the item function or Python s built in functions. com NeuromatchAcademy course content dl tree main tutorials W1D1_BasicsAndPytorch solutions W1D1_Tutorial1_Solution_524e1dab. Concatenation In this example we concatenate two matrices along rows axis 0 the first element of the shape vs. py tensor 82 Section 2. CIFAR10 will automatically download and load all images from the dataset. When converting to a numpy array the information being tracked by the tensor will be lost i. Groups need standards. train This is also not an obligatory method but it is a good practice to have. seed 0 Here we define for you a function called set_seed that does the job for you Now let s use the set_seed function in the previous example. Prepare Data for PyTorch Now let s prepare the data in a format suitable for PyTorch convert everything into tensors. 2 Create a Simple Neural Network Video 11 Generating the Neural NetworkFor this example we want to have a simple neural network consisting of 3 layers 1 input layer of size 2 our points have 2 coordinates 1 hidden layer of size 16 you can play with different numbers here 1 output layer of size 2 we want the have the scores for the two classes During the course you will deal with differend kinds of neural networks. Module the base class for neural network modules provided by Pytorch Define the structure of your network The network is defined as a sequence of operations Transformation from the input to the hidden layer Activation function ReLU is a non linearity which is widely used because it reduces computation. Squeezing tensors When processing batches of data you will quite often be left with singleton dimensions. The code below will not work because imshow expects to have the image in a different format H times W times C. A clean modular API for building and deploying deep learning models. unsqueeze method to do the opposite. Pandas provides many functions for reading files in varios formats. the output is true if the inputs are not alike otherwise the output is false. Generate sample data we used scikit learn moduleNow we can load the data from the CSV file using the Pandas library. Reproducibility DataLoader will reseed workers following Randomness in multi process data loading algorithm. Even though there are infinitely many solutions a neat solution when f x is ReLU is begin equation y f x_1 f x_2 f x_1 x_2 end equation Try to set the weights and biases to implement this function after you played enough Play with the parameters to solve XOR Do you think we can solve the discrete XOR only 4 possibilities with only 2 hidden units Section 4 EthicsLet us watch the coded bias movie together and discuss Video 15 Ethics Bonus Video 16 Be a group Video 17 It s a wrap Video 18 SyllabusMeet our lecturers Week 1 the building blocks Konrad Kording https kordinglab. As in any numpy array the first element has index 0 and ranges are specified to include the first but before the last element. Complete the second function such that it is performs the same operations as the first function but entirely on the GPU. So the size of the 4D tensor is B times C times H times W. 5369 device cuda 0 grad_fn Predicted labels tensor 0 0 0 0 0 device cuda 0 Section 3. Play with the widget and observe that you can not solve the continuous XOR dataset. Here are some ideas what you could try Increase or decrease the number of epochs for training Increase or decrease the size of the hidden layer Add one additional hidden layerCan you get the network to better fit the data Video 14 XOR WidgetExclusive OR XOR logical operation gives a true 1 output when the number of true inputs is odd. In order to compress tensors along their singleton dimensions we can use the. org tutorials Documentation https pytorch. The function returns 0 if it receives any negative input but for any positive value x it returns that value back. Indexing is also referred to as slicing. Look at the various clusters. size print input_var Click for solution https github. view methods used a lot to reshape tensors. Fortunately PyTorch offers some great tools that help you organize and manipulate your data samples. cuda garbage collection dot product to remove solution title Video 7 Getting Data Import dataset and dataloaders related packages Download and load the images from the CIFAR10 dataset path where the images will be stored all images should be downloaded transform the images to tensors Print the number of samples in the loaded dataset Choose a random sample TODO Uncomment the following line to see the error that arises from the current image format TODO Comment the above line and fix this code by reordering the tensor dimensions title Video 8 Train and Test Load the training samples Load the test samples title Video 9 Data Augmentation Transformations Create dataloaders with Load the next batch Display the first image from the batch Display a random grayscale image title Video 10 CSV Files title Generate sample data markdown we used scikit learn module Create a dataset of 256 points with a little noise Store the data as a Pandas data frame and save it to a CSV file Load the data from the CSV file in a Pandas DataFrame Create a 2D numpy array from the x0 and x1 columns Create a 1D numpy array from the y column Print the sizes of the generated 2D points X and the corresponding labels Y Visualize the dataset. This is because we have a 64 images in the batch B and each image has 3 dimensions channels C height H and width W. Tutorial 1 PyTorch Week 1 Day 1 Basics and PyTorch By Neuromatch Academy __Content creators __ Shubh Pachchigar Vladimir Haltakov Matthew Sargent Konrad Kording__Content reviewers __ Kelson Shilling Scrivo Deepak Raya Siwei Bai__Content editors __ Anoop Kulkarni Spiros Chavlis__Production editors __ Arush Tagade Spiros Chavlis Our 2021 Sponsors including Presenting Sponsor Facebook Reality Labs Tutorial ObjectivesThen have a few specific objectives for this tutorial Learn about PyTorch and tensors Tensor Manipulations Data Loading GPUs and Cuda Tensors Train NaiveNet Get to know your pod Start thinking about the course as a whole Tutorial slides These are the slides for the videos in this tutorial today SetupThroughout your Neuromatch tutorials most probably all notebooks contain setup cells. to method as before or the. Section 1 Welcome to Neuromatch Deep learning course Video 1 Welcome and History This will be an intensive 3 week adventure. 3 Train Your Neural Network Video 12 Train the NetworkNow it is time to train your network on your dataset. You can inspect the file directly in Colab by going to Files on the left side and opening the CSV file. We can load the training and test datasets separately. To open a new scratch cell go to Insert Scratch code cell. You can use the function below to generate an example dataset consisting of 2D points along two interleaving half circles. The goal here is just to get some experience with the data structures that are passed to the forward and predict methods and their results. Stacks of matrices are broadcast together as if the matrices were elements. dot or manipulate the axes of your tensors and do matrix multiplication we will cover that in the next section. However in PyTorch most common Python operators are overridden. This minor inconvenience is actually quite important when you perform operations on the CPU or on GPUs you do not want to halt computation waiting to see whether the NumPy package of Python might want to be doing something else with the same chunk of memory. It calls the forward method and chooses the label with the highest score. Conversion to Other Python Objects Converting to a NumPy tensor or vice versa is easy. Where do you see yourself in this map Appendix Official PyTorch resources Tutorialshttps pytorch. In short we get the power of parallising our tensor computations on GPUs whilst only writing relatively simple Python Let s make some CUDA tensors Operations between cpu tensors and cuda tensors Note that the type of the tensor changed after calling. Video 4 Tensor Operators Tensor Tensor operations We can perform operations on tensors using methods under torch. The converted result does not share memory. Datasets The torchvision package gives you easy access to many of the publicly available datasets. 2 Classify some samplesNow let s pass some of the points of our dataset through the network and see if it works. to method to change the device a tensor lives on Uncomment the following line and run this cell moving to cpu alternatively you can use x x. A begin bmatrix 1 1 1 1 end bmatrix B begin bmatrix 1 2 3 1 2 3 end bmatrix Out begin bmatrix 2 2 end bmatrix cdot 12 begin bmatrix 24 24 end bmatrix Function B This function takes in a square matrix C and returns a 2D tensor consisting of a flattened C with the index of each element appended to this tensor in the row dimension e. We want the tensors A 20 by 21 tensor consisting of ones B a tensor with elements equal to the elements of numpy array Z C a tensor with the same number of elements as A but with values sim U 0 1 D a 1D tensor containing the even numbers between 4 and 40 inclusive. If both inputs are false 0 or both are true or false output results. zeros which initialises the elements of the tensor with zeros. Color images are modeled as 3 dimensional tensors. The first dimension corresponds to the channels C of the image in this case we have RGB images. Check that your network works Create an instance of your model and visualize it Coding Exercise 3. Don t worry if you don t fully understand everything yet we wil cover training in much more details in the next days. In case of two inputs X and Y the following truth table is applied begin array ccc X Y text XOR hline0 0 0 0 1 1 1 0 1 1 1 0 end array Here with 0 we denote False and with 1 we denote True in boolean terms. com NeuromatchAcademy course content dl tree main tutorials W1D1_BasicsAndPytorch solutions W1D1_Tutorial1_Solution_2a69ad55. permute dims rearranges the original tensor according to the desired ordering and returns a new multidimensional rotated tensor. com document d 1eHKIkaNbAlbx_92tLQelXnicKXEcvFzlyzzeWjEtifM edit usp sharing. html object similar to NumPy Array https numpy. py All correct Section 2. permute 1 0 print its size and the permuted tensorprint input_var. We can achieve this with the. This works in a similar way as permute but can only swap two dimensions at once. There are many predefined transformations in the torchvision. It is common to have to express 2D data in 1D format. There is a subtle difference between. You can find more information about PyTorch in the appendix. ", "id": "joseguzman/w1d1t1-basicpytorch", "size": "29187", "language": "python", "html_url": "https://www.kaggle.com/code/joseguzman/w1d1t1-basicpytorch", "git_url": "https://www.kaggle.com/code/joseguzman/w1d1t1-basicpytorch", "script": "my_data_load torch.utils.data __init__ Compose make_moons display torch simpleFunGPU IFrame ipywidgets predict IPython.core.interactiveshell train DataLoader YouTubeVideo Path forward simpleFun functionC ToTensor plot_decision_boundary numpy BiliVideo(IFrame) checkExercise1 pathlib NaiveNet(nn.Module) torchvision simplefun functionA simple_operations nn Grayscale Image matplotlib.pyplot pandas set_seed widgets dot_product sklearn.datasets functionB timeFun InteractiveShell tensor_creation set_device datasets IPython.display HTML torchvision.transforms ", "entities": "(('bmatrix Hint 1 1 1 3 2 3 0 end torch', 'end 2 3 0 Out bmatrix'), 'begin') (('you', 'test train argument'), 'py') (('good you', 'network'), 'predict') (('Data 1 LoadingFirst we', 'network'), 'need') (('cell', 'below True'), 'for') (('Similarly it', '2D tensor'), 'be') (('You', 'tensor'), 'need') (('Color images', '3 dimensional tensors'), 'model') (('you', 'numpy'), 'behave') (('cuda type', 'tensor'), 'get') (('5122 device', 'Network output 0 tensor'), 'cuda') (('data', 'important neural network'), 'be') (('columns', '1 second shape'), 'axis') (('Pandas', 'varios formats'), 'provide') (('tensor', '0 1 1D even numbers'), 'want') (('you', 'notebook'), 'allow') (('loaded image', 'Google Colab Resources https research'), 'dataset') (('org doc stable reference', 'numpy'), 'generate') (('You', 'bottom'), 'do') (('We', 'negative indices'), 'access') (('Code hint python', 'size 2 4input_var torch'), 'create') (('We', 'cpu fashion'), 'happen') (('method', 'later notebook'), 'use') (('we', 'reproducibility'), 'note') (('data', '2D generated points'), 'product') (('you', 'next days'), 'focus') (('network', 'it'), 'check') (('group 16 17 It', 'only 2 hidden units'), 'clear') (('section', 'network'), 'walk') (('we', 'also the'), 'dot') (('Tensor Operators Tensor Tensor Video 4 operations We', 'torch'), 'perform') (('This', 'as only two dimensions'), 'work') (('widely it', 'computation'), 'module') (('Solving 3 we', 'visualization Tensorflow famous team'), 'xorhere') (('PyTorch NumPy', 'things'), 'set') (('it', 'network'), 'let') (('dims', 'new multidimensional rotated tensor'), 'permute') (('us', 'pure Python'), 'provide') (('third', 'height image'), 'be') (('GPU stuff', 'day'), 'NOTE') (('we', 'next section'), 'dot') (('hence bit faster you', 'just tensor'), 'be') (('it', 'GPU'), 'have') (('you', 'Compose transform'), 'transform') (('Where you', 'map'), 'see') (('such it', 'entirely GPU'), 'complete') (('You', 'appendix'), 'find') (('CIFAR10', 'dataset'), 'download') (('Python most common operators', 'However PyTorch'), 'be') (('below imshow', 'different format'), 'work') (('which', 'zeros'), 'zero') (('you', 'PyTorch'), 'py') (('NumPy package', 'memory'), 'be') (('colour dimension', 'dim 3x48x64'), 'give') (('you', 'data samples'), 'offer') (('we', 'one'), 'need') (('documentation', 'appendix'), 'find') (('12 it', 'dataset'), 'train') (('that', 'positive value'), 'return') (('you', 'cells'), 'feel') (('size dimensions permutedinput_var', 'input_var'), 'print') (('we', 'more detail'), 'start') (('you', 'PyTorch'), 'restart') (('we', 'GPUs'), 'start') (('directly it', 'net'), 'do') (('activation functions', 'what'), 'specify') (('it', 't plan'), 'mess') (('You', 'CSV file'), 'inspect') (('you', 'pythonimport as well randomrandom'), 'manual_seed') (('that', 'methods'), 'be') (('end bmatrix Function 12 24 24 function', 'row dimension e.'), 'begin') (('you', 'network'), 'module') (('Construct', 'that'), 'notice') (('boundary Code', 'notebook https jonchar'), 'add') (('It', 'highest score'), 'call') (('Video 1 This', 'Neuromatch learning Section 1 Deep course'), 'welcome') (('combination', 'above functions'), 'complete') (('converted result', 'memory'), 'share') (('You', 'directly when class'), 'be') (('Function function', 'B i.'), 'a') (('third 2', 'last elements'), 'select') (('you', 'torch'), 'use') (('most probably notebooks', 'setup cells'), 'Basics') (('html which', 'vehicles'), 'dataset') (('Reproducibility DataLoader', 'process data loading multi algorithm'), 'reseed') (('data resulting structure', 'data samples'), 'treat') (('5', 'matplotlib'), 'Display') (('3 output 0 lengths 3 second tensor', '1 lengths'), 'see') (('So size', '4D tensor'), 'be') (('permuted tensorprint', 'size'), 'permute') (('Transformations', 'etc'), 'apply') (('tensorprint', 'size'), 'print') (('only function', 'GPU function'), 'get') (('core PyTorch', 'a few key features'), 'section') (('output true one', 'gate'), 'be') (('number seed 0 Random generators', 'npnp'), 'pythonimport') (('we', 'shape'), 'Concatenation') (('you', 'one increment'), 'set') (('tensors', 'torch'), 'Learning') (('you', 'about 100 words'), 'be') (('Conversion', 'NumPy tensor'), 'be') (('we', 'directly names'), 'reference') (('expressions', 'incomplete missing lines'), 'begin') (('We', 'training'), 'load') (('we', 'learning when real deep project'), '1') (('cells', 'Python packages required e.'), 'import') (('You', 'half circles'), 'use') (('We', 'C H W.'), 'denote') (('image', 'dimensions channels C height 3 H'), 'be') (('you', 'neural networks'), 'create') (('Now s', 'tensors'), 'prepare') (('you', 'XOR continuous dataset'), 'play') (('view methods', 'tensors'), 'use') (('together matrices', 'matrices'), 'broadcast') (('correctly it', 'actually points'), 'expect') (('It', '1D format'), 'be') (('Now s', 'previous example'), 'seed') (('neuro blake richards', 'Josh Vogelstein https jovo'), 'phd') (('we', 'functions'), 'invoke') (('0 ranges', 'last element'), 'have') (('us', 'Coding Exercise'), 'do') (('Multiplication', 'scalars'), 'allow') (('example', 'end'), 'mean') (('alternatively you', 'y'), 'use') (('that', 'tensors two e.'), 'return') (('Mathematically speaking', 'inequality function i.'), 'represent') (('training process', 'Video 13 it'), 'visualize') (('when number', 'true inputs'), 'be') (('we', 'RGB images'), 'correspond') (('then we', 'function'), 'need') (('we', 'the'), 'use') (('you', 'singleton quite often dimensions'), 'leave') (('Creating you', 'incomplete code'), '1') (('first We', 'the'), 'want') (('we', 'Pandas library'), 'generate') (('number Reproducibility PyTorch random You', 'torch'), 'use') (('Now s', 'detail'), 'let') (('size', 'original'), 'remain') (('tensors indexing multidimensional rules', 'same way numpy'), 'have') (('We', 'torch'), 'perform') (('color', 'labels'), 'determine') (('16 17 It', 'Ethics Bonus together Video 15 Video'), 'begin') (('data', 'file'), 'store') (('when data', 'it'), 'specify') (('numel', 'solution https github'), 'be') (('imagesThe goal', 'grayscale images'), 'load') (('Dataloaders Getting DataWhen network 5 7 training neural you', 'data'), 'Datasets') (('neuro blake richards Jane Wang', 'www'), 'ca') (('network forward neural modules', 'forward method'), 'need') (('separately topic', 'next days'), 'use') (('edu Week 2 things', 'Alona Fyshe https webdocs'), 'ungar') (('We', 'code'), 'Transfer') (('DataLoader train_dataset batch_size num_workers num_workers worker_init_fn seed_worker generator We', 'it'), 'manual_seed') (('alternatively you', 'x.'), 'use') (('CUDA', 'GPUs'), 'be') (('sample', 'image'), 'consist') (('All', 'numpy equivalents'), 'find') (('2 Simple tensor', 'matrices'), 'be') (('symbol', 'matrix multiplication'), 'feel') (('Programing', 'network neural modules'), 'provide') (('shuffle argument', 'minibatches'), 'use') (('Transposes', 'torch'), 'obtain') (('we', 'boolean terms'), 'apply') (('different parameters', 'what'), 'Tweak') (('you', 'scratch code cells'), 'note') (('function', '2D tensors two D'), 'begin') (('empty', 'seemingly small numbers'), 'return') (('how it', 'training'), 'function') (('Here we', 'ceparate cell'), 'implement') (('information', 'tensor'), 'lose') (('operations Tensor Methods Tensors', 'common arithmetic operations'), 'lift') (('torchvision package', 'publicly available datasets'), 'dataset') (('multiple times numbers', 'celll'), 'execute') (('We', 'page'), 'do') (('yet we', 'next days'), 'worry') ", "extra": "['test', 'bag']"}