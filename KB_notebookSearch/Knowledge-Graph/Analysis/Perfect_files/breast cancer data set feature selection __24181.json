{"name": "breast cancer data set feature selection ", "full_name": " h1 I INTRODUCTION h4 Data analysis h4 Data visualization h4 Basic ML analysis h4 Hyperparameter tuning h4 I wanna be the very best Ash h4 Conclusion h1 Enjoy your data analysis h1 II Data Analysis h1 III Visualization h1 IV Basic Machine Learning Analysis h1 But let s start with the beginning what do we want h2 The first thing we want is a single performance criterion h2 The second thing we want is a good generalization of our algorithm h2 The less features the better h2 1 Feature Selection with correlation and Random Forest classification h2 2 Recursive feature elimination RFE with Random Forest h2 3 Recursive feature elimination with cross validation and Random Forest classification h2 4 Improving the estimate of the generalization error h1 What can we do about it h2 First step averaging the solutions of the RFECV to obtain an averaged RFECV h2 Second step but what are these 11 features h1 V Conclusion h2 I hope you enjoyed this kernel h2 If you have any question or advice don t hesitate ", "stargazers_count": 0, "forks_count": 0, "description": "Let s look at the evolution of the recall with the increase in the number of features Looking at this plot it appears that keeping 8 14 17 20 21 or 26 features roughly gives the same recall output. com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. Other techniques exist but ML experts focus on Standardization. The upcoming study is about finding the best machine learning to detect the presence or absence of malignant breast cancer tumors. Disclaimer 3 most of this work is based on the kernel of another Kaggler user DATAI. Cancer death rates falling but slowly. If we are not relevant on the estimate of the generalization error then we can t be relevant in the choice of the best algorithm. Early detection seems to be of paramount importance in order to decrease the death rate of cancer. The number N obtained in the first step is an averaged that does not refer to any particular set of features. The steps of the method are as follow 1 Run the RFECV many times to get the average generalization error and its evolution with the number of features. Therefore before trying to optimize our algorithm let s first check the quality of the estimate of the generalization error and improve it if necessary. Each result should give a different set of N features a priori. A priori different set of N features could express high recall. Society for Industrial and Applied Mathematics. Doing so we end up with a problem leading to our second step. In this data analysis report the data comes from Breast Cancer Wisconsin Diagnostic Data Set. Features are computed from a digitized image of a fine needle aspirate FNA of a breast mass. When a large test data set cannot be held out or easily acquired resampling methods such as cross validation are commonly used to estimate the generalization error. My main objective in this kernel is to put in practice the things I learn in data science and machine learning. It seems to me that when detecting cancer other performance criteria could be introduced. This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. Like the previous method we will use 16 features however this time the 16 features will be computed through the RFE. This would give the best number of features selected statistically speaking. Cancer 94 10 2766 2792. 02 and 500 features I m pushing it to the extreme but I never saw the number of features taken into account in the final decision of which algorithm is better. Though predominantly in women breast cancer can also occur in men. It is well known that machine learning algorithms do not converge well when facing unscaled features. 6 which is acceptable. 2 Recursive feature elimination RFE with Random ForestRFE uses one of the classification methods random forest in our example assign weights to each feature. The recall on the test set is 94. At this stage we still don t care about cross validation. Therefore I won t consider the number of features as part of the performance criterion. The less features the betterFor a given performance criterion would you rather have a machine algorithm with a performance of 97. As it can be seen in the heatmap figure radius_mean perimeter_mean and area_mean are correlated with each other so we will only use area_mean. One common way to estimate generalization capabilities is to measure the performance of the learned classifier on test data that has not been used to train the classifier. The use of swarmplots could help me in my decision making and I invite you to try it. In order to do so we ll use Seaborn. Compactness_worst concavity_worst and concave points_worst are correlated I choose concavity_worst. What can we do about it I don t have any expertise in the fields of algorithm validation or feature selection so the method I propose might either be erroneous or already in practice. The first step would be for a given k fold cross validation to average the curves obtained via the RFECV over a large range of random states. Now that we have features what do they mean Ten real valued features are computed for each cell nucleus of the image 1 radius mean of distances from center to points on the perimeter 2 texture standard deviation of gray scale values 3 perimeter 4 area 5 smoothness local variation in radius lengths 6 compactness perimeter 2 area 1. Using the k cross fold validation proved deficient to properly estimate the generalization error so I developed a methodology to improve this estimation. Since we want to detect cancer everytime there is one we might focus on having a good recall rather than a good accuracy or precision https towardsdatascience. It is therefore a classification problem. org bci bhealth QA q_and_a. read_csv data visualization library head method shows only first 5 rows feature names as an Index panda object including a list of column names and dtype. columns gives columns names in data y includes our labels and x includes our features M or B M 212 B 357 sort True default in order to be sure B M are in the right order alphabetical check if there is a NaN value in our data frame x a False indicates there are no missing values process of normalization by standardization first ten features Second ten features Third ten features correlation map do not modify x we will use it later correlation map It appears that the function recall_score needs to have y in a binary format unlike accuracy_score can take into account the alphabetical format too. If two violins look similar it might indicate a correlation between the features and if two features are correlated one can ask if it s possible or not to drop one. In this section I will give the general methodology of machine learning for a given classifier. After dropping correlated features we end up with a heatmap matrix that is almost uncorrelated We reduced the number of features from 30 to 16. Either way please tell me and help me learn more about the subjects. Disclaimer 2 like John Snow I know nothing. Furthermore breast cancer is the second leading cause of death for women in the United States and is the leading cause of cancer deaths among women ages 40 59 1 2. it s not about improving the generalization error it s about improving the estimate of the generalization error. VisualizationThe Visualization will help us understand our data. radius_worst perimeter_worst and area_worst are correlated I choose area_worst. As I said right now I choose to keep the best score without any consideration for the number of features. 2 Pick N the number of features that maximizes the averaged recall3 Run the RFE many times with a fixed number of features N4 Choose the N features that get picked up most often in the RFE5 Train a large number of Random Forest with this set of features and average the recall on the test setLet s wrap up what s left to do 1 We still have to work on the k fold cross validation as I still don t understand the influence of k on the generalization error2 Once we have a methodology to properly estimate the generalization error we can tune the hyperparameters of the Random Forest to optimize it3 We can do the same with other algorithms and choose the best I hope you enjoyed this kernel If you have any question or advice don t hesitate. Chosen 16 best features by rfe are different than the one obtained with the previous naive method. This is already quite nice to have such results with a naive feature selection and without optimization. 0 7 concavity severity of concave portions of the contour 8 concave points number of concave portions of the contour 9 symmetry 10 fractal dimension coastline approximation The mean standard error and worst or largest mean of the three largest values of these features were computed for each image resulting in 30 features. 4 Improving the estimate of the generalization errorIn our search for the best algorithm we should consider fine tuning the hyperparameters of the machine learning algorithm here the Random Forest. Another criterion to choose area_mean backed up by data this time is that the feature seems to express more differences between malignant and benign tumors on my violin plots. Now that we have our set of 11 features let s compute our final recall. 5 unfound 6 Warren J. radius_se perimeter_se and area_se are correlated I choose area_se. We found the best 20 features for best classification. Jerez Aragon\u00e9s Jos\u00e9 A. Lets look at features of data. They describe characteristics of the cell nuclei present in the image. The first thing we want is a single performance criterionI am still new to Kaggle but in the Kernels I browsed I rarely saw any discussion on the choice of the performance criterion. That procedure is recursively repeated on the pruned set until the desired number of features is reached. Radius_mean Perimeter_mean Area_mean Compactness_mean Concavity_mean Concave_points_mean are well separated between Malignant and Benign tumors as the 75 percentile of Benign tumors is below the 25 percentile of Malignant tumors. area_worst and area_mean are correlated I choose area_mean. Did you know about this methodology It is wrong I m afraid of either being wrong or having invented the wheel again. For these two reasons we need a normalization of our features prior to anything else vizualisation feature selection classification algorithm. The article states that cross validation can no longer be trusted to prove the generalization of a model when we compare a high number of models together. Our observations will lead to the conclusion that the cross validation is not a good estimate of the generalization error and a methodology will be proposed to improve the estimate of the generalization error. Secondly we used a Machine Learning algorithm namely the Random Forest in order to classify the data between malignant and benign tumors. This proves that doing a single k fold cross validation is erroneous in our application as it leads to stochastic results. 0 I could have evaluate the test error with a single random_state but I chose to average the results to stay consistent with the procedure. First of all how should we choose k to ensure the best estimate of the generalization Secondly training the same algorithm with different random_states will lead to different results and impact the generalization error and everything that comes after feature selection. We start with violin plots. pdfThis previous text explanation can be found in many books as it is the very starting point of Machine Learning yet I chose this reference because of the title On the dangers of cross validation. 1 and accuracy and F1 scores are good. In this section we will select feature with different methods that are feature selection with correlation recursive feature elimination RFE and recursive feature elimination with cross validation RFECV. INTRODUCTION Welcome everyone This is my first kernel. Artificial intelligence in medicine 34 2 113 127. com accuracy precision recall or f1 331fb37c5cb9. We write y in the binary format with B 0 and M 1 split data train 70 and test 30 random forest classifier with n_estimators 10 default split data train 70 and test 30 this time with x and not x_1 in order to have all the features Create the RFE object and rank each pixel 5 fold cross validation Plot number of features VS. Disclaimer 1 this kernel is going to evolve through time. Our feature selection will be based on the averaged optimal number of features The smoothness of the curve is such that I gain confidence in my result. According to The American Cancer Society 39 800 breast cancer related deaths are expected in 2003 2. Therefore once the optimal number N of features is fixed one has to evaluate a RFE algorithm with a fixed value of N features for a large number of random states. But let s start with the beginning what do we want In a general classification problem the goal is to learn a classifier that performs well on unseen data drawn from the same distribution as the available data in other words to learn classifiers with good generalization. However keep in mind that cross validation is not a perfect way to ensure a good generalization of your model. I m going to use the recall as my performance criterion for the rest of this analysis. the 25 percentile the median and the 75 percentile. Since we have 30 features to study I created 3 groups with 10 features each. As things go I should be able to propose a complete study with an apropriate comparaison of the different ML algorithms taking into account the relevant hyperparameters of the problem. All I see is accuracy everywhere. Let s interpret the plot above. Concave points_worst and Concavity_worst also seem correlated. Once the computations are done pick the N most occuring features. The idea is to apply the previous RFE with an additional hyperparameter that is the appropriate number of features. On the contrary fracta_dimension_mean has the same median for both tumor types so it wouldn t be a good candidate for the classifier. Here I find that 11 features are required to optimize my algorithm for a given k fold cross validation and for a given set of hyperparameters of the algorithm. Let s now look at the evolution of the optimal number of features The number of optimal features fluctuates a lot for a same classifier. With human eye we should expect being able to find intuition about what features could be good candidates for the classification. The information about Diagnosis must be kept. Even though in the last couple of decades with increased emphasis towards cancer related research new and innovative methods for early detection and treatment have been developed which helped decrease the cancer related death rates 4 6 cancer in general and breast cancer in specific is still a major cause of concern in the United States. It is therefore quite erroneous to base our understanding of the optimal feature selection on a single occurence of the cross validation. For example we clearly see that area_mean and smoothness_means are not on the same scale. The computation of the generalisation error is based on a k fold cross validation with k being another problematic hyperparameter. The previous analysis gave us good insights on what should be relevant for our classifier and it also comforted us in the possibility of having good results with a classifier. Right now the recall is computed on the test set and we will do the same with the next method. helps us understanding our data. texture_mean and texture_worst are correlated are correlated I choose texture_mean. In the United States of the 40 600 deaths from breast cancer in 2001 400 were men 3. The same sort of separation can be observed for the following features From the graph above we notice some similarities between radius_worst and perimeter_worst on one hand and concavity_worst and concave points_worst on the other hand. Let s now find the optimal number of features 3 Recursive feature elimination with cross validation and Random Forest classificationScikit proposes an algorithm that automatically finds the optimal number and choice of features required for best scoring the RFECV. First we did a data analysis and a data vizualisation. Here is something I read on Breast Cancer it dates back from 2005 but quick readings suggest things didn t evolve much in the meantime Breast cancer is a major cause of concern in the United States today. Artificial intelligence could go further and help curing cancer by tailoring specific treatments with the genome analysis of the patient Precision Medicine 7. ConclusionLet s wrap up what we did. An experimental evaluation. This is the tradeoff between having a good score and having a good understanding in your ML. If I say stupid things in these areas please do correct me. I ll propose a methodology to solve this issue but I m not sure about it. Or did we First of all I must say that I don t like this solution. WebMD medical news 2 003. Seaborn is a Python data visualization library based on matplotlib. Second step but what are these 11 features The previous algorithme gave you the best number of features statistically speaking but we lost the information provided by the determinist RFECV that is the set of features itself. cross validation scores 5 fold cross validation 5 fold cross validation 5 fold cross validation Create the RFE object and rank each pixel flatten the list find the 11 most common features do not modify x we will use it later split data train 70 and test 30 random forest classifier with n_estimators 10 default. Annual report to the nation on the status of cancer 1973 1999 featuring implications of age and aging on US cancer burden. Standardization will be used to ensure a good normalization of our features. It can only give you an estimate of the generalization. 01 and 5 features or a machine algorithm with a performance of 97. Compactness_mean concavity_mean and concave points_mean are correlated I choose concavity_mean. I might be wrong on this one. It might actually not be the best criterion and if someone tells me why I ll update it. Both of them were very basic and gave us good insights on our data. I represent the various quartiles of the solution alongside the violins i. Everytime a feature appears in the solution I increment its weight by 1. At a rate of nearly one in three cancers diagnosed breast cancer is the most frequently diagnosed cancer in women in the United States. Here the count ratio between Malignant tumorts and Benign tumors is close to 0. It s always simpler to work on a classification problem when the different classes are equally represented. I can t stress this enough but this section is not about improving any algorithm i. To check if our feature selection is correct let s use the Random Forest algorithm and find the recall for the chosen features. The American Cancer Society projected that 211 300 invasive and 55 700 in situ cases would be diagnosed in 2003 1. 7 slighlty better than the previous naive approach for the same classifier with the same Random_State and the same test set. Let s use the technique with a 5 fold cross validation and with a Random_state of 43. In the present data analysis we are not going to compare so many models together so this specific danger doesn t affect us. Basic Machine Learning AnalysisOk. Other similarities exist of course. edu romer papers CrossVal_SDM08. Compactness_se concavity_se and concave points_se are correlated I choose concavity_se. Let s not bother with cross validation yet. about breast cancer or biology medicine in general. Some remarks concerning the data 1 The id column is useless for the classification2 Diagnosis is the class label M for Malignant B for Benign3 Unnamed 32 feature includes NaN and is useless for the classification4 The range of values between features seems quite high we will use the describe function later on From these preliminary remarks one can rearrange the data by first eliminating the columns id Unnamed 32 and Diagnosis. Personnally I would have chosen 8 features rather than 20. In order to compare two features let s first use joint plot. From this small extract one can note that artificial intelligence has been around for a long time in the field of medicine. 1 Feature Selection with correlation and Random Forest classificationWe start by the simplest possible method. First of all let s look at the evolution of the recall found with the RFECV with a fixed k fold cross validation but a varying Random_state I stopped at ten random states just enough to see the variance between the results for a same k fold cross validation. 2 is a closer estimate of the generalization error. We are going to do some machine learning. Therefore we do need to calculate recall again. G\u00f3mez Ruiz Gonzalo Ramos Jim\u00e9nez Jos\u00e9 Mu\u00f1oz P\u00e9rez Emilio Alba Conejo A combined neural network and decision trees model for prognosis of breast cancer relapse Artificial Intelligence in Medicine Volume 27 Issue 1 2003 Pages 45 63 ISSN 0933 3657 4 Edwards B. According to the provided information all values are presents but let s not trust what we read on the internet Let s now use the describe function in order to look at our features These type of information on our features mean std. An equation is clearer than an explanation III. Two parameters got my attention k from the k fold cross validation and random_state the thing you put in the ML algorithm to initialize it. 7 Bertalan Mesko 2017 The role of artificial intelligence in precision medicine Expert Review of Precision Medicine and Drug Development 2 5 239 241 DOI 10. Moreover it is much more difficult to visualize our data when the features are not at the same scale. The resulting estimates of generalization can also be used for model selection by choosing from various possible classification algorithms models the one that has the lowest cross validation error and hence the lowest expected generalization error. This first criterion is not backed up by data but if I had to give a wild guess I would say that experimental measures on area might have lower uncertainties than measures of radius or perimeter. To do so we use a training with this 11 features for a large number of random_states. These 6 features would be good candidates for the classifier. At some degree being able to humanly understand your machine learning algorithm should be taken into account shouldn t it Reducing the number of features in our application could simplify the work of doctors when detecting malignant tumors. A second objective of this kernel is to learn more about breast cancer and how data science can help. However features will still be selected in order to maximize the classical performance criterion recall. What if we want to observe all correlation between features To do so Seaborn proposes a useful beautiful heatmap function. Whose absolute weights are the smallest are pruned from the current set of features. As expected radius_worst and perimeter_worst are strongly correlated there is a linear relationship between the two features linked to the 2pi ratio between perimeter and radius. Predicting breast cancer survivability a comparison of three data mining methods. But I have some questions regarding this k fold cross validation. In Proceedings of the 2008 SIAM International Conference on Data Mining pp. At first the kernel will propose a basic method of machine learning to deal with the problem and I will leave a lot of open questions things that I truly don t know. First step averaging the solutions of the RFECV to obtain an averaged RFECVWe saw that doing one cross validation is erroneous for our feature selection. We also find that 91. The second thing we want is a good generalization of our algorithmTo ensure that we have access to the k fold cross validation for instance. This study is decomposed in six steps Data analysis Data visualization Basic ML analysisIn this section I will insist on the proper way to estimate the generalization error Hyperparameter tuningComing soon I wanna be the very best AshComing soon Conclusion Enjoy your data analysis 1 http www. On the dangers of cross validation. Having radius_mean perimeter_mean and area_mean together doesn t seem good to me. I ll update the kernel accordingly. We choose the features that appear most often in the solution. All feature values are recoded with four significant digits. The idea is to take a feature compute its mean and standard deviation then substract the feature by its mean and divide the result by the standard deviation. For instance field 3 is Mean Radius field 13 is Radius SE field 23 is Worst Radius. We will use the Random Forest classification to train our model. It provides a high level interface for drawing attractive and informative statistical graphics. The algorithm will evaluate the generalisation error obtained by keeping N features and choose N in order to minimize the generalisation error or maximize the recall. However it is useless to fine tune hyperparameters if in the end it doesn t even matter you can t estimate the generalization error properly. Right now I don t know. To retrieve this information let s use a RFE algorithm with a fixed number of 11 features. Data AnalysisBefore making anything like feature selection feature extraction and classification we start with basic data analysis. ", "id": "quantumofronron/breast-cancer-data-set-feature-selection", "size": "24181", "language": "python", "html_url": "https://www.kaggle.com/code/quantumofronron/breast-cancer-data-set-feature-selection", "git_url": "https://www.kaggle.com/code/quantumofronron/breast-cancer-data-set-feature-selection", "script": "sklearn.metrics RFECV violin_plot recall_score collections seaborn numpy sklearn.ensemble sklearn.model_selection f1_score confusion_matrix RandomForestClassifier matplotlib.pyplot RFE pandas subprocess accuracy_score check_output Counter joint_plot sklearn.feature_selection train_test_split ", "entities": "(('number', 'same classifier'), 'let') (('s', 'final recall'), 'now') (('6 features', 'good classifier'), 'be') (('that', 'classifier'), 'be') (('even you', 'generalization t error'), 'be') (('k', 'k cross fold validation'), 'base') (('data how science', 'breast cancer'), 'be') (('you', '97'), 'feature') (('right now I', 'features'), 'choose') (('read_csv data visualization library head method', 'column names'), 'show') (('This', 'optimization'), 'be') (('that', 'good generalization'), 'let') (('feature values', 'four significant digits'), 'recode') (('information', 'Diagnosis'), 'keep') (('time feature', 'violin plots'), 'be') (('enough section', 'algorithm i.'), 'stress') (('me', 'subjects'), 'tell') (('I', 'performance criterion'), 'be') (('artificial intelligence', 'medicine'), 'note') (('mean deviation', 'standard deviation'), 'be') (('when features', 'same scale'), 'be') (('then we', 'best algorithm'), 'be') (('that', 'RFECV'), 'let') (('count Here ratio', 'Benign 0'), 'be') (('it', 'classifier'), 'give') (('It', 'python docker image https kaggle github'), 'come') (('concave I', 'concavity_worst'), 'correlate') (('we', 'prior anything'), 'for') (('Personnally I', '8 features'), 'choose') (('Breast cancer', 'United States'), 'be') (('I', 'result'), 'base') (('I', 'data science'), 'be') (('that', 'feature selection'), 'choose') (('I', 'area_mean'), 'correlate') (('Disclaimer 3 most', 'Kaggler user DATAI'), 'base') (('it', 'stochastic results'), 'prove') (('features', 'features VS'), 'write') (('I', 'area_worst'), 'correlate') (('smallest', 'features'), 'be') (('we', 'n_estimators 10 default'), 'create') (('Standardization', 'features'), 'use') (('However features', 'performance criterion classical recall'), 'select') (('Features', 'breast mass'), 'compute') (('It', 'cross validation'), 'be') (('varying I', 'cross same k fold validation'), 'let') (('features', 'good classification'), 'expect') (('almost We', '16'), 'end') (('it', 'good classifier'), 'have') (('39 800 breast cancer related deaths', '2003'), 'expect') (('2', 'generalization closer error'), 'be') (('s', 'features'), 'be') (('so Seaborn', 'heatmap useful beautiful function'), 'propose') (('result', 'priori'), 'give') (('one', 'random states'), 'have') (('we', 'Random learning here Forest'), '4') (('accuracy_score', 'alphabetical format'), 'give') (('s', 'chosen features'), 'let') (('They', 'present image'), 'describe') (('I', '1'), 'appear') (('us', 'data'), 'help') (('doing', 'feature selection'), 'see') (('Therefore I', 'performance criterion'), 'win') (('I', 'procedure'), '0') (('danger together so specific doesn', 'us'), 'go') (('data', 'Breast Cancer Wisconsin Diagnostic Data Set'), 'come') (('We', 'machine learning'), 'go') (('very best soon Conclusion', 'data'), 'decompose') (('we', 'other hand'), 'observe') (('I', 'it'), 'help') (('We', 'model'), 'use') (('it', 'one'), 'indicate') (('we', 'good recall'), 'be') (('Chosen', 'previous naive method'), 'be') (('we', 'instance'), 'be') (('I', 'it'), 'propose') (('I', 'problem'), 'be') (('feature elimination 2 Recursive RFE', 'feature'), 'use') (('computations', 'N most occuring features'), 'pick') (('algorithm', 'recall'), 'evaluate') (('that', 'cross validation lowest error'), 'use') (('It', 'attractive statistical graphics'), 'provide') (('Seaborn', 'Python data visualization matplotlib'), 'be') (('methodology', 'generalization error'), 'lead') (('best machine', 'breast cancer malignant tumors'), 'be') (('s', 'cross validation'), 'let') (('I', 'already practice'), 'do') (('yet I', 'cross validation'), 'find') (('we', 'cross validation'), 'don') (('steps', 'features'), 'be') (('that', 'feature cross validation recursive RFECV'), 'select') (('clearly area_mean', 'same scale'), 'see') (('I', '10 features'), 'create') (('so I', 'estimation'), 'use') (('we', 'next method'), 'compute') (('I', 'solution'), 'do') (('I', 'violins'), 'represent') (('so we', 'second step'), 'end') (('truly t', 'that'), 'propose') (('I', 'wheel'), 'know') (('that', 'most often solution'), 'choose') (('11 features', 'algorithm'), 'find') (('ML experts', 'Standardization'), 'exist') (('I', 'concavity_mean'), 'correlate') (('keeping', 'recall roughly same output'), 'let') (('I', 'concavity_se'), 'correlate') (('We', 'best classification'), 'find') (('I', 'nothing'), 'disclaimer') (('experimental measures', 'radius'), 'back') (('Artificial intelligence', 'Precision patient Medicine'), 'go') (('leading cause', '1 40 59 2'), 'be') (('desired number', 'features'), 'repeat') (('when different classes', 'classification always problem'), 's') (('radius_worst', 'perimeter'), 'correlate') (('This', 'features'), 'give') (('I', 'me'), 'correct') (('that', 'features'), 'feature') (('Reducing', 'when malignant tumors'), 'take') (('that', 'appropriate features'), 'be') (('Both', 'data'), 'be') (('s', '11 features'), 'let') (('which', 'United States'), 'be') (('75 percentile', 'Malignant tumors'), 'separate') (('when we', 'models'), 'state') (('mean standard error', '30 features'), 'compute') (('211 55 300 700', '2003'), 'project') (('why I', 'it'), 'be') (('Early detection', 'cancer'), 'seem') (('Ten real valued features', 'radius lengths'), 'feature') (('25', 'median'), 'percentile') (('k given fold', 'random states'), 'be') (('s', '43'), 'let') (('priori', 'high recall'), 'express') (('s', 'it'), 'check') (('easily resampling methods', 'generalization commonly error'), 'use') (('it', 'generalization error'), 's') (('Random classificationWe', 'simplest possible method'), 'Selection') (('Therefore we', 'recall'), 'need') (('First we', 'data analysis'), 'do') (('It', 'generalization'), 'give') (('I', 'given classifier'), 'give') (('that', 'features'), 'be') (('performance cancer other criteria', 'me'), 'seem') (('I', 'analysis'), 'm') (('you', 'it'), 'get') (('don t', 'question'), 'n') (('I', 'texture_mean'), 'correlate') (('machine well learning algorithms', 'well when unscaled features'), 'know') (('one', 'Unnamed 32'), 'remark') (('we', 'data basic analysis'), 'AnalysisBefore') (('equation', 'explanation III'), 'be') (('Secondly we', 'malignant tumors'), 'use') (('we', 'only area_mean'), 'correlate') (('This', 'ML'), 'be') (('together t', 'me'), 'have') (('however time 16 features', 'RFE'), 'compute') (('algorithm', 'which'), 'feature') (('1 kernel', 'time'), 'go') (('k fold', 'cross validation'), 'have') (('I', 'area_se'), 'correlate') (('This', 'everyone'), 'introduction') (('s', 'first joint plot'), 'let') (('so we', 'random_states'), 'use') (('that', 'model'), 'be') ", "extra": "['biopsy of the greater curvature', 'patient', 'test', 'diagnosis', 'procedure']"}