{"name": "semi supervised classification using autoencoders ", "full_name": " h1 Semi Supervised Classification using AutoEncoders h2 Introduction h1 Fraud Detection using Semi Supervised Learning ", "stargazers_count": 0, "forks_count": 0, "description": "com arthurtok introduction to ensembling stacking in python Feature Engineering Credits 1. Simple Linear Classifier 7. We will use keras package. I will use the popular titanic dataset for this purpose. com c titanic dataset. T SNE t Distributed Stochastic Neighbor Embedding is a dataset decomposition technique which reduced the dimentions of data and produces only top n components with maximum information. A number of kagglers have shared different approaches such as dataset balancing anomaly detection boosting models deep learning etc but this approach is different. Visualize Fraud Vs Non Fraud Transactions 3. The dataset consists of 28 anonymized variables 1 amount variable 1 time variable and 1 target variable Class. If you liked it please upvote. Obtain the Latent Representations 5. This can be accessed by the weights of the trained model. In this technique the model aims to find the relationships among the independent and dependent variable. This is because the autoencoder will try to learn only one class and automaticlly distinuish the other class. Let s look at the distribution of target. Applying the same technique on Titanic Dataset 1. To distinguish these characteristics we need to show the autoencoders only one class of data. Create the model architecture by compiling input layer and output layers. Now we dont need any complex model to classify this even the simpler models can be used to predict. Now we can just train a simple linear classifier on the dataset. The beauty of this approach is that we do not need too many samples of data for learning the good representations. Visualize Latent Representations Fraud vs Non Fraud 6. Every dot in the following represents a transaction. Fraud Detection using Semi Supervised Learning I am using the dataset of Credit Card Fraud Detection https www. Semi Supervised Learning is combination of supervised and unsupervised learning processes in which the unlabelled data is used for training a model as well. Here is the before and after view of Fraud and Non Fraud transactions. From the above graph we can observe that there are many non_fraud transactions which are very close to fraud transactions thus are difficult to accurately classify from a model. The following Feature Engineering Code is derived from this kernel https www. The same model will be used to generate the representations of fraud cases and we expect them to be different from non fraud ones. Visualize the latent representations Fraud Vs Non FraudNow we will create a training dataset using the latent representations obtained and let s visualize the nature of fraud vs non fraud cases. The two axis are the components extracted by tsne. Performing Some Feature Engineering Used in this Competition. Generate the hidden representations of two classes non fraud and fraud by predicting the raw inputs using the above model. Obtain the Latent Representations Now the model is trained. Dataset Preparation 2. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. These low level features are then deformed back to project the actual data. com shivamb how autoencoders work intro and usecases https i. Broadly their exists three different machine learning processes 1. Later I am also applying the same technique on Titanic https www. Before training let s perform min max scaling. Consider only 1000 rows of non fraud cases 2. Additionally We do not need to run this model for a large number of epochs. In this approach the properties of unspervised learning are used to learn the best possible representation of data and the properties of supervised learning are used to learn the relationships in the representations which are then used to make predictions. Supervised Learning is a process of training a machine learning model on a labelled dataset ie. Explanation The choice of small samples from the original dataset is based on the intuition that one class characteristics non fraud will differ from that of the other fraud. In this technique the model aims to find the most relevant patterns in the data or the segments of data. We will create another network containing sequential layers and we will only add the trained weights till the third layer where latent representation exists. We are intereseted in obtaining latent representation of the input learned by the model. AutoEncoders to the rescue What are Autoencoders Autoencoders are a special type of neural network architectures in which the output is same as the input. com sinakhorami titanic titanic best working classifierNext define the autoencoder modelTrain the modelObtain the Hidden RepresentationTrain the classifier Thus we can see that approach gives a decent results. Non Fraud transactions are represented as Green while Fraud transactions are represented as Red. For our use case let s take only about 1000 rows of non fraud transactions. Semi Supervised Classification using AutoEncoders IntroductionBy definition machine learning can be defined as a complex process of learning the best possible and most relevant patterns relationships or associations from a dataset which can be used to predict the outcomes on unseen data. More about Autoencoders If you want to gain more understanding about autoencoders you can refer to the following kernel https www. An autoencoder is a regression task where the network is asked to predict its input in other words model the identity function. Dataset PreparationFirst we will load all the required libraries and load the dataset using pandas dataframe. 17 cases are fraud transactions. With more data one can definately expect improvements. the shape of non fraud cases. But the advantage of the representation learning approach is that it is still able to handle such imbalance nature of the problems. One of the biggest challenge of this problem is that the target is highly imbalanced as only 0. Examples of supervised learning are classification regression and forecasting. Examples of unsupervised learning are clustering segmentations dimensionality reduction etc. We will use only 2000 rows of non fraud cases to train the autoencoder. The model will try to learn the best representation of non fraud cases. AutoEncoders Latent Representation Extraction 4. What a perfect graph we can observe that now fraud and non fraud transactions are pretty visibile and are linearly separable. png We will create an autoencoder model in which we only show the model non fraud cases. In this kernel I have explained how to perform classification task using semi supervised learning approach. Create a network with one input layer and one output layer having identical dimentions ie. These networks has a tight bottleneck of a few neurons in the middle forcing them to create effective representations that compress the input into a low dimensional code that can be used by the decoder to reproduce the original input. com arthurtok introduction to ensembling stacking in python2. Unsupervised Learning is a process of training a machine learning model on a dataset in which target variable is not known. Also add the optimizer and loss function I am using adadelta as the optimizer and mse as the loss function. This approach makes use of autoencoders to learn the representation of the data then a simple linear classifier is trained to classify the dataset into respective classes. Applying to a different dataset Titanic Let s Apply this approach to another dataset. Visualize Fraud and NonFraud Transactions Let s visualize the nature of fraud and non fraud transactions using T SNE. com mlg ulb creditcardfraud by ULB machine learning group. a dataset in which the target variable is known. ", "id": "shivamb/semi-supervised-classification-using-autoencoders", "size": "8230", "language": "python", "html_url": "https://www.kaggle.com/code/shivamb/semi-supervised-classification-using-autoencoders", "git_url": "https://www.kaggle.com/code/shivamb/semi-supervised-classification-using-autoencoders", "script": "sklearn.metrics display keras.layers Sequential TSNE seaborn numpy Input tsne_plot sklearn.model_selection sklearn Image matplotlib.pyplot Dense sklearn.manifold pandas classification_report LogisticRegression accuracy_score regularizers Model sklearn.linear_model get_title keras keras.models train_test_split IPython.display HTML preprocessing ", "entities": "(('s', 'target'), 'let') (('which', 'then predictions'), 'use') (('We', 'model'), 'interesete') (('dot', 'transaction'), 'represent') (('I', 'learning supervised approach'), 'explain') (('level low features', 'then back actual data'), 'deform') (('s', 'fraud T non SNE'), 'let') (('dataset', '28 anonymized variables'), 'consist') (('model', 'independent variable'), 'aim') (('that', 'original input'), 'have') (('two axis', 'tsne'), 'be') (('we', 'data'), 'distinguish') (('Later I', 'https Titanic www'), 'apply') (('Supervised Learning', 'dataset labelled ie'), 'be') (('approach', 'decent results'), 'com') (('Dataset PreparationFirst we', 'pandas dataframe'), 'load') (('I', 'purpose'), 'use') (('s', 'non fraud transactions'), 'let') (('them', 'non fraud ones'), 'use') (('Feature Engineering following Code', 'kernel https www'), 'derive') (('com how autoencoders', 'https i.'), 'shivamb') (('I', 'Credit Card Fraud Detection https www'), 'detection') (('We', 'autoencoder'), 'use') (('Here before', 'Non Fraud transactions'), 'be') (('This', 'trained model'), 'access') (('non_fraud many which', 'thus accurately model'), 'observe') (('which', 'unseen data'), 'define') (('target variable', 'which'), 'be') (('unlabelled data', 'model'), 'be') (('autoencoder', 'automaticlly other class'), 'be') (('model', 'data'), 'aim') (('we', 'non fraud only model cases'), 'create') (('it', 'problems'), 'be') (('input', 'identity function'), 'be') (('Now model', 'Latent Representations'), 'train') (('Fraud transactions', 'Red'), 'represent') (('class non one characteristics fraud', 'other fraud'), 'explanation') (('Autoencoders', 'input data'), 'train') (('linear then simple classifier', 'respective classes'), 'train') (('model', 'non fraud cases'), 'try') (('Now we', 'dataset'), 'train') (('Additionally We', 'epochs'), 'need') (('decomposition dataset which', 'maximum information'), 'be') (('you', 'kernel https following www'), 'More') (('I', 'loss function'), 'add') (('s', 'dataset'), 'apply') (('s', 'scaling'), 'let') (('where latent representation', 'third layer'), 'create') (('output', 'input'), 'autoencoder') (('Examples', 'segmentations dimensionality reduction'), 'cluster') (('even simpler models', 'complex model'), 'need') (('one', 'definately improvements'), 'expect') (('approach', 'deep etc'), 'share') (('target', 'highly only 0'), 'be') (('target variable', 'which'), 'dataset') (('Examples', 'supervised learning'), 'be') (('we', 'good representations'), 'be') (('s', 'non fraud cases'), 'visualize') ", "extra": "['outcome']"}