{"name": "ai bootcamp w1 challenge 2 layer nn group 6 ", "full_name": " h1 Week 1 Challenge h2 The task h2 The data h3 Loading the data into Python h2 Logistic regression h2 2 Layer network h2 3 Layer network h2 Excel tips h3 Matrix multiplication h3 You cannot change part of an array h3 Transpose h3 Exponents h3 Softmax h3 Random initialization h2 Grading h2 Submission ", "stargazers_count": 0, "forks_count": 0, "description": "3 Layer networkAfter you have implemented a 2 layer network open a new sheet and implement a 3 layer network. So we will use these bottles as our training data. ExponentsYou can calculate e x using the EXP function. Because this is a multi class problem the output y has already been converted to one hot matrix. This is what we return at the end. Say you have a sample with 3 values you want to compute softmax for. The dataYou can find all data in the datasource connected to this notebook on kaggle. Not that if you use transpose you always need to use CONTROL SHIFT ENTER otherwise it will not do anything. Make sure to use Paste values only when you copy over the weights Try to experiment with the learning rate while you train and get the loss as low as possible. 1 A_1 1 A_1 2 A_1 3 EXP A1 SUM EXP A1 C1 EXP B1 SUM EXP A1 C1 EXP C1 SUM EXP A1 C1 Random initializationYou can create random numbers in Excel using the RAND https support. com en us article TRANSPOSE function ed039415 ed8a 4a81 93e9 4b6dfac76027 function. In a famous wine region a disaster has happened. Your first offer to distinguish the three wine makers by taste and then spend a couple of months drinking wine and labeling bottles as been refused. The taskYou got a call from Italy. Sometimes you might encounter a warning You cannot change part of an array. SoftmaxYou will have to enter the formula for softmax element wise. You will learn about normalization next week but the basic goal is to ensure that all features of the data have the same mean and standard deviation. 0 A B C 1 0. See this youtube tutorial https www. Get labels Get inputs Print shapes just to check Package imports Matplotlib is a matlab like plotting library Numpy handles matrix operations SciKitLearn is a useful machine learning utilities library The sklearn dataset module helps generating datasets Display plots inline Just some helper functions we moved over from the last chapter sigmoid function Log Loss function Log loss derivative Load parameters from model Linear step First activation function Second linear step Second activation function Load parameters from model Load forward propagation results Backpropagation Calculate loss derivative with respect to output Calculate loss derivative with respect to second layer weights Calculate loss derivative with respect to second layer bias Calculate loss derivative with respect to first layer Calculate loss derivative with respect to first layer weights Calculate loss derivative with respect to first layer bias Store gradients Helper function to plot a decision boundary. So scroll to the right to see the full sheet. It uses a softmax activation for the output layer. You can train this network the same way as the logistic regression network. Just make sure that the output has a size of 3. If you don t fully understand this function don t worry it just generates the contour plot below. GradingThis weeks challenge is not a competition for the most accurate prediction. For the python notebook create a public kernel and share the link via slack. The labeling machine has mixed up the labels of three wine cultivars. The network has an input size of 13 and an output of 3. Now there are thousands of bottles of which nobody knows who made them. In this case either hit CONTROL SHIFT ENTER to apply the formula for the entire area or exit with ESC. Forward propagation Backpropagation Gradient descent parameter update Assign new parameters to the model Print loss accuracy every 100 iterations Hyper parameters I picked this value because it showed good results in my experiments Initialize the parameters to random values. SubmissionFor the excel challenges submit your finished Excel notebook through slack. Instead you are to build a classifier which recognizes the wine maker from 13 attributes of the wine. The activation function of the hidden layer is tanh the activation function for the output layer is softmax again. com en us article MMULT function 40593ed7 a3cd 4b6b b9a3 e4ad3c7245eb. The goal of the challenge is to deepen your understanding of how neural networks work. TransposeTo transpose a matrix in excel you can use the TRANSPOSE https support. 2 Layer networkIn the next sheet you will implement a 2 layer neural network. The Excel file contains everything you need to solve the challenge in Excel. com watch v 5bNooxRm960 if you have trouble. For this you will have to hit CONTROL SHIFT ENTER again. You deserve to kick your feed up and enjoy a glass of wine. Instead points are awarded to teams who submit the following implementations Excel Logistic Regression 2 points 2 Layer Neural Net 2 points 3 Layer Neural Net 4 pointsPython 2 Layer Neural Net 2 points 3 Layer Neural Net 4 pointsA total of 14 points can be won. You will implement a multi class classifier in Excel and Python. To update parameters copy over the New W1 and New b1 over into W1 and b1 on the left of the sheet. We need to learn these. To make training easier the data has also been normalized already. This makes it easier to deal with the data. So to create random numbers once and then freeze them use RAND for all cells first then copy it and paste the values using paste values only on the same cells. The original data contains the 13 measurements for all 178 bottles. You cannot change part of an array. Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280 OD315 of diluted wines Proline The wine makers had 178 bottles left in their cellars. To calculate the exponent of multiple values element wise like you have to do for softmax you enter EXP and then the range of cells. Note however that the rand function create a new random number every time excel refreshes. com en us article RAND function 4cbfa695 8869 4788 8d90 021ea9f5be73 function. You may choose the size of the two hidden layers yourself. So you need to make the reference to those fixed. The first week is also the most math heavy. That is you enter the formula for one cell and can then expand it for the other cells. So it is all downhill from here. But first let s solve this weeks challenge. You can load the data like this Here the parameters are initialized for 2 layer neural network. Note that you have to copy over two sets of weights and biases this time. Set min and max values and give it some padding Generate a grid of points with distance h between them Predict the function value for the whole gid Plot the contour and training examples Do forward pass get y_hat Turn values to either 1 or 0 Get total number of examples Do a prediction with the model Ensure prediction and truth vector y have the same shape Calculate the number of wrong examples Calculate accuracy First layer weights First layer bias Second layer weights Second layer bias Package and return model Load parameters Update parameters Store and return parameters Gradient descent. To make the sheet easier to handle Forward pass Backward pass and weight update are ordered horizontally not vertically. To multiply two matrices select the output area where you want the output to be enter the formula and hit CONTROL SHIFT ENTER. Week 1 ChallengeCongratulations You made it through the first weeks content The first week is the hardest because you have to learn about many new concepts. The hidden unit has a size of 5. Excel tips Matrix multiplicationThe excel function for matrix multiplication is called MMULT https support. Note that softmax needs all cells of the example. Loading the data into PythonYou can fork this notebook or simply create a new Kernel connected to this Kernels datasource in Kaggle. Logistic regressionYour first task it to implement logistic regression in Excel. The normalized data and one hot encodings have been copied over to the logistic regression and 2 Layer Wine net sheet. ", "id": "erkkinool/ai-bootcamp-w1-challenge-2-layer-nn-group-6", "size": "6206", "language": "python", "html_url": "https://www.kaggle.com/code/erkkinool/ai-bootcamp-w1-challenge-2-layer-nn-group-6", "git_url": "https://www.kaggle.com/code/erkkinool/ai-bootcamp-w1-challenge-2-layer-nn-group-6", "script": "log_loss sigmoid plot_decision_boundary forward_prop numpy matplotlib.pyplot tanh_derivative calc_accuracy initialize_parameters pandas predict train backward_prop log_loss_derivative update_parameters ", "entities": "(('it', 'just contour plot'), 'generate') (('Excel tips Matrix multiplicationThe excel function', 'matrix multiplication'), 'call') (('EXP A1 SUM EXP A1 C1 EXP B1 SUM EXP A1 C1 EXP C1 SUM EXP A1 Random 1 2 3 initializationYou', 'RAND https support'), '1') (('you', 'loss'), 'make') (('it', 'data'), 'make') (('GradingThis weeks challenge', 'most accurate prediction'), 'be') (('which', 'wine'), 'be') (('You', 'Excel'), 'implement') (('you', 'softmax'), 'say') (('You', 'two hidden layers'), 'choose') (('Update', 'Gradient descent'), 'Set') (('activation function', 'output layer'), 'be') (('you', 'Excel'), 'contain') (('Backward pass update', 'Forward'), 'pass') (('who', 'them'), 'be') (('you', 'then cells'), 'enter') (('dataYou', 'kaggle'), 'find') (('So you', 'those'), 'need') (('class multi output', 'already one hot matrix'), 'be') (('You', 'array'), 'encounter') (('wine makers', 'cellars'), 'Alcalinity') (('rand however function', 'new random number'), 'note') (('you', 'weights'), 'note') (('Logistic Neural Neural Neural Excel Regression 2 2 Net 2 3 Neural Net 4 pointsPython 2 2 3 4 pointsA total', '14 points'), 'award') (('output', 'CONTROL SHIFT ENTER'), 'select') (('you', 'trouble'), 'watch') (('Here parameters', '2 layer'), 'load') (('normalized data', 'one hot logistic regression'), 'copy') (('otherwise it', 'anything'), 'need') (('labeling machine', 'wine three cultivars'), 'mix') (('features', 'same mean deviation'), 'learn') (('Just output', '3'), 'make') (('softmax', 'example'), 'note') (('excel challenges', 'slack'), 'submit') (('it', 'values'), 'update') (('So we', 'training data'), 'use') (('You', 'wine'), 'deserve') (('SoftmaxYou', 'softmax element'), 'have') (('CONTROL', 'ESC'), 'SHIFT') (('how neural networks', 'understanding'), 'be') (('you', 'TRANSPOSE https support'), 'transpose') (('layer 2 network', 'layer 3 network'), 'implement') (('Layer 2 next sheet you', 'layer neural 2 network'), 'networkin') (('you', 'many new concepts'), 'make') (('ExponentsYou', 'EXP function'), 'calculate') (('bias Store', 'decision boundary'), 'be') (('disaster', 'wine famous region'), 'happen') (('CONTROL SHIFT', 'ENTER'), 'have') (('python notebook', 'slack'), 'create') (('It', 'output layer'), 'use') (('original data', '178 bottles'), 'contain') (('you', 'other cells'), 'be') (('You', 'logistic regression same way network'), 'train') (('first s', 'weeks challenge'), 'let') (('Loading', 'Kaggle'), 'fork') (('network', '3'), 'have') ", "extra": "[]"}