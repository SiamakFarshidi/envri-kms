{"name": "automated feature engineering for titanic dataset ", "full_name": " h1 Automated Feature Engineering and Selection for Titanic Dataset h1 Table of contents h2 1 Introduction h2 2 Load data h2 3 Clean data h2 4 Perform automated feature engineering h2 5 Curse of dimensionality Feature reduction and selection h3 5 1 Determine collinear features h3 5 2 Detect the most relevant features using linear models penalized with the L1 norm h2 6 Training and testing the simple model ", "stargazers_count": 0, "forks_count": 0, "description": "In order to come up with an accurate model it is necessary to apply combine and compare dozens of methods. However if we compare it to the accuracy that we would receive with the same random forest classfier using 83 features from features_positive then we would get the accuracy of 0. Be aware however that it is not a good idea to remove features only by correlation without understanding the removal process. Features that have very high correlation for example Embarked. 2 Detect the most relevant features using linear models penalized with the L1 normThe next step is to use linear models penalized with the L1 norml. As we can see the most of transformation functions are applied to datetime or time dependent variables. But this topic is outside of the scope of this Kernel. Clean data clean_data 4. Additionally now we are facing another problem known as the curse of dimensionality. It is aimed at automated feature engineering. If we use 20 selected features we get the accuracy of 0. The recommended approach is to combine outputs of featuretools with the human expert knowledge and use crossvalidation in order to analyze learning curves and pick up the most efficient model. Higher depth values will stack more primitives. But keep in mind that feature selection can hurt the performance of ML models. Simple calculations can be stacked on top of each other to create complex features. I hope you find this kernel helpful and some UPVOTES would be very much appreciated. Clean data First of all it is necessary to clean the data. Of course human expertise cannot be substituted but nevertheless featuretools can automate a large amount of routine work. Fare with significant difference between may require additional inve. Introduction introduction 2. Since the number of features is smaller than the number of observations in train_X the parameter dual is equal to False. Now we will apply a deep feature synthesis dfs function that will generate new features by automatically applying suitable aggregations I selected a depth of 2. Therefore we will apply the code of feature cleaning taken from one of existing Kernels Best Titanic Survival Prediction for Beginners https www. In our dataset we do not have such variables. If we maintain such features in the mode it might be difficult to assess the effect of independent features on target variable. However it does not completely subsitute the human domain knowledge. Once the entity set is created it is possible to generate new features using so called feature primitives. To work with featuretools package we should specify our dataframes train_df and test_df as entities of the entity set. The entity is just a table with a uniquely identifying column known as an index. Curse of dimensionality Feature reduction and selection To deal with the curse of dimensionality it s necessary to apply the feature reduction and selection which means removing low value features from the data. The featuretools can automatically infer the variable types numeric categorical datetime of the columns but it s a good idea to also pass in specific datatypes to override this behavior. The achieved resuls will be compared to the results obtained with handcrafted feature engineering and manual hyperparameter optimisation. To create such tables we will use categorical boolean and integer variables. Age means the sum of Age values for each unique value of Title. com vin1234 best titanic survival prediction for beginners where an interested reader can find a very detailed explanation of each steps of the data cleaning procedure. A feature primitive is an operation applied to data to create a new feature. I will rather concentrate on the following methods Determine collinear features Detect the most relevant features using linear models penalized with the L1 norm 5. Please notice that I skip essential steps such as crossvalidation the analysis of learning curves etc. For the exploration purpose I will use a well known Titanic dataset. The main takeaways from this notebook are Firstly going from 11 total features to 146 using automated feature engineering featuretools package. The featuretools is a powerful package that allows saving time to create new features from multiple tables of data. The aggregation works across multiple tables using relationships between tables. Therefore we will detect these features and delete them though applying a manual revision before removal. Therefore manual guidance is necessary. This is a list of new features. Perform automated feature engineering Once the data is cleaned we can proceed to the cake in our party i. Training and testing the simple model ttm 1. 74162 on public leaderboard. 74162 on the public leaderboard with a basic random forest classifier. Transformation these functions work on one or multiple columns of a single table. Load data load_data 3. Curse of dimensionality Feature reduction and selection frs 6. However we can create dummy tables using normalize_entity function. By using featuretools we were able to generate 146 features just in a moment. Therefore these functions will not be used. It s definitely not the deterministic process with strict rules that should be followed to achieve success. Since the main focus of this notebook is to explore featuretools we will not re invent the wheel in data cleaning. Feature primitives fall into two categories Aggregation these functions group together child datapoints for each parent and then calculate a statistic such as mean min max or standard deviation. Introduction If you have ever manually created hundreds of features for your ML project I am sure you did it then you will be happy to find out how the Python package called featuretools can help out with this task. The tricky thing is that the design of ML models contains an artistic component. 1 Determine collinear featuresCollinearity means high intercorrelations among independent features. Training and testing the simple model Finally we will create a basic random forest classifier with 2000 estimators. Indeed it is not high. In our case we do not have different tables linked between each other. What will it give us Well this way we will be able to apply both aggregation and transformation functions to generate new features. Automated feature engineering afe 5. Automated Feature Engineering and Selection for Titanic Dataset Liana Napalkova 3 September 2018 Table of contents1. automated feature engineering. The good news is that this package is very easy to use. In this notebook I will not explain all possible approaches to deal with the curse of dimensionality. Secondly applying the feature reduction and selection methods to select X most relevant features out of 146 features. ", "id": "liananapalkova/automated-feature-engineering-for-titanic-dataset", "size": "7282", "language": "python", "html_url": "https://www.kaggle.com/code/liananapalkova/automated-feature-engineering-for-titanic-dataset", "git_url": "https://www.kaggle.com/code/liananapalkova/automated-feature-engineering-for-titanic-dataset", "script": "sklearn.ensemble featuretools sklearn.feature_selection RandomForestClassifier seaborn LinearSVC matplotlib.pyplot featuretools.variable_types sklearn.svm pandas featuretools.primitives SelectFromModel Numeric ", "entities": "(('feature primitive', 'new feature'), 'be') (('Python how package', 'task'), 'introduction') (('where interested reader', 'data cleaning procedure'), 'com') (('we', 'categorical boolean variables'), 'use') (('Determine collinear 1 featuresCollinearity', 'independent features'), 'mean') (('it', 'target variable'), 'be') (('Age', 'Title'), 'mean') (('that', 'example'), 'feature') (('powerful that', 'data'), 'be') (('Therefore we', 'removal'), 'detect') (('nevertheless featuretools', 'routine work'), 'substitute') (('achieved resuls', 'hyperparameter handcrafted feature engineering optimisation'), 'compare') (('recommended approach', 'most efficient model'), 'be') (('However it', 'domain completely human knowledge'), 'subsitute') (('Determine collinear', 'L1 norm'), 'concentrate') (('we', 'just moment'), 'be') (('design', 'artistic component'), 'be') (('it', 'data'), 'datum') (('entity', 'index'), 'be') (('most', 'datetime dependent variables'), 'apply') (('Fare', 'additional inve'), 'require') (('parameter dual', 'False'), 'be') (('it', 'methods'), 'in') (('Additionally now we', 'dimensionality'), 'face') (('way we', 'transformation new features'), 'give') (('Therefore we', 'Beginners https www'), 'apply') (('aggregation', 'tables'), 'work') (('It', 'feature automated engineering'), 'aim') (('However we', 'normalize_entity function'), 'create') (('main takeaways', 'feature engineering featuretools automated package'), 'go') (('feature selection', 'ML models'), 'keep') (('however it', 'removal process'), 'be') (('I', 'curves'), 'notice') (('that', 'success'), 's') (('I', '2'), 'apply') (('I', 'Titanic well known dataset'), 'use') (('most relevant features', 'L1'), 'be') (('which', 'data'), 's') (('functions', 'single table'), 'work') (('we', '0'), 'get') (('min max', 'such mean'), 'fall') (('depth Higher values', 'more primitives'), 'stack') (('we', 'other'), 'have') (('we', 'entity set'), 'specify') (('Finally we', '2000 estimators'), 'create') (('Simple calculations', 'complex features'), 'stack') (('we', 'such variables'), 'have') (('it', 'behavior'), 'infer') (('we', 'data cleaning'), 'invent') (('topic', 'Kernel'), 'be') (('I', 'dimensionality'), 'explain') (('then we', '0'), 'get') (('it', 'feature so called primitives'), 'be') (('we', 'party i.'), 'perform') ", "extra": "['test', 'procedure']"}