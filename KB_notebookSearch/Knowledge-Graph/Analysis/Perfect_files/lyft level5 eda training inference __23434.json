{"name": "lyft level5 eda training inference ", "full_name": " h1 Lyft Motion Prediction for Autonomous Vehicles n Build motion prediction models for self driving vehicles n Table of contents n n 0 Introduction 0 n n 1 Import Packages 1 n n 2 Utility Functions 2 n 3 Basic EDA 3 n 3 1 Data Formats 3 1 n 3 1 1 Introduction 3 1 1 n 3 1 2 Zarr Format 3 1 2 n 3 1 3 Train Validation And Test Zarr 3 1 3 n 3 2 Checking Configuration Fields 3 2 n 3 3 Loading Data 3 3 n 3 4 Data Overview 3 4 n 3 4 1 Agents 3 4 1 n 3 4 2 Scenes 3 4 2 n 3 4 3 Frames 3 4 3 n 3 5 Dataset Package 3 5 n 3 5 1 ChunckedDataset 3 5 1 n 3 6 Visualize Autonomous Vehicle 3 6 n 3 6 1 Visualizing Various Rasterizer Objects 3 6 1 n 3 6 2 Visualize Trajectory Semantic View 3 6 2 n 3 6 3 Visualize Trajectory Satellite View 3 6 3 n 3 6 4 Visualize Agent 3 6 4 n 3 6 5 Visualize Individual Scene Semantic 3 6 5 n 3 6 6 Visualize Individual Scene Satellite 3 6 6 n n 4 Pytorch Baseline 4 n 4 1 Configuration 4 1 n 4 2 Loading Training Data 4 2 n 4 3 Training DataLoader 4 3 n 4 4 Model resnet50 Pytorch 4 4 n 4 5 Compilation 4 5 n 4 6 Training 4 6 n 4 7 Saving Model 4 7 n n 5 Prediction and Results 5 n 5 1 Test DataLoader 5 1 n 5 2 Getting Predictions 5 2 n 5 2 Submission 5 3 n n 6 Reference 6 n 0 Introduction n n Table of Contents 0 1 n nThis competition is hosted by ridesharing company Lyft https www lyft com which started Level 5 https self driving lyft com level5 self driving division to tackle the challenges in the field of self driving cars n nIn this competition our task is to build motion prediction models for self driving vehicles Model which can predict the movement of traffic agents around the Autonomous Vehicles such as cars cyclists and pedestrians etc We are required to predict how these different agents move in Autonomous Vehicles s environment n n What is Autonomous Vehicle n nAn autonomous vehicle or a driverless vehicle is one that is able to operate itself and perform necessary functions without any human intervention through ability to sense its surroundings n nAn autonomous vehicle utilises a fully automated driving system in order to allow the vehicle to respond to external conditions that a human driver would manage n n Competition Data n nThis dataset https www kaggle com c lyft motion prediction autonomous vehicles data is the largest collection of the traffic agent motion data This dataset includes the logs of movement of cars cyclists pedestrians and other traffic agents encountered by Lyft s autonomous fleet These logs come from processing raw lidar camera and radar data through our team s perception systems and are ideal for training motion prediction models The dataset includes n n 1000 hours of traffic agent movement n 16k miles of data from 23 vehicle n 15k semantic map annotations n nHere is the reserach paper of Prediction Dataset https paperswithcode com paper one thousand and one hours self driving n nThe dataset consists of 170 000 scenes capturing the environment around the autonomous vehicle Each scene encodes the state of the vehicle s surroundings at a given point in time source https self driving lyft com level5 prediction n n n n n n n nThis baseline solution is trained on over 2 million samples from the agent locations contained within the dataset The model predicts a single agent at a time First a raster generates a bird s eye view BEV top down raster which encodes all agents and the map The network infers the future coordinates of the agent based upon this raster n n n n n What we are predicting n nOur task in the competition is to predict the motion of external objects such as cars cyclist pedestrains etc in order to assist the self driving car We have to predict the location of objects agents in the next 50 frames n n Evaluation Metric Negative log likelihood n nWe calculate the negative log likelihood of the ground truth data given the multi modal predictions You can get more information here https www kaggle com c lyft motion prediction autonomous vehicles overview evaluation n n n https camo githubusercontent com b3634eea5be5501318957e21086781666018efa1 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239 n https camo githubusercontent com 8048a110a20827715a17eb76f8039302a576d503 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302b25323025354373756d5f742532302535436c6f672532302535436d61746863616c2537424e253744253238785f74253743253543626172253742782537445f742535456b2532432532302535437369676d61253344312532392532302535436d61746863616c2537424e253744253238795f74253743253543626172253742792537445f742535456b2532432532302535437369676d6125334431253239253744 n https camo githubusercontent com 9ba94f5c0c40666d66b93fba994cc5f7623ebd98 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302d25354366726163253742312537442537423225374425323025354373756d5f74253230253238253543626172253742782537445f742535456b2532302d253230785f74253239253545322532302b253230253238253543626172253742792537445f742535456b2532302d253230795f7425323925354532253744 n Lyft s Autonomous Vehicle AV n n Lyft s AV Introducton video n 1 Import Packages n n Table of Contents 0 1 n nWe are required to use L5Kit toolkit https github com lyft l5kit provided by the competition host to prepare preprocess data trian and evaluate the model Please add this utility script https www kaggle com pestipeti lyft l5kit unofficial fix provided by Peter s https www kaggle com pestipeti first by clicking on the Add data section inside your notebook before starting The L5Kit toolkit https github com lyft l5kit has some issue right now You can check this https www kaggle com c lyft motion prediction autonomous vehicles discussion 177125 discussion here for more information n n L5Kit is a library which lets you n n Load driving scenes from zarr files n Read semantic maps n Read aerial maps n Create birds eye view BEV images which represent a scene around an AV or another vehicle n Sample data n Train neural networks n Visualize results n 2 Utility Functions n nThanks to Trigram https www kaggle com nxrprime for providing this function n 3 EDA n n Table of Contents 0 1 n nThe data is huge around 22 GB We will use Lyft s L5Kit https github com lyft l5kit to process our data loading and visualization and training n n 3 1 Data Formats n nWe need to be familiar with the data we are using Let s dive in n n 3 1 1 Introduction n nThe L5Kit toolkit uses data format that consists of a set of numpy structured arrays Structured arrays https numpy org doc stable user basics rec html are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields The structured array can store various types of features Structured arrays are stored in memory in an interleaved format this means that one row or sample is grouped together in memory Let us take an example to understand this nlet s add some data in this structured array nAs we can see structured arrays allow us to mix different data types into a single array We will develop more intuition ahead n 3 1 2 Zarr Format n n Table of Contents 0 1 n nThe L5Kit uses zarr format to store and read these structured numpy arrays The data is available in zarr file format which can be easily load using the L5Kit https github com lyft l5kit Most of the traditional numpy operations can be handled using zarr files The zarr files are flat compact and highly performant for loading Each of the zarr file contains n n scenes driving episodes acquired from a given vehicle n frames snapshots in time of the pose of the vehicle A frame is a snapshot in time which consists of ego pose time and multiple agent states n agents a generic entity captured by the vehicle s sensors Note that only 4 of the 17 possible agent label probabilities are present in this dataset Each agent state describes the position orientation bounds and type n agents mask a mask that for train and validation masks out objects that aren t useful for training In test the mask provided in files as mask npz masks out any test object for which predictions are NOT required n traffic light faces traffic light information n n 3 1 3 Train Validation And Test Zarr n n Table of Contents 0 1 nWe can see that our train zarr file has set of 4 arrays All this 4 fields are described above in the Data Format section above Let us check these fields We will check 1 observation from each nThe data is expected to live in a folder that can be configured using the L5KIT DATA FOLDER env variable We will now develop some intuition about the data n 3 2 Checking Configuration Fields n n Table of Contents 0 1 n nWe will look at this yaml file from an external dataset provided in L5kit examples Let s look the raster params filed It contains information related to the transformation of the 3D world onto image plane You can also check for various other information n n NOTE We can make our own configuration file n 3 3 Loading Data n n Table of Contents 0 1 n nWe re building a LocalDataManager object This will resolve relative paths from the config using the L5KIT DATA FOLDER env variable we have just set We will work with sample zarr for developing intuition regarding the data Please use train zarr validate zarr and test zarr for actual model training validation and prediction nWe are going to load our sample zarr file The 2020 Lyft competition dataset is stored in four structured arrays scenes frames agents and tl faces n 3 4 Data Overview n n Table of Contents 0 1 n nWe will see each field inside the zarr files train validation and test We will use sample zarr here n 3 4 1 Agents n nAn agent is an observation by the AV of some other detected object Each entry describes the object in terms of its attributes such as position and velocity gives the agent a tracking number to track it over multiple frames but only within the same scene and its most probable label n Points to note n centroid position of agent n extent agent dimension n yaw rotation of an agent with respect to vertical axis A yaw rotation is a movement around the yaw axis of a rigid body that changes the direction it is pointing to the left or right of its direction of motion n velocity speed of the agent n track id unique id to track agent in different frames n label probabilities prabability an agent belong to one of the 17 classes We are only given three labels cyclist pedestrians and cars n Centroid Distribution nWe will see the distribution of centroid now Since centroid column consist of list per sample we will make two new columns centroid x and centroid y n Extent Distribution n nFirst we need to make new columns for extent x extent y and extent z as we have one extent column n Points to note n We can see extent distributions are right skewed n We can see long tails in positive direction n Extent Distribution Scatterplot n Yaw Distribution n Velocity Distribution nAs we can see indeed there are only four types of agents provided in the dataset such as Cars Cyclists Pedestrians and Unknown We can see that Unknown label is more as compared to other three agent labels n 3 4 2 Scenes n Table of contents 0 1 n nA scene is identified by the host i e which car was used to collect it and a start and end time It consists of multiple frames snapshots at discretized time intervals The scene datatype stores references to its corresponding frames in terms of the start and end index within the frames array described in dataframe below The frames in between these indices all correspond to the scene including start index excluding end index n Points to note n frame index interval frame index including start index excluding end index n host car used to collect data n start time start time of scene n end time end time of scene n Frame Index n Points to note n We can see linear trend here n Both host cars collected data within particular time intervals n Host Count n nWe will now see the host counts Ego vehicle used to collect the data n Points to note n We have two host cars which were used to collect the data host a013 and host a101 n 3 4 3 Frames n Table of contents 0 1 n nA frame captures all information that was observed at a time This includes the following fields as mentioned in dataframe below n Points to note n timestamp frame s timestamp n agent index interval agents vehicles cyclists and pedestrians that were captured by the ego s sensors n traffic light faces index interval traffic light index n ego translation position of host car n ego rotation rotation of host car which is collecting data using ego sensors n Ego Translation Distribution n Points to note n We can see the distributions are multi models here n Ego Translation Scatterplot n Ego Rotations Distribution n 3 4 3 Traffic Light Faces n Table of contents 0 1 n nThe traffic light bulbs red green yellow are refered as face n face id unique id for traffic light bulb n traffic light id traffic light status n traffic light face status out of red green yellow which face is active unactive unknown n 3 5 Dataset Package n n Table of Contents 0 1 n nThe dataset package for example already implements PyTorch ready datasets so you can hit the ground running and start coding immediately There are two classes in the dataset package We will be using below two datasets classes to generate inputs and targets n n EgoDataset this dataset iterates over the AV annotations n AgentDataset this dataset iterates over other agents annotations n nBoth of them can be iterated and return multi channel images from the rasterizer along with future trajectories offsets and other information We will see ahead about rasterizer and trajectories soon n nWe need cfg Zarr Dataset ChunkedDataset and rasterizer object to instantiate these n n 3 5 1 ChunckedDataset n n Table of Contents 0 1 n nThe ChunckedDataset 0 2 click class as you can see above returned four structured arrays scenes frames agents and tl faces all are described above in detail Both the EgoDataset and AgentDataset are using zarr dataset object which is made using ChunckedDataset 0 2 n n 3 6 Visualize Autonomous Vehicle n n Table of Contents 0 1 n nNow we will look into the visualisation utility of L5Kit Toolkit https github com lyft l5kit There are two core packages for visualisation n n rasterization contains classes for getting visual data as multi channel tensors and turning them into interpretable RGB images We will talk about these classes in detail shortly Each class inside this rasterization https github com lyft l5kit tree master l5kit l5kit rasterization package conatin has at least a rasterize method to get the tensor and a to rgb method to convert it into an image n n BoxRasterizer this object renders agents e g vehicles or pedestrians as oriented 2D boxes n StubRasterizer this object doesn t do anything It return all black image and can be used for testing n SatelliteRasterizer this object renders an oriented crop from a satellite map n SatBoxRasterizer this object combine a Satellite and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a satellite image n SemanticRasterizer this object renders semantic map which contains lane crosswalk information n SemBoxRasterizer this object combine a Semantic Map and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a semantic image n nTo instantiate each of these object we will use build rasterizer method n n n visualization contains utilities to draw additional information e g trajectories onto RGB images These utilities are commonly used after a to rgb call to add other information to the final visualisation Following utilities are available n n draw arrowed line Draw a single arrowed line in an RGB image n draw trajectory Draw a trajectory on oriented arrow onto an RGB image n draw reference trajectory Draw a trajectory as points onto the image n 3 6 1 Visualizing Various Rasterizer Objects n nWe will visualize different raster objects credits https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n n Table of Contents 0 1 n 3 6 2 Visualize Trajectory Semantic View n n Table of Contents 0 1 n 3 6 3 Visualize Trajectory Satellite View n n Table of Contents 0 1 n nWe can get the satellite view by changing the parameter map type inside raster params in our configuration file cfg n 3 6 4 Visualize Agent n n Table of Contents 0 1 n nWe will visualize our agents using AgentDataset class nThe green box is our AV agent The blue boxes are entities which we are captured by the sensors We want to predict the motion of these entities so that our AV can more effectively predict its path n 3 6 5 Visualize Individual Scene Semantic n n Table of Contents 0 1 n nWe will visualize the scene in depth n The green box is our AV agent and the arrow on top of it represent its motion n The blue boxes are agents cars cyclists predestrians n We can see intersection of roads n In animation we can see the AV is moving on straight path n 3 6 6 Visualize Individual Scene Satellite n n Table of Contents 0 1 n nWe are now going to visualize the satellite view for more detailed understanding n We can see the green AV agent and blue entities cars bicycles and pedestrians n We can see our agent in motion in realtion to the movement of other agents vehicles n 4 Pytorch Baseline n n Table of Contents 0 1 n n 4 1 Configuration n nLet us first make our configurations for training and testing then we will load our data n 4 2 Loading Training Data n n Table of Contents 0 1 n 4 3 Training DataLoader n n Table of Contents 0 1 n n 4 4 Model resnet50 Pytorch n n Table of Contents 0 1 nThis is for the purpose of demonstration only You can set pretrained True and train your model in separate notebook keeping on internet connection for downloading pretrained weights and make inference in separate notebook keeping internet off since subission requires you to turn off your internet connection I will use my trained weights for making inference n 4 5 Compilation n n Table of Contents 0 1 n 4 6 Training n n Table of Contents 0 1 n 4 7 Saving Model n n Table of Contents 0 1 n 5 Prediction and Results n n Table of Contents 0 1 n nBe careful you have turned of your internet connection in order to make submission Please make separate notebook for the inference This notebook is for the purpose of demonstration Train using pretrained resnet weights in one notebook and make inference using another notebook n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 300px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 246 349 n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 350px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 169 83 n n 5 1 Test DataLoader n n Table of Contents 0 1 n 5 2 Getting Predictions n n Table of Contents 0 1 n For loading checkpoint n nWEIGHT FILE kaggle input lyft l5 weights resnet34 300x300 model state 15000 pth n nmodel state torch load WEIGHT FILE map location device n nmodel load state dict model state model state dict nWe need to run below two cell to make predictions and generate submission csv I am going to use my submission csv here since I am getting memory error while making predictions n 5 3 Submission n n Table of Contents 0 1 n 6 References n n Table of Contents 0 1 n n https www twi global com technical knowledge faqs what is an autonomous vehicle n Negative log likelihood https github com lyft l5kit blob master competition md n https www kaggle com pestipeti pytorch baseline train n https self driving lyft com level5 prediction n https www kaggle com corochann lyft deep into the l5kit library n https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n https www kaggle com gpreda lyft first data exploration h1 n Build motion prediction models for self driving vehicles n Table of contents n n 0 Introduction 0 n n 1 Import Packages 1 n n 2 Utility Functions 2 n 3 Basic EDA 3 n 3 1 Data Formats 3 1 n 3 1 1 Introduction 3 1 1 n 3 1 2 Zarr Format 3 1 2 n 3 1 3 Train Validation And Test Zarr 3 1 3 n 3 2 Checking Configuration Fields 3 2 n 3 3 Loading Data 3 3 n 3 4 Data Overview 3 4 n 3 4 1 Agents 3 4 1 n 3 4 2 Scenes 3 4 2 n 3 4 3 Frames 3 4 3 n 3 5 Dataset Package 3 5 n 3 5 1 ChunckedDataset 3 5 1 n 3 6 Visualize Autonomous Vehicle 3 6 n 3 6 1 Visualizing Various Rasterizer Objects 3 6 1 n 3 6 2 Visualize Trajectory Semantic View 3 6 2 n 3 6 3 Visualize Trajectory Satellite View 3 6 3 n 3 6 4 Visualize Agent 3 6 4 n 3 6 5 Visualize Individual Scene Semantic 3 6 5 n 3 6 6 Visualize Individual Scene Satellite 3 6 6 n n 4 Pytorch Baseline 4 n 4 1 Configuration 4 1 n 4 2 Loading Training Data 4 2 n 4 3 Training DataLoader 4 3 n 4 4 Model resnet50 Pytorch 4 4 n 4 5 Compilation 4 5 n 4 6 Training 4 6 n 4 7 Saving Model 4 7 n n 5 Prediction and Results 5 n 5 1 Test DataLoader 5 1 n 5 2 Getting Predictions 5 2 n 5 2 Submission 5 3 n n 6 Reference 6 n 0 Introduction n n Table of Contents 0 1 n nThis competition is hosted by ridesharing company Lyft https www lyft com which started Level 5 https self driving lyft com level5 self driving division to tackle the challenges in the field of self driving cars n nIn this competition our task is to build motion prediction models for self driving vehicles Model which can predict the movement of traffic agents around the Autonomous Vehicles such as cars cyclists and pedestrians etc We are required to predict how these different agents move in Autonomous Vehicles s environment n n What is Autonomous Vehicle n nAn autonomous vehicle or a driverless vehicle is one that is able to operate itself and perform necessary functions without any human intervention through ability to sense its surroundings n nAn autonomous vehicle utilises a fully automated driving system in order to allow the vehicle to respond to external conditions that a human driver would manage n n Competition Data n nThis dataset https www kaggle com c lyft motion prediction autonomous vehicles data is the largest collection of the traffic agent motion data This dataset includes the logs of movement of cars cyclists pedestrians and other traffic agents encountered by Lyft s autonomous fleet These logs come from processing raw lidar camera and radar data through our team s perception systems and are ideal for training motion prediction models The dataset includes n n 1000 hours of traffic agent movement n 16k miles of data from 23 vehicle n 15k semantic map annotations n nHere is the reserach paper of Prediction Dataset https paperswithcode com paper one thousand and one hours self driving n nThe dataset consists of 170 000 scenes capturing the environment around the autonomous vehicle Each scene encodes the state of the vehicle s surroundings at a given point in time source https self driving lyft com level5 prediction n n n n n n n nThis baseline solution is trained on over 2 million samples from the agent locations contained within the dataset The model predicts a single agent at a time First a raster generates a bird s eye view BEV top down raster which encodes all agents and the map The network infers the future coordinates of the agent based upon this raster n n n n n What we are predicting n nOur task in the competition is to predict the motion of external objects such as cars cyclist pedestrains etc in order to assist the self driving car We have to predict the location of objects agents in the next 50 frames n n Evaluation Metric Negative log likelihood n nWe calculate the negative log likelihood of the ground truth data given the multi modal predictions You can get more information here https www kaggle com c lyft motion prediction autonomous vehicles overview evaluation n n n https camo githubusercontent com b3634eea5be5501318957e21086781666018efa1 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239 n https camo githubusercontent com 8048a110a20827715a17eb76f8039302a576d503 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302b25323025354373756d5f742532302535436c6f672532302535436d61746863616c2537424e253744253238785f74253743253543626172253742782537445f742535456b2532432532302535437369676d61253344312532392532302535436d61746863616c2537424e253744253238795f74253743253543626172253742792537445f742535456b2532432532302535437369676d6125334431253239253744 n https camo githubusercontent com 9ba94f5c0c40666d66b93fba994cc5f7623ebd98 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302d25354366726163253742312537442537423225374425323025354373756d5f74253230253238253543626172253742782537445f742535456b2532302d253230785f74253239253545322532302b253230253238253543626172253742792537445f742535456b2532302d253230795f7425323925354532253744 n Lyft s Autonomous Vehicle AV n n Lyft s AV Introducton video n 1 Import Packages n n Table of Contents 0 1 n nWe are required to use L5Kit toolkit https github com lyft l5kit provided by the competition host to prepare preprocess data trian and evaluate the model Please add this utility script https www kaggle com pestipeti lyft l5kit unofficial fix provided by Peter s https www kaggle com pestipeti first by clicking on the Add data section inside your notebook before starting The L5Kit toolkit https github com lyft l5kit has some issue right now You can check this https www kaggle com c lyft motion prediction autonomous vehicles discussion 177125 discussion here for more information n n L5Kit is a library which lets you n n Load driving scenes from zarr files n Read semantic maps n Read aerial maps n Create birds eye view BEV images which represent a scene around an AV or another vehicle n Sample data n Train neural networks n Visualize results n 2 Utility Functions n nThanks to Trigram https www kaggle com nxrprime for providing this function n 3 EDA n n Table of Contents 0 1 n nThe data is huge around 22 GB We will use Lyft s L5Kit https github com lyft l5kit to process our data loading and visualization and training n n 3 1 Data Formats n nWe need to be familiar with the data we are using Let s dive in n n 3 1 1 Introduction n nThe L5Kit toolkit uses data format that consists of a set of numpy structured arrays Structured arrays https numpy org doc stable user basics rec html are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields The structured array can store various types of features Structured arrays are stored in memory in an interleaved format this means that one row or sample is grouped together in memory Let us take an example to understand this nlet s add some data in this structured array nAs we can see structured arrays allow us to mix different data types into a single array We will develop more intuition ahead n 3 1 2 Zarr Format n n Table of Contents 0 1 n nThe L5Kit uses zarr format to store and read these structured numpy arrays The data is available in zarr file format which can be easily load using the L5Kit https github com lyft l5kit Most of the traditional numpy operations can be handled using zarr files The zarr files are flat compact and highly performant for loading Each of the zarr file contains n n scenes driving episodes acquired from a given vehicle n frames snapshots in time of the pose of the vehicle A frame is a snapshot in time which consists of ego pose time and multiple agent states n agents a generic entity captured by the vehicle s sensors Note that only 4 of the 17 possible agent label probabilities are present in this dataset Each agent state describes the position orientation bounds and type n agents mask a mask that for train and validation masks out objects that aren t useful for training In test the mask provided in files as mask npz masks out any test object for which predictions are NOT required n traffic light faces traffic light information n n 3 1 3 Train Validation And Test Zarr n n Table of Contents 0 1 nWe can see that our train zarr file has set of 4 arrays All this 4 fields are described above in the Data Format section above Let us check these fields We will check 1 observation from each nThe data is expected to live in a folder that can be configured using the L5KIT DATA FOLDER env variable We will now develop some intuition about the data n 3 2 Checking Configuration Fields n n Table of Contents 0 1 n nWe will look at this yaml file from an external dataset provided in L5kit examples Let s look the raster params filed It contains information related to the transformation of the 3D world onto image plane You can also check for various other information n n NOTE We can make our own configuration file n 3 3 Loading Data n n Table of Contents 0 1 n nWe re building a LocalDataManager object This will resolve relative paths from the config using the L5KIT DATA FOLDER env variable we have just set We will work with sample zarr for developing intuition regarding the data Please use train zarr validate zarr and test zarr for actual model training validation and prediction nWe are going to load our sample zarr file The 2020 Lyft competition dataset is stored in four structured arrays scenes frames agents and tl faces n 3 4 Data Overview n n Table of Contents 0 1 n nWe will see each field inside the zarr files train validation and test We will use sample zarr here n 3 4 1 Agents n nAn agent is an observation by the AV of some other detected object Each entry describes the object in terms of its attributes such as position and velocity gives the agent a tracking number to track it over multiple frames but only within the same scene and its most probable label n Points to note n centroid position of agent n extent agent dimension n yaw rotation of an agent with respect to vertical axis A yaw rotation is a movement around the yaw axis of a rigid body that changes the direction it is pointing to the left or right of its direction of motion n velocity speed of the agent n track id unique id to track agent in different frames n label probabilities prabability an agent belong to one of the 17 classes We are only given three labels cyclist pedestrians and cars n Centroid Distribution nWe will see the distribution of centroid now Since centroid column consist of list per sample we will make two new columns centroid x and centroid y n Extent Distribution n nFirst we need to make new columns for extent x extent y and extent z as we have one extent column n Points to note n We can see extent distributions are right skewed n We can see long tails in positive direction n Extent Distribution Scatterplot n Yaw Distribution n Velocity Distribution nAs we can see indeed there are only four types of agents provided in the dataset such as Cars Cyclists Pedestrians and Unknown We can see that Unknown label is more as compared to other three agent labels n 3 4 2 Scenes n Table of contents 0 1 n nA scene is identified by the host i e which car was used to collect it and a start and end time It consists of multiple frames snapshots at discretized time intervals The scene datatype stores references to its corresponding frames in terms of the start and end index within the frames array described in dataframe below The frames in between these indices all correspond to the scene including start index excluding end index n Points to note n frame index interval frame index including start index excluding end index n host car used to collect data n start time start time of scene n end time end time of scene n Frame Index n Points to note n We can see linear trend here n Both host cars collected data within particular time intervals n Host Count n nWe will now see the host counts Ego vehicle used to collect the data n Points to note n We have two host cars which were used to collect the data host a013 and host a101 n 3 4 3 Frames n Table of contents 0 1 n nA frame captures all information that was observed at a time This includes the following fields as mentioned in dataframe below n Points to note n timestamp frame s timestamp n agent index interval agents vehicles cyclists and pedestrians that were captured by the ego s sensors n traffic light faces index interval traffic light index n ego translation position of host car n ego rotation rotation of host car which is collecting data using ego sensors n Ego Translation Distribution n Points to note n We can see the distributions are multi models here n Ego Translation Scatterplot n Ego Rotations Distribution n 3 4 3 Traffic Light Faces n Table of contents 0 1 n nThe traffic light bulbs red green yellow are refered as face n face id unique id for traffic light bulb n traffic light id traffic light status n traffic light face status out of red green yellow which face is active unactive unknown n 3 5 Dataset Package n n Table of Contents 0 1 n nThe dataset package for example already implements PyTorch ready datasets so you can hit the ground running and start coding immediately There are two classes in the dataset package We will be using below two datasets classes to generate inputs and targets n n EgoDataset this dataset iterates over the AV annotations n AgentDataset this dataset iterates over other agents annotations n nBoth of them can be iterated and return multi channel images from the rasterizer along with future trajectories offsets and other information We will see ahead about rasterizer and trajectories soon n nWe need cfg Zarr Dataset ChunkedDataset and rasterizer object to instantiate these n n 3 5 1 ChunckedDataset n n Table of Contents 0 1 n nThe ChunckedDataset 0 2 click class as you can see above returned four structured arrays scenes frames agents and tl faces all are described above in detail Both the EgoDataset and AgentDataset are using zarr dataset object which is made using ChunckedDataset 0 2 n n 3 6 Visualize Autonomous Vehicle n n Table of Contents 0 1 n nNow we will look into the visualisation utility of L5Kit Toolkit https github com lyft l5kit There are two core packages for visualisation n n rasterization contains classes for getting visual data as multi channel tensors and turning them into interpretable RGB images We will talk about these classes in detail shortly Each class inside this rasterization https github com lyft l5kit tree master l5kit l5kit rasterization package conatin has at least a rasterize method to get the tensor and a to rgb method to convert it into an image n n BoxRasterizer this object renders agents e g vehicles or pedestrians as oriented 2D boxes n StubRasterizer this object doesn t do anything It return all black image and can be used for testing n SatelliteRasterizer this object renders an oriented crop from a satellite map n SatBoxRasterizer this object combine a Satellite and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a satellite image n SemanticRasterizer this object renders semantic map which contains lane crosswalk information n SemBoxRasterizer this object combine a Semantic Map and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a semantic image n nTo instantiate each of these object we will use build rasterizer method n n n visualization contains utilities to draw additional information e g trajectories onto RGB images These utilities are commonly used after a to rgb call to add other information to the final visualisation Following utilities are available n n draw arrowed line Draw a single arrowed line in an RGB image n draw trajectory Draw a trajectory on oriented arrow onto an RGB image n draw reference trajectory Draw a trajectory as points onto the image n 3 6 1 Visualizing Various Rasterizer Objects n nWe will visualize different raster objects credits https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n n Table of Contents 0 1 n 3 6 2 Visualize Trajectory Semantic View n n Table of Contents 0 1 n 3 6 3 Visualize Trajectory Satellite View n n Table of Contents 0 1 n nWe can get the satellite view by changing the parameter map type inside raster params in our configuration file cfg n 3 6 4 Visualize Agent n n Table of Contents 0 1 n nWe will visualize our agents using AgentDataset class nThe green box is our AV agent The blue boxes are entities which we are captured by the sensors We want to predict the motion of these entities so that our AV can more effectively predict its path n 3 6 5 Visualize Individual Scene Semantic n n Table of Contents 0 1 n nWe will visualize the scene in depth n The green box is our AV agent and the arrow on top of it represent its motion n The blue boxes are agents cars cyclists predestrians n We can see intersection of roads n In animation we can see the AV is moving on straight path n 3 6 6 Visualize Individual Scene Satellite n n Table of Contents 0 1 n nWe are now going to visualize the satellite view for more detailed understanding n We can see the green AV agent and blue entities cars bicycles and pedestrians n We can see our agent in motion in realtion to the movement of other agents vehicles n 4 Pytorch Baseline n n Table of Contents 0 1 n n 4 1 Configuration n nLet us first make our configurations for training and testing then we will load our data n 4 2 Loading Training Data n n Table of Contents 0 1 n 4 3 Training DataLoader n n Table of Contents 0 1 n n 4 4 Model resnet50 Pytorch n n Table of Contents 0 1 nThis is for the purpose of demonstration only You can set pretrained True and train your model in separate notebook keeping on internet connection for downloading pretrained weights and make inference in separate notebook keeping internet off since subission requires you to turn off your internet connection I will use my trained weights for making inference n 4 5 Compilation n n Table of Contents 0 1 n 4 6 Training n n Table of Contents 0 1 n 4 7 Saving Model n n Table of Contents 0 1 n 5 Prediction and Results n n Table of Contents 0 1 n nBe careful you have turned of your internet connection in order to make submission Please make separate notebook for the inference This notebook is for the purpose of demonstration Train using pretrained resnet weights in one notebook and make inference using another notebook n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 300px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 246 349 n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 350px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 169 83 n n 5 1 Test DataLoader n n Table of Contents 0 1 n 5 2 Getting Predictions n n Table of Contents 0 1 n For loading checkpoint n nWEIGHT FILE kaggle input lyft l5 weights resnet34 300x300 model state 15000 pth n nmodel state torch load WEIGHT FILE map location device n nmodel load state dict model state model state dict nWe need to run below two cell to make predictions and generate submission csv I am going to use my submission csv here since I am getting memory error while making predictions n 5 3 Submission n n Table of Contents 0 1 n 6 References n n Table of Contents 0 1 n n https www twi global com technical knowledge faqs what is an autonomous vehicle n Negative log likelihood https github com lyft l5kit blob master competition md n https www kaggle com pestipeti pytorch baseline train n https self driving lyft com level5 prediction n https www kaggle com corochann lyft deep into the l5kit library n https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n https www kaggle com gpreda lyft first data exploration h3 Build motion prediction models for self driving vehicles n Table of contents n n 0 Introduction 0 n n 1 Import Packages 1 n n 2 Utility Functions 2 n 3 Basic EDA 3 n 3 1 Data Formats 3 1 n 3 1 1 Introduction 3 1 1 n 3 1 2 Zarr Format 3 1 2 n 3 1 3 Train Validation And Test Zarr 3 1 3 n 3 2 Checking Configuration Fields 3 2 n 3 3 Loading Data 3 3 n 3 4 Data Overview 3 4 n 3 4 1 Agents 3 4 1 n 3 4 2 Scenes 3 4 2 n 3 4 3 Frames 3 4 3 n 3 5 Dataset Package 3 5 n 3 5 1 ChunckedDataset 3 5 1 n 3 6 Visualize Autonomous Vehicle 3 6 n 3 6 1 Visualizing Various Rasterizer Objects 3 6 1 n 3 6 2 Visualize Trajectory Semantic View 3 6 2 n 3 6 3 Visualize Trajectory Satellite View 3 6 3 n 3 6 4 Visualize Agent 3 6 4 n 3 6 5 Visualize Individual Scene Semantic 3 6 5 n 3 6 6 Visualize Individual Scene Satellite 3 6 6 n n 4 Pytorch Baseline 4 n 4 1 Configuration 4 1 n 4 2 Loading Training Data 4 2 n 4 3 Training DataLoader 4 3 n 4 4 Model resnet50 Pytorch 4 4 n 4 5 Compilation 4 5 n 4 6 Training 4 6 n 4 7 Saving Model 4 7 n n 5 Prediction and Results 5 n 5 1 Test DataLoader 5 1 n 5 2 Getting Predictions 5 2 n 5 2 Submission 5 3 n n 6 Reference 6 n 0 Introduction n n Table of Contents 0 1 n nThis competition is hosted by ridesharing company Lyft https www lyft com which started Level 5 https self driving lyft com level5 self driving division to tackle the challenges in the field of self driving cars n nIn this competition our task is to build motion prediction models for self driving vehicles Model which can predict the movement of traffic agents around the Autonomous Vehicles such as cars cyclists and pedestrians etc We are required to predict how these different agents move in Autonomous Vehicles s environment n n What is Autonomous Vehicle n nAn autonomous vehicle or a driverless vehicle is one that is able to operate itself and perform necessary functions without any human intervention through ability to sense its surroundings n nAn autonomous vehicle utilises a fully automated driving system in order to allow the vehicle to respond to external conditions that a human driver would manage n n Competition Data n nThis dataset https www kaggle com c lyft motion prediction autonomous vehicles data is the largest collection of the traffic agent motion data This dataset includes the logs of movement of cars cyclists pedestrians and other traffic agents encountered by Lyft s autonomous fleet These logs come from processing raw lidar camera and radar data through our team s perception systems and are ideal for training motion prediction models The dataset includes n n 1000 hours of traffic agent movement n 16k miles of data from 23 vehicle n 15k semantic map annotations n nHere is the reserach paper of Prediction Dataset https paperswithcode com paper one thousand and one hours self driving n nThe dataset consists of 170 000 scenes capturing the environment around the autonomous vehicle Each scene encodes the state of the vehicle s surroundings at a given point in time source https self driving lyft com level5 prediction n n n n n n n nThis baseline solution is trained on over 2 million samples from the agent locations contained within the dataset The model predicts a single agent at a time First a raster generates a bird s eye view BEV top down raster which encodes all agents and the map The network infers the future coordinates of the agent based upon this raster n n n n n What we are predicting n nOur task in the competition is to predict the motion of external objects such as cars cyclist pedestrains etc in order to assist the self driving car We have to predict the location of objects agents in the next 50 frames n n Evaluation Metric Negative log likelihood n nWe calculate the negative log likelihood of the ground truth data given the multi modal predictions You can get more information here https www kaggle com c lyft motion prediction autonomous vehicles overview evaluation n n n https camo githubusercontent com b3634eea5be5501318957e21086781666018efa1 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239 n https camo githubusercontent com 8048a110a20827715a17eb76f8039302a576d503 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302b25323025354373756d5f742532302535436c6f672532302535436d61746863616c2537424e253744253238785f74253743253543626172253742782537445f742535456b2532432532302535437369676d61253344312532392532302535436d61746863616c2537424e253744253238795f74253743253543626172253742792537445f742535456b2532432532302535437369676d6125334431253239253744 n https camo githubusercontent com 9ba94f5c0c40666d66b93fba994cc5f7623ebd98 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302d25354366726163253742312537442537423225374425323025354373756d5f74253230253238253543626172253742782537445f742535456b2532302d253230785f74253239253545322532302b253230253238253543626172253742792537445f742535456b2532302d253230795f7425323925354532253744 n Lyft s Autonomous Vehicle AV n n Lyft s AV Introducton video n 1 Import Packages n n Table of Contents 0 1 n nWe are required to use L5Kit toolkit https github com lyft l5kit provided by the competition host to prepare preprocess data trian and evaluate the model Please add this utility script https www kaggle com pestipeti lyft l5kit unofficial fix provided by Peter s https www kaggle com pestipeti first by clicking on the Add data section inside your notebook before starting The L5Kit toolkit https github com lyft l5kit has some issue right now You can check this https www kaggle com c lyft motion prediction autonomous vehicles discussion 177125 discussion here for more information n n L5Kit is a library which lets you n n Load driving scenes from zarr files n Read semantic maps n Read aerial maps n Create birds eye view BEV images which represent a scene around an AV or another vehicle n Sample data n Train neural networks n Visualize results n 2 Utility Functions n nThanks to Trigram https www kaggle com nxrprime for providing this function n 3 EDA n n Table of Contents 0 1 n nThe data is huge around 22 GB We will use Lyft s L5Kit https github com lyft l5kit to process our data loading and visualization and training n n 3 1 Data Formats n nWe need to be familiar with the data we are using Let s dive in n n 3 1 1 Introduction n nThe L5Kit toolkit uses data format that consists of a set of numpy structured arrays Structured arrays https numpy org doc stable user basics rec html are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields The structured array can store various types of features Structured arrays are stored in memory in an interleaved format this means that one row or sample is grouped together in memory Let us take an example to understand this nlet s add some data in this structured array nAs we can see structured arrays allow us to mix different data types into a single array We will develop more intuition ahead n 3 1 2 Zarr Format n n Table of Contents 0 1 n nThe L5Kit uses zarr format to store and read these structured numpy arrays The data is available in zarr file format which can be easily load using the L5Kit https github com lyft l5kit Most of the traditional numpy operations can be handled using zarr files The zarr files are flat compact and highly performant for loading Each of the zarr file contains n n scenes driving episodes acquired from a given vehicle n frames snapshots in time of the pose of the vehicle A frame is a snapshot in time which consists of ego pose time and multiple agent states n agents a generic entity captured by the vehicle s sensors Note that only 4 of the 17 possible agent label probabilities are present in this dataset Each agent state describes the position orientation bounds and type n agents mask a mask that for train and validation masks out objects that aren t useful for training In test the mask provided in files as mask npz masks out any test object for which predictions are NOT required n traffic light faces traffic light information n n 3 1 3 Train Validation And Test Zarr n n Table of Contents 0 1 nWe can see that our train zarr file has set of 4 arrays All this 4 fields are described above in the Data Format section above Let us check these fields We will check 1 observation from each nThe data is expected to live in a folder that can be configured using the L5KIT DATA FOLDER env variable We will now develop some intuition about the data n 3 2 Checking Configuration Fields n n Table of Contents 0 1 n nWe will look at this yaml file from an external dataset provided in L5kit examples Let s look the raster params filed It contains information related to the transformation of the 3D world onto image plane You can also check for various other information n n NOTE We can make our own configuration file n 3 3 Loading Data n n Table of Contents 0 1 n nWe re building a LocalDataManager object This will resolve relative paths from the config using the L5KIT DATA FOLDER env variable we have just set We will work with sample zarr for developing intuition regarding the data Please use train zarr validate zarr and test zarr for actual model training validation and prediction nWe are going to load our sample zarr file The 2020 Lyft competition dataset is stored in four structured arrays scenes frames agents and tl faces n 3 4 Data Overview n n Table of Contents 0 1 n nWe will see each field inside the zarr files train validation and test We will use sample zarr here n 3 4 1 Agents n nAn agent is an observation by the AV of some other detected object Each entry describes the object in terms of its attributes such as position and velocity gives the agent a tracking number to track it over multiple frames but only within the same scene and its most probable label n Points to note n centroid position of agent n extent agent dimension n yaw rotation of an agent with respect to vertical axis A yaw rotation is a movement around the yaw axis of a rigid body that changes the direction it is pointing to the left or right of its direction of motion n velocity speed of the agent n track id unique id to track agent in different frames n label probabilities prabability an agent belong to one of the 17 classes We are only given three labels cyclist pedestrians and cars n Centroid Distribution nWe will see the distribution of centroid now Since centroid column consist of list per sample we will make two new columns centroid x and centroid y n Extent Distribution n nFirst we need to make new columns for extent x extent y and extent z as we have one extent column n Points to note n We can see extent distributions are right skewed n We can see long tails in positive direction n Extent Distribution Scatterplot n Yaw Distribution n Velocity Distribution nAs we can see indeed there are only four types of agents provided in the dataset such as Cars Cyclists Pedestrians and Unknown We can see that Unknown label is more as compared to other three agent labels n 3 4 2 Scenes n Table of contents 0 1 n nA scene is identified by the host i e which car was used to collect it and a start and end time It consists of multiple frames snapshots at discretized time intervals The scene datatype stores references to its corresponding frames in terms of the start and end index within the frames array described in dataframe below The frames in between these indices all correspond to the scene including start index excluding end index n Points to note n frame index interval frame index including start index excluding end index n host car used to collect data n start time start time of scene n end time end time of scene n Frame Index n Points to note n We can see linear trend here n Both host cars collected data within particular time intervals n Host Count n nWe will now see the host counts Ego vehicle used to collect the data n Points to note n We have two host cars which were used to collect the data host a013 and host a101 n 3 4 3 Frames n Table of contents 0 1 n nA frame captures all information that was observed at a time This includes the following fields as mentioned in dataframe below n Points to note n timestamp frame s timestamp n agent index interval agents vehicles cyclists and pedestrians that were captured by the ego s sensors n traffic light faces index interval traffic light index n ego translation position of host car n ego rotation rotation of host car which is collecting data using ego sensors n Ego Translation Distribution n Points to note n We can see the distributions are multi models here n Ego Translation Scatterplot n Ego Rotations Distribution n 3 4 3 Traffic Light Faces n Table of contents 0 1 n nThe traffic light bulbs red green yellow are refered as face n face id unique id for traffic light bulb n traffic light id traffic light status n traffic light face status out of red green yellow which face is active unactive unknown n 3 5 Dataset Package n n Table of Contents 0 1 n nThe dataset package for example already implements PyTorch ready datasets so you can hit the ground running and start coding immediately There are two classes in the dataset package We will be using below two datasets classes to generate inputs and targets n n EgoDataset this dataset iterates over the AV annotations n AgentDataset this dataset iterates over other agents annotations n nBoth of them can be iterated and return multi channel images from the rasterizer along with future trajectories offsets and other information We will see ahead about rasterizer and trajectories soon n nWe need cfg Zarr Dataset ChunkedDataset and rasterizer object to instantiate these n n 3 5 1 ChunckedDataset n n Table of Contents 0 1 n nThe ChunckedDataset 0 2 click class as you can see above returned four structured arrays scenes frames agents and tl faces all are described above in detail Both the EgoDataset and AgentDataset are using zarr dataset object which is made using ChunckedDataset 0 2 n n 3 6 Visualize Autonomous Vehicle n n Table of Contents 0 1 n nNow we will look into the visualisation utility of L5Kit Toolkit https github com lyft l5kit There are two core packages for visualisation n n rasterization contains classes for getting visual data as multi channel tensors and turning them into interpretable RGB images We will talk about these classes in detail shortly Each class inside this rasterization https github com lyft l5kit tree master l5kit l5kit rasterization package conatin has at least a rasterize method to get the tensor and a to rgb method to convert it into an image n n BoxRasterizer this object renders agents e g vehicles or pedestrians as oriented 2D boxes n StubRasterizer this object doesn t do anything It return all black image and can be used for testing n SatelliteRasterizer this object renders an oriented crop from a satellite map n SatBoxRasterizer this object combine a Satellite and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a satellite image n SemanticRasterizer this object renders semantic map which contains lane crosswalk information n SemBoxRasterizer this object combine a Semantic Map and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a semantic image n nTo instantiate each of these object we will use build rasterizer method n n n visualization contains utilities to draw additional information e g trajectories onto RGB images These utilities are commonly used after a to rgb call to add other information to the final visualisation Following utilities are available n n draw arrowed line Draw a single arrowed line in an RGB image n draw trajectory Draw a trajectory on oriented arrow onto an RGB image n draw reference trajectory Draw a trajectory as points onto the image n 3 6 1 Visualizing Various Rasterizer Objects n nWe will visualize different raster objects credits https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n n Table of Contents 0 1 n 3 6 2 Visualize Trajectory Semantic View n n Table of Contents 0 1 n 3 6 3 Visualize Trajectory Satellite View n n Table of Contents 0 1 n nWe can get the satellite view by changing the parameter map type inside raster params in our configuration file cfg n 3 6 4 Visualize Agent n n Table of Contents 0 1 n nWe will visualize our agents using AgentDataset class nThe green box is our AV agent The blue boxes are entities which we are captured by the sensors We want to predict the motion of these entities so that our AV can more effectively predict its path n 3 6 5 Visualize Individual Scene Semantic n n Table of Contents 0 1 n nWe will visualize the scene in depth n The green box is our AV agent and the arrow on top of it represent its motion n The blue boxes are agents cars cyclists predestrians n We can see intersection of roads n In animation we can see the AV is moving on straight path n 3 6 6 Visualize Individual Scene Satellite n n Table of Contents 0 1 n nWe are now going to visualize the satellite view for more detailed understanding n We can see the green AV agent and blue entities cars bicycles and pedestrians n We can see our agent in motion in realtion to the movement of other agents vehicles n 4 Pytorch Baseline n n Table of Contents 0 1 n n 4 1 Configuration n nLet us first make our configurations for training and testing then we will load our data n 4 2 Loading Training Data n n Table of Contents 0 1 n 4 3 Training DataLoader n n Table of Contents 0 1 n n 4 4 Model resnet50 Pytorch n n Table of Contents 0 1 nThis is for the purpose of demonstration only You can set pretrained True and train your model in separate notebook keeping on internet connection for downloading pretrained weights and make inference in separate notebook keeping internet off since subission requires you to turn off your internet connection I will use my trained weights for making inference n 4 5 Compilation n n Table of Contents 0 1 n 4 6 Training n n Table of Contents 0 1 n 4 7 Saving Model n n Table of Contents 0 1 n 5 Prediction and Results n n Table of Contents 0 1 n nBe careful you have turned of your internet connection in order to make submission Please make separate notebook for the inference This notebook is for the purpose of demonstration Train using pretrained resnet weights in one notebook and make inference using another notebook n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 300px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 246 349 n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 350px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 169 83 n n 5 1 Test DataLoader n n Table of Contents 0 1 n 5 2 Getting Predictions n n Table of Contents 0 1 n For loading checkpoint n nWEIGHT FILE kaggle input lyft l5 weights resnet34 300x300 model state 15000 pth n nmodel state torch load WEIGHT FILE map location device n nmodel load state dict model state model state dict nWe need to run below two cell to make predictions and generate submission csv I am going to use my submission csv here since I am getting memory error while making predictions n 5 3 Submission n n Table of Contents 0 1 n 6 References n n Table of Contents 0 1 n n https www twi global com technical knowledge faqs what is an autonomous vehicle n Negative log likelihood https github com lyft l5kit blob master competition md n https www kaggle com pestipeti pytorch baseline train n https self driving lyft com level5 prediction n https www kaggle com corochann lyft deep into the l5kit library n https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n https www kaggle com gpreda lyft first data exploration h3 n Table of contents n n 0 Introduction 0 n n 1 Import Packages 1 n n 2 Utility Functions 2 n 3 Basic EDA 3 n 3 1 Data Formats 3 1 n 3 1 1 Introduction 3 1 1 n 3 1 2 Zarr Format 3 1 2 n 3 1 3 Train Validation And Test Zarr 3 1 3 n 3 2 Checking Configuration Fields 3 2 n 3 3 Loading Data 3 3 n 3 4 Data Overview 3 4 n 3 4 1 Agents 3 4 1 n 3 4 2 Scenes 3 4 2 n 3 4 3 Frames 3 4 3 n 3 5 Dataset Package 3 5 n 3 5 1 ChunckedDataset 3 5 1 n 3 6 Visualize Autonomous Vehicle 3 6 n 3 6 1 Visualizing Various Rasterizer Objects 3 6 1 n 3 6 2 Visualize Trajectory Semantic View 3 6 2 n 3 6 3 Visualize Trajectory Satellite View 3 6 3 n 3 6 4 Visualize Agent 3 6 4 n 3 6 5 Visualize Individual Scene Semantic 3 6 5 n 3 6 6 Visualize Individual Scene Satellite 3 6 6 n n 4 Pytorch Baseline 4 n 4 1 Configuration 4 1 n 4 2 Loading Training Data 4 2 n 4 3 Training DataLoader 4 3 n 4 4 Model resnet50 Pytorch 4 4 n 4 5 Compilation 4 5 n 4 6 Training 4 6 n 4 7 Saving Model 4 7 n n 5 Prediction and Results 5 n 5 1 Test DataLoader 5 1 n 5 2 Getting Predictions 5 2 n 5 2 Submission 5 3 n n 6 Reference 6 n 0 Introduction n n Table of Contents 0 1 n nThis competition is hosted by ridesharing company Lyft https www lyft com which started Level 5 https self driving lyft com level5 self driving division to tackle the challenges in the field of self driving cars n nIn this competition our task is to build motion prediction models for self driving vehicles Model which can predict the movement of traffic agents around the Autonomous Vehicles such as cars cyclists and pedestrians etc We are required to predict how these different agents move in Autonomous Vehicles s environment n n What is Autonomous Vehicle n nAn autonomous vehicle or a driverless vehicle is one that is able to operate itself and perform necessary functions without any human intervention through ability to sense its surroundings n nAn autonomous vehicle utilises a fully automated driving system in order to allow the vehicle to respond to external conditions that a human driver would manage n n Competition Data n nThis dataset https www kaggle com c lyft motion prediction autonomous vehicles data is the largest collection of the traffic agent motion data This dataset includes the logs of movement of cars cyclists pedestrians and other traffic agents encountered by Lyft s autonomous fleet These logs come from processing raw lidar camera and radar data through our team s perception systems and are ideal for training motion prediction models The dataset includes n n 1000 hours of traffic agent movement n 16k miles of data from 23 vehicle n 15k semantic map annotations n nHere is the reserach paper of Prediction Dataset https paperswithcode com paper one thousand and one hours self driving n nThe dataset consists of 170 000 scenes capturing the environment around the autonomous vehicle Each scene encodes the state of the vehicle s surroundings at a given point in time source https self driving lyft com level5 prediction n n n n n n n nThis baseline solution is trained on over 2 million samples from the agent locations contained within the dataset The model predicts a single agent at a time First a raster generates a bird s eye view BEV top down raster which encodes all agents and the map The network infers the future coordinates of the agent based upon this raster n n n n n What we are predicting n nOur task in the competition is to predict the motion of external objects such as cars cyclist pedestrains etc in order to assist the self driving car We have to predict the location of objects agents in the next 50 frames n n Evaluation Metric Negative log likelihood n nWe calculate the negative log likelihood of the ground truth data given the multi modal predictions You can get more information here https www kaggle com c lyft motion prediction autonomous vehicles overview evaluation n n n https camo githubusercontent com b3634eea5be5501318957e21086781666018efa1 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239 n https camo githubusercontent com 8048a110a20827715a17eb76f8039302a576d503 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302b25323025354373756d5f742532302535436c6f672532302535436d61746863616c2537424e253744253238785f74253743253543626172253742782537445f742535456b2532432532302535437369676d61253344312532392532302535436d61746863616c2537424e253744253238795f74253743253543626172253742792537445f742535456b2532432532302535437369676d6125334431253239253744 n https camo githubusercontent com 9ba94f5c0c40666d66b93fba994cc5f7623ebd98 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302d25354366726163253742312537442537423225374425323025354373756d5f74253230253238253543626172253742782537445f742535456b2532302d253230785f74253239253545322532302b253230253238253543626172253742792537445f742535456b2532302d253230795f7425323925354532253744 n Lyft s Autonomous Vehicle AV n n Lyft s AV Introducton video n 1 Import Packages n n Table of Contents 0 1 n nWe are required to use L5Kit toolkit https github com lyft l5kit provided by the competition host to prepare preprocess data trian and evaluate the model Please add this utility script https www kaggle com pestipeti lyft l5kit unofficial fix provided by Peter s https www kaggle com pestipeti first by clicking on the Add data section inside your notebook before starting The L5Kit toolkit https github com lyft l5kit has some issue right now You can check this https www kaggle com c lyft motion prediction autonomous vehicles discussion 177125 discussion here for more information n n L5Kit is a library which lets you n n Load driving scenes from zarr files n Read semantic maps n Read aerial maps n Create birds eye view BEV images which represent a scene around an AV or another vehicle n Sample data n Train neural networks n Visualize results n 2 Utility Functions n nThanks to Trigram https www kaggle com nxrprime for providing this function n 3 EDA n n Table of Contents 0 1 n nThe data is huge around 22 GB We will use Lyft s L5Kit https github com lyft l5kit to process our data loading and visualization and training n n 3 1 Data Formats n nWe need to be familiar with the data we are using Let s dive in n n 3 1 1 Introduction n nThe L5Kit toolkit uses data format that consists of a set of numpy structured arrays Structured arrays https numpy org doc stable user basics rec html are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields The structured array can store various types of features Structured arrays are stored in memory in an interleaved format this means that one row or sample is grouped together in memory Let us take an example to understand this nlet s add some data in this structured array nAs we can see structured arrays allow us to mix different data types into a single array We will develop more intuition ahead n 3 1 2 Zarr Format n n Table of Contents 0 1 n nThe L5Kit uses zarr format to store and read these structured numpy arrays The data is available in zarr file format which can be easily load using the L5Kit https github com lyft l5kit Most of the traditional numpy operations can be handled using zarr files The zarr files are flat compact and highly performant for loading Each of the zarr file contains n n scenes driving episodes acquired from a given vehicle n frames snapshots in time of the pose of the vehicle A frame is a snapshot in time which consists of ego pose time and multiple agent states n agents a generic entity captured by the vehicle s sensors Note that only 4 of the 17 possible agent label probabilities are present in this dataset Each agent state describes the position orientation bounds and type n agents mask a mask that for train and validation masks out objects that aren t useful for training In test the mask provided in files as mask npz masks out any test object for which predictions are NOT required n traffic light faces traffic light information n n 3 1 3 Train Validation And Test Zarr n n Table of Contents 0 1 nWe can see that our train zarr file has set of 4 arrays All this 4 fields are described above in the Data Format section above Let us check these fields We will check 1 observation from each nThe data is expected to live in a folder that can be configured using the L5KIT DATA FOLDER env variable We will now develop some intuition about the data n 3 2 Checking Configuration Fields n n Table of Contents 0 1 n nWe will look at this yaml file from an external dataset provided in L5kit examples Let s look the raster params filed It contains information related to the transformation of the 3D world onto image plane You can also check for various other information n n NOTE We can make our own configuration file n 3 3 Loading Data n n Table of Contents 0 1 n nWe re building a LocalDataManager object This will resolve relative paths from the config using the L5KIT DATA FOLDER env variable we have just set We will work with sample zarr for developing intuition regarding the data Please use train zarr validate zarr and test zarr for actual model training validation and prediction nWe are going to load our sample zarr file The 2020 Lyft competition dataset is stored in four structured arrays scenes frames agents and tl faces n 3 4 Data Overview n n Table of Contents 0 1 n nWe will see each field inside the zarr files train validation and test We will use sample zarr here n 3 4 1 Agents n nAn agent is an observation by the AV of some other detected object Each entry describes the object in terms of its attributes such as position and velocity gives the agent a tracking number to track it over multiple frames but only within the same scene and its most probable label n Points to note n centroid position of agent n extent agent dimension n yaw rotation of an agent with respect to vertical axis A yaw rotation is a movement around the yaw axis of a rigid body that changes the direction it is pointing to the left or right of its direction of motion n velocity speed of the agent n track id unique id to track agent in different frames n label probabilities prabability an agent belong to one of the 17 classes We are only given three labels cyclist pedestrians and cars n Centroid Distribution nWe will see the distribution of centroid now Since centroid column consist of list per sample we will make two new columns centroid x and centroid y n Extent Distribution n nFirst we need to make new columns for extent x extent y and extent z as we have one extent column n Points to note n We can see extent distributions are right skewed n We can see long tails in positive direction n Extent Distribution Scatterplot n Yaw Distribution n Velocity Distribution nAs we can see indeed there are only four types of agents provided in the dataset such as Cars Cyclists Pedestrians and Unknown We can see that Unknown label is more as compared to other three agent labels n 3 4 2 Scenes n Table of contents 0 1 n nA scene is identified by the host i e which car was used to collect it and a start and end time It consists of multiple frames snapshots at discretized time intervals The scene datatype stores references to its corresponding frames in terms of the start and end index within the frames array described in dataframe below The frames in between these indices all correspond to the scene including start index excluding end index n Points to note n frame index interval frame index including start index excluding end index n host car used to collect data n start time start time of scene n end time end time of scene n Frame Index n Points to note n We can see linear trend here n Both host cars collected data within particular time intervals n Host Count n nWe will now see the host counts Ego vehicle used to collect the data n Points to note n We have two host cars which were used to collect the data host a013 and host a101 n 3 4 3 Frames n Table of contents 0 1 n nA frame captures all information that was observed at a time This includes the following fields as mentioned in dataframe below n Points to note n timestamp frame s timestamp n agent index interval agents vehicles cyclists and pedestrians that were captured by the ego s sensors n traffic light faces index interval traffic light index n ego translation position of host car n ego rotation rotation of host car which is collecting data using ego sensors n Ego Translation Distribution n Points to note n We can see the distributions are multi models here n Ego Translation Scatterplot n Ego Rotations Distribution n 3 4 3 Traffic Light Faces n Table of contents 0 1 n nThe traffic light bulbs red green yellow are refered as face n face id unique id for traffic light bulb n traffic light id traffic light status n traffic light face status out of red green yellow which face is active unactive unknown n 3 5 Dataset Package n n Table of Contents 0 1 n nThe dataset package for example already implements PyTorch ready datasets so you can hit the ground running and start coding immediately There are two classes in the dataset package We will be using below two datasets classes to generate inputs and targets n n EgoDataset this dataset iterates over the AV annotations n AgentDataset this dataset iterates over other agents annotations n nBoth of them can be iterated and return multi channel images from the rasterizer along with future trajectories offsets and other information We will see ahead about rasterizer and trajectories soon n nWe need cfg Zarr Dataset ChunkedDataset and rasterizer object to instantiate these n n 3 5 1 ChunckedDataset n n Table of Contents 0 1 n nThe ChunckedDataset 0 2 click class as you can see above returned four structured arrays scenes frames agents and tl faces all are described above in detail Both the EgoDataset and AgentDataset are using zarr dataset object which is made using ChunckedDataset 0 2 n n 3 6 Visualize Autonomous Vehicle n n Table of Contents 0 1 n nNow we will look into the visualisation utility of L5Kit Toolkit https github com lyft l5kit There are two core packages for visualisation n n rasterization contains classes for getting visual data as multi channel tensors and turning them into interpretable RGB images We will talk about these classes in detail shortly Each class inside this rasterization https github com lyft l5kit tree master l5kit l5kit rasterization package conatin has at least a rasterize method to get the tensor and a to rgb method to convert it into an image n n BoxRasterizer this object renders agents e g vehicles or pedestrians as oriented 2D boxes n StubRasterizer this object doesn t do anything It return all black image and can be used for testing n SatelliteRasterizer this object renders an oriented crop from a satellite map n SatBoxRasterizer this object combine a Satellite and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a satellite image n SemanticRasterizer this object renders semantic map which contains lane crosswalk information n SemBoxRasterizer this object combine a Semantic Map and a Box Rasterizers into a single class You can visualize agents e g vehicles or pedestrians as oriented 2D boxes in a semantic image n nTo instantiate each of these object we will use build rasterizer method n n n visualization contains utilities to draw additional information e g trajectories onto RGB images These utilities are commonly used after a to rgb call to add other information to the final visualisation Following utilities are available n n draw arrowed line Draw a single arrowed line in an RGB image n draw trajectory Draw a trajectory on oriented arrow onto an RGB image n draw reference trajectory Draw a trajectory as points onto the image n 3 6 1 Visualizing Various Rasterizer Objects n nWe will visualize different raster objects credits https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n n Table of Contents 0 1 n 3 6 2 Visualize Trajectory Semantic View n n Table of Contents 0 1 n 3 6 3 Visualize Trajectory Satellite View n n Table of Contents 0 1 n nWe can get the satellite view by changing the parameter map type inside raster params in our configuration file cfg n 3 6 4 Visualize Agent n n Table of Contents 0 1 n nWe will visualize our agents using AgentDataset class nThe green box is our AV agent The blue boxes are entities which we are captured by the sensors We want to predict the motion of these entities so that our AV can more effectively predict its path n 3 6 5 Visualize Individual Scene Semantic n n Table of Contents 0 1 n nWe will visualize the scene in depth n The green box is our AV agent and the arrow on top of it represent its motion n The blue boxes are agents cars cyclists predestrians n We can see intersection of roads n In animation we can see the AV is moving on straight path n 3 6 6 Visualize Individual Scene Satellite n n Table of Contents 0 1 n nWe are now going to visualize the satellite view for more detailed understanding n We can see the green AV agent and blue entities cars bicycles and pedestrians n We can see our agent in motion in realtion to the movement of other agents vehicles n 4 Pytorch Baseline n n Table of Contents 0 1 n n 4 1 Configuration n nLet us first make our configurations for training and testing then we will load our data n 4 2 Loading Training Data n n Table of Contents 0 1 n 4 3 Training DataLoader n n Table of Contents 0 1 n n 4 4 Model resnet50 Pytorch n n Table of Contents 0 1 nThis is for the purpose of demonstration only You can set pretrained True and train your model in separate notebook keeping on internet connection for downloading pretrained weights and make inference in separate notebook keeping internet off since subission requires you to turn off your internet connection I will use my trained weights for making inference n 4 5 Compilation n n Table of Contents 0 1 n 4 6 Training n n Table of Contents 0 1 n 4 7 Saving Model n n Table of Contents 0 1 n 5 Prediction and Results n n Table of Contents 0 1 n nBe careful you have turned of your internet connection in order to make submission Please make separate notebook for the inference This notebook is for the purpose of demonstration Train using pretrained resnet weights in one notebook and make inference using another notebook n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 300px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 246 349 n nInference Trained using Google Colab n Model Single mode baseline resnet18 n Steps Trained for 30000 iterations batch 16 n Size Input size 350px history 1s 10 frames n Optimizer Adam 1e 3 n Loss MSE Loss n LB 169 83 n n 5 1 Test DataLoader n n Table of Contents 0 1 n 5 2 Getting Predictions n n Table of Contents 0 1 n For loading checkpoint n nWEIGHT FILE kaggle input lyft l5 weights resnet34 300x300 model state 15000 pth n nmodel state torch load WEIGHT FILE map location device n nmodel load state dict model state model state dict nWe need to run below two cell to make predictions and generate submission csv I am going to use my submission csv here since I am getting memory error while making predictions n 5 3 Submission n n Table of Contents 0 1 n 6 References n n Table of Contents 0 1 n n https www twi global com technical knowledge faqs what is an autonomous vehicle n Negative log likelihood https github com lyft l5kit blob master competition md n https www kaggle com pestipeti pytorch baseline train n https self driving lyft com level5 prediction n https www kaggle com corochann lyft deep into the l5kit library n https www kaggle com corochann lyft deep into the l5kit library 1 Understanding Rasterizer class n https www kaggle com gpreda lyft first data exploration ", "stargazers_count": 0, "forks_count": 0, "description": "We are going to load our sample. 2 Getting Predictions 5 2 5. We want to predict the motion of these entities so that our AV can more effectively predict its path. 5 Dataset Package 3 5 3. Train using pretrained resnet weights in one notebook and make inference using another notebook. vehicles or pedestrians as oriented 2D boxes in a semantic image. 0 filter all other agents. com which started Level 5 https self driving. 2 Checking Configuration Fields Table of Contents 0. 1 We can see that our train. credits https www. You can visualize agents e. Utility Functions 2 3. train validation and test. 1 Visualizing Various Rasterizer Objects We will visualize different raster objects. 3 Visualize Trajectory Satellite View Table of Contents 0. 1 Be careful you have turned of your internet connection in order to make submission. The structured array can store various types of features. 1 Agents 3 4 1 3. 3 Loading Data Table of Contents 0. Note that only 4 of the 17 possible agent label_probabilities are present in this dataset. This will resolve relative paths from the config using the L5KIT_DATA_FOLDER env variable we have just set. com level5 prediction https www. A frame is a snapshot in time which consists of ego pose time and multiple agent states. zarr for actual model training validation and prediction. Points to note centroid position of agent extent agent dimension yaw rotation of an agent with respect to vertical axis. The blue boxes are agents cars cyclists predestrians. 3 Training DataLoader Table of Contents 0. 2 Checking Configuration Fields 3 2 3. com lyft l5kit has some issue right now. 2 click class as you can see above returned four structured arrays scenes frames agents and tl_faces all are described above in detail. 4 Visualize Agent Table of Contents 0. 7 Saving Model 4 7 5. Extent Distribution Scatterplot Yaw Distribution Velocity DistributionAs we can see indeed there are only four types of agents provided in the dataset such as Cars Cyclists Pedestrians and Unknown. 1 For loading checkpoint WEIGHT_FILE kaggle input lyft l5 weights resnet34_300x300_model_state_15000. Understanding Rasterizer class https www. The data is available in. 2 Loading Training Data Table of Contents 0. The data is expected to live in a folder that can be configured using the L5KIT_DATA_FOLDER env variable. You can also check for various other information. 1 Introduction 3 1 1 3. com corochann lyft deep into the l5kit library 1. velocity speed of the agent track_id unique id to track agent in different frames label_probabilities prabability an agent belong to one of the 17 classes. Prediction and Results Table of Contents 0. It contains information related to the transformation of the 3D world onto image plane. Inference Trained using Google Colab Model Single mode baseline resnet18 Steps Trained for 30000 iterations batch 16 Size Input size 300px history 1s 10 frames Optimizer Adam 1e 3 Loss MSE Loss LB 246. 3 Submission Table of Contents 0. com level5 self driving division to tackle the challenges in the field of self driving cars. 2 Zarr Format 3 1 2 3. npz masks out any test object for which predictions are NOT required. The scene datatype stores references to its corresponding frames in terms of the start and end index within the frames array described in dataframe below. Let s look the raster_params filed. Competition DataThis dataset https www. We will talk about these classes in detail shortly. The L5Kit toolkit https github. 5 Visualize Individual Scene Semantic 3 6 5 3. The dataset consists of 170 000 scenes capturing the environment around the autonomous vehicle. Utility Functions Thanks to Trigram https www. 1 ChunckedDataset 3 5 1 3. com pestipeti lyft l5kit unofficial fix provided by Peter s https www. 4 Data Overview 3 4 3. NOTE We can make our own configuration file. com c lyft motion prediction autonomous vehicles overview evaluation. 4 Visualize Agent 3 6 4 3. 1 This competition is hosted by ridesharing company Lyft https www. Ego Translation Scatterplot Ego Rotations Distribution 3. The blue boxes are entities which we are captured by the sensors. 1 The L5Kit uses zarr format to store and read these structured numpy arrays. face_id unique id for traffic light bulb traffic_light_id traffic light status traffic_light_face_status out of red green yellow which face is active unactive unknown 3. csv here since I am getting memory error while making predictions. SatelliteRasterizer this object renders an oriented crop from a satellite map. com 9ba94f5c0c40666d66b93fba994cc5f7623ebd98 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302d25354366726163253742312537442537423225374425323025354373756d5f74253230253238253543626172253742782537445f742535456b2532302d253230785f74253239253545322532302b253230253238253543626172253742792537445f742535456b2532302d253230795f7425323925354532253744 Lyft s Autonomous Vehicle AV Lyft s AV Introducton video. Each class inside this rasterization https github. 6 Visualize Individual Scene Satellite Table of Contents 0. com c lyft motion prediction autonomous vehicles data is the largest collection of the traffic agent motion data. We will use sample. 1 AgentsAn agent is an observation by the AV of some other detected object. 3 Frames Table of contents 0. draw_arrowed_line Draw a single arrowed line in an RGB image. EgoDataset this dataset iterates over the AV annotations AgentDataset this dataset iterates over other agents annotations Both of them can be iterated and return multi channel images from the rasterizer along with future trajectories offsets and other information. We can see the green AV agent and blue entities cars bicycles and pedestrians. To instantiate each of these object we will use build_rasterizer method. This notebook is for the purpose of demonstration. 1 Data Formats 3 1 3. 2 Zarr Format Table of Contents 0. 6 Visualize Autonomous Vehicle Table of Contents 0. Points to note timestamp frame s timestamp agent_index_interval agents vehicles cyclists and pedestrians that were captured by the ego s sensors traffic_light_faces_index_interval traffic light index ego_translation position of host car. 2 Loading Training Data 4 2 4. com lyft l5kit tree master l5kit l5kit rasterization package conatin has at least a rasterize method to get the tensor and a to_rgb method to convert it into an image. com lyft l5kit to process our data loading and visualization and training. com 8048a110a20827715a17eb76f8039302a576d503 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532302533442532302d2532302535436c6f6725323025354373756d5f6b253230652535452537422535436c6f67253238632535456b2532392532302b25323025354373756d5f742532302535436c6f672532302535436d61746863616c2537424e253744253238785f74253743253543626172253742782537445f742535456b2532432532302535437369676d61253344312532392532302535436d61746863616c2537424e253744253238795f74253743253543626172253742792537445f742535456b2532432532302535437369676d6125334431253239253744 https camo. SatBoxRasterizer this object combine a Satellite and a Box Rasterizers into a single class. 1 The traffic light bulbs red green yellow are refered as face. These utilities are commonly used after a to_rgb call to add other information to the final visualisation. 5 show those obstacles with 0. We will be using below two datasets classes to generate inputs and targets. ego_rotation rotation of host car which is collecting data using ego sensors Ego Translation Distribution Points to note We can see the distributions are multi models here. 1 The data is huge around 22 GB. html are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields. Basic EDA 3 3. Please add this utility script https www. In animation we can see the AV is moving on straight path. 349Inference Trained using Google Colab Model Single mode baseline resnet18 Steps Trained for 30000 iterations batch 16 Size Input size 350px history 1s 10 frames Optimizer Adam 1e 3 Loss MSE Loss LB 169. Evaluation Metric Negative log likelihoodWe calculate the negative log likelihood of the ground truth data given the multi modal predictions. We can see that Unknown label is more as compared to other three agent labels. SemBoxRasterizer this object combine a Semantic Map and a Box Rasterizers into a single class. 3 Train Validation And Test Zarr Table of Contents 0. 5 Visualize Individual Scene Semantic Table of Contents 0. 1 We are now going to visualize the satellite view for more detailed understanding. 6 Training Table of Contents 0. The network infers the future coordinates of the agent based upon this raster. Each scene encodes the state of the vehicle s surroundings at a given point in time. From 0 to 1 per axis 0. 2 Visualize Trajectory Semantic View 3 6 2 3. 5 Compilation Table of Contents 0. 1 This is for the purpose of demonstration only. I am going to use my submission. Following utilities are available. We can see intersection of roads. Extent DistributionFirst we need to make new columns for extent_x extent_y and extent_z as we have one extent column. 4 Model resnet50 Pytorch Table of Contents 0. com b3634eea5be5501318957e21086781666018efa1 68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354362675f77686974652532302535436c617267652532304c2532302533442532302d2532302535436c6f6725323070253238785f253742312532432532302535436c646f747325324325323054253744253243253230795f253742312532432532302535436c646f74732532432532305425374425374363253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742782537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253243253230253543626172253742792537445f253742312532432532302535436c646f747325324325323054253744253545253742312532432532302535436c646f74732532432532304b253744253239 https camo. 1 We can get the satellite view by changing the parameter map_type inside raster_params in our configuration file cfg. Model which can predict the movement of traffic agents around the Autonomous Vehicles such as cars cyclists and pedestrians etc. There are two classes in the dataset package. 3 Traffic Light Faces Table of contents 0. 2 Submission 5 3 6. 2 Visualize Trajectory Semantic View Table of Contents 0. com corochann lyft deep into the l5kit library https www. 6 Training 4 6 4. We will develop more intuition ahead. StubRasterizer this object doesn t do anything. 1 We will see each field inside the zarr files. agents_mask a mask that for train and validation masks out objects that aren t useful for training. vehicles or pedestrians as oriented 2D boxes in a satellite image. We will work with sample. L5Kit is a library which lets you Load driving scenes from zarr files Read semantic maps Read aerial maps Create birds eye view BEV images which represent a scene around an AV or another vehicle Sample data Train neural networks Visualize results 2. The green box is our AV agent. All this 4 fields are described above in the Data Format section above. Both the EgoDataset and AgentDataset are using zarr dataset object which is made using ChunckedDataset 0. 3 Visualize Trajectory Satellite View 3 6 3 3. load_state_dict model_state model_state_dict We need to run below two cell to make predictions and generate submission. com nxrprime for providing this function. We need cfg Zarr Dataset ChunkedDataset and rasterizer object to instantiate these. 5 Compilation 4 5 4. The frames in between these indices all correspond to the scene including start index excluding end index. Points to note frame_index_interval frame index including start index excluding end index host car used to collect data start_time start time of scene end_time end time of scene Frame Index Points to note We can see linear trend here. We can see our agent in motion in realtion to the movement of other agents vehicles. Points to note We can see extent distributions are right skewed. References Table of Contents 0. What we are predicting Our task in the competition is to predict the motion of external objects such as cars cyclist pedestrains etc in order to assist the self driving car. There are two core packages for visualisation rasterization contains classes for getting visual data as multi channel tensors and turning them into interpretable RGB images. Let us check these fields. which car was used to collect it and a start and end time. 2 Scenes 3 4 2 3. load WEIGHT_FILE map_location device model. In this competition our task is to build motion prediction models for self driving vehicles. 4 Data Overview Table of Contents 0. 1 ConfigurationLet us first make our configurations for training and testing then we will load our data. 1 We will look at this yaml file from an external dataset provided in L5kit examples. I will use my trained weights for making inference. 7 Saving Model Table of Contents 0. BoxRasterizer this object renders agents e. It return all black image and can be used for testing. 1 Now we will look into the visualisation utility of L5Kit Toolkit https github. The 2020 Lyft competition dataset is stored in four structured arrays scenes frames agents and tl_faces. let s add some data in this structured array. pth model_state torch. 1 The dataset package for example already implements PyTorch ready datasets so you can hit the ground running and start coding immediately. Structured arrays https numpy. Each entry describes the object in terms of its attributes such as position and velocity gives the agent a tracking number to track it over multiple frames but only within the same scene and its most probable label. Host CountWe will now see the host counts Ego vehicle used to collect the data. An autonomous vehicle utilises a fully automated driving system in order to allow the vehicle to respond to external conditions that a human driver would manage. source https self driving. 1 Visualizing Various Rasterizer Objects 3 6 1 3. com level5 prediction This baseline solution is trained on over 2 million samples from the agent locations contained within the dataset. We have to predict the location of objects agents in the next 50 frames. draw_trajectory Draw a trajectory on oriented arrow onto an RGB image. visualization contains utilities to draw additional information e. com pestipeti first by clicking on the Add data section inside your notebook before starting. Pytorch Baseline Table of Contents 0. Both host cars collected data within particular time intervals. Lyft Motion Prediction for Autonomous Vehicles 128663 Build motion prediction models for self driving vehicles 128663 Table of contents 0. 2 Getting Predictions Table of Contents 0. zarr file has set of 4 arrays. Import Packages 1 2. 5 would show the ego centered in the image. What is Autonomous Vehicle An autonomous vehicle or a driverless vehicle is one that is able to operate itself and perform necessary functions without any human intervention through ability to sense its surroundings. 3 Frames 3 4 3 3. This dataset includes the logs of movement of cars cyclists pedestrians and other traffic agents encountered by Lyft s autonomous fleet. The dataset includes 1000 hours of traffic agent movement 16k miles of data from 23 vehicle 15k semantic map annotationsHere is the reserach paper of Prediction Dataset https paperswithcode. Prediction and Results 5 5. We will see ahead about rasterizer and trajectories soon. Structured arrays are stored in memory in an interleaved format this means that one row or sample is grouped together in memory. 6 Visualize Individual Scene Satellite 3 6 6 4. traffic_light_faces traffic light information. A yaw rotation is a movement around the yaw axis of a rigid body that changes the direction it is pointing to the left or right of its direction of motion. draw_reference_trajectory Draw a trajectory as points onto the image. 1 Configuration 4 1 4. You can set pretrained True and train your model in separate notebook keeping on internet connection for downloading pretrained weights and make inference in separate notebook keeping internet off since subission requires you to turn off your internet connection. 5 Dataset Package Table of Contents 0. 5 probability of being one of the classes we care about cars bikes peds etc. This includes the following fields as mentioned in dataframe below. The zarr files are flat compact and highly performant for loading. Each agent state describes the position orientation bounds and type. com pestipeti pytorch baseline train https self driving. Since centroid column consist of list per sample we will make two new columns centroid_x and centroid_y. 1 Test DataLoader Table of Contents 0. 1 Test DataLoader 5 1 5. com lyft l5kit blob master competition. 2 Scenes Table of contents 0. 3 Loading Data 3 3 3. com lyft l5kit provided by the competition host to prepare preprocess data trian and evaluate the model. We will check 1 observation from each. zarr file format which can be easily load using the L5Kit https github. 6 Visualize Autonomous Vehicle 3 6 3. frames snapshots in time of the pose of the vehicle. It consists of multiple frames snapshots at discretized time intervals. 1 A frame captures all information that was observed at a time. 3 Train Validation And Test Zarr 3 1 3 3. We are only given three labels cyclist pedestrians and cars Centroid DistributionWe will see the distribution of centroid now. Points to note We have two host cars which were used to collect the data host a013 and host a101. inference cfg root directory set env variable for data training cfg rasterizer dataloader set pretrained True while training This is 512 for resnet18 and resnet34 And it is 2048 for the other resnets X Y coords for the future positions output shape Bx50x2 You can add more layers here. zarr file contains scenes driving episodes acquired from a given vehicle. You can get more information here https www. Import Packages Table of Contents 0. org doc stable user basics. The model predicts a single agent at a time. You can check this https www. Most of the traditional numpy operations can be handled using. Pytorch Baseline 4 4. com gpreda lyft first data exploration import packages level5 toolkit level5 toolkit visualization deep learning check files in directory animation for scene set env variable for data get configuration yaml Raster Parameters Prepare all rasterizer and EgoDataset for each rasterizer print key key raster object for visualization EgoDataset object select one example from our dataset plot ground truth trajectory EgoDataset object AgentDataset object animation satellite view animation training cfg raster s spatial resolution meters per pixel the size in the real world one pixel corresponds to. the keys are relative to the dataset environment variable e. com technical knowledge faqs what is an autonomous vehicle Negative log likelihood https github. First a raster generates a bird s eye view BEV top down raster which encodes all agents and the map. com c lyft motion prediction autonomous vehicles discussion 177125 discussion here for more information. com paper one thousand and one hours self driving. 1 Data FormatsWe need to be familiar with the data we are using. 3 Training DataLoader 4 3 4. 1 We will visualize our agents using AgentDataset class. Understanding Rasterizer class Table of Contents 0. We will use Lyft s L5Kit https github. 2 compiling model get hardware type CPU GPU TPU training loop forward pass not all the output steps are valid but we can filter them out from the loss using availabilities Backward pass save full trained model test configuration Rasterizer Test dataset dataloader Saved state dict from the training notebook submission. zarr for developing intuition regarding the data. We are required to predict how these different agents move in Autonomous Vehicles s environment. 1 We are required to use L5Kit toolkit https github. Introduction Table of Contents 0. These logs come from processing raw lidar camera and radar data through our team s perception systems and are ideal for training motion prediction models. As we can see structured arrays allow us to mix different data types into a single array. We will now develop some intuition about the data. We can see long tails in positive direction. 1 The ChunckedDataset 0. 1 We will visualize the scene in depth. 1 IntroductionThe L5Kit toolkit uses data format that consists of a set of numpy structured arrays. 4 Model resnet50 Pytorch 4 4 4. trajectories onto RGB images. In test the mask provided in files as mask. 1 ChunckedDataset Table of Contents 0. The green box is our AV agent and the arrow on top of it represent its motion. 0 include every obstacle 0. agents a generic entity captured by the vehicle s sensors. 1 A scene is identified by the host i. Introduction 0 1. EDA Table of Contents 0. Let us take an example to understand this. Please make separate notebook for the inference. SemanticRasterizer this object renders semantic map which contains lane crosswalk information. 1 We re building a LocalDataManager object. vehicles or pedestrians as oriented 2D boxes. ", "id": "kool777/lyft-level5-eda-training-inference", "size": "23434", "language": "python", "html_url": "https://www.kaggle.com/code/kool777/lyft-level5-eda-training-inference", "git_url": "https://www.kaggle.com/code/kool777/lyft-level5-eda-training-inference", "script": "torch.utils.data animation draw_reference_trajectory display __init__ resnet34 torch ChunkedDataset LyftModel(nn.Module) PrettyTable l5kit.visualization optim PERCEPTION_LABELS torchvision.models.resnet TARGET_POINTS_COLOR l5kit.evaluation DataLoader resnet50 Dict forward draw_trajectory collections seaborn numpy AgentDataset read_gt_csv Back animate_solution write_pred_csv load_config_data l5kit.configs typing Fore nn clear_output matplotlib.pyplot create_chopped_dataset pandas visualize_rgb_image l5kit.dataset build_rasterizer EgoDataset Counter tqdm animate LocalDataManager prettytable l5kit.geometry compute_metrics_csv resnet18 Style colorama matplotlib transform_points l5kit.data l5kit.rasterization IPython.display HTML ", "entities": "(('It', 'testing'), 'return') (('load_state_dict model_state We', 'submission'), 'model_state_dict') (('This', 'demonstration'), '1') (('zarr files', 'flat highly loading'), 'be') (('com c lyft motion vehicles prediction autonomous data', 'traffic agent motion largest data'), 'be') (('cfg Zarr Dataset ChunkedDataset', 'these'), 'need') (('mask', 'mask'), 'provide') (('we', 'L5KIT_DATA_FOLDER env variable'), 'resolve') (('we', 'etc'), 'ped') (('datatype', 'named fields'), 'be') (('labels cyclist only three pedestrians', 'centroid'), 'give') (('which', 'crosswalk lane information'), 'SemanticRasterizer') (('that', 'numpy structured arrays'), '1') (('This', 'dataframe'), 'include') (('agent', '17 classes'), 'speed') (('task', 'self driving vehicles'), 'be') (('Sample data Train neural Visualize', 'AV'), 'be') (('human driver', 'that'), 'utilise') (('scene', 'time'), 'encode') (('structured array', 'features'), 'store') (('I', 'inference'), 'use') (('we', 'columns two new centroid_x'), 'make') (('WEIGHT_FILE kaggle lyft input l5', 'resnet34_300x300_model_state_15000'), '1') (('tl_faces all', 'above detail'), 'return') (('we', 'build_rasterizer method'), 'use') (('Evaluation Metric Negative log likelihoodWe', 'modal multi predictions'), 'calculate') (('face', 'red green yellow'), 'face_id') (('one row', 'together memory'), 'mean') (('draw_reference_trajectory', 'image'), 'draw') (('We', 'next 50 frames'), 'have') (('lyft l5kit tree master l5kit l5kit rasterization package conatin', 'image'), 'com') (('4 fields', 'Data Format above section'), 'describe') (('which', 'cars such cyclists'), 'model') (('subission', 'internet connection'), 'set') (('We', 'raster different objects'), 'visualize') (('that', 'useful training'), 'agents_mask') (('which', 'ChunckedDataset'), 'use') (('BoxRasterizer', 'agents e.'), 'render') (('visualization', 'information additional e.'), 'contain') (('logs', 'motion prediction models'), 'come') (('model', 'time'), 'predict') (('It', 'image plane'), 'contain') (('Extent Distribution Scatterplot Yaw Distribution Velocity we', 'Cars Cyclists such Pedestrians'), 'distributionas') (('We', 'more detailed understanding'), '1') (('1 We', 'L5Kit toolkit https github'), 'require') (('We', 'LocalDataManager object'), 'build') (('We', 'configuration own file'), 'NOTE') (('file zarr which', 'L5Kit https easily github'), 'format') (('we', 'data'), 'need') (('that', 'traffic index ego_translation host light car'), 'point') (('We', 'positive direction'), 'see') (('Backward', 'training notebook submission'), 'get') (('You', 'https here www'), 'get') (('We', 'detail'), 'talk') (('baseline solution', 'dataset'), 'com') (('one pixel', 'real world'), 'com') (('AV green agent', 'cars blue bicycles'), 'see') (('We', 'ahead rasterizer'), 'see') (('distributions', 'ego sensors Ego Translation Distribution Points'), 'rotation') (('predicting', 'self driving car'), 'be') (('now host', 'data'), 'see') (('Most', 'traditional numpy operations'), 'handle') (('that', 'L5KIT_DATA_FOLDER env variable'), 'expect') (('AgentsAn 1 agent', 'other detected object'), 'be') (('object doesn', 'anything'), 'do') (('SatelliteRasterizer', 'satellite map'), 'render') (('1 We', 'AgentDataset class'), 'visualize') (('Now we', 'L5Kit Toolkit https github'), 'look') (('driverless that', 'surroundings'), 'be') (('Both', 'trajectories future offsets'), 'iterate') (('Inference', 'Size Input size 300px 16 history'), 'Trained') (('entry', 'only same scene'), 'describe') (('here I', 'predictions'), 'csv') (('which', 'pose ego time'), 'be') (('host cars', 'time particular intervals'), 'collect') (('You', 'also various other information'), 'check') (('AV', 'more effectively path'), 'want') (('scene datatype stores', 'dataframe'), 'reference') (('red green yellow', 'face'), '1') (('keys', 'variable e.'), 'be') (('predictions', 'which'), 'mask') (('AV', 'straight path'), 'see') (('car', 'it'), 'use') (('network', 'raster'), 'infer') (('We', 'inputs'), 'use') (('It', 'time discretized intervals'), 'consist') (('us', 'this'), 'let') (('utilities', 'final visualisation'), 'use') (('competition', 'company Lyft https www'), '1') (('then we', 'data'), '1') (('notebook', 'demonstration'), 'be') (('agent state', 'position orientation bounds'), 'describe') (('Unknown label', 'agent other three labels'), 'see') (('dataset', 'reserach Prediction Dataset https paperswithcode'), 'include') (('you', 'submission'), 'be') (('dataset', 'traffic other autonomous fleet'), 'include') (('which', 'agents'), 'generate') (('we', 'one extent column'), 'need') (('We', 'L5kit examples'), '1') (('it', 'motion'), 'be') (('Prediction', 'Contents'), '0') (('ego', 'image'), 'show') (('which', 'https self Level 5 driving'), 'com') (('we', 'sensors'), 'be') (('draw_arrowed_line', 'RGB image'), 'draw') (('AV agent', 'motion'), 'be') (('s', 'structured array'), 'let') (('us', 'single array'), 'allow') (('L5Kit', 'numpy structured arrays'), '1') (('frames', 'end index'), 'correspond') (('1 We', 'zarr files'), 'see') (('object', 'Box single class'), 'combine') (('1 We', 'configuration file cfg'), 'get') (('you', 'ground'), '1') (('We', 'data'), 'develop') (('only 4', 'dataset'), 'note') (('zarr file', 'given vehicle'), 'contain') (('how different agents', 'Autonomous environment'), 'require') (('Bx50x2 You', 'more layers'), 'set') (('We', 'linear trend'), 'start') (('We', 'agents other vehicles'), 'see') (('dataset', 'autonomous vehicle'), 'consist') (('that', 'time'), 'capture') (('Lyft competition 2020 dataset', 'arrays scenes frames four structured agents'), 'store') (('agents cars', 'predestrians'), 'be') (('which', 'a101'), 'point') ", "extra": "['annotation', 'test']"}