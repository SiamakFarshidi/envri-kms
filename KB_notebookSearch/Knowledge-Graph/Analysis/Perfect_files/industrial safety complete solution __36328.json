{"name": "industrial safety complete solution ", "full_name": " h1 AIML Capstone Project Industrial Safety and Health Analytics Database h2 Table of Contents h2 1 Overview h3 Domain Industrial safety NLP based Chatbot h3 Context h3 Data Description h2 2 Import the necessary libraries h4 Setting Options h2 3 Data Collection h4 Shape of the data h4 Data type of each attribute h2 Data Collection Summary h2 4 Data Cleansing h4 Remove Unnamed 0 and Rename Data Countries Genre Employee or Third Party columns h4 Check Duplicates h4 Drop Duplicates h4 Check Outliers h4 Check Missing Values h2 Data Cleansing Summary h2 5 Data Pre processing h2 6 EDA Data Analysis and Preparation h4 Variable Identification h4 Univariate Analysis h4 Bivariate Analysis and Hypothesis testing h4 Study Summary Statistics h4 Study Correlation h2 EDA Summary h2 7 NLP Analysis h2 8 NLP Pre processing h4 Get the Length of each line and find the maximum length h4 WordCloud h4 NLP text summary statistics h2 NLP Pre processing Summary h2 9 Feature Engineering h4 Variable Creation Word2Vec Embeddings h4 Variable Creation Glove Word Embeddings h4 Variable Creation TFIDF Features h4 Variable Creation Label Encoding h4 Combine Glove and Encoded Features h4 Combine TFIDF and Encoded Features h4 Sampling Techniques Create Training and Test Set h4 Resampling Techniques Oversample minority class h4 SMOTE Generate synthetic samples upsample smaller class h4 Varible Tansformation Normalization and Scaling h4 Use PCA Extract Principal Components that capture about 95 of the variance in the data h2 10 Design train and test machine learning classifiers h4 Here we can use the DummyClassifier to predict all accident levels just to show how misleading accuracy can be h4 Define MultiClass Logloss h4 Train and test model h4 Train and test all models h4 Model with Hyperparameter Tuning h4 1 Modelling Logistic Regression h4 2 Decision Tree Random Forest Classifier h4 3 Modelling Logistic Regression Oversampling h4 4 Modelling Logistic Regression SMOTE h4 All models Original data h4 All models Oversampling data h4 All models SMOTE data h4 Hyperparameter tuning with original features h4 Bootstrap Sampling RandomForestClassifier h4 Bootstrap Sampling AdaBoostClassifier h4 Bootstrap Sampling XGBClassifier h2 11 Design train and test Neural networks classifiers h4 Get ANN Multiclass Classification Metrics h4 Convert Classification to Numeric problem h4 Multiclass classification Target variable One hot encoded h4 Multiclass classification Target variable One hot encoded with SMOTE data h2 12 Design train and test RNN or LSTM classifiers h4 Architecture h2 13 Conclusion h2 14 Recommendations h2 15 Limitations h2 16 Closing Reflections h2 17 References ", "stargazers_count": 0, "forks_count": 0, "description": "As we know this database comes from one of the biggest industry in Brazil which has four climatological seasos as below. Building a Linear Regression model Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Building a Random Forest Classifier on Training set Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Building a Linear Regression model Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Building a Linear Regression model Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Train and Test all models with Lasso interaction terms Train and Test all models with Lasso interaction terms Train and Test all models with Lasso interaction terms define regressor models define model parameters penalty none l1 l2 elasticnet class_weight none balanced multi_class ovr multinomial class_weight balanced balanced_subsample none class_weight balanced balanced_subsample none Considering all Predictors Consider only top 30 GLOVE features Number of bootstrap samples to create size of a bootstrap sample run bootstrap empty list that will hold the scores for each bootstrap iteration prepare train and test sets Sampling with replacement picking rest of the data not considered in sample fit model fit against independent variables and corresponding target values Take the target column for all rows in test set evaluate model predict based on independent variables in the test data plot scores confidence intervals for 95 confidence tail regions on right and left. 57 which is a bad result. Another national holidays are election days. 1 n_estimators 30 95 Confidence interval range 0. Decision Tree Random Forest Classifier While in every machine learning problem it s a good rule of thumb to try a variety of algorithms it can be especially beneficial with imbalanced datasets. Columns description Data timestamp or time date information Countries which country the accident occurred anonymised Local the city where the manufacturing plant is located anonymised Industry sector which sector the plant belongs to Accident level from I to VI it registers how severe was the accident I means not severe but VI means very severe Potential Accident Level Depending on the Accident Level the database also registers how severe the accident could have been due to other factorsinvolved in the accident Genre if the person is male of female Employee or Third Party if the injured person is an employee or a third party Critical Risk some description of the risk involved in the accident Description Detailed description of how the accident happened. Bootstrap sampling with AdaBoostClassifier model with an accuracy of 51. 5 bagging_frequency 8 boosting_type dart early_stopping_rounds 200 feature_fraction 0. Limitations We have less number of observations to analyse the cause of accidents correctly and rather we should collect more number of observations to get better results. one hot encoded Transform independent features Scaling only first 6 feautres Scaling only first 6 feautres generating the covariance matrix and the eigen values for the PCA analysis the relevanat covariance matrix generating the eigen values and the eigen vectors the cumulative variance explained analysis Plotting the variance expalained by the principal components and the cumulative variance explained. 25 on each side indicated by P value border disable keras warnings get the accuracy precision recall f1 score from model predict probabilities for test set Multiclass predict crisp classes for test set Multiclass Multilabel reduce to 1d array accuracy tp tn p n precision tp tp fp recall tp tp fn f1 2 tp 2 tp fp fn Multiclass Multilabel fix random seed for reproducibility define the model compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves fix random seed for reproducibility define the model Multilabel compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format fix random seed for reproducibility define the model Multilabel compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format Select input and output features Encode labels in column Accident Level. Accident Levels by Seasons Is the distribution of accident levels and potential accident levels differ significantly in different seasons Observations Both of the two accident level have the tendency that non severe levels decreased throughout the year but severe levels did not changed much and some of these levels increased slightly in the second half of the year. Bootstrap sampling with RandomForestClassifier model with an accuracy of 66. 6906 Total duration 51. CatBoostClassifier Best F1_Score 0. I will build the real chatbot using Streamlit or some other applications. Target variable Accident Level distribution is not equal I 309 II 40 III 31 IV 30 V 8. But let s check is that difference is statistically significant 1. AIML Capstone Project Industrial Safety and Health Analytics Database Table of Contents1. Proportion of third party remote employees in each gender is not equal. define grid search summarize results note the end time calculate the total duration For multiclass problems only newton cg sag saga and lbfgs handle multinomial loss liblinear is limited to one versus rest schemes. Accident Levels by Month Is the distribution of accident levels and potential accident levels differ significantly in different months Observations Both of the two accident level have the tendency that non severe levels decreased throughout the year but severe levels did not changed much and some of these levels increased slightly in the second half of the year. 1 n_estimators 30 0. Import the necessary libraries import libraries 3. But let s check the proportion of metals mining and others sector in Country_01 and is that difference is statistically significant 1. Accident Levels by Weekday Is the distribution of accident levels and potential accident levels differ significantly in different weekday Observations Both of the two accident level is thought that non severe levels decreased in the first and the last of the week but severe levels did not changed much. 81 with original data. Table of Contents table of contents 6. 0 gamma scale kernel rbf 95 Confidence interval range 0. NLP Pre processingFew of the NLP pre processing steps taken before applying model on the data Converting to lower case avoid any capital cases Converting apostrophe to the standard lexicons Removing punctuations Lemmatization Removing stop words Get the Length of each line and find the maximum lengthAs different lines are of different length. 6890 Total duration 305. 81 and f1 score of 72. Class imbalance issue is handled using below methods and found out that for this particular dataset with original data we have achieved the better results. Note Considering official holidays only. Country Percentage of accidents occurred in respective countries 59 in Country_01 31 in Country_02 and 10 in Country_03. Table of Contents table of contents 7. 38 with original data. x in colab Setting Options Table of Contents table of contents 3. 32 manufacturing plants belongs to Metals sector. 028932 with max_features sqrt n_estimators 1000 95 Confidence interval range 0. Proportion of own employees in each gender is not equal. We have no outliers in this dataset. io api Streamlit API https docs. Both of the two accident level have the tendency that non severe levels decreased throughout the year but severe levels did not changed much and some of these levels increased slightly in the second half of the year. Finally the output dense layer will have five neuorns corresponding to each accident level. 6831 Total duration 12. Create a model with Text inputs accident description alone only surprisingly achieved a test accuracy of 73. 6625 Total duration 2. References Good EDA Notebook https www. GradientBoostingClassifier Best F1_Score 0. Above one is underfit model it can be identified from the learning curve of the training loss only. State the H0 and Ha Ho The proportions of own employees in each gender is equal. Design train and test RNN or LSTM classifiers nlp models Creating a Model with Text Inputs Only nlp models text input Creating a Model with Categorical features Only nlp models cat features Creating a Model with Multiple Inputs nlp models multiple input 13. Create a model with Multiple inputs. 6831 Total duration 428. Employee type 44 Third party empoyees 43 own empoyees and 13 Third party Remote empoyees working in this industry. Both of the two accident level is thought that non severe levels decreased in the first and the last of the week but severe levels did not changed much. They work by learning a hierarchy of if else questions and this can force both classes to be addressed. Closing Reflections closing reflections 17. Categorical columns Countries Local Industry Sector Accident Level Potential Accident Level Genre Employee or Third Party Critical Risk Description Date column Data Data Collection Summary 1. Define MultiClass Logloss Train and test model Train and test all models Model with Hyperparameter Tuning 1. Data Collection Shape of the data Data type of each attribute From the above output we see that except first column all other columns datatype is object. Gender Industry Sector Proportion of Metals Mining and Others sector employees in each gender is not equal Gender Accident Levels Males have a higher accident levels than females. 23 of data where accident description 100 is captured in low potential accident level. We need to load the built in GloVe word embeddings Build a LSTM Neural Network LSTM_Layer_1 LSTM 128 embedding_layer dense_layer_1 Dense 5 activation softmax LSTM_Layer_1 model Model inputs deep_inputs outputs dense_layer_1 Use earlystopping callback tf. There are quite a lot of Critical risk descriptions and we don t see any outliers but with the help of SME we can decide whether this column has outliers or not. Accident Levels by Gender Is the distribution of accident levels and potential accident levels differ significantly in different genders Observations Proportion of accident levels in each gender is not equal and males have a higher accident levels than females. 616087 using alpha 0. 0001 max_depth 10 metric multi_logloss min_child_samples 486 min_child_weight 0. There are a plenty of unofficial ethnic and religious holidays in Brazil. Creating a Model with Categorical features OnlyIn this section we will create a classification model that uses categorical columns alone. KNeighborsClassifier Best F1_Score 0. This submodel will consist of an input shape layer an embedding layer and bidirectional LSTM layer of 128 neurons followed by max pool layer drop out and dense layers. Create a model with Multiple Inputs concatenated the layers from text input model and categorical features input model surprisingly achieved a test accuracy of 73. 89 with accident description alone. 01 min_data_in_leaf 90 n_estimators 1000 num_class 5 num_leaves 1550 objective multiclass verbosity 1 0. Distribution of industry sector differ significantly in each country. org stable Keras API https keras. 028086 with max_features log2 min_samples_split 13 n_estimators 90 95 Confidence interval range 0. We have seven duplicate values in this dataset and dropped those duplicate values. Sometimes they also die in such environment. 6844 Total duration 688. There are many low risks at general accident level but many high risks at potential accident level. Creating a Model with Text Inputs OnlyIn this section we will create a classification model that uses accident description column alone. We have no missing values in this dataset. There are only five Accident Level types which are in sequence so there are no outliers in Accident Level column. 622235 using max_features sqrt n_estimators 1000 0. Identify the test statistic Z test of proportions 4. With more number of observations than current number of records 425 so that we can feed more data into ML ANN NLP models to train evaluate the performance of those models and get the better results. Decide to Reject or Accept Null HypothesisHence we fail to reject Null Hypothesis we have enough 95 evidence to prove that the proportion of own employees in each gender is equal. Above one is good fit it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values. Country Industry Sector Metals and Mining industry sector plants are not available in Country_03. org user_guide Customizing_Plots. And it is also thought that it takes so much time to analyze risks and reasons why the accidents occur. Table of Contents table of contents 12. Critical Risk Because most part of the Critical Risks are classified as Others it is thought that there are too many risks to classify precisely. Number of accidents increased during the middle of the week and declined since the middle of th week. Extracted the day month and year from Date column and created new features such as weekday weekofyear and seasons. 001 fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format Select input and output features Encode labels in column Accident Level. com koheimuramatsu industrial accident causal analysis Holoviews plot tips http holoviews. Accident Levels Employee type For both accident levels the incidence of Employee is higher at low accident levels but the incidence of Third parties seems to be slightly higher at high accident levels. 026945 with C 1. Table of Contents table of contents 17. There are few alphanumeric characters like AFO 755 captured in description where removing these characters might help. Data Cleansing Remove Unnamed 0 and Rename Data Countries Genre Employee or Third Party columns Check Duplicates There is no need to worry about preserving the data it is already a part of the industry dataset and we can merely remove or drop these rows from your cleaned data Drop Duplicates Check OutliersAs we know there is no concept of outliers detection in categorical variables nominal and ordinal as each value is count as labels. Table of Contents table of contents Bivariate Analysis and Hypothesis testing a. com ihmstefanini industrial safety and health analytics database Table of Contents table of contents 2. 13 Third party Remote empoyees working in this industry. How to build different ANN model architectures for handling multi class classification problems. This is accomplished using the make_scorer function of scikit learn. 1 penalty l1 solver liblinear 0. With more detailed information such as machining data ex. 030167 with learning_rate 0. So there are no outliers in the Date column. csv file Get the top 5 rows Check datatypes Check Data frame info Column names of Data frame Remove Unnamed 0 column from Data frame Rename Data Countries Genre Employee or Third Party columns in Data frame Get the top 2 rows Check duplicates in a data frame View the duplicate records Delete duplicate rows Get the shape of Industry data Check unique values of all columns except Description column Check the presence of missing values Visualize missing values function to create month variable into seasons Check the proportion of Industry sector in different countries Z test proportions More than 2 samples not implemented yet hence I am passing two elements Summary statistics Check the Correlation Checking 5 random Description and accident_levels from the data Checking 5 random Descriptions and accident_levels from the data where the length of headline is 100 Checking 5 random Descriptions and pot_accident_levels from the data where the length of headline is 100 Text preprocessing and stopwords custom module Get length of each line Get length of each line define training data train model summarize the loaded model summarize vocabulary save model load model this function creates a normalized vector for the whole sentence create sentence GLOVE embeddings vectors using the above function for training and validation set To replace white space everywhere in Employee type To replace white space everywhere in Critical Risk Create Industry DataFrame Label encoding convert integers to dummy variables i. 5182700157166 Table of Contents table of contents Bootstrap Sampling RandomForestClassifier Bootstrap Sampling AdaBoostClassifier Bootstrap Sampling XGBClassifier Table of Contents table of contents 11. Resampling techniques Oversampling minority class b. py Heroku with Python https devcenter. 630245 using learning_rate 0. 034210 with max_samples 0. We could see it accuracy continually rise during training. Capturing 90 variance of the data DummyClassifier to predict all Accident levels checking unique labels checking accuracy Checking unique values Convert actual to a binary array if it s not already Fit the model on Training set Fit the model on Training set Intercept and Coefficients Predict on Test set Initialise mc_logloss Model Confusion matrix Model Classification report Store the accuracy results for each model in a dataframe for final comparison Save the model return all the metrics along with predictions define classification models early_stopping_rounds 30 early_stopping_rounds 30 Train and Test the model Store the accuracy results for each model in a dataframe for final comparison note the start time Before starting with grid search we need to create a scoring function. 50 cm deep 30 kg where removing the digits wouldn t help. Gender There are more men working in this industry as compared to women. Table of Contents table of contents 2. Conclusion conclusion 14. Based on some random headlines seen above it appears that the data is mostly lower cased. How to build different NLP architectures for handling both numerical and text data. NLP Pre processing nlp pre processing Word Cloud wordcloud 9. Data Pre processing To better understand the data I am extracting the day month and year from Date column and creating new features such as weekday weekofyear. EDA Data Analysis and Preparation eda Univariate Analysis univariate analysis Bivariate Analysis and Hypothesis testing bivariate analysis 7. We can create holidays variable based on Brazil holidays list from 2016 and 2017. BaggingClassifier Best F1_Score 0. Multiclass classification Target variable One hot encodedIn this section we will create a classification model that uses categorical columns and tf idf features from accident description and one hot encoded target variable. 25 on each side indicated by P value border Number of bootstrap samples to create size of a bootstrap sample run bootstrap empty list that will hold the scores for each bootstrap iteration prepare train and test sets Sampling with replacement picking rest of the data not considered in sample fit model fit against independent variables and corresponding target values Take the target column for all rows in test set evaluate model predict based on independent variables in the test data plot scores confidence intervals for 95 confidence tail regions on right and left. Design train and test RNN or LSTM classifiers Architecture1. Overview overview 2. We observed that there are records of accidents from 1st Jan 2016 to 9th July 2017 in every month. one hot encoded Dummy variables encoding Merge the above dataframe with the original dataframe ind_feat_df Check NaN values Consider only top 30 GLOVE features Consider only top 30 GLOVE features Considering all Predictors Display old accident level counts Concatenate our training data back together Get the majority and minority class Upsample Level1 minority class sample with replacement to match majority class Upsample Level2 minority class sample with replacement to match majority class Upsample Level3 minority class sample with replacement to match majority class Upsample Level4 minority class sample with replacement to match majority class Combine majority class with upsampled minority classes Display new accident level counts Separate input features and target Considering all Predictors Separate input features and target Considering all Predictors Display new accident level counts convert integers to dummy variables i. Accident Levels by Employee type Is the distribution of accident levels and potential accident levels differ significantly in different employee types Observations For both accident levels the incidence of Employee is higher at low accident levels but the incidence of Third parties seems to be slightly higher at high accident levels. Design train and test machine learning classifiers Here we can use the DummyClassifier to predict all accident levels just to show how misleading accuracy can be. Able to predict the accident level with a test accuracy of 73. All models SMOTE dataBy comparing the results from all above methods all are over fitting the training data. Accident Levels The number of accidents decreases as the Accident Level increases and increases as the Potential Accident Level increases. We can use simple densely connected neural networks to make predictions. We had 7 duplicate instances in the dataset and dropped those duplicates. Sentences can have different lengths and therefore the sequences returned by the Tokenizer class also consist of variable lengths. 38 All models Oversampling dataBy comparing the results from all above methods we can select best method as Ridge classifier with f1 score 62. Accident Levels The number of accidents decreases as the Accident Level increases. Data Collection data collection 4. Lack of access to quality data. Removed Unnamed 0 column and renamed Data Countries Genre Employee or Third Party columns in the dataset. Since the data for these columns is well structured and doesn t contain any sequential or spatial pattern we can use simple densely connected neural networks to make predictions. 1 n_estimators 70 0. Body related left right hand finger face foot and glove Employee related employee operator collaborator assistant worker and mechanic Movement related fall hit lift and slip Equipment related equipment pump meter drill truck and tube Accident related accident activity safety injury causing NLP text summary statistics NLP Pre processing Summary 74 of data where accident description 100 is captured in low accident level. 34 of data where accident description 100 is captured in high medium potential accident level. Number of accidents are high in beginning of the year and it keeps decreasing later. Design train and test machine learning classifiers ml models All models Original data ml models original data All models Oversampling data ml models oversampling data All models SMOTE data ml models smote data Hyperparameter tuning with original features ml models hyperparam tuning Bootstrap Sampling ml models bootstrap sampling 11. 629247 using metric euclidean n_neighbors 13 weights uniform 0. 026945 with C 0. 25 of data where accident description 100 is captured in medium potential accident level. Divide our data into testing and training sets Convert both the training and test labels into one hot encoded vectors The first step in word embeddings is to convert the words into thier corresponding numeric indexes. 75 n_estimators 10 0. Table of Contents table of contents 3. Number of accidents are very high in particular days like 4 8 and 16 in every month. Study Summary Statistics Study Correlation Observations WeekofYear featuer is having very high positive correlation with Month feature. Table of Contents table of contents 4. Ha The proportions of own employees in each gender is not equal. 628276 using max_features log2 min_samples_split 13 n_estimators 90 0. Ridge Best F1_Score 0. It is an urgent need for industries companies around theglobe to understand why employees still suffer some injuries accidents in plants. Hyperparameter tuning with original features1. EDA Summary Local Highest manufacturing plants are located in Local_03 city and lowest in Local_09 city. RandomForestClassifier Best F1_Score 0. No missing values in dataset. 023211 with alpha 0. com Brazil Spring September to November Summer December to February Autumn March to May Winter June to AugustWe can create seasonal variable based on month variable. Accident Levels Calendar Accidents are recorded from 1st Jan 2016 to 9th July 2017 in every month there are high number of accidents in 2016 and less in 2017. We need to pad the our sequences using the max length. Lowest manufacturing plants are located in Local_09 city. Industry Sector by Countries Is the distribution of industry sector different significantly in differ countries or not Observations Metals and Mining industry sector plants are not available in Country_03. There are about 425 rows and 11 columns in the dataset. There are no outliers in the dataset. Data Pre processing data preprocessing 6. 68 and all other methods are over fitting the training data. 620604 using max_samples 0. There are only three Industry Sector types which are in sequence so there are no outliers in Industry Sector column. 28 with original data. What can you do to enhance the solution Need to work on limitations. NLP Analysis nlp analysis 8. 613227 using learning_rate 0. Closing Reflections What did we learned from the process How to work on Data Science project to end to end. Observations 34 of data where accident description 100 is captured in high medium potential accident level. Table of Contents table of contents 14. What I do differently next time Perhaps I will explore more feature engineering and feature selection techniques. EarlyStopping monitor loss patience 5 min_delta 0. Recommendations recommendations 15. 6831 Total duration 2430. 11 manufacturing plants belongs to Others sector. I realized that the detail information of accidents like Description is so useful to analyze the cause. Data Description This The database is basically records of accidents from 12 different plants in 03 different countries which every line in the data is anoccurrence of an accident. Table of Contents table of contents 8. State the H0 and Ha Ho The proportions of industry sector is not differ in different countries Ha The proportions of industry sector is differ in different countries 2. Recommendations In this project we discovered that the main causes of accidents are mistakes in hand operation and time related factor. Create a model with Text inputs only. Modelling Logistic Regression 2. 03 95 Confidence interval range 0. Creating a Model with Multiple InputsThe first submodel will accept textual input in the form of accident description. Let s check the unique and frequency mode of each variable. 629247 using depth 4 iterations 300 l2_leaf_reg 4 learning_rate 0. Multiclass classification Target variable One hot encoded achieved a test accuracy of 73. com articles getting started with python used to supress display of warnings nlp libraries sampling methods import zscore for scaling the data save models pre processing methods the classification models ensemble models methods and classes for evaluation cross validation methods feature selection methods pre processing methods Deep learning libraries Keras pre processing Reproduce the results suppress display of warnings display all dataframe columns to set the limit to 3 decimals display all dataframe rows Read IHMStefanini_industrial_safety_and_health_database_with_accidents_description. Above one is overfit model it can be identified from the learning curve of the training and validation loss only. Decide to Reject or Accept Null Hypothesis Hence we reject Null Hypothesis we have enough 95 evidence to prove that the mining sector in country 1 is differ from metals sector Hence we reject Null Hypothesis we have enough 95 evidence to prove that the mining sector in country 1 is differ from others sector b. By comparing the results from all ML methods with original data we can select the best method as AdaBoost classifier with f1 score 65. To reduce the occurrences of accidents more stringent safety standards in hand operation will be needed in period when many accidents occur. 026945 with metric euclidean n_neighbors 13 weights uniform 95 Confidence interval range 0. 033099 with learning_rate 0. 1 n_estimators 70 95 Confidence interval range 0. 75 n_estimators 10 95 Confidence interval range 0. Where does our model fall short in the real world Once we deploy the finalised model in Production we might get less f1 score as compared to productionalized model results. Design train and test Neural networks classifiers Get ANN Multiclass Classification Metrics Convert Classification to Numeric problemIn this section we will create a classification model that uses categorical columns and tf idf features from accident description and label encoded target variable. How to handle class imbalance data set. Octoberfest Brazilian Carnival Kinderfest Fenaostra Fenachopp Musikfest Schutzenfest Kegelfest Cavalhadas Oberlandfest Tirolerfest Marejada are among them. The loss of the model will almost always be lower on the training dataset than the validation dataset. There are 12 Local cities where manufacturing plant is located and it s types are in sequence so there are no outliers in Local column. Convert Classification to Numerical problem achieved a test accuracy of 53. SMOTE Generate synthetic samples8. Context The database comes from one of the biggest industry in Brazil and in the world. The second submodel will accept input in the form of meta information which consists of dense batch norm and drop out layers. 026945 with bagging_fraction 0. Logistic Regression Best F1_Score 0. Finally bidirectional LSTM model can be considered to productionalized the model and predict the accident level. level 326 Dumper 01 where removing the digits wouldn t help. Feature Engineering feature engineering 10. 81 and f1 score of 73. 6831 Total duration 7. 1 penalty l1 solver liblinear 95 Confidence interval range 0. Feature Engineering Variable Creation Word2Vec Embeddings Variable Creation Glove Word Embeddings Variable Creation TFIDF Features Variable Creation Label Encoding Combine Glove and Encoded Features Combine TFIDF and Encoded Features Sampling Techniques Create Training and Test Set Resampling Techniques Oversample minority class SMOTE Generate synthetic samples upsample smaller class Varible Tansformation Normalization and Scaling Use PCA Extract Principal Components that capture about 95 of the variance in the data Table of Contents table of contents 10. Employee type 44 Third party empoyees working in this industry. 3370 CX 212 captured in description where removing these characters might help. Proportion of Mining sector employees in each gender is not equal. Table of Contents table of contents 13. Link to download the dataset https www. ExtraTreesClassifier Best F1_Score 0. References references 1. html Scikit Learn API https scikit learn. We noticed that except a date column all other columns are categorical columns. 02 95 Confidence interval range 0. EDA Data Analysis and Preparation Variable Identification Target variable Accident Level Potential Accident Level Predictors Input varibles Date Country Local Industry Sector Gender Employee type Critical Risk Description Univariate Analysis Country 59 accidents occurred in Country_01 31 accidents occurred in Country_02 10 accidents occurred in Country_03 Local Highest manufacturing plants are located in Local_03 city. There are few alphanumeric characters like 042 TC 06 Nv. There are only three Employee types in the provided data so there are no outliers in Gender column. 0 gamma scale kernel rbf 0. 629247 using bagging_fraction 0. Explored below options in Neural Networks. CNC Current Voltage in plants weather information employee s personal data ex. Pre processing such as removing punctuations and lemmatization can be used. SVC Best F1_Score 0. age experience in the industry sector work performance we can clarify the cause of accidents more correctly. Proportion of Others sector employees in each gender is not equal. Modelling Logistic Regression Oversampling 4. Few of the NLP pre processing steps taken before applying model on the data Converting to lower case avoid any capital cases Converting apostrophe to the standard lexicons Removing punctuations Lemmatization Removing stop words After pre processing steps Minimum line length 64 Maximum line length 672 Minimum number of words 10 Maximum number of words 98 Table of Contents table of contents 9. 026945 with depth 4 iterations 300 l2_leaf_reg 4 learning_rate 0. It is showing noisy values of relatively high loss indicating that the model was unable to learn the training dataset at all and model does not have a suitable capacity for the complexity of the dataset. We are left with 418 rows and 10 columns after data cleansing. The number of accidents increases as the Potential Accident Level increases. As expected we see the learning curves for accuracy on the test dataset plateau indicating that the model has no longer overfit the training dataset and it is generalized model. There are only two Gender types in the provided data so there are no outliers in Gender column. 43 own empoyees working in this industry. LGBMClassifier Best F1_Score 0. 6801 Total duration 176. Create a model with Categorical inputs only. 81 with original data TF IDF features from accident description column. The output from the dropout layer of the first submodel and the output from the batch norm layer of the second submodel will be concatenated together and will be used as concatenated input to another dense layer with 10 neurons. Modelling Logistic Regression SMOTE Table of Contents table of contents All models Original dataBy comparing the results from all above methods we can select the best method as AdaBoost classifier with f1 score 65. There are only three country types so there are no outliers in Country column. There are only six Potential Accident Level types which are in sequence so there are no outliers in Potential Accident Level column. Industry Sector Percentage of manufacturing plants belongs to respective sectors 57 to Mining sector 32 to Metals sector and 11 to Others sector. Multiclass classification Target variable One hot encoded with SMOTE dataIn this section we will create a classification model that uses categorical columns and tf idf features from accident description and one hot encoded target variable. 6794 Total duration 165. Data Cleansing data cleansing 5. Note Surprisingly we observe that same f1 score 73. Critical Risk Most of the critical risks are classified as Others. Calculate the p_value using test statistic 5. 8 learning_rate 0. Decision trees frequently perform well on imbalanced data. Bootstrap sampling with XGBClassifier model with an accuracy of 63. Calendar Accidents are recorded from 1st Jan 2016 to 9th July 2017 in every month there are high number of accidents in 2016 and less in 2017. Check Missing Values Data Cleansing Summary 1. Create a model with Categorical features only achieved a test accuracy of 73. io daniellewisdl streamlit cheat sheet app. Industry Sector 57 manufacturing plants belongs to Mining sector. AdaBoostClassifier Best F1_Score 0. 01 min_data_in_leaf 90 n_estimators 1000 num_class 5 num_leaves 1550 objective multiclass verbosity 1 95 Confidence interval range 0. Decide the significance level alpha 0. Industry Sector by Gender Is the distribution of industry sector differ significantly in different genders Observations Proportion of Metals sector employees in each gender is not equal. Employee type by Gender Is the distribution of employee type differ significantly in different genders Observations Proportion of third party employees in each gender is equal. Less number of features available in dataset. Design train and test Neural networks classifiers ann models Convert Classification to Numeric problem ann models clas to num problem Multiclass classification Target variable One hot encoded ann models multi class 12. Table of Contents table of contents 5. There are quite a lot of Critical risk descriptions but with the help of SME we can decide whether this column has outliers or not and also SME can help us in understanding the data better. Gender Employee type Proportion of third party employees in each gender is equal third party remote employees in each gender is not equal and own employees in each gender is not equal. 6831 Total duration 6367. Divide our data into testing and training sets Convert both the training and test labels into one hot encoded vectors Variable transformation using StandardScaler Scaling only first 6 feautres Scaling only first 6 feautres fix random seed for reproducibility compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format fix random seed for reproducibility compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves. There are digits in the description for e. Overview Domain Industrial safety. WordCloud Observations There are many body related employee related movement related equipment related and accident related words. Since we have ordinal relationship between each category in target variable I have considered this one as numerical regression problem and try to observe the ANN behaviour. Table of Contents table of contents 15. NLP Analysis Observations 74 of data where accident description 100 is captured in low accident level. Import the necessary librariesFirstly let s select TensorFlow version 2. Limitations limitations 16. Since we are predicting the accident level we need to be 100 sure or at least close to 100 so that we can prevent the lot of accidents in industry. ", "id": "vinayakshanawad/industrial-safety-complete-solution", "size": "36328", "language": "python", "html_url": "https://www.kaggle.com/code/vinayakshanawad/industrial-safety-complete-solution", "git_url": "https://www.kaggle.com/code/vinayakshanawad/industrial-safety-complete-solution", "script": "Flatten RidgeClassifier sklearn.metrics makedirs PCA Concatenate Embedding month2seasons np_utils Dropout DummyClassifier EarlyStopping tensorflow.keras.models Lasso DecisionTreeClassifier SelectKBest SMOTE optimizers opts ReduceLROnPlateau sklearn.decomposition keras.constraints Tokenizer sklearn.model_selection f1_score confusion_matrix KFold Activation os classification_report LogisticRegression LGBMClassifier tensorflow.keras load_model holoviews missingno sklearn.svm keras.wrappers.scikit_learn train_test_split Metrics(tf.keras.callbacks.Callback) statsmodels.stats.proportion cross_val_score sklearn.naive_bayes l1_l2 sklearn.discriminant_analysis tensorflow.keras.layers StackingClassifier multiclass_logloss train_test_allmodels maxnorm TfidfVectorizer KNeighborsClassifier recall_score on_epoch_end seaborn numpy sklearn.pipeline gensim.models scipy.stats Input ModelCheckpoint metrics LabelEncoder LinearDiscriminantAnalysis tensorflow keras.utils pandas word_tokenize Pipeline keras.optimizers unit_norm BatchNormalization nltk.corpus GridSearchCV Model sklearn.linear_model sklearn.dummy pad_sequences keras.models RFECV get_classification_metrics stopwords keras.callbacks model_from_json BaggingClassifier l1 SGD sent2vec sklearn.feature_extraction.text Word2Vec CatBoostClassifier XGBClassifier ExtraTreesClassifier RFE l2 GlobalMaxPool1D accuracy_score _render sklearn.feature_selection text_preprocess_py KerasClassifier on_train_begin roc_auc_score * #(custom module) GaussianNB zscore sklearn.utils randint as sp_randint randint __init__ resample Bidirectional keras.regularizers sklearn.tree keras.layers lightgbm hyperparameterstune_model AdaBoostClassifier Sequential tensorflow.keras.preprocessing.sequence precision_score LSTM proportions_ztest reset_random_seeds tensorflow.keras.preprocessing.text RepeatedStratifiedKFold train_test_model GradientBoostingClassifier sklearn.ensemble sklearn RandomForestClassifier imblearn.over_sampling matplotlib.pyplot f_classif Dense StandardScaler tqdm min_max_norm PowerTransformer TruncatedSVD plot_model nltk.tokenize sklearn.neighbors SVC StratifiedKFold catboost RandomizedSearchCV sklearn.preprocessing xgboost mutual_info_classif keras.layers.merge ", "entities": "(('model', 'dataset'), 'show') (('425 we', 'better results'), 'feed') (('sector employees', 'gender'), 'be') (('I', 'weekday such weekofyear'), 'Pre') (('EarlyStopping', 'loss patience'), 'monitor') (('models', 'ml models Bootstrap Sampling ml models sampling'), 'model') (('also SME', 'data'), 'be') (('it', 'Others'), 'risk') (('manufacturing Lowest plants', 'city'), 'locate') (('this', 'classes'), 'work') (('Decision trees', 'frequently well imbalanced data'), 'perform') (('activation softmax LSTM_Layer_1 model Model LSTM 128 embedding_layer Dense 5 deep_inputs', 'callback'), 'need') (('accident where description', 'accident 100 low potential level'), '23') (('NLP processing nlp', 'Word Cloud'), 'Pre') (('severe levels', 'week'), 'be') (('we', 'end'), 'reflection') (('maximum lengthAs different lines', 'different length'), 'Pre') (('train', 'right'), 'build') (('Distribution', 'significantly country'), 'differ') (('I', 'Streamlit'), 'build') (('Accident Level only five which', 'Accident Level so column'), 'be') (('types', 'Local column'), 'be') (('output', '10 neurons'), 'concatenate') (('Pre processing', 'such punctuations'), 'use') (('we', 'better results'), 'handle') (('EDA Summary Local manufacturing Highest plants', 'city'), 'locate') (('Context database', 'world'), 'come') (('euclidean metric n_neighbors 13 weights', 'Confidence interval 95 range'), 'uniform') (('detail information', 'so cause'), 'realize') (('We', 'dataset'), 'have') (('much some', 'year'), 'have') (('Target variable Accident Level distribution', 'I'), 'be') (('we', 'industry'), 'predict') (('dataBy', 'training data'), 'be') (('31 accidents', 'Local_03 city'), 'varible') (('Multiclass classification Target variable', '73'), 'achieve') (('h5 format Select input', 'column Accident Level'), 'fit') (('manufacturing 11 plants', 'Others sector'), 'belong') (('Finally output dense layer', 'accident level'), 'have') (('that', 'target variable'), 'get') (('manufacturing 32 plants', 'Metals sector'), 'belong') (('accident where description', 'accident 100 high medium potential level'), 'observation') (('Industry manufacturing Sector 57 plants', 'Mining sector'), 'belong') (('line', 'accident'), 'description') (('that', 'accident tf idf description'), 'variable') (('Country Industry industry sector Sector Metals plants', 'Country_03'), 'be') (('incidence', 'accident slightly high levels'), 'be') (('we', 'model productionalized results'), 'fall') (('Summer December', 'month variable'), 'com') (('number', 'Potential Accident Level increases'), 'increase') (('data', 'it'), 'appear') (('equal employees', 'gender'), 'be') (('correctly rather we', 'better results'), 'have') (('overfit it', 'training loss'), 'be') (('results suppress display', 'rows Read dataframe IHMStefanini_industrial_safety_and_health_database_with_accidents_description'), 'article') (('where removing', 'characters'), '3370') (('Country Percentage', 'Country_03'), 'occur') (('h5 format Select input', 'column Accident Level'), 'get') (('Industry Sector Percentage', 'Others sector'), 'belong') (('sentence GLOVE', 'variables dummy i.'), 'get') (('why employees', 'plants'), 'be') (('librariesFirstly s', 'TensorFlow version'), 'let') (('LSTM Finally bidirectional model', 'accident level'), 'consider') (('mining sector', 'sector b.'), 'decide') (('Proportion', 'gender'), 'be') (('accident level new counts', 'variables dummy i.'), 'consider') (('accident where description', 'accident 100 low level'), 'leave') (('it', 'training longer dataset'), 'expect') (('which', 'layers'), 'accept') (('main causes', 'hand operation'), 'recommendation') (('just how misleading accuracy', 'accident levels'), 'classifier') (('severe levels', 'week'), 'think') (('we', 'AdaBoost f1 score'), 'by') (('loss', 'validation dataset'), 'be') (('that', 'accident tf idf description'), 'encode') (('total duration', 'rest schemes'), 'note') (('we', 'accidents'), 'experience') (('accident where description', 'accident 100 high medium potential level'), '34') (('we', 'predictions'), 'be') (('proportions', 'gender'), 'be') (('Most', 'Others'), 'classify') (('accident where description', 'accident 100 low level'), 'observation') (('you', 'limitations'), 'do') (('underfit it', 'training loss'), 'be') (('Convert Classification', '53'), 'achieve') (('Observations Proportion', 'gender'), 'be') (('We', '2017 month'), 'observe') (('Accident number', 'Accident Level increases'), 'level') (('max_features', '1000 Confidence interval n_estimators 95 range'), 'sqrt') (('that', 'accident description column'), 'create') (('proportions', 'different countries'), 'state') (('which', 'four climatological seasos'), 'come') (('Ha proportions', 'gender'), 'state') (('industry sector Observations Metals plants', 'Country_03'), 'be') (('other columns', 'date column'), 'notice') (('we', 'AdaBoost f1 score'), 'table') (('Calendar Accidents', '2017'), 'record') (('Gender Accident Levels Males', 'females'), 'be') (('difference', 'Country_01'), 'let') (('43 own empoyees', 'industry'), 'work') (('I', 'ANN behaviour'), 'consider') (('Number', 'month'), 'be') (('train', 'right'), '25') (('males', 'females'), 'be') (('cumulative variance', 'principal components'), 'feature') (('nominal value', 'labels'), 'Remove') (('columns other datatype', 'first column'), 'Shape') (('accident where description', 'accident 100 medium potential level'), '25') (('column', 'outliers'), 'be') (('where removing', 'characters'), 'be') (('first step', 'thier corresponding numeric indexes'), 'be') (('Octoberfest Brazilian Carnival Kinderfest Fenaostra Fenachopp Musikfest Schutzenfest Kegelfest Cavalhadas Oberlandfest Tirolerfest Marejada', 'them'), 'be') (('We', 'duplicate values'), 'have') (('incidence', 'accident slightly high levels'), 'type') (('that', 'categorical columns'), 'create') (('Multiple InputsThe first submodel', 'accident description'), 'accept') (('We', 'predictions'), 'use') (('Accident number', 'Potential Accident Level increases'), 'level') (('that', 'loss two final values'), 'be') (('Number', 'th week'), 'increase') (('other methods', 'training data'), 'be') (('proportion', 'gender'), 'fail') (('number', 'curves'), 'divide') (('Detailed how accident', 'accident'), 'description') (('much some', 'year'), 'be') (('submodel', 'layers'), 'consist') (('Industry Sector only three which', 'Industry Sector so column'), 'be') (('it', 'especially imbalanced datasets'), 'Classifier') (('Create', '73'), 'concatenate') (('Accident Level only six Potential which', 'Potential Accident Level so column'), 'be') (('data TF original IDF', 'accident description column'), 'feature') (('This', 'scikit'), 'accomplish') (('differently next time Perhaps I', 'feature more feature engineering techniques'), 'do') (('Create', '73'), 'achieve') (('629247', 'depth 4 iterations'), 'use') (('io daniellewisdl', 'cheat sheet app'), 'streamlit') (('Lemmatization Removing', 'contents'), 'pre') (('We', 'data 10 cleansing'), 'leave') (('Surprisingly we', 'f1 same score'), 'note') (('Accident Levels Calendar Accidents', '2017'), 'record') (('We', 'max length'), 'need') (('when many accidents', 'period'), 'need') (('it', 'year'), 'be') (('accuracy', 'continually training'), 'see') (('koheimuramatsu accident causal analysis Holoviews plot com industrial tips', 'holoviews'), 'http') (('we', 'f1 score'), 'model') (('we', 'scoring function'), 'capture') (('s', 'variable'), 'let') (('We', 'duplicates'), 'have') (('We', '2016'), 'create') (('therefore sequences', 'also variable lengths'), 'have') (('Study Summary Statistics Study Correlation Observations WeekofYear featuer', 'Month feature'), 'have') (('Sometimes they', 'also such environment'), 'die') (('why accidents', 'risks'), 'think') (('Varible Tansformation Scaling Use Extract Principal that', 'contents'), 'Creation') ", "extra": "['biopsy of the greater curvature', 'gender', 'test', 'bag']"}