{"name": "what does a cnn see ", "full_name": " h1 Contents h2 1 Import the required libraries h2 2 Do everything we can to make our results reproducible h2 3 Load dataset and process it h2 4 Data Augmentation h2 5 Data Generator h2 6 Modelling h2 7 Model Interpretability h2 8 Can we or should we use CAM for model interpretability h3 Advantages h3 Disadvantages h2 9 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "You can even say that the first feature that is far by the most important to our model is the face of a monkey. Deep learning models are data hungry. Modelling modelling 7. I hope you enjoyed the kernel. Data AugmentationWhen you have limited data deep models don t do very well. You can choose whichever network you want. Define a new model vis_model that takes the same input as our model s input but outputs activations of all the intermediate layers we have selected. Do everything we can to make our results reproducibleThumb rule Always set the seed 3. We will define a sequence of augmentations and for each image one of these augmentations will be applied to the image during training 5. Let me know if you have any suggestions. Well this statement is neither completely true not it is completely false. Machine learning models especially Deep Learning models are often considered as a black box and hard to interpret. Conclusion conclusion 1. This is where data augmentation really comes handy. The labels are n0 n1 n2. How do you infer that for that particular class if the neck was more discriminative feature or face is Though it gives you an idea of discriminative features it fails to capture the high level semantic relationship within the hidden layers. It is not an easy task at all but the on going research in this area will lead to better ways of interpretability in the nearby future. Do you have any idea what are you doin http media. com media SRx5tBBrTQOBi giphy. For example if you look at the examples above there are some cases where we have the activation heatmap ranging from neck to face. Model Interpretability interpretability 8. We will also create a mapping for the names corresponding to a class. This is why I always store information about the dataset in dataframes and then use a generator to load the data on the fly. We will be using imgaug https github. Also as the dataset is very small and very very similar to Imagenet we would make minimal changes in the network to keep the trainable parameters as few as possible 7. Here we will explore two methods that are very simple to use and can give some good insights about model predictions. These are Visualizing the intermediate layers outputs Class Activation Mapping http cnnlocalization. Although there have been many advancements in explaining the activations outputs of a neural network for example check this https distill. This suggests that for our model at least face is a very discriminative feature. But this isn t always the scenario. read_csv Set the seed for hash based operations in python Set the numpy seed Set the random seed in tensorflow at graph level Make the augmentation sequence deterministic As usual define some paths first to make life simpler Read the file Convert the data into a pandas dataframe Sneak peek Create a dictionary to map the labels to integers map labels to common names Creating a dataframe for the training dataset Define the path to the images Get the list of all the images stored in that directory Store each image path and corresponding label shuffle the dataset Creating dataframe for validation data in a similar fashion shuffle the dataset How many samples do we have in our training and validation data sneak peek of the training and validation dataframes some constants not truly though dimensions to consider for the images batch size for training total number of classes in the dataset Augmentation sequence horizontal flips roatation random brightness Get total number of samples in the data Get a numpy array of all the indices of the input data Define two numpy arrays for containing batch data and labels shuffle indices for the training data get the next batch process the next batch training data generator validation data generator simple function that returns the base model get the base model get the output of the second last dense layer add new layers define a new model Freeze all the base model layers compile the model and check it always user earlystopping the restore_best_weights parameter load the weights of the best iteration once the training finishes checkpoint to save model number of training and validation steps for training and validation number of epochs train the model let s plot the loss and accuracy get the training and validation accuracy from the history object get the loss get the number of entries visualize What is the final loss and accuracy on our validation data select all the layers for which you want to visualize the outputs and store it in a list Define a new model that generates the above output check if we have all the layers we require for visualization store the layer names we are interested in we want the activations for the predicted label choose the last conv layer in your model get the gradients wrt to the last conv layer take mean gradient per feature map Define a function that generates the values for the output and gradients get the values iterate over each feature map in yout conv output and multiply the gradient values with the conv output values. Can we or should we use CAM for model interpretability infer_results 9. com aleju imgaug a very powerful library for augmenting our images. You can load the data into numpy arrays which then can be directly used for training. This gives an indication of how important a feature is we have 512 features in our last conv layer create a heatmap remove negative values normalize select the sample and read the corresponding image and label pre process the image generate activation maps from the intermediate layers using the visualization model get the label predicted by our original model choose any random activation map from the activation maps normalize the sample activation map convert pixel values between 0 255 get the heatmap for class activation map CAM Get the intermediate activations and plot the heatmap first visualize all the activation maps for this sample. edu Here are all the steps we are going to do We will start by selecting all the layers up to last convolution block in VGG16 excluding the Input layer. Most of the time you won t be able to load the entire dataset in the memory. There are some advantages and disadvantages when using CAM. txt file to extract the information about the labels. The last kernel was on Kaggle Survey Challenge 2018 https www. Disadvantages Even though CAM gives a fair idea about dominant features but it lacks to provide specific details. Check visualize_intermediate_activations function for more details. com aakashnain is it better than 2017. Load dataset and process it dataloading 4. pub 2018 building blocks a lot more has to be done in this direction. Can we or should we use CAM for model interpretability If you look at the above results of CAM you will find that in most of the class activation maps face of a monkey seems to be highly activated. Load dataset and process itWe will read the monkey_labels. We also compute the gradients for this layer which we will use to generate a heatmap. This will be used for displaying all the activation maps of each convolution block for any sample image from our validation data. The more data you provide to a deep learning model the more it performance improves until unless your algorithm has reached a limit. We will use the names during visualizations of these activation maps Pretty cool isn t it Let s check the CAM for few more samples now 8. We will create a mapping of these labels where each class will be represented by an integer starting from 0 to number of classes. Import the required libraries 2. ModellingWe will be doing transfer learning here and I am choosing vgg16 as the base network. How do we explain the output of our model For example given an image what does the network consider important when classifying the image Can we get info about it Can we make our model a little grey box LOL Answering the above question is a bit difficult. Perfect Store the names of all these intermediate layers in a list. Do everything we can to make our results reproducible reproducibility 3. I am listing all of which I am aware of Advantages CAM is a weak supervision technique and it works well out of the box It is easy and simple to implement It gives a fair idea of the dominant features for which a model is looking for It is one of the easiest methods to explain the outputs of a CNN to anyone including even a non technical person. Hello Kagglers It has been a while since I made a new kernel. Happy Kaggling linear algebra data processing CSV file I O e. We can store this information in a list which then can be converted into a pandas dataframe. See the get_CAM function for details. pub 2018 building blocks 9. For more details check out this excellent article https distill. Data Augmentation augmentation 5. We will be using Common Name for the last partThis is a very small dataset. Data Generator data gen 6. Model InterpretabilityNow starts the most important part. You have to figure out how you can make the predictions interpretable. Import the required libraries imports. It is a fact that debugging a deep learning model is way harder than other machine learning models but there are ways by which you can get insights about your model and to an extent you can see what is happening. We will be doing the same thing here. For CAM we will take the same sample image and get the output of the last convolution layer. ConclusionMachine learning models aren t pure black box. Today we are going to deep dive into Model Interpretability. ", "id": "aakashnain/what-does-a-cnn-see", "size": "6968", "language": "python", "html_url": "https://www.kaggle.com/code/aakashnain/what-does-a-cnn-see", "git_url": "https://www.kaggle.com/code/aakashnain/what-does-a-cnn-see", "script": "Flatten sklearn.metrics visualize_intermediate_activations makedirs skimage.transform keras.layers keras.callbacks get_base_model train_test_split isdir VGG16 EarlyStopping Dropout Path Sequential SGD Adam imread listdir Conv2D show_random_sample exists seaborn numpy os.path pathlib plot_confusion_matrix Input ModelCheckpoint sklearn.model_selection join confusion_matrix matplotlib.pyplot isfile RMSprop Dense os keras.applications.vgg16 pandas expanduser keras.utils keras.optimizers tensorflow imgaug.augmenters get_CAM resize to_categorical data_generator mlxtend.plotting preprocess_input Callback load_model remove Model MaxPooling2D imgaug getcwd abspath backend backend as K keras keras.models skimage.io ", "entities": "(('itWe', 'monkey_labels'), 'read') (('why I', 'fly'), 'be') (('we', 'model interpretability'), 'Can') (('255 heatmap', 'sample'), 'give') (('This', 'validation data'), 'use') (('It', 'nearby future'), 'be') (('one', 'training'), 'define') (('you', 'memory'), 'be') (('which', 'then directly training'), 'load') (('that', 'monkey'), 'say') (('here I', 'base network'), 'do') (('don t', 'limited data deep models'), 'have') (('I', 'new kernel'), 'Kagglers') (('reproducibleThumb rule', 'Always seed'), 'do') (('you', 'network'), 'choose') (('it', 'hidden layers'), 'infer') (('names', 'class'), 'create') (('learning ConclusionMachine models', 't pure black box'), 'aren') (('we', 'heatmap'), 'compute') (('Deep Learning especially models', 'often black box'), 'consider') (('we', 'intermediate layers'), 'define') (('pub', 'building 2018 lot more direction'), 'have') (('Model InterpretabilityNow', 'most important part'), 'start') (('where class', 'classes'), 'create') (('last kernel', 'Kaggle Survey https Challenge 2018 www'), 'be') (('we', 'convolution last layer'), 'take') (('we', 'as few'), 'be') (('it', 'specific details'), 'disadvantage') (('s', 'few more samples'), 'use') (('results', 'reproducibility'), 'do') (('We', 'last partThis'), 'use') (('Today we', 'Model Interpretability'), 'go') (('you', 'monkey'), '-PRON-') (('grey LOL little Answering', 'above question'), 'explain') (('which', 'pandas then dataframe'), 'store') (('at least face', 'model'), 'suggest') (('that', 'model predictions'), 'explore') (('values', 'output conv values'), 'Set') (('you', 'http media'), 'have') (('where we', 'face'), 'be') (('We', 'Input layer'), 'edu') (('algorithm', 'limit'), 'improve') (('model', 'even non technical person'), 'list') (('Happy Kaggling', 'linear algebra data CSV file'), 'process') (('Load it', '4'), 'dataset') (('intermediate layers', 'Class Activation Mapping http cnnlocalization'), 'visualize') (('what', 'extent'), 'be') ", "extra": "[]"}