{"name": "experiment tracking with weights and biases ", "full_name": " h1 What is W B h2 Why is W B useful h1 Dashboard experiment tracking h2 Track metrics h2 Visualize results h2 Train anywhere h2 Stay organized h1 Artifacts dataset versioning h3 Manage your pipeline Track data lineage h1 TL DR h1 1 Set up W B h2 1a Install wandb h2 1b Import wandb and log in h1 I have saved my API token with wandb api as Label h1 If you use some other Label make sure to change the same below h1 2 Build Input Pipeline h1 3 Set up your model h1 3 Train and evaluate with W B h2 3 Use wandb init to initialize a new W B run h2 3b Update wandb config h3 Filtering and Grouping h2 3c Log evaluation score with wandb log h1 4 Create a W B Artifact h2 4a Save your hard work with wandb log artifact h1 Resources ", "stargazers_count": 0, "forks_count": 0, "description": "This is where W B comes in Dashboard experiment tracking Log and visualize experiments in real time Keep data and results in one convenient place. config Save model as Model Artifact Finish W B run. Add your model file to the Artifact with wandb. Dashboard experiment tracking Use the Dashboard as a central place to organize and visualize results from your machine learning models. I also created some dataset Artifacts for this competition and you can check them all out here https wandb. gif Train anywhereThe W B dashboard is centralized whether you re training on a local machine lab cluster or spot instances in the cloud all of your results get logged to a single place. com a 65997094 8663152 for more details. login key wandb_api More on W B login here https docs. W B Artifacts lets you log the data that goes in like a dataset and out like trained model weights of these processes. png ResourcesI hope you find this kernel useful and I encourage you to try out Weights Biases. Filtering and GroupingYou can use filter and group feature on W B dashboard to either hide crashed run group together multiple runs under one experiment group according to job_type select runs that satisfy a condition etc. Typical job types are train evaluate etc. ai guides track launch Initialize a new run. For example you can create group for different model architecture names. Pro tip If you want to silence W B related logs use this code snippet os. Visualize resultsW B supports a large variety of media types visualize graphs images videos audio 3D objects and more https docs. init config argument as above in 3a. This callback automatically saves all the metrics and the loss values tracked in model. from kaggle_secrets import UserSecretsClientuser_secrets UserSecretsClient I have saved my API token with wandb_api as Label. Or you might use the wrong model for generating submission. ai ref app pages run page charts tab System Tab https docs. Artifacts dataset model versioning Store and version datasets models and results Know exactly what data a model is being trained on. gif At the end of the gif there s a graph that shows the lineage the deep thunder 8 save run produced the efficientnetb0 v0 model. Create an empty Artifact with wandb. Weights and Biases comes with a light weight integration for Keras. 2 img_width 224 img_height 224 batch_size 64 epochs 10 learning_rate 0. ai ayush thakur plant pathology runs 5yq0kz7t. config with the wandb. In other words Artifacts are a way to save your datasets and models. Check out this discussion post https www. Later on I ll have a run that groups by EfficientNet so that I can compare runs from the different architectures more easily. We will be using W B Keras integration WandbCallback to automatically save all the metrics and the loss values tracked in model. ai guides integrations keras. Save your hard work with wandb. fit but not for model. Import other dependencies 2. wandb_api user_secrets. log Finish the run Save model Initialize a new W B run Note the job_type Update wandb. Consider this as a repository of experiments. config This argument sets wandb. Build Input PipelineAfter importing wandb and other dependencies you can set up an input pipeline as normal. ai to learn more about the best practices and advanced features. com ayuraj efficientnet mixup k fold using tf and wandb HPA Segmentation Mask Visualization with W B https www. entity An entity is a username or team name where you re sending runs. Why is W B useful Kaggle competitions require fast paced model development and evaluation. Check out our Keras integration docs https docs. ai guides track log logging objects. ai ref python data types. There are two ways you can login using a Kaggle kernel 1. Set up your modelBuild your model definition next. Install wandb wandb the W B library comes baked into your Kaggle kernels However since wandb is rapidly improving I recommend pip install ing the latest version with the upgrade and q flags. ai ref python run is a W B unit of computation typically an ML experiment. Set up W BW B is very easy to set up and integrate. init arguments project This argument specifies the name of the W B project where the run gets sent to. See more setup options in this Colab https colab. config a dictionary like object that stores hyperparameters input settings and other independent variables. Helper functions for input pipeline Convert the compressed string to a 3D uint8 tensor Normalize image Resize the image to the desired size Load image Resize image Parse label 7. ai guides integrations. init to initialize a new W B run. config Evaluate model Log scores using wandb. Visualize results in real time in an interactive centralized dashboard Identify how changing data affects the resulting modelPlus W B is fast get started with 6 lines of code https docs. com ayuraj hpa multi label classification with tf and w b BirdCLEF Quick EDA with W B https www. EfficientNet Mixup K Fold using TF and wandb https www. gif You can click through this Run Page here https wandb. This can be a good starting point. Train and evaluate with W BIn this section I ll use wandb. Since it s a multi label classification task the output activation is sigmoid. Access your API key https wandb. Initialize W B run Add type and kaggle_competition to wandb. com ayuraj birdclef quick eda with w b 1. Pro tip Use run. ai guides track config An object that stores hyperparameters and settings related to a run. If you use some other Label make sure to change the same below. Log evaluation score with wandb. ai ref cli wandb login. ai guides integrations keras to learn more. Setting a run type makes it easier to later filter and group runs for example if you want to compare multiple train runs. com wandb examples for curated and minimal examples. config a dictionary like object for saving inputs to your job like hyperparameters for a model or settings for a data preprocessing job. ai guides track log Log a dict of scalars metrics like accuracy and loss and any other type of wandb object https docs. If you don t have a Weights Biases account you can go to https wandb. group Specify a group to organize individual runs into a larger experiment. com github wandb examples blob master colabs wandb log Configs_in_W 26B. finish https docs. This is a super handy feature. log_artifact to save the Artifact. png TL DRWeights Biases helps you build better models faster. Build data loaders Data loader sanity check 8. config directly Train Close W B run Initialize a run Note the job_type Update wandb. init to the beginning of your training script as well as your evaluation script and each piece would be tracked as a run in W B. You can also use Kaggle secrets to store your API key and use the code snippet below to login. The Plant Pathology 2021 competition is about multi label classification. gif Pro tip Note that the bar chart apprears if there are more than one value for a key. Training and validation split 6. Import wandb and log inYou will need a unique API key to log in to Weights Biases. It will ask for the API key which you can copy paste in. Store your hyperparameters as a dictionary because you can later directly log this config dict to W B. Here s a gif showing the Charts Tab https docs. This makes it easy to compare thousands of different models and find the best performing model for different tasks. log WandbCallback can be used for model. You can use this Colab https colab. ai fully connected is a home for curated tutorials free form dicussions paper summaries industry expert advices and more. Run object https docs. Check out the docs to learn more about this integration https docs. Plus the dashboard is interactive hover for more options and information. Keep track of which model was trained on which data Manage your pipeline Track data lineageYou get a bird eye view on your entire machine learning workflow. com c plant pathology 2021 fgvc8 to demonstrate some of its features Dashboard experiment tracking and Artifacts dataset and model versioning. Once you ve saved something as an Artifact all modifications are automatically logged giving you a complete history of changes. If the project is not specified the run is put in an Uncategorized project. ai site and create a FREE account. job_type Specify the type of run which is useful when you re grouping runs together into larger experiments using group. As a reminder here s what s currently in CONFIG pythonCONFIG dict num_labels 6 train_val_split 0. Define model EfficientNetB0 trained on ImageNet as backbone Model sanity check Initialize model Compile model Update CONFIG dict with the name of the model. job_type This argument specifies a run type for example train or evaluate. Here s a gif that shows how an Artifact appears within a Run Page. ai ayush thakur plant pathology artifacts. com github wandb examples blob master colabs wandb artifacts Pipeline_Versioning_with_W 26B_Artifacts. To build the input pipeline I have used tf. config This sets wandb. There are multiple ways to set up wandb. What is W B Weights Biases W B is a set of machine learning tools that helps you build better models faster. ai guides track advanced grouping. log_artifact Within a run there are three steps for creating and saving a model Artifact. Set hyperparameters 4. Each run gets its own Run Page which has tabs that contain more information about the run. finish to close the initialized W B run after a job_type is finished. ai ref app pages run page model tab from the run above img https i. centralized dashboard https i. Run a cell with wandb. Here are some relevant links that you might want to check out Check out the official documentation https docs. ai ref python finish Finish and close a run. init https docs. environ WANDB_SILENT true after import os. com watch v EIgoKitLUqM t 92s 1. Collaborating with your teammates is easier with such a holistic view of your pipeline. com ayuraj hpa segmentation mask visualization with w b HPA Multi Label Classification with TF and W B https www. Lots of components lots of places to go wrong lots of time spent debugging You might miss important details and have to retrain your model or you might train on the wrong data information leakage. Check out the examples GitHub repository https github. In an ML training pipeline you could add wandb. I ll use this Knowledge based competition Plant Pathology 2021 FGVC8 https www. Take a note of these arguments. If you want to log metrics from evaluation you should call wandb. WandbCallback is a lightweight Keras callback that I ll use later on. ipynb scrollTo xFf3zjBSixC1. Track metrics Visualize results Train anywhere Stay organized Save everything in one place and never lose your progress again Track metricsTrack model performance in real time and identify problem areas immediately. ai quickstart and flexible integrated with every major ML framework https docs. com product feedback 114053 to learn more about Kaggle secrets. ipynb to learn more about Artifacts. 001 architecture CNN infra Kaggle model_name efficientnetb0 group This argument specifies a value to group individual runs by. Pro tip Head over to the W B dashboard my clicking on the link generated above. Here I m creating a new project called plant pathology and sending the run there at the same time. png Stay organizedW B logs data into powerful querably tables that you can search filter sort and group. In this section I ll show you how to create a model Artifact. With W B you can also group by config values meaning that you can compare the settings of different runs and see how they affect the output. Set the random seeds 3. png Artifacts dataset versioning Use W B Artifacts to track and version your datasets models dependencies and results across machine learning pipelines. csv file Get absolute path Parse labels Look at the dataframe 5. The main objective of the competition is to develop machine learning based models to accurately classify a given leaf image from the test dataset to a particular disease category and to identify an individual disease from multiple disease symptoms on a single leaf image. Here s a quick 1 42 intro thumbnail https i. There are a lot of components exploring the training data training different models combining trained models in different combinations ensembling and so on. Think of an Artifact as a versioned folder of data. Check out this Stackoverflow answer https stackoverflow. Here are some other Kaggle kernels instrumented with Weights Biases that you might find useful. Create a W B ArtifactW B s Dashboard lets you log the model training process things like output logs code versions configuration hyperparameters and metrics. Fully Connected https wandb. Here s a gif showing how W B logs and displays a run s training configuration img https i. config Saving your training configuration is useful for analyzing your experiments and reproducing your work later. You can learn more about Group feature here https docs. gif Now I ll use WandbCallback the lightweight Keras integration that I mentioned earlier. Build input pipeline Encode competition provided labels Helper fu Read train. ai ref app pages run page system tab and Model Tab https docs. project The name of the project where you re sending the new run. config https docs. get_secret wandb_api wandb. ", "id": "ayuraj/experiment-tracking-with-weights-and-biases", "size": "14283", "language": "python", "html_url": "https://www.kaggle.com/code/ayuraj/experiment-tracking-with-weights-and-biases", "git_url": "https://www.kaggle.com/code/ayuraj/experiment-tracking-with-weights-and-biases", "script": "get_model models numpy load_image parse_labels sklearn.model_selection seed_everything tensorflow_addons matplotlib.pyplot decode_image WandbCallback tensorflow pandas wandb.keras tensorflow.keras layers show_batch train_test_split make_path ", "entities": "(('code snippet', 'use'), 'tip') (('Plant Pathology 2021 competition', 'label multi classification'), 'be') (('architecture CNN Kaggle model_name efficientnetb0 infra argument', 'group individual runs'), 'group') (('track log', 'object https docs'), 'guide') (('log_artifact', 'three model'), 'be') (('Initialize W B', 'wandb'), 'run') (('querably you', 'filter sort'), 'stay') (('path Parse absolute labels', 'dataframe'), 'get') (('I', 'competition Plant Knowledge based Pathology'), 'use') (('job_type', 'W B initialized run'), 'finish') (('you', 'https out here wandb'), 'create') (('ai ref python run', 'W B computation'), 'be') (('You', 'below login'), 'use') (('argument', 'train'), 'specify') (('init', 'W B.'), 'track') (('track launch', 'new run'), 'guide') (('Track lineageYou', 'machine learning entire workflow'), 'keep') (('I', 'different architectures'), 'have') (('you', 'better models'), 'be') (('Build input pipeline Encode competition', 'labels Helper fu Read train'), 'provide') (('you', 'model architecture different names'), 'create') (('here what', 'num_labels'), 's') (('you', 'better models'), 'help') (('Set', 'W BW B'), 'be') (('run', 'Uncategorized project'), 'put') (('how they', 'output'), 'group') (('I', 'Weights Biases'), 'hope') (('you', 'wandb'), 'call') (('that', 'condition'), 'use') (('you', 'paste'), 'ask') (('all', 'single place'), 'centralize') (('stores hyperparameters', 'run'), 'guides') (('Artifacts', 'datasets'), 'be') (('rapidly I', 'upgrade'), 'wandb') (('I', 'Artifact'), 'show') (('when you', 'group'), 'specify') (('you', 'train multiple runs'), 'make') (('deep thunder', 'save v0 8 efficientnetb0 model'), 'gif') (('you', 'submission'), 'use') (('We', 'loss model'), 'use') (('Visualize resultsW B', 'audio 3D objects'), 'support') (('I', 'input pipeline'), 'build') (('modifications', 'changes'), 'save') (('ai ref app pages', 'img https i.'), 'run') (('I', 'wandb'), 'train') (('group', 'larger experiment'), 'specify') (('ai', 'more'), 'guide') (('you', 'documentation https official docs'), 'be') (('config', 'work'), 'be') (('com ayuraj', 'b w 1'), 'birdclef') (('how Artifact', 'Run Page'), 's') (('it', 'different tasks'), 'make') (('You', 'https here docs'), 'learn') (('two you', 'Kaggle kernel'), 'be') (('Kaggle useful competitions', 'model fast paced development'), 'be') (('where run', 'W B project'), 'project') (('you', 'data information wrong leakage'), 'miss') (('com c plant pathology', 'features'), 'fgvc8') (('ai ref python', 'run'), 'finish') (('other you', 'input pipeline'), 'set') (('Build data', 'Data loader sanity check'), 'loader') (('ai ref app pages', 'page charts tab System Tab https docs'), 'run') (('ai ref app pages', 'page system tab'), 'run') (('callback', 'loss model'), 'save') (('ai', 'form dicussions paper summaries industry expert fully curated tutorials free advices'), 'be') (('I', 'that'), 'gif') (('that', 'processes'), 'let') (('com ayuraj efficientnet', 'W B https www'), 'mixup') (('Here I', 'there same time'), 'm') (('gif You', 'Run https Page here wandb'), 'click') (('main objective', 'leaf single image'), 'be') (('you', 'other Label'), 'make') (('com github wandb examples master colabs blob wandb', '26B_Artifacts'), 'artifacts') (('UserSecretsClientuser_secrets I', 'Label'), 'from') (('W where B', 'one convenient place'), 'be') (('stores', 'input settings'), 'config') (('Import wandb', 'Weights Biases'), 'need') (('ai ayush thakur plant pathology', '5yq0kz7 t'), 'run') (('Helper functions', 'Load image Resize Parse label'), 'Convert') (('you', 'Weights Biases'), 'be') (('model', 'models'), 'dataset') (('how changing data', 'code https docs'), 'Identify') (('Collaborating', 'pipeline'), 'be') (('username where you', 'runs'), 'be') (('where you', 'new run'), 'project') (('Dashboard experiment tracking', 'machine learning models'), 'Use') (('you', 'wandb'), 'add') (('dashboard', 'interactive more options'), 'be') (('gif Pro tip bar chart', 'more than one key'), 'note') (('you', 'W B.'), 'store') (('png Artifacts', 'machine learning pipelines'), 'dataset') (('ArtifactW Dashboard you', 'output logs code versions configuration hyperparameters'), 'a') (('Weights', 'Keras'), 'come') (('backbone Model sanity check Initialize model Compile model Update CONFIG', 'model'), 'train') (('that', 'run'), 'get') (('you', 'https wandb'), 'have') (('Pro', 'link'), 'tip') (('environ WANDB_SILENT', 'import os'), 'true') (('Track metrics Visualize Train', 'problem areas'), 'result') ", "extra": "['disease', 'test']"}