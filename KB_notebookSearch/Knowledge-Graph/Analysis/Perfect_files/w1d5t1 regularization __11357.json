{"name": "w1d5t1 regularization ", "full_name": " h1 Tutorial 1 Regularization techniques part 1 h1 Tutorial Objectives h1 Setup h2 Install dependencies h2 Figure Settings h2 Loading Animal Faces data h2 Loading Animal Faces Randomized data h2 Plotting functions h2 Set random seed h2 Set device GPU or CPU Execute set device h1 Section 0 Defining useful functions h1 Section 1 Regularization is Shrinkage h2 Video 1 Introduction to Regularization h2 Video 2 Regularization as Shrinkage h2 Coding Exercise 1 Frobenius Norm h1 Section 2 Overfitting h2 Video 3 Overparameterization and Overfitting h2 Section 2 1 Visualizing Overfitting h3 Animation Run Me h3 Plot the train and test losses h3 Think 2 1 Interpreting losses h4 Student Response h2 Section 2 2 Overfitting on Test Dataset h4 Validation Dataset h1 Section 3 Memorization h2 Data Visualizer h1 Section 4 Early Stopping h2 Video 4 Early Stopping h2 Coding Exercise 4 Early Stopping h2 Think 4 Early Stopping h3 Student Response h1 Summary h2 Airtable Submission Link h1 Bonus Train with randomized labels h2 Think Bonus Does it Generalize ", "stargazers_count": 0, "forks_count": 0, "description": "If we overfit our training data there is always the evaluation on test data to keep us honest. Frobenious norm of the modelFinally you can compare the Frobenius norm per layer in the model before and after training. 2 Overfitting on Test DatasetIn principle we should not touch our test set until after we have chosen all our hyperparameters. Read more here https pytorch. Ideally we would only touch the test data once to assess the very best model or to compare a small number of models to each other real world test data is seldom discarded after just one use. Set the arguments Intialize the model Train the model train and test acc plot markdown Plotting them all together Run Me. Shuffling some of the data is a form of regularization one of many ways of adding noise to the training data. Frobenius norm per layer before and after training Section 2. Click for solution https github. Notice how we split the data. A model with big weights can fit more data perfectly whereas a model with smaller weights tends to underperform on the train set but can surprisingly do very well on the test set. Validation DatasetA common practice to address this problem is to split our data in three ways using a validation dataset or validation set to tune the hyperparameters. Tutorial 1 Regularization techniques part 1 Week 1 Day 5 Regularization By Neuromatch Academy __Content creators __ Ravi Teja Konkimalla Mohitrajhu Lingan Kumaraian Kevin Machado Gamboa Kelson Shilling Scrivo Lyle Ungar__Content reviewers __ Piyush Chauhan Siwei Bai Kelson Shilling Scrivo__Content editors __ Roberto Guidotti Spiros Chavlis__Production editors __ Saeed Salehi Spiros Chavlis Our 2021 Sponsors including Presenting Sponsor Facebook Reality Labs Tutorial Objectives1. Were we to use the test data in the model selection process there is a risk that we might overfit the test data. then remove or comment the line below to test your function raise NotImplementedError Complete the early_stopping_main function Number of successive epochs that you want to wait before stopping training process in number of epcos Keps track of number of epochs during which the val_acc was less than best_acc train the model calculate training accuracy calculate validation accuracy add event to airtable Set the arguments Initialize the model title Student Response title Airtable Submission Link Here we have 15 partially shuffled train data. We have learned about overfitting one of the worst caveats in deep learning and finally we learned a method of reducing overfitting in our models called early stopping. For this we can modify our calculate_frobenius_norm function as shown below. com NeuromatchAcademy course content dl tree main tutorials W1D5_Regularization solutions W1D5_Tutorial1_Solution_683d27d3. 1 Visualizing OverfittingLet s create some synthetic dataset that we will use to illustrate overfitting in neural networks. then remove or comment the line below to test your function raise NotImplementedError Define calculate_frobenius_norm function Sum the square of all parameters Take a square root of the sum of squares of all the parameters add event to airtable Seed added for reproducibility Frobenius Norm per Layer initialization of variables Sum all the parameters Take a square root of the sum of squares of all the parameters Plots the weights Creates a new model Calculates the forbenius norm per layer Plots the weights title Video 3 Overparameterization and Overfitting add event to airtable creating train data input output adding small error in the data visualizing trian data creating test dataset Network Class 2D train the network on toy dataset Calculates frobenius before training initializing variables losses model norm Initializing variables to store weights frobenius norm per epoch training title Animation Run Me create a figure and axes organizing subplots title Plot the train and test losses title Student Response markdown Frobenious norm of the model Change title to Frobenious norm of the model markdown Frobenius norm per layer before and after training Dataloaders for the Dataset defining number of examples for train val test using pathlib to be compatible with all OS s Dataloaders for the Original Dataset For reproducibility Creating train_loader and Val_loader Dataloaders for the Random Dataset For reproducibility splitting randomized data into training and validation data using pathlib to be compatible with all OS s Randomized train and validation dataloader Dataloaders for the Partially Random Dataset For reproducibility Splitting data between training and validation dataset for partially randomized data using pathlib to be compatible with all OS s Training and Validation loader for partially randomized data Network Class Animal Faces Here we have 100 true train data. But if we overfit the test data how would we ever know Note that there is another kind of overfitting you do honest fitting on one set of images or posts or medical records but it may not generalize to other sets of images posts or medical records. Hint Use functions model. Airtable Submission Link Bonus Train with randomized labelsIn this part let s train on a partially shuffled dataset where 15 of the labels are noisy. com NeuromatchAcademy course content dl tree main tutorials W1D5_Regularization solutions W1D5_Tutorial1_Solution_c705db1a. You should have also observed from the above plots that the train test loss on real data is not very smooth and hence you might guess that the choice of epoch can play a very large role on the val test accuracy of your model. You should see that the value of weights increases over the epochs. Execute set_device especially if torch modules used. title Tutorial slides markdown These are the slides for the videos in this tutorial title Install dependencies generate airtable form Imports title Figure Settings title Loading Animal Faces data title Loading Animal Faces Randomized data title Plotting functions unnormalize title Set random seed markdown Executing set_seed seed seed you are setting the seed for DL its critical to set the random seed so that students can have a baseline to compare their results to expected results. 6572162508964539 Apart from calculating the weight size for an entire model we could also determine the weight size in every layer. named_parameters Click for solution https github. Let s create an overparametrized Neural Network that can fit on the dataset that we just created and train it. Animation Run Me Plot the train and test losses Think 2. Animal Faces Dataset2. Regularization as shrinkage of overparameterized models early stopping SetupNote that some of the code for today can take up to an hour to run. Section 4 Early Stopping Time estimate 20 mins Video 4 Early StoppingNow that we have established that the validation accuracy reaches the peak well before the model overfits we want to somehow stop the training early. Section 1 Regularization is Shrinkage Time estimate 20 mins Video 1 Introduction to RegularizationA key idea of neural nets is that they use models that are too complex complex enough to fit all the noise in the data. First let s build the model architecture Next let s define the different parameters for training our model At this point we can now train our model. t to train and test losses Where do you see the minimum of these losses What does it tell us about the model we trained Student Response Click for solution https github. Set the arguments Initialize the network Train the network Train and Test accuracy plot markdown Frobenius norm for AnimalNet before and after training Choose the datapoint you would like to visualize choose that datapoint using index and permute the dimensions and bring the pixel values between 0 1 Convert the torch tensor into numpy Call the function Here we have 100 completely shuffled train data. The more complex the model the better it fits the training data but if it is too complex it generalizes less well it memorizes the training data but is less accurate on future test data. com NeuromatchAcademy course content dl tree main tutorials W1D5_Regularization solutions W1D5_Tutorial1_Solution_62d845ba. Then we would be in serious trouble. py Also it is interesting to note that sometimes the model trained on slightly shuffled data does slightly better than the one trained on pure data. org docs stable notes randomness. Have a look how it works Using the last function calculate_frobenius_norm we can also obtain the Frobenius Norm per layer for a whole NN model and use the plot_weigts function to visualize them. Section 2 Overfitting Time estimate 15 mins Video 3 Overparameterization and Overfitting Section 2. begin equation A _F sqrt sum_ i 1 m sum_ j 1 n a_ ij 2 end equation This is just a measure of how big the matrix is analogous to how big a vector is. In case that DataLoader is used title Set device GPU or CPU. Coding Exercise 4 Early StoppingReimplement the main function to include early stopping as described above. In this section we train three MLPs one each on 1. Now let s define a model which has many parameters compared to the training dataset size and train it on these datasets. Here we have classes cat dog wild. imshow expects imput to be in numpy format and in the format P_x P_y 3 where P_x and P_y are the number of pixels along axis x and y respectively. html Call set_seed function in the exercises to ensure reproducibility. parameters or model. Coding Exercise 1 Frobenius NormBefore we start let s define the Frobenius norm sometimes also called the Euclidean norm of an m n matrix A as the square root of the sum of the absolute squares of its elements. Having the weights too small can also be an issue as it can then underfit the model. Network Class Animal Faces sum up batch loss get the index of the max log probability title Video 1 Introduction to Regularization add event to airtable title Video 2 Regularization as Shrinkage add event to airtable Fill in all missing code below. com NeuromatchAcademy course content dl tree main tutorials W1D5_Regularization solutions W1D5_Tutorial1_Solution_1f125e8e. Frobenius Norm of Single Linear Layer 0. Isn t it surprising to see that the NN was able to achieve 100 training accuracy on randomly shuffled labels This is one of the reasons why training accuracy is not a good indicator of model performance. We have therefore hidden the code and shown the resulting outputs. 1 Interpreting lossesRegarding the train and test graph above discuss among yourselves What trend do you see w. inform the user if the notebook uses GPU or CPU. py Random seed 2021 has been set. Section 3 Memorization Time estimate 20 mins Given sufficiently large networks and enough training Neural Networks can achieve almost 100 train accuracy by remembering each training example. One then needs to regularize them to make the models fit complex enough but not too complex. Install dependencies Figure Settings Loading Animal Faces data Loading Animal Faces Randomized data Plotting functions Set random seed Executing set_seed seed seed you are setting the seed Set device GPU or CPU. Now that we have finished training let s see how the model has evolved over the training process. Video 2 Regularization as ShrinkageOne way to think about Regularization is to think in terms of the magnitude of the overall weights of the model. This week we use the sum of Frobenius Norm of all the tensors in the model as a measure of the size of the model. py Example output Think 4 Early StoppingDiscuss among your pod why or why not Do you think early stopping can be harmful for training your network Student Response Click for solution https github. The test function takes in the current model after every epoch and calculates the accuracy on the test dataset. A partially Noisy Dataset Random Shuffling of 15 labels Now think for a couple of minutes as to what the train and test accuracies of each of these models might be given that you train for sufficient time and use a powerful network. Early stopping stops training when the validation accuracies stop increasing. com NeuromatchAcademy course content dl tree main tutorials W1D5_Regularization solutions W1D5_Tutorial1_Solution_8f1d49a8. Then run the code below to validate your implementation. First let s create the required dataloaders for all three datasets. Now let s train the network on the shuffled data and see if it memorizes. We train on a fraction of the dataset as it will be faster to train and will overfit more clearly. Plotting them all together Run Me Think Bonus Does it Generalize Given that the Neural Network fit memorize the training data perfectly Do you think it generalizes well What makes you think it does or doesn t Click for solution https github. A Completely Noisy Dataset Random Shuffling of all labels 3. ANNs memorize some but generalize well3. This is bad however because it will mean that the model will fail when presented with new data. Execute set_device Section 0 Defining useful functionsLet s start the tutorial by defining some functions which we will use frequently today such as AnimalNet train test and main. py SummaryIn this tutorial you have been introduced to the regularization technique where we have described it as shrinkage. Before training our BigAnimalNet calculate the Frobenius norm again. Big ANNs are efficient universal approximators due to their adaptive basis functions2. Set the arguments Intialize the model Train the model Train and Test accuracy plot title Video 4 Early Stopping add event to airtable Fill in all missing code below. Now train our BigAnimalNet model Frobenius norm for AnimalNet before and after training Data VisualizerBefore we train the model on a data with random labels let s visualize and verify for ourselves that the data is random. If you have time left you can learn how a model behaves when is trained with randomized labels. py Now let s visualize the Frobenious norm of the model as we trained. The train function takes in the current model along with the train_loader and loss function and updates the parameters for a single pass of the entire dataset. ", "id": "joseguzman/w1d5t1-regularization", "size": "11357", "language": "python", "html_url": "https://www.kaggle.com/code/joseguzman/w1d5t1-regularization", "git_url": "https://www.kaggle.com/code/joseguzman/w1d5t1-regularization", "script": "torch.optim __init__ display seed_worker IFrame tqdm.auto ipywidgets early_stop_plot on_button_clicked Net(nn.Module) AirtableForm train IPython zipfile ImageFolder YouTubeVideo visualize_data forward torch.nn numpy BiliVideo(IFrame) main torchvision ZipFile plot_weights BigAnimalNet(nn.Module) print_function test clear_output display as IPydisplay matplotlib.pyplot torchvision.datasets frame set_seed widgets calculate_frobenius_norm tqdm imshow early_stopping_main torch.nn.functional evaltools.airtable transforms __future__ set_device matplotlib.animation AnimalNet(nn.Module) IPython.display HTML ", "entities": "(('week we', 'model'), 'use') (('Here we', 'train 100 completely shuffled data'), 'initialize') (('data', 'ourselves'), 'train') (('you', 'when randomized labels'), 'learn') (('you', 'powerful network'), 'think') (('Neural enough Networks', 'training example'), 'estimate') (('finally we', 'models'), 'learn') (('DataLoader', 'case'), 'in') (('too small also it', 'then model'), 'be') (('train function', 'entire dataset'), 'take') (('we', 'hyperparameters'), 'overfitting') (('value', 'epochs'), 'see') (('less well it', 'test less future data'), 'model') (('we', 'train frequently today such AnimalNet test'), 'section') (('fit', 'then them'), 'need') (('Student Response Airtable Submission Here we', 'train 15 partially shuffled data'), 'remove') (('model', 'when new data'), 'be') (('some', 'hour'), 'regularization') (('we', 'them'), 'obtain') (('First s', 'three datasets'), 'let') (('you', 'w.'), 'interpret') (('it', 'dataset'), 'train') (('we', 'calculate_frobenius_norm function'), 'modify') (('Shrinkage', 'airtable Fill'), 'sum') (('we', 'solution https github'), 't') (('how vector', 'equation _ F sqrt sum'), 'begin') (('choice', 'model'), 'observe') (('early stopping', 'Student Response solution https github'), 'think') (('we', 'us'), 'be') (('P_y 3 where P_x', 'x'), 'expect') (('you', 'seed'), 'randomized') (('we', 'layer'), '6572162508964539') (('we', 'test data'), 'be') (('you', 'before training'), 'norm') (('torch especially modules', 'set_device'), 'Execute') (('we', 'somehow training'), 'estimate') (('test function', 'test dataset'), 'take') (('we', 'model'), 'let') (('it', 'shuffled data'), 'let') (('Big ANNs', 'basis efficient universal due adaptive functions2'), 'be') (('that', 'data'), 'be') (('Ideally we', 'seldom just one use'), 'touch') (('we', 'neural networks'), '1') (('Video 2 Regularization', 'model'), 'be') (('sometimes model', 'pure data'), 'py') (('which', 'datasets'), 'let') (('Here we', 'train 100 true data'), 'remove') (('Shuffling', 'training data'), 'be') (('medical it', 'images posts'), 'know') (('accuracy plot Train 4 Early Stopping', 'airtable Fill'), 'Set') (('where 15', 'labels'), 'let') (('training why accuracy', 'model good performance'), 't') (('it', 'solution https github'), 'plot') (('we', 'just it'), 'let') (('students', 'expected results'), 'slide') (('s', 'elements'), 'let') (('where we', 'shrinkage'), 'py') (('how model', 'training process'), 'let') (('common practice', 'hyperparameters'), 'be') (('We', 'resulting outputs'), 'hide') (('we', 'now model'), 'let') (('we', 'one 1'), 'train') (('perfectly model', 'test surprisingly very well set'), 'fit') (('notebook', 'GPU'), 'inform') ", "extra": "['test']"}