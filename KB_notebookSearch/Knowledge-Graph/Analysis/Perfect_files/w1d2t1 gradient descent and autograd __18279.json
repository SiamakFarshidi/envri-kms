{"name": "w1d2t1 gradient descent and autograd ", "full_name": " h1 Tutorial 1 Gradient Descent and AutoGrad h1 Tutorial Objectives h2 Tutorial slides h1 Setup h2 Install dependencies h2 Figure settings h2 Plotting functions h2 Set random seed h2 Set device GPU or CPU Execute set device h1 Section 0 Introduction h2 Video 0 Introduction h1 Section 1 Gradient Descent Algorithm h2 Section 1 1 Gradients Steepest Ascent h3 Video 1 Gradient Descent h3 Analytical Exercise 1 1 Gradient vector Optional h4 Solution h3 Coding Exercise 1 1 Gradient Vector h4 Video 2 Gradient Descent Discussion h2 Section 1 2 Gradient Descent Algorithm h3 Analytical Exercise 1 2 Gradients h2 Section 1 3 Computational Graphs and Backprop h3 Video 3 Computational Graph h3 Analytical Exercise 1 3 Chain Rule Optional h4 Solution h1 Section 2 PyTorch AutoGrad h2 Video 4 Auto Differentiation h2 Section 2 1 Forward Propagation h3 Coding Exercise 2 1 Buiding a Computational Graph h2 Section 2 2 Backward Propagation h2 AUTOMATIC DIFFERENTIATION WITH TORCH AUTOGRAD h1 Section 3 PyTorch s Neural Net module nn Module h2 Video 5 PyTorch nn module h2 Section 3 1 Training loop in PyTorch h3 Coding Exercise 3 1 Training Loop h1 Summary h2 Video 6 Tutorial 1 Wrap up h2 Airtable Submission Link ", "stargazers_count": 0, "forks_count": 0, "description": "The example below shows that the tensor c a b is created by the Add operation and the gradient function is the object. backward is called on the loss which is the last node on the graph. Read more here https pytorch. You can use the command dir my_object to observe all variables and associated methods to your object e. The second tutorial will help us build a better intuition about neural networks and basic hyper parameters. Click for solution https github. html AUTOGRAD MECHANICS https pytorch. html A kind of Tensor that is to be considered a module parameter. org tutorials beginner basics autogradqs_tutorial. py Example output We can see from the plot that for any given x_0 and y_0 the gradient vector left dfrac partial z partial x dfrac partial z partial y right top _ x_0 y_0 points in the direction of x and y for which z increases the most. c a b or c torch. backward accumulates gradients in the leaf nodes i. AutoGrad is PyTorch s automatic differentiation engine. detach it from the graph by calling the. The gradient of a function always points in the direction of the steepest ascent. Solution begin equation dfrac partial f partial y dfrac partial f partial e dfrac partial e partial d dfrac partial d partial c dfrac partial c partial b dfrac partial b partial y left 1 tanh 2 e right cdot frac 1 d 1 cdot z cdot frac a b 2 cdot cos y end equation For more Calculus on Computational Graphs Backpropagation https colah. grad on the relevant tensors. This is called the backward pass from which the backpropagation of errors algorithm gets its name. grad attributes see autograd. Loss function The loss that we are going to be optimizing which is often combined with regularization terms conming up in few days. Since negative gradients always point locally in the direction of steepest descent the algorithm makes small steps at each point towards the minimum. Execute set_device Section 0 IntroductionToday we will go through 3 tutorials. html AUTOMATIC DIFFERENTIATION WITH TORCH. By breaking the computation into simple operations on intermediate variables we can use the chain rule to calculate any gradient begin equation dfrac partial f partial x dfrac partial f partial e dfrac partial e partial d dfrac partial d partial c dfrac partial c partial a dfrac partial a partial x left 1 tanh 2 e right cdot frac 1 d 1 cdot z cdot frac 1 b cdot 2 end equation Conveniently the values for e b and d are available to us from when we did the forward pass through the graph. You can run the cell multiple times and see how the parameters are being updated and the loss is reducing. It does so by calculating intermediate variables a b c d and e. Execute set_device especially if torch modules used. Here we start by covering the essentials of AutoGrad and you will learn more in the coming days. Generate the sample datasetLet s define a very wide 512 neurons neural net with one hidden layer and Tanh activation function. Optimizer PyTorch provides us with many optimization methods different versions of gradient descent. Thus we can simply use the aforementioned formula to find the local minima. t bias title Video 5 PyTorch nn module add event to airtable markdown Generate the sample dataset creating an instance Create a mse loss function Stochstic Gradient Descent optimizer you will learn about momentum soon learning rate Reset all gradients to zero Forward pass Compute the output of the model on the features inputs like wide_net. In 1847 Augustin Louis Cauchy used negative of gradients to develop the Gradient Descent algorithm as an iterative method to minimize a continuous and ideally differentiable function of many variables. The task is to train a wide nonlinear using tanh activation function neural net for a simple sin regression task. For gradient descent it is only required to have the gradients of cost function with respect to the variables we wish to learn. org docs stable notes autograd. Tutorial 1 Gradient Descent and AutoGrad Week 1 Day 2 Linear Deep Learning By Neuromatch Academy __Content creators __ Saeed Salehi Vladimir Haltakov Andrew Saxe__Content reviewers __ Polina Turishcheva Antoine De Comite Kelson Shilling Scrivo__Content editors __ Anoop Kulkarni Spiros Chavlis__Production editors __ Khalid Almubarak Spiros Chavlis Our 2021 Sponsors including Presenting Sponsor Facebook Reality Labs Tutorial ObjectivesDay 2 Tutorial 1 will continue on buiding PyTorch skillset and motivate its core functionality Autograd. org tutorials beginner blitz autograd_tutorial. Important Notes Learnable parameters i. different activation and loss functions and much more packed in the torch. begin equation mathbf w t 1 mathbf w t eta nabla f mathbf w t end equation where eta 0 and nabla f mathbf w left frac partial f mathbf w partial w_1. In neural nets weights and biases are often the learnable parameters. We have seen all of these using PyTorch modules and we compared the analytical solutions with the ones provided directly by the PyTorch module. To create a custom model parameter we can use nn. Video 0 Introduction Section 1 Gradient Descent Algorithm Time estimate 30 45 mins Since the goal of most learning algorithms is minimizing the risk also known as the cost or loss function optimization is often the core of most machine learning techniques The gradient descent algorithm along with its variations such as stochastic gradient descent is one of the most powerful and popular optimization methods used for deep learning. Each variable has a grad_fn attribute that references a function that has created the Tensor except for Tensors created by the user these have None as grad_fn. io posts 2015 08 Backprop Section 2 PyTorch AutoGrad Time estimate 30 45 mins Video 4 Auto DifferentiationDeep learning frameworks such as PyTorch JAX and TensorFlow come with a very efficient and sophisticated set of algorithms commonly known as Automatic Differentiation. Coding Exercise 3. detach method on that tensor. Let s perform one training iteration. Video 2 Gradient Descent Discussion Section 1. title Video 0 Introduction add event to airtable title Video 1 Gradient Descent add event to airtable Complete the function and remove or comment the line below add event to airtable title Video 2 Gradient Descent Discussion add event to airtable title Video 3 Computational Graph add event to airtable title Video 4 Auto Differentiation add event to airtable add event to airtable Complete the function and remove or comment the line below Complete the function and remove or comment the line below input tensor target tensor analytical gradients remember detaching first we should call the backward to build the graph we calculate the derivative w. If we build a neural network using torch. Video 6 Tutorial 1 Wrap up Airtable Submission Link title Tutorial slides markdown These are the slides for the videos in this tutorial title Install dependencies init airtable form Imports title Figure settings interactive display title Plotting functions title Set random seed markdown Executing set_seed seed seed you are setting the seed for DL its critical to set the random seed so that students can have a baseline to compare their results to expected results. So any operation that is applied to Y will be part of the computational graph. com NeuromatchAcademy course content dl tree main tutorials W1D2_LinearDeepLearning solutions W1D2_Tutorial1_Solution_0c8e3872. 2 Gradient Descent AlgorithmLet f mathbf w mathbb R d rightarrow mathbb R be a differentiable function. For more complex functions printing the grad_fn would only show the last operation even though the object tracks all the operations up to that point Now let s kick off the backward pass to calculate the gradients by calling. 2 GradientsGiven f x y z tanh left ln left 1 z frac 2x sin y right right how easy is it to derive dfrac partial f partial x dfrac partial f partial y and dfrac partial f partial z hint you don t have to actually calculate them Section 1. reguires_grad tensors are contagious. org docs stable notes randomness. t weights we calculate the derivative w. 1 Forward PropagationEverything starts with the forward propagation pass. In case that DataLoader is used title Set device GPU or CPU. Finally in tutorial 3 we learn about the learning dynamics what the a good deep network is learning and why sometimes they may perform poorly. weights begin equation dfrac partial Loss partial mathbf w left dfrac partial Loss partial w_1 dfrac partial Loss partial w_2. html Section 3 PyTorch s Neural Net module nn. org docs stable generated torch. zero_grad on the loss or optimizer to zero out all. Parameter https pytorch. Starting from x y and z and following the arrows and expressions you would see that our graph returns the same function as f. In PyTorch Tensor and Function are interconnected and build up an acyclic graph that encodes a complete history of computation. Coding Exercise 2. html Call set_seed function in the exercises to ensure reproducibility. 1 Training loop in PyTorchWe use a regression problem to study the training loop in PyTorch. This is called the forward pass. com NeuromatchAcademy course content dl tree main tutorials W1D2_LinearDeepLearning solutions W1D2_Tutorial1_Solution_6668feea. 1 Gradients Steepest Ascent Video 1 Gradient DescentBefore introducing the gradient descent algorithm let s review a very important property of gradients. The training process in PyTorch is interactive you can perform training iterations as you wish and inspect the results after each iteration. 1 Gradient VectorImplement complete the function which returns the gradient vector for z sin x 2 y 2. AUTOGRAD https pytorch. Wide neural networks are thought to be really good at generalization. This function is still extraordinarily simple compared to the loss functions of modern neural networks. 3 Chain Rule Optional For the function above calculate the dfrac partial f partial y using the computational graph and chain rule. nn layers the weights and biases are already in requires_grad mode and will be registered as model parameters. the input nodes to the node of interest. backward on the tensor we wish to initiate the backpropagation from. py It is important to appreciate the fact that PyTorch can follow our operations as we arbitrarily go through classes and functions. Module Time estimate 30 mins Video 5 PyTorch nn modulePyTorch provides us with ready to use neural network building blocks such as layers e. PyTorch tracks all the instructions as we declare the variables and operations and it builds the graph when we call the. org docs stable autograd. You will learn more details about choosing the right model architecture loss function and optimizer later in the course. It is important to note that gradient vectors only see their local values not the whole landscape Also length size of each vector which indicates the steepness of the function can be very small near local plateaus i. html AUTOMATIC DIFFERENTIATION PACKAGE TORCH. Now let s start from f and work our way against the arrows while calculating the gradient of each expression as we go. 1 Gradient vector Optional Given the following function begin equation z h x y sin x 2 y 2 end equation find the gradient vector begin equation begin bmatrix dfrac partial z partial x dfrac partial z partial y end bmatrix end equation hint use the chain rule Chain rule For a composite function F x g h x equiv g circ h x begin equation F x g h x cdot h x end equation or differently denoted begin equation frac dF dx frac dg dh frac dh dx end equation Solution We can rewrite the function as a composite function begin equation z f left g x y right f u sin u g x y x 2 y 2 end equation Using chain rule begin align dfrac partial z partial x dfrac partial f partial g dfrac partial g partial x cos g x y 2x cos x 2 y 2 cdot 2x dfrac partial z partial y dfrac partial f partial g dfrac partial g partial y cos g x y 2y cos x 2 y 2 cdot 2y end align Coding Exercise 1. 1 Buiding a Computational GraphIn PyTorch to indicate that a certain tensor contains learnable parameters we can set the optional argument requires_grad to True. Optimizer holds the current state of the model and by calling the step method will update the parameters based on the computed gradients. Starting with Gradient Descent the workhorse of deep learning algorithms in this tutorial. 3 Computational Graphs and Backprop Video 3 Computational Graph Exercise 1. PyTorch will then track every operation using this tensor while configuring the computational graph. py Example output SummaryIn this tutorial we covered one of the most basic concepts of deep learning the computational graph and how a network learns via gradient descent and the backpropagation algorithm. 2 is an example of how overwhelming the derivation of gradients can get as the number of variables and nested functions increases. Therefore if we need to plot or store a tensor that is reguires_grad we must first. The following exercise will help clarify this. dfrac partial Loss partial w_d right top end equation Analytical Exercise 1. backward https pytorch. In this notebook we will cover the key concepts and ideas of Gradient descent PyTorch Autograd PyTorch nn module Tutorial slides These are the slides for the videos in this tutorial SetupThis a GPU Free tutorial Install dependencies Figure settings Plotting functions Set random seed Executing set_seed seed seed you are setting the seed Set device GPU or CPU. These variables are often called learnable trainable parameters or simply parameters in PyTorch. For this exercise use the provided tensors to build the following graph which implements a single neuron with scalar input and output. PyTorch rebuilds the graph every time we iterate or change it or simply put PyTorch uses a dynamic graph. We can now create an instance of our neural net and print its parameters. 1 Training LoopUsing everything we ve learned so far we ask you to complete the train function below. parameters on the model. Replace with other single operations e. So how can we as well as PyTorch and similar frameworks approach such beasts Let s look at the function again begin equation f x y z tanh left ln left 1 z frac 2x sin y right right end equation We can build a so called computational graph shown below to break the original function into smaller and more approachable expressions. For training we need three things Model parameters Model parameters refer to all the learnable parameters of the model which are accessible by calling. That is the partial derivatives have simple expressions in terms of the intermediate variables a b c d e that we calculated and stored during the forward pass. Analytical Exercise 1. com NeuromatchAcademy course content dl tree main tutorials W1D2_LinearDeepLearning solutions W1D2_Tutorial1_Solution_5204c053. Vanilla Algorithm inputs initial guess mathbf w 0 step size eta 0 number of steps T For t 0 2 dots T 1 do qquad mathbf w t 1 mathbf w t eta nabla f mathbf w t end return mathbf w t 1 Hence all we need is to calculate the gradient of the loss function with respect to the learnable parameters i. sin a and examine the results. Recall that in python we can access variables and associated methods with. Before doing that let s calculate the loss gradients by hand frac partial loss partial w 2 x y_t y_p 1 y_p 2 frac partial loss partial b 2 y_t y_p 1 y_p 2 Where y_t is the target true label and y_p is the prediction model output. We can then compare it to PyTorch gradients which can be obtained by calling. Today we will introduce the basics but you will learn much more about Optimization in the coming days Week 1 Day 4. forward inputs Compute the loss Perform backpropagation to build the graph and compute the gradients Optimizer takes a tiny step in the steepest direction negative of gradient and updates the weights and biases of the network keeping recods of loss Complete the function and remove or comment the line below set gradients to 0 Compute model prediction output Compute the loss Compute gradients backward pass update parameters optimizer takes a step add event to airtable Cauchy Exercices d analyse et de physique mathematique 1847 title Video 6 Tutorial 1 Wrap up add event to airtable title Airtable Submission Link. 2 Backward PropagationHere is where all the magic lies. frac partial f mathbf w partial w_d right. References and more A GENTLE INTRODUCTION TO TORCH. This code block is the core of everything to come please make sure you go line by line through all the commands and discuss their purpose with the pod. Please note that NOT all the requires_grad tensors are seen as model parameters. Gradient Descent is an iterative algorithm for minimizing the function f starting with an initial value for variables mathbf w taking steps of size eta learning rate in the direction of the negative gradient at the current point to update the variables mathbf w. inform the user if the notebook uses GPU or CPU. Let s look at a simple example Y W X where X is the feature tensors and W is the weight tensor learnable parameters reguires_grad the newly generated output tensor Y will be also reguires_grad. ", "id": "joseguzman/w1d2t1-gradient-descent-and-autograd", "size": "18279", "language": "python", "html_url": "https://www.kaggle.com/code/joseguzman/w1d2t1-gradient-descent-and-autograd", "git_url": "https://www.kaggle.com/code/joseguzman/w1d2t1-gradient-descent-and-autograd", "script": "__init__ display seed_worker torch IFrame ipywidgets AirtableForm train make_axes_locatable IPython YouTubeVideo forward numpy BiliVideo(IFrame) nn fun_dz matplotlib.pyplot set_seed fun_z widgets pi evaltools.airtable ex1_plot sq_loss ex3_plot WideNet(nn.Module) set_device SimpleGraph display as IPyDisplay math IPython.display mpl_toolkits.axes_grid1 ", "entities": "(('Chain Rule 3 function', 'computational graph rule'), 'Optional') (('weights', 'model parameters'), 'be') (('we', 'derivative w.'), 'add') (('you', 'pod'), 'be') (('we', 'variables'), 'recall') (('30 Auto DifferentiationDeep learning 45 mins Video 4 frameworks', 'Automatic commonly Differentiation'), 'posts') (('which', 'few days'), 'function') (('z', 'most'), 'py') (('we', 'tensor'), 'first') (('we', 'True'), '1') (('you', 'iteration'), 'be') (('we', 'backpropagation'), 'backward') (('you', 'coming days'), 'introduce') (('function', 'modern neural networks'), 'be') (('which', 'plateaus very local i.'), 'be') (('when we', 'graph'), 'use') (('Gradient Descent', 'mathbf w.'), 'be') (('Wide neural networks', 'really generalization'), 'think') (('Optimizer', 'computed gradients'), 'hold') (('why sometimes they', 'what'), 'learn') (('which', 'scalar input'), 'use') (('right f u end u g y 2 2 equation', 'align dfrac partial z partial dfrac partial f partial g dfrac partial g partial'), 'begin') (('w', 'dfrac partial Loss partial w_1 dfrac partial Loss partial w_2'), 'begin') (('s', 'gradients'), '1') (('Forward 1 PropagationEverything', 'propagation forward pass'), 'start') (('we', 'torch'), 'build') (('different activation functions', 'much more torch'), 'packed') (('Execute set_device 0 IntroductionToday we', '3 tutorials'), 'Section') (('Thus we', 'local minima'), 'use') (('we', 'PyTorch directly module'), 'see') (('torch especially modules', 'set_device'), 'Execute') (('that', 'html Tensor'), 'kind') (('backpropagation', 'name'), 'call') (('we', 'learnable parameters'), 'input') (('Training 1 loop', 'PyTorch'), 'use') (('task', 'sin regression simple task'), 'be') (('We', 'parameters'), 'create') (('Now s', 'gradients'), 'show') (('cdot z 1 cdot', 'Computational Graphs Backpropagation https colah'), 'begin') (('these', 'grad_fn'), 'have') (('that', 'computation'), 'interconnect') (('you', 'seed'), 'cover') (('we', 'variables'), 'require') (('how network', 'gradient descent'), 'output') (('that', 'computational graph'), 'be') (('notebook', 'GPU'), 'inform') (('us', 'neural networks'), 'help') (('DataLoader', 'case'), 'in') (('time we', 'dynamic graph'), 'rebuild') (('which', 'PyTorch gradients'), 'compare') (('we', 'expression'), 'let') (('nn Video 5 modulePyTorch', 'layers such e.'), 'estimate') (('end right right We', 'smaller more expressions'), 'approach') (('we', 'arbitrarily classes'), 'py') (('html AUTOMATIC DIFFERENTIATION', 'TORCH'), 'package') (('weight learnable parameters', 'output newly generated tensor'), 'let') (('which', 'last graph'), 'call') (('which', 'z sin'), 'complete') (('nabla mathbf 0 w', 'f mathbf w partial partial w_1'), 'begin') (('gradient', 'steepest ascent'), 'point') (('You', 'later course'), 'learn') (('output', 'wide_net'), 'add') (('gradient function', 'Add c operation'), 'show') (('you', 'coming days'), 'start') (('It', 'b c d'), 'do') (('variables', 'often learnable trainable simply PyTorch'), 'call') (('z dfrac partial f partial you', 'don actually them'), 'leave') (('algorithm', 'minimum'), 'make') (('loss', 'cell'), 'run') (('so far we', 'train function'), '1') (('following exercise', 'this'), 'help') (('Augustin Louis Cauchy', 'many variables'), 'use') (('graph', 'f.'), 'see') (('1847 title Video 6 Tutorial 1 Wrap', 'title Airtable Submission airtable Link'), 'input') (('Optimizer PyTorch', 'gradient descent'), 'provide') (('we', 'nn'), 'use') (('ObjectivesDay 2 Tutorial', 'core functionality'), 'Descent') (('You', 'associated object'), 'use') (('which', 'model'), 'need') (('b c d we', 'forward pass'), 'be') (('students', 'expected results'), 'wrap') (('PyTorch', 'computational graph'), 'track') (('number', 'variables'), 'be') (('gradient descent', 'deep learning'), 'section') (('true y_p', 'hand frac partial loss'), 'let') (('frac', 'f partial w partial right'), 'mathbf') (('requires_grad tensors', 'model parameters'), 'note') (('when we', 'the'), 'track') ", "extra": "[]"}