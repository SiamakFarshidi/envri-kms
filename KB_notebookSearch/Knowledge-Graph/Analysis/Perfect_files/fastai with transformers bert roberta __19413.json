{"name": "fastai with transformers bert roberta ", "full_name": " h1 Fastai with HuggingFace Transformers BERT RoBERTa XLNet XLM DistilBERT h1 Introduction Story of transfer learning in NLP h1 Integrating transformers with fastai for multiclass classification h2 Libraries Installation h2 The example xa0task h2 Main transformers classes h2 Util function h2 Data pre processing h3 Custom Tokenizer h3 Custom Numericalizer h3 Custom processor h2 Setting up the Databunch h3 Custom model h2 Learner xa0 Custom Optimizer Custom xa0Metric h2 Discriminative Fine tuning and Gradual unfreezing Optional h2 Train h2 Export Learner h2 Creating prediction h1 Conclusion h1 References ", "stargazers_count": 0, "forks_count": 0, "description": "Main transformers classesIn transformers each model architecture is associated with 3 main types of classes A model class to load store a particular pre train model. For example if we use the RobBERTa model and that we observe his architecture by making print learner. Integrating transformers with fastai for multiclass classificationBefore beginning the implementation note that integrating transformers within fastai can be done in multiple different ways. html bertconfig for the configuration class. embeddings learner. References Hugging Face Transformers GitHub Nov 2019 https github. read_csv fastai transformers Parameters model_type bert pretrained_model_name bert base uncased model_type distilbert pretrained_model_name distilbert base uncased model_type xlnet pretrained_model_name xlnet base cased Python cpu vars cpu vars gpu vars defining our model architecture attention_mask Mask to avoid performing attention on padding token indices. Creating predictionNow that the model is trained we want to generate predictions from the test dataset. com p fastai with transformers bert roberta xlnet xlm distilbert 4f41ee18ecb2 source email 29c8f5cf1dc4 writer. The BaseTokenizer object https docs. From this analyse we suggest two ways to adapt the fastai numericalizer 1. There is multible ways to create a DataBunch in our implementation we use the data block API https docs. In this implementation be carefull about 3 things 1. Therefore we can simply create a new class TransformersBaseTokenizer that inherits from BaseTokenizer and overwrite a new tokenizer function. As mentioned in the HuggingFace documentation BERT RoBERTa XLM and DistilBERT are models with absolute position embeddings so it s usually advised to pad the inputs on the right rather than the left. com huggingface transformers models always output tuples every model s forward method always outputs a tuple with various elements depending on the model and the configuration parameters. We can find this learning rate by using a learning rate finder which can be called by using lr_find. For those models the encoding methods should be called with add_prefix_space set to True. Even if the first solution seems to be simpler Transformers does not provide for all models a straightforward way to retreive his list of tokens. Print the available values for pretrained_model_name shortcut names corresponding to the model_type used. In our case the parameter pretrained_model_name is a string with the shortcut name of a pre trained model tokenizer configuration to load e. We can decide to divide the model in 14 blocks 1 Embedding 12 transformer 1 classifierIn this case we can split our model in this way Check groups Note that I didn t found any document that has studied the influence of Discriminative Fine tuning and Gradual unfreezing or even Slanted Triangular Learning Rates with transformers. Although these models are powerful fastai do not integrate all of them. html load_learner you have to be careful that each custom classes like TransformersVocab are first defined before executing load_learner. You can also find this information on the HuggingFace documentation https huggingface. Dev Sharma s article Using RoBERTa with Fastai for NLP https medium. The Tokenizer object https docs. Custom processorNow that we have our custom tokenizer and numericalizer we can create the custom processor. html Discriminative layer training. More precisely I try to make the minimum of modification in both libraries while making them compatible with the maximum amount of transformer architectures. The difficulty here is that each pre trained model that we will fine tune requires exactly the same specific pre process tokenization numericalization than the pre process used during the pre train part. Therefore I implemented the second solution which runs for each model type. layer 2 learner. html Tokenizer takes as tok_func argument a BaseTokenizer object. ai Fastai documentation Nov 2019 https docs. co transformers pretrained_models. So you just have to instal transformers with The current versions of the fastai and transformers libraries are respectively 1. export and load_learner https docs. As we will see later fastai manage it automatically during the creation of the DataBunch. pre_classifier For xlnet base cased list_layers learner. Create a new class TransformersVocab that inherits from Vocab and overwrite numericalize and textify functions. Next we will use fit_one_cycle with the chosen learning rate as the maximum learning rate. A tokenizer class to pre process the data and make it compatible with a particular model. com analytics vidhya using roberta with fastai for nlp 7ed3fed21f6c which makes pytorch_transformers library compatible with fastai. Let s first analyse how we can integrate the transformers tokenizer within the TokenizeProcessor function. Although these articles are of high quality some part of their demonstration is not anymore compatible with the last version of transformers. The TokenizeProcessor object https docs. The transformers library can be self sufficient but incorporating it within the fastai library provides simpler implementation compatible with powerful fastai tools like Discriminate Learning Rate Gradual Unfreezing or Slanted Triangular Learning Rates. The example taskThe chosen task is a multi class text classification on Movie Reviews https www. ai course https course. Custom NumericalizerIn fastai NumericalizeProcessor object https docs. Libraries InstallationBefore starting the implementation you will need to install the fastai and transformers libraries. layer 8 learner. Here we unfreeze all the groups. html Jeremy Howard Sebastian Ruder Universal Language Model Fine tuning for Text Classification May 2018 https arxiv. We can find all the shortcut names in the transformers documentation here https huggingface. transformer_model model_class. NB The functions __gestate__ and __setstate__ allow the functions export https docs. html berttokenizer for the tokenizer class and BertConfig https huggingface. g bert base uncased. For more information please check the fastai documentation here https docs. Put learn in FP16 precision mode. layer 11 learner. We can now submit our predictions to Kaggle In our example without playing too much with the parameters we get a score of 0. layer 1 learner. Unfortunately the model architectures are too different to create a unique generic function that can split all the model types in a convenient way. com huggingface transformers Fast. To do so you can modify the config instance or either modify like in Keita Kurita s article https mlexplained. Since the introduction of ULMFiT Transfer Learning became very popular in NLP and yet Google BERT Transformer XL XLNet Facebook RoBERTa XLM or even OpenAI GPT GPT 2 begin to pre train their own model on very large corpora. Custom TokenizerThis part can be a little bit confusing because a lot of classes are wrapped in each other and with similar names. layer 0 learner. com 2019 05 13 a tutorial to fine tuning bert with fast ai May 2019 Dev Sharma s article Using RoBERTa with Fastai for NLP https medium. html load_learner to work correctly with TransformersVocab. html The data block API which gives more flexibility. layer 9 learner. An instruction to perform that split is described in the fastai documentation here https docs. The first time I heard about ULMFiT was during a fast. It worth noting that the integration of the HuggingFace transformers library in fastai has already been demonstrated in Keita Kurita s article A Tutorial to Fine Tuning BERT with Fast AI https mlexplained. co transformers in each model section. com analytics vidhya using roberta with fastai for nlp 7ed3fed21f6c Section 1. html TokenizeProcessor takes as tokenizer argument a Tokenizer object. Some models like RoBERTa require a space to start the input string. A configuration class to load store the configuration of a particular model. Data pre processingTo match pre training we have to format the model input sequence in a specific format. Setting up the DatabunchFor the DataBunch creation you have to pay attention to set the processor argument to our new custom processor transformer_processor and manage correctly the padding. Regarding XLNET it is a model with relative position embeddings therefore you can either pad the inputs on the right or on the left. If you found any interesting documents please let us know in the comment. com huggingface transformers installation. Below you can find the resume of each pre process requirement for the 5 model types used in this tutorial. com 2019 05 13 a tutorial to fine tuning bert with fast ai as the function get_preds does not return elements in order by default you will have to resort the elements into their correct order. You will see later that those classes share a common class method from_pretrained pretrained_model_name. bert CLS tokens SEP padding roberta CLS prefix_space tokens SEP padding distilbert CLS tokens SEP padding xlm CLS tokens SEP padding xlnet padding tokens SEP CLS It is worth noting that we don t add padding in this part of the implementation. Meanwhile this tutorial is a good starter. This implementation is a supplement of the Medium article Fastai with Transformers BERT RoBERTa XLNet XLM DistilBERT https medium. To resume if we look attentively at the fastai implementation we notice that 1. TrainNow we can finally use all the fastai build in features to train our model. 06146 Keita Kurita s article A Tutorial to Fine Tuning BERT with Fast AI https mlexplained. NumericalizeProcessor vocab vocab. com 2019 05 13 a tutorial to fine tuning bert with fast ai Section Initializing the Learner the num_labels argument. It is worth noting that in the dataset there are no individual movie reviews but rather phrases taken out of context and split into smaller parts each with an assigned sentiment label. word_embedding learner. html BaseTokenizer implement the function tokenizer t str List str that take a text t and returns the list of its tokens. Therefore we first freeze all the groups but the classifier with We check which layer are trainable. This year the transformers became an essential tool to NLP. Here 2x10 3 seems to be a good value. In our case we are interested to access only to the logits. com huggingface transformers. It consists of using the functions convert_tokens_to_ids and convert_ids_to_tokens in respectively numericalize and textify. Learner Custom Optimizer Custom MetricIn pytorch transformers HuggingFace had implemented two specific optimizers BertAdam and OpenAIAdam that have been replaced by a single AdamW optimizer. com huggingface transformers https github. postDistributed sk 119c3e5d748b2827af3ea863faae6376 I made another version available on my GitHub TODO add link. Thereby you will have to implement a custom split for each different model architecture. com r url https 3A 2F 2Farxiv. Most of the models require special tokens placed at the beginning and end of the sequences. Notice we are passing the include_bos False and include_eos False options. One way to access them is to create a custom model. Fortunately the tokenizer class from transformers provides the correct pre process tools that correspond to each pre trained model. As a result besides significantly outperforming many state of the art tasks it allowed with only 100 labeled examples to match performances equivalent to models trained on 100 more data. As a result without even tunning the parameters you can obtain rapidly state of the art results. He demonstrated how it was easy thanks to the fastai library to implement the complete ULMFit method with only a few lines of codes. Like the ULMFiT method we will use Slanted Triangular Learning Rates Discriminate Learning Rate and gradually unfreeze the model. I hope you enjoyed this first article and found it useful. We then unfreeze the second group of layers and repeat the operations. The sentiment labels are 0 Negative 1 Somewhat negative 2 Neutral 3 Somewhat positive 4 PositiveThe data is loaded into a DataFrame using pandas. html bertforsequenceclassification for the model class BertTokenizer https huggingface. Attention is all you need https arxiv. Fastai with HuggingFace Transformers BERT RoBERTa XLNet XLM DistilBERT fastai Transformers https i. It aims to make you understand where to look and modify both libraries to make them work together. Now you can predict examples with Export LearnerIn order to export and load the learner you can do these operations As mentioned here https docs. For example if you want to use the Bert architecture for text classification you would use BertForSequenceClassification https huggingface. Note that in addition to this NoteBook and the Medium article https medium. md installation and here https github. com c sentiment analysis on movie reviews overview. layer 6 learner. Formerly knew as pytorch transformers or pytorch pretrained bert this library brings together over 40 state of the art pre trained NLP models BERT GPT 2 RoBERTa CTRL. To make our transformers adapted to multiclass classification before loading the pre trained model we need to precise the number of labels. It is worth noting that in this case we use the transformers library only for a multi class text classification task. Check batch and tokenizer Check batch and numericalizer Custom modelAs mentioned here https github. You can like decribed in the Dev Sharma s article https medium. Because of that I think that pre trained transformers architectures will be integrated soon to future versions of fastai. Util functionFunction to set the seed for generating random numbers. com analytics vidhya using roberta with fastai for nlp 7ed3fed21f6c Sep 2019 linear algebra data processing CSV file I O e. This optimizer matches Pytorch Adam optimizer Api therefore it becomes straightforward to integrate it within fastai. com 2019 05 13 a tutorial to fine tuning bert with fast ai which makes pytorch_pretrained_bert library compatible with fastai. To do so you have to first tokenize and then numericalize the texts correctly. This is because fastai adds its own special tokens by default which interferes with the CLS and SEP tokens added by our custom tokenizer. He also explained key techniques also demonstrated in ULMFiT to fine tune the models like Discriminate Learning Rate Gradual Unfreezing or Slanted Triangular Learning Rates. The implementation gives interesting additional utilities like tokenizer optimizer or scheduler. Thanks for reading and don t hesitate in leaving questions or suggestions. co qspmrcm fastai transformers 1. In order to switch easily between classes each related to a specific model type I created a dictionary that allows loading the correct classes by just specifying the correct model type name. ai and Sebastian Ruder introduced the Universal Language Model Fine tuning for Text Classification https medium. This time instead of using the AWD LSTM neural network they all used a more powerful architecture based on the Transformer cf. For that reason I decided to bring simple solutions that are the most generic and flexible. In his demo he used an AWD LSTM neural network pre trained on Wikitext 103 and get rapidly state of the art results. For Slanted Triangular Learning Rates you have to use the function one_cycle. Therefore using these tools does not guarantee better results. The point here is to allow anyone expert or non expert to get easily state of the art results and to make NLP uncool again. To do so just follow the instructions here https github. from_pretrained pretrained_model_name num_labels 5 Show graph of learner stats and metrics after each epoch. Likely it allows you to use Slanted Triangular Learning Rates Discriminate Learning Rate and even Gradual Unfreezing. In the fastai library data pre processing is done automatically during the creation of the DataBunch. To use our one_cycle we will need an optimum learning rate. 70059 which leads us to the 5th position on the leaderboard ConclusionIn this NoteBook I explain how to combine the transformers library with the beloved fastai library. It is worth noting that for reproducing BertAdam specific behavior you have to set correct_bias False. As specified in Keita Kurita s article https mlexplained. postDistributed sk 119c3e5d748b2827af3ea863faae6376. ai videos lesson 4 given by Jeremy Howard. Setting Up the Tokenizer retreive the list of tokens and create a Vocab object. We evaluate the outputs of the model on classification accuracy. Note here that we use slice to create separate learning rate for each group. layer 7 learner. layer 4 learner. Fortunately HuggingFace https huggingface. In Kaggle the fastai library is already installed. html NumericalizeProcessor takes as vocab argument a Vocab object https docs. com fastai fastai blob master README. As we are not using RNN we have to limit the sequence length to the model input size. Mask values selected in 0 1 1 for tokens that are NOT MASKED 0 for MASKED tokens. As you will see in the DataBunch implementation the tokenizer and numericalizer are passed in the processor argument under the following format processor TokenizeProcessor tokenizer tokenizer. These model types are BERT from Google XLNet from Google CMU XLM from Facebook RoBERTa from Facebook DistilBERT from HuggingFace However if you want to go further by implementing another type of model or NLP task this tutorial still an excellent starter. co transformers model_doc bert. layer 5 learner. co created the well know transformers library https github. sequence_summary For roberta base create a link to download the dataframe which was saved with. For that reason this tutorial integrates only the transformer architectures that have a model for sequence classification implemented. Discriminative Fine tuning and Gradual unfreezing Optional To use discriminative layer training and gradual unfreezing fastai provides one tool that allows to split the structure model into groups. html pretrained models. layer 3 learner. layer 10 learner. ULMFiT was the first Transfer Learning method applied to NLP. For each text movie review the model has to predict a label for the sentiment. We will pick a value a bit before the minimum where the loss still improves. Also remember the upvote button is next to the fork button and it s free too Introduction Story of transfer learning in NLPIn early 2018 Jeremy Howard co founder of fast. Seems to not working For DistilBERT list_layers learner. ", "id": "maroberti/fastai-with-transformers-bert-roberta", "size": "19413", "language": "python", "html_url": "https://www.kaggle.com/code/maroberti/fastai-with-transformers-bert-roberta", "git_url": "https://www.kaggle.com/code/maroberti/fastai-with-transformers-bert-roberta", "script": "torch.optim fastai.text __init__ TransformersVocab(Vocab) TransformersBaseTokenizer(BaseTokenizer) seed_all BertForSequenceClassification XLMForSequenceClassification BertConfig XLNetForSequenceClassification PretrainedConfig RobertaConfig tokenizer XLNetConfig AdamW Path forward transformers __getstate__ XLMConfig numpy pathlib PreTrainedTokenizer CustomTransformerModel(nn.Module) DistilBertTokenizer DistilBertForSequenceClassification XLMTokenizer XLNetTokenizer textify __setstate__ functools IPython.display PreTrainedModel RobertaTokenizer pandas fastai.callbacks partial get_preds_as_nparray numericalize DistilBertConfig fastai create_download_link __call__ BertTokenizer RobertaForSequenceClassification HTML ", "entities": "(('configuration class', 'particular model'), 'store') (('One way', 'custom model'), 'be') (('encoding methods', 'True'), 'call') (('fastai library', 'Kaggle'), 'instal') (('Most', 'sequences'), 'require') (('TrainNow we', 'model'), 'use') (('them', 'transformer architectures'), 'try') (('pre processing', 'DataBunch'), 'do') (('We', 'classification accuracy'), 'evaluate') (('NLP tutorial', 'model'), 'be') (('so you', 'first then texts'), 'have') (('we', '0'), 'submit') (('that', 'Slanted Triangular Learning even transformers'), 'decide') (('that', 'model type just correct name'), 'in') (('they', 'Transformer cf'), 'use') (('layer', 'first groups'), 'freeze') (('html TokenizeProcessor', 'Tokenizer object'), 'take') (('Next we', 'learning maximum rate'), 'use') (('integrating', 'multiple different ways'), 'integrate') (('that', 'numericalize functions'), 'create') (('it', 'right'), 'mention') (('Google BERT XL XLNet RoBERTa yet XLM', 'very large corpora'), 'become') (('we', 'fastai numericalizer'), 'suggest') (('DataBunch you', 'correctly padding'), 'have') (('later classes', 'from_pretrained pretrained_model_name'), 'see') (('us', 'comment'), 'let') (('therefore you', 'left'), 'pad') (('how it', 'codes'), 'demonstrate') (('it', '100 more data'), 'as') (('we', 'print learner'), 'for') (('we', 'test dataset'), 'want') (('_ _ setstate _ functions', 'export https docs'), 'allow') (('it', 'first article'), 'hope') (('You', 'HuggingFace documentation https huggingface'), 'find') (('parameter pretrained_model_name', 'e.'), 'be') (('you', 'Slanted Triangular Learning Rates Discriminate Learning Rate'), 'allow') (('transformers library', 'Discriminate Learning Rate Gradual Unfreezing'), 'be') (('expert expert', 'art results'), 'be') (('we', 'implementation'), 'roberta') (('Keita Kurita article', 'Fast AI https'), 'mlexplaine') (('you', 'correct_bias False'), 'be') (('pre_classifier', 'list_layers learner'), 'case') (('little bit lot', 'similar names'), 'be') (('year transformers', 'essential NLP'), 'become') (('therefore it', 'fastai'), 'match') (('where loss', 'bit minimum'), 'pick') (('library', 'fastai'), 'vidhya') (('sequence classification', 'model'), 'integrate') (('pre process', 'tutorial'), 'find') (('Setting', 'Vocab object'), 'retreive') (('Check Check batch batch', 'numericalizer https modelAs here github'), 'mention') (('that', 'AdamW single optimizer'), 'implement') (('we', 'labels'), 'make') (('implementation', 'Transformers'), 'be') (('NoteBook I', 'fastai beloved library'), '70059') (('we', 'custom processor'), 'create') (('He', 'Discriminate Learning Rate Gradual Unfreezing'), 'explain') (('So you', 'fastai libraries'), 'have') (('you', 'fastai libraries'), 'library') (('version', 'link'), 'sk') (('PositiveThe Negative 1 Somewhat 2 Neutral 3 Somewhat positive 4 data', 'pandas'), 'be') (('we', 'data block API https docs'), 'be') (('that', 'groups'), 'tuning') (('we', 'model input size'), 'have') (('you', 'Keita article https mlexplained'), 'modify') (('you', 'https here docs'), 'predict') (('models', 'input string'), 'require') (('transformers trained architectures', 'fastai'), 'think') (('shortcut names', 'transformers documentation'), 'find') (('first how we', 'TokenizeProcessor function'), 'let') (('that', 'simple solutions'), 'for') (('Thereby you', 'model different architecture'), 'have') (('that', 'MASKED 0 tokens'), 'value') (('you', 'function'), 'have') (('forward method', 'model'), 'com') (('implementation', 'tokenizer interesting additional optimizer'), 'give') (('which', 'model type'), 'implement') (('pretrained_model_name xlnet model_type xlnet base', 'token indices'), 'uncase') (('we', 'include_bos False False options'), 'notice') (('We', 'operations'), 'unfreeze') (('ai', 'Text Classification https medium'), 'introduce') (('you', 'art results'), 'as') (('which', 'SEP custom tokenizer'), 'be') (('here we', 'group'), 'note') (('It', 'respectively numericalize'), 'consist') (('ULMFiT', 'Transfer Learning first NLP'), 'be') (('we', 'class text classification only multi task'), 'be') (('that', 'tokenizer new function'), 'create') (('we', '1'), 'notice') (('Therefore using', 'better results'), 'guarantee') (('part', 'transformers'), 'be') (('that', 'pre trained model'), 'provide') (('that', 'tokens'), 'implement') (('model', 'sentiment'), 'have') (('I', 'fast'), 'be') (('integration', 'Fast AI https'), 'worth') (('html load_learner', 'correctly TransformersVocab'), 'work') (('You', 'article https medium'), 'decribe') (('fine tune', 'train pre part'), 'be') (('pytorch_pretrained_bert library', 'fastai'), 'com') (('model classesIn architecture', 'train particular pre model'), 'associate') (('tok_func', 'BaseTokenizer object'), 'take') (('which', 'dataframe'), 'create') (('models', 'them'), 'integrate') (('you', 'BertForSequenceClassification https huggingface'), 'use') (('them', 'where libraries'), 'aim') (('that', 'convenient way'), 'be') (('Transformers', 'tokens'), 'seem') (('Fastai', 'HuggingFace Transformers'), 'BERT') (('tokenizer', 'format processor TokenizeProcessor tokenizer following tokenizer'), 'pass') (('Thanks', 'don questions'), 'hesitate') (('we', 'specific format'), 'training') (('html NumericalizeProcessor', 'vocab Vocab object https docs'), 'take') (('which', 'more flexibility'), 'html') (('library', 'NLP models BERT GPT'), 'know') (('movie individual rather phrases', 'sentiment each assigned label'), 'be') (('we', 'only logits'), 'be') (('taskThe', 'class text Movie Reviews https multi www'), 'be') (('which', 'lr_find'), 'find') (('we', 'learning optimum rate'), 'need') (('it', 'particular model'), 'class') (('instruction', 'fastai documentation'), 'describe') (('we', 'gradually model'), 'use') (('he', 'art results'), 'use') (('custom classes', 'first load_learner'), 'have') (('you', 'correct order'), 'com') (('Jeremy early 2018 Howard', 'fast'), 'remember') (('later fastai', 'DataBunch'), 'manage') ", "extra": "['test']"}