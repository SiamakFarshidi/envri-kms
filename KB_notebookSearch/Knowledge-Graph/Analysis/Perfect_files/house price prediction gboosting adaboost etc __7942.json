{"name": "house price prediction gboosting adaboost etc ", "full_name": " ", "stargazers_count": 0, "forks_count": 0, "description": "STEP 3 FINDING CORRELATION In this step we check by finding correlation of all the features wrt target variable i. id It is the unique numeric number assigned to each house being sold. price to see whether they are positively correlated or negatively correlated to find if they help in prediction process in model building process or not. lat It determines the latitude of the location of the house. Just after depth 5 training score increase upwards and validation score starts to goes down so I it begins to suffer from overfitting. The description for the 20 features is given below 1. If you fit the model on the training dataset then you implicitly minimize error or find correct responses. As we see from the curve max depth of 5 best generalizes the unseen data. What is the size of the data. STEP 5 SPLITTING DATA INTO TRAINING AND TESTING SET The training dataset and test dataset must be similar usually have the same predictors or variables. In fact it adapts too much to the data. STEP 1 IMPORTING LIBRARIES STEP 2 DATA CLEANING AND PREPROCESSING In this step we check whether data contain null or missing values. It means the model transfers prediction or learning in real sense. Our model is biased in that it assumes that the data will behave in a certain fashion even though that assumption may not be true. Welcome to my kernel In this dataset we have to predict the sales price of houses in King County Seattle. They differ on the observations and specific values in the variables. We should keep the balance between the two. But this is also one of the most important step as it also involves domain knowledge of the field of the data means you cannot simply remove the feature from your prediction process just because it is negatively correlated because it may contribute in future prediction for this you should take help of some domain knowledge personnel. price It is the price of house which we have to predict so this is our target variable and aprat from it are our features. Apart from that your valuable suggestions for further improvement and optimization are always welcome from my side do comment Copying data to another dataframe df_train for our convinience so that original dataframe remain intact. Similar to the learning curves the shaded regions of both the complexity curves denote the uncertainty in those curves and the model is scored on both the training and validation sets using the performance_metric function. Thus it s possible to prevent overfitting. bathrooms It determines number of bathrooms in a bedroom of a house. waterfront This feature determines whether a house has a view to waterfront 0 means no 1 means yes. STEP 6 APPLYING MACHINE LEARNING MODEL STEP 7 ANALYZING TRAINING TIME EACH MODEL HAS TAKEN From the above figure it is inferred that decision tree has taken negligible amount of time to train where as Randome forest has taken maximum time and it is yet obvious because as we increase the number of tree 400 in this case training time will increase so we should look out for optimal model which has greater accuracy and less training time in comparison to otherSo in this case GBoost is the best choice as its accuracy is highest and it is taking less time to train wrt accuracy. STEP 7 PLOTTING OF COMPLEXITY CURVE The following code cell produces a graph for a Gradient Boosting model that has been trained and validated on the training data using different maximum depths. The graph produces two complexity curves one for training and one for validation. yr_built It detrmines the date of building of the house. As max depth increases bias becomes lower and variance becomes higher. However our model uses a very complex curve to get as close to every data point as possible. date It is the date on which the house was sold out. Consequently a model with high variance has very low bias because it makes little to no assumption about the data. At a maximun depth of 10 model suffers from high variance since training score is 1. 87 which is very close to 1. view This feature determines whether a house has been viewed or not 0 means no 1 means yes. 0 but validation score is about 0. You have more confidence since the test dataset is similar to the training dataset but not the same nor seen by the model. Before doing anything we should first know about the dataset what it contains what are its features and what is the structure of data. condition It determines the overall condition of a house on a scale of 1 to 5. Now lets see the first five rows of the data Check the number of features in the data set Check the data types of each column Check any number of columns with NaN or missing values Check any number of data points with NaN As id and date columns are not important to predict price so we are discarding it for finding correlation Finding Correlation of price woth other variables to see how many variables are strongly correlated with price Printing all the correlated features value with respect to price which is target variable Pairplots to visualize strong correlation Comparing Models on the basis of Model s Accuracy Score and Explained Variance Score of different models Create 10 cross validation sets for training and testing Vary the max_depth parameter from 1 to 10 Calculate the training and testing scores Find the mean and standard deviation for smoothing Plot the validation curve Visual aesthetics. There is however something wrong with the model itself in that it s not complex enough to model our data. sqft_lot It is also the measurement variable which determines square foot of the lot. sqft_above It determines square footage of house apart from basement. sqft_lot15 lotSize area in 2015 implies some renovations Now we know about the overall structure of a dataset. In other words the model is underfitting. zipcode It determines the zipcode of the location of the house. What is the datatype of each column. So let s apply some of the steps that we should generally do while applying machine learning models. Conclusion So we have seen that accuracy of gradient boosting is around 89. If the model predicts good also on the test dataset you have more confidence. Two scores are quite close but both the scores are too far from acceptable level so I think it s a high bias problem. The data points obviously follow some sort of curve but our predictor isn t complex enough to capture that information. sqft_living15 Living room area in 2015 implies some renovations 21. As zipcode is negatively correlated with sales price so we can discard it for sales price prediction. It includes homes sold between May 2014 and May 2015. The fitted model provides a good prediction on the training dataset. floors It determines total floors means levels of house. Again the data points suggest a sort of graceful curve. sqft_living It is the measurement variable which determines the measurement of house in square foot. sqft_basement It determines square footage of the basement of the house. 28 and also achieved decent variance score of 0. So by splitting dataset into training and testing subset we can efficiently measure our trained model since it never sees testing data before. I am just splitting dataset into 20 of test data and remaining 80 will used for training the model. grade It determines the overall grade given to the housing unit based on King County grading system on a scale of 1 to 11. A key point is that there s nothing wrong with our training this is the best possible fit that a linear model can achieve. The dataset cantains 20 house features plus the price along with 21613 observations. Then you test the model on the test dataset. What are unique values of categorical variables etc. yr_renovated It detrmines year of renovation of house. bedrooms It determines number of bedrooms in a house. Therefore it is inferred that Gradient Boosting is the suitable model for this dataset. So that s why 5 should be a good choice. STEP 4 EDA or DATA VISUALIZATION This is also a very important step in your prediction process as it help you to get aware you about existing patterns in the data how it is relating to your target variables etc. Further we can also perform model optimization by using GridSearch to find the appropriate parameters to increase the accuracy by fine tuning hyperparameters. Interpretation of the Curve At a maximum depth of 1 model suffers from high bias. In other words a model is overfitting. long It determines the longitude of the location of the house. ", "id": "sid321axn/house-price-prediction-gboosting-adaboost-etc", "size": "7942", "language": "python", "html_url": "https://www.kaggle.com/code/sid321axn/house-price-prediction-gboosting-adaboost-etc", "git_url": "https://www.kaggle.com/code/sid321axn/house-price-prediction-gboosting-adaboost-etc", "script": "sklearn.metrics sklearn.tree explained_variance_score DecisionTreeRegressor division r2_score ShuffleSplit seaborn numpy AdaBoostRegressor RandomForestRegressor GradientBoostingRegressor scipy.stats sklearn.learning_curve time linear_model sklearn.ensemble ModelComplexity sklearn sklearn.model_selection matplotlib.pyplot cross_validation pandas pearsonr __future__ sklearn.linear_model sklearn.cross_validation tree train_test_split LinearRegression ", "entities": "(('validation', 'Visual aesthetics'), 'see') (('it', 'data'), 'measure') (('It', 'real sense'), 'mean') (('It', 'house'), 'yr_renovated') (('that', 'different maximum depths'), 'STEP') (('data Again points', 'graceful curve'), 'suggest') (('etc', 'unique categorical variables'), 'be') (('It', 'house'), 'bedroom') (('even assumption', 'certain fashion'), 'bias') (('However model', 'data as close point'), 'use') (('house', 'which'), 'date') (('we', 'sales price prediction'), 'correlate') (('total floors', 'house'), 'floor') (('It', '1 to 11'), 'grade') (('fitted model', 'training dataset'), 'provide') (('Now we', 'dataset'), 'imply') (('sqft_basement It', 'house'), 'determine') (('Gradient Boosting', 'suitable dataset'), 'be') (('so this', 'target it'), 'price') (('long It', 'house'), 'determine') (('you', 'domain knowledge personnel'), 'be') (('data', 'values'), 'step') (('28', '0'), 'achieve') (('model', 'other words'), 'underfitte') (('It', 'house'), 'yr_built') (('we', 'machine learning generally models'), 'let') (('measurement also which', 'lot'), 'sqft_lot') (('data points', 'predictor isn t enough information'), 'follow') (('how it', 'target variables'), 'eda') (('model', 'performance_metric function'), 'curve') (('structure', 'data'), 'before') (('Thus it', 'overfitting'), 's') (('sqft_living15 Living room area', 'renovations'), 'imply') (('description', '1'), 'give') (('positively negatively they', 'model building process'), 'price') (('test dataset', 'model'), 'have') (('It', '1 to 5'), 'condition') (('They', 'specific variables'), 'differ') (('Then you', 'test dataset'), 'test') (('It', 'house'), 'bathroom') (('It', 'house'), 'lat') (('It', 'apart basement'), 'sqft_above') (('It', 'house'), 'zipcode') (('It', 'May'), 'include') (('measurement which', 'square foot'), 'sqft_live') (('it', 'overfitting'), 'increase') (('it', 'too far acceptable level'), 'be') (('then you', 'correct responses'), 'minimize') (('original dataframe', 'convinience'), 'comment') (('best possible linear model', 'wrong training'), 'be') (('you', 'more confidence'), 'have') (('we', 'King County Seattle'), 'welcome') (('We', 'two'), 'keep') (('it', 'data'), 'have') (('house', 'waterfront'), 'waterfront') (('Further we', 'tuning fine hyperparameters'), 'perform') (('training score', 'high variance'), 'suffer') (('we', 'target variable i.'), 'correlation') (('we', 'unseen best data'), 'generalize') (('remaining', 'model'), 'split') (('dataset', '21613 observations'), 'cantain') (('graph', 'validation'), 'produce') (('it', 'enough data'), 'be') (('test be', 'usually same predictors'), 'datum') (('accuracy', '89'), 'conclusion') (('it', 'wrt accuracy'), 'MODEL') ", "extra": "['biopsy of the greater curvature', 'test']"}