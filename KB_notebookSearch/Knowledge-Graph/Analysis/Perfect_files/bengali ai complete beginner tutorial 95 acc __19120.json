{"name": "bengali ai complete beginner tutorial 95 acc ", "full_name": " h1 NOTE IF you are looking at this like a beginner s tutorial please read every line carefully Download and edit for better view h3 While going through tutorials we all have been there getting bored and skipping lines just to get to the core of it includig me but lees we hat the core is in the lines I ll try my best to provide as much information as I can about the code and process because this is what I want to learn and how I have learned what is this line of code doing why this parameter how is this happening why did we do this and all the waves of questions that arise from the very process of implementation of a code be it a simple Hello World or a very Deep Network h2 Another Note h4 I just assume that you know and have practiced a bit of if you are here I m sure you know it all in Python 3 Numpy jupyter lab Pandas and basics of Machine Learning sklearn Deep Learning Neural Networks Convolution Neural Networks keras If not just in case I highly recommend learning these in the order h3 Last Important Note h4 This might not be the most elegent notebook and efficient code out there but WE as a beginners care less about the efficiency but the leraning curve I myself have self studued all the concepts by MYSELF withthe help of this online coding community so here is a little effort for that coomunity and the Data Science enthusiasts yet to come If you see something that you can not understand in a loop method just implement it pieces by pieces yourself You ll develop the core understanding of the libraries as well as the process of how to do it If I can save one fellow coder a trouble I de be repaying my debt to the community Please feel free to correct comment and contact me if you see anything wrong or something that can be replaced with something better as I am a learner myslf too h2 The Problem h2 Solution h2 Import Libraries h2 Files Paths Outputs h2 Importing Images h2 Labels info h2 Plotting h1 Model h3 NOTE h2 Custom metric h2 Compiling h2 Callbacks h2 Helper Functions h3 Resize h3 Generator h2 Training h2 Garbage Collection h2 Predictions and Submissions h2 What next h2 What Next h3 Thank you all for coping with my gramatical errors ", "stargazers_count": 0, "forks_count": 0, "description": "com kaggle docker python For example here s several helpful packages to load in linear algebra data processing CSV file I O e. Import LibrariesImport all the needed libraries all at one place so that you don t have to be confused. Let us just get the summary of layers and activation functions as they appear in our network. I chose to go for second. So instead of viewing the results from the original images for EDA perform it on the grapheme column and view some results by converting characters to images using matplotlib Below is a code just to see that there is no difference betwwen the two. Few of the videos on Keras are available here https www. Why do we need to Good one So common that even Facebook has the answer literally. com channel UCYO_jab_esuFRV4b17AJtAw playlists view 50 sort dd shelf_id 20 and Fundamentals of Deep Learning and Neural Networks https www. com profile Yoshua Bengio on Practical recommendations for gradient based training of deep architectures https arxiv. Not Fair And I can not give you a whole lot of Deep Learning in this mere notebook but before we start into the solution I highly rcommend and I repeat I HIGHLY RECOMMEND to watch these Neural Network series https www. number of unique classes in each number. Normalization BatchNormalization Normalize the activations of the previous layer at each batch i. There is not panacea. Pooling MaxPool2D Pooling layers provide an approach to down sampling feature maps by summarizing the presence of features in patches of the feature map. 9 will be outputted to 0 but 1 2 76 0. This is not necessary but it is to show that dataframe has very good method for showing important values we will set the file path in arguments get the character used in glapheme set the figure size s dimensions to width 10 height 3. We have many reasons for these but the main 2 reasons in our case are computational power and memory. cmap gray depicts Black White second subplot of the 1 row 2 columns get the properties of the fonts plot the text as an image generalises the function for any df so a bit slow Input layer just takes into account of the size of the input filter size is 64 and kernal is a 3 3 matrix. Out of Notebook question 0 marks scored. just for training data these are the final dimensions of an image. Overfitting is cramming in a sense. There are 2 methods that I could come up with. So Check the the data under your commit histories and you can use the data of that kernal for a new kernal. If given enough time data and wrong label to a network it ll learn to classify dog as a car with 100 accuracy So if it learn to classify a dog that is in the center of image everytime facing right it won t be able to classify the same dog in corner or facing left and so on. epoch is the iteration for which we will train the model. Categorical loss is used as the loss function. First one is image_id and from the second column to last in our case it is marked as 0 32331 we have pixels of image ranging in 0 255. If you have any queries you can check the keras official github keras documentation. For images we use 2D and for videos and sound 3D. I think the later one got your attention. We can get a very good result by losing a very little information. Last Important Note This might not be the most elegent notebook and efficient code out there but WE as a beginners care less about the efficiency but the leraning curve. There is not BEST size our model processes data in the batches. Simply N D tensor collapsed into 1 D. 15 will zoom in or out the image propotional to original image width_shift_range 0. 00001 and give us little insight on what has happened verbose 1 stop the model from fitting data if validation loss has not decreased by 0. Importing Images DO NOT IMPORT ALL OF. We can use multiple metrices and define each output layer s loss and metric seperately by using the loss layer_1_name list of metrices. Repeat Garbage Collection Even though Grabage collection is done automatically in Python by the gc and in periods but we ll use the gc. com keras team keras issues 10306 We can also define our own custom metrices and loss functions. We could also use Relu as layer too instead of activation relu momentum 0. For example a tensor samples 10 20 1 will be flattened to samples 10 20 1. NOTE IF you are looking at this like a beginner s tutorial please read every line carefully. I ll try my best to provide as much information as I can about the code and process because this is what I want to learn and how I have learned what is this line of code doing why this parameter how is this happening why did we do this and all the waves of questions that arise from the very process of implementation of a code be it a simple Hello World or a very Deep Network. This is not a good practice though I did it just to show how to do what I ll do later in one go Labels infotrain_classe will tell us how many classes are there what are the min max classes and some other information about the Data distrubution on the data we have Plotting This function plots the images side by side. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. out_1 because it s recall matters twice empty dictonary which stores the values of pixels for each image iterate through all the images in dataframe apply resize transformations on per row basis reshape accordingly resizing swaps the rows to columns so Transpose sets to default all the labels array will be concatenated in this single array define a dict which maps the key y1 y2 etc to lengths of corresponding label_array to store the ordering in which the labels Y were passed in this class for the first time loop it s empty so insert first element concat each array of y_labels key lengths will be different for different range of classes in each class due to_categorical ONE HOT encodings. Input Layer Input The input layer of a neural network is composed of artificial input neurons and brings the initial data into the system for further processing by subsequent layers of artificial neurons. What it does is to take all the image files either from directory or in form of rank 4 metrices number_of_pictures width height channel and maps to the corresponding labels. nothing saving nothing is like committing when offline and preparing a log if crash happens load the best weights so far so that we don t have over fitting by the time the mode lhad stopped model predictions placeholder row_id place holder. Ex some have 2 classes red yellow but other can have 4 Audi BMW Ferrari Toyota so we have to keep track due to inner working of super. ttf file is a font file format created by Apple but used on both Macintosh and Windows platforms. In my case it has been a journey of luck. While going through tutorials we all have been there getting bored and skipping lines just to get to the core of it includig me but lees we hat the core is in the lines. com and obviously you can always follow and ask questions on Stackoverflow https stackoverflow. Each color has it s own values in case of 3 channel images but in grayscale we just have one channel so we get B W pictures. it searches for parameters in batches. I have nothing to get from this but I had a sudden revealation from these such as Neural Networks JUST find the patterns and we have high hopes that they ll find and to our surprise they even just do. You ll develop the core understanding of the libraries as well as the process of how to do it. backend For Later CompilingUnlike other frameworks like PyTorch Keras is not so pythonic in case of dynamic behaviour. e MetricToWatch_LayerName ReduceLROnPlateau will reduce the leraning rate of optimizer by a defined factor if any of the output metric average metric hits plateau. The input layer is the very beginning of the workflow for the artificial neural network. That is why we had ordered_labels and key_lengths It ll extract the elements ordering vise else there will be conflict just a random dataframe nothing much columns to be dropped after merging with dataframe so that we can have only numerical pixel values calling garbage collector to free up memory from last delete Pandas automically assigns the correct columns by looking at the values in image_id so we don t have to worry about alignment if the image_id in train_classes are correct to the respect classes get the numerical values of classes and convert them to categorical so that 0 1 2 becomes 1 0 0 0 1 0 0 0 1 where the size of the later depends on the maximum number that is present in the original matrix. NN don t even know what they found. Dropout Dropout Dropout Layer will drop n proportion of values it gets from the previous layers. EarlyStopping is used to avoid resource wastage basically. For example running this by clicking run or pressing Shift Enter will list all files under the input directory Any results you write to the current directory are saved as output. If not just in case I highly recommend learning these in the order. NN CNN works on finding and memorizing the patterns like we humans do but subconciously. read_csv plotting to plot the font see progress bar Darw picture from font Resizing of image resizng of image image augmentation on training images ONLY splitting the data for custom metrices implementations and other processes that we define keras layers Model class Call backs acts like milestones and if else while model is being trained garbage collector Input data files are available in the. First method can not perform well on or above images which are resized as 64 64 but with second we lose the element of randomness as we just have 1 4 of the original sample to choose from. Position of 1 defines the presence of the elemnt in the list. You can access flattened layers by model. 15 means move the mean and variance by a factor of 0. IF Dropout is set to. Sequential Model Easy to implement but not customizable and using Functional API Very customizable and relatively not so difficult to implement. IT LL MAKE YOUR KERNAL CRASH AT ONE POINT OF TIME LATER. Compilation will calculate all the required numbers of parameters to train the loss function to use assign them to each layer get a memory space from os assign accuracy metric etc. You an not load all the images resize reshape train test split and fit the model. Rather than computing learnable parameters and tweaking them using gradients Search for it in case. Please feel free to correct comment and contact me if you see anything wrong or something that can be replaced with something better as I am a learner myslf too. It ll stop the model from training if given parameters like accuracy is not improving or loss is not decreasing significantly. com questions 59603353 using multi output labels in keras imagedatagenerator flow and using model fit and custom metric in keras example https datascience. org on deep learning requires rethinking https arxiv. In our case the organizers have already given us the metric on which they ll be evaluating our scores so why not to use it from start when we will be judged in the end on this basis Wish we could have same for the life too. Right So we will be asking from our models to do the common work so that we can use the common works done by common the lower layers and use it in out output. So we ll be using Functional API. For that we ll build a Neural Network and more specifically a Convolution Network. Classes ranges are in different. It can be resized to any size without losing quality and looks the same when printed as it does on the screen. using LeakyRelu as a new layer. We need these names as they act as the keys for mapping output to each later. I am just trying to give you the best of I have after searching through tons of tutorials and blogs. We have 32333 columns. com jeffheaton s research on The Number of Hidden Layers https www. Put any value of i that exists in df. If I can save one fellow coder a trouble I de be repaying my debt to the community. 1 LeakyRelu LeakyRelu Leaky version of a Rectified Linear Unit ReLu. Only if the 3 labels are independent of each other and do not affect each other s output in any sense We can build our model by 2 different styles. com 2017 06 01 hidden layers. Another Note I just assume that you know and have practiced a bit of if you are here I m sure you know it all in Python 3 Numpy jupyter lab Pandas and basics of Machine Learning sklearn Deep Learning Neural Networks Convolution Neural Networks keras. 5 given if learning rate is above 0. And some amazing visual explanations of Working of Gradients Significance of Eignveectors and EignValues and why and how are they used How Convolution Pooling ACTUALLY works and so on. Import data resize merge in 4 iterations and then train the whole data at once Import data resize train in 4 different iterationsEach method has its own drawback. The moment you need some new library you come up here and import it Files Paths OutputsIf you have any problem while finding a file data and path you can find it easily on the screen printed above. The community still stands divided on this whether to delete and force collector free up memory. Kaggle saves the output only when you commit and can be accessed as the output of that successful comitted version. We define the significance ModelCheckPoint is used to save the whole model or just the weights if our model improves by the criteria of improvement defined. shape 0 in our case split the data in training and test to cross validate our model s performance. Accuracy is the metric we are using. It is not used on the output layer. It means that if validation loss is not improving or if accuracy is not improving decrease the learning rate. Max Pooling calculate the maximum value for each patch of the feature map or in very very simple words for example it ll return a Maximum value of each 2 2 matrix Max 00 01 10 11 that is present in each N N it encounters IFF step 1. You can follow Medium blogs and one of my favourites blogs is Machine Leraning Mastry https machinelearningmastery. parquet FILES AT ONCE. 5533 and Jeff Heaton https github. We are using callbacks as whole but can use these on each individual output layers too as monitor val_loss_out_1 i. More epochs than required can lead model to overfitting. batch 1 single image train_classes have our respective label for every image import any random image file given in input print the first 5. com playlist list PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU where mazing people giving you INSIGHTS of what happens ACTUALLY INSIDE the mathemetics of something how things work what are PRACTICAL SIGNIFICANCE of formulas we use and all the things. Sometimes I get the freed space sometimes I don t but we are forcing the garbage collector to collect and free up space for the sake of it. index We will delete this dataframe later and import it again. I myself have self studued all the concepts by MYSELF withthe help of this online coding community so here is a little effort for that coomunity and the Data Science enthusiasts yet to come. com and Cross Validated https stats. basic info such rows columns each one s data type and memory used there are 5 columns corresponding to each image. com watch v bwb4r3UVKko SolutionWe are trying to find the root vowel and consonent in each image that will be presented to us. As we have a continuous values of pixels we have to convert it to the dimensions of the given input already given to us. NOTE Before you go on searching and wondering about why these many or so less layers why these many numbers of neurons why droput is set to be at this fraction why normalization and why why not s of ocean these are just trial and error parameter tunings and testing with permutations which cost us very less computational power and time. collect method to force the collector but it is thought to be a bad practice to use del because it removes the reference of that object from the memory. Predictions and Submissions What next What Next From this point onwards you can make the models more robust and efficient using different techniques such as adding more layers until the model reaches saturation point using dimentionality reduction techniques such as PCA ISOmap to reduce the overhead by keepong the most significant data pixels and tweaking the parameters of your model. 15 will shift the width 0. 1 will be outputted as they are 1 2 76 0. Each image has a unique image_id and belongs to atleast one of the 3 classes. With a Leaky ReLU you won t face the dead ReLU or dying ReLU problem which happens when your ReLU always have values under 0. Wish you all the best Thank you all for coping with my gramatical errors. In other words these layers summarize the presence of features in an input image or extract features. html Custom metricIn keras we can implement a custom metric as only few of the common metrices are available though you can get those metrices later too. It is simply a measure of how accurae we were in predictions. Out N subplots will acquire the areas this area accordingly each one having same area first subplot of the 1 row 2 columns subplots set title of the first subplot show the image which is as index i of the input DataFrame. For example if 1 3 is original array then one hot encoded array will look something like 0 0 0 0 0 1 0100 2 0 0 0 0 3 0 0 0 1 NOTE this is not the actual representatin but the values are correct. Optimizers help in reducing the number of computations drastically. You have to check it for yourself and see what fits better. I can not plot a 2 D array in s remove all the columnsthat are not of use resize the images and then divide each pixel by 255 to limit the range from 0 255 to 0 1 and convet as float32 memory efficient reshape into rank 4 matrix 1 checks for the best dimension. To get the best of your model and knowledge read more apply the most. Not getting it right Well for example Flatten will take a tensor of any shape and transform it into a one dimensional tensor plus the samples dimension but keeping all values in the tensor. Flatten Flatten Flatten layer performs a flatten operation on a tensor reshapes the tensor to have the shape that is equal to the number of elements contained in tensor non including the batch dimension. It allows a small gradient when the unit is not active. com questions 45165 how to get accuracy f1 precision and recall for a keras model CallbacksCallbacks are a method of invisibly inserting our commands basically if else conditions to show stop or do something when some conditions are met. For example if the values from the previous layers in range 100 to 100000 it ll normalize the entire batch. We resize the image so that it fits in out memory and uses less computational power. You have to compile it before you can train your model. TrainingWe do our work in loop. Simply it ll tell us the average difference between the classes that were actually there and those were predicted by the model and if we get a difference in the values scold our model and ask every neuron in process to tweak their output so that the resulting somehow becomes the original. 137 236 32332 pixels take more memory and computational power than that of 74 74 5476 pixels. Sotmax activation softmax Softmax activation function gives the probability of each class. Model For this problem we have to answer what is better Taking 3 different vehicles or sharing a common with friends when all we have to do is to go to a common place common theatre but side by side seats. Obviously we lose a bit of information but the ratio between computation memory to the performace information will be decreased greatly. This process goes from the output layer to input layer and is called Backpropagation. Let us suppose that in each class in our problem we have 169 11 7 classes respectively for each label we get a probability the class assigned to the image will be the class with the highest probability. If you see something that you can not understand in a loop method just implement it pieces by pieces yourself. We are using Adam as an optimizer by defining optimizer adam. 03530 tells that a Neural Network crams the wrong data too much that it tells them correctly. One class has over 160 values but second one merely has 7 classes. Download and edit for better view. We need custom metrices because it is the problem s demand. So we use 137 236 specified by one who gave us the data. com playlist list PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL. 4 it ll randomly drop the weights 40 of neurons which were provided by the previous layer. You can follow custom loss function example https stackoverflow. We can pass in the images ourselves too but what this generator best is that it augments the images. We will count the total execution time of this script This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github. flow final dictonary that ll be yielded keeps count of the ordering of the labels and their lengths Extract to from the range of length of labels values. ReLu activation relu ReLu will convert every negative value to 0 and use the ive value as it is. But for more details on this one you can read research paper from Yoshua Bengio https www. Helper Functions ResizeWe need to resize our images. See all the important description about classes. We do not have to apply a lot of load because we have memorized and learned to memorize the patterns from billions of years of evolution and we are trying to teach this this to computers within what a hundred years or less. padding SAME means that at each side o s are padded to each image so that the corner bits get same attention as the middle ones. We want model to show or do something while in training we do not want to interrupt training we use callbacks. If you have queries regarding what and why about this Layer model you can see the stackoverflow answer https stackoverflow. 0025 in the last 5 epochs and restore best weights for the next time save the weights in a file name specified only if the validation loss of out_1 layer has improved from last save. It ll be equal to number of elements or df. Must read paper from Chiyuan Zhang http pluskid. Dropout may be implemented on any or all hidden layers in the network as well as the visible or input layer. 15 Get maximum value of every 2 by 2 matrix present in matrix it gets and return a new matrix of these values Drop weights from 30 of neurons from previous layer Convert into 1 D tensor names of output layers. To get a better understanding of how does all of this makes sense just visit this link https www. Layers in Keras are by default named as layer_type_position suc as first Conv2d and Dense are named as conn2d_1 and dense_1 unless specified by name something_else in layer. The ProblemWe have a set of almost 250000 single channel gray scale handwritten images of dimensions 137 236 converted to digital format and then converted to pixel values where pixel values range from 0 255 after exploring the dataset we ll know and saved as the 32332 columns 137 236 32332 Every color is made up of Red Green Blue 3 channels and in computers we represent these colors by numbers ranging from 0 255 where 0 represents White and 255 represents Black. At each epoch same images will be shown to model and the weights and biases will be adjusted accordingly. random_state produces the same data every time for reproducibility so it is kind of pseudo randomness featurewise_center would have set input mean to 0 over the dataset if stated True samplewise_center would have set each sample mean to 0 if stated True featurewise_std_normalization would have divided inputs by std of the dataset if set True samplewise_std_normalization would have divided each input by its std if True rotation_range 8 will rotate the image in range 8 degrees zoom_range 0. You can surely try different dimensions. Last columns is what tell us which grapheme is used in image. Convuolution Layer Conv2D Convolutional layers are the layers where filters are applied to the original image or to other feature maps in a deep CNN. Metrices is keras can be implemented using keras. For example in case of Cancer detection we can use Recall rather than accuracy because a False Positive model says you have Cancer when you actually don t will lead to few more tests from doctors but a False Negative model says you are fit but you are not rally will lead to a delayed treatment that can be harmful. For this very demo you can see a very good example of using muti loss multi metrics for multi output NN https github. 15 points respective to total width this will JUST calculate parameters required PCA ZCA and others if True no transformations performed gives you 1 at which epoch model stopped due to early stopping labels are givesn as a dictonary parameter to the fit_generator because it is multi output problem. One from the DataFrame and other one from the grapheme used in. It can resize crop shift flip rotate iamges for us and so on. You can also use the Adam imported already and set the learning rate. Neural network is no magic but it just learns the images somehow. So to make our model robust we need augmentations. So we ll be applying the process as1. We can use any number of callbacks but we ll be using 3 in our case. com questions 59420425 meaning and working of a function fvariable v a function followed by varia regarding this one. fit final line of our model construction code tell the system only the start and end it ll find the path if validation loss of out_1 is not decreasing for 3 consecutive epochs decrease the learning rate by 0. Not in this case Try doing it. Generator ImageDataGenerator is a very good tool for generating and augmenting images. Every neuron involved in producing that final value will ask every other previous value to tell teak parameters. It is a method of preventing Overfitting. ", "id": "deshwalmahesh/bengali-ai-complete-beginner-tutorial-95-acc", "size": "19120", "language": "python", "html_url": "https://www.kaggle.com/code/deshwalmahesh/bengali-ai-complete-beginner-tutorial-95-acc", "git_url": "https://www.kaggle.com/code/deshwalmahesh/bengali-ai-complete-beginner-tutorial-95-acc", "script": "Flatten keras.preprocessing.image PIL.Image skimage.transform tqdm # see progress bar keras.layers Model #Model class keras.callbacks tqdm.auto EarlyStopping Dropout MaxPool2D plot_comp resize_image Conv2D numpy resize # Resizing of image Input ReduceLROnPlateau flow keras.backend resize as cv2_resize # resizng of image ModelCheckpoint sklearn.model_selection CustomDataGenerator(ImageDataGenerator) Adam #optimizer matplotlib.pyplot Dense LeakyReLU # keras layers pandas keras.optimizers resize ImageDataGenerator  # image augmentation on training images ONLY BatchNormalization train_test_split  # splitting the data matplotlib.font_manager keras.models cv2 ", "entities": "(('backend', 'dynamic behaviour'), 'be') (('you', 'Yoshua Bengio https www'), 'for') (('weights', 'epoch same images'), 'show') (('final value', 'teak parameters'), 'neuron') (('we', 'Neural Network'), 'build') (('we', 'augmentations'), 'need') (('we', 'it'), 'get') (('accuracy', 'learning rate'), 'mean') (('32331 we', '0'), 'be') (('True rotation_range', 'zoom_range'), 'produce') (('where filters', 'deep CNN'), 'be') (('right it', 'left'), 'give') (('EarlyStopping', 'resource wastage'), 'use') (('actual values', '0'), 'look') (('that', 'batch dimension'), 'perform') (('You', 'as well how it'), 'develop') (('we', 'case'), 'use') (('second one', 'merely 7 classes'), 'have') (('it', 'value'), 'convert') (('We', 'optimizer adam'), 'use') (('255 where 0', '255 Black'), 'have') (('So even Facebook', 'answer'), 'need') (('corner bits', 'middle ones'), 'mean') (('you', 'output'), 'list') (('It', 'elements'), 'be') (('you', 'keras documentation'), 'github') (('main 2 reasons', 'case'), 'have') (('delete collector', 'memory'), 'stand') (('that', 'original matrix'), 'be') (('We', 'custom also own metrices'), 'issue') (('BEST model', 'batches'), 'be') (('Numpy jupyter lab 3 Pandas', 'Deep Learning Neural Networks Convolution Neural Networks keras'), 'note') (('you', 'metrices'), 'implement') (('KERNAL CRASH', 'TIME LATER'), 'MAKE') (('we', 'side common seats'), 'model') (('process', 'input layer'), 'go') (('It', 'python docker image https kaggle github'), 'count') (('Position', 'list'), 'define') (('obviously you', 'Stackoverflow https stackoverflow'), 'com') (('that', 'activation standard close 1'), 'apply') (('these', 'final image'), 'be') (('Convolution Pooling How ACTUALLY', 'Eignveectors'), 'explanation') (('does', 'corresponding labels'), 'be') (('I', 'order'), 'recommend') (('64', 'input filter size'), 'depict') (('You', 'custom loss function example https stackoverflow'), 'follow') (('It', 'Overfitting'), 'be') (('they', 'surprise'), 'have') (('any', 'plateau'), 'reduce') (('Dropout', 'network'), 'implement') (('We', 'very little information'), 'get') (('I', 'tutorials'), 'try') (('such rows', '5 image'), 'column') (('You', 'model'), 'access') (('when conditions', 'something'), 'com') (('Optimizers', 'computations'), 'help') (('that', 'labels values'), 'keep') (('beginners', 'less efficiency'), 'note') (('what', 'yourself'), 'have') (('that', 'delayed treatment'), 'use') (('best it', 'images'), 'pass') (('Few', 'Keras'), 'be') (('Sotmax activation softmax Softmax activation function', 'class'), 'give') (('who', 'data'), 'use') (('it', 'custom metrices'), 'need') (('One', 'other grapheme'), 'use') (('array', 'class'), 'concatenate') (('Pooling MaxPool2D Pooling layers', 'feature map'), 'provide') (('index We', 'later it'), 'delete') (('activation they', 'network'), 'let') (('You', 'favourites blogs'), 'follow') (('More epochs', 'overfitting'), 'lead') (('Normalization BatchNormalization', 'batch i.'), 'Normalize') (('Import data once resize', 'own drawback'), 'merge') (('we', 'callbacks'), 'want') (('layers', 'features'), 'summarize') (('garbage collector Input data files', 'the'), 'see') (('just model', 'improvement'), 'define') (('You', 'learning already rate'), 'use') (('only when you', 'successful comitted version'), 'save') (('we', 'PRACTICAL formulas'), 'list') (('character', '10 height'), 'be') (('validation loss', '0'), '00001') (('too much it', 'them'), 'tell') (('validation only loss', 'last save'), 'save') (('which', 'very less computational power'), 'note') (('It', 'us'), 'resize') (('Generator ImageDataGenerator', 'very good images'), 'be') (('ttf file', 'Macintosh platforms'), 'be') (('Flatten', 'tensor'), 'get') (('15', 'propotional original image'), 'zoom') (('it', 'luck'), 'be') (('Helper Functions ResizeWe', 'images'), 'need') (('we', 'life'), 'give') (('core', 'lines'), 'be') (('we', '3D.'), 'for') (('you', 'new kernal'), 'check') (('you', 'line'), 'NOTE') (('Categorical loss', 'loss function'), 'use') (('com channel UCYO_jab_esuFRV4b17AJtAw playlists', 'Deep shelf_id Learning'), 'view') (('so we', 'super'), 'have') (('mode lhad', 'model predictions placeholder'), 'be') (('it', 'less computational power'), 'resize') (('sense', 'link https just www'), 'get') (('10 20 1', 'samples'), 'sample') (('Layer about you', 'stackoverflow answer https stackoverflow'), 'see') (('resulting', 'output'), 'tell') (('we', 'model'), 'be') (('which', 'input'), 'show') (('they', 'later'), 'need') (('which', 'previous layer'), '4') (('shape', 'performance'), 'split') (('Input Layer input layer', 'artificial neurons'), 'input') (('it', 'previous layers'), 'drop') (('we', 'already us'), 'have') (('We', 'too monitor'), 'use') (('model', 'model'), 'prediction') (('you', 'easily screen'), 'moment') (('so here little effort', 'coomunity'), 'studue') (('it', 'just images'), 'be') (('we', 'a hundred years'), 'have') (('loss', 'accuracy'), 'stop') (('I', 'HIGHLY Neural Network series https www'), 'fair') (('100 to 100000 it', 'entire batch'), 'normalize') (('input layer', 'neural artificial network'), 'be') (('you', 'output NN https multi github'), 'see') (('better I', 'something'), 'feel') (('keras', 'keras'), 'be') (('class', 'highest probability'), 'let') (('it', 'fit_generator'), 'calculate') (('that', 'us'), 'v') (('we', 'original sample'), 'perform') (('org', 'https arxiv'), 'require') (('images', 'model'), 'load') (('it', 'code'), 'try') (('remove', 'best dimension'), 'plot') (('it', 'IFF step'), 'calculate') (('Drop', 'output layers'), '15') (('when it', 'screen'), 'resize') (('we', 'side side'), 'be') (('humans', 'we'), 'work') (('ratio', 'performace information'), 'lose') (('we', 'gc'), 'Collection') (('15', '0'), 'mean') (('We', '2 different styles'), 'be') (('image random file', 'first 5'), 'batch') (('you', 'pieces'), 'implement') (('validation loss', '0'), 'fit') (('you', 'model'), 'have') (('you', 'Import needed all one place'), 'LibrariesImport') (('when unit', 'small gradient'), 'allow') (('grapheme', 'image'), 'be') (('when ReLU', '0'), 'win') (('I', 'community'), 'save') (('simply how we', 'predictions'), 'be') (('we', 'B W pictures'), 'have') (('we', 'out output'), 'ask') (('137 236 32332 pixels', '74 74 5476 pixels'), 'take') (('We', 'activation relu too instead momentum'), 'use') (('it', 'memory'), 'collect') (('image', '3 classes'), 'have') (('Compilation', 'metric etc'), 'calculate') (('I', '2 that'), 'be') (('We', 'metrices'), 'use') (('Layers', 'layer'), 'be') (('later one', 'attention'), 'think') ", "extra": "['test', 'bag']"}