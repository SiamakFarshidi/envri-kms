{"name": "notebook1cc9bab3f3 ", "full_name": " h1 Deep Learning in Biomedical Engineering h2 Assignment 2 h3 Due date 11 59 pm Febraury 25 2021 h2 Seyedhamidreza Alaie sa3724 h2 Introduction h2 Instructions h2 How to submit h3 1 5 pts According to the CIFAR10 dataset descriptions and other online resources please identify the quantities below h1 1 B Therefore total number of classes are 10 shown above h2 c therefore the size of each image is 32x32 above h3 2 0 pts Import the required packages in the following code block h3 3 5 pts Load train and test sets using Pytorch datasets functions h3 4 10 pts Using the matplotlib library make a figure with N times4 grid cells where N is the number of classes Display one random sample of each class pulled from the train set in its corresponding row that depends on its class index and the first column and its histogram in the second column Repeat this for the third and fourth columns but pull images from the test set h3 5 5 pts Split up the train set into new train and validation sets so that the number of samples in the validation set is equal to the number of samples in the test set Then create a DataLoader for each set including the test set with a batch size of 32 h3 6 5 pts Display the number of samples for each class in the train validation and test sets as a stacked bar plot similar to the FirstTutorial h3 7 10 pts According to the LeNet architecture below create a fully connected model Also identify the architeture s hyper parameters activation functions and tensor shapes h3 8 5 pts Create an instance of ADAM optimizer with an initial learning rate of 0 0001 and an instance of Mean Squared Error MSE loss function Briefly explain the ADAM optimizer algorithm and the MSE loss function h3 9 15 pts Train the model for 200 epochs using the train set and validate your model s performance at the end of each epoch using the validation sets h3 10 5 pts Display the learning curve and illustrate the best epoch Explain your criteria for the best epoch h3 12 5 pts Display five random samples of each class titled with the true label and the predicted label Comment on your model s performance h3 13 20 pts Repeat the training validation and testing with the Cross Entropy loss function and initial learning rate of 0 005 Explain how the model s performance changed ", "stargazers_count": 0, "forks_count": 0, "description": "To evaluate the proformance on the test data I used the following. Please remove the comments before filling the blocks. Load the model s weights at the best epoch and test your model performance on the test set. Repeat the training validation and testing with the Cross Entropy https pytorch. Copy the Public URL 4. Download your completed notebook as a. How to submit After you have completed the assignment 1. Make sure the intensity range is between 0 1 and images are stored as tensor type. Display the learning curve and illustrate the best epoch. According to the LeNet architecture below create a fully connected model. Don t comment on every detail. This is consitent with the knowledge that logistic regressions and therefore classification is advantageous for classification of linearly seperable data. Howevr I implemented the first oneTest dataset does not need the aforementioned reshaping because is not used in the training loop. However It is my understanding that kernel 5 is what is asked for given the input size is 32x32x3. To increase the training speed use the GPU accelerator. You may use transformers while downloading the dataset to do the job. ipynb files on the CourseWorks https courseworks2. Just remember they are using a different architecture and they are using TensorFlow for implementations. og for learning and code writen here IntroductionIn this assignment you will implement train and test LeNet https en. Make the saved version public Share Public 3. com soroush361 deeplearninginbme firsttutorial. To avoid overwriting your previously trained model change the save directories in the training loop. Seyedhamidreza Alaie sa3724In preparing this notebook I used the following resources https pytorch. Display one random sample of each class pulled from the train set in its corresponding row that depends on its class index and the first column and its histogram in the second column. Currently the markdown blocks have a comment like your answer here. You can always add more blocks if you need to or it just makes your answers well organized. Moreover these models are older models than CNNs. 0001 and an instance of Mean Squared Error MSE https pytorch. First I take a look at the images to ensure that they look fine 10. The MSE loss function is L y hat y frac 1 N sum_ i 1 N y_i hat y _i 2 Where y is the true value hat y is the predicted value N is the number of classes. If you are using an online code or paper make sure you cite their work properly to avoid plagiarism. The performance is worse 43 accuracy while the model has more weights than LeNEt 57 accuracy. DataLoader for each set including the test set with a batch size of 32. Import the required packages in the following code block. The code blocks are only for Python codes and comments and currently have a comment like Your code here. Write comments for your codes in the big picture mode. Using the matplotlib library make a figure with N times4 grid cells where N is the number of classes. LeNet Architecture https raw. As such the model performs resonably well. Implementing and reporting results using other architectures than LeNet will grant you an extra 20 on grade. Essentially you need to copy all the codes above and just change the loss function and edit the learning rate. CrossEntropyLoss loss function and initial learning rate of 0. You can always come back here and import another package please keep them all in the following block to make your code well organized. This is consistent with the knowledge that densenets are inefficent and inferior to CNNs for image classification. Display the number of samples for each class in the train validation and test sets as a stacked bar plot similar to the FirstTutorial https www. Looking at the misclassified samples their classification is difficult with for human. The same function was also used in the training loss. The learning curve shows the model s loss and accuracy at the end of each epoch for all epochs 200 epochs. Although in the training process softmax activation function was used based on the type of loss the network needs a softmax function for classification. png Note that a model with kernel size of 3 in the first convolution layer also works well 62 accuracy. Adam with an initial learning rate of 0. Look at this tutorial Pytorch CIFAR10 https pytorch. 57 The model performs reasonably well with a 57 accuracy. The Pytorch CIFAR10 https pytorch. html algorithms Loss function list https pytorch. One comment line at the top of each code block is necessary to explain what you did in that block. In this Kaggle Notebook https www. Explain how the model s performance changed. Then create a DataLoader https pytorch. Keep in mind that y is a one hot vector like y begin bmatrix 0 0 1 vdots end bmatrix This example of y indicates that the sample belongs to class ID 2 remember it is zero indexed and hat y begin bmatrix 0. org docs stable generated torch. Keep in mind that you identified the W H and N Which refers to the number of classes in the first question. com soroush361 deeplearninginbme firsttutorialI also used pytorch. Architecture hyper parameter includes the number of layers number of kernels in each layer size of the kernels stride zero padding size. org tutorials beginner blitz cifar10_tutorial. html We check if the datasets are transformed to tensors with 0 1 value rangeTherefore they are stored as tensr see above. Display five random samples of each class titled with the true label and the predicted label. Using this formt we can define the MSE loss L2 distance between the labels and output tensors. com soroush361 deeplearninginbme firsttutorial for more help. For your information here is the mathematics behind the ADAM optimizer For each parameter w j v_t beta_1v_ t 1 1 beta_1 g_t s_t beta_2s_ t 1 1 beta_2 g_t 2 Delta w j eta frac v_t sqrt s_t epsilon g_t w j_ t 1 w j_t Delta w j Where eta is the initial learning rate g_t is the gradient at time t along w j v_t is the exponential average of gradients along w j s_t is the exponential average of squares of gradients along w j beta_1 beta_2 are the hyper parameters and epsilon is a small number to avoid dividing by zero. Describe your criteria for choosing the best epoch I choose the epoch with the lowest validation loss MSE as the optimal choice. cuda functions to show an image get some random training images show images create a directory for the first model saving generate the CNN model train the model loop over the dataset multiple times get the inputs data is a list of inputs labels zero the parameter gradients forward backward optimize plot the validation and training loss find the epoch woth the lowesr validation loss and load that model weights load all test data into one bacth for evaluation take the first only bactch from the loader calculated the pridction lowesr L2 distance from the labels and store them in Prediction and truelabels from the test data print the confusion tableand acuracy of the test data take one batch of 256 samples from the test data plot 5 test images from each category and print the prediction and theground truth get some random training images show images create another CNN with the same architecture but differnt learning rate loop over the dataset multiple times get the inputs data is a list of inputs labels zero the parameter gradients forward backward optimize plot the validation and train loss choose the best epoch with the lowes validation loss using the lowest logit distance from the labels calassify the images and store the predictions in Prediction print confusion matrix extranet is a densnet of 5 layes without concolutional layers as the extra credit use the defult learning rate and use the crossentroppy loss train the 3rd network loop over the dataset multiple times get the inputs data is a list of inputs labels zero the parameter gradients forward backward optimize plot training and validation loss choose the poch with the lowest validation loss using the lowest logit distance from the labels calassify the images and store the predictions in Prediction print confusion matrix and accuracy. For evaluation the L2 distance between the labels and the CNN output is the metric for classification. In the majority random samples from the test dataset the model performed well. Name your variables properly that represent the data they are holding such as test_set. The criteria for the best epoch can be the minimum loss or maximum accuracy or other criteria. Look at the FirstTutorial https www. Samples must be pulled from the test set. org wiki LeNet model to classify CIFAR10 http www. Alternatively I could have change the CNN output s shape to batch 1 to match the lables. For ADAM optimizer keep other arguments as default. InstructionsDepending on each question there are empty blocks you need to fill. It also includes a dropout layer to minimize overfit. Comment on your model s performance. To fix this issue once solution is to reshape the labels such that the lables and the LeNET s output have the same shape i. Although this model is old and known to be worse than CNNs comparison is insightful. Its original shape matches the CNN output for MSE loss calculation make lists of different classes in test training and validation datasets calculate number of samples in each class and dataset If GPU available move the model to GPU. Save a version You can use Quick Save to avoid re running the whole notebook 2. com roblexnana cifar10 with cnn for beginer you may find useful information about how your outputs must look. According to the CIFAR10 http www. Explain your criteria for the best epoch. Repeat this for the third and fourth columns but pull images from the test set. Load train and test sets using Pytorch datasets functions. For more help look at this implementation https github. Deep Learning in Biomedical Engineering Assignment 2 Due date 11 59 pm Febraury 25 20211. com icpm pytorch cifar10 blob master models LeNet. Display the confusion matrix and classification report. Make sure none of the samples in the validation set exists in the new train set. html tutorial will also help you here. Also identify the architeture s hyper parameters activation functions and tensor shapes. if cuda our_CNN. e Intensity rangeA total Number of samples are 10k and 50k for test and training data as shown bellow 1 B Therefore total number of classes are 10 shown above c therefore the size of each image is 32x32 above 1 d based on the data plotted bellow it looks that class 0 airplane 1 car 2 bird 3 cat 4 elk deer 5 dog 6 frog 7 horse 8 ship 9 Truck bold text bold text 1 e searching through pixel values of 100 images shows that the value ranges from 0 o 255 bellow 2. org docs stable optim. The output shape of labels is bacth 1 whilethe output of the CNN is batch 10. However the model with cross entropy trains faster and slightly oytperforms accuracy 60. html dataset descriptions and other online resources please identify the quantities below a Total Number of samples b Number of classes c Image size d Write class names and their corresponding label index e. plot 24 images from each category seacrhing through the images and finding the lowest and highest value in each image array loading the datasets CIFAR10 checking the min max values in tensors fro sample 1 to 100 hh2 is a list of 10 lists each listing the images for the gaegories 0 to 9 hh2t is a list of 10 lists each listing the images for the gaegories 0 to 9 in the test data calculate the histogram of a numpy array function plot on random image from training and one from test data for each category and plot their histograms reshaping the labels in the train dataset from bacth 1 to bacth 10 reshaping the labels in the validation dataset from bacth 1 to bacth 10 loading the training and test datasets loading the test dataset looking at a label in he train dataset. Extra credit 3rd architecture I eimplemented the densenet archrictecture 5 dense layers without any CNNs. Upload the Public URL and the. Although you may use other available online resources such as GitHub Kaggle Notebooks it is highly recommended to try your best to do it yourself. The model misclasify birds the most 13. List item List item Kernel Size activation functions and maxpool kernel and bias are some of the hyper parameters that I dentified 8. Based on pytorch s help CrossEntropyLoss also act as a softmax layer therefore a softmax layer is not required In this case I observe similar performance for both models. However you need to create a new instance of the architecture. Both this methods should give similar results. html loss functions Explaine ADAM and MSE here 9. Create an instance of ADAM optimizer https pytorch. Using other students works is absolutely forbidden and considered cheating. com soroush361 DeepLearningInBME main Ass1_Arch1. Train the model for 200 epochs using the train set and validate your model s performance at the end of each epoch using the validation sets. The markdown blocks are only for plain or laTex text that you may use for answering descriptive questions. Split up the train set into new train and validation sets so that the number of samples in the validation set is equal to the number of samples in the test set. I imlemenetd this distance bellow for evaluation of the test data. hh is a list of 10 lists each listing the images for the gaegories 0 to 9 hh is a list of 10 lists each listing the images for the gaegories 0 to 9 looking at sample 2 in the category 1. Do not forget to save the model at the end of each epoch. For other optimization algorithms and loss functions check the links below Optimizers list https pytorch. Otherwise the weights would be the same as the last epoch or the best epoch in the last part. In other word we choose the logit distance from the lables as a mean for classification. Therefore min and max value of each tensor is between 0 and 1 above http 4. Obviously you don t need to re import the dataset and the libraries. ipynb file File Download 5. Briefly explain the ADAM optimizer algorithm and the MSE loss function. Just by looking at the architecture itself you should be able to identify the hyper parameters. MSELoss loss function. 8 vdots end bmatrix shows the probability of belonging to each class for the same sample and predicted by the model. ", "id": "seyedhamidrezaalaie/notebook1cc9bab3f3", "size": "12793", "language": "python", "html_url": "https://www.kaggle.com/code/seyedhamidrezaalaie/notebook1cc9bab3f3", "git_url": "https://www.kaggle.com/code/seyedhamidrezaalaie/notebook1cc9bab3f3", "script": "torch.nn.functional sklearn.metrics LeNet(torch.nn.Module) __init__ confusion_matrix forward torch.nn Extra_DenseNet(torch.nn.Module) numpy matplotlib.pyplot GetHistograms accuracy_score imshow ", "entities": "(('you', 'hyper parameters'), 'be') (('Howevr I', 'training loop'), 'implement') (('network', 'classification'), 'need') (('Therefore min value', 'http'), 'be') (('You', 'job'), 'use') (('loss functions', 'https pytorch'), 'check') (('they', 'images'), 'take') (('where N', 'classes'), 'make') (('learning curve', 'epochs 200 epochs'), 'show') (('I', 'optimal choice'), 'describe') (('1 images', 'tensor type'), 'make') (('therefore classification', 'linearly seperable data'), 'be') (('none', 'train new set'), 'make') (('code', 'following block'), 'come') (('we', 'labels'), 'define') (('they', 'such test_set'), 'name') (('such lables', 's same shape'), 'be') (('However model', 'faster accuracy'), 'oytperform') (('they', 'implementations'), 'remember') (('just answers', 'always more blocks'), 'add') (('output shape', '1 whilethe CNN'), 'be') (('methods', 'similar results'), 'give') (('densenets', 'image classification'), 'be') (('Essentially you', 'learning rate'), 'need') (('you', 'block'), 'be') (('LeNet', 'grade'), 'grant') (('You', 'whole notebook'), 'save') (('asked', 'input size'), 'be') (('model', 'test'), 'perform') (('that', 'first second column'), 'display') (('model', 'reasonably well 57 accuracy'), '57') (('hat zero y', 'bmatrix'), 'keep') (('model', 'also well 62 accuracy'), 'Note') (('html dataset', 'classes c Image size d Write class names'), 'identify') (('GPU', 'GPU'), 'match') (('vdots end 8 bmatrix', 'model'), 'show') (('writen here assignment you', 'LeNet https'), 'og') (('distance', 'test data'), 'imlemenetd') (('I', 'resources https following pytorch'), 'sa3724in') (('It', 'overfit'), 'include') (('criteria', 'best epoch'), 'be') (('L2 distance', 'CNN classification'), 'be') (('it', 'it'), 'use') (('worse 43 model', 'LeNEt 57 accuracy'), 'be') (('Obviously you', 'don dataset'), 'need') (('credit 3rd Extra I', 'CNNs'), 'architecture') (('Samples', 'test set'), 'pull') (('Architecture hyper parameter', 'kernels stride zero padding size'), 'include') (('Otherwise weights', 'best last part'), 'be') (('predicted N', 'classes'), 'be') (('classification', 'human'), 'be') (('number', 'test set'), 'split') (('value', 'o'), 'be') (('I', 'models'), 'act') (('inputs multiple times data', 'print confusion matrix'), 'get') (('you', 'descriptive questions'), 'be') (('However you', 'architecture'), 'need') (('hyper parameters', 'small zero'), 'be') (('we', 'classification'), 'choose') (('empty you', 'question'), 'be') (('I', 'following'), 'use') (('soroush361 deeplearninginbme firsttutorialI', 'also pytorch'), 'com') (('code blocks', 'code'), 'be') (('Moreover models', 'older CNNs'), 'be') (('same function', 'training also loss'), 'use') (('you', 'properly plagiarism'), 'make') (('to 9 hh', '2 category'), 'be') (('I', '8'), 'be') (('Which', 'first question'), 'keep') (('how outputs', 'useful information'), 'roblexnana') (('works', 'other students'), 'forbid') (('Alternatively I', 'lables'), 'change') (('tensr', 'value 0 1 rangeTherefore'), 'check') (('1 10 training', 'he'), 'plot') (('How you', 'assignment'), 'submit') (('markdown Currently blocks', 'answer'), 'have') (('Load train', 'functions'), 'dataset') ", "extra": "['test']"}