{"name": "build simple neural network using python ", "full_name": " h1 Let s understand Neural Network more Mathematically and Programmatically h3 Creation of Feed Forward Neural Networks using Python only h3 Let s design a code of single neuron with 3 inputs example h3 3 neuron layer with 4 inputs h3 Using the dot product for a neuron s calculation h3 Using the dot product with a layer of neurons h3 This error is just to show why shapes are important You can execute the below cells now h3 Data often comes in input batch h3 Example of what an array of a batch of samples looks like compared to a single sample h2 More hidden layers and non linear data h3 Matrix product with row and column vectors with a batch of inputs to the neural network h4 Q 1 How many neurons we have in the second layer h3 Extracting the data h3 Why how two or more hidden layers w nonlinear activation functions works with neural networks deep learning h4 Dense layer h3 Activation Functions h3 Softmax Activation Function h3 One hot encoded values ", "stargazers_count": 0, "forks_count": 0, "description": "There are generally 2 types of activation functions used in NN. In our case with a softmax classifier so I used categorical cross entropy. Sigmoid activation function4. Softmax Activation FunctionFor batchesSoftmax Activation CodeExponentFinal CodeCategorical Cross Entropy1. Doing dot product with a layer of neurons and multiple input4. Generally there are following types 1. And we are using Log because Log 1 is 0 and that means 100 where our Loss will be 0 and we have to approach to this value only and here we have achieve the concept of Categorical Cross EntropyCalculating LossSoftmax output array is giving out the values which which are representing the corresponding to it s class One hot encoded values. Linear activation function Last Layer for regression 3. Rectified Linear Units ReLU Activation simple code We can see the values have been clipped to 0. How to use Activation Function5. Problems with log add a very small value to the actual values like 1e 7Final Code add bias add to final Dense Layer initialize weight and biases forwards pass test class create the layer perform a forward pass see output Relu Activation Class forward pass calculate max of 0 input values Dense Layer initialize weight and biases forwards pass test class create the layer perform a forward pass perform activation of Relu see output Using e x because this function converts negative values to positive values Min value 0 Max value infinity as e x is monotonic funtion need to do e x e 2. We use activation function because is activation function is non linear it allows for neural networks with 2 or more layers to map non linear functions. 71828182846 now normalize them to simplify to single value per sample use keep_dims To prevend exploding values let s substract the max value from each array to retain the values between infinity and 1 Softmax Activation forward pass Dense Layer initialize weight and biases forwards pass test class create the layer perform a forward pass perform activation of Relu perform softmax see output our numpy solution Common loss class calculates data and reg loss given model output and truth values calculate sample losses mean loss Categorical Cross Entropy Loss forward pass number of samples in a batch clip data to remove log 0 and negative loss probs for target values if categorical labels. Use Softmax Activation function with deep learning. To have some measure of how wrong the model is we will usea loss function. What is a batch a group of input data at a time. You gonna see here is 1. 71828182846 need to do e x e 2. How non linearity comes will see later. Log 1 0 and to get ve value of Log we added minus sign because we want log function as positive2. Now we need to multiply this group of input data with group of weights both are matrix Example of what an array of a batch of samples looks like compared to a single sample. You can execute the below cells now Data often comes in input batch. Let s understand Neural Network more Mathematically and Programmatically Creation of Feed Forward Neural Networks using Python only No any other libaries are used here. More hidden layers and non linear dataA deep neural network is a neural network with 2 or more hidden layersSo for each layer we will have a different set of weights Matrix product with row and column vectors with a batch of inputs to the neural network Q 1 How many neurons we have in the second layer Extracting the data Why how two or more hidden layers w nonlinear activation functions works with neural networks deep learning Dense layer Activation FunctionsActivation function is applied to the output of a neuron which modifies outputs. Let s design a code of single neuron with 3 inputs exampleWhat if we have 4 inputs 3 neuron layer with 4 inputs Using the dot product for a neuron s calculation Using the dot product with a layer of neurons This error is just to show why shapes are important. Creation of a simple layer of neurons with 4 inputs3. One in the hidden layers and 1 in the final output layer. Creation of a basic neuron with 3 and different inputs 2. Step activation function 2. ", "id": "rahuldogra/build-simple-neural-network-using-python", "size": "3025", "language": "python", "html_url": "https://www.kaggle.com/code/rahuldogra/build-simple-neural-network-using-python", "git_url": "https://www.kaggle.com/code/rahuldogra/build-simple-neural-network-using-python", "script": "Layer_Dense __init__ Activation_Relu forward calculate numpy matplotlib.pyplot spiral_data Activation_Softmax nnfs.datasets Loss_CategoricalCrossEntropy(Loss) Loss ", "entities": "(('I', 'cross categorical entropy'), 'use') (('neural networks', 'linear non functions'), 'use') (('values', '0'), 'code') (('Use Softmax Activation', 'deep learning'), 'function') (('array', 'single sample'), 'need') (('now Data', 'input often batch'), 'execute') (('we', 'loss function'), 'have') (('which', 'outputs'), 'layer') (('just why shapes', 'neurons'), 'let') (('monotonic funtion', 'e e 2'), 'add') (('Categorical Cross Entropy loss Loss', 'categorical labels'), 'normalize') (('only other libaries', 'Python'), 'let') (('we', 'positive2'), 'add') (('which', 'class One hot encoded values'), 'use') (('batch', 'time'), 'be') ", "extra": "['test']"}