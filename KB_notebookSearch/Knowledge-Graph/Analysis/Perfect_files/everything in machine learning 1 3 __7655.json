{"name": "everything in machine learning 1 3 ", "full_name": " h2 Distance Measurements Between Data Points h3 This parameter specifies how the distance between data points in the classification clustering input is measured The options are h1 Supervised Learning h1 Generating Synthetics Datasets h1 Classification h1 Regression h1 Linear Regression h1 Linear Regression Example Plot h1 Ridge Regression h2 Ridge regression with feature normalization h2 Ridge regression with regularization parameter alpha h1 Lasso regression h2 Lasso regression with regularization parameter alpha h2 Polynomial regression h2 Logistic regression h4 Logistic regression for binary classification on fruits dataset using height width features positive class apple negative class others h2 Logistic regression on simple synthetic dataset h2 Logistic regression regularization C parameter h2 Application to real dataset h1 Support Vector Machines h2 Linear Support Vector Machine h2 Linear Support Vector Machine C parameter h2 Application to real dataset h2 Multi class classification with linear models h2 LinearSVC with M classes generates M one vs rest classifiers h2 Multi class results on the fruit dataset h1 Kernelized Support Vector Machines h1 Classification h2 Support Vector Machine with RBF kernel gamma parameter h2 Support Vector Machine with RBF kernel using both C and gamma parameter h2 Application of SVMs to a real dataset unnormalized data h2 Application of SVMs to a real dataset normalized data with feature preprocessing using minmax scaling h1 Cross validation h1 Example based on k NN classifier with fruit dataset 2 features h3 A note on performing cross validation for more advanced scenarios h2 Validation curve example h2 Decision Trees h2 Setting max decision tree depth to help avoid overfitting h2 Visualizing decision trees h2 Feature importance h4 Decision Trees on a real world dataset Breast Cancer Dataset h3 Random Forests h3 Random forest Fruit dataset h4 Random Forests on a real world dataset h2 Gradient Boosted Decision Trees h4 Gradient boosted decision trees on the fruit dataset h4 Gradient boosted decision trees on a real world dataset h2 Neural Networks h4 Activation funcitons h3 Neural Networks Classification h4 Synthetic dataset 1 single hidden layer h4 Synthetic dataset 1 two hidden layers h4 Regularization parameter alpha h4 The effect of different choices of activation function h3 Neural networks Regression h4 Application to real world dataset for classification h2 Naive Bayes Classifiers h3 Application to real world dataset h1 Evaluation for Classification h2 Preamble h4 Binarization h2 Dummy Classifiers h1 Confusion matrices h2 Binary two class confusion matrix h1 Evaluation metrics for binary classification h1 Decision functions h1 Precision recall curves h1 ROC curves Area Under Curve AUC h1 Evaluation measures for multi class classification h2 Multi class confusion matrix h1 Multi class classification report h2 Micro vs macro averaged metrics h1 Regression evaluation metrics h1 Model selection using evaluation metrics h2 Cross validation example h1 Grid search example h2 Evaluation metrics supported for model selection h1 Two feature classification example using the digits dataset h2 Optimizing a classifier using different evaluation metrics h2 Precision recall curve for the default SVC classifier with balanced class weights h2 Getting Started with Unsupervised Learning h2 Dimentionality Reduction and Manifold Learning h3 Principal Componant Analysis PCA h4 Using PCA to find the first two principal componants in the Breast Cancer dataset h3 Before applying PCA each feature should be centered zero mean and with unit variance h4 plotting the magnitude of each feature value for the first two principal componants h4 PCA on the fruit dataset for comparison h3 Manifold Learning h4 Multidimensional scaling MDS on the fruits dataset h4 each feature should be centered zero mean and with unit variance h4 Multidimensional Scaling on Breast Cancer dataset h3 t SNE on the fruit dataset h4 t SNE on the breast cancer dataset h2 Clustering h3 K means h3 Agglomerative Clustering h4 Create a dendrogram using scipy h3 DBSCAN Clustering h3 This is just the first draft version Will include some more of my own code with lots of updates in parts 2 and 3 ", "stargazers_count": 0, "forks_count": 0, "description": "intercept_ r g b y plt. amax y 1 color_list_light FFFFAA EFEFEF AAFFAA AAAAFF color_list_bold EEEE00 000000 00CC00 0000CC cmap_light ListedColormap color_list_light 0 numClasses cmap_bold ListedColormap color_list_bold 0 numClasses h 0. Pearson Correlation Use the Pearson Correlation coefficient to cluster together genes or samples with similar behavior genes or samples with opposite behavior are assigned to different clusters. t SNE on the breast cancer dataset Clustering K meansExample showing k means used to find n clusters in the fruits dataset. org wiki Minkowski_distance Supervised Learning Generating Synthetics Datasets Classification Regression Linear Regression Linear Regression Example Plot Ridge Regression Ridge regression with feature normalization Ridge regression with regularization parameter alpha Lasso regression Lasso regression with regularization parameter alpha Polynomial regression Logistic regression Logistic regression for binary classification on fruits dataset using height width features positive class apple negative class others Logistic regression on simple synthetic dataset Logistic regression regularization C parameter Application to real dataset Support Vector Machines Linear Support Vector Machine Linear Support Vector Machine C parameter Application to real dataset Multi class classification with linear models LinearSVC with M classes generates M one vs rest classifiers. Spearman Use Spearman Correlation to cluster together genes whose expression profiles have similar shapes or show similar general trends e. 7 x_0_range np. when feature values have very different ranges we ve seen the need to scale or normalize the training and test sets before use with a classifier. While these are beyond the scope of this course further information is available in the scikit learn documentation here http scikit learn. To do this the easiest way in scikit learn is to use pipelines. genes that are highly correlated and those that are highly anti correlated are clustered together. Here it doesn t work as well at finding structure in the small fruits dataset compared to other methods like MDS. org stable modules generated sklearn. scatter X_test 0 X_test 1 c y_test cmap cmap_bold s plot_symbol_size marker edgecolor black train_score clf. append patch subplot. 9 of the points corresponds to the index of the points in the X array above. 5 cm second example a larger elongated fruit with mass 100g width 6. min x_max X 0. edu ml datasets Communities and Crime Unnormalized remove features with poor coverage or lower relevance and keep ViolentCrimesPerPop target column plot k NN regression on sample dataset for different values of K make into a binary problem apples vs everything else plt. Euclidean Use the standard Euclidean as the crow flies distance. ravel P P. max y_min X 1. scatter X 0 X 1 c y cmap cmap_bold s plot_symbol_size edgecolor black subplot. Will include some more of my own code with lots of updates in parts 2 and 3 create a mapping from fruit label value to fruit name to make results easier to interpret plotting a scatter matrix plotting a 3D scatter plot For this example we use the mass width and height features of each fruit instance default is 75 25 train test split first example a small fruit with mass 20g width 4. com watch v _EEcjn0UirwMinkowski Distance https en. 76 are the closest two points and are clustered first. scatter X_fruits_2d height X_fruits_2d width c y_fruits_2d cmap cmap_fruits edgecolor black alpha. score X y test_score clf. DataFrame in str type X_fruits elif numpy. 5 x_plot_adjust 0. The options are 1. score X_test y_test title title nTrain score. Instead scaling normalizing must be computed and applied for each cross validation fold separately. set_title title if target_names is not None legend_handles for i in range 0 len target_names patch mpatches. format train_score test_score subplot. set_ylim y_min y_plot_adjust y_max y_plot_adjust if X_test is not None subplot. Euclidean Squared Use the Euclidean squared distance in cases where you would use regular Euclidean distance in Jarvis Patrick or K Means clustering. 1 y_plot_adjust 0. arange y_min k y_max k h P clf. M\u00fcller and Sarah Guido O Reilly Media. Agglomerative Clustering Create a dendrogram using scipy This dendrogram plot is based on the dataset created in the previous step with make_blobs but only 10 samples have been selectedAnd here s the dendrogram corresponding to agglomerative clustering of the 10 points above using Ward s method. show The default SVC kernel is radial basis function RBF Compare decision boundries with polynomial kernel degree 3 This code based on scikit learn validation_plot example See http scikit learn. shape if plot_decision_regions subplot. linspace 10 15 for w b color in zip clf. Pearson Squared Use the squared Pearson Correlation coefficient to cluster together genes with similar or opposite behaviors i. html def plot_class_regions_for_classifier_subplot_forndarray clf X y X_test y_test title subplot target_names None plot_decision_regions True numClasses numpy. k NN 4 which distance function https www. xlabel height plt. htmlor the Pipeline section in the recommended textbook Introduction to Machine Learning with Python by Andreas C. 5 cm we choose 5 nearest neighbors we must apply the scaling to the test set that we computed for the training set synthetic dataset for simple regression synthetic dataset for more complex regression synthetic dataset for classification binary more difficult synthetic dataset for classification binary with classes that are not linearly separable Breast cancer dataset for classification Communities and Crime dataset Communities and Crime dataset for regression https archive. legend loc 0 handles legend_handles if pandas. Validation curve example Decision Trees Setting max decision tree depth to help avoid overfitting Visualizing decision trees Feature importance Decision Trees on a real world dataset Breast Cancer Dataset Random Forests Random forest Fruit dataset Random Forests on a real world dataset Gradient Boosted Decision Trees Gradient boosted decision trees on the fruit dataset Gradient boosted decision trees on a real world dataset Neural Networks Activation funcitons Neural Networks Classification Synthetic dataset 1 single hidden layer Synthetic dataset 1 two hidden layers Regularization parameter alpha The effect of different choices of activation function Neural networks Regression Application to real world dataset for classification Naive Bayes Classifiers Application to real world dataset Evaluation for Classification Preamble Binarization Dummy ClassifiersDummyClassifier is a classifier that makes predictions using simple rules which can be useful as a baseline for comparison against actual classifiers especially with imbalanced classes. contourf x2 y2 P cmap cmap_light alpha 0. For example point 0 5. 1 plot_symbol_size 50 x_min X 0. figure figsize 6 6 colors r g b y cmap_fruits ListedColormap FF0000 00FF00 0000FF FFFF00 plt. set_xlim x_min x_plot_adjust x_max x_plot_adjust subplot. org stable auto_examples model_selection plot_validation_curve. Manhattan Use the Manhattan city block distance. plot x_0_range x_0_range w 0 b w 1 c color alpha. max x2 y2 numpy. Multi class results on the fruit dataset Kernelized Support Vector Machines Classification Support Vector Machine with RBF kernel gamma parameter Support Vector Machine with RBF kernel using both C and gamma parameter Application of SVMs to a real dataset unnormalized data Application of SVMs to a real dataset normalized data with feature preprocessing using minmax scaling Cross validation Example based on k NN classifier with fruit dataset 2 features A note on performing cross validation for more advanced scenarios. Chebychev Use Chebychev distance to cluster together genes that do not show dramatic expression differences in any samples genes with a large expression difference in at least one sample are assigned to different clusters. ndarray in str type X_fruits. Patch color color_list_bold i label target_names i legend_handles. Distance Measurements Between Data PointsThis parameter specifies how the distance between data points in the classification clustering input is measured. macro averaged metrics Regression evaluation metrics Model selection using evaluation metrics Cross validation example Grid search example Evaluation metrics supported for model selection Two feature classification example using the digits dataset Optimizing a classifier using different evaluation metrics Precision recall curve for the default SVC classifier with balanced class weights Getting Started with Unsupervised Learning Dimentionality Reduction and Manifold Learning Principal Componant Analysis PCA Using PCA to find the first two principal componants in the Breast Cancer dataset Before applying PCA each feature should be centered zero mean and with unit variance plotting the magnitude of each feature value for the first two principal componants PCA on the fruit dataset for comparison Manifold Learning Multidimensional scaling MDS on the fruits dataset each feature should be centered zero mean and with unit variance Multidimensional Scaling on Breast Cancer dataset t_SNE on the fruit datasetSome dimensionality reduction methods may be less successful on some datasets. arange x_min k x_max k h numpy. DBSCAN Clustering This is just the first draft version. increasing expression with time but whose expression levels may be very different. The proper way to do cross validation when you need to scale the data is not to scale the entire dataset with a single transform since this will indirectly leak information into the training data about the whole dataset including the test data see the lecture on data leakage later in the course. as_matrix Creating a dataset with imbalanced binary classes Negative class 0 not digit 1 Positive class 1 digit 1 Negative class 0 is the most frequent class Accuracy of Support Vector Machine classifier Negative class 0 is most frequent Therefore the dummy most_frequent classifier always predicts class 0 Negative class 0 is most frequent produces random predictions w same class proportion as training set Accuracy TP TN TP TN FP FN Precision TP TP FP Recall TP TP FN Also known as sensitivity or True Positive Rate F1 2 Precision Recall Precision Recall Combined report with all above metrics show the decision_function scores for first 20 instances show the probability of positive class for first 20 instances Plot outputs again making this a binary problem with digit 1 as positive class and not 1 as negative class accuracy is the default scoring metric use AUC as scoring metric use recall as scoring metric default metric to optimize over grid parameters accuracy alternative metric to optimize over grid parameters AUC Create a two feature input vector matching the example plot above We jitter the points add a small amount of random noise in case there are areas in feature space where many instances have the same features. ylabel width plt. Note that in general it s important to scale the individual features before applying k means clustering. min y_max X 1. Confusion matrices Binary two class confusion matrix Evaluation metrics for binary classification Decision functions Precision recall curves ROC curves Area Under Curve AUC Evaluation measures for multi class classification Multi class confusion matrix Multi class classification report Micro vs. legend target_names_fruits plt. create a two feature input vector matching the example plot above each feature should be centered zero mean and with unit variance plot_labelled_scatter X_mds y_cancer malignant benign First MDS dimension Second MDS dimension Breast Cancer Dataset MDS n_components 2 plot_labelled_scatter X_tsne y_cancer malignant benign First t SNE feature Second t SNE feature Breast cancer dataset t SNE. ", "id": "akshayvaru103/everything-in-machine-learning-1-3", "size": "7655", "language": "python", "html_url": "https://www.kaggle.com/code/akshayvaru103/everything-in-machine-learning-1-3", "git_url": "https://www.kaggle.com/code/akshayvaru103/everything-in-machine-learning-1-3", "script": "sklearn.metrics PCA Ridge make_friedman1 DummyClassifier MinMaxScaler Lasso DecisionTreeClassifier SCORERS plot_fruit_knn sklearn.decomposition sklearn.model_selection confusion_matrix f1_score plot_two_class_knn plot_decision_tree mpl_toolkits.mplot3d libraryplot classification_report LogisticRegression sklearn.datasets precision_recall_curve validation_curve roc_curve sklearn.svm ward datasets MLPClassifier train_test_split cross_val_score load_iris sklearn.naive_bayes KMeans KNeighborsClassifier recall_score mean_squared_error seaborn numpy shutil plot_feature_importances make_blobs scipy.cluster.hierarchy load_digits copyfile load_breast_cancer pandas matplotlib.colors GridSearchCV sklearn.linear_model matplotlib sklearn.dummy dendrogram plot_labelled_scatter_modified plot_class_regions_for_classifier_subplot r2_score TSNE LinearSVC auc Axes3D accuracy_score MDS sklearn.metrics.scorer load_crime_dataset AgglomerativeClustering roc_auc_score GaussianNB plot_class_regions_for_classifier_subplot_forndarray make_regression sklearn.tree sklearn.cluster plot_labelled_scatter make_classification KNeighborsRegressor sklearn.neural_network precision_score PolynomialFeatures GradientBoostingClassifier sklearn.ensemble DBSCAN sklearn RandomForestClassifier MLPRegressor matplotlib.pyplot sklearn.manifold cm StandardScaler plot_class_regions_for_classifier sklearn.neighbors SVC ListedColormap sklearn.preprocessing LinearRegression DummyRegressor ", "entities": "(('this', 'later course'), 'be') (('Instead scaling normalizing', 'cross validation'), 'compute') (('Clustering', 'This'), 'be') (('expression dramatic differences', 'different clusters'), 'assign') (('Here it', 'MDS'), 'doesn') (('cmap_light cmap_bold 000000 0 ListedColormap', 'AAFFAA AAAAFF'), 'EFEFEF') (('which', 'especially imbalanced classes'), 'example') (('Support Vector Machines Linear Support Vector Machine Linear Support Vector Machine C parameter Application', 'rest one classifiers'), 'supervise') (('http here scikit', 'scikit'), 'be') (('Second t SNE feature Breast cancer', 't SNE'), 'create') (('Pearson Correlation', 'different clusters'), 'Use') (('n clusters', 'fruits'), 'SNE') (('we', 'classifier'), 'have') (('how distance', 'classification clustering input'), 'specify') (('9', 'X array'), 'correspond') (('expression levels', 'time'), 'be') (('general it', 'k means'), 'note') (('width c y_fruits_2d cmap scatter X_fruits_2d height X_fruits_2d cmap_fruits', 'black alpha'), 'edgecolor') (('Manhattan', 'Manhattan city block distance'), 'Use') (('target column k NN regression', 'everything'), 'dataset') (('ROC', 'class classification Multi class confusion matrix Multi class classification multi report'), 'curve') (('where you', 'Jarvis Patrick'), 'Use') (('expression profiles', 'trends similar general e.'), 'Spearman') (('unit zero mean variance', 'less datasets'), 'average') (('http scikit', '3 scikit'), 'show') (('0 target_names', 'range'), 'title') (('figure', 'r g 6 6 b cmap_fruits'), 'figsize') (('Euclidean', 'flies distance'), 'use') (('that', 'Crime dataset Crime regression https archive'), 'choose') (('i', 'legend_handles'), 'color_list_bold') (('Pearson Squared', 'behaviors similar i.'), 'Use') (('where many instances', 'same features'), 'create') (('we', 'train g 75 25 first small mass 20 width'), 'include') (('DataFrame', 'str type'), 'numpy') (('dendrogram here corresponding', '10 points method'), 'create') ", "extra": "['test']"}