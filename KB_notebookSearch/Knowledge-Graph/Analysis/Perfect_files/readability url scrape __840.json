{"name": "readability url scrape ", "full_name": " h3 Scrape URLs for CommonLit Readability Prize competition h1 kids frontiersin org h1 en wikibooks org h1 simple wikipedia org h1 en wikipedia org ", "stargazers_count": 0, "forks_count": 0, "description": "You re welcome Count Url 196 simple. org 571 Total 95 Missing kids. org select rows that have urls grab the domain name list all reference urls by frequency in descending order remove some artifacts not present in competition data remove tables remove spans remove un ordered lists remove ordered lists remove sup tags remove artifact from using requests library on wikipedia remove another artifact. I only scrape 570 URLs from 3 4 separate domains. Scrape URLs for CommonLit Readability Prize competition 1. There may be some undetected artifacts in the text so use with caution. You should perform your own exploratory data analysis to discover any remaining artifacts that occured during scraping. I created a notebook https www. There contains over 600 URLs to scrape. com teeyee314 readability external data eda with additional preparation for use with competition training. Wikipedia was the most annoying to scrape cleanly. ", "id": "teeyee314/readability-url-scrape", "size": "840", "language": "python", "html_url": "https://www.kaggle.com/code/teeyee314/readability-url-scrape", "git_url": "https://www.kaggle.com/code/teeyee314/readability-url-scrape", "script": "filter_newline remove_ufeff seaborn numpy matplotlib.pyplot BeautifulSoup show_html clean_frontiersin clean_brackets remove_xa0 pandas remove_copyright show_html_wiki remove_http_url clean_http clean_newline bs4 ", "entities": "(('I', '3 4 separate domains'), 'scrape') (('that', 'scraping'), 'perform') (('lists', 'artifact'), 'select') ", "extra": "[]"}