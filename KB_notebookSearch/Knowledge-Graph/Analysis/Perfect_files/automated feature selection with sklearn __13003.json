{"name": "automated feature selection with sklearn ", "full_name": " h1 Automated feature selection with sklearn h2 Discussion h2 GenericUnivariateSelect h2 Model based feature selection SelectFromModel h2 Recursive feature elimination RFE and RFECV h1 Conclusion ", "stargazers_count": 0, "forks_count": 0, "description": "While the number of features is small or you have time to sit down and consider them all feature selection is mainly a hand driven process. feature_selection tool that allows you to select features from a dataset using a scoring function. The F test statistics only find linear relationships making them only appropriate for performing linear regression and the chi squared test requires that the data be appropriately scaled which it rarely is and non negative which it only sometimes is. It supports selecting columns in one of a few different configurations k for when you want a specific number of columns percentile for when you want to a percentage of the total number of columns and so on. Measuring feature importance this way is simple These results agree decently well with the results we got from the information theoretic approach in the previous section. To demonstrate I ll use the Kepler dataset. It appears that I accidentally chose a good number of predictor variables at least as far as RFECV is concerned While I won t spend any more time treating RFECV here you can read a bit more on cross validation and get some insight into how this algorithm works by looking at this notebook https www. This is because they provide an easily interpretable measurement Gini efficiency and have been found to work great at ranking feature importance in practice. Defining the metric is up to you you pass it a function and it does the rest. These are non parametric tests which use k nearest neighbors to measure the scale of the relationship between the predictor values and the target variable. Up to you whether or not you re comfortable using this toolchain. Keep doing this until the desired number of features is reached. GenericUnivariateSelect makes it easy to implement this strategy. Feature selection is the process of tuning down the number of predictor variables used by the models you build. sklearn comes with a simple wrapper for turning these feature importance scores into a feature subselection called SelectFromModel. The obvious example is linear regression which works by applying a coefficient multiplier to each of the features. That s a lot of statistical trickery. Luckily sklearn defines a handful of different pre built statistical tests that we may use. This approach is good because it is non parametric basing our selection on features that get picked by an actual machine learning algorithm not a statistical test makes a lot of sense. That model is simpler to train simpler to understand easier to run and less likely to be leaky. Obviously the higher the coefficient the more valuable the feature. com residentmario automated feature selection with boruta. Here s all you need to do Using this function revolves around inputting a function that takes the X and y arrays performs some kind of statistical test on the values and then returns the score per feature in X. html univariate feature selection points out the best options for this approach linear_model. Tuning the number of parameters in a model is a natural part of data science in practice and is something that comes naturally as part of the model building process. However this is not always possible. See that notebook here https www. Here s one example of a glyph from the dataset In training on this dataset it may naturally occur to us that hey most of the space in the image is actually whitespace the space around the character not taken up by the character itself. This is due to laziness as there is a boundary to how many variables your average notebook author is willing to deal with and how much time they re willing to dedicate to the task at hand. I prefer the mutual_info_ tests because they just work out of the box. This results in sometimes different sometimes very different column selections depending on the algorithm. Here is a heatmap demonstrating what implementing this strategy would look like This heatmap shows the mean value of each pixel in the dataset. com residentmario lasso regression with tennis odds by taking the coef_ of features deemed insignificant to 0 and decision trees which feature expose Gini importance via a feature_importances_ class variable. Other times there just isn t enough time. The second of these is obviously give me the N best features but the remaining three options take a bit of explaining. I find almost all the sklearn feature selection algorithms to be useful if not to use then at least to think about so this notebook has very similar coverage to the material covered in the official documentation http scikit learn. Sometimes the dataset has too many variables. Model based feature selection SelectFromModel Some machine learning algorithms naturally assign importance to dataset features in some way. DecisionTreeClassifier for classification. To demonstrate the API let s first try GenericUnivariateSelect out on a simple character recognition problem. Recursive feature elimination RFE and RFECV Recursive feature elimination is the sklearn sort of implementation of a really old idea stepwise selection that used to be very popular. fdr is the false discovery rate and can be specified to select columns with a certain level of risk that actually correlated columns will be rejected. PS you may also like this thread https www. Of course in this case the choice of the algorithm you use matters a lot. Rebuild the classifier especially in later steps the coefficient coverage will change then repeat the process. The advantage of this approach is that it will not remove variables which were deemed insignificant at the beginning of the process but become more and more significant as lesser features are removed. com questions and answers 46335 on the Kaggle Q A Forum. LogisticRegression and svm. White hot pixels are the ones that most often have data a glyph passes through them while black pixels lack any data. DecisionTreeRegressor for regression and one of linear_model. Note I demonstrated how to build this exact feature importance chart in even fewer steps using the yellowbricks ML visualization library in this previous notebook https www. We can perform feature selection by using the feature ranks generated by such machine learning models as a ranking and then pruning the features based on that ranking. All three of them are statistical significance tests e. Here are the columns that we selected Note the use of the get_support method on the GenericUnivariateSelect object to get the indices of the columns that were carried over. This dataset consists of several thousand 32 by 32 pixel images of handwritten Arabic characters. Many machine learning related Kaggle notebooks for example are based on easily processed data or only on well understood subsets of data. com residentmario ml visualization with yellowbrick 1. LinearSVC and trees. But for automated feature selection we probably want to avoid coming up with our own expressions. In this notebook I examine and demonstrate some tools available in Python for performing automated feature selection. This operator s main feature is its lack of opinion. Given this fact we might decide that a worthwhile strategy would be to first preprocess the data to remove the pixel values that are mostly whitespace. In scenarios where the number of variables are overwhelming or your time is limited automated or semi automated feature selection can speed things up. Then we would do So far we ve been using GenericUnivariateSelect with percentile arguments however the available selection options are actually percentile k_best fpr fdr fwe. The disadvantage is that since you have to train the model many times this approach is multiplicatively slower than the one and done. Given a set of data build a model on that data then assess the importance of the variables and prune the weakest feature. And even when you do have the incentive to hand roll and curate your features automated feature selection provides some useful early directions for exploration during the exploratory process. Unsurprisingly the variable that best predicts whether or not Kepler will classify an object as a planet or not is the predisposition mdash that is whether or not the planet is already considered a CANDIDATE or not usually the alternative is FALSE POSITIVE and Kepler is just double checking. Datasets may easily have hundreds or even thousands of variables quickly outrunning human comprehension. The following list considers the 3 possible pairs of feature choices and the difference in the sets chosen by the selectors for the Kepler dataset That these different approaches came to such deviant conclusions is not unexpected it s mainly a signal that the weaker of the 20 features we ve selected mostly come down to noise as all of the feature importance and ranking visualizations we have constructed so far have shown. Finally fwe is family wide error rate this can be used to control the level of risk that at least one of the columns returned is not actually correlated. fpr is the false positive rate we may use this argument to select columns with a certain level of risk e. ConclusionAlthough the different approaches agree on the most important variables in the dataset they disagree on the finer points. These are For regression f_regression mutual_info_regression For classification chi2 f_classif mutual_info_classif All of these tests in some way attempt to infer the relatedness of the predictor variables with the target variables. RFECV is a slight tweak to RFE that uses cross validation to determine the optimal stopping point in terms of numbers of columns eliminating user choice in the n_features_to_select attribute. For example when faced with two models with the same or nearly the same score but with the latter model using more variables your immediate instinct should be to choose the one with fewer variables. This methodology is implemented in RFE and is basically just a looped alternative to the SelectFromModel all at once approach. drop str columns drop columns with greater than 500 null values. Here I ve plotted out the resulting per variable score in a heatmap. We can apply mutual_info_classif by running a simple built in. That concludes this notebook In the next notebook I will step beyond sklearn built in to consider a feature selection algorithm implemented as an sklearn contrib module boruta. It s really useful to able to hand roll your metrics like this. Other good examples are lasso regression which features built in variable selection I show an application of this feature here https www. This dataset contains information on potential planets observed and confirmed or unconfirmed by the Kepler Space Observatory. There are two classes of these tests one implemented for regression and the other implemented for classification. Suppose we want to work with just the top fifty percent of the columns here. For the official documentation on GenericUnivariateSelect click here http scikit learn. 25 that they not correlated after all. GenericUnivariateSelect GenericUnivariateSelect is a sklearn. As we go further right we retain fewer and fewer of the original pixels. html univariate feature selection. If your goal is to build a logistic model or a linear support vector machine then it makes sense to make your feature selection decisions based on outputs from those two models but in general decision trees are by far the most popular choice for model based feature selection. com residentmario cross validation schemes with food consumption. Automated feature selection with sklearn DiscussionThe optimal machine learning problem approach is to take a dataset perform extensive EDA on it and understand many to most of the important properties of the predictors _before_ getting as far as seriously training models on these variables. With that function in tow here is the entire process It doesn t come equipped with the cool statistical doo dads that GenericUnivariateSelect has access to but in practice this approach is probably used far more often. The sklearn documentation describing this approach here http scikit learn. they are designed to select all columns which pass a certain p value threshold. For datasets with many variables relatively strongly correlated with one another and relatively weakly correlated with the target variable this approach may result in slightly different feature choices from those made by naive model based selection. org stable modules feature_selection. By looking at this point you might reasonably conclude that curating the feature space by 50 would be appropriate. ", "id": "residentmario/automated-feature-selection-with-sklearn", "size": "13003", "language": "python", "html_url": "https://www.kaggle.com/code/residentmario/automated-feature-selection-with-sklearn", "git_url": "https://www.kaggle.com/code/residentmario/automated-feature-selection-with-sklearn", "script": "sklearn.feature_selection DecisionTreeClassifier RFECV sklearn.tree GenericUnivariateSelect seaborn numpy matplotlib.pyplot RFE pandas sp mutual_info_classif SelectFromModel ", "entities": "(('they', 'finer points'), 'agree') (('rarely non it', 'only linear regression'), 'find') (('many times approach', 'multiplicatively one'), 'be') (('Kaggle related notebooks', 'data'), 'base') (('we', 'that'), 'define') (('other', 'classification'), 'be') (('Sometimes dataset', 'too many variables'), 'have') (('_', 'variables'), 'selection') (('main feature', 'opinion'), 'be') (('I', 'Kepler dataset'), 'demonstrate') (('sklearn', 'feature subselection'), 'come') (('feature selection decisions', 'feature by far most popular model based selection'), 'make') (('feature automated selection', 'exploratory process'), 'provide') (('you', 'scoring function'), 'tool') (('when you', 'columns'), 'support') (('black pixels', 'data'), 'be') (('you', 'matters'), 'in') (('com residentmario', 'food consumption'), 'cross') (('that', 'n_features_to_select attribute'), 'be') (('feature automated selection', 'things'), 'speed') (('methodology', 'basically just looped SelectFromModel'), 'implement') (('com residentmario', 'boruta'), 'automate') (('at least one', 'columns'), 'be') (('non parametric which', 'predictor values'), 'be') (('that', 'pixel values'), 'decide') (('drop str columns', 'greater than 500 null values'), 'drop') (('feature selection', 'time'), 'be') (('feature selection Model based machine learning algorithms', 'way'), 'selectfrommodel') (('it', 'strategy'), 'make') (('it', 'rest'), 'be') (('PS you', 'thread https also www'), 'like') (('further right we', 'original pixels'), 'retain') (('three', 'them'), 'be') (('we', 'columns'), 'suppose') (('they', 'just box'), 'prefer') (('that', 'actually columns'), 'be') (('dataset', 'handwritten Arabic characters'), 'consist') (('immediate instinct', 'fewer variables'), 'be') (('they', 'practice'), 'be') (('I', 'feature automated selection'), 'examine') (('linear which', 'features'), 'be') (('ranking we', 'feature importance'), 'consider') (('http here scikit', 'sklearn approach'), 'documentation') (('most', 'character'), 's') (('s', 'character recognition simple problem'), 'demonstrate') (('I', 'https here www'), 'be') (('they', 'hand'), 'be') (('Datasets', 'quickly human comprehension'), 'have') (('arrays', 'X.'), 's') (('desired number', 'features'), 'keep') (('http here scikit', 'GenericUnivariateSelect click'), 'for') (('we', 'probably own expressions'), 'want') (('dataset', 'Kepler Space Observatory'), 'contain') (('I', 'notebook https previous www'), 'demonstrate') (('We', 'simple'), 'apply') (('approach', 'naive model based selection'), 'correlate') (('false positive we', 'risk e.'), 'be') (('statistical test', 'sense'), 'be') (('heatmap', 'dataset'), 'be') (('already usually alternative', 'planet'), 'unsurprisingly') (('remaining three options', 'explaining'), 'give') (('approach', 'practice'), 'be') (('you', 'models'), 'be') (('that', 'model building process'), 'be') (('we', 'previous section'), 'be') (('coefficient coverage', 'then process'), 'rebuild') (('feature html univariate selection', 'approach linear_model'), 'point') (('which', 'p value certain threshold'), 'design') (('scikit', 'documentation official http'), 'find') (('how algorithm', 'notebook https www'), 'appear') (('These', 'target variables'), 'be') (('Here I', 'heatmap'), 'plot') (('selection however available options', 'percentile arguments'), 'do') (('more more lesser features', 'process'), 'be') (('We', 'ranking'), 'perform') (('that', 'idea stepwise really old selection'), 'elimination') (('It', 'this'), 's') (('I', 'sklearn contrib module boruta'), 'conclude') (('reasonably curating', '50'), 'conclude') (('that', 'columns'), 'be') (('decision which', 'feature_importances _ class variable'), 'regression') (('you', 'toolchain'), 'up') ", "extra": "['test']"}