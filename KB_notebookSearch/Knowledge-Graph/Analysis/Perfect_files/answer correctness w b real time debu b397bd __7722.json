{"name": "answer correctness w b real time debu b397bd ", "full_name": " h2 Weights and Biases h1 1 train csv h2 1 1 Columns individual analysis h3 Predefined functions h3 Inspect numerical features h3 Inspect Categorical Features many values h3 Inspect Categorical Features fiew values h2 View the plots saved in W B dashboard h2 1 2 Data Processing h1 W B Artifacts h2 1 4 Save and delete h1 2 questions csv h2 2 1 Inspect the columns h3 Save and delete h1 3 lectures csv h2 3 1 Inspect the columns h2 3 3 Save and delete h2 View the dashboard in real Time Blurb h1 1 Feature Engineering h2 1 1 Feature Engineering Create Data h2 Save the artifacts to cloud h1 Download the Uploaded artifacts BLURB here h2 1 2 Predefined Functions for Preprocesing h2 1 3 Apply Functions getting data ready h1 2 XGBoost Model h2 2 1 Baseline Model h3 Helper Function that runs multiple models h1 3 LightGBM Model h1 W B Reports h2 Use Cases ", "stargazers_count": 0, "forks_count": 0, "description": "cdeotte Features target and train test split Create DMatrix is optimized for both memory efficiency and training speed. Hence we need to carefully handle them. ai openai published work Learning Dexterity End to End VmlldzoxMTUyMDQ Import the Rapids suite here takes abot 1. csv row_id int64 ID code for the row. Note However I would erase all pupils user_id that have less than 5 appearences in the data no prediction can be made on these students Note So yes the average performance increases along with the number of times one student appears in the data. part top level category code for the lecture. Feature Engineering 1. Can be compared with the train user_answer column to check if the user was right. Think of an Artifact as of a folder of data to which we can add individual files and then upload to the cloud as a part of our W B project which also supports automatic versioning of datasets and models. artifacts https i. The meaning of the tags will not be provided but these codes are sufficient for clustering the questions together. tag one tag codes for the lecture. Here s an example dashboard of a project. Read in data train train. Collaboration Share findings with your colleagues. Artifacts also track the training pipelines as DAGs. parquet save it as model artifact on W B Clean the environment Data Information Find Missing Data if any Fill in missing values with 1 question_id The table contains an equal number of IDs for each question bundle_id Checkpoint save to parquet Encode type_of column Data Information lecture_id The table contains an equal number of IDs for each question There are 151 unique tags Import the data Lectures we won t load as we are not supposed to predict for these rows Let s exclude all observations where content_type_id 1 answered_correctly 1 Parameters Split data into train data feature engineering data to use for past performance Timestamp is in descending order meaning that the last 10 observations have the biggest chance of having had some performance recorded before so looking at the performance in the past we ll try to predict the performance now STUDENT ANSWERS Group by student CONTENT ID ANSWERS Group by content Features for ML We need to convert True False variables to integers Add past information Apply Fill in missing spots Scaling the data did not perform as I expected to so for now we will exclude it We don t want to scale the target also train_df scale_data features_data train_df train True features_to_keep features_to_keep target target Comment this if you re scaling RAPIDS roc_auc_score is 16x faster than sklearn. timestamp int64 the time in milliseconds between this user interaction and the first event completion from that user. prior_question_elapsed_time float32 The average time in milliseconds it took a user to answer each question in the previous question bundle ignoring any lectures in between is null for a user s first question bundle or lecture prior_question_had_explanation bool Whether or not the user saw an explanation and the correct response s after answering the previous question bundle ignoring any lectures in between. 2 Data Processing Note The outliers might strongly influence the future models. 5 mins Regular Libraries for DICOM images Color Palette Set Style Set tick size Rapids Imports CuPy is an open source array library accelerated with NVIDIA CUDA. feather format lighter takes about 7 seconds to upload using cudf and delete the dataframes. tags one or more detailed tag codes for the question. The value is shared across a single question bundle and is null for a user s first question bundle or lecture. 4 Save and delete To keep the notebook as light as possible and to not overload the memory we save the train data in. 1 Inspect the columns Note Again part 5 is very proeminent. 71628 in less than 10 seconds. task_container_id int16 ID code for the batch of questions or lectures. drop columns row_id axis 1 inplace True Data Information Find Missing Data if any Fill in missing values with 1 To speed up the process We need to extract both the name of the category and the no. 3 Apply Functions getting data ready 2. Use Cases Notes Add a graph with a quick note to yourself. The notebook runs at the moment in 3 minutes. Note The only 2 columns with missing data explained in documentation NULL values are present for the first question bundle 1. View the plots saved in W B dashboardYou can see you live dashboard as you log metrics and plots by simply calling wandb. csv question_id foreign key for the train test content_id column when the content type is question 0. 1 Feature Engineering Create Data Save FE data we will use it for the test set too Save the artifacts to cloudWe have used artifacts to to track all the files that we ve pre processes. bundle_id code for which questions are served together. 3 Save and delete View the dashboard in real Time Blurb Let s look again at the structure of our data This section uses the cuML package and XGBoost to compute the predictions. 2 Predefined Functions for Preprocesing Combine new features with the train_df 1. type_of brief description of the core purpose of the lecture string so this data needs to be treated a bit different no missing values 3. Inspect numerical features Inspect Categorical Features many values Inspect Categorical Features fiew values There are only a fiew cases where content_type_id is 1 meaning lectures which is good we re not supposed to predict those anyways. 6fBE0hz 20 20Imgur. Create Train the model Make prediction save model to file We ll do a train validation test situation Covertion to CPU data Training Loop X_valid to predict. user_id int32 ID code for the user. The Test of English for International Communication TOEIC is an international standardized test of English language proficiency for non native speakers. Here s an exmaple of artifacts graph. Weights and BiasesEach W B project has a dashboard that contains information about all the experiments in that project. Read 1 as null for lectures. If we would have had more than 1 worker Dask would have performed even better In this section we ll use the cudf and cupy libraries provided by RAPIDS combined with numpy for the plotting part. Now let s log these artifacts so that we don t have to repeat these steps Download the Uploaded artifacts BLURB here 1. It displays the dashboard of the currently executing run. LightGBM Model W B ReportsReports let you organize visualizations describe your findings and share updates with collaborators. The meaning of the tags will not be provided but these codes are sufficient for clustering the lectures together. 1 Columns individual analysis numerical features distplot timestamp prior_question_elapsed_time categorical features distplot user_id count content_id count task_container_id count categorical features barplot user_answer count answered_correctly count prior_question_had_explanation count Predefined functions Because there is no possibility yet to use Rapids for visualization we need to preprocess and convert the data to numpy arrays and plot it afterwards. However by trying to erase the outliers we can erase up to 10 of the data which is valuable information for training our models. csv lecture_id foreign key for the train test content_id column when the content type is lecture 1. answered_correctly int8 if the user responded correctly. part the relevant section of the TOEIC test. 96 in the Kaggle GPU Accelerator. content_id int16 ID code for the user interaction content_type_id bool 0 if the event was a question being posed to the user 1 if the event was the user watching a lecture. correct_answer the answer to the question. 1 Inspect the columns categorical features distplot question_id count bundle_id count tags count categorical features barplot correct_answer part Note majority of the questions are from part 5 if this distribution doesn t match the test set there might be some issues Save and delete 3. W B Artifacts You can store different versions of your datasets and models in the cloud as Artifacts. png Note Can t use Dask cuDF because we oly have 1 worker and Memory 13. Work log Track what you ve tried and plan next stepsCheckout this W B report by OpenAI How the OpenAI Robotics Team Uses W B Reports https wandb. a user might see three questions in a row before seeing the explanations for any of them those three would all share a task_container_id user_answer int8 the user s answer to the question if any. 1 Baseline Model Helper Function that runs multiple modelsWe have a ROC score of 0. of appearences Total rows we started with Compute Outliers We ll look only at the upper interval outliers Select ids to erase Erase the ids del train Count how many times the user answered correctly out of all available times Create intervals for number of appearences between 0 and 1000 1000 and 2500 and 2500 Convert to numpy arrays so we can plot Plot Checkpoint save to. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback. ", "id": "ayushexel/answer-correctness-w-b-real-time-debu-b397bd", "size": "7722", "language": "python", "html_url": "https://www.kaggle.com/code/ayushexel/answer-correctness-w-b-real-time-debu-b397bd", "git_url": "https://www.kaggle.com/code/ayushexel/answer-correctness-w-b-real-time-debu-b397bd", "script": "skimage.transform train_xgb_model lightgbm barplot_features display_html dask_cuda scale_data seaborn numpy tabulate scipy.stats to_bool wandb.lightgbm PIL LocalCUDACluster wandb_callback matplotlib.image sklearn.model_selection sklearn metrics cuml.preprocessing.model_selection Image matplotlib.pyplot condition pandas pearsonr StandardScaler resize dask.distributed combine_features cuml.metrics wait missingno Client roc_auc_score sklearn.preprocessing StratifiedKFold train_test_split IPython.display distplot_features ", "entities": "(('Artifacts', 'DAGs'), 'track') (('data', 'lecture string'), 'description') (('Cases Notes', 'yourself'), 'Use') (('Again part', 'columns'), 'inspect') (('we', 'those'), 'inspect') (('that', '0'), 'have') (('we', 'Plot Checkpoint'), 'of') (('you', 'simply wandb'), 'view') (('user', 'lectures'), 'float32') (('you', 'faster sklearn'), 'save') (('we', 'train data'), 'Save') (('we', 'pre processes'), 'use') (('three', 'question'), 'see') (('section', 'predictions'), 'Save') (('content_id content when type', 'train foreign test'), 'csv') (('which', 'valuable models'), 'by') (('that', 'project'), 'have') (('Test', 'non native speakers'), 'be') (('W B You', 'Artifacts'), 'artifact') (('example Here dashboard', 'project'), 's') (('where they', 'feedback'), 'be') (('value', 'question first bundle'), 'share') (('1 event', 'lecture'), 'code') (('we', 'plotting part'), 'perform') (('Collaboration Share', 'colleagues'), 'finding') (('Rapids suite', 'here abot'), 'publish') (('content_id content when type', 'csv train foreign test'), 'question_id') (('It', 'currently executing run'), 'display') (('OpenAI Robotics How Team', 'W B Reports https wandb'), 'log') (('notebook', '3 minutes'), 'run') (('distribution 5 doesn', '3'), 'inspect') (('we', '1 worker'), 'png') (('codes', 'questions'), 'provide') (('we', 'it'), 'count') (('codes', 'lectures'), 'provide') (('only 2 columns', 'first question'), 'note') (('you', 'share collaborators'), 'LightGBM') (('one student', 'data'), 'note') (('user', 'train user_answer column'), 'compare') (('feather format', 'dataframes'), 'take') (('We', 'category'), 'axis') (('which', 'datasets'), 'think') (('We', 'CPU data Training Loop'), 'train') (('Data Processing 2 outliers', 'strongly future models'), 'Note') (('questions', 'which'), 'code') (('we', 'artifacts Uploaded BLURB'), 'let') (('cdeotte Features target', 'train test split Create memory efficiency'), 'optimize') (('Rapids Imports CuPy', 'source array NVIDIA open CUDA'), 'min') ", "extra": "['test']"}