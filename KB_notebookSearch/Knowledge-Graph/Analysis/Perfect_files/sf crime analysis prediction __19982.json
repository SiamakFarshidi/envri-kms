{"name": "sf crime analysis prediction ", "full_name": " h1 Introduction h1 Definition h2 Project Overview h2 Problem Statement h2 Metrics h1 Analysis h2 Data Exploration h3 Dates Day of the week h3 Category h3 Police District h3 Address h3 X Longitude Y Latitude h2 Exploratory Visualization h2 Algorithms and Techniques h2 Benchmark h1 Methodology h2 Data Preprocessing h3 Data Wrangling h3 Feature Engineering h3 Feature Scaling h3 Feature Selection h2 Building the Initial Model h2 Refinement h2 Building the final model h1 Model Evaluation and Validation h1 Conclusion h2 Free Form Visualization h2 Reflection h2 Improvements ", "stargazers_count": 0, "forks_count": 0, "description": "92716 AdaBoost Default Scikit Learn Parameters 3. The duplicates removed and the outliers imputed. Indeed by removing the Minute feature from the dataset we had an increase of loss to 2. Analysis Data Exploration The dataset is in a tabular form and includes chronological geographical and text data and contains incidents derived from the SFPD Crime Incident Reporting system. Downloading the shapefile of the area Unzipping it Loading to a geopandas dataframe Defining the coordinate system to longitude latitude Merging our train dataset with the geo dataframe Transforming the coordinate system to Spherical Mercator for compatibility with the tiling background Calculating the incidents per day for every district Ploting the data restore original x y limits Adding the background Adding the name of the districts Loading the data Data cleaning Feature Engineering Encoding the Categorical Variables Creating the model Submitting the results Create object that can calculate shap values Calculate Shap values. For this reason we had to be creative and use advanced techniques during the hyperparameter optimization to make a difference. After cleaning the dataset from outliers and duplicates we examine the variables. Let s see how many they are. This will be the final goal creating a model that predicts the probability of each type of crime based on the location and the date. Address Address as a text field requires advanced techniques to use it for the prediction. SF Crime Analysis Prediction Base Model https www. This way we were able to monitor the validation process in real time. We can achieve this with functions like Hx sin 2 \u03c0 H 23 Hy cos 2 \u03c0 H 23 for the hour and accordingly for the rest. Doing this would require re training an estimator for each feature which can be computationally intensive. To conclude to the final model we used five folds Cross Validation for 100 epochs and early stopping with Bayesian Optimization. 25697 on the training set which is 16 lower from the naive prediction 2. The second diagram presents the average number of incidents per hour for five of the crimes categories. Although longitude does not contain any outliers latitude includes some 90o values which correspond to the North Pole. Building the Initial ModelTo build the model we used the LightGBM s Python API. X Longitude Y Latitude We have tested that the coordinates belong inside the boundaries of the city. In contrast during the night the probability for BURGLARY class 4 increases up to 2 and during the day the probability for DISORDERLY CONDUCT class 5 decreases. The specific problem does not belong to a field that humans excel like computer vision or NLP so as a proxy for the Bayes error rate https en. As before these are sharp pieces of evidence that the time parameters will have a significant role also. For each incident we will predict a set of predicted probabilities one for every class and we will calculate the average deviation from the real values. Police District There are significant differences between the different districts of the City with the Southern district having the most incidents 17. To get a little more intuition on the metric for a specific incident We get 0 loss from the categories of crimes that did not happen since yij 0 yijlog pij no matter our predicted probability. We can solve this issue and understand even deeper our model by using Partial Dependencies. png Refinement Instead of the most popular methods of Exhaustive Grid Search and Randomized Parameter Optimization we selected another more efficient way to tune the hyperparameters of the algorithm Bayesian optimization. The probability model also called the surrogate or response surface is easier to optimize than the actual objective function. We will explore a dataset of nearly 12 years of crime reports from across all of San Francisco s neighborhoods and we will create a model that predicts the category of crime that occurred given the time and location. The median frequency of incidents is 389 per day with a standard deviation of 48. com yannisp sf crime analysis prediction base model notebook scriptVersionId 9334889 base_model https i. Feature EngineeringThen we created additional features. We have to note here that since all the probabilities for a specific incident sum to 1 each probability we predict for a category that did not happen creates an indirect loss since it decreases our predicted probability for the category of crime that happened. To do that we can remove each feature from the dataset re train the estimator and check the impact. This is a hard problem to solve with a heavily unbalanced dataset and the unpredictability up to some point of the human factor. cc paper 7062 a unified approach to interpreting model predictions for details. 91656 LIghtGBM Default Scikit Learn Parameters 2. Feature SelectionAfter the feature engineering described above we ended up with 11 features. We run the optimization process until we noticed in Tensorboard that the models converge. Thus we do not expect this variable to play a significant role in the prediction. In the modern United States history crime rates increased after World War II peaking from the 1970s to the early 1990s. We did not notice any anomalies on these variables. org wiki Bayes_error_rate we will use the score of the best kernel so far which is initial benchmark need tuning https www. The small distance between the baseline score and the Bayes error rate indicate that this is a hard problem with a low margin of improvement. com sergeylebedev initial benchmark need tuning by the user Sergey Lebedev https www. The implementation of the above technique showed that there is no need for any feature removal since all of them have a positive impact in the dataset. example of the process here https www. Data Exploration for understanding the variables and create intuition on the data. 87 followed by Mission 13. This fact is a reliable indication that the location coordinates Police District will be a significant factor for the analysis and the forecasting. The Permutation importance is a great tool to understand how much a specific feature affect our prediction but it does not tell us anything about the direction it affects it. Choose the leaf with max delta loss to grow. Details in SF Crime Analysis Prediction Naive Prediction https www. From the Address field we extracted if the incident has taken place in a crossroad or on a building block. Then we used Cross Validation with early stopping 10 rounds and parameters Objective multiclass Metric multi_logloss Num_class 39 The above setup achieved 2. 91 Non Criminal 10. 98336 SGD scored the best initial result but after a lengthy hyperparameter tuning it was not able to pass a 2. 46799 cross validation score after 23 epochs and 2. The idea is that the importance of a feature can be measured by looking at how much the loss decreases when a feature is not available. This prediction will be a baseline score to compare with our model s score to evaluate if we have any significant progress. 58856 XGBoost Default Scikit Learn Parameters 2. 67 and Northern 12. Descript detailed description of the crime incident DayOfWeek the day of the week PdDistrict the name of the Police Department District Resolution The resolution of the crime incident Address the approximate street address of the crime incident X Longitude Y LatitudeThe dataset contains a lot of object variables aka strings that we will need to encode. The frequency can be calculated easily by dividing the sum of incidents of each category by the number of rows of the training set. Until recently crime prevention was studied based on strict behavioral and social methods but the recent developments in Data Analysis have allowed a more quantitative approach in the subject. As an example let s select a row from the testing dataset This incident has taken place in 03 30 in the night in a block. Finally from the algorithms that scored under 3. Then we stopped and evaluated the results and move to the next iteration. The above was up to some point an iterative process. We can notice that this baseline is already lower than the initial score of our classifiers. Property crime more than doubled over the same period. In other words the metric evaluates the certainty of our model for each category of crime incident. Feature ScalingDeciding to continue with a tree based algorithm there was no need for scaling on the final dataset. A way to solve it is to imagine the hour in a real clock and take their projections on the axes passing from the center of the clock. SHAP connects game theory with local explanations uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on expectations see the SHAP NIPS paper http papers. Instead in this project we will use it to extract if the incident has happened on the road or in a building block. Bayesian methods select the next values to evaluate by applying a criterion usually Expected Improvement to the surrogate. png Benchmark There are two types of benchmarks we need to set. First we created the dataset by combining the features the target and declaring the PdDistrict as a categorical variable using lightgbm. In other words if for an incident we change only the value of one feature how will this affect the probability of each crime category As an example we can evaluate how the Hour affects the probabilities of three different crimes. 4 In the following figure we present the performance of the best model from each step of optimization. Training Testing data creation to evaluate the performance of our models and fine tune their hyperparameters. The most challenging part was that due to the nature of the features there was a little room for feature engineering. We get log pij loss from the category that happened where pij is our predicted probability for the specific category. Dates timestamp of the crime incident Category category of the crime incident. Taking into account the low margin between the naive and the benchmark we knew that we would probably have a small improvement. This technique speeds up training and reduces memory usage. Initially we evaluated several appropriate algorithms from Linear Models Stochastic Gradient Descent Nearest Neighbors K nearest neighbors Ensemble methods Random Forests AdaBoost and Boosting Algorithms XGBoost LIghtGBM using basic feature engineering and the default parameters to evaluate if any of them has a significant head start Algorithm Parameters Logloss Stochastic Gradient Descent Default Scikit Learn Parameters with log loss 2. To identify if any of them increased the complexity of the model without adding significant gain to the model we used the method of Permutation Importance. We can see that the hour does not affect the probability for BRIBERY class 3. It also contains 2323 duplicates that we should remove. Problem StatementTo examine the specific problem we will apply a full Data Science life cycle composed of the following steps 1. 29263 Random Forest Default Scikit Learn Parameters 2. ReflectionAs described in the previous sections a full cycle data processing have been followed and lead us to a satisfactory prediction model. The first will be a naive prediction. com sergeylebedev with score 2. That being said we could say that the results are satisfactory. 0 we decided to work with LightGBM due to its efficiency and versatility in the hyperparameters tuning. 53743 and by removing the DayOfWeek feature the loss increased to 2. We can see that there is a 10 probability for BURGLARY and that this is mostly increased because it takes place to a block not on a crossroad and from the time hour and minute. Category There are 39 discrete categories that the police department file the incidents with the most common being Larceny Theft 19. In layman terms the algorithm works like this 1. Both of these are aligned again with our intuition making us more confident about the validity of our model. com yannisp sf crime analysis prediction optimiz ex First we optimized a few basic hyperparameters including Boosting selection between gbdt and dart Max_delta_step uniformly in the range 0 2 Min_data_in_leaf uniformly in the range 10 30 Num_leaves uniformly in the range 20 40 After the model converged a second round of tuning followed Boosting gbdt Max_delta_step uniformly in the range 0. The concept is to limit the evaluations of the objective function by spending more time choosing the next values to try. Some points are misplaced. Based on the Permutation Importance analysis we performed before the model should be susceptible to changes in Minute and the coordinates and less sensitive to changes in Day Year or Day of the week. 49136 on the testing set. Let s see if our model aligns with our intuition. By extracting if the incident has happened to a block or a crossroad we have extracted the minimum gain from this feature and maybe there are some patterns to exploit and give us even better score. Go to step 2 lightgbm https lightgbm. That being said we present two diagrams to visualize the importance of these variables. ImprovementsWe are sure there is space for improvement. Exploratory VisualizationBased on the Project s statement we need to predict the probability of each type of crime based on time and location. MetricsThe most appropriate evaluation metric for such problems is the multi class logarithmic loss. From Sunset to SOMA and Marina to Excelsior this project analyzes 12 years of crime reports from across all of San Francisco s neighborhoods to create a model that predicts the category of crime that occurred given time and location. Insights like this are possible with the use of the SHAP library. Increase the weight to the incorrect samples. Dates Day of the week These variables are distributed uniformly between 1 1 2003 to 5 13 2015 and Monday to Sunday and split between the training and the testing dataset as mentioned before. Instead we can replace it with noise by shuffle values for a feature. More specifically From the Dates field we extracted the Day the Month the Year the Hour the Minute the Weekday and the number of days since the first day in the data. Use embeddings or any other text processing technique for the addresses. com yannisp sf crime analysis prediction naive prediction Notebook. Data Normalization and Data Transformation for preparing the dataset for the learning algorithms if needed. We can conclude that the model is aligned with our intuition. The baseline calculated this way is 2. Violent crime nearly quadrupled between 1960 and its peak in 1991. io en latest _images leaf wise. SHAP SHapley Additive exPlanations is a unified approach to explain the output of any machine learning model. IntroductionSan Francisco was infamous for housing some of the world s most notorious criminals on the inescapable island of Alcatraz. Instead Bayesian optimization also called Sequential Model Based Optimization SMBO implements this idea by building a probability model of the objective function that maps input values to a probability of a loss p loss input values. Fit a decision tree to the data2. Two additional techniques we would like to implement if there was the necessary time would be Create ordinal representations for the features that present a kind of cyclicity Month Weekday Hour Minute. Another critical benchmark is usually the Human Performance as a proxy for the Bayes error rate. In a Multiclass Classification the best way to calculate the baseline is by assuming that the probability of each category equals its average frequency in the train set. LightGBM is a decision tree boosting algorithm uses histogram based algorithms which bucket continuous feature attribute values into discrete bins. Also there is no significant deviation of incidents frequency throughout the week. We will also evaluate the position of the data points using the coordinates. Conclusion Free Form VisualizationAn interesting visualization would be to depict how each feature affects a specific prediction. Some examples are that prostitution picks during the evening and all through the night Gambling incidents start late at night until the morning and Burglary picks early in the morning until the afternoon. As we saw in the Partial Dependencies graphs before BURGLARY has a higher probability and burglaries happen by definition in blocks. It is evident that different crimes have different frequency during different times of the day. The reasoning behind this is that if we take the Hour as an example the default representation implies that 23 and 00 midnight are 23 units away although in reality they are 1 unit apart. We will replace the outlying coordinates with the average coordinates of the district they belong. The data ranges from 1 1 2003 to 5 13 2015 creating a training dataset with nine features and 878 049 samplesMore specifically it includes the following variables. 86631 K Nearest Neighbors Default Scikit Learn Parameters 23. The problem with the two techniques mentioned above is that they do not use previous results to pick the next input values. We can see that although the epicenter of most of the crimes resides on the northeast of the city each crime has a different density on the rest of the city. Feature Engineering to create additional variables from the existing. The first one presents the geographic density of 9 random crime categories. Methodology Data Preprocessing Data WranglingFollowing the methodology described in the Problem Statement we identified 2323 duplicate values and 67 wrong latitudes. Definition Project Overview Crime is a social phenomenon as old as societies themselves and although there will never be a free from crime society just because it would need everyone in that society to think and act in the same way societies always look for a way to minimize it and prevent it. Data Wrangling to audit the quality of the data and perform all the necessary actions to clean the dataset. Model selection and evaluation. Since the 1990s however crime in the United States has declined steadily. 1 2 Finally we concluded to the following hyperparameters Boosting gbdt Max_delta_step 0. png 1 Building the final model Model Evaluation and ValidationThe final model scored 2. This is our target variable. 5 Min_data_in_leaf uniformly in the range 10 25 Num_leaves uniformly in the range 20 45 Max_bin uniformly in the range 200 500 Learning_rate uniformly in the range 0. Also we created a custom callback function so we can write proper logs that can be read by Tensorboard. 9 Min_data_in_leaf 21 Num_leaves 41 Max_bin 465 Learning_rate 0. This way the distance between 23 and 00 is the same as between 00 and 01. Today the city is known more for its tech scene than its criminal past. Algorithms and Techniques The specific problem is a typical multiclass classification problem and there are several categories of algorithms for solving it. 68015 and 2 better than the benchmark. Logarithmic loss measures the performance of a classification model where the prediction output is a probability value between 0 and 1. ", "id": "yannisp/sf-crime-analysis-prediction", "size": "19982", "language": "python", "html_url": "https://www.kaggle.com/code/yannisp/sf-crime-analysis-prediction", "git_url": "https://www.kaggle.com/code/yannisp/sf-crime-analysis-prediction", "script": "geopandas create_gdf lightgbm pdp contextily train_test_split pyplot as plt Point PermutationImportance seaborn numpy geoplot pyplot SimpleImputer sklearn.impute feature_engineering info_plots get_dataset sklearn.model_selection LabelEncoder matplotlib.pyplot pandas cm LGBMClassifier add_basemap shapely.geometry sklearn.preprocessing matplotlib eli5.sklearn pdpbox ", "entities": "(('that', 'location'), 'be') (('way we', 'real time'), 'be') (('incident', 'block'), 'take') (('Bayes error this', 'improvement'), 'indicate') (('that', 'time'), 'explore') (('morning', 'afternoon'), 'be') (('Police District', 'significant analysis'), 'be') (('which', 'North Pole'), 'include') (('we', 'Permutation Importance'), 'identify') (('first one', 'crime 9 random categories'), 'present') (('we', 'Python API'), 'build') (('46799', '23 epochs'), 'cross') (('we', 'hyperparameters'), '0') (('model', 'intuition'), 'let') (('we', 'variables'), 'say') (('we', 'week'), 'perform') (('Bayesian methods', 'usually Expected surrogate'), 'select') (('Randomized Parameter we', 'algorithm Bayesian optimization'), 'select') (('Analysis Data dataset', 'SFPD Crime Incident Reporting system'), 'Exploration') (('models', 'Tensorboard'), 'run') (('metric', 'crime incident'), 'evaluate') (('SHAP', 'expectations'), 'see') (('loss', '2'), 'feature') (('model', 'intuition'), 'conclude') (('SHAP SHapley Additive exPlanations', 'machine learning model'), 'be') (('however crime', 'United States'), 'decline') (('BURGLARY', 'blocks'), 'see') (('frequency', 'training set'), 'calculate') (('concept', 'next values'), 'be') (('most challenging part', 'feature little engineering'), 'be') (('hour', 'BRIBERY class'), 'see') (('algorithm', '1'), 'work') (('we', 'time'), 'VisualizationBased') (('20 model', 'Boosting gbdt uniformly range'), 'com') (('878 049 samplesMore specifically it', 'following variables'), 'range') (('cycle data full processing', 'prediction satisfactory model'), 'follow') (('we', '2323 duplicate values'), 'Data') (('that', 'cyclicity Month Weekday Hour Minute'), 'be') (('police Category 39 discrete department', 'most common being'), 'be') (('Insights', 'SHAP library'), 'be') (('technique', 'memory usage'), 'speed') (('we', 'variables'), 'examine') (('we', 'impact'), 'remove') (('This', 'human factor'), 'be') (('that', 'crime'), 'have') (('LIghtGBM Default Scikit', 'Parameters'), '91656') (('all', 'dataset'), 'show') (('incident', 'building block'), 'use') (('we', 'png two benchmarks'), 'Benchmark') (('different crimes', 'day'), 'be') (('they', 'district'), 'replace') (('we', '2'), 'by') (('way', 'clock'), 'be') (('baseline', 'classifiers'), 'notice') (('coordinates', 'city'), 'test') (('We', 'coordinates'), 'evaluate') (('variables', 'training'), 'distribute') (('probability', 'train set'), 'be') (('above they', 'input next values'), 'be') (('AdaBoost Default Scikit', 'Parameters'), '92716') (('we', 'data'), 'extract') (('Then we', 'next iteration'), 'stop') (('we', 'also 2323 duplicates'), 'contain') (('that', 'yijlog yij 0 pij'), 'get') (('ImprovementsWe', 'improvement'), 'be') (('Today city', 'criminal past'), 'know') (('some', 'Alcatraz'), 'be') (('we', 'significant progress'), 'be') (('we', 'maybe even better score'), 'extract') (('Instead we', 'feature'), 'replace') (('specific problem', 'it'), 'Algorithms') (('we', 'following steps'), 'examine') (('we', 'Bayesian Optimization'), 'stop') (('societies', 'it'), 'be') (('recent developments', 'subject'), 'study') (('evaluation most appropriate metric', 'such problems'), 'be') (('incident', 'building block'), 'extract') (('when feature', 'how much'), 'be') (('functions', 'accordingly rest'), 'achieve') (('We', 'variables'), 'notice') (('Metric Num_class 39 above setup', '2'), 'use') (('Feature EngineeringThen we', 'additional features'), 'create') (('First we', 'lightgbm'), 'create') (('mostly it', 'time hour'), 'see') (('which', '16 naive prediction'), '25697') (('that', 'Tensorboard'), 'create') (('Address Address', 'prediction'), 'require') (('we', 'real values'), 'predict') (('which', 'feature'), 'require') (('how feature', 'specific prediction'), 'form') (('humans', 'Bayes error rate https en'), 'belong') (('crime', 'city'), 'see') (('Property crime', 'more than same period'), 'double') (('Random Forest Default Scikit', 'Parameters'), '29263') (('where pij', 'predicted specific category'), 'get') (('which', 'discrete bins'), 'be') (('Training Testing data creation', 'fine hyperparameters'), 'tune') (('we', 'optimization'), '4') (('way distance', 'between 00'), 'be') (('second diagram', 'crimes categories'), 'present') (('We', 'Partial Dependencies'), 'solve') (('critical benchmark', 'Bayes error rate'), 'be') (('prediction where output', 'probability 0'), 'measure') (('1 Finally we', 'gbdt'), 'conclude') (('K Nearest Neighbors Default Scikit', 'Parameters'), '86631') (('us', 'model'), 'align') (('how Hour', 'three different crimes'), 'change') (('XGBoost Default Scikit', 'Parameters'), '58856') (('time parameters', 'significant role'), 'be') (('probability', 'CONDUCT DISORDERLY class'), 'in') (('Create that', 'Calculate Shap values'), 'download') (('that', '3'), 'from') (('we', 'difference'), 'for') (('so far which', 'benchmark https initial www'), 'use') (('Violent crime', '1991'), 'quadruple') (('it', 'it'), 'be') (('above', 'iterative process'), 'be') (('median frequency', '48'), 'be') (('variable', 'prediction'), 'expect') (('it', '2'), 'score') (('Algorithm Parameters Logloss Stochastic Gradient Descent Default Scikit', 'log loss'), 'evaluate') (('feature we', '11 features'), 'end') (('we', 'probably small improvement'), 'know') (('that', 'loss p loss input values'), 'call') (('1 Building', 'ValidationThe final 2'), 'score') (('they', '23 away reality'), 'be') (('World War II', 'early 1990s'), 'increase') (('also surrogate surface', 'actual objective function'), 'call') (('we', 'that'), 'detailed') (('that', 'time'), 'analyze') ", "extra": "['biopsy of the greater curvature', 'test']"}