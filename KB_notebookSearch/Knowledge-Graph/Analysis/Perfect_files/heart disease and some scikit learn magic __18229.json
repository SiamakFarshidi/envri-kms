{"name": "heart disease and some scikit learn magic ", "full_name": " h1 Introduction h1 Loading Data h3 I decided to rename columns for easier understanding in EDA part h3 Again renaming the categorical variables for easier EDA interpretation h1 Univariate Analysis h4 For this part we going to inspect how s the data distribution is and what patterns we can inspect h2 Categorical Data h3 Here we can do these observations h2 Numerical Data h3 Most of the continuous variables somewhat close to gaussian distribution with small skews left or right except for oldpeak Again there are some outliers espacially a strong one in Cholesterol worth to take a look later h1 Bivariate Analysis h2 Categorical Data vs Target h3 Here we can do these observations h2 Numerical Data vs Target h3 Here we can do these observations h1 Multivariate Analysis h2 Cholesterol Max Heart Rate Age St Depression vs Target h3 Here I tried to fit every single numerical feature into one graph so we can have some visualized version of the effects 3D scatterplot is great tool for doing that h4 On X axis we have Cholesterol levels on Y Max Heart Rate presented and Z axis is patient Age marker sizes are based on ST Depression levels and coloring based on the patient condition h1 Inspecting Age Closer h1 Correlations h4 We going to use pearson correlation for to find linear relations between features heatmap is decent way to show these relations h3 Here I chose top related features with outcome condition seems thal is the most correlated one h1 Modelling h3 Ok Here s the fun part who doesn t love a bit of modelling We start by loading our train data and labels as X and y s and we get dummy variables for categorical data using one hot encoding Then we import loads of sklearn modules h1 Classifiers h3 Here I selected some common sklearn classifiers I didn t want to include some common ml algorithms like xgboost lightgbm or catboost since I wanted to play with sklearn only for this notebook I ll do small quotes for each classifier from sklearn s official page h2 GradientBoostingClassifier h2 KNeighborsClassifier h2 DecisionTreeClassifier h2 Support Vector Machines h2 RandomForestClassifier h2 AdaBoostClassifier h2 MLP Classifier h2 GaussianNB h3 Ok Let s get building them h1 Baseline Results h3 We have many metrics but I decided to sort them by F1 score since precision and recall is pretty important on this case Looking at our first results showing RandomForestClassifier is the best performing one in the list of not tuned classifiers followed by MLP and GradientBoosting classifiers h3 But we can see most of our decision tree based models are overfitting that s something we should take a look at soon h2 Since our decision tree based models overfitting I wanted to look which features are most effecting these decisions I sampled two of the tree based models you can see below h1 Automatic Outlier Detection h2 Before going to tune our models I decided to get rid of some outliers we have pretty small database and we can actually remove them by hand or more basic methods But I wanted to use what sklearn can offer us for this so we gonna try couple sklearn features h1 Isolation Forest h1 Elliptic Envelope h1 Discretization h3 Since we have small and noisy data I thougt binning them would be better choice of action for this purpose I m going to choose another sklearn tool h2 K Bbins Discretization h3 This method discretizes features into k bins Sklearn module takes several strategy parameters but we going to use kmeans strategy which defines bins based on a k means clustering procedure performed on each feature independently h3 By looking at the results binning improved some models little but few of them like SVC got huge boost to their score F1 score 0 44 0 80 h1 Learning Curves h3 Before finalizing our modelling I wanted to use another tool sklearn offers Learning Curves That can show us how fast the models learning and especially how is the model doing with the number of data given so we can decide if more data needed for better results In our case we can see that some models overfitting and most of our models can get better with the more data h1 RandomizedSearchCV h3 Let s get rid of overfitting one of the easiest ways of doing it is tuning parameters for our estimators and regularize them Thankfully sklearn is coming to help with useful tools for this case too We going to use RandomizedSearchCV for this h3 I m going to choose small amount of estimators and not many parameters to search for timing purposes but you ll see even that s useful h1 Tuned Model Results h3 Alright As you can see even little bit tuned parameters added regularization to our models and increased the CV score for them That s a good sign Now we have three decent models to make predictions h1 Dimension Reduction Using PCA h2 Explained Variance h3 This graph shows that first 15 components explains more than 80 of the variance in the data and currently we have 46 of them So it s safe to reduce some h2 5 Components h3 We start with 5 components It looks like these 5 explains half of the variance in our data h2 3 Components h3 More we reduce dimensions further we can visualize it better for our graphs At 3D we can have this scatterplot showing us some kind of meaningful clusters h3 On 2D space we can still diverse the clusters according to our target variables These two components explains almost one third of the variance h1 Reduced Dimension Model Results h3 These are pretty good results We almost have same model metrics for most of the classifiers and even got some better regularization for some estimators h1 Decision Regions h3 With these contour plots we can see how the models decide on their predictions based on 2D data with confidence intervals Looks cool h1 Confusion Matrix h3 One last thing before we finish our sklearn journey I wanted to use another cool sklearn tool to show confusion matrices for each model It s important for this case since we don t want our models to predict no disease on actually unhealty person or vice versa It d be very bad for patients in actual use So we want less false positives and negatives but don t forget we still have some overfitted models so be careful about checking overfitted models like decision tree etc h1 Final Words h3 Well this concludes my notebook It was fun sklearn journey for me I hope you had fun while reading it and thanks for taking a look h3 Feel free to comment I ll try to answer it all and as usual if you liked my work please don t forget to vote have good one all ", "stargazers_count": 0, "forks_count": 0, "description": "So it s safe to reduce some. The data includes 303 patient level features including if they have heart disease at the end or not. Elliptic EnvelopeLet s try another automatic outlier detection method. Meanwhile it s less than half for not having it. It involves the analysis of two variables for the purpose of determining the empirical relationship between them. I m going to choose small amount of estimators and not many parameters to search for timing purposes but you ll see even that s useful Tuned Model Results Alright As you can see even little bit tuned parameters added regularization to our models and increased the CV score for them. Adding parameters that do not influence the performance does not decrease efficiency. I find max heart rate distribution a bit interesting expecting the other way around but it might be due to testing conditions and if you have normal results on ECG while exercising instructors might be increasing your excercise density It s pretty clear that heart disease likelihood increases with ST depression levels. com cherngs heart disease cleveland uci. Univariate Analysis Univariate analysis is the simplest form of analyzing data. For that we will assign a color to each point in the mesh x_min x_max x y_min y_max. On X axis we have Cholesterol levels on Y Max Heart Rate presented and Z axis is patient Age marker sizes are based on ST_Depression levels and coloring based on the patient condition. This is not the case here since we have very small data but we still can use it for visualization which I find it cool. Basically I set contamination rate of our data to 10 and dropped them using masks. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Reduced Dimension Model Results These are pretty good results We almost have same model metrics for most of the classifiers and even got some better regularization for some estimators Decision Regions With these contour plots we can see how the models decide on their predictions based on 2D data with confidence intervals. Support Vector Machines Support vector machines SVMs are a set of supervised learning methods used for classification regression and outliers detection. Well let s get goin then Loading Data I decided to rename columns for easier understanding in EDA part. Isolation Forest The IsolationForest isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Cholesterol Max Heart Rate Age St Depression vs Target Here I tried to fit every single numerical feature into one graph so we can have some visualized version of the effects. Discretization Discretization otherwise known as quantization or binning provides a way to partition continuous features into discrete values. KNeighborsClassifier Neighbors based classification is a type of instance based learning or non generalizing learning it does not attempt to construct a general internal model but simply stores instances of the training data. By looking at the results binning improved some models little but few of them like SVC got huge boost to their score. Here I chose top related features with outcome condition seems thal is the most correlated one. Put the result into a color plot Plot the training points Plot the testing points Displaying confusion matrix for each estimator. Cholesterol Serum Cholesterol in mg dl Fasting Blood Sugar 0 Less Than 120mg ml 1 Greater Than 120mg ml Resting Electrocardiographic Measurement 0 Normal 1 ST T Wave Abnormality 2 Left Ventricular Hypertrophy Max Heart Rate Achieved Maximum Heart Rate Achieved Exercise Induced Angina 1 Yes 0 No ST Depression ST depression induced by exercise relative to rest. Since recursive partitioning can be represented by a tree structure the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node. Displaying 3 components 2 Component PCA project from 46 to 2 dimensions Displaying 2 PCA Reduced Dimension Model Results preprocess dataset split into training and test part Just plot the dataset first Iterate over classifiers Plot the decision boundary. Chest pain type is very subjective and has no direct relation on the outcome asymptomatic chest pains having highest disease outcome. Number of unique train observartions Renaming cateorical data for easier understanding Masks for easier selection in future Display categorical data Displaying numeric distribution Categorical data vs condition Displaying numeric distribution vs condition Numeric data vs each other and condition 3D scatterplot of numeric data Loading data for corrmap Correlation heatmap between variables Top correlated variables vs condition Setting train and condition data One hot encoding train features Loading sklearn packages Selecting some sklearn classifiers Setting 5 fold CV Baseline check Feature importances Applying Isolation Forest Checking isolated models Applying Elliptical Envelope Applying K bins discretizer Plot learning curve Displaying learning curves Searching parameters for fine tuning Checking binned models Fitting PCA Explaining variance ratio 5 Component PCA Displaying 50 of the variance 3 Component PCA Project from 46 to 3 dimensions. Again renaming the categorical variables for easier EDA interpretation. We assumed our distribution close to gaussian while inspecting the data so elliptic envelope worth to take a look. That s a good sign Now we have three decent models to make predictions Dimension Reduction Using PCA PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. It d be very bad for patients in actual use. Reducing dimensions is useful for bigger datasets because by transforming a large set of variables into a smaller one that still contains most of the information in the large set makes your modelling faster. Having exercise induced angina is pretty strong indicator for heart disease patients are almost 3 times more likely to have disease if they have exercise induced angina. Lastly older patients are more likely to have heart disease. So we want less false positives and negatives but don t forget we still have some overfitted models so be careful about checking overfitted models like decision tree etc. Features are like Age Obvious one. Slope Slope of the peak exercise ST segment 0 Upsloping 1 Flat 2 Downsloping Thalassemia A blood disorder called Thalassemia 0 Normal 1 Fixed Defect 2 Reversable Defect Number of Major Vessels Number of major vessels colored by fluoroscopy. There s is also one outlier there with no disease pretty interesting. I d say it gave me more reasonable results and decided to stick with it both datasets are pretty close but targets are reversed. MLP Classifier Multi layer Perceptron classifier. ensemble module includes two averaging algorithms based on randomized decision trees the RandomForest algorithm and the Extra Trees method. That can show us how fast the models learning and especially how is the model doing with the number of data given so we can decide if more data needed for better results. Looking at our first results showing RandomForestClassifier is the best performing one in the list of not tuned classifiers followed by MLP and GradientBoosting classifiers. Hence when a forest of random trees collectively produce shorter path lengths for particular samples they are highly likely to be anomalies. Again there are some outliers espacially a strong one in Cholesterol worth to take a look later. Classifiers Here I selected some common sklearn classifiers I didn t want to include some common ml algorithms like xgboost lightgbm or catboost since I wanted to play with sklearn only for this notebook. Multivariate Analysis Multivariate analysis MVA is based on the principles of multivariate statistics which involves observation and analysis of more than one statistical outcome variable at a time. For instance pre processing with a discretizer can introduce nonlinearity to linear models. The Minimum Covariance Determinant MCD method is a highly robust estimator of multivariate location and scatter for which a fast algorithm is available. It s important for this case since we don t want our models to predict no disease on actually unhealty person or vice versa. In this part we goin to take our variables and compare them against our target condition which is if the observed patient has disease or not. Uses a subset of training points in the decision function called support vectors so it is also memory efficient. Resing electrocardiographic observations are evenly distributed between normal and left ventricular hypertrophy with ST T minority 67 of the patients had no exercise induced angina Peak exercise slope seems mainly divided between upsloping and flat. Since we have small and noisy data I thougt binning them would be better choice of action for this purpose I m going to choose another sklearn tool K Bbins Discretization This method discretizes features into k bins. Still effective in cases where number of dimensions is greater than the number of samples. Number of major vessels observed seems on similar levels for patients who have disease but 0 observations is good sign for not having disease. Disabling warnings Styling Seeding Reading csv file Taking random samples from data Checking null values Renaming columns. Here s the fun part who doesn t love a bit of modelling We start by loading our train data and labels as X and y s and we get dummy variables for categorical data using one hot encoding. This path length averaged over a forest of such random trees is a measure of normality and our decision function. Baseline Results We have many metrics but I decided to sort them by F1 score since precision and recall is pretty important on this case. It also serves as a convenient and efficient tool for outlier detection. Feel free to comment I ll try to answer it all and as usual if you liked my work please don t forget to vote have good one all Loading packages. In scikit learn PCA is implemented as a transformer object that learns components in its fit method and can be used on new data to project it on these components. AdaBoostClassifier An AdaBoost classifier is a meta estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. GBDT is an accurate and effective off the shelf procedure that can be used for both regression and classification problems in a variety of areas including Web search ranking and ecology. Bivariate analysis can be helpful in testing simple hypotheses of association. Versatile different Kernel functions can be specified for the decision function. GaussianNB GaussianNB implements the Gaussian Naive Bayes algorithm for classification. Certain datasets with continuous features may benefit from discretization because discretization can transform the dataset of continuous attributes to one with only nominal attributes. Looks cool Confusion Matrix One last thing before we finish our sklearn journey I wanted to use another cool sklearn tool to show confusion matrices for each model. This one did a little bit better than isolation forest so let s stick with it for this case. But we can see most of our decision tree based models are overfitting that s something we should take a look at soon. DecisionTreeClassifier Decision Trees DTs are a non parametric supervised learning method used for classification and regression. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. Common kernels are provided but it is also possible to specify custom kernels. Explained Variance This graph shows that first 15 components explains more than 80 of the variance in the data and currently we have 46 of them. Basically I tried to show distribution of data relations between variables and target as well as correlations between each other then did some basic model building. The prediction of the ensemble is given as the averaged prediction of the individual classifiers. 80 Learning Curves Before finalizing our modelling I wanted to use another tool sklearn offers Learning Curves. For this part we going to inspect how s the data distribution is and what patterns we can inspect. It looks like these 5 explains half of the variance in our data. The advantages of support vector machines are Effective in high dimensional spaces. These two components explains almost one third of the variance. RandomForestClassifier The sklearn. Typically MVA is used to address the situations where multiple measurements are made on each experimental unit and the relations among these measurements and their structures are important. I ll do small quotes for each classifier from sklearn s official page GradientBoostingClassifier Gradient Tree Boosting or Gradient Boosted Decision Trees GBDT is a generalization of boosting to arbitrary differentiable loss functions. Sklearn module takes several strategy parameters but we going to use kmeans strategy which defines bins based on a k means clustering procedure performed on each feature independently. Both algorithms are perturb and combine technique specifically designed for trees. Having defected thalium test results is pretty strong indicator for heart disease. Inspecting Age Closer Correlations We going to use pearson correlation for to find linear relations between features heatmap is decent way to show these relations. After doing some usual exploratory data analysis I noticed some of the results doesn t make sense I ain t no expert in field but made me curious and then I found this topic here The ultimate guide to this dataset https www. Bivariate Analysis Bivariate analysis is one of the simplest forms of quantitative analysis. 3 Components More we reduce dimensions further we can visualize it better for our graphs. Rest ECG results showing no direct results but having normal ECG is pretty good sign. Numerical Data vs Target Here we can do these observations Having higher resting blood pressure shows you are little bit more likely to have heart disease. Then we import loads of sklearn modules. Since our decision tree based models overfitting I wanted to look which features are most effecting these decisions I sampled two of the tree based models you can see below Automatic Outlier Detection Before going to tune our models I decided to get rid of some outliers we have pretty small database and we can actually remove them by hand or more basic methods. Categorical Data vs Target Here we can do these observations Males are much more likely for heart diseases. Introduction Hello all In this notebook I tried to play with some sklearn features while exploring and visualizing the heart disease data we have given. Blood sugar has no direct effect on the disease. At 3D we can have this scatterplot showing us some kind of meaningful clusters. The deeper the tree the more complex the decision rules and the fitter the model. On 2D space we can still diverse the clusters according to our target variables. 5 Components We start with 5 components. Let s get building them. In our case we can see that some models overfitting and most of our models can get better with the more data. It was fun sklearn journey for me I hope you had fun while reading it and thanks for taking a look. This has two main benefits over an exhaustive search A budget can be chosen independent of the number of parameters and possible values. It didn t do great on the results we have pretty small dataset and removing some more damaging model performances probably. Numerical Data Most of the continuous variables somewhat close to gaussian distribution with small skews left or right except for oldpeak. RandomizedSearchCV implements a randomized search over parameters where each setting is sampled from a distribution over possible parameter values. The likelihood of the features is assumed to be Gaussian. It doesn t deal with causes or relationships unlike regression and it s major purpose is to describe It takes data summarizes that data and finds patterns in the data. Uni means one so in other words your data has only one variable. Random partitioning produces noticeably shorter paths for anomalies. But I wanted to use what sklearn can offer us for this so we gonna try couple sklearn features. Decision trees learn from data to approximate a sine curve with a set of if then else decision rules. RandomizedSearchCV Let s get rid of overfitting one of the easiest ways of doing it is tuning parameters for our estimators and regularize them. 3D scatterplot is great tool for doing that. It s up to you what dataset you choose the original one gave me better F1 score but as I said EDA didn t make sense to but I have to tell I m just inspecting the data and have no medical knowledge on the field. Again same for Cholesterol it s not strong indicator but patients are little bit more likely to have disease with high cholesterol. Final Words Well this concludes my notebook. Sex 0 Female 1 Male Chest Pain Type 0 Typical Angina 1 Atypical Angina 2 Non Anginal Pain 3 Asymptomatic Resting Blood Pressure Person s resting blood pressure. Patients who had flat slope distribution are more likely to have disease. One hot encoded discretized features can make a model more expressive while maintaining interpretability. Classification is computed from a simple majority vote of the nearest neighbors of each point a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. Categorical Data Here we can do these observations Males on the dataset is more than double of the female observations. com ronitf heart disease uci discussion 105877 his points made sense so I decided to use this dataset after inspecting it Heart Disease Cleveland UCI https www. Most common ches pain type is Asymptomatic ones which is almost 50 of the data 85 of the patients has no high levels of fastin blood sugar. One last note I used some sklearn features just for the sake of showing them they might be not needed in actual use for this case. Thankfully sklearn is coming to help with useful tools for this case too We going to use RandomizedSearchCV for this While using a grid of parameter settings is currently the most widely used method for parameter optimization other search methods have more favourable properties. Even though it s pretty rare in the data if you ST T wave abnormality you are 3 times more likely to have heart disease. This model optimizes the log loss function using LBFGS or stochastic gradient descent. ", "id": "datafan07/heart-disease-and-some-scikit-learn-magic", "size": "18229", "language": "python", "html_url": "https://www.kaggle.com/code/datafan07/heart-disease-and-some-scikit-learn-magic", "git_url": "https://www.kaggle.com/code/datafan07/heart-disease-and-some-scikit-learn-magic", "script": "sklearn.metrics cross_val_score PCA model_check matplotlib.gridspec sklearn.tree seed_all sklearn.naive_bayes kbin_cat AdaBoostClassifier f_imp ctn_freq KNeighborsClassifier matplotlib.ticker IsolationForest sklearn.neural_network plot_learning_curve DecisionTreeClassifier EllipticEnvelope prob_reg cross_validate seaborn numpy conf_mat plot_confusion_matrix learning_curve GradientBoostingClassifier sklearn.decomposition plotly.express sklearn.ensemble hyperparameter_tune sklearn.model_selection MaxNLocator KFold RandomForestClassifier matplotlib.pyplot pandas matplotlib.colors ctg_dist sklearn.covariance interp sklearn.neighbors SVC ListedColormap RandomizedSearchCV GaussianNB sklearn.svm sklearn.preprocessing KBinsDiscretizer MLPClassifier train_test_split ", "entities": "(('Bivariate analysis', 'association'), 'be') (('where setting', 'parameter possible values'), 'search') (('Displaying', 'decision boundary'), 'split') (('they', 'particular samples'), 'be') (('number', 'terminating node'), 'be') (('Males', 'more than female observations'), 'Data') (('support it', 'decision function'), 'use') (('Basically I', 'masks'), 'set') (('diverse set', 'classifier construction'), 'mean') (('Kernel Versatile different functions', 'decision function'), 'specify') (('DecisionTreeClassifier Decision Trees DTs', 'learning non parametric supervised classification'), 'be') (('we', 'mesh'), 'assign') (('even little bit tuned parameters', 'them'), 'm') (('two components', 'variance'), 'explain') (('I', 'model'), 'look') (('as usual you', 'don t Loading good one packages'), 'try') (('5', 'data'), 'look') (('I', 'Learning Curves'), 'Curves') (('you', 'look'), 'be') (('discretization', 'only nominal attributes'), 'benefit') (('We', 'so elliptic worth look'), 'assume') (('most', 'more data'), 'see') (('angina Peak exercise exercise induced slope', 'mainly upsloping'), 'distribute') (('which', 'time'), 'base') (('thalium test defected results', 'heart pretty strong disease'), 'be') (('Asymptomatic which', 'fastin blood sugar'), 'be') (('that', 'efficiency'), 'decrease') (('further we', 'better graphs'), 'component') (('ST T Wave Left Ventricular Hypertrophy Max Heart 0 1 2 Rate', 'relative rest'), 'Cholesterol') (('strong patients', 'high cholesterol'), 'same') (('where number', 'samples'), 'effective') (('Univariate Analysis Univariate analysis', 'simplest data'), 'be') (('Meanwhile it', 'less than it'), 's') (('more data', 'better results'), 'show') (('Disabling warnings', 'columns'), 'rename') (('I', 'only notebook'), 'classifier') (('It', 'them'), 'involve') (('that', 'variance'), 's') (('Final this', 'notebook'), 'word') (('it', 'which'), 'be') (('so we', 'sklearn couple features'), 'want') (('we', 'etc'), 'want') (('Blood sugar', 'disease'), 'have') (('they', 'case'), 'use') (('then I', 'https here ultimate dataset www'), 'notice') (('It', 'outlier detection'), 'serve') (('we', 'hand basic methods'), 'base') (('processing', 'models'), 'introduce') (('pretty targets', 'it'), 'say') (('RandomizedSearchCV s', 'them'), 'let') (('little', 'score'), 'improve') (('MLP Classifier Multi', 'Perceptron classifier'), 'layer') (('so I', 'Heart Disease Cleveland UCI https www'), 'discussion') (('structures', 'measurements'), 'use') (('you', 'heart 3 times more disease'), 's') (('Chest Asymptomatic Resting Pressure 0 Female 1 Male Pain 0 Typical Angina 1 Atypical 2 Non Anginal Pain 3 Person', 'blood pressure'), 'sex') (('Random partitioning', 'anomalies'), 'produce') (('blood disorder', 'fluoroscopy'), 'Slope') (('who', 'more disease'), 'be') (('Males', 'heart much more diseases'), 'Data') (('budget', 'parameters'), 'have') (('0 observations', 'good disease'), 'seem') (('we', 'target variables'), 'diverse') (('Support Vector Machines Support vector machines SVMs', 'classification regression'), 'be') (('Age marker patient sizes', 'patient condition'), 'have') (('you', 'heart little bit more disease'), 'Data') (('Decision trees', 'decision then else rules'), 'learn') (('how models', 'confidence intervals'), 'Results') (('we', 'model more damaging performances'), 'do') (('Elliptic EnvelopeLet s', 'detection automatic outlier method'), 'try') (('ensemble module', 'decision randomized trees'), 'include') (('Lastly older patients', 'heart more disease'), 'be') (('advantages', 'high dimensional spaces'), 'be') (('linear relations', 'decent relations'), 'inspect') (('K Bbins method', 'k bins'), 'be') (('3D scatterplot', 'great that'), 'be') (('currently most widely used method', 'more favourable properties'), 'come') (('path length', 'normality'), 'average') (('Isolation IsolationForest', 'selected feature'), 'Forest') (('currently we', 'them'), 'Explained') (('I', 'GradientBoostingClassifier Gradient Tree Gradient Boosted Decision Trees loss arbitrary differentiable functions'), 'do') (('thal', 'outcome condition'), 'choose') (('Rest ECG results', 'normal ECG'), 'be') (('they', 'end'), 'include') (('fast algorithm', 'which'), 'be') (('we', 'effects'), 'Depression') (('so s', 'case'), 'do') (('major It', 'data'), 'doesn') (('we', 'patterns'), 'go') (('y we', 'one hot encoding'), 's') (('that', 'data features'), 'be') (('model', 'LBFGS'), 'optimize') (('Discretization Discretization', 'discrete values'), 'provide') (('I', 'pretty case'), 'Results') (('Bivariate Analysis Bivariate analysis', 'quantitative analysis'), 'be') (('algorithms', 'specifically trees'), 'be') (('they', 'induced angina'), 'be') (('I', 'field'), 's') (('observed patient', 'disease'), 'go') (('model', 'more interpretability'), 'make') (('shelf accurate that', 'Web search ranking'), 'be') (('modelling', 'large set'), 'be') (('don models', 'actually unhealty person'), 's') (('which', 'point'), 'compute') (('Chest pain type', 'disease highest outcome'), 'be') (('it', 'training data'), 'be') (('pretty heart', 'ST depression levels'), 'find') (('such subsequent classifiers', 'more difficult cases'), 'be') (('we', 'look'), 'see') (('I', 'EDA'), 'let') (('GaussianNB GaussianNB', 'classification'), 'implement') (('it', 'custom also kernels'), 'provide') (('Fitting PCA', '46'), 'number') (('Basically I', 'model then basic building'), 'do') (('likelihood', 'features'), 'assume') (('that', 'components'), 'learn') (('data', 'only one variable'), 'have') (('we', 'meaningful clusters'), 'have') (('prediction', 'individual classifiers'), 'give') (('which', 'feature'), 'take') (('we', 'heart disease data'), 'try') ", "extra": "['biopsy of the greater curvature', 'disease', 'outcome', 'patient', 'test', 'procedure']"}