{"name": "arabic handwritten digits recognizer ", "full_name": " h1 Arabic Handwritten Digits Recognizer h3 Author Abdelwahed Ashraf h3 Linkedin Link h3 Kaggle Link h2 Keras with CNN h3 My Goal is Accuracy 0 9999 h2 Outline h1 Introduction to CNN Keras Acc 0 999 top 8 h2 1 Introduction h2 2 Data preparation h2 3 CNN h2 4 Evaluate the model h2 5 Prediction and submition h1 1 Introduction h1 2 Data preparation h2 2 1 Load data h2 2 2 Check for null and missing values h2 2 3 Normalization h2 2 4 Reshape h2 2 5 Label encoding h2 2 6 Split training and valdiation set h1 3 CNN h2 3 1 Define the model h2 3 2 Set the optimizer and annealer h2 3 3 Data augmentation h1 4 Evaluate the model h2 4 1 Training and validation curves h2 4 2 Confusion matrix ", "stargazers_count": 0, "forks_count": 0, "description": "e the area size pooled each time more the pooling dimension is high more the downsampling is important. We have to choose the pooling size i. Confusion matrix section four b 5. The first is the convolutional Conv2D layer. This metric function is similar to the loss function except that the results from the metric evaluation are not used when training the model only for evaluation. To keep the advantage of the fast computation time with a high LR i decreased the LR dynamically every X steps epochs depending if it is necessary when accuracy is not improved. 17 sklearn versions. Sometime it is very difficult to catch the difference between 4 and 9 when curves are smooth. In order to make the optimizer converge faster and closest to the global minimum of the loss function i used an annealing method of the learning rate LR. Moreover the CNN converg faster on 0. 3 Data augmentation In order to avoid overfitting problem we need to expand artificially our handwritten digit dataset. input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session Import the necessary libs convert to one hot encoding Load the data plot Count numbers Check the data Normalize the data to make CNN faster Reshape image is 3D array height 28px width 28px canal 1 Encode labels to one hot vectors ex 2 0 0 1 0 0 0 0 0 0 0 Set the random seed Split the train and the validation set for the fitting Draw an example of a data set to see Please try with this number to understand how augment you data 1111 111 101 144 663 Creating CNN model print out model look Define the optimizer Compile the model Set a learning rate annealer Audjusting learning rate Turn epochs to 30 to get 0. Evaluate the model 4. 114 With data augmentation i achieved 99. 671 of accuracy with this CNN trained in 2h30 on a single CPU i5 2500k. OutlineThe following sections are included in this notebook Introduction to CNN Keras Acc 0. Define the model section three a 2. com in abdelwahed ashraf 090523169 Kaggle Link https www. I choosed to set 32 filters for the two firsts conv2D layers and 64 filters for the two last ones. The LR is the step by which the optimizer walks through the loss landscape. Keras requires an extra dimension in the end which correspond to channels. I want to see the most important errors. There is no missing values in the train and test dataset. Split training and valdiation set section two f 3. Prediction and submition section five 1. The Flatten layer is use to convert the final feature maps into a one single 1D vector. 67 of accuracyFor the data augmentation i choosed to Randomly rotate some training images by 10 degrees Randomly Zoom by 10 some training images Randomly shift images horizontally by 10 of the width Randomly shift images vertically by 10 of the height I did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9. For those six case the model is not ridiculous. 99286 set input mean to 0 over the dataset set each sample mean to 0 divide inputs by std of the dataset divide each input by its std apply ZCA whitening randomly rotate images in the range degrees 0 to 180 Randomly zoom image randomly shift images horizontally fraction of total width randomly shift images vertically fraction of total height randomly flip images randomly flip images Fit the model Prediction model Draw the loss and accuracy curves of the training set and the validation set. Data preparation section two 1. 999 top 8 1. I ll show you the training and validation curves i obtained from the model i build with 30 epochs 2h30 The model reaches almost 99 98. Training and validation curves section four a 2. Be carefull with some unbalanced dataset a simple random split could cause inaccurate evaluation during the validation. Each filter transforms a part of the image defined by the kernel size using the kernel filter. These are used to reduce computational cost and to some extent also reduce overfitting. Check for null and missing values section two b 3. Data preparation 2. It is the error rate between the oberved labels and the predicted ones. This function will iteratively improve parameters filters kernel values weights and bias of neurons. 1 Load data a random split of the train set doesn t cause some labels to be over represented in the validation set. However it seems that our CNN has some little troubles with the 4 digits hey are misclassified as 9. The metric function accuracy is used is to evaluate the performance our model. 5 Label encodingLabels are 10 digits numbers from 0 to 9. For RGB images there is 3 channels we would have reshaped 784px vectors to 28x28x3 3D matrices. The rectifier activation function is used to add non linearity to the network. Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Data augmentation section three c 4. callbacks i choose to reduce the LR by half if the accuracy is not improved after 3 epochs. The improvement is important Without data augmentation i obtained an accuracy of 98. Firstly I will prepare the data handwritten digits images then i will focus on the CNN modeling and evaluation. com abdelwahed43 data image png base64 iVBORw0KGgoAAAANSUhEUgAAAX8AAACECAMAAABPuNs7AAAAflBMVEX 8AAADe3t5QUFDQ0NA1NTV3d3e3t7fb29suLi5LS0u0tLRzc3Ps7Ozj4 OpqamUlJTCwsLz8 NZWVlnZ2cPDw8oKCiIiIgjIyP39 dDQ0PU1NTJyckcHBzo6OhiYmIXFxeenp4 Pj6AgICMjIyYmJgzMzN2dnasrKxtbW2l rdzAAAR50lEQVR4nO2d6aKiOLeGw6CgCAiIEkCUQbfe w0ehgRWwmJ31e7C7q9P3h9VWwIZHkLmlRCipKSkpKSkpKSkpKSkpKSkpKQEFToWKuYcOJhe7uBaoA87OXsY99qhg2uGeu1kgyt94c7M6xwPuRhcXdHVGX46AY8XnqhwcNVxr7N1 Hsaqpg5V7gzIxzgrjf2MO6qsXQ2uGvDEC48zLy 4a6McC5ejdLhf P7eHmDq4 71h lf2bOe9z5e 6nX J 7H9Y3kl0Pf4S xPuivK Bs61 yNiD8f4w9 zv6zO3wepnvGvUxgZnP91M6RskX 87P6F NMwT8V7EP6pS5wd sH JPAWuC E 77h n7MOZz haMzJx YpweGcOyyD qL0H Pomke2T 56jpSnW3TPqfP AfWwv8r EDfgoC 9vhthF8WZn xR9 GizVMv8yg5GZ8TesgpIiH6qLRf4l7f6F BuiS0W5xD8JKC0KRw9ZYL K wTI gv8fV4IDoL8D7ZTijljXf4N 0Ooc7zfwgjI M FoQGtXEffmH8jfbFVt5L5t8G7FpCJSPyv7TNDsuI4n1JyW xL0AxssD FgaFC94 4P OvvpLqZV8hP92unBY4E 4d GPDDGmGL873nblKqHZovAX6sD4sIvXeCftAFFyRD nP mdLt4TXnF B dsXC 4 V bOm7LbGnFt7EP3Z8npL6E xP4ELz2 wTmxRb6Ar57 su7VHhOtQ3vc2cf vovhb4n9tuSMrDF nvyjInNCxsl3hTQTHxj8jo6QJ zWrf 4M4Y0E18d8E7LMwyFjtrck glfOKP Iwq4A5B9bJBMbEpC Ty6tb3bbyjyG7pCZZP53E5ZtkP D8PIhLokO d qLM9fUfc T3Z nvPXCnv8ql4L b t 9lFz Zv2GzkvQamjxpK KPA3glQfnf6RhPif87pFIvDfJPiXesg6y743S7o zf4VL d8bc cxZwci8huV ScdCHPA 2CV7NrZY hD5J0HRvqGzHnLCE 99wdJ9ICEPaUX iXAF56 ZsLEP R9nfTih O9GMkLhBUn8n3kIuwCQv0 MzXg5XOLfTFkYtj8v4YHzDxKEfxT2l5uxtQrKn2KIXFnUGe mf4w Xv60pfQT59 MeskC xsJ67PgLPE EB26Q 6gM3Qhi 2fKrN4GQ75n3MWryte p9YVRfy1wfrX8 4GrXXFlB cF2d 9UFF8Irzn8zNQWk rckUidKbP8c5B6lxP9rkb9PdINfbghd4q9ZIW97Cf2vdxj2pHfZK57zj8ti8Dx26Vbi3zq6YRF0qbrR9 r8hfp3CG7O qWC zbiiy8RKC I K BtJomcT RKkfTeEA sHUtIl0sl3k 6K8lBL7v9YQxonxFflfTVYqtZeHv4T 74aVifcgv67OH7Q s2qBf yBbCy2 5OmbYEGl ENSO3 d1gYGpDE 95 54X9OiQz vcx s20TeL Ovx0xT57123ve LZmkS e HzzJ kXLGf5RBndX5v8efJoOI8C cJf5tbrm0RVj2xvl35QisIeT2j7Y 5Dqhul9J BNCe7BGV4ezepB5LfBv2zAo 7b57poZofxmgX9Nv jzVdb3MhbG33warc1 jDghjyX 2sOeKmBs C0jhJYo 9Z3OHo3498qrrqZlWAj1r9tzeK2om2 4Hczr8XxhyMv4eTxt7oNqxkbXwJ V5867IlHl lrrn1enX JpncYfox BGZM0PHn6OUOkZ PPzTEv3 LvwvyWA4sJ 6bAYnnHca7mNci 42ePVH oiD VLjp0WXxJf5PvZs WHv8uZ DK0cYc 7JZWrlLMy 3Mw UXP 12DqjS7xZxLGf07dvGBzB84of606Dh AL Nvv9Yv2Xlx L 6VJ Yf4H64fxX1bcakPG36Da1j36D 1w4f65f5r 1LNZsH96 gvqY OPM32GP6Z E 8rc05x5z8x 63 P W DvX35p X4m 94i uKz c0B09seXDP04UPJHj58Yc5vtv7EeqNesxUm4QkPmXldHlCvWSvCxuP1YA9 oV6f2ChAgHp94CtflJSUlJSUlJSUlJSUlJSUlJSUlJSUlJSU kLedYPouWfOt MO0d0cXPMEe3jHrTnOmOvmzua ygTzOmGTN26Mue74vNx2h3mdsHk5U4zXc7h5xydvNqjXMZv ctBon48r8cdn2 iS3P E O9ZWv 24PU O 8b15f3FNqfn3 Pa758a8b DUNZgX JFrj m ff6zbd4bTCWeIfvcQXgfJvHGuYFkf5R OcucS eoUwyTj c83i9gP d4slY8bff8P7ZP5xegArj1blH2 7RX6ERnyhrMQ zUshSXP 7oruAcLI5R SnH hkuIDe4T ddD0eF4IR1u gH fWFi9i atqE6vE kn7yygoKMsSp Q2c bbbGTS5 olCwsJP5VyUllNhttdwt0p3zP581Y4E 8YxXscD 7JOy 79bfxvQ4If8b R1R k3VNiEQuB oKTQdTpZJqzKf6 Pv7c3jL9mC2mS He22160jzWnvzTjX7XssnG9q8C LK6aIMA bd9pb9rS8g8PF2oI K P6t1qD41XAf qrEYfWTEj888C2waWg5D KSzq o5P82fYZvytb yvb4Rmj uQVF in761pM7MDBh5CfxDYV8DkX8cZsch6 ZBC9wNAP97aVmObtqe5fjT2kbA3yDlcC0OzCfKP UuJ9gKg z9YeVZWnzE ldL 5J SYH5l8j 4BGfZaPoWEn8s9APiHVv9OlLF jXui8aL8Hyx8uedc 3H07hTnxvx3t5iuq7rsqejvZ6APgfw684QNIQm64IPGP3OPODabPD A 595guhSavARalX kgUso 4fQFBX4W0T4NkT bbXu sm4NcqMv3YqwsLL8nFBIeBftQGAtmII T9AdghG 1VY jeskkhG826Jf0mi2AI1CeAfuQ57gtvOrMrfgAZ4OP8NfWiTIP8qk cwgfxf9pQElH byQyD2GNAgP NAJOb1ldHw vfy htQv4pq7KMkL bmf11qj1zd7SNgvZ3lntl6eQF0Jr89zD74 y1EFbAkH 77Yjm71L9WxPzKTjP 78n8AYBf9eFdmMO3S3wP4U8mpD LqDDaniK89 ZedLFrkT4tx9PF3S3YPcT N8uoYHJ1sWaOP8HBblcKH8CeZcyyf7RJ7nQz5nz10FHAvAPIf t8DEwr0X7L48zFNqfe7dPikG59Yxs 9vFINHHEQDI 0LK3jv6Cf6d a rRcPmdYPFOWZ B1oqkH bd g2 2u7krgXWI7L CMCOAv5f3zlZ4cu2r 3eQO3vyt7eM7oi8D HoT99ddIGPLfFPR2f5C8 AT frDNHbBkQzMRGf xvMkYQCr pTak3P6Po5D40zcy43 EPWXAn7o8xHfBd4ZiXov8z7yjKvKPSdcE0sdOrsB xzpaT JWc 5dqUqJd962bbfP8GdEWWgI xoYwEvtn2IyJcT4a10uIw1vyc34lwvjP1vid6jumwsJee3PvJbtv1gROLM Da7G1EsX GeUJWfH99iQxn Ksjy3lTiP50f457w1jNo ljj LSV2vXlOY1XY E9pE ON73 iHT3QhwX8r5QEvu9bbeU0fj3Ma4l zFpn8viDT3NvGkMQ E82lQ9iVgj QTVlNkBrlv8hLbxWhTc2VBD u9wbKQj82wqWUDO3tnwAG OfbC6 GfSb6M34G4XtR0my21R3sf9V9 Ys1AJjTzj toOM8j87LhgMEu3fpxqpJNki 6tF8vvK LXavKX7VlMBj43 P6YKQB7 YfZIpDCTBf6d0qNzxPhrhl EnpeXR8n Wqs6w6AbHCFiXsv8G5y luxvU8WzNP5 9 tL Pj OSfr279LY2A4 6oZiwl5 DMx0vJQH42o34Zjef4r7gLC5r uu93ufL3GP51 OSzwF7Q4 3Ituiy NP i9RWFsj ttcC Gvbw c Yn 6v8Wf6z Dn6fzR tt Mc 9J bflnfNZfqe 77eDH Je660v7b1NZRMWfdRsWGKkL8YbYLPMFdeV3t4l7zoUDcdazoca DbxPlfe815fFCHxaGKJWUlJSUlJSUlJSUlJSUlJSUlJSUlJT 3aImKn7aaoa65myc3cUf5oP0uCuzXSVejnrNBukp6mryo21t3Gs2SB9KD 1SvHI2 l gzsIS2T nNc6f cD81 sf5tc2LKaX5v v91 uxMf37 98DPkPofmv dn78mzLhi FPXZQcJ YB ww LWoH lp hNuMvWib80 y7HY4zZqcy4 9AG1GE 9uhhP6I 85ogsnwUuS S9O0X32WciulH D WrA 0t7ijRL uNrDdd2fOf XBz8 1H4UGT u8YldsBXkS w3zBjXpF ZLdPHqfyHPKvMjMYfB1tSEf Rt3qIFp2LPDnx4PJ EtCYT0i8W8jYj mFYFr8 eblpMS5y QlPkHhD52S cfcb24yePgOvw422YtQITnH V6ek 6TPhF2BlezOu377mZ6bn2RbA9wMuffDxoahCPaO66OTh6TOS o nx6H7I hQcALxgf2RQWGtJ PfEfi e zXosKsoGQpigf8jFA Hg wfk 3X3mRWFMzrnNTG5rmPjl4B6xGU 8nB Ueu0xBg1Sbyz4o2suZkIL8u 0vf O0PTsDtrxPhkEaJPxUrU8B cxuwlb19x5BYgb8lNwAE vpzM84E84frHnEqgCGje8 4Bgo ze9XfNiSpbIv08csM5bl38HtLz2qcPPH0zodAaRzP8xHhsm87 2XZ624Ni qAKfuBirwFbRLPbCVZ0gH jgnXB5QCGeQ3qXzucfkD Y5SW G DSNtOxwRj M3pfOxV R9aJtmQumLh Ed3yf5U0wqpLzXxr2yakVcZEi8k2e50m PXLnZIvWAy3hDsj2Dh3ptfI wjPR zMNx oOEv4JbzXVxE i v2S3vx xPu Ds6lTSD HvEt0MqRuscub8M9jYF nr8GhOgX9UkD5VlnEIxg6E3P68RQ6ZMjrkL9g1Bf3XMOevHWiGnH9qFPyeg52i K3uWNRbqPOIifxvhUfDbLIAXJP TU0e9K2ybVkf6cv8y kvuzEP 4sOFvUbfaesvJ8 yUX2FnD808ptFvd908g KtsHI4A K8W31rAD9D258Y DuFlKH NcMr9eHTn udvdikMx9W c 4 rChF jVxl85 j5Pr7kGFunvG 5zB lI4 zcDhmERgftvQA LcQMhWP7f3OGEtdRFz38chyUcdvwp1v 9TdXDmvxvfdPE8Mbsj CvYWUotX884j5AUS23Py0Swn6OzP pw IL8m adm33f6xVb4ss7v9zz8h5zl97DYal fh2Rf6lxxL3IMF5kT88AHo1 merM Htd ExlvgbIciLEv N3qZ5Qjyz w2ICw4RnO3 YMOj5SH uAmp7x9L37dJNhgZY y1sRIV Bte299I Gn3A4F onPL3rgk5SL j T LtF5P LODT2x869BYSz3f dtwt28SXH 2tVp V qf9uCGG6zIYz xKfXAPTFd1NC d9Q m0VbDaZNX2YAv DAL2Pnk8X7O8q jH nUKdNyTm O wlJiNvyVGNz4eDu3Ief83 nLZ3nAI 5ba0H7dG9fZ Ge83xt7Ixkb88xrafyNZhj znraAd1rgX jToOf8XGBfzINQKzPfzjndpF CBpA6Pi UerhbP r0bFxiWmkKH tRaju6a55Of9w JmyHeR w 7Lnk1eIPyv9tgCWJX u2USwDYMtv Ad mfRTd lenS FtcemGfTZHx 7Q0zXzIjz hv HEf4O OTMhw8b H MAxLrjD1v IcRllfmv6tgn5s Pf5V8nvSP2z9exiEoZX aC O GT dP87 fWR8memv7X 8wfOnxXz zhM8Wv8F7z R Yfi8rAxBEe9pgqtjzFTLFn9zyqqKtRMf5 inmdsmk4d4OHzLy 7DGvU5YvMjxefAmDgXvNzGYt OFxfkpJSUlJSUlJSUlJSUlJSUlJSUlJSUlJSUnpc o DwttbqVDDroAAAAASUVORK5CYII If this Kernel helped you in any way I would be very much appreciated to your UPVOTES Keras with CNN My Goal is Accuracy 0. Evaluate the model section four 1. I choosed to build it with keras API Tensorflow backend which is very intuitive. The last 9 is also very misleading it seems for me that is a 0. Predict and Submit results section five a 1. Once our model is ready we fit the training dataset. Load data section two a 2. The validation accuracy is greater than the training accuracy almost evry time during the training. Can judge whether it is under fitting or over fitting Look at confusion matrix Predict the values from the validation dataset Convert predictions classes to one hot vectors Convert validation observations to one hot vectors compute the confusion matrix plot the confusion matrix Show some wrong results and the difference between the predicted label and the real labe Errors are difference between predicted labels and true labels Probabilities of the wrong predicted numbers Predicted probabilities of the true values in the error set Difference between the probability of the predicted label and the true label Sorted list of the delta prob errors Top 6 errors Show the top 6 errors predict results y_pred Evaluate model select the indix with the maximum probability Save the final result in cnn_mnist_submission. Computation will be much much faster For computational reasons i set the number of steps epochs to 2 if you want to achieve 99 of accuracy set it to 30. So we can safely go ahead. In the last layer Dense 10 activation softmax the net outputs distribution of probability of each class. 2 Confusion matrixConfusion matrix can be very helpfull to see your model drawbacks. relu is the rectifier activation function max 0 x. in order to minimise the loss. 2 Set the optimizer and annealerOnce our layers are added to the model we need to set up a score function a loss function and an optimisation algorithm. fit X_train Y_train batch_size batch_size epochs epochs validation_data X_val Y_val verbose 2 With data augmentation to prevent overfitting accuracy 0. This technique also improves generalization and reduces the overfitting. 1 Load dataWe have similar counts for the 10 digits. It is like a set of learnable filters. 6 Split training and valdiation set I choosed to split the train set in two parts a small fraction 10 became the validation set which the model is evaluated and the rest 90 is used to train the model. Its better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function. 0 GPU capabilites from GTX 650 to recent GPUs you can use tensorflow gpu with keras. you found this notebook helpful or you just liked it some upvotes would be very much appreciated That will keep me motivated This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github. We need to encode these lables to one hot vectors ex 2 0 0 1 0 0 0 0 0 0 0. IntroductionThis is a 5 layers Sequential Convolutional Neural Network for digits recognition trained on MNIST dataset. Dataframe as 1D vectors of 784 values. Combining convolutional and pooling layers CNN are able to combine local features and learn more global features of the image. For those who have a 3. The most important function is the optimizer. Since i set epochs 2 on this notebook. That means that our model dosen t not overfit the training set. Some popular augmentations people use are grayscales horizontal flips vertical flips random crops color jitters translations rotations and much more. Let s investigate for errors. Some of these errors can also be made by humans especially for one the 9 that is very close to a 4. Normalization section two c 4. It looks at the 2 neighboring pixels and picks the maximal value. I choosed RMSprop with default values it is a very effective optimizer. Arabic Handwritten Digits Recognizer Author Abdelwahed Ashraf Linkedin Link https www. We define the loss function to measure how poorly our model performs on images with known labels. By applying just a couple of these transformations to our training data we can easily double or triple the number of training examples and create a very robust model. The CNN can isolate features that are useful everywhere from these transformed images feature maps. We could also have used Stochastic Gradient Descent sgd optimizer but it is slower than RMSprop. With the ReduceLROnPlateau function from Keras. 9999 Prerequisites fundamental coding skills a bit of linear algebra especially matrix operations and perhaps understanding how images are stored in computer memory. Dropout is a regularization method where a proportion of nodes in the layer are randomly ignored setting their wieghts to zero for each training sample. 3 NormalizationWe perform a grayscale normalization to reduce the effect of illumination s differences. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. The most important errors are also the most intrigous. This layer simply acts as a downsampling filter. For example the number is not centered The scale is not the same some who write with big small numbers The image is rotated. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive monotonically decreasing learning rate. 1 Training and validation curvesThe code below is for plotting loss and accuracy curves for training and validation. The second important layer in CNN is the pooling MaxPool2D layer. MNIST images are gray scaled so it use only one channel. 7 accuracy on the validation dataset after 2 epochs. com kaggle docker python For example here s several helpful packages to load Input data files are available in the read only. 98114 history model. Reshape section two d 5. We reshape all data to 28x28x1 3D matrices. In the end i used the features in two fully connected Dense layers which is just artificial an neural networks ANN classifier. Label encoding section two e 6. Since we have 60 000 training images of balanced labels see 2. Filters can be seen as a transformation of the image. 1 Define the modelI used the Keras Sequential API where you have just to add one layer at a time starting from the input. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima. Our model is very well trained 4. To start with machine learning course and deep learning specialization 5 courses we suggest coursera courses by Andrew Ng. Set the optimizer and annealer section three b 3. This Notebook follows three main parts The data preparation The CNN modeling and evaluation The results prediction and submission 2. We can make your existing dataset even larger. For that purpose i need to get the difference between the probabilities of real value and the predicted ones in the results. The higher LR the bigger are the steps and the quicker is the convergence. CNN section three 1. We use a specific form for categorical classifications 2 classes called the categorical_crossentropy. 4 ReshapeTrain and test images 28px x 28px has been stock into pandas. The kernel filter matrix is applied on the whole image. To avoid that you could use stratify True option in train_test_split function Only for 0. 2 Check for null and missing valuesI check for corrupted images missing values inside. We can get a better sense for one of these examples by visualising the image and looking at the label. I plot the confusion matrix of the validation results. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit. Here we can see that our CNN performs very well on all digits with few errors considering the size of the validation set 4 200 images. It combines all the found local features of the previous convolutional layers. 9967 accuracy Without data augmentation i obtained an accuracy of 0. This flattening step is needed so that you can make use of fully connected layers after some convolutional maxpool layers. Introduction section one 2. ", "id": "abdelwahed43/arabic-handwritten-digits-recognizer", "size": "18562", "language": "python", "html_url": "https://www.kaggle.com/code/abdelwahed43/arabic-handwritten-digits-recognizer", "git_url": "https://www.kaggle.com/code/abdelwahed43/arabic-handwritten-digits-recognizer", "script": "Flatten sklearn.metrics keras.preprocessing.image keras.layers keras.callbacks train_test_split mnist Dropout Sequential MaxPool2D keras.datasets to_categorical # convert to one-hot-encoding keras.utils.np_utils Conv2D seaborn numpy plot_confusion_matrix ReduceLROnPlateau sklearn.model_selection confusion_matrix ImageDataGenerator IPython.display Image matplotlib.pyplot RMSprop Dense keras.utils pandas keras.optimizers BatchNormalization plot_model display_errors keras.models matplotlib.image ", "entities": "(('you', 'maxpool convolutional layers'), 'need') (('90', 'model'), 'set') (('regularization where proportion', 'training sample'), 'be') (('validation accuracy', 'evry almost time training'), 'be') (('which', 'keras API Tensorflow backend'), 'choose') (('We', 'one hot vectors'), 'need') (('Confusion matrixConfusion 2 matrix', 'model very drawbacks'), 'be') (('Filters', 'image'), 'see') (('how poorly model', 'known labels'), 'define') (('Label 5 encodingLabels', '9'), 'be') (('when curves', '4'), 'be') (('OutlineThe following sections', 'Keras Acc'), 'include') (('Audjusting', '0'), 'list') (('7 accuracy', '2 epochs'), 'dataset') (('you', 'keras'), '0') (('CNN', '4 200 images'), 'see') (('We', 'categorical classifications 2 classes'), 'use') (('curvesThe 1 Training code', 'training'), 'be') (('It', 'error oberved labels'), 'be') (('It', 'previous convolutional layers'), 'combine') (('We', 'label'), 'get') (('technique', 'overfitting'), 'improve') (('when someone', 'digit'), 'be') (('model dosen', 'training set'), 'mean') (('RMSProp update', 'learning aggressive monotonically decreasing rate'), 'adjust') (('activation rectifier function', 'network'), 'use') (('We', '28x28x1 3D matrices'), 'reshape') (('function', 'neurons'), 'improve') (('model', 'almost 99 98'), 'show') (('perhaps how images', 'computer memory'), '9999') (('we', 'training dataset'), 'be') (('labels', 'validation set'), 'datum') (('optimizer', 'probably local minima'), 'be') (('gray it', 'only one channel'), 'be') (('hey', '9'), 'seem') (('it', 'such 6'), 'rotate') (('image', 'big small numbers'), 'center') (('where you', 'input'), 'use') (('that', 'data augmentation techniques'), 'approach') (('then i', 'CNN modeling'), 'focus') (('Combining', 'image'), 'be') (('I', '64 two last ones'), 'choose') (('results', 'only evaluation'), 'be') (('layer', 'downsampling simply filter'), 'act') (('optimizer', 'loss landscape'), 'be') (('simple random split', 'validation'), 'cause') (('you', '30'), 'be') (('that', 'images feature everywhere transformed maps'), 'isolate') (('60 training 000 images', '2'), 'see') (('second important layer', 'CNN'), 'be') (('validation', 'training set'), 'mean') (('These', 'also overfitting'), 'use') (('671', 'CPU i5 single 2500k'), 'train') (('you', 'Only 0'), 'avoid') (('3 NormalizationWe', 'differences'), 'perform') (('i', '98'), 'be') (('i', 'predicted results'), 'need') (('when accuracy', 'X steps dynamically epochs'), 'decrease') (('that', 'very 4'), 'make') (('This', 'distributed way'), 'drop') (('IntroductionThis', 'MNIST dataset'), 'be') (('grayscales horizontal vertical flips', 'crops color jitters translations rotations'), 'be') (('Load', '10 digits'), 'have') (('it', 'RMSprop'), 'use') (('Goal', 'CNN'), 'image') (('which', 'channels'), 'require') (('Flatten layer', '1D one single vector'), 'be') (('maximum probability', 'cnn_mnist_submission'), 'judge') (('test 4 ReshapeTrain 28px 28px', 'pandas'), 'image') (('i', 'learning rate LR'), 'use') (('we', 'digit artificially handwritten dataset'), 'augmentation') (('model', 'six case'), 'be') (('i', '0'), 'accuracy') (('function metric accuracy', 'performance'), 'use') (('Notebook', 'data CNN results prediction'), 'follow') (('3 we', '28x28x3 3D matrices'), 'be') (('kernel filter matrix', 'whole image'), 'apply') (('accuracy', '3 epochs'), 'callback') (('Training', '2'), 'section') (('It', 'maximal value'), 'look') (('fit X_train batch_size epochs epochs validation_data X_val batch_size Y_val', 'overfitting accuracy'), 'Y_train') (('it', 'default values'), 'choose') (('we', 'loss function'), 'set') (('we', 'Andrew Ng'), 'start') (('It', 'kaggle python Docker image https github'), 'find') (('which', 'Dense two fully connected layers'), 'use') (('i', '99'), '114') (('that', 'me'), 'seem') (('filter', 'kernel filter'), 'transform') (('I', 'validation results'), 'plot') (('we', 'very robust model'), 'double') ", "extra": "['test']"}