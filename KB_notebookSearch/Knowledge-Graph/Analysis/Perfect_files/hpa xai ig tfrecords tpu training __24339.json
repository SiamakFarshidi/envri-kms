{"name": "hpa xai ig tfrecords tpu training ", "full_name": " h1 Human Protein Atlas Single Cell Classification h2 Exploratory Data Analysis EDA h2 TABLE OF CONTENTS h3 0 xa0 xa0 xa0 xa0IMPORTS h3 1 xa0 xa0 xa0 xa0BACKGROUND INFORMATION h3 2 xa0 xa0 xa0 xa0SETUP h3 3 xa0 xa0 xa0 xa0HELPER FUNCTIONS h3 4 xa0 xa0 xa0 xa0DATASET PREPERATION h3 5 xa0 xa0 xa0 xa0MODEL BUILDING h3 6 xa0 xa0 xa0 xa0MODEL TRAINING h3 1 1 WHAT IS EXPLAINABLE AI GENERAL INFO h3 1 2 THE GOAL h3 1 3 CURRENT XAI APPROACHES h3 1 4 WHERE IS XAI NEEDED h3 1 4 Why Is XAI Needed The Case For Growing Global AI Regulation h3 6 1 Model Paramaters Configuration h3 6 2 Model Creation Initialization ", "stargazers_count": 0, "forks_count": 0, "description": "One other place XAI can be used is when working with black box models. We are however not bound by this and can use a smaller larger size if we want. An example technique that most are familiar with is clustering t SNE https en. As an example consider this image https commons. NoiseTunnel Depends on the choice of above mentioned attribution algorithm 1. Layer Gradient Activation Gradient Computes element wise product of layer activations and the gradient of the output w. It adds gaussian noise to each input example samples times selects a random point between each sample and randomly drawn baseline from baselines distribution computes the gradient for it and multiples it with input baseline. Guided GradCam Gradient Computes the element wise product of Guided BackProp and up sampled positive GradCam attributions. Tree Ensembles Support Vector Machines Multi Layer Neural Network MLPNN Convolutional Neural Network CNN Recurrent Neural Network RNN 1. How will your model generalize to new fireboats What about fireboats without water jets Read on to learn more about how IG works and how to apply IG to your models to better understand the relationship between their predictions and underlying features. Feature Ablation Perturbation Assigns an importance score to each input feature based on the magnitude changes in model output or loss when those features are replaced by a baseline usually zeros based on an input feature mask. to the neurons multiplied by the gradients of the neurons w. A survey on explainable artificial intelligence XAI Towardsmedical XAI. To understand what that is we will see the definitions and examples of the terms Transparent and Black Box Models TRANSPARENT MODELSThese are models algorithms that are easily interpretable and DO NOT generally requre XAI. On the other hand a mathematical analysis of patterns may provideinformation in high dimensions. In it we will walk through an implementation of IG step by step to understand the pixel feature importances of an image classifier. reduce_sum 1 y_hat y axis 0 soft_f1 2 tp 2 tp fn fp 1e 16 cost 1 soft_f1 reduce 1 soft f1 in order to increase soft f1 macro_cost tf. jpg of a fireboat spraying jets of water. This is not an exhaustive list of black box models. The original image sizes used for every version of EfficientNet are EfficientNetB0 224 224 3 EfficientNetB1 240 240 3 EfficientNetB2 260 260 3 EfficientNetB3 300 300 3 EfficientNetB4 380 380 3 EfficientNetB5 456 456 3 EfficientNetB6 528 528 3 EfficientNetB7 600 600 3 I LL BE USING EfficientNetB2 WITH 512x512x3 INPUT 7 nbsp nbsp MODEL TRAINING8 nbsp nbsp APPENDIX HOW TO RECREATE THE TFRECORDS For later visualization Machine Learning and Data Science Imports Built In Imports Visualization Imports PRESETS TPU detection no TPU found detect GPUs for GPU or multi GPU machines Define the root data directory Define the paths to the training and testing tfrecord and image folders respectively Capture all the relevant full image paths Capture all the relevant full tfrec paths Define paths to the relevant csv files Create the relevant dataframe objects Not the lowest as it is very underrepresented explicit size needed for TPU Defaults are not specified since both keys are required. float32 f1 2 tp 2 tp fn fp 1e 16 macro_f1 tf. org tutorials interpretability images IG_fireboat. Args y int32 Tensor targets array of shape BATCH_SIZE N_LABELS y_hat float32 Tensor probability matrix of shape BATCH_SIZE N_LABELS Returns cost scalar Tensor value of the cost function for the batch y tf. the inputs along the path from baseline to inputs. Feature Permutation Perturbation Assigns an importance score to each input feature based on the magnitude changes in model output or loss when those features are permuted based on input feature mask. Shapely Value Sampling Perturbation Similar to Shapely value but instead of considering all feature permutations it considers only samples random permutations. org wiki T distributed_stochastic_neighbor_embedding 1. 4 Why Is XAI Needed The Case For Growing Global AI Regulation Many regulatory bodies have begun to encourage or enforce explainability in predictive algorithms used in the public domain. IG Example https www. 0737 https arxiv. reduce_sum y_hat 1 y axis 0 fn tf. Args y int32 Tensor targets array of shape BATCH_SIZE N_LABELS y_hat float32 Tensor probability matrix from forward propagation of shape BATCH_SIZE N_LABELS Returns cost scalar Tensor value of the cost function for the batch y tf. images text structured data ease of implementation theoretical justifications and computational efficiency relative to alternative approaches that allows it to scale to large networks and feature spaces such as images. shard 4 index 0 yellow_train_ds train_ds. GradientSHAP Gradient Approximates SHAP values based on the expected gradients. skip N_VAL val_ds val_ds. As an example for the difficulty with perceptive interpretability when a visual evidence is given erroneously the underlying mathematical structure may not seem to provide useful clues on the mistakes. It adds each feature for each permutation one by one to the baseline and computes the magnitudes of output changes for each feature which are ultimately being averaged across all permutations to estimate final attribution score. selected input layer averages them for each output channel and multiplies with the layer activations. 4 WHERE IS XAI NEEDED Obviously things like the weakly supervised tasks in this competition may require XAI XAI can be used for a wide range of things that we won t get into here protecting against bias protecting against overfitting detecting features etc. IG has become a popular interpretability technique due to its broad applicability to any differentiable model e. Occlusion Perturbation Assigns an importance score to each input feature based on the magnitude changes in model output when those features are replaced by a baseline usually zeros using rectangular sliding windows and sliding strides. 1 Model Paramaters Configuration 6. png 3 nbsp nbsp NOTEBOOK SETUP4 nbsp nbsp HELPER FUNCTIONS5 nbsp nbsp DATASET PREPERATION6 nbsp nbsp MODEL CREATION6. Define a parser None or path to model num_parallel_reads None forces the order to be preserved See an example See examples ANNOYINGLY THIS DOES NOT WORK AS THE QUADRUPLE OF CHANNEL IMAGES IS IN A DIFFERENT ORDER EVERY TIME red_train_ds train_ds. IG is an Explainable AI XAI technique introduced in the paper Axiomatic Attribution for Deep Networks https arxiv. Finally the latter is multiplied by input baseline. See below list was created roughly a year ago GDPR Article 22 empowers individuals with the right to demand an explanation of how anautomated system made a decision that affects them. count_nonzero y_pred y axis 0 tf. It is simply the more common black box models. shard 4 index 2 blue_train_ds train_ds. Layer Conductance Gradient Decomposes integrated gradients via chain rule. count_nonzero 1 y_pred y axis 0 tf. It has many use cases including understanding feature importances identifying data skew and debugging model performance. reduce_mean cost average on all labels return macro_cost def macro_f1 y y_hat thresh 0. Human Protein Atlas Single Cell ClassificationExploratory Data Analysis EDA CREATED BY DARIEN SCHETTLERTABLE OF CONTENTS 0 nbsp nbsp nbsp nbsp IMPORTS 1 nbsp nbsp nbsp nbsp BACKGROUND INFORMATION 2 nbsp nbsp nbsp nbsp SETUP 3 nbsp nbsp nbsp nbsp HELPER FUNCTIONS 4 nbsp nbsp nbsp nbsp DATASET PREPERATION 5 nbsp nbsp nbsp nbsp MODEL BUILDING 6 nbsp nbsp nbsp nbsp MODEL TRAINING 0 nbsp nbsp IMPORTS1 nbsp nbsp XAI BACKGROUND INFORMATION1. 5 Compute the macro F1 score on a batch of observations average F1 across labels Args y int32 Tensor labels array of shape BATCH_SIZE N_LABELS y_hat float32 Tensor probability matrix from forward propagation of shape BATCH_SIZE N_LABELS thresh probability value above which we predict positive Returns macro_f1 scalar Tensor value of macro F1 for the batch y_pred tf. This version uses the computation of soft F1 for both positive and negative class for each label. org wiki File San_Francisco_fireboat_showing_off. LayerGradCam Gradient Computes the gradients of model outputs w. Create a Features message using tf. Create a dictionary mapping the feature name to the tf. reduce_sum 1 y_hat 1 y axis 0 soft_f1_class1 2 tp 2 tp fn fp 1e 16 soft_f1_class0 2 tn 2 tn fn fp 1e 16 cost_class1 1 soft_f1_class1 reduce 1 soft f1_class1 in order to increase soft f1 on class 1 cost_class0 1 soft_f1_class0 reduce 1 soft f1_class0 in order to increase soft f1 on class 0 cost 0. Shapely Value Perturbation Computes feature importances based on all permutations of all input features. float32 tp tf. float32 fp tf. 1 WHAT IS EXPLAINABLE AI GENERAL INFO For the purposes of this notebook and my explanation I will be logically seperating explainable AI into two seperate branches. batch BATCH_SIZE. Saliency Gradient The gradients of the output w. DeepLiftSHAP Gradient An extension of DeepLift that approximates SHAP values. AUTOTUNE loading in Tensorflow s SavedModel format def macro_double_soft_f1 y y_hat Compute the macro soft F1 score as a cost average 1 soft F1 across all labels. float32 y_hat tf. If there are any RELUs present in the model their gradients will be overridden so that only positive gradients of the inputs in case of Guided BackProp and outputs in case of deconvnet are back propagated. Starting with a high LR would break the pre trained weights. 2701 Establishes a commission on automated decision making transparency fairness and individual rights. They can only be easily perceived once the pattern is brought into lower dimensions abstracting some fine grained information we could not yet prove is not discriminative with measurable certainty. Layer Activation Computes the inputs or outputs of selected layer. arXiv preprint arXiv 1907. shard 4 index 3 green_train_ds train_ds. Washington Bill 1655 Establishes guidelines for the use of automated decision systems to protectconsumers improve transparency and create more market predictability. reduce_mean cost average on all labels return macro_cost def macro_soft_f1 y y_hat Compute the macro soft F1 score as a cost. Massachusetts Bill H. 2 THE GOAL Pull the veil back on black box machine learning models and help users understand how why a model makes the decisions that it does. Final SHAP values represent the expected values of gradients input baseline for each input example. Illinois House Bill 3415 States predictive data analytics determining creditworthiness or hiringdecisions may not include information that correlates with the applicant race or zip code. shard 4 index 1 36 is length of id always SEEDING KERNEL INIT val_ds train_ds. Algorithmic Accountability Act 2019 Requires companies to provide an assessment of the risks posed bythe automated decision system to the privacy or security and the risks that contribute to inaccurate unfair biased or discriminatory decisions impacting consumers California Consumer Privacy Act Requires companies to rethink their approach to capturing storing and sharing personal data to align with the new requirements by January 1 2020. Go to this notebook to see the implementation of IG. Linear Logistic Regression Decision Trees K Nearest Neighbors Rule Based Learners General Additive Models Bayesian ModelsBLACK BOX MODELSThese are models algorithms that are NOT easily interpretable and DO requre XAI. 2 nbsp nbsp INTEGRATED GRADIENTS BACKGROUND INFORMATIONThis notebook will show how to implement Integrated Gradients IG for this competition. reduce_mean f1 return macro_f1 Using an LR ramp up because fine tuning a pre trained model. count_nonzero y_pred 1 y axis 0 tf. Use probability values instead of binary predictions. Average 1 soft F1 across all labels. It approximates the integral of gradients defined by a chain rule described as the gradients of the output w. ipynb I ntegrated G radients IG aims to explain the relationship between a model s predictions in terms of its features. pdf PERCEPTIVE In short this is interpretability that can be observed by humans. For each input example it considers a distribution of baselines and computes the expected value of the attributions based on DeepLift algorithm across all input baseline pairs. Although occasionally post hoc analysis is required or basic explainability tools. 2 Model Creation Initialization The original image size from the EfficientNet paper for EfficientNetB2 is 260x260x3. Your model will also classify this image as a fireboat later on in this tutorial however does it highlight the same pixels as important when explaining its decision In the images below titled IG Attribution Mask and Original IG Mask Overlay you can see that your model instead highlights in purple the pixels comprising the boat s water cannons and jets of water as being more important than the boat itself to its decision. reduce_sum y_hat y axis 0 fp tf. VIEW SCHEDULE saving in Tensorflow s SavedModel format BytesList won t unpack a string from an EagerTensor. 3 CURRENT XAI APPROACHES Algorithm Type Description Integrated Gradients Gradient Approximates the integral of gradients along the path straight line from baseline to input sand multiplies with input baseline DeepLift Application Explains differences in the non linear activations outputs in terms of the differences of the input from its corresponding reference. Guided BackProp DeconvNet Gradient Computes the gradients of the model outputs w. You would classify this image as a fireboat and might highlight the pixels making up the boat and water cannons as being important to your decision. Layer Internal Influence Gradient Approximates the integral of gradients along the path from baseline to inputs for selected input layer. This can inform on how to improve the model as well as being useful for identifying things like bias and overfitting XAI APPROACHES https i. 5 cost_class1 cost_class0 take into account both class 1 and class 0 macro_cost tf. pdf The two major categories presented here namely perceptive interpretability and interpretability by mathematical structures appear to present different polarities within the notion of interpretability. float32 fn tf. co ZXdBQ4D Screen Shot 2020 07 07 at 10 24 16 AM. com github tensorflow docs blob master site en tutorials interpretability integrated_gradients. If a features is located in multiple hyper rectangles the importance scores are averaged across those hyper rectangles. MATHEMATICAL In short this is interpretability that can only be observed by first applying mathematical manipulations to the data. take N_VAL train_ds train_ds. Example compatible data type. Input Gradient Gradient Multiplies model inputs with the gradients of the model outputs w. reduce_sum 1 y_hat y axis 0 tn tf. greater y_hat thresh tf. Often the explanations arising through this branch are obvious to humans or already known. See this excerpt paper that explains the branches in more detail. 01365 LINKS nbsp nbsp nbsp nbsp Tensorflow Google Colab This Is Heavily Based Off Of https colab. ", "id": "dschettler8845/hpa-xai-ig-tfrecords-tpu-training", "size": "24339", "language": "python", "html_url": "https://www.kaggle.com/code/dschettler8845/hpa-xai-ig-tfrecords-tpu-training", "git_url": "https://www.kaggle.com/code/dschettler8845/hpa-xai-ig-tfrecords-tpu-training", "script": "glob macro_f1 plot_ex add_head_to_bb tif_gzip_to_png decode lrfn plotly.graph_objects matplotlib.patches macro_double_soft_f1 convert_rgby_to_rgb collections seaborn numpy get_new_data _bytes_feature _int64_feature PIL load_image plotly.express get_backbone macro_soft_f1 tensorflow_addons Image matplotlib.pyplot decode_image tqdm.notebook tensorflow pandas kaggle_datasets plot_rgb download_and_convert_tifgzip_to_png str_2_multi_hot_encoding matplotlib.colors Counter tqdm serialize flatten_list_of_lists augment KaggleDatasets ListedColormap _float_feature get_class_wts datetime preprocess_tfrec_ds ", "entities": "(('Model Creation image 2 original size', 'EfficientNetB2'), 'initialization') (('we', 'image classifier'), 'walk') (('example', 'image https commons'), 'consider') (('N_LABELS Returns', 'tf'), 'target') (('here namely perceptive interpretability', 'interpretability'), 'pdf') (('when features', 'input feature mask'), 'assign') (('we', 'smaller larger size'), 'bind') (('Many regulatory bodies', 'public domain'), '4') (('Finally latter', 'input baseline'), 'multiply') (('You', 'decision'), 'classify') (('Guided GradCam Gradient', 'GradCam up positive attributions'), 'compute') (('that', 'data'), 'MATHEMATICAL') (('version', 'label'), 'use') (('we', 'yet measurable certainty'), 'perceive') (('that', 'humans'), 'PERCEPTIVE') (('it', 'input baseline pairs'), 'consider') (('It', 'model data skew performance'), 'have') (('This', 'box exhaustive black models'), 'be') (('it', 'that'), '2') (('model', 'decision'), 'classify') (('2701', 'transparency fairness'), 'Establishes') (('example samples times', 'input baseline'), 'add') (('only positive gradients', 'deconvnet'), 'be') (('Layer Activation', 'selected layer'), 'compute') (('erroneously underlying mathematical structure', 'mistakes'), 'seem') (('This', 'XAI APPROACHES https i.'), 'inform') (('Often explanations', 'humans'), 'be') (('reduce_mean cost average', 'macro_cost def'), 'return') (('that', 'them'), 'create') (('most', 'SNE https'), 'cluster') (('1 1e 16 cost_class1 soft_f1_class1', 'class 0 cost'), 'reduce_sum') (('water jets', 'predictions'), 'generalize') (('input selected layer', 'layer activations'), 'average') (('NoiseTunnel', 'algorithm'), 'depend') (('XAI', 'box when black models'), 'be') (('QUADRUPLE', 'DIFFERENT ORDER'), 'define') (('I', 'two seperate branches'), '1') (('it', 'feature such images'), 'ease') (('Washington Establishes 1655 guidelines', 'market more predictability'), 'Bill') (('reduce_mean cost average', 'cost'), 'return') (('Gradient Approximates', 'expected gradients'), 'SHAP') (('that', 'applicant race'), 'include') (('fn 1 2 16 soft_f1', 'f1 soft macro_cost'), 'reduce_sum') (('that', 'January'), 'require') (('cost_class1 5 cost_class0', 'account'), 'take') (('which', 'attribution final score'), 'add') (('VIEW SCHEDULE', 'EagerTensor'), 'win') (('Value Perturbation Shapely Computes', 'input features'), 'feature') (('IG', 'features'), 'ntegrate') (('Layer Gradient Activation Gradient Computes', 'output w.'), 'element') (('when features', 'strides'), 'assign') (('It', 'output w.'), 'approximate') (('Layer Internal Influence Gradient', 'input selected layer'), 'approximate') (('that', 'SHAP values'), 'Gradient') (('that', 'more detail'), 'see') (('Tensorflow Google 01365 nbsp This', 'https Heavily colab'), 'LINKS') (('mathematical analysis', 'high dimensions'), 'provideinformation') (('AUTOTUNE', 'labels'), 'format') (('it', 'only random permutations'), 'perturbation') (('BackProp DeconvNet Guided Gradient', 'model outputs w.'), 'compute') (('Layer Conductance Gradient Decomposes', 'chain rule'), 'integrate') (('models that', 'easily generally XAI'), 'see') (('1 i', 'KERNEL always INIT'), 'be') (('notebook', 'competition'), 'INTEGRATED') (('SHAP Final values', 'example'), 'represent') (('IG', 'Axiomatic Deep Networks https arxiv'), 'be') (('models that', 'easily XAI'), 'Trees') (('importance scores', 'hyper rectangles'), 'rectangle') (('we', 'detecting features overfitting etc'), '4') (('nbsp HELPER 3 nbsp 4 nbsp nbsp DATASET', 'BACKGROUND nbsp 0 1 2 nbsp SETUP'), 'EDA') (('IG', 'model differentiable e.'), 'become') (('Gradient', 'model outputs w.'), 'LayerGradCam') (('keys', 'TPU Defaults'), 'be') (('Starting', 'pre trained weights'), 'break') (('scalar Tensor macro_f1 value', 'batch'), 'Compute') (('CURRENT 3 XAI', 'corresponding reference'), 'explain') ", "extra": "['biopsy of the greater curvature', 'test']"}